text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"Transcriptional Regulatory Networks of Craniofacial Development Abstract Human craniofacial development is a complex process and frequently goes awry to cause a major class of birth defects, orofacial clefting, which affects approximately 1 in 700 live births. Proper facial development in mouse and human requires three sets of paired facial prominences coming together by growth, morphogenesis, and fusion. Embryonic facial development is strikingly similar in human and mouse, making the mouse the best available model system for human. Previous studies have shown that the expression of many thousands of genes changes across tissue layer, age, and/or prominence, as well as cell population during early mouse facial development. However, we still only have a rudimentary understanding of how these changes are regulated by the interaction of transcriptional modulators in the developing face. To understand how genes are transcriptionally regulated during facial development, this research seeks to construct transcriptional regulatory networks in a temporospatial manner by in silico analysis of publicly available multi- omic datasets. Aim 1 will focus on the identification and verification of transcriptional regulatory networks operating in facial mesenchyme with a focus on super-enhancers. Aim 2 will adopt a similar approach to study the ectoderm which acts as a vital signaling center for the mesenchyme. Finally, in Aim 3 I will apply knowledge from Aims 1 and 2 to build transcriptional regulatory networks at the single cell level. These aims will take advantage of available RNA-seq, ATAC-seq, histone marker ChIP-seq, transcription factor ChIP-seq, bulk and single cell RNA-seq data from wild-type or mutant mice, as well as facial enhancer expression databases. Accomplishment of these studies will predict how genes are transcriptionally regulated in a temporospatial manner during facial development and discover sets of core transcription factors and super- enhancers controlling facial development. These transcriptional regulatory networks will be relevant to the genetic and molecular underpinnings of human orofacial clefting, and will provide clear testable predictions about transcription factor function and the consequences of aberrant expression. Performance and accomplishment of these Aims will also act as a major component of my career development plan, in which my goal is to obtain and independent tenure-track faculty position and serve as a mentor to the next generation of scientists. A major aspect of my career development plan is to build on my growing strength in bioinformatics by learning more advanced techniques in this specialty alongside new computational based approaches, such as machine learning. In this respect, my Aims and career development plan are aligned with a Notice of Special Interest (NOSI) of NIDCR in Supporting Dental, Oral, and Craniofacial Research Using Bioinformatic, Computational, and Data Science Approaches (NOT-DE-20-006) for which this application is targeted. I have recruited a mentorship team with specialties in craniofacial biology, bioinformatics, machine learning, and career development to help me achieve these goals. Project Narrative Human craniofacial development is a complex process and requires suites of genes to be switched on and off at appropriate times and spaces during formation of the embryo. There is now a wealth of data available in public databases concerning which genes are expressed when and where during mammalian facial development, including for the transcription factors which are the proteins that regulate these critical expression programs, but we have not yet begun to connect this information together to derive logical predictions about the critical networks responsible for craniofacial development. This proposal will address that gap using both computational and laboratory-based methods to derive and verify transcriptional regulatory networks relevant to the genetic and molecular underpinnings of normal facial development as well as how these are disrupted to cause defects such as human orofacial clefting.",Transcriptional Regulatory Networks of Craniofacial Development,10284443,K01DE030923,"['ATAC-seq', 'Address', 'Adopted', 'Affect', 'Age', 'Automobile Driving', 'Bioinformatics', 'Biological Assay', 'Biological Models', 'Biology', 'Cartilage', 'Cells', 'ChIP-seq', 'Complex', 'Computational Science', 'Computer Models', 'Congenital Abnormality', 'Data', 'Data Science', 'Data Set', 'Databases', 'Defect', 'Dental', 'Dependence', 'Development', 'Development Plans', 'Ectoderm', 'Embryo', 'Enhancers', 'Face', 'FaceBase', 'Faculty', 'Foundations', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Models', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Growth', 'Histones', 'Human', 'Human Genetics', 'Instruction', 'Knowledge', 'Laboratories', 'Learning', 'Live Birth', 'Machine Learning', 'Mentors', 'Mentorship', 'Mesenchymal', 'Mesenchyme', 'Methods', 'Molecular', 'Morphogenesis', 'Mus', 'Muscle', 'Mutant Strains Mice', 'National Institute of Dental and Craniofacial Research', 'Oral', 'Pathology', 'Pattern', 'Performance', 'Population', 'Positioning Attribute', 'Process', 'Proteins', 'Regulatory Element', 'Research', 'Resources', 'Scientist', 'Signal Pathway', 'Signal Transduction', 'Solid', 'Study models', 'System', 'Techniques', 'Technology', 'Time', 'Tissues', 'Transcriptional Regulation', 'Transgenic Organisms', 'Validation', 'Wild Type Mouse', 'base', 'bone', 'career', 'career development', 'cell type', 'craniofacial', 'craniofacial development', 'critical period', 'design', 'differential expression', 'in silico', 'interest', 'medical specialties', 'mouse model', 'multiple omics', 'network models', 'next generation', 'orofacial cleft', 'programs', 'promoter', 'recruit', 'research and development', 'single-cell RNA sequencing', 'spatiotemporal', 'tenure track', 'transcription factor', 'transcriptome', 'transcriptome sequencing']",NIDCR,UNIVERSITY OF COLORADO DENVER,K01,2021,130135
"Computational and brain predictors of emotion cue integration The purpose of this project is to develop computational and brain-based models of emotion cue integration: people’s inferences about others’ emotions based on dynamic, multimodal cues. Observers often decide how targets feel based on cues such as facial expressions, prosody, and language. Such inferences scaffold healthy social interaction, and abnormal inference both marks and exacerbates social deficits in numerous psychiatric disorders. Psychologists and neuroscientists have studied emotion inference for decades, but the vast majority of this work employs simplified social cues, such as vignettes or static images of faces. By contrast, “real world” emotion cues are complex, dynamic, and multimodal. Cue integration—inference based on naturalistic emotion information—likely differs from simpler inference at cognitive and neural levels, but this phenomenon remains poorly understood. This means that scientists lack a clear model of how observers adaptively process complex emotion cues, and how that processing goes awry in mental illness. Especially lacking are mechanistic models that can describe the computations and brain processes involved in cue integration with sufficient precision to predict inference in new cases, observers, and samples. This project will merge tools from social psychology, computer science, and neuroscience to generate a novel and rigorous model of emotion cue integration. We have demonstrated that in the face of complex emotion cues, observers dynamically “weight” cues from each modality (e.g., visual, linguistic) over time, a process that (i) tracks shifts in brain activity and connectivity; and (ii) can be captured using Bayesian models. Here, we will expand this work in several ways. First, we will develop precise computational tools to isolate features of emotion cues—such as facial movements, prosody, and linguistic sentiment—that track observers’ use of each cue modality during integration. Second, we will develop multi-region “signatures” of brain activity and connectivity that track emotion inference in each modality. We will use these signatures in conjunction with machine learning to predict unimodal emotion inference and cue integration in new observers and samples, based on brain data alone. Third, we will explore the context-dependence of naturalistic emotion inference by testing whether reinforcement learning can bias observers’ cue integration and accompanying brain signatures. Finally, we will model computational and neural abnormalities associated with cue integration in patients with Major Depressive Disorder and Bipolar Disorder. At the level of basic science, these data will generate a fundamentally new—and more naturalistic—approach to the neuroscience of emotion inference. The computational and brain metrics we produce will also be made publically available to facilitate the open and cumulative study of emotion inference across labs. At a translational level, we will provide a mechanistic, rich account of abnormal emotion inference in mood disorders, paving the way for computational and brain markers that can be used to assess social dysfunction and treatment efficacy in these and other mental illnesses. PROJECT NARRATIVE The proposed research will use methods from social psychology, cognitive neuroscience, and computer science to (i) precisely model people’s inferences about others’ emotions based on complex, dynamic cues, (ii) generate multi-region, brain-based predictors of these inferences, and (iii) characterize abnormalities in inference among individuals with mood disorders. Several psychiatric and neurodevelopmental disorders are characterized by difficulties understanding others’ emotions, which in turn worsen social functioning in patients. In addition to providing new insights about social processes—a core target within the NIMH’s research domain criteria (RDoC) framework—the proposed research will offer powerful, novel computational and neural targets through which to assess and treat difficulties in emotion inference, and to eventually reduce the social burden faced by people with mental illness on a broad scale.",Computational and brain predictors of emotion cue integration,10138024,R01MH112560,"['Affect', 'Agreement', 'Base of the Brain', 'Basic Science', 'Bayesian Modeling', 'Bipolar Disorder', 'Brain', 'Brain region', 'Classification', 'Cognitive', 'Complex', 'Computer Models', 'Cues', 'Data', 'Dependence', 'Emotional', 'Emotional disorder', 'Emotions', 'Event', 'Exhibits', 'Face', 'Face Processing', 'Facial Expression', 'Functional Magnetic Resonance Imaging', 'Future', 'Image', 'Individual', 'Language', 'Lateral', 'Learning', 'Life', 'Linguistics', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental disorders', 'Methods', 'Modality', 'Modeling', 'Mood Disorders', 'Moods', 'Movement', 'National Institute of Mental Health', 'Neurodevelopmental Disorder', 'Neurosciences', 'Observer Variation', 'Participant', 'Patients', 'Pattern', 'Perception', 'Process', 'Psychological reinforcement', 'Psychologist', 'Research', 'Research Domain Criteria', 'Research Personnel', 'Running', 'Sampling', 'Scanning', 'Scientist', 'Sensory', 'Social Functioning', 'Social Interaction', 'Social Processes', 'Social Psychology', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Testing', 'Time', 'Training', 'Treatment Efficacy', 'Visual', 'Weight', 'Work', 'affective computing', 'base', 'brain abnormalities', 'cognitive neuroscience', 'computer science', 'computerized tools', 'executive function', 'insight', 'language comprehension', 'multimodality', 'neuroimaging', 'novel', 'recruit', 'relating to nervous system', 'response', 'scaffold', 'social', 'social deficits', 'tool']",NIMH,STANFORD UNIVERSITY,R01,2021,249264
"Unpacking Emotion Inflexibility and Prospective Prediction of Affective Disease PROJECT ABSTRACT/SUMMARY The prevalence of psychiatric disorders has reached nearly epidemic proportions. Rates of common affective diseases (unipolar depression, anxiety and stress disorders) are high across the lifespan and these diseases place a tremendous social and economic burden on the individual and society. Clear evidence indicates that most affective disorders emerge at the intersection of pre-existing vulnerability and significant, highly stressful, life-events. However, current models of emotion-related risk do not adequately account for this confluence of biological, historical, and situational factors. In this investigation, we build upon our prior work demonstrating broad associations between flexible emotion processing and psychological health and adjustment, and in-flexible emotion and psychological risk and affective disease. Specifically, we will recruit 400 adults in hospital following a potentially traumatic event (e.g., accident, violence, fire, etc.) in order to model the influence of early emotion processing on trajectories of adjustment. We focus our investigation on the super-ordinate construct of Emotion flexibility (EF) which encompasses the ability to generate or up-regulate emotions, as well as to shift or down-regulate emotions according to needs and/or environmental demands. EF is well-suited to inform models of emotion-related risk and adjustment as it characterizes an optimal balance of two biologically-based, constituent dimensions: “bottom-up” threat-related processing and “top-down” cognitive control increasingly recognized as central to all emotion processing. We propose rigorous methods to assess EF and related processing in-vivo in lab and via experience sampling. Moreover, we will follow participants to 18 months post event so as to effectively model the association between emotion processing and trajectories of adjustment, while also considering established influences such as physical health status, psychiatric history, childhood maltreatment, daily stress/hassles, and social support. In particular, we will incorporate recent developments in advanced statistical modelling to better characterize the complex and interactive influence of historical and contemporary factors on moment-level emotion processing, EF and adjustment. Broadly, this project is in line with the most recent NIMH strategic plan and will contribute to more complex models of the most common affective diseases, including facilitating the charting of illness trajectories to help determine when, where, and how to intervene. Moreover, this research will directly examine how variation in key systems can influence emotion-processing and adjustment to aversive life events, fitting complex influences more directly into models of risk for the most common and burdensome affective diseases. PUBLIC HEALTH RELEVANCE/NARRATIVE Emotion-related psychiatric disorders, including depression and anxiety, affect a considerable portion of adults in this country and rank as many of the most burdensome diseases worldwide. In this investigation, we will follow an at-risk sample of adults in order to better understand how one key pathway, relating to how individuals process emotion, influences risk for emotion-related diseases over time. In addition, we test the role by which certain other factors, both contemporary and historical (physical health, life stress, social support, psychiatric treatment history, or childhood experiences) may increase or decrease risk via this particular pathway.",Unpacking Emotion Inflexibility and Prospective Prediction of Affective Disease,10089147,R01MH113622,"['Accidents', 'Adult', 'Affect', 'Affective', 'Anxiety', 'Anxiety Disorders', 'Behavioral', 'Biological', 'Categories', 'Child Abuse and Neglect', 'Childhood', 'Clinical', 'Clinical Sciences', 'Complex', 'Country', 'Derivation procedure', 'Development', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Progression', 'Early Intervention', 'Economic Burden', 'Emotions', 'Epidemic', 'Equilibrium', 'Event', 'Fire - disasters', 'Health', 'Health Status', 'Heritability', 'Hospitals', 'Individual', 'Inherited', 'Investigation', 'Life', 'Life Stress', 'Longevity', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Mood Disorders', 'National Institute of Mental Health', 'Nature', 'Participant', 'Pathway interactions', 'Patients', 'Pattern', 'Phenotype', 'Post-Traumatic Stress Disorders', 'Prevalence', 'Process', 'Psychiatric therapeutic procedure', 'Psychological adjustment', 'Qualifying', 'Recording of previous events', 'Regulation', 'Research', 'Research Domain Criteria', 'Risk', 'Risk Adjustment', 'Risk Factors', 'Role', 'Sampling', 'Shapes', 'Sleep disturbances', 'Social support', 'Societies', 'Statistical Models', 'Strategic Planning', 'Stress', 'Symptoms', 'System', 'Testing', 'Time', 'Unipolar Depression', 'Variant', 'Violence', 'Work', 'base', 'childhood adversity', 'clinical diagnostics', 'clinically relevant', 'cognitive control', 'emotion regulation', 'experience', 'flexibility', 'improved', 'in vivo', 'indexing', 'laboratory experience', 'longitudinal design', 'machine learning method', 'negative mood', 'network models', 'physical conditioning', 'post-traumatic stress', 'prospective', 'psychologic', 'public health relevance', 'recruit', 'response', 'social', 'stress disorder', 'tool', 'traumatic event']",NIMH,KENT STATE UNIVERSITY,R01,2021,664089
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases ﻿    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand. PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9841303,R01DC014498,"['3-Dimensional', 'Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'American Sign Language', 'Articulation', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Databases', 'Detection', 'Devices', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Human', 'Image', 'Individual', 'Intervention', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deaf', 'deafness', 'design', 'experience', 'experimental study', 'face perception', 'innovation', 'instructor', 'interest', 'machine learning algorithm', 'prevent', 'public health relevance', 'reconstruction', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2021,317844
"Optimized Affective Computing Measures of Social Processes and Negative Valence in Youth Psychopathology ABSTRACT Difficulties with emotion expression and social behavior characterize multiple psychiatric conditions and negatively impact child development. However, existing measurement tools for indexing social-emotional function are imprecise and subjective, or require specialized training that is costly and time-intensive, prohibiting widespread implementation. The imprecision of existing tools has a major negative impact not only on research, but on the ability to assess and treat individuals with mental health concerns – especially among underserved and under-resourced populations. Here, we propose to address this problem by quantifying social and emotional behavior using novel biobehavioral markers derived from computer vision (facial expression analysis) and computational linguistics (social/sentiment analysis). Our team has successfully used these markers to predict the presence of autism spectrum disorder (ASD) with 91% accuracy. In this proposal, we determine the extent to which our markers can serve as continuous measures of social behavior and negative emotion to advance clinical phenotyping and interventions. The proposal brings together two high-bandwidth clinical research programs at the Children’s Hospital of Philadelphia and Baylor College of Medicine to collect data on 750 adolescents (ages 12-17 inclusive) with ASD, a primary anxiety or depressive disorder, or without any developmental/psychiatric condition. At a single assessment, all youth will participate in an extensive clinical phenotyping battery consisting of validated clinical interviews and child-/parent-report scales assessing converging and diverging mental health constructs, and three tasks eliciting positive/negative emotion, social stress, and mild frustration. A subsample of 150 adolescents will be reassessed 6-10 weeks later to allow retest/stability analyses. A novel camera apparatus will capture naturalistic synchronized verbal and nonverbal signals from dyads. Our analytic approach combines state-of-the-art machine learning, computational linguistics, and computer vision – including facial emotion recognition methods that rival several commonly used alternatives. The ultimate goal of this proposal is to develop valid and objective measures of the Social and Negative Valence Systems using novel biobehavioral markers in a large transdiagnostic sample of youth. Secondary goals are to develop easy-to-follow methods to widely disseminate our tools and procedures, and to characterize individual variability in these key RDoC metrics by age, gender, race/ethnicity, and diagnosis. The achievement of these goals will provide researchers with sorely needed measures of social and emotional behavior, and provide clinicians with a new set of tools for identifying and tracking youth in need of mental health treatment. PROJECT NARRATIVE Problems with negative emotion and social behavior are present across many behavioral health conditions, but are poorly measured with existing tools. This application addresses this problem by developing and validating novel facial expression and linguistic measures that can be easily and widely disseminated across research and clinical settings.",Optimized Affective Computing Measures of Social Processes and Negative Valence in Youth Psychopathology,10183399,R01MH125958,"['Achievement', 'Address', 'Adolescence', 'Adolescent', 'Age', 'Anxiety', 'Anxiety Disorders', 'Behavior', 'Behavior assessment', 'Behavioral Sciences', 'Child', 'Child Development', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Computational Linguistics', 'Computer Vision Systems', 'Computers', 'Data', 'Data Collection', 'Depressive disorder', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Elements', 'Emotional', 'Emotions', 'Ethnic Origin', 'Face', 'Facial Expression', 'Factor Analysis', 'Frustration', 'Funding', 'Gender', 'Goals', 'Grain', 'Hour', 'Human', 'Individual', 'Individual Differences', 'Intervention', 'Interview', 'Investments', 'Judgment', 'Laboratories', 'Linguistics', 'Machine Learning', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Mental Health', 'Methodology', 'Methods', 'Modeling', 'Mood Disorders', 'Moods', 'Multivariate Analysis', 'NIH Program Announcements', 'National Institute of Mental Health', 'Negative Valence', 'Parents', 'Participant', 'Pediatric Hospitals', 'Phenotype', 'Philadelphia', 'Play', 'Population', 'Predictive Analytics', 'Procedures', 'Psychiatric Diagnosis', 'Psychopathology', 'Quality of life', 'Race', 'Reporting', 'Research', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Sampling', 'Signal Transduction', 'Site', 'Smiling', 'Social Behavior', 'Social Processes', 'Specialist', 'System', 'Testing', 'Thinness', 'Time', 'Training', 'Translating', 'Trier Social Stress Test', 'Variant', 'Youth', 'affective computing', 'aged', 'autism spectrum disorder', 'autistic children', 'base', 'behavior measurement', 'behavioral health', 'biobehavior', 'clinical phenotype', 'college', 'cost', 'digital', 'emotional behavior', 'emotional functioning', 'experimental study', 'indexing', 'individual variation', 'natural language', 'novel', 'programs', 'repetitive behavior', 'response', 'showing emotion', 'social', 'social anxiety', 'social deficits', 'social metrics', 'social stress', 'tool']",NIMH,CHILDREN'S HOSP OF PHILADELPHIA,R01,2021,861615
"Automatic Multimodal Affect Detection for Research and Clinical Use  Project Summary A reliable and valid automated system for quantifying human affective behavior in ecologically important naturalistic environments would be a transformational tool for research and clinical practice. With NIMH support (MH R01-096951), we have made fundamental progress toward this goal. In the proposed project, we extend current capabilities in automated multimodal measurement of affective behavior (visual, acoustic, and verbal) to develop and validate an automated system for detecting the constructs of Positive, Aggressive, and Dysphoric behavior and component lower-level affective behaviors and verbal content. The system is based on the manual Living in Family Environments Coding System that has yielded critical findings related to developmental psychopathology and interpersonal processes in depression and other disorders. Two models will be developed. One will use theoretically-derived features informed by previous research in behavioral science and affective computing; the other empirically derived features informed by Deep Learning. The models will be trained in three separate databases of dyadic and triadic interaction tasks from over 1300 adolescent and adult participants from the US and Australia. Intersystem reliability with manual coding will be evaluated using k-fold cross-validation for both momentary and session level summary scores. Differences between models and in relation to participant factors will be tested using the general linear model. To ensure generalizability, we further will train and test between independent databases as well. To evaluate construct validity of automated coding, we will use the ample validity data available in the three databases to determine whether automated coding achieves the same or better pattern of findings with respect to depression risk and development. Following procedures already in place for sharing databases and software tools, we will design the automated systems for use by non-specialists and make them available for research and clinical use. Achieving these goals will provide behavioral science with powerful tools to examine basic questions in emotion, psychopathology, and interpersonal processes; and clinicians to improve assessment and ability to track change in clinical and interpersonal functioning over time. Relevance For behavioral science, automated coding of affective behavior from multimodal (visual, acoustic, and verbal) input will provide researchers with powerful tools to examine basic questions in emotion, psychopathology, and interpersonal processes. For clinical use, automated measurement will help clinicians to assess vulnerability and protective factors and response to treatment for a wide range of disorders. More generally, automated measurement would contribute to advances in intelligent tutors in education, training in social skills and persuasion in counseling, and affective computing more broadly. Narrative Observational methods of measuring affective behavior have yielded critical insights into emotion, socio-emotional development, and psychopathology. A persistent barrier to their wide application is that they are labor-intensive to learn and to use. Our interdisciplinary team of behavioral and computer scientists will develop and validate a fully automated system for measuring affective behavior from multimodal (face, gaze, body, voice, and speech) input for research and clinical use.",Automatic Multimodal Affect Detection for Research and Clinical Use ,10162316,R01MH096951,"['Acoustics', 'Adolescent', 'Adult', 'Affect', 'Affective', 'Anxiety', 'Australia', 'Behavior', 'Behavioral', 'Behavioral Sciences', 'Clinical', 'Code', 'Communities', 'Computer software', 'Computers', 'Counseling', 'Data', 'Databases', 'Detection', 'Development', 'Disease', 'Emotional', 'Emotions', 'Ensure', 'Environment', 'Etiology', 'Face', 'Facial Expression', 'Family', 'Goals', 'Human', 'Intervention', 'Laboratories', 'Learning', 'Linear Models', 'Linguistics', 'Machine Learning', 'Manuals', 'Measurement', 'Measures', 'Mental Depression', 'Mental Health', 'Methods', 'Modality', 'Modeling', 'Motion', 'National Institute of Child Health and Human Development', 'National Institute of Mental Health', 'Pain', 'Parents', 'Participant', 'Pattern', 'Personal Computers', 'Persuasive Communication', 'Procedures', 'Process', 'Psychopathology', 'Research', 'Research Personnel', 'Risk', 'Running', 'Scientist', 'Software Tools', 'Speech', 'Standardization', 'Subgroup', 'System', 'Testing', 'Time', 'Training', 'Training and Education', 'Triad Acrylic Resin', 'United States National Institutes of Health', 'Validation', 'Verbal Behavior', 'Visual', 'Voice', 'Voice Quality', 'Work', 'affective computing', 'base', 'clinical practice', 'conduct problem', 'deep learning', 'design', 'emotional functioning', 'follow-up', 'gaze', 'improved', 'insight', 'intelligent tutoring system', 'interpersonal conflict', 'multimodality', 'protective factors', 'psychological distress', 'shared database', 'social skills', 'tool', 'treatment response']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2021,581743
"Neurocomputational Approaches to Emotion Representation Maintaining an adaptive balance of emotions is central to well-being, and dysregulated emotions contribute broadly to clinical disorders that impart high personal and societal burdens. Recognizing the transdiagnostic importance of emotion to mental health, the National Institute of Health's Research Domain Criteria (RDoC) matrix contains overarching domains of Negative Valence, Positive Valence, and Arousal. However, the matrix underspecifies how specific affective states like sadness, anxiety, or craving are organized within and across these domains, in part because it is unknown whether representations of discrete emotions are reliably differentiated. Other RDoC constructs, such as rumination and worry, modify the temporal parameters of emotions that confer psychopathology risk and exacerbate symptom maintenance. Nonetheless, it is unknown how these processes interface with emotional brain circuits to impact affect dynamics, particularly as they often occur spontaneously during mind wandering. The proposed research promises to improve the RDoC depiction of these emotion-related constructs by taking an affective computing approach. During combined recording of psychophysiology and functional magnetic resonance imaging (fMRI), adult participants will experience emotions to vignettes and movie clips spanning the arousal and valence dimensions, and will report on their spontaneous emotions during resting-state fMRI scans. Machine learning algorithms will decode emotion- specific signals across the levels of analysis, which will be integrated using Bayesian state-space modeling. An analysis of classifier errors will test competing predictions from emotion theories regarding the optimal structure of affective space. Using graph theoretic tools, we will characterize the neural network architecture of the discrete emotion representations to identify provincial and connector hubs that can be used as novel targets for future symptom-specific or co-morbid neuromodulation interventions, respectively. We will apply the emotion-specific maps to resting-state data from the same participants to create neurophysiological indices of spontaneous emotions and to relate their frequencies to measures of trait and state affect as a validation step. Using stochastic modeling of the resting-state data, we will derive temporal dynamics metrics to test the hypothesis that rumination and worry promote emotional inertia during mind wandering. Finally, we will use existing data repositories to demonstrate that our novel indices of affect dynamics transdiagnostically differentiate resting-state fMRI activity patterns in mental health disorders from healthy controls. The proposed research will improve upon current RDoC formulations of Negative Affect, Positive Affect, and Arousal domains by informing how discrete emotions are organized within and across these domains, by integrating emotion representations across multiple RDoC units of analysis, by informing how rumination and worry impact neurophysiological signatures of spontaneous emotions, and by establishing the clinical utility of computationally-derived metrics of emotion dynamics. PROJECT NARRATIVE  The overall goal of this project is to characterize how emotions are represented in the brain and autonomic nervous system. Computational modeling approaches will be applied to integrate the data across multiple sources and tasks, to test competing theories about how emotions are organized, and to derive new metrics of emotion dynamics. The outcome of the work will inform how emotions should be conceptualized within a broader research framework of mental health domains.",Neurocomputational Approaches to Emotion Representation,10227196,R01MH124112,"['Adult', 'Affect', 'Affective', 'Age', 'Anxiety', 'Arousal', 'Autonomic nervous system', 'Basic Science', 'Behavioral', 'Brain', 'Categories', 'Classification', 'Clinical', 'Clip', 'Code', 'Computer Models', 'Data', 'Depressed mood', 'Dimensions', 'Disease', 'Emotional', 'Emotions', 'Equilibrium', 'Exhibits', 'Formulation', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Graph', 'Human', 'Individual', 'Individual Differences', 'Intervention', 'Link', 'Machine Learning', 'Maintenance', 'Maps', 'Measures', 'Mental Health', 'Mental disorders', 'Methods', 'Mind', 'Modeling', 'Negative Valence', 'Outcome', 'Participant', 'Patient Self-Report', 'Pattern', 'Personal Satisfaction', 'Physiological', 'Population', 'Positive Valence', 'Process', 'Psychopathology', 'Psychophysiology', 'Reporting', 'Research', 'Research Domain Criteria', 'Rest', 'Risk', 'Role', 'Signal Transduction', 'Source', 'Space Models', 'Structure', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Work', 'affective computing', 'anxious', 'anxious individuals', 'base', 'biobehavior', 'comorbidity', 'craving', 'data repository', 'experience', 'functional MRI scan', 'improved', 'indexing', 'machine learning algorithm', 'markov model', 'movie', 'negative affect', 'neural network architecture', 'neurophysiology', 'neuroregulation', 'novel', 'organizational structure', 'relating to nervous system', 'repository', 'theories', 'tool', 'trait']",NIMH,DUKE UNIVERSITY,R01,2021,774017

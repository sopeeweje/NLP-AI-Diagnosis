text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA   DESCRIPTION: Dysarthria is a frequent result of several neurological disorders,      including Parkinson disease, stroke, cerebellar pathologies, multiple                sclerosis, and traumatic brain injury. Dysarthrias often lead to decreased           speech intelligibility, but they also affect other dimensions of spoken              language, including voice quality, prosody, and paralinguistic features. These       have not been studied collectively in large numbers of individuals with              dysarthria. This project uses a multiple-task protocol with both perceptual and      acoustic measures to examine intelligibility, voice quality, prosody, and            paralinguistic aspects in children and adults with acquired dysarthria. Several      newly developed analyses will be used to provide quantitative data toward the        construction of profiles of speech impairment and neurologic lesion. Included        will be the first systematic replication of the original work that led to the        contemporary classification of the dysarthrias. The data on dysarthria will be       integrated with data on speech development and normal adult speech in a neural       network model of speech production that is based on internal models of auditory      and articulatory representations of speech.                                                                                                                               n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6833535,R01DC000319,"['Parkinson&apos', 's disease', 'adult human (21+)', 'amyotrophic lateral sclerosis', 'artificial intelligence', 'behavioral /social science research tag', 'cerebellar disorders', 'cerebral palsy', 'child (0-11)', 'clinical research', 'disease /disorder classification', 'dysarthria', 'human subject', 'multiple sclerosis', 'nervous system disorder diagnosis', 'neural information processing', 'perception', 'speech', 'speech disorder diagnosis', 'stroke', 'vocabulary', 'voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,2005,437328,0.36927790685644113
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA   DESCRIPTION: Dysarthria is a frequent result of several neurological disorders,      including Parkinson disease, stroke, cerebellar pathologies, multiple                sclerosis, and traumatic brain injury. Dysarthrias often lead to decreased           speech intelligibility, but they also affect other dimensions of spoken              language, including voice quality, prosody, and paralinguistic features. These       have not been studied collectively in large numbers of individuals with              dysarthria. This project uses a multiple-task protocol with both perceptual and      acoustic measures to examine intelligibility, voice quality, prosody, and            paralinguistic aspects in children and adults with acquired dysarthria. Several      newly developed analyses will be used to provide quantitative data toward the        construction of profiles of speech impairment and neurologic lesion. Included        will be the first systematic replication of the original work that led to the        contemporary classification of the dysarthrias. The data on dysarthria will be       integrated with data on speech development and normal adult speech in a neural       network model of speech production that is based on internal models of auditory      and articulatory representations of speech.                                                                                                                               n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6689543,R01DC000319,"['Parkinson&apos', 's disease', 'adult human (21+)', 'amyotrophic lateral sclerosis', 'artificial intelligence', 'behavioral /social science research tag', 'cerebellar disorders', 'cerebral palsy', 'child (0-11)', 'clinical research', 'disease /disorder classification', 'dysarthria', 'human subject', 'multiple sclerosis', 'nervous system disorder diagnosis', 'neural information processing', 'perception', 'speech', 'speech disorder diagnosis', 'stroke', 'vocabulary', 'voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,2004,424599,0.36927790685644113
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA   DESCRIPTION: Dysarthria is a frequent result of several neurological disorders,      including Parkinson disease, stroke, cerebellar pathologies, multiple                sclerosis, and traumatic brain injury. Dysarthrias often lead to decreased           speech intelligibility, but they also affect other dimensions of spoken              language, including voice quality, prosody, and paralinguistic features. These       have not been studied collectively in large numbers of individuals with              dysarthria. This project uses a multiple-task protocol with both perceptual and      acoustic measures to examine intelligibility, voice quality, prosody, and            paralinguistic aspects in children and adults with acquired dysarthria. Several      newly developed analyses will be used to provide quantitative data toward the        construction of profiles of speech impairment and neurologic lesion. Included        will be the first systematic replication of the original work that led to the        contemporary classification of the dysarthrias. The data on dysarthria will be       integrated with data on speech development and normal adult speech in a neural       network model of speech production that is based on internal models of auditory      and articulatory representations of speech.                                                                                                                               n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6626845,R01DC000319,"[""Parkinson's disease"", ' adult human (21+)', ' amyotrophic lateral sclerosis', ' artificial intelligence', ' behavioral /social science research tag', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' clinical research', ' disease /disorder classification', ' dysarthria', ' human subject', ' multiple sclerosis', ' nervous system disorder diagnosis', ' neural information processing', ' perception', ' speech', ' speech disorder diagnosis', ' stroke', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,2003,412234,0.36927790685644113
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA   DESCRIPTION: Dysarthria is a frequent result of several neurological disorders,      including Parkinson disease, stroke, cerebellar pathologies, multiple                sclerosis, and traumatic brain injury. Dysarthrias often lead to decreased           speech intelligibility, but they also affect other dimensions of spoken              language, including voice quality, prosody, and paralinguistic features. These       have not been studied collectively in large numbers of individuals with              dysarthria. This project uses a multiple-task protocol with both perceptual and      acoustic measures to examine intelligibility, voice quality, prosody, and            paralinguistic aspects in children and adults with acquired dysarthria. Several      newly developed analyses will be used to provide quantitative data toward the        construction of profiles of speech impairment and neurologic lesion. Included        will be the first systematic replication of the original work that led to the        contemporary classification of the dysarthrias. The data on dysarthria will be       integrated with data on speech development and normal adult speech in a neural       network model of speech production that is based on internal models of auditory      and articulatory representations of speech.                                                                                                                               n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6489511,R01DC000319,"[""Parkinson's disease"", ' adult human (21+)', ' amyotrophic lateral sclerosis', ' artificial intelligence', ' behavioral /social science research tag', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' clinical research', ' disease /disorder classification', ' dysarthria', ' human subject', ' multiple sclerosis', ' nervous system disorder diagnosis', ' neural information processing', ' perception', ' speech', ' speech disorder diagnosis', ' stroke', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,2002,402685,0.36927790685644113
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA   DESCRIPTION: Dysarthria is a frequent result of several neurological disorders,      including Parkinson disease, stroke, cerebellar pathologies, multiple                sclerosis, and traumatic brain injury. Dysarthrias often lead to decreased           speech intelligibility, but they also affect other dimensions of spoken              language, including voice quality, prosody, and paralinguistic features. These       have not been studied collectively in large numbers of individuals with              dysarthria. This project uses a multiple-task protocol with both perceptual and      acoustic measures to examine intelligibility, voice quality, prosody, and            paralinguistic aspects in children and adults with acquired dysarthria. Several      newly developed analyses will be used to provide quantitative data toward the        construction of profiles of speech impairment and neurologic lesion. Included        will be the first systematic replication of the original work that led to the        contemporary classification of the dysarthrias. The data on dysarthria will be       integrated with data on speech development and normal adult speech in a neural       network model of speech production that is based on internal models of auditory      and articulatory representations of speech.                                                                                                                               n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6286948,R01DC000319,"[""Parkinson's disease"", ' adult human (21+)', ' amyotrophic lateral sclerosis', ' artificial intelligence', ' behavioral /social science research tag', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' clinical research', ' disease /disorder classification', ' dysarthria', ' human subject', ' multiple sclerosis', ' nervous system disorder diagnosis', ' neural information processing', ' perception', ' speech', ' speech disorder diagnosis', ' stroke', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,2001,407733,0.36927790685644113
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA The goal of this research is to develop improved assessment protocols            that afford a quantitative and analytic evaluation of speech impairment          in children and adults with neurological disorders.  Speech impairments          (dysarthrias) will be studied in persons with amyotrophic lateral                sclerosis, Parkinson's disease, stroke, cerebellar degeneration, cerebral        palsy, and developmental speech disorders.  Improved evaluation of speech        intelligibility is a particular focus of this work, but issues of speech         and voice quality also are addressed.  The methods to be used are a              combination of standard clinical assessments (such as rating scales),            intelligibility evaluations, and computer-based acoustic analyses.               Specifically, the methods include: perceptual ratings of speech by               experienced clinicians, quantitative assessment of intelligibility, a            multiple-parameter acoustic analysis, computer correction of speech              abnormalities through LPC resynthesis of the acoustic signal, and                derivation of vocal tract shape from acoustic parameters.  Work in all           these areas will be based on recordings of speech samples from a large           number of individuals with dysarthria.  One product of the research will         be a library of clinical profiles including intelligibility scores,              phonetic feature analyses, ratings of speech/voice quality, acoustic             measures, and neurological diagnosis.  Particular attention will be given        to the influences of subject age and sex on the characteristics of               dysarthria for a given neurological diagnosis.  The assessment protocols         will be implemented on microcomputers and designed to be incorporated in         clinical practice.  The research also will contribute to the development         of expert systems for the rating and classification of dysarthria.                n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,6030145,R01DC000319,"[""Parkinson's disease"", ' amyotrophic lateral sclerosis', ' artificial intelligence', ' audiotape', ' behavioral /social science research tag', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' computer data analysis', ' computer system design /evaluation', ' disease /disorder classification', ' dysarthria', ' human subject', ' information systems', ' larynx', ' mathematical model', ' nervous system disorder diagnosis', ' personal computers', ' speech', ' speech disorder diagnosis', ' stroke', ' technology /technique', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,1999,352814,0.35559317501889737
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA The goal of this research is to develop improved assessment protocols            that afford a quantitative and analytic evaluation of speech impairment          in children and adults with neurological disorders.  Speech impairments          (dysarthrias) will be studied in persons with amyotrophic lateral                sclerosis, Parkinson's disease, stroke, cerebellar degeneration, cerebral        palsy, and developmental speech disorders.  Improved evaluation of speech        intelligibility is a particular focus of this work, but issues of speech         and voice quality also are addressed.  The methods to be used are a              combination of standard clinical assessments (such as rating scales),            intelligibility evaluations, and computer-based acoustic analyses.               Specifically, the methods include: perceptual ratings of speech by               experienced clinicians, quantitative assessment of intelligibility, a            multiple-parameter acoustic analysis, computer correction of speech              abnormalities through LPC resynthesis of the acoustic signal, and                derivation of vocal tract shape from acoustic parameters.  Work in all           these areas will be based on recordings of speech samples from a large           number of individuals with dysarthria.  One product of the research will         be a library of clinical profiles including intelligibility scores,              phonetic feature analyses, ratings of speech/voice quality, acoustic             measures, and neurological diagnosis.  Particular attention will be given        to the influences of subject age and sex on the characteristics of               dysarthria for a given neurological diagnosis.  The assessment protocols         will be implemented on microcomputers and designed to be incorporated in         clinical practice.  The research also will contribute to the development         of expert systems for the rating and classification of dysarthria.                n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,2733645,R01DC000319,"[""Parkinson's disease"", ' amyotrophic lateral sclerosis', ' artificial intelligence', ' audiotape', ' behavioral /social science research tag', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' computer data analysis', ' computer system design /evaluation', ' disease /disorder classification', ' dysarthria', ' human subject', ' information systems', ' larynx', ' mathematical model', ' nervous system disorder diagnosis', ' speech', ' speech disorder diagnosis', ' stroke', ' technology /technique', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,1998,339245,0.35559317501889737
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA The goal of this research is to develop improved assessment protocols            that afford a quantitative and analytic evaluation of speech impairment          in children and adults with neurological disorders.  Speech impairments          (dysarthrias) will be studied in persons with amyotrophic lateral                sclerosis, Parkinson's disease, stroke, cerebellar degeneration, cerebral        palsy, and developmental speech disorders.  Improved evaluation of speech        intelligibility is a particular focus of this work, but issues of speech         and voice quality also are addressed.  The methods to be used are a              combination of standard clinical assessments (such as rating scales),            intelligibility evaluations, and computer-based acoustic analyses.               Specifically, the methods include: perceptual ratings of speech by               experienced clinicians, quantitative assessment of intelligibility, a            multiple-parameter acoustic analysis, computer correction of speech              abnormalities through LPC resynthesis of the acoustic signal, and                derivation of vocal tract shape from acoustic parameters.  Work in all           these areas will be based on recordings of speech samples from a large           number of individuals with dysarthria.  One product of the research will         be a library of clinical profiles including intelligibility scores,              phonetic feature analyses, ratings of speech/voice quality, acoustic             measures, and neurological diagnosis.  Particular attention will be given        to the influences of subject age and sex on the characteristics of               dysarthria for a given neurological diagnosis.  The assessment protocols         will be implemented on microcomputers and designed to be incorporated in         clinical practice.  The research also will contribute to the development         of expert systems for the rating and classification of dysarthria.                n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,2443579,R01DC000319,"[""Parkinson's disease"", ' amyotrophic lateral sclerosis', ' artificial intelligence', ' audiotape', ' behavioral /social science research tag', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' computer data analysis', ' computer system design /evaluation', ' disease /disorder classification', ' dysarthria', ' human subject', ' information systems', ' larynx', ' mathematical model', ' nervous system disorder diagnosis', ' speech', ' speech disorder diagnosis', ' stroke', ' technology /technique', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,1997,326197,0.35559317501889737
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA The goal of this research is to develop improved assessment protocols that afford a quantitative and analytic evaluation of speech impairment in children and adults with neurological disorders.  Speech impairments (dysarthrias) will be studied in persons with amyotrophic lateral sclerosis, Parkinson's disease, stroke, cerebellar degeneration, cerebral palsy, and developmental speech disorders.  Improved evaluation of speech intelligibility is a particular focus of this work, but issues of speech and voice quality also are addressed.  The methods to be used are a combination of standard clinical assessments (such as rating scales), intelligibility evaluations, and computer-based acoustic analyses. Specifically, the methods include: perceptual ratings of speech by experienced clinicians, quantitative assessment of intelligibility, a multiple-parameter acoustic analysis, computer correction of speech abnormalities through LPC resynthesis of the acoustic signal, and derivation of vocal tract shape from acoustic parameters.  Work in all these areas will be based on recordings of speech samples from a large number of individuals with dysarthria.  One product of the research will be a library of clinical profiles including intelligibility scores, phonetic feature analyses, ratings of speech/voice quality, acoustic measures, and neurological diagnosis.  Particular attention will be given to the influences of subject age and sex on the characteristics of dysarthria for a given neurological diagnosis.  The assessment protocols will be implemented on microcomputers and designed to be incorporated in clinical practice.  The research also will contribute to the development of expert systems for the rating and classification of dysarthria.  n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,2125440,R01DC000319,"[""Parkinson's disease"", ' amyotrophic lateral sclerosis', ' artificial intelligence', ' audiotape', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' computer data analysis', ' computer system design /evaluation', ' disease /disorder classification', ' dysarthria', ' human subject', ' information systems', ' larynx', ' mathematical model', ' nervous system disorder diagnosis', ' speech', ' speech disorder diagnosis', ' stroke', ' technology /technique', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,1994,287644,0.35559317501889737
"INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA The goal of this research is to develop improved assessment protocols that afford a quantitative and analytic evaluation of speech impairment in children and adults with neurological disorders.  Speech impairments (dysarthrias) will be studied in persons with amyotrophic lateral sclerosis, Parkinson's disease, stroke, cerebellar degeneration, cerebral palsy, and developmental speech disorders.  Improved evaluation of speech intelligibility is a particular focus of this work, but issues of speech and voice quality also are addressed.  The methods to be used are a combination of standard clinical assessments (such as rating scales), intelligibility evaluations, and computer-based acoustic analyses. Specifically, the methods include: perceptual ratings of speech by experienced clinicians, quantitative assessment of intelligibility, a multiple-parameter acoustic analysis, computer correction of speech abnormalities through LPC resynthesis of the acoustic signal, and derivation of vocal tract shape from acoustic parameters.  Work in all these areas will be based on recordings of speech samples from a large number of individuals with dysarthria.  One product of the research will be a library of clinical profiles including intelligibility scores, phonetic feature analyses, ratings of speech/voice quality, acoustic measures, and neurological diagnosis.  Particular attention will be given to the influences of subject age and sex on the characteristics of dysarthria for a given neurological diagnosis.  The assessment protocols will be implemented on microcomputers and designed to be incorporated in clinical practice.  The research also will contribute to the development of expert systems for the rating and classification of dysarthria.  n/a",INTELLIGIBILITY ASSESSMENT IN DYSARTHRIA,3216549,R01DC000319,"[""Parkinson's disease"", ' amyotrophic lateral sclerosis', ' artificial intelligence', ' audiotape', ' cerebellar disorders', ' cerebral palsy', ' child (0-11)', ' computer data analysis', ' computer system design /evaluation', ' disease /disorder classification', ' dysarthria', ' human subject', ' information systems', ' larynx', ' mathematical model', ' nervous system disorder diagnosis', ' speech', ' speech disorder diagnosis', ' stroke', ' technology /technique', ' vocabulary', ' voice']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R01,1993,303915,0.35559317501889737
"PHYSIOLOGICAL CORRELATES OF STUTTERING Stuttering is a disorder of speech with a prevalence estimated to be 1 %         of the world's population of school-age children. It is often a                  significant communicative problem for the individual, limiting educational       and employment opportunities and social and psychological adjustment. The        etiology of stuttering is unknown, and standardized, successful treatments       for stuttering have not been developed. A major impediment to                    understanding the etiology of stuttering and to the development of               successful therapeutic techniques is the lack of understanding of the            physiological bases of the disorder. Stuttering manifests itself as a            breakdown in speech motor processes. The complex variables known to affect       the occurrence of stuttering, such as emotional state or linguistic              complexity, must ultimately have an effect on the physiological events           necessary for the production of speech. Therefore, to understand                 stuttering it is essential to understand the physiological mechanisms            underlying disruptions of speech motor processes in stuttering.                                                                                                   The research proposed in the present application addresses this general          question: What is the nature of the movement disorder associated with            stuttering? The specific aims are (1) to determine whether motor processes       show evidence of continuous, underlying disturbances in stutterers'              speech, (2) to assess whether failures in speech movement control in             stuttering are related to autonomic nervous system activity and/or to            metabolic respiratory control, (3) to develop new metrics for the analysis       of physiological signals related to speech and to apply these new metrics        to the assessment of stuttering, and (4) to develop pattern recognition          algorithms to determine if there is a consistent set of physiological            events associated with stuttering. The results of the proposed studies and       those completed in the past years of this project should help us to              understand the complex human behavior that is stuttering. In addition,           work on this project has significant implications for the study of normal        speech production and a variety of motor speech disorders that occur in          neurologically impaired individuals.                                              n/a",PHYSIOLOGICAL CORRELATES OF STUTTERING,2443589,R01DC000559,"['artificial intelligence', ' behavioral /social science research tag', ' biomechanics', ' blood volume', ' electromyography', ' facial muscles', ' galvanic skin response', ' heart rate', ' human subject', ' jaw movement', ' lip', ' mathematical model', ' muscle function', ' neuromuscular disorder', ' neuromuscular function', ' plethysmography', ' psychological stressor', ' psychomotor function', ' pulmonary respiration', ' reading', ' speech', ' speech disorder diagnosis', ' stress', ' stuttering', ' sympathetic nervous system', ' tremor']",NIDCD,PURDUE UNIVERSITY WEST LAFAYETTE,R01,1997,189757,0.2849455250585434
"PHYSIOLOGICAL CORRELATES OF STUTTERING Stuttering is a disorder of speech with a prevalence estimated to be 1 %  of the world's population of school-age children. It is often a  significant communicative problem for the individual, limiting educational  and employment opportunities and social and psychological adjustment. The  etiology of stuttering is unknown, and standardized, successful treatments  for stuttering have not been developed. A major impediment to  understanding the etiology of stuttering and to the development of  successful therapeutic techniques is the lack of understanding of the  physiological bases of the disorder. Stuttering manifests itself as a  breakdown in speech motor processes. The complex variables known to affect  the occurrence of stuttering, such as emotional state or linguistic  complexity, must ultimately have an effect on the physiological events  necessary for the production of speech. Therefore, to understand  stuttering it is essential to understand the physiological mechanisms  underlying disruptions of speech motor processes in stuttering.    The research proposed in the present application addresses this general  question: What is the nature of the movement disorder associated with  stuttering? The specific aims are (1) to determine whether motor processes  show evidence of continuous, underlying disturbances in stutterers'  speech, (2) to assess whether failures in speech movement control in  stuttering are related to autonomic nervous system activity and/or to  metabolic respiratory control, (3) to develop new metrics for the analysis  of physiological signals related to speech and to apply these new metrics  to the assessment of stuttering, and (4) to develop pattern recognition  algorithms to determine if there is a consistent set of physiological  events associated with stuttering. The results of the proposed studies and  those completed in the past years of this project should help us to  understand the complex human behavior that is stuttering. In addition,  work on this project has significant implications for the study of normal  speech production and a variety of motor speech disorders that occur in  neurologically impaired individuals.  n/a",PHYSIOLOGICAL CORRELATES OF STUTTERING,2125802,R01DC000559,"['artificial intelligence', ' biomechanics', ' blood volume', ' electromyography', ' facial muscles', ' galvanic skin response', ' heart rate', ' human subject', ' jaw movement', ' lip', ' mathematical model', ' muscle function', ' neuromuscular disorder', ' neuromuscular function', ' plethysmography', ' psychological stressor', ' psychomotor function', ' pulmonary respiration', ' reading', ' speech', ' speech disorder diagnosis', ' stress', ' stuttering', ' sympathetic nervous system', ' tremor']",NIDCD,PURDUE UNIVERSITY WEST LAFAYETTE,R01,1995,197085,0.2849455250585434
"PHYSIOLOGICAL CORRELATES OF STUTTERING Stuttering is a disorder of speech with a prevalence estimated to be 1 % of the world's population of school-age children. It is often a significant communicative problem for the individual, limiting educational and employment opportunities and social and psychological adjustment. The etiology of stuttering is unknown, and standardized, successful treatments for stuttering have not been developed. A major impediment to understanding the etiology of stuttering and to the development of successful therapeutic techniques is the lack of understanding of the physiological bases of the disorder. Stuttering manifests itself as a breakdown in speech motor processes. The complex variables known to affect the occurrence of stuttering, such as emotional state or linguistic complexity, must ultimately have an effect on the physiological events necessary for the production of speech. Therefore, to understand stuttering it is essential to understand the physiological mechanisms underlying disruptions of speech motor processes in stuttering.  The research proposed in the present application addresses this general question: What is the nature of the movement disorder associated with stuttering? The specific aims are (1) to determine whether motor processes show evidence of continuous, underlying disturbances in stutterers' speech, (2) to assess whether failures in speech movement control in stuttering are related to autonomic nervous system activity and/or to metabolic respiratory control, (3) to develop new metrics for the analysis of physiological signals related to speech and to apply these new metrics to the assessment of stuttering, and (4) to develop pattern recognition algorithms to determine if there is a consistent set of physiological events associated with stuttering. The results of the proposed studies and those completed in the past years of this project should help us to understand the complex human behavior that is stuttering. In addition, work on this project has significant implications for the study of normal speech production and a variety of motor speech disorders that occur in neurologically impaired individuals.  n/a",PHYSIOLOGICAL CORRELATES OF STUTTERING,2125801,R01DC000559,"['artificial intelligence', ' biomechanics', ' blood volume', ' electromyography', ' facial muscles', ' galvanic skin response', ' heart rate', ' human subject', ' jaw movement', ' lip', ' mathematical model', ' muscle function', ' neuromuscular disorder', ' neuromuscular function', ' plethysmography', ' psychological stressor', ' psychomotor function', ' pulmonary respiration', ' reading', ' speech', ' speech disorder diagnosis', ' stress', ' stuttering', ' sympathetic nervous system', ' tremor']",NIDCD,PURDUE UNIVERSITY WEST LAFAYETTE,R01,1994,231943,0.2849455250585434
"Articulatory recovery from speech acoustics    DESCRIPTION (provided by applicant): The project's goal is to build methods and algorithms for the recovery of articulatory movement from acoustic speech signals. In the immediate future this will be accomplished by employing analysis-by-techniques for sonorant sounds of English, but with a view to extend this to obstruents and the speech of other languages. There are four main areas of research that will be conducted to reach these goals. The first area is to study the relationship between small changes in articulation and the resulting acoustics, which is feasible now that a large amount of simultaneously recorded articulatory movement and acoustic data are available. The second area of research is in the kinematics of articulatory movement, particularly that of the tongue. Flesh point data enables a data-driven approach to the modeling of tongue kinematics. A quantitative approach to the kinematics of line segments between flesh points, secant lines, is being pursued because it is a first approximation to a kinematics of tongue shape, which is more closely related to acoustics than the flesh points themselves. Because the method for articulatory recovery is analysis-by-synthesis, it is necessary to have an articulatory synthesizer for an internal speech production model. The third area of research is the construction of an articulatory synthesizer that includes knowledge gained from the second area of research. With progress already made in articulatory synthesis, it is possible to concentrate on the kinematic control of the synthesizer. This also will be done from a data-driven approach with piecewise polynomials fit to secant line kinematic trajectories derived from flesh point data. Finally, the fourth area of research is in the recovery algorithms themselves, which include methods for normalization between different vocal tracts and the articulatory synthesizer's vocal tract. Both veridical (actual space-time articulatory trajectories are reproduced) and non-veridical (categorical segmental properties are reproduced for perceptually accurate identification) articulatory will be tested. Veridical articulatory recovery from speech acoustics is useful for the laboratory and clinic when acoustic and partial articulatory information are available and the scientist or clinician wants to know more about articulation. Non-veridical articulatory recovery is important for models of speech and language learning.         n/a",Articulatory recovery from speech acoustics,7046438,R01DC001247,"['X ray', 'artificial intelligence', 'behavioral /social science research tag', 'computational neuroscience', 'computer program /software', 'human data', 'mathematical model', 'speech', 'speech recognition', 'speech synthesizers']",NIDCD,"CRESS, LLC",R01,2006,198938,0.18953011596343408
"Production modeling for articulatory recovery The speech production model developed in this work will serve as an internal model in an analysis-by-synthesis algorithm intended to recover articulatory movement from speech acoustics. The speech production model will have three components: 1) an improved version of the Haskins articulatory synthesizer, ASY, 2) an improved task-dynamic model, and 3) an inverse normalizing map, which relates ASY vocal tract shapes to human vocal tract shapes.  In order for ASY and the task-dynamic model to serve as components of a model of a human talker it is necessary that the task-dynamic, with end effectors in ASY, induce the articulatory kinematics observed in the talker as the image of the inverse normalizing map.  ASY and the task-dynamic model are parts of a veridical model of human speech production only in conjunction with an inverse normalizing map.  The approach here will be to make ASY and the task-dynamic model as realistic as possible without compromising their relative simplicity.  This will enable inverse normalizing maps to be mathematically regular functions, which is an important property for articulatory recovery. There have been many empirical studies on vocal tract shape performed since the time ASY was constructed that can be included into ASY.  Improvements in the transfer function calculation can also be made at this time.  With improvements to ASY will necessarily come improvements in the task-dynamic model, particularly tongue control and control during obstruent production.  Various methods for constructing the inverse normalizing map, along with its ability to map articulatory kinematics, will also be tested.  With all three components, the task-dynamic model will be tested using X-ray microbeam data of human speech production.  n/a",Production modeling for articulatory recovery,6787430,R01DC001247,"['X ray', ' artificial intelligence', ' behavioral /social science research tag', ' computational neuroscience', ' computer program /software', ' human data', ' mathematical model', ' speech', ' speech recognition', ' speech synthesizers']",NIDCD,"CRESS, LLC",R01,2003,49334,0.2222893988046089
"Production modeling for articulatory recovery The speech production model developed in this work will serve as an internal model in an analysis-by-synthesis algorithm intended to recover articulatory movement from speech acoustics. The speech production model will have three components: 1) an improved version of the Haskins articulatory synthesizer, ASY, 2) an improved task-dynamic model, and 3) an inverse normalizing map, which relates ASY vocal tract shapes to human vocal tract shapes.  In order for ASY and the task-dynamic model to serve as components of a model of a human talker it is necessary that the task-dynamic, with end effectors in ASY, induce the articulatory kinematics observed in the talker as the image of the inverse normalizing map.  ASY and the task-dynamic model are parts of a veridical model of human speech production only in conjunction with an inverse normalizing map.  The approach here will be to make ASY and the task-dynamic model as realistic as possible without compromising their relative simplicity.  This will enable inverse normalizing maps to be mathematically regular functions, which is an important property for articulatory recovery. There have been many empirical studies on vocal tract shape performed since the time ASY was constructed that can be included into ASY.  Improvements in the transfer function calculation can also be made at this time.  With improvements to ASY will necessarily come improvements in the task-dynamic model, particularly tongue control and control during obstruent production.  Various methods for constructing the inverse normalizing map, along with its ability to map articulatory kinematics, will also be tested.  With all three components, the task-dynamic model will be tested using X-ray microbeam data of human speech production.  n/a",Production modeling for articulatory recovery,6634451,R01DC001247,"['X ray', ' artificial intelligence', ' behavioral /social science research tag', ' computational neuroscience', ' computer program /software', ' human data', ' mathematical model', ' speech', ' speech recognition', ' speech synthesizers']",NIDCD,"CRESS, LLC",R01,2003,138502,0.2222893988046089
"Production modeling for articulatory recovery The speech production model developed in this work will serve as an internal model in an analysis-by-synthesis algorithm intended to recover articulatory movement from speech acoustics. The speech production model will have three components: 1) an improved version of the Haskins articulatory synthesizer, ASY, 2) an improved task-dynamic model, and 3) an inverse normalizing map, which relates ASY vocal tract shapes to human vocal tract shapes.  In order for ASY and the task-dynamic model to serve as components of a model of a human talker it is necessary that the task-dynamic, with end effectors in ASY, induce the articulatory kinematics observed in the talker as the image of the inverse normalizing map.  ASY and the task-dynamic model are parts of a veridical model of human speech production only in conjunction with an inverse normalizing map.  The approach here will be to make ASY and the task-dynamic model as realistic as possible without compromising their relative simplicity.  This will enable inverse normalizing maps to be mathematically regular functions, which is an important property for articulatory recovery. There have been many empirical studies on vocal tract shape performed since the time ASY was constructed that can be included into ASY.  Improvements in the transfer function calculation can also be made at this time.  With improvements to ASY will necessarily come improvements in the task-dynamic model, particularly tongue control and control during obstruent production.  Various methods for constructing the inverse normalizing map, along with its ability to map articulatory kinematics, will also be tested.  With all three components, the task-dynamic model will be tested using X-ray microbeam data of human speech production.  n/a",Production modeling for articulatory recovery,6516103,R01DC001247,"['X ray', ' artificial intelligence', ' behavioral /social science research tag', ' computational neuroscience', ' computer program /software', ' human data', ' mathematical model', ' speech', ' speech recognition', ' speech synthesizers']",NIDCD,"CRESS, LLC",R01,2002,138502,0.2222893988046089
"Production modeling for articulatory recovery The speech production model developed in this work will serve as an internal model in an analysis-by-synthesis algorithm intended to recover articulatory movement from speech acoustics. The speech production model will have three components: 1) an improved version of the Haskins articulatory synthesizer, ASY, 2) an improved task-dynamic model, and 3) an inverse normalizing map, which relates ASY vocal tract shapes to human vocal tract shapes.  In order for ASY and the task-dynamic model to serve as components of a model of a human talker it is necessary that the task-dynamic, with end effectors in ASY, induce the articulatory kinematics observed in the talker as the image of the inverse normalizing map.  ASY and the task-dynamic model are parts of a veridical model of human speech production only in conjunction with an inverse normalizing map.  The approach here will be to make ASY and the task-dynamic model as realistic as possible without compromising their relative simplicity.  This will enable inverse normalizing maps to be mathematically regular functions, which is an important property for articulatory recovery. There have been many empirical studies on vocal tract shape performed since the time ASY was constructed that can be included into ASY.  Improvements in the transfer function calculation can also be made at this time.  With improvements to ASY will necessarily come improvements in the task-dynamic model, particularly tongue control and control during obstruent production.  Various methods for constructing the inverse normalizing map, along with its ability to map articulatory kinematics, will also be tested.  With all three components, the task-dynamic model will be tested using X-ray microbeam data of human speech production.  n/a",Production modeling for articulatory recovery,6370912,R01DC001247,"['X ray', ' artificial intelligence', ' behavioral /social science research tag', ' computational neuroscience', ' computer program /software', ' human data', ' mathematical model', ' speech', ' speech recognition', ' speech synthesizers']",NIDCD,"CRESS, LLC",R01,2001,137046,0.2222893988046089
"ANALYSIS AND IMPROVEMENT OF ALARYNGEAL SPEECH Restoration of speech following total larynx removal is an integral part of the treatment plan for many patients with laryngeal cancer.  This project is concerned with both fundamental and highly practical aspects of the speech restoration process.  One broad objective of the proposed research is to expand current knowledge about acoustic characteristics of alaryngeal speech.  Three specific aims of the initial phase of the project are (1) to complete detailed analyses of the acoustic properties of volume velocity functions of the voicing sources of esophageal and tracheoesophageal speakers, (2) to identify acoustic properties that differentiate voicing source outputs of normal, esophageal, and tracheoesophageal speakers, (3) to quantify neck tissue coupling and transmission properties underlying the use of neck-type artificial larynges.  Specific aims of a second phase of the project are to complete detailed analyses of important articulatory-based acoustic properties of alaryngeal speech; namely, to delineate acoustic properties of vowels produced by tracheoesophageal speakers.  In a third phase of this project, the aim is to develop a computer simulated LPC analysis-synthesis system for use initially by laryngectomized patients. The long-term goal of this phase of the project is to successfully incorporate new speech analysis and synthesis techniques into a variety of prosthetic devices or speech aids that alaryngeal and other types of communicatively impaired individuals can use to enhance oral communication.  n/a",ANALYSIS AND IMPROVEMENT OF ALARYNGEAL SPEECH,2126514,R29DC001440,"['artificial intelligence', ' computer simulation', ' computer system design /evaluation', ' human subject', ' mathematical model', ' prosthesis', ' psychoacoustics', ' speech synthesizers', ' speech therapy', ' vocal cords']",NIDCD,UNIVERSITY OF ARIZONA,R29,1994,96162,0.3042943028984628
"ANALYSIS AND IMPROVEMENT OF ALARYNGEAL SPEECH Restoration of speech following total larynx removal is an integral part of the treatment plan for many patients with laryngeal cancer.  This project is concerned with both fundamental and highly practical aspects of the speech restoration process.  One broad objective of the proposed research is to expand current knowledge about acoustic characteristics of alaryngeal speech.  Three specific aims of the initial phase of the project are (1) to complete detailed analyses of the acoustic properties of volume velocity functions of the voicing sources of esophageal and tracheoesophageal speakers, (2) to identify acoustic properties that differentiate voicing source outputs of normal, esophageal, and tracheoesophageal speakers, (3) to quantify neck tissue coupling and transmission properties underlying the use of neck-type artificial larynges.  Specific aims of a second phase of the project are to complete detailed analyses of important articulatory-based acoustic properties of alaryngeal speech; namely, to delineate acoustic properties of vowels produced by tracheoesophageal speakers.  In a third phase of this project, the aim is to develop a computer simulated LPC analysis-synthesis system for use initially by laryngectomized patients. The long-term goal of this phase of the project is to successfully incorporate new speech analysis and synthesis techniques into a variety of prosthetic devices or speech aids that alaryngeal and other types of communicatively impaired individuals can use to enhance oral communication.  n/a",ANALYSIS AND IMPROVEMENT OF ALARYNGEAL SPEECH,3461865,R29DC001440,"['artificial intelligence', ' computer simulation', ' computer system design /evaluation', ' human subject', ' mathematical model', ' prosthesis', ' psychoacoustics', ' speech synthesizers', ' speech therapy', ' vocal cords']",NIDCD,UNIVERSITY OF ARIZONA,R29,1993,93503,0.3042943028984628
"ANALYSIS AND IMPROVEMENT OF ALARYNGEAL SPEECH Restoration of speech following total larynx removal is an integral part of the treatment plan for many patients with laryngeal cancer.  This project is concerned with both fundamental and highly practical aspects of the speech restoration process.  One broad objective of the proposed research is to expand current knowledge about acoustic characteristics of alaryngeal speech.  Three specific aims of the initial phase of the project are (1) to complete detailed analyses of the acoustic properties of volume velocity functions of the voicing sources of esophageal and tracheoesophageal speakers, (2) to identify acoustic properties that differentiate voicing source outputs of normal, esophageal, and tracheoesophageal speakers, (3) to quantify neck tissue coupling and transmission properties underlying the use of neck-type artificial larynges.  Specific aims of a second phase of the project are to complete detailed analyses of important articulatory-based acoustic properties of alaryngeal speech; namely, to delineate acoustic properties of vowels produced by tracheoesophageal speakers.  In a third phase of this project, the aim is to develop a computer simulated LPC analysis-synthesis system for use initially by laryngectomized patients. The long-term goal of this phase of the project is to successfully incorporate new speech analysis and synthesis techniques into a variety of prosthetic devices or speech aids that alaryngeal and other types of communicatively impaired individuals can use to enhance oral communication.  n/a",ANALYSIS AND IMPROVEMENT OF ALARYNGEAL SPEECH,3461864,R29DC001440,"['artificial intelligence', ' computer simulation', ' computer system design /evaluation', ' human subject', ' mathematical model', ' prosthesis', ' psychoacoustics', ' speech synthesizers', ' speech therapy', ' vocal cords']",NIDCD,UNIVERSITY OF ARIZONA,R29,1992,110316,0.3042943028984628
"ACOUSTIC CORRELATES OF PHONETIC PERCEPTION DESCRIPTION:  Numerous practical and theoretical problems could be addressed     if we had a better understanding of the auditory mechanisms underlying           phonetic recognition.  Among the practical applications of this knowledge        are:  (1) the improvement of speech synthesis devices, (2) the development       of robust speech recognition devices, (3) the development of acoustically        based training devices for hearing-impaired speakers, and (4) improvement in     Cochlear-implant signal processors.  The proposed experiments fall into          three major categories.  One set of experiments follows in a rather direct       way from vowel perception studies conducted during the previous grant            period.  These experiments address issues such as the role of dynamic            spectral cues and voice fundamental frequency in vowel perception.  A second     series of experiments address more fundamental issues regarding the spectral     representations that control phonetic quality.  A major goal of these            experiments is to test the validity of a method of representing speech that      was developed during the previous grant period.  The ""Masked Peak                Representation"" (MPR) was developed as an alternative to both formant            representations and whole spectrum models.  The MPR involves a series of         spectral manipulations that are designed to remove aspects of the spectrum       that do not appear to have a strong influence on phonetic quality, while         retaining those features that are most relevant to phonetic quality              judgments.  The MPR will be evaluated with:  (1) an experiment comparing         MPR-based predictions of perceived phonetic distance with those of a more        traditional auditory model, (2) speech recognition tests that use a Hidden       Markov Model to map sequences of MPR spectra onto words or phonetic              segments, and (3) listening tests with speech resynthesized from MPR             spectra.  A third set of studies is aimed at modeling the low-level auditory     mechanisms that are responsible for spectrum analysis.  The goal of this         work is to evaluate a model of spectrum analysis that is carried out by the      central auditory system rather than the auditory periphery.  A software          simulation of the model will be developed in an effort to determine the          extent to which the central-spectrum model can account for a broad range of      findings from the auditory psychophysics literature.  Experiments are also       proposed that address the implications of this model for vowel perception        and for the representation of pitch and periodicity.                              n/a",ACOUSTIC CORRELATES OF PHONETIC PERCEPTION,6329219,R01DC001661,"['behavioral /social science research tag', ' clinical research', ' hearing', ' human subject', ' mathematical model', ' neural information processing', ' perception', ' perceptual maskings', ' psychoacoustics', ' sound frequency', ' speech recognition']",NIDCD,WESTERN MICHIGAN UNIVERSITY,R01,2001,319368,0.14875947952812807
"ACOUSTIC CORRELATES OF PHONETIC PERCEPTION DESCRIPTION:  Numerous practical and theoretical problems could be addressed     if we had a better understanding of the auditory mechanisms underlying           phonetic recognition.  Among the practical applications of this knowledge        are:  (1) the improvement of speech synthesis devices, (2) the development       of robust speech recognition devices, (3) the development of acoustically        based training devices for hearing-impaired speakers, and (4) improvement in     Cochlear-implant signal processors.  The proposed experiments fall into          three major categories.  One set of experiments follows in a rather direct       way from vowel perception studies conducted during the previous grant            period.  These experiments address issues such as the role of dynamic            spectral cues and voice fundamental frequency in vowel perception.  A second     series of experiments address more fundamental issues regarding the spectral     representations that control phonetic quality.  A major goal of these            experiments is to test the validity of a method of representing speech that      was developed during the previous grant period.  The ""Masked Peak                Representation"" (MPR) was developed as an alternative to both formant            representations and whole spectrum models.  The MPR involves a series of         spectral manipulations that are designed to remove aspects of the spectrum       that do not appear to have a strong influence on phonetic quality, while         retaining those features that are most relevant to phonetic quality              judgments.  The MPR will be evaluated with:  (1) an experiment comparing         MPR-based predictions of perceived phonetic distance with those of a more        traditional auditory model, (2) speech recognition tests that use a Hidden       Markov Model to map sequences of MPR spectra onto words or phonetic              segments, and (3) listening tests with speech resynthesized from MPR             spectra.  A third set of studies is aimed at modeling the low-level auditory     mechanisms that are responsible for spectrum analysis.  The goal of this         work is to evaluate a model of spectrum analysis that is carried out by the      central auditory system rather than the auditory periphery.  A software          simulation of the model will be developed in an effort to determine the          extent to which the central-spectrum model can account for a broad range of      findings from the auditory psychophysics literature.  Experiments are also       proposed that address the implications of this model for vowel perception        and for the representation of pitch and periodicity.                              n/a",ACOUSTIC CORRELATES OF PHONETIC PERCEPTION,6124981,R01DC001661,"['behavioral /social science research tag', ' clinical research', ' hearing', ' human subject', ' mathematical model', ' neural information processing', ' perception', ' perceptual maskings', ' psychoacoustics', ' sound frequency', ' speech recognition']",NIDCD,WESTERN MICHIGAN UNIVERSITY,R01,2000,310068,0.14875947952812807
"ACOUSTIC CORRELATES OF PHONETIC PERCEPTION DESCRIPTION:  Numerous practical and theoretical problems could be addressed     if we had a better understanding of the auditory mechanisms underlying           phonetic recognition.  Among the practical applications of this knowledge        are:  (1) the improvement of speech synthesis devices, (2) the development       of robust speech recognition devices, (3) the development of acoustically        based training devices for hearing-impaired speakers, and (4) improvement in     Cochlear-implant signal processors.  The proposed experiments fall into          three major categories.  One set of experiments follows in a rather direct       way from vowel perception studies conducted during the previous grant            period.  These experiments address issues such as the role of dynamic            spectral cues and voice fundamental frequency in vowel perception.  A second     series of experiments address more fundamental issues regarding the spectral     representations that control phonetic quality.  A major goal of these            experiments is to test the validity of a method of representing speech that      was developed during the previous grant period.  The ""Masked Peak                Representation"" (MPR) was developed as an alternative to both formant            representations and whole spectrum models.  The MPR involves a series of         spectral manipulations that are designed to remove aspects of the spectrum       that do not appear to have a strong influence on phonetic quality, while         retaining those features that are most relevant to phonetic quality              judgments.  The MPR will be evaluated with:  (1) an experiment comparing         MPR-based predictions of perceived phonetic distance with those of a more        traditional auditory model, (2) speech recognition tests that use a Hidden       Markov Model to map sequences of MPR spectra onto words or phonetic              segments, and (3) listening tests with speech resynthesized from MPR             spectra.  A third set of studies is aimed at modeling the low-level auditory     mechanisms that are responsible for spectrum analysis.  The goal of this         work is to evaluate a model of spectrum analysis that is carried out by the      central auditory system rather than the auditory periphery.  A software          simulation of the model will be developed in an effort to determine the          extent to which the central-spectrum model can account for a broad range of      findings from the auditory psychophysics literature.  Experiments are also       proposed that address the implications of this model for vowel perception        and for the representation of pitch and periodicity.                              n/a",ACOUSTIC CORRELATES OF PHONETIC PERCEPTION,2837963,R01DC001661,"['behavioral /social science research tag', ' clinical research', ' hearing', ' human subject', ' mathematical model', ' neural information processing', ' perception', ' perceptual maskings', ' psychoacoustics', ' sound frequency', ' speech recognition']",NIDCD,WESTERN MICHIGAN UNIVERSITY,R01,1999,301037,0.14875947952812807
"ACOUSTIC CORRELATES OF PHONETIC PERCEPTION DESCRIPTION:  Numerous practical and theoretical problems could be addressed     if we had a better understanding of the auditory mechanisms underlying           phonetic recognition.  Among the practical applications of this knowledge        are:  (1) the improvement of speech synthesis devices, (2) the development       of robust speech recognition devices, (3) the development of acoustically        based training devices for hearing-impaired speakers, and (4) improvement in     Cochlear-implant signal processors.  The proposed experiments fall into          three major categories.  One set of experiments follows in a rather direct       way from vowel perception studies conducted during the previous grant            period.  These experiments address issues such as the role of dynamic            spectral cues and voice fundamental frequency in vowel perception.  A second     series of experiments address more fundamental issues regarding the spectral     representations that control phonetic quality.  A major goal of these            experiments is to test the validity of a method of representing speech that      was developed during the previous grant period.  The ""Masked Peak                Representation"" (MPR) was developed as an alternative to both formant            representations and whole spectrum models.  The MPR involves a series of         spectral manipulations that are designed to remove aspects of the spectrum       that do not appear to have a strong influence on phonetic quality, while         retaining those features that are most relevant to phonetic quality              judgments.  The MPR will be evaluated with:  (1) an experiment comparing         MPR-based predictions of perceived phonetic distance with those of a more        traditional auditory model, (2) speech recognition tests that use a Hidden       Markov Model to map sequences of MPR spectra onto words or phonetic              segments, and (3) listening tests with speech resynthesized from MPR             spectra.  A third set of studies is aimed at modeling the low-level auditory     mechanisms that are responsible for spectrum analysis.  The goal of this         work is to evaluate a model of spectrum analysis that is carried out by the      central auditory system rather than the auditory periphery.  A software          simulation of the model will be developed in an effort to determine the          extent to which the central-spectrum model can account for a broad range of      findings from the auditory psychophysics literature.  Experiments are also       proposed that address the implications of this model for vowel perception        and for the representation of pitch and periodicity.                              n/a",ACOUSTIC CORRELATES OF PHONETIC PERCEPTION,2608271,R01DC001661,"['behavioral /social science research tag', ' clinical research', ' hearing', ' human subject', ' mathematical model', ' neural information processing', ' perception', ' perceptual maskings', ' psychoacoustics', ' sound frequency', ' speech recognition']",NIDCD,WESTERN MICHIGAN UNIVERSITY,R01,1998,291816,0.14875947952812807
"ACOUSTIC CORRELATES OF PHONETIC PERCEPTION DESCRIPTION:  Numerous practical and theoretical problems could be addressed     if we had a better understanding of the auditory mechanisms underlying           phonetic recognition.  Among the practical applications of this knowledge        are:  (1) the improvement of speech synthesis devices, (2) the development       of robust speech recognition devices, (3) the development of acoustically        based training devices for hearing-impaired speakers, and (4) improvement in     Cochlear-implant signal processors.  The proposed experiments fall into          three major categories.  One set of experiments follows in a rather direct       way from vowel perception studies conducted during the previous grant            period.  These experiments address issues such as the role of dynamic            spectral cues and voice fundamental frequency in vowel perception.  A second     series of experiments address more fundamental issues regarding the spectral     representations that control phonetic quality.  A major goal of these            experiments is to test the validity of a method of representing speech that      was developed during the previous grant period.  The ""Masked Peak                Representation"" (MPR) was developed as an alternative to both formant            representations and whole spectrum models.  The MPR involves a series of         spectral manipulations that are designed to remove aspects of the spectrum       that do not appear to have a strong influence on phonetic quality, while         retaining those features that are most relevant to phonetic quality              judgments.  The MPR will be evaluated with:  (1) an experiment comparing         MPR-based predictions of perceived phonetic distance with those of a more        traditional auditory model, (2) speech recognition tests that use a Hidden       Markov Model to map sequences of MPR spectra onto words or phonetic              segments, and (3) listening tests with speech resynthesized from MPR             spectra.  A third set of studies is aimed at modeling the low-level auditory     mechanisms that are responsible for spectrum analysis.  The goal of this         work is to evaluate a model of spectrum analysis that is carried out by the      central auditory system rather than the auditory periphery.  A software          simulation of the model will be developed in an effort to determine the          extent to which the central-spectrum model can account for a broad range of      findings from the auditory psychophysics literature.  Experiments are also       proposed that address the implications of this model for vowel perception        and for the representation of pitch and periodicity.                              n/a",ACOUSTIC CORRELATES OF PHONETIC PERCEPTION,2014474,R01DC001661,"['behavioral /social science research tag', ' clinical research', ' hearing', ' human subject', ' mathematical model', ' neural information processing', ' perception', ' perceptual maskings', ' psychoacoustics', ' sound frequency', ' speech recognition']",NIDCD,WESTERN MICHIGAN UNIVERSITY,R01,1997,293809,0.14875947952812807
"ACOUSTIC CORRELATES OF PHONETIC PERCEPTION Numerous practical and theoretical problems could be addressed if we had a better understanding of the auditory mechanisms underlying phonetic recognition.  This proposal is aimed at improving our understanding of these mechanisms, with a particular focus on vowel perception.  Although there is a long tradition of representing vowels by the spectral pattern sampled at a single time slice, a growing body of literature suggests that dynamic properties play an important role in vowel identification.  Despite this literature, relatively little is known about the precise mechanisms that are involved in mapping dynamic spectral cues onto perceived vowel quality.  Some of the proposed experiments will test specific hypotheses about the way in which this mapping might occur.  The experiments will make use of a large database consisting of vowels spoken by 150 talkers (men, women, and children).  Measurements of fundamental frequency (F0) and formant contours from these signals will be used in a series of experiments designed to determine the role played by F0, vowel duration, and spectral change in vowel identification.  Specific hypotheses will be tested by: (1) acoustic analysis of tokens in the 150-talker database, and (2) listening tests involving various kinds of stimuli resynthesized from these tokens.  A second goal of this project is to evaluate the ""Masked Peak Representation"" (MPR), a new method of representing speech which was developed as an alternative to both traditional formant representations and ""whole spectrum"" representations.  Formant representations are widely used because these they can account for a relatively large number of findings in phonetic perception.  The principal weakness of formant theory is that tracking formants in natural speech is a difficult and essentially unresolved problem.  Largely in response to this problem, some investigators have proposed a whole spectrum approach in which phonetic quality is controlled by overall spectral shape.  The whole spectrum approach, however, cannot account for very convincing data showing that judgments of phonetic quality are affected primarily by the frequencies of spectral peaks, and relatively unaffected by spectral shape details in nonpeak regions.  The MPR was designed to retain maximal sensitivity to spectral peaks but without requiring the explicit tracking of formants. The basic idea behind the MPR is to:  (1) obtain a pitch-independent spectrum through cepstral smoothing, (2) stimulate nonlinear auditory frequency coding by computing a bark-scale transform, and (3) simulate lateral suppression by subtracting a running average of spectral values. The resulting ""masked spectrum"" retains spectral peaks but removes most other spectral shape details.  The MPR will be evaluated with:  (1) an experiment comparing MPR-based predictions of perceived phonetic distance with those of a more traditional auditory model, (2) speech recognition tests that use a Hidden Markov Model to map sequences of MPR spectra onto either words of phonetic segments, and (3) listening tests with speech resynthesized from MPR spectra.  n/a",ACOUSTIC CORRELATES OF PHONETIC PERCEPTION,2126667,R01DC001661,"['hearing', ' human subject', ' neural information processing', ' perception', ' psychoacoustics', ' sound frequency', ' speech', ' speech synthesizers']",NIDCD,WESTERN MICHIGAN UNIVERSITY,R01,1994,177881,0.09655322399270795
"ACOUSTIC CORRELATES OF PHONETIC PERCEPTION Numerous practical and theoretical problems could be addressed if we had a better understanding of the auditory mechanisms underlying phonetic recognition.  This proposal is aimed at improving our understanding of these mechanisms, with a particular focus on vowel perception.  Although there is a long tradition of representing vowels by the spectral pattern sampled at a single time slice, a growing body of literature suggests that dynamic properties play an important role in vowel identification.  Despite this literature, relatively little is known about the precise mechanisms that are involved in mapping dynamic spectral cues onto perceived vowel quality.  Some of the proposed experiments will test specific hypotheses about the way in which this mapping might occur.  The experiments will make use of a large database consisting of vowels spoken by 150 talkers (men, women, and children).  Measurements of fundamental frequency (F0) and formant contours from these signals will be used in a series of experiments designed to determine the role played by F0, vowel duration, and spectral change in vowel identification.  Specific hypotheses will be tested by: (1) acoustic analysis of tokens in the 150-talker database, and (2) listening tests involving various kinds of stimuli resynthesized from these tokens.  A second goal of this project is to evaluate the ""Masked Peak Representation"" (MPR), a new method of representing speech which was developed as an alternative to both traditional formant representations and ""whole spectrum"" representations.  Formant representations are widely used because these they can account for a relatively large number of findings in phonetic perception.  The principal weakness of formant theory is that tracking formants in natural speech is a difficult and essentially unresolved problem.  Largely in response to this problem, some investigators have proposed a whole spectrum approach in which phonetic quality is controlled by overall spectral shape.  The whole spectrum approach, however, cannot account for very convincing data showing that judgments of phonetic quality are affected primarily by the frequencies of spectral peaks, and relatively unaffected by spectral shape details in nonpeak regions.  The MPR was designed to retain maximal sensitivity to spectral peaks but without requiring the explicit tracking of formants. The basic idea behind the MPR is to:  (1) obtain a pitch-independent spectrum through cepstral smoothing, (2) stimulate nonlinear auditory frequency coding by computing a bark-scale transform, and (3) simulate lateral suppression by subtracting a running average of spectral values. The resulting ""masked spectrum"" retains spectral peaks but removes most other spectral shape details.  The MPR will be evaluated with:  (1) an experiment comparing MPR-based predictions of perceived phonetic distance with those of a more traditional auditory model, (2) speech recognition tests that use a Hidden Markov Model to map sequences of MPR spectra onto either words of phonetic segments, and (3) listening tests with speech resynthesized from MPR spectra.  n/a",ACOUSTIC CORRELATES OF PHONETIC PERCEPTION,3218256,R01DC001661,"['hearing', ' human subject', ' neural information processing', ' perception', ' psychoacoustics', ' sound frequency', ' speech', ' speech synthesizers']",NIDCD,WESTERN MICHIGAN UNIVERSITY,R01,1993,175328,0.09655322399270795
"ACOUSTIC CORRELATES OF PHONETIC PERCEPTION Numerous practical and theoretical problems could be addressed if we had a better understanding of the auditory mechanisms underlying phonetic recognition.  This proposal is aimed at improving our understanding of these mechanisms, with a particular focus on vowel perception.  Although there is a long tradition of representing vowels by the spectral pattern sampled at a single time slice, a growing body of literature suggests that dynamic properties play an important role in vowel identification.  Despite this literature, relatively little is known about the precise mechanisms that are involved in mapping dynamic spectral cues onto perceived vowel quality.  Some of the proposed experiments will test specific hypotheses about the way in which this mapping might occur.  The experiments will make use of a large database consisting of vowels spoken by 150 talkers (men, women, and children).  Measurements of fundamental frequency (F0) and formant contours from these signals will be used in a series of experiments designed to determine the role played by F0, vowel duration, and spectral change in vowel identification.  Specific hypotheses will be tested by: (1) acoustic analysis of tokens in the 150-talker database, and (2) listening tests involving various kinds of stimuli resynthesized from these tokens.  A second goal of this project is to evaluate the ""Masked Peak Representation"" (MPR), a new method of representing speech which was developed as an alternative to both traditional formant representations and ""whole spectrum"" representations.  Formant representations are widely used because these they can account for a relatively large number of findings in phonetic perception.  The principal weakness of formant theory is that tracking formants in natural speech is a difficult and essentially unresolved problem.  Largely in response to this problem, some investigators have proposed a whole spectrum approach in which phonetic quality is controlled by overall spectral shape.  The whole spectrum approach, however, cannot account for very convincing data showing that judgments of phonetic quality are affected primarily by the frequencies of spectral peaks, and relatively unaffected by spectral shape details in nonpeak regions.  The MPR was designed to retain maximal sensitivity to spectral peaks but without requiring the explicit tracking of formants. The basic idea behind the MPR is to:  (1) obtain a pitch-independent spectrum through cepstral smoothing, (2) stimulate nonlinear auditory frequency coding by computing a bark-scale transform, and (3) simulate lateral suppression by subtracting a running average of spectral values. The resulting ""masked spectrum"" retains spectral peaks but removes most other spectral shape details.  The MPR will be evaluated with:  (1) an experiment comparing MPR-based predictions of perceived phonetic distance with those of a more traditional auditory model, (2) speech recognition tests that use a Hidden Markov Model to map sequences of MPR spectra onto either words of phonetic segments, and (3) listening tests with speech resynthesized from MPR spectra.  n/a",ACOUSTIC CORRELATES OF PHONETIC PERCEPTION,3218255,R01DC001661,"['hearing', ' human subject', ' neural information processing', ' perception', ' psychoacoustics', ' sound frequency', ' speech', ' speech synthesizers']",NIDCD,WESTERN MICHIGAN UNIVERSITY,R01,1992,190609,0.09655322399270795
"SIMULATION OF VOICE QUALITIES IN SPEECH This research addresses one of the remaining challenges in speech science,       high quality speech simulation.  We define speech simulation as a form of        speech synthesis in which the movement of air and tissue is under                experimental control, rather than the resulting acoustic signal.  From the       early days of speech synthesis nearly a half century ago, the expectation        has always been that a better representation of the laws of physics of air       and tissue in motion would produce better synthesis. Although this               expectation still exists today, the payoff has been slow, primarily              because there are few data sets from which to build theoretical                  generalizations. In this proposal, the principal investigator and his            colleagues draw upon experience gained with simulation of the phonatory          processes to include the entire vocal tract in sentence-level speech             production.  The first phase will be to obtain naturalness in speech             quality that is comparable to formant synthesis by modeling a few specific       speakers from whom extensive data sets will be available.  The second            phase will be to develop scaling and modification rules that will allow          the voice of a given speaker to be transformed into a different age,             gender, emotion, and voice quality. The transformation will also include         induced or corrected voice and speech disorders.  The idea of voice              transformation (conversion) is not new, but the attempt to do it all in          the articulatory domain is relatively untried.  The results will have            practical and theoretical impact on the development of assistive devices         for voice/speech impaired populations, for surgery performed on the larynx       and upper respiratory tract, and for speech training and rehabilitation.          n/a",SIMULATION OF VOICE QUALITIES IN SPEECH,6030194,R01DC002532,"['artificial intelligence', ' behavioral /social science research tag', ' biomechanics', ' computed axial tomography', ' computer program /software', ' computer simulation', ' electrical measurement', ' human subject', ' magnetic resonance imaging', ' medical rehabilitation related tag', ' psychoacoustics', ' respiratory airflow measurement', ' respiratory imaging /visualization', ' speech', ' speech synthesizers', ' vocal cords', ' vocalization', ' voice']",NIDCD,DENVER CENTER FOR THE PERFORMING ARTS,R01,1999,238912,0.3776224305423975
"SIMULATION OF VOICE QUALITIES IN SPEECH This research addresses one of the remaining challenges in speech science,       high quality speech simulation.  We define speech simulation as a form of        speech synthesis in which the movement of air and tissue is under                experimental control, rather than the resulting acoustic signal.  From the       early days of speech synthesis nearly a half century ago, the expectation        has always been that a better representation of the laws of physics of air       and tissue in motion would produce better synthesis. Although this               expectation still exists today, the payoff has been slow, primarily              because there are few data sets from which to build theoretical                  generalizations. In this proposal, the principal investigator and his            colleagues draw upon experience gained with simulation of the phonatory          processes to include the entire vocal tract in sentence-level speech             production.  The first phase will be to obtain naturalness in speech             quality that is comparable to formant synthesis by modeling a few specific       speakers from whom extensive data sets will be available.  The second            phase will be to develop scaling and modification rules that will allow          the voice of a given speaker to be transformed into a different age,             gender, emotion, and voice quality. The transformation will also include         induced or corrected voice and speech disorders.  The idea of voice              transformation (conversion) is not new, but the attempt to do it all in          the articulatory domain is relatively untried.  The results will have            practical and theoretical impact on the development of assistive devices         for voice/speech impaired populations, for surgery performed on the larynx       and upper respiratory tract, and for speech training and rehabilitation.          n/a",SIMULATION OF VOICE QUALITIES IN SPEECH,2733685,R01DC002532,"['artificial intelligence', ' behavioral /social science research tag', ' biomechanics', ' computed axial tomography', ' computer program /software', ' computer simulation', ' electrical measurement', ' human subject', ' magnetic resonance imaging', ' psychoacoustics', ' respiratory airflow measurement', ' respiratory imaging /visualization', ' speech', ' speech synthesizers', ' vocal cords', ' vocalization', ' voice']",NIDCD,DENVER CENTER FOR THE PERFORMING ARTS,R01,1998,229724,0.3776224305423975
"SIMULATION OF VOICE QUALITIES IN SPEECH This research addresses one of the remaining challenges in speech science,       high quality speech simulation.  We define speech simulation as a form of        speech synthesis in which the movement of air and tissue is under                experimental control, rather than the resulting acoustic signal.  From the       early days of speech synthesis nearly a half century ago, the expectation        has always been that a better representation of the laws of physics of air       and tissue in motion would produce better synthesis. Although this               expectation still exists today, the payoff has been slow, primarily              because there are few data sets from which to build theoretical                  generalizations. In this proposal, the principal investigator and his            colleagues draw upon experience gained with simulation of the phonatory          processes to include the entire vocal tract in sentence-level speech             production.  The first phase will be to obtain naturalness in speech             quality that is comparable to formant synthesis by modeling a few specific       speakers from whom extensive data sets will be available.  The second            phase will be to develop scaling and modification rules that will allow          the voice of a given speaker to be transformed into a different age,             gender, emotion, and voice quality. The transformation will also include         induced or corrected voice and speech disorders.  The idea of voice              transformation (conversion) is not new, but the attempt to do it all in          the articulatory domain is relatively untried.  The results will have            practical and theoretical impact on the development of assistive devices         for voice/speech impaired populations, for surgery performed on the larynx       and upper respiratory tract, and for speech training and rehabilitation.          n/a",SIMULATION OF VOICE QUALITIES IN SPEECH,2443629,R01DC002532,"['artificial intelligence', ' behavioral /social science research tag', ' biomechanics', ' computed axial tomography', ' computer program /software', ' computer simulation', ' electrical measurement', ' human subject', ' magnetic resonance imaging', ' psychoacoustics', ' respiratory airflow measurement', ' respiratory imaging /visualization', ' speech', ' speech synthesizers', ' vocal cords', ' vocalization', ' voice']",NIDCD,DENVER CENTER FOR THE PERFORMING ARTS,R01,1997,220888,0.3776224305423975
"Neural Modeling and Imaging of Speech PROJECT SUMMARY The overall goal of this study is to improve our understanding of the neural mechanisms that underlie speech production and their breakdown in voice disorders. Our goals will be achieved through a tight coupling of computational modeling and functional magnetic resonance imaging (fMRI) studies organized around the Directions Into Velocities of Articulators (DIVA) model of speech motor control. The goal of Aim 1 is to develop a general computational framework for quantitatively comparing cognitive models with neuroimaging. Software will be developed to enable (i) the generation of whole-brain activity patterns from any model of brain function whose components are specified by a computational load function (e.g., the number of items in working memory) and a standard brain volume template location, and (ii) direct quantitative comparison between simulated brain activity and observed activity measured by a given imaging modality (e.g., fMRI, positron emission tomography, electroencephalography, and electrocorticography). DIVA and two alternative speech production models will be implemented in this framework and used to simulate whole-brain fMRI activity for a variety of speech tasks. The framework allows quantitative comparisons between models for fitting neuroimaging data, marking a significant improvement over the qualitative comparative methods that currently prevail. The goal of Aim 2 is to characterize the brain mechanisms underlying somatosensory feedback control of phonation. fMRI will be used to measure brain responses while neurotypical participants speak short sentences. On a subset of sentences, a somatosensory perturbation will be applied to the larynx. Perturbed utterances will be contrasted with unperturbed utterances to highlight the brain network responsible for counteracting the perturbation. DIVA-based hypotheses regarding this network will be tested by comparing observed brain activity to that resulting from simulations of the task using the framework developed in Aim 1. The goal of Aim 3 is to investigate somatosensory feedback control of phonation in individuals with the voice disorder adductor spasmodic dysphonia (ADSD). Prior findings suggest ADSD is due to hyper-function of the somatosensory feedback control system. This hypothesis will be tested using the larynx perturbation fMRI protocol of Aim 2 applied to individuals with ADSD. Measured brain activity from ADSD and neurologically normal participant groups will be contrasted, and computer simulations of normal and impaired versions of the DIVA model will be compared to the experimental results. Successful completion of our aims will move neuroscience from qualitative descriptions to quantitatively testable accounts of the neural processes responsible for speech and other cognitive tasks. In the longer term, the improved understanding of brain mechanisms underlying normal speech and their breakdowns in disorders such as ADSD will pave the way for improved assessment and treatment of these disorders. NARRATIVE This project will improve our understanding of how the brain controls speech production and how this control is disrupted in voice disorders. The outcome of this research is expected to significantly accelerate the development of treatments for voice and other speech disorders, which would profoundly impact the quality of life of millions of people.",Neural Modeling and Imaging of Speech,9982930,R01DC002852,"['Adoption', 'Articulators', 'Basal Ganglia', 'Behavior', 'Bilateral', 'Brain', 'Brain region', 'Cognitive', 'Communities', 'Computer Models', 'Computer Simulation', 'Computer software', 'Coupling', 'Data', 'Data Set', 'Databases', 'Disease', 'Electrocorticogram', 'Electroencephalography', 'Equilibrium', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Generations', 'Goals', 'Human', 'Image', 'Impairment', 'Individual', 'Inferior', 'Larynx', 'Link', 'Location', 'Maps', 'Measurement', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Modernization', 'Motor', 'Neurologic', 'Neurons', 'Neurosciences', 'Online Systems', 'Outcomes Research', 'Output', 'Participant', 'Pattern', 'Phonation', 'Positron-Emission Tomography', 'Process', 'Production', 'Protocols documentation', 'Quality of life', 'Severities', 'Short-Term Memory', 'Source', 'Spastic Dysphonias', 'Specific qualifier value', 'Speech', 'Speech Disorders', 'Structure of supramarginal gyrus', 'System', 'Testing', 'Voice', 'Voice Disorders', 'artificial neural network', 'base', 'behavior observation', 'brain volume', 'cognitive task', 'comparative', 'computer framework', 'experimental study', 'frontal lobe', 'imaging modality', 'imaging study', 'improved', 'mind control', 'motor behavior', 'motor control', 'neural model', 'neuroimaging', 'neuromechanism', 'novel', 'relating to nervous system', 'response', 'simulation', 'somatosensory', 'theories', 'therapy development']",NIDCD,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2020,350625,0.2933269999291995
"Neural Modeling and Imaging of Speech PROJECT SUMMARY The overall goal of this study is to improve our understanding of the neural mechanisms that underlie speech production and their breakdown in voice disorders. Our goals will be achieved through a tight coupling of computational modeling and functional magnetic resonance imaging (fMRI) studies organized around the Directions Into Velocities of Articulators (DIVA) model of speech motor control. The goal of Aim 1 is to develop a general computational framework for quantitatively comparing cognitive models with neuroimaging. Software will be developed to enable (i) the generation of whole-brain activity patterns from any model of brain function whose components are specified by a computational load function (e.g., the number of items in working memory) and a standard brain volume template location, and (ii) direct quantitative comparison between simulated brain activity and observed activity measured by a given imaging modality (e.g., fMRI, positron emission tomography, electroencephalography, and electrocorticography). DIVA and two alternative speech production models will be implemented in this framework and used to simulate whole-brain fMRI activity for a variety of speech tasks. The framework allows quantitative comparisons between models for fitting neuroimaging data, marking a significant improvement over the qualitative comparative methods that currently prevail. The goal of Aim 2 is to characterize the brain mechanisms underlying somatosensory feedback control of phonation. fMRI will be used to measure brain responses while neurotypical participants speak short sentences. On a subset of sentences, a somatosensory perturbation will be applied to the larynx. Perturbed utterances will be contrasted with unperturbed utterances to highlight the brain network responsible for counteracting the perturbation. DIVA-based hypotheses regarding this network will be tested by comparing observed brain activity to that resulting from simulations of the task using the framework developed in Aim 1. The goal of Aim 3 is to investigate somatosensory feedback control of phonation in individuals with the voice disorder adductor spasmodic dysphonia (ADSD). Prior findings suggest ADSD is due to hyper-function of the somatosensory feedback control system. This hypothesis will be tested using the larynx perturbation fMRI protocol of Aim 2 applied to individuals with ADSD. Measured brain activity from ADSD and neurologically normal participant groups will be contrasted, and computer simulations of normal and impaired versions of the DIVA model will be compared to the experimental results. Successful completion of our aims will move neuroscience from qualitative descriptions to quantitatively testable accounts of the neural processes responsible for speech and other cognitive tasks. In the longer term, the improved understanding of brain mechanisms underlying normal speech and their breakdowns in disorders such as ADSD will pave the way for improved assessment and treatment of these disorders. NARRATIVE This project will improve our understanding of how the brain controls speech production and how this control is disrupted in voice disorders. The outcome of this research is expected to significantly accelerate the development of treatments for voice and other speech disorders, which would profoundly impact the quality of life of millions of people.",Neural Modeling and Imaging of Speech,9750660,R01DC002852,"['Adoption', 'Articulators', 'Basal Ganglia', 'Behavior', 'Bilateral', 'Brain', 'Brain region', 'Cognitive', 'Communities', 'Computer Simulation', 'Computer software', 'Coupling', 'Data', 'Data Set', 'Databases', 'Disease', 'Electrocorticogram', 'Electroencephalography', 'Equilibrium', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Generations', 'Goals', 'Human', 'Image', 'Impairment', 'Individual', 'Inferior', 'Larynx', 'Link', 'Location', 'Maps', 'Measurement', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Modernization', 'Motor', 'Neurologic', 'Neurons', 'Neurosciences', 'Online Systems', 'Outcomes Research', 'Output', 'Participant', 'Pattern', 'Phonation', 'Positron-Emission Tomography', 'Process', 'Production', 'Protocols documentation', 'Quality of life', 'Severities', 'Short-Term Memory', 'Source', 'Spastic Dysphonias', 'Specific qualifier value', 'Speech', 'Speech Disorders', 'Structure of supramarginal gyrus', 'System', 'Testing', 'Voice', 'Voice Disorders', 'artificial neural network', 'base', 'behavior observation', 'brain volume', 'cognitive task', 'comparative', 'computer framework', 'experimental study', 'frontal lobe', 'imaging modality', 'imaging study', 'improved', 'mind control', 'motor control', 'neural model', 'neuroimaging', 'neuromechanism', 'novel', 'relating to nervous system', 'response', 'simulation', 'somatosensory', 'theories', 'therapy development']",NIDCD,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2019,350625,0.2933269999291995
"Neural Modeling and Imaging of Speech PROJECT SUMMARY The overall goal of this study is to improve our understanding of the neural mechanisms that underlie speech production and their breakdown in voice disorders. Our goals will be achieved through a tight coupling of computational modeling and functional magnetic resonance imaging (fMRI) studies organized around the Directions Into Velocities of Articulators (DIVA) model of speech motor control. The goal of Aim 1 is to develop a general computational framework for quantitatively comparing cognitive models with neuroimaging. Software will be developed to enable (i) the generation of whole-brain activity patterns from any model of brain function whose components are specified by a computational load function (e.g., the number of items in working memory) and a standard brain volume template location, and (ii) direct quantitative comparison between simulated brain activity and observed activity measured by a given imaging modality (e.g., fMRI, positron emission tomography, electroencephalography, and electrocorticography). DIVA and two alternative speech production models will be implemented in this framework and used to simulate whole-brain fMRI activity for a variety of speech tasks. The framework allows quantitative comparisons between models for fitting neuroimaging data, marking a significant improvement over the qualitative comparative methods that currently prevail. The goal of Aim 2 is to characterize the brain mechanisms underlying somatosensory feedback control of phonation. fMRI will be used to measure brain responses while neurotypical participants speak short sentences. On a subset of sentences, a somatosensory perturbation will be applied to the larynx. Perturbed utterances will be contrasted with unperturbed utterances to highlight the brain network responsible for counteracting the perturbation. DIVA-based hypotheses regarding this network will be tested by comparing observed brain activity to that resulting from simulations of the task using the framework developed in Aim 1. The goal of Aim 3 is to investigate somatosensory feedback control of phonation in individuals with the voice disorder adductor spasmodic dysphonia (ADSD). Prior findings suggest ADSD is due to hyper-function of the somatosensory feedback control system. This hypothesis will be tested using the larynx perturbation fMRI protocol of Aim 2 applied to individuals with ADSD. Measured brain activity from ADSD and neurologically normal participant groups will be contrasted, and computer simulations of normal and impaired versions of the DIVA model will be compared to the experimental results. Successful completion of our aims will move neuroscience from qualitative descriptions to quantitatively testable accounts of the neural processes responsible for speech and other cognitive tasks. In the longer term, the improved understanding of brain mechanisms underlying normal speech and their breakdowns in disorders such as ADSD will pave the way for improved assessment and treatment of these disorders. NARRATIVE This project will improve our understanding of how the brain controls speech production and how this control is disrupted in voice disorders. The outcome of this research is expected to significantly accelerate the development of treatments for voice and other speech disorders, which would profoundly impact the quality of life of millions of people.",Neural Modeling and Imaging of Speech,9532142,R01DC002852,"['Adoption', 'Articulators', 'Basal Ganglia', 'Behavior', 'Bilateral', 'Brain', 'Brain region', 'Cognitive', 'Communities', 'Computer Simulation', 'Computer software', 'Coupling', 'Data', 'Data Set', 'Databases', 'Disease', 'Electrocorticogram', 'Electroencephalography', 'Equilibrium', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Generations', 'Goals', 'Human', 'Image', 'Impairment', 'Individual', 'Inferior', 'Larynx', 'Link', 'Location', 'Maps', 'Measurement', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Modernization', 'Motor', 'Neurologic', 'Neurons', 'Neurosciences', 'Online Systems', 'Outcomes Research', 'Output', 'Participant', 'Pattern', 'Phonation', 'Positron-Emission Tomography', 'Process', 'Production', 'Protocols documentation', 'Quality of life', 'Severities', 'Short-Term Memory', 'Source', 'Spastic Dysphonias', 'Specific qualifier value', 'Speech', 'Speech Disorders', 'Structure of supramarginal gyrus', 'System', 'Testing', 'Voice', 'Voice Disorders', 'artificial neural network', 'base', 'behavior observation', 'brain volume', 'cognitive task', 'comparative', 'computer framework', 'experimental study', 'frontal lobe', 'imaging modality', 'imaging study', 'improved', 'mind control', 'motor control', 'neural model', 'neuroimaging', 'neuromechanism', 'novel', 'relating to nervous system', 'response', 'simulation', 'somatosensory', 'theories', 'therapy development']",NIDCD,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2018,350625,0.2933269999291995
"NEURAL NETWORK MODELING OF SPEECH PRODUCTION The long-term objectives of the proposed research are to elucidate the           stages of speaking skill development in infants and the motor control            mechanisms for speech production in adults.  These research areas are            important for early diagnosis and proper treatment of speech disorders.          The proposed research consists primarily of the development, refinement,         and experimental testing of a comprehensive neural network modeling              framework for speech production based on preliminary work described in           Guenther (Appendices A).  Model additions will include an articulatory           mechanism that allows synthesis of speech wave-forms and an acoustic-like        coordinate frame for speech movement planning.  Proposed research also           includes the development of software for producing speaker-specific vocal        tract models based on Magnetic Resonance Imaging (MRI) scans.  These models      will allow synthesis of speech signals that account for the differences in       vocal tract sizes and shapes of different individuals, thus providing a          more accurate means for investigating the acoustic/articulatory                  relationships of individuals acting as subjects in speech production             experiments.  An experimental investigation utilizing these speaker-             specific vocal tract models is also proposed to test an hypothesis               generated by the modeling framework.  Some speakers use two entirely             different articulator configurations, called ""bunched"" and retroflex"", to        produce /r/ in different contexts.  The proposed model predicts that the         same target is specified tot he production mechanism in the two cases, but       that different articulator configurations arise in different contexts due        to two properties of the speech production mechanism: (1) movement planning      in an acoustic-like coordinate frame, and (2) transformation of the planned      acoustic trajectories into articulator movements via a direction-to-             direction mapping.  Speaker-specific vocal tract models corresponding to         two subjects will be incorporated into the proposed modeling framework,          which will then be used to predict which configuration each subject will         use to produce /r/ in each of four contexts.  The hypothesis will be tested      by comparing model performance with the performance of the subjects while        producing /r/ in the same four contests, as measured in an Electro-Magnetic      Midsagittal Articulometer (EMMA) study.  Finally, the proposed modeling          framework will be used to investigate several other issues in speech             production, including two modeling studies of speech motor development in        infants (made possible by the self-organizing nature of the proposed             model), and an investigation of intrinsic timing issues.                             GRANT=R01DE09161                                                             The ability to utilize hemin and hemin containing compounds as an iron           source has been documented for several pathogenic bacteria, including the        periodontopathogen, Porphyromonas gingivalis.  We have previously                determined that P. gingivalis transports the entire hemin moiety into the        cell by an energy-dependent mechanism and that the binding and accumulation      of hemin are induced by growth of cultures in the presence of hemin.             However, the specific P. gingivalis components involved in hemin binding         and transport have not been identified.  Growth of P. gingivalis under           hemin-replete conditions has also been shown to influence the expression of      several virulence factors; however, the role of hemin in the regulation of       specific virulence genes has not been precisely defined.                                                                                                          The primary objectives of the present application are to define the              molecular mechanisms involved in hemin binding and transport in P.               gingivalis and to examine the regulation of hemin responsive genes.  Four        specific aims are proposed:                                                                                                                                       1.  To identify and characterize athe P. gingivalis hemin receptor(s).  P.       gingivalis outer membrane proteins involved in hemin binding will be             identified by hemin affinity chromatography.  The specificity of the             putative receptor(s) will be defined by examining the binding of 14[C]hemin      to P. gingivalis in the presence of hemin and nonahemin iron sources.                                                                                             2.  To clone P. gingivalis genes encoding proteins involved in hemin             binding and transport.  P. gingivalis genes encoding hemin binding proteins      will be cloned by screening E. coli recombinants for the ability to bind         hemin.  P. gingivalis genes encoding proteins involved in hemin transport        will be cloned by screening E. coli hemA mutants for the ability to grow         with hemin.  Corresponding P. gingivalis mutants will be obtained by             insertional inactivation of the cloned genes and characterized both in           vitro and in vivo.                                                                                                                                                3.  To elucidate the regulation of hemin binding and transport genes.  The       regulation of cloned P. gingivalis genes involved in hemin binding and           transport will be examined by analysis of mRNA and transcriptional fusions       under hemin-deplete and -replete conditions.                                                                                                                      4.  To identify P. gingivalis genes that are regulated by the ferric uptake      regulator (Fur).  Our preliminary results indicate that the expression of        hemin/iron-responsive genes in P. gingivalis may be controlled by the            negative ferric uptake regulator protein, Fur.  We will begin to identify        P. gingivalis genes that are regulated by Fur by screening a P. gingivalis       genomic library using the Fur titration asssay.                                                                                                                   The results obtained in these studies will allow us to identify specific         components of the hemin transport system in P. gingivalis and will provide       important information on the regulation of hemin responsive genes.                n/a",NEURAL NETWORK MODELING OF SPEECH PRODUCTION,6150504,R29DC002852,"['artificial intelligence', ' auditory feedback', ' auditory stimulus', ' behavioral /social science research tag', ' clinical research', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' digital imaging', ' human subject', ' language development', ' model design /development', ' psychoacoustics', ' sound', ' speech', ' speech recognition', ' vocal cords', ' vocalization']",NIDCD,BOSTON UNIVERSITY,R29,2000,114255,0.12372628073243666
"NEURAL NETWORK MODELING OF SPEECH PRODUCTION The long-term objectives of the proposed research are to elucidate the           stages of speaking skill development in infants and the motor control            mechanisms for speech production in adults.  These research areas are            important for early diagnosis and proper treatment of speech disorders.          The proposed research consists primarily of the development, refinement,         and experimental testing of a comprehensive neural network modeling              framework for speech production based on preliminary work described in           Guenther (Appendices A).  Model additions will include an articulatory           mechanism that allows synthesis of speech wave-forms and an acoustic-like        coordinate frame for speech movement planning.  Proposed research also           includes the development of software for producing speaker-specific vocal        tract models based on Magnetic Resonance Imaging (MRI) scans.  These models      will allow synthesis of speech signals that account for the differences in       vocal tract sizes and shapes of different individuals, thus providing a          more accurate means for investigating the acoustic/articulatory                  relationships of individuals acting as subjects in speech production             experiments.  An experimental investigation utilizing these speaker-             specific vocal tract models is also proposed to test an hypothesis               generated by the modeling framework.  Some speakers use two entirely             different articulator configurations, called ""bunched"" and retroflex"", to        produce /r/ in different contexts.  The proposed model predicts that the         same target is specified tot he production mechanism in the two cases, but       that different articulator configurations arise in different contexts due        to two properties of the speech production mechanism: (1) movement planning      in an acoustic-like coordinate frame, and (2) transformation of the planned      acoustic trajectories into articulator movements via a direction-to-             direction mapping.  Speaker-specific vocal tract models corresponding to         two subjects will be incorporated into the proposed modeling framework,          which will then be used to predict which configuration each subject will         use to produce /r/ in each of four contexts.  The hypothesis will be tested      by comparing model performance with the performance of the subjects while        producing /r/ in the same four contests, as measured in an Electro-Magnetic      Midsagittal Articulometer (EMMA) study.  Finally, the proposed modeling          framework will be used to investigate several other issues in speech             production, including two modeling studies of speech motor development in        infants (made possible by the self-organizing nature of the proposed             model), and an investigation of intrinsic timing issues.                             GRANT=R01DE09161                                                             The ability to utilize hemin and hemin containing compounds as an iron           source has been documented for several pathogenic bacteria, including the        periodontopathogen, Porphyromonas gingivalis.  We have previously                determined that P. gingivalis transports the entire hemin moiety into the        cell by an energy-dependent mechanism and that the binding and accumulation      of hemin are induced by growth of cultures in the presence of hemin.             However, the specific P. gingivalis components involved in hemin binding         and transport have not been identified.  Growth of P. gingivalis under           hemin-replete conditions has also been shown to influence the expression of      several virulence factors; however, the role of hemin in the regulation of       specific virulence genes has not been precisely defined.                                                                                                          The primary objectives of the present application are to define the              molecular mechanisms involved in hemin binding and transport in P.               gingivalis and to examine the regulation of hemin responsive genes.  Four        specific aims are proposed:                                                                                                                                       1.  To identify and characterize athe P. gingivalis hemin receptor(s).  P.       gingivalis outer membrane proteins involved in hemin binding will be             identified by hemin affinity chromatography.  The specificity of the             putative receptor(s) will be defined by examining the binding of 14[C]hemin      to P. gingivalis in the presence of hemin and nonahemin iron sources.                                                                                             2.  To clone P. gingivalis genes encoding proteins involved in hemin             binding and transport.  P. gingivalis genes encoding hemin binding proteins      will be cloned by screening E. coli recombinants for the ability to bind         hemin.  P. gingivalis genes encoding proteins involved in hemin transport        will be cloned by screening E. coli hemA mutants for the ability to grow         with hemin.  Corresponding P. gingivalis mutants will be obtained by             insertional inactivation of the cloned genes and characterized both in           vitro and in vivo.                                                                                                                                                3.  To elucidate the regulation of hemin binding and transport genes.  The       regulation of cloned P. gingivalis genes involved in hemin binding and           transport will be examined by analysis of mRNA and transcriptional fusions       under hemin-deplete and -replete conditions.                                                                                                                      4.  To identify P. gingivalis genes that are regulated by the ferric uptake      regulator (Fur).  Our preliminary results indicate that the expression of        hemin/iron-responsive genes in P. gingivalis may be controlled by the            negative ferric uptake regulator protein, Fur.  We will begin to identify        P. gingivalis genes that are regulated by Fur by screening a P. gingivalis       genomic library using the Fur titration asssay.                                                                                                                   The results obtained in these studies will allow us to identify specific         components of the hemin transport system in P. gingivalis and will provide       important information on the regulation of hemin responsive genes.                n/a",NEURAL NETWORK MODELING OF SPEECH PRODUCTION,2872135,R29DC002852,"['artificial intelligence', ' auditory feedback', ' auditory stimulus', ' behavioral /social science research tag', ' clinical research', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' digital imaging', ' human subject', ' language development', ' model design /development', ' psychoacoustics', ' sound', ' speech', ' speech recognition', ' vocal cords', ' vocalization']",NIDCD,BOSTON UNIVERSITY,R29,1999,109863,0.12372628073243666
"NEURAL NETWORK MODELING OF SPEECH PRODUCTION The long-term objectives of the proposed research are to elucidate the           stages of speaking skill development in infants and the motor control            mechanisms for speech production in adults.  These research areas are            important for early diagnosis and proper treatment of speech disorders.          The proposed research consists primarily of the development, refinement,         and experimental testing of a comprehensive neural network modeling              framework for speech production based on preliminary work described in           Guenther (Appendices A).  Model additions will include an articulatory           mechanism that allows synthesis of speech wave-forms and an acoustic-like        coordinate frame for speech movement planning.  Proposed research also           includes the development of software for producing speaker-specific vocal        tract models based on Magnetic Resonance Imaging (MRI) scans.  These models      will allow synthesis of speech signals that account for the differences in       vocal tract sizes and shapes of different individuals, thus providing a          more accurate means for investigating the acoustic/articulatory                  relationships of individuals acting as subjects in speech production             experiments.  An experimental investigation utilizing these speaker-             specific vocal tract models is also proposed to test an hypothesis               generated by the modeling framework.  Some speakers use two entirely             different articulator configurations, called ""bunched"" and retroflex"", to        produce /r/ in different contexts.  The proposed model predicts that the         same target is specified tot he production mechanism in the two cases, but       that different articulator configurations arise in different contexts due        to two properties of the speech production mechanism: (1) movement planning      in an acoustic-like coordinate frame, and (2) transformation of the planned      acoustic trajectories into articulator movements via a direction-to-             direction mapping.  Speaker-specific vocal tract models corresponding to         two subjects will be incorporated into the proposed modeling framework,          which will then be used to predict which configuration each subject will         use to produce /r/ in each of four contexts.  The hypothesis will be tested      by comparing model performance with the performance of the subjects while        producing /r/ in the same four contests, as measured in an Electro-Magnetic      Midsagittal Articulometer (EMMA) study.  Finally, the proposed modeling          framework will be used to investigate several other issues in speech             production, including two modeling studies of speech motor development in        infants (made possible by the self-organizing nature of the proposed             model), and an investigation of intrinsic timing issues.                             GRANT=R01DE09161                                                             The ability to utilize hemin and hemin containing compounds as an iron           source has been documented for several pathogenic bacteria, including the        periodontopathogen, Porphyromonas gingivalis.  We have previously                determined that P. gingivalis transports the entire hemin moiety into the        cell by an energy-dependent mechanism and that the binding and accumulation      of hemin are induced by growth of cultures in the presence of hemin.             However, the specific P. gingivalis components involved in hemin binding         and transport have not been identified.  Growth of P. gingivalis under           hemin-replete conditions has also been shown to influence the expression of      several virulence factors; however, the role of hemin in the regulation of       specific virulence genes has not been precisely defined.                                                                                                          The primary objectives of the present application are to define the              molecular mechanisms involved in hemin binding and transport in P.               gingivalis and to examine the regulation of hemin responsive genes.  Four        specific aims are proposed:                                                                                                                                       1.  To identify and characterize athe P. gingivalis hemin receptor(s).  P.       gingivalis outer membrane proteins involved in hemin binding will be             identified by hemin affinity chromatography.  The specificity of the             putative receptor(s) will be defined by examining the binding of 14[C]hemin      to P. gingivalis in the presence of hemin and nonahemin iron sources.                                                                                             2.  To clone P. gingivalis genes encoding proteins involved in hemin             binding and transport.  P. gingivalis genes encoding hemin binding proteins      will be cloned by screening E. coli recombinants for the ability to bind         hemin.  P. gingivalis genes encoding proteins involved in hemin transport        will be cloned by screening E. coli hemA mutants for the ability to grow         with hemin.  Corresponding P. gingivalis mutants will be obtained by             insertional inactivation of the cloned genes and characterized both in           vitro and in vivo.                                                                                                                                                3.  To elucidate the regulation of hemin binding and transport genes.  The       regulation of cloned P. gingivalis genes involved in hemin binding and           transport will be examined by analysis of mRNA and transcriptional fusions       under hemin-deplete and -replete conditions.                                                                                                                      4.  To identify P. gingivalis genes that are regulated by the ferric uptake      regulator (Fur).  Our preliminary results indicate that the expression of        hemin/iron-responsive genes in P. gingivalis may be controlled by the            negative ferric uptake regulator protein, Fur.  We will begin to identify        P. gingivalis genes that are regulated by Fur by screening a P. gingivalis       genomic library using the Fur titration asssay.                                                                                                                   The results obtained in these studies will allow us to identify specific         components of the hemin transport system in P. gingivalis and will provide       important information on the regulation of hemin responsive genes.                n/a",NEURAL NETWORK MODELING OF SPEECH PRODUCTION,2654421,R29DC002852,"['artificial intelligence', ' auditory feedback', ' auditory stimulus', ' behavioral /social science research tag', ' clinical research', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' digital imaging', ' human subject', ' language development', ' model design /development', ' psychoacoustics', ' sound', ' speech', ' speech recognition', ' vocal cords', ' vocalization']",NIDCD,BOSTON UNIVERSITY MEDICAL CAMPUS,R29,1998,122419,0.12372628073243666
"NEURAL NETWORK MODELING OF SPEECH PRODUCTION The long-term objectives of the proposed research are to elucidate the           stages of speaking skill development in infants and the motor control            mechanisms for speech production in adults.  These research areas are            important for early diagnosis and proper treatment of speech disorders.          The proposed research consists primarily of the development, refinement,         and experimental testing of a comprehensive neural network modeling              framework for speech production based on preliminary work described in           Guenther (Appendices A).  Model additions will include an articulatory           mechanism that allows synthesis of speech wave-forms and an acoustic-like        coordinate frame for speech movement planning.  Proposed research also           includes the development of software for producing speaker-specific vocal        tract models based on Magnetic Resonance Imaging (MRI) scans.  These models      will allow synthesis of speech signals that account for the differences in       vocal tract sizes and shapes of different individuals, thus providing a          more accurate means for investigating the acoustic/articulatory                  relationships of individuals acting as subjects in speech production             experiments.  An experimental investigation utilizing these speaker-             specific vocal tract models is also proposed to test an hypothesis               generated by the modeling framework.  Some speakers use two entirely             different articulator configurations, called ""bunched"" and retroflex"", to        produce /r/ in different contexts.  The proposed model predicts that the         same target is specified tot he production mechanism in the two cases, but       that different articulator configurations arise in different contexts due        to two properties of the speech production mechanism: (1) movement planning      in an acoustic-like coordinate frame, and (2) transformation of the planned      acoustic trajectories into articulator movements via a direction-to-             direction mapping.  Speaker-specific vocal tract models corresponding to         two subjects will be incorporated into the proposed modeling framework,          which will then be used to predict which configuration each subject will         use to produce /r/ in each of four contexts.  The hypothesis will be tested      by comparing model performance with the performance of the subjects while        producing /r/ in the same four contests, as measured in an Electro-Magnetic      Midsagittal Articulometer (EMMA) study.  Finally, the proposed modeling          framework will be used to investigate several other issues in speech             production, including two modeling studies of speech motor development in        infants (made possible by the self-organizing nature of the proposed             model), and an investigation of intrinsic timing issues.                             GRANT=R01DE09161                                                             The ability to utilize hemin and hemin containing compounds as an iron           source has been documented for several pathogenic bacteria, including the        periodontopathogen, Porphyromonas gingivalis.  We have previously                determined that P. gingivalis transports the entire hemin moiety into the        cell by an energy-dependent mechanism and that the binding and accumulation      of hemin are induced by growth of cultures in the presence of hemin.             However, the specific P. gingivalis components involved in hemin binding         and transport have not been identified.  Growth of P. gingivalis under           hemin-replete conditions has also been shown to influence the expression of      several virulence factors; however, the role of hemin in the regulation of       specific virulence genes has not been precisely defined.                                                                                                          The primary objectives of the present application are to define the              molecular mechanisms involved in hemin binding and transport in P.               gingivalis and to examine the regulation of hemin responsive genes.  Four        specific aims are proposed:                                                                                                                                       1.  To identify and characterize athe P. gingivalis hemin receptor(s).  P.       gingivalis outer membrane proteins involved in hemin binding will be             identified by hemin affinity chromatography.  The specificity of the             putative receptor(s) will be defined by examining the binding of 14[C]hemin      to P. gingivalis in the presence of hemin and nonahemin iron sources.                                                                                             2.  To clone P. gingivalis genes encoding proteins involved in hemin             binding and transport.  P. gingivalis genes encoding hemin binding proteins      will be cloned by screening E. coli recombinants for the ability to bind         hemin.  P. gingivalis genes encoding proteins involved in hemin transport        will be cloned by screening E. coli hemA mutants for the ability to grow         with hemin.  Corresponding P. gingivalis mutants will be obtained by             insertional inactivation of the cloned genes and characterized both in           vitro and in vivo.                                                                                                                                                3.  To elucidate the regulation of hemin binding and transport genes.  The       regulation of cloned P. gingivalis genes involved in hemin binding and           transport will be examined by analysis of mRNA and transcriptional fusions       under hemin-deplete and -replete conditions.                                                                                                                      4.  To identify P. gingivalis genes that are regulated by the ferric uptake      regulator (Fur).  Our preliminary results indicate that the expression of        hemin/iron-responsive genes in P. gingivalis may be controlled by the            negative ferric uptake regulator protein, Fur.  We will begin to identify        P. gingivalis genes that are regulated by Fur by screening a P. gingivalis       genomic library using the Fur titration asssay.                                                                                                                   The results obtained in these studies will allow us to identify specific         components of the hemin transport system in P. gingivalis and will provide       important information on the regulation of hemin responsive genes.                n/a",NEURAL NETWORK MODELING OF SPEECH PRODUCTION,2331289,R29DC002852,"['artificial intelligence', ' auditory feedback', ' auditory stimulus', ' behavioral /social science research tag', ' clinical research', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' digital imaging', ' human subject', ' language development', ' model design /development', ' psychoacoustics', ' sound', ' speech', ' speech recognition', ' vocal cords', ' vocalization']",NIDCD,BOSTON UNIVERSITY MEDICAL CAMPUS,R29,1997,118268,0.12372628073243666
"Speech Prosody and Articulatory Dynamics in Spoken Language DESCRIPTION (provided by applicant): One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, local rate modulation, and pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities. The long term objective of the proposed research program is to understand how linguistic structure and communicative context condition the spatiotemporal realization of articulatory movement during speaking. Our linguist-engineer team studies the ""signatures"" of prosody at the level of articulatory patterning. The specific aims of this proposal are to understand and model how speakers differentially modulate the spatiotemporal organization of articulatory gestures as a function of the cognitive source of a break in the speech stream and how the communicative context influences the temporal flow of the speech stream in articulation as speakers interact with one another. We outline a research strategy that investigates the relation between speech initiation/cessation and the control and coordination of articulation. Speech may start, pause, or cease for a variety of reasons in addition to linguistically structured phrase edges. Some breaks in the speech stream may be cognitively ""planned,"" such as interlocutor turn-taking in discourse. Other disruptions in the speech stream might be ""unplanned,"" such as interruptions and word finding challenges. Our approach investigates the articulation of speech in the vocal tract at turn-taking and interruptions in structured dialogue and in the vicinity of pauses that occur for cognitive speech planning reasons. We complement this experimental work with computational modeling of phrasal junctures and pauses, and with machine learning approaches to classifying breaks in speech arising from differing sources. The specific aims will be pursued by using articulatory movement data collected with magnetometer systems for tracking movement inside the mouth and by using our team's computational model of speech production. The experiment work and the concomitant computational modeling of the articulatory findings will provide a profile of the manner in which articulatory patterning is shapd by the larger informational structuring of utterances and by the demands of speech planning in a communicative context. One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.",Speech Prosody and Articulatory Dynamics in Spoken Language,8828663,R01DC003172,"['Acoustics', 'Address', 'Affective', 'Articulators', 'Behavior', 'Biological Models', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Conceptions', 'Data', 'Disease', 'Engineering', 'Environment', 'Evaluation', 'Gestures', 'Grant', 'Human', 'Indium', 'Interruption', 'Investigation', 'Joints', 'Language', 'Learning', 'Linguistics', 'Loudness', 'Machine Learning', 'Modeling', 'Movement', 'Nervous System Trauma', 'Oral cavity', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Process', 'Production', 'Property', 'Reading', 'Research', 'Research Personnel', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Sorting - Cell Movement', 'Source', 'Speech', 'Stream', 'Stroke', 'Structure', 'System', 'Techniques', 'Time', 'Ursidae Family', 'Work', 'autism spectrum disorder', 'base', 'cognitive function', 'cognitive load', 'constriction', 'kinematics', 'novel strategies', 'phrases', 'programs', 'research study', 'spatiotemporal', 'speech disorder diagnosis', 'speech recognition', 'syntax']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2015,472112,0.3980542295257347
"Speech Prosody and Articulatory Dynamics in Spoken Language     DESCRIPTION (provided by applicant): One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, local rate modulation, and pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities. The long term objective of the proposed research program is to understand how linguistic structure and communicative context condition the spatiotemporal realization of articulatory movement during speaking. Our linguist-engineer team studies the ""signatures"" of prosody at the level of articulatory patterning. The specific aims of this proposal are to understand and model how speakers differentially modulate the spatiotemporal organization of articulatory gestures as a function of the cognitive source of a break in the speech stream and how the communicative context influences the temporal flow of the speech stream in articulation as speakers interact with one another. We outline a research strategy that investigates the relation between speech initiation/cessation and the control and coordination of articulation. Speech may start, pause, or cease for a variety of reasons in addition to linguistically structured phrase edges. Some breaks in the speech stream may be cognitively ""planned,"" such as interlocutor turn-taking in discourse. Other disruptions in the speech stream might be ""unplanned,"" such as interruptions and word finding challenges. Our approach investigates the articulation of speech in the vocal tract at turn-taking and interruptions in structured dialogue and in the vicinity of pauses that occur for cognitive speech planning reasons. We complement this experimental work with computational modeling of phrasal junctures and pauses, and with machine learning approaches to classifying breaks in speech arising from differing sources. The specific aims will be pursued by using articulatory movement data collected with magnetometer systems for tracking movement inside the mouth and by using our team's computational model of speech production. The experiment work and the concomitant computational modeling of the articulatory findings will provide a profile of the manner in which articulatory patterning is shapd by the larger informational structuring of utterances and by the demands of speech planning in a communicative context.          One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.            ",Speech Prosody and Articulatory Dynamics in Spoken Language,8643200,R01DC003172,"['Acoustics', 'Address', 'Affective', 'Articulators', 'Behavior', 'Biological Models', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Conceptions', 'Data', 'Disease', 'Engineering', 'Environment', 'Evaluation', 'Gestures', 'Grant', 'Human', 'Indium', 'Interruption', 'Investigation', 'Joints', 'Language', 'Learning', 'Linguistics', 'Loudness', 'Machine Learning', 'Modeling', 'Movement', 'Nervous System Trauma', 'Oral cavity', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Process', 'Production', 'Property', 'Reading', 'Research', 'Research Personnel', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Simulate', 'Sorting - Cell Movement', 'Source', 'Speech', 'Stream', 'Stroke', 'Structure', 'System', 'Techniques', 'Time', 'Ursidae Family', 'Work', 'autism spectrum disorder', 'base', 'cognitive function', 'constriction', 'kinematics', 'novel strategies', 'phrases', 'programs', 'research study', 'spatiotemporal', 'speech disorder diagnosis', 'speech recognition', 'syntax']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2014,475459,0.3980542295257347
"Speech Prosody and Articulatory Dynamics in Spoken Language     DESCRIPTION (provided by applicant): One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, local rate modulation, and pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities. The long term objective of the proposed research program is to understand how linguistic structure and communicative context condition the spatiotemporal realization of articulatory movement during speaking. Our linguist-engineer team studies the ""signatures"" of prosody at the level of articulatory patterning. The specific aims of this proposal are to understand and model how speakers differentially modulate the spatiotemporal organization of articulatory gestures as a function of the cognitive source of a break in the speech stream and how the communicative context influences the temporal flow of the speech stream in articulation as speakers interact with one another. We outline a research strategy that investigates the relation between speech initiation/cessation and the control and coordination of articulation. Speech may start, pause, or cease for a variety of reasons in addition to linguistically structured phrase edges. Some breaks in the speech stream may be cognitively ""planned,"" such as interlocutor turn-taking in discourse. Other disruptions in the speech stream might be ""unplanned,"" such as interruptions and word finding challenges. Our approach investigates the articulation of speech in the vocal tract at turn-taking and interruptions in structured dialogue and in the vicinity of pauses that occur for cognitive speech planning reasons. We complement this experimental work with computational modeling of phrasal junctures and pauses, and with machine learning approaches to classifying breaks in speech arising from differing sources. The specific aims will be pursued by using articulatory movement data collected with magnetometer systems for tracking movement inside the mouth and by using our team's computational model of speech production. The experiment work and the concomitant computational modeling of the articulatory findings will provide a profile of the manner in which articulatory patterning is shapd by the larger informational structuring of utterances and by the demands of speech planning in a communicative context.          One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.            ",Speech Prosody and Articulatory Dynamics in Spoken Language,8445225,R01DC003172,"['Acoustics', 'Address', 'Affective', 'Articulators', 'Behavior', 'Biological Models', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Conceptions', 'Data', 'Disease', 'Engineering', 'Environment', 'Evaluation', 'Gestures', 'Grant', 'Human', 'Indium', 'Interruption', 'Investigation', 'Joints', 'Language', 'Learning', 'Linguistics', 'Loudness', 'Machine Learning', 'Modeling', 'Movement', 'Nervous System Trauma', 'Oral cavity', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Process', 'Production', 'Property', 'Reading', 'Research', 'Research Personnel', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Simulate', 'Sorting - Cell Movement', 'Source', 'Speech', 'Stream', 'Stroke', 'Structure', 'System', 'Techniques', 'Time', 'Ursidae Family', 'Work', 'autism spectrum disorder', 'base', 'cognitive function', 'constriction', 'kinematics', 'novel strategies', 'phrases', 'programs', 'research study', 'spatiotemporal', 'speech disorder diagnosis', 'speech recognition', 'syntax']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2013,450674,0.3980542295257347
"Speech Prosody and Articulatory Dynamics in Spoken Language     DESCRIPTION (provided by applicant): One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, local rate modulation, and pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities. The long term objective of the proposed research program is to understand how linguistic structure and communicative context condition the spatiotemporal realization of articulatory movement during speaking. Our linguist-engineer team studies the ""signatures"" of prosody at the level of articulatory patterning. The specific aims of this proposal are to understand and model how speakers differentially modulate the spatiotemporal organization of articulatory gestures as a function of the cognitive source of a break in the speech stream and how the communicative context influences the temporal flow of the speech stream in articulation as speakers interact with one another. We outline a research strategy that investigates the relation between speech initiation/cessation and the control and coordination of articulation. Speech may start, pause, or cease for a variety of reasons in addition to linguistically structured phrase edges. Some breaks in the speech stream may be cognitively ""planned,"" such as interlocutor turn-taking in discourse. Other disruptions in the speech stream might be ""unplanned,"" such as interruptions and word finding challenges. Our approach investigates the articulation of speech in the vocal tract at turn-taking and interruptions in structured dialogue and in the vicinity of pauses that occur for cognitive speech planning reasons. We complement this experimental work with computational modeling of phrasal junctures and pauses, and with machine learning approaches to classifying breaks in speech arising from differing sources. The specific aims will be pursued by using articulatory movement data collected with magnetometer systems for tracking movement inside the mouth and by using our team's computational model of speech production. The experiment work and the concomitant computational modeling of the articulatory findings will provide a profile of the manner in which articulatory patterning is shapd by the larger informational structuring of utterances and by the demands of speech planning in a communicative context.        PUBLIC HEALTH RELEVANCE: One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.              One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.            ",Speech Prosody and Articulatory Dynamics in Spoken Language,8282659,R01DC003172,"['Acoustics', 'Address', 'Affective', 'Articulators', 'Behavior', 'Biological Models', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Conceptions', 'Data', 'Disease', 'Engineering', 'Environment', 'Evaluation', 'Gestures', 'Grant', 'Human', 'Indium', 'Interruption', 'Investigation', 'Joints', 'Language', 'Learning', 'Linguistics', 'Loudness', 'Machine Learning', 'Modeling', 'Movement', 'Nervous System Trauma', 'Oral cavity', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Process', 'Production', 'Property', 'Reading', 'Research', 'Research Personnel', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Simulate', 'Sorting - Cell Movement', 'Source', 'Speech', 'Stream', 'Stroke', 'Structure', 'System', 'Techniques', 'Time', 'Ursidae Family', 'Work', 'autism spectrum disorder', 'base', 'cognitive function', 'constriction', 'kinematics', 'novel strategies', 'phrases', 'programs', 'research study', 'spatiotemporal', 'speech disorder diagnosis', 'speech recognition', 'syntax']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2012,549459,0.39153822286512785
"Speech Prosody and Articulatory Dynamics in Spoken Language DESCRIPTION (provided by applicant): One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, local rate modulation, and pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities. The long term objective of the proposed research program is to understand how linguistic structure and communicative context condition the spatiotemporal realization of articulatory movement during speaking. Our linguist-engineer team studies the ""signatures"" of prosody at the level of articulatory patterning. The specific aims of this proposal are to understand and model how speakers differentially modulate the spatiotemporal organization of articulatory gestures as a function of the cognitive source of a break in the speech stream and how the communicative context influences the temporal flow of the speech stream in articulation as speakers interact with one another. We outline a research strategy that investigates the relation between speech initiation/cessation and the control and coordination of articulation. Speech may start, pause, or cease for a variety of reasons in addition to linguistically structured phrase edges. Some breaks in the speech stream may be cognitively ""planned,"" such as interlocutor turn-taking in discourse. Other disruptions in the speech stream might be ""unplanned,"" such as interruptions and word finding challenges. Our approach investigates the articulation of speech in the vocal tract at turn-taking and interruptions in structured dialogue and in the vicinity of pauses that occur for cognitive speech planning reasons. We complement this experimental work with computational modeling of phrasal junctures and pauses, and with machine learning approaches to classifying breaks in speech arising from differing sources. The specific aims will be pursued by using articulatory movement data collected with magnetometer systems for tracking movement inside the mouth and by using our team's computational model of speech production. The experiment work and the concomitant computational modeling of the articulatory findings will provide a profile of the manner in which articulatory patterning is shapd by the larger informational structuring of utterances and by the demands of speech planning in a communicative context. One powerfully communicative aspect of language is prosody, which is the use of pitch, loudness, and temporal properties such as pauses in speech to signal its informational and affective content. Speech production deficits due to neurological damage such as stroke or due to Autism Spectrum Disorder are often characterized by prosodic irregularities and understanding the influence of structural prosody and its deployment in communication on the temporal flow of speech can have critical translational impact in that disfluencies are typically used as a basis for diagnosis of speech disorders. Our research uses instrumental tracking of articulatory movements during speech to provide an understanding of normative production of modulation and pauses in speech flow that could support evidence-driven assessments and treatments of prosodic breakdown in clinical populations, including deploying assistive technologies for the impaired such as automatic speech recognition and machine speech synthesis.",Speech Prosody and Articulatory Dynamics in Spoken Language,9036989,R01DC003172,"['Acoustics', 'Address', 'Affective', 'Articulators', 'Behavior', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Conceptions', 'Data', 'Disease', 'Engineering', 'Environment', 'Evaluation', 'Gestures', 'Grant', 'Human', 'Indium', 'Interruption', 'Investigation', 'Joints', 'Language', 'Learning', 'Linguistics', 'Loudness', 'Machine Learning', 'Modeling', 'Movement', 'Nervous System Trauma', 'Oral cavity', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Process', 'Production', 'Property', 'Reading', 'Research', 'Research Personnel', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Sorting - Cell Movement', 'Source', 'Speech', 'Stream', 'Stroke', 'Structure', 'System', 'Techniques', 'Time', 'Ursidae Family', 'Work', 'autism spectrum disorder', 'base', 'cognitive function', 'cognitive load', 'constriction', 'dynamic system', 'kinematics', 'novel strategies', 'phrases', 'programs', 'research study', 'spatiotemporal', 'speech disorder diagnosis', 'speech recognition', 'syntax']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2016,477237,0.3980542295257347
"Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates     DESCRIPTION (provided by applicant): 90% of the 1.5 million Americans living with idiopathic Parkinson's disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria. Dysarthria has devastating consequences for life quality and participation in society due to its effects on employment, leisure activities and social relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by the gold standard perceptual construct of intelligibility is thus of vital importance. Owing to the scarcity of impartial comparative studies, the choice of one technique over others is often based on trial and error or reflects clinician bias, both of which are at odds with evidence-based practice. This project has sought to address this critical gap in knowledge regarding the comparative merits of dysarthria treatment techniques since its inception. Toward this end, published studies from the past funding cycle compared the acoustic and perceptual merits of three common, global dysarthria treatment techniques including 1) rate manipulation, 2) an increased vocal intensity and 3) clear speech in MS and PD as well as age and sex matched neurotypical talkers. Global treatment techniques by their very nature elicit co-occurring acoustic changes (e.g., duration, segmental articulation). Because an explanatory, acoustically-based model of intelligibility is lacking, the acoustic change(s) causing or explainin the improved perceptual outcomes of global treatment techniques are unknown. Determining the acoustic variables explaining intelligibility variation in dysarthria would not only tremendousy advance theoretical understanding of intelligibility but also would strengthen the scientific basis for treatment. Treatment focused on those acoustic variables explanatory for improved intelligibility may further accelerate progress in therapy. Importantly, research from the previous funding cycle suggests the promise of speech analysis-resynthesis for identifying segmental and suprasegmental acoustic variables explanatory for intelligibility in dysarthria. Building upon this work, the overarching goal of the continuation is to contribute towards development of an acoustically-based explanatory model of intelligibility. Our approach 1) employs established perceptual procedures and acoustic measures, 2) uses an innovative analysis-resynthesis technique that permits conclusions concerning the explanatory relationship between acoustic changes accompanying dysarthria therapy techniques and intelligibility, and 3) leverages methods from machine learning to build a predictive model of intelligibility from acoustics. The impact of this work is in its contribution to 1) advancing conceptual understanding of intelligibility, 2) strengthening the scientific basis for treatment, and 3) optimizing clinical implementation of dysarthria therapy techniques. PUBLIC HEALTH RELEVANCE: This project is directly relevant to the mission of NIDCD due to its focus on investigating the therapeutic techniques for maximizing intelligibility in dysarthra secondary to Parkinson's disease and Multiple Sclerosis. By advancing conceptual understanding of intelligibility, this research will strengthen the scientific basis for dysarthria treatments and will optimize their clinical implementation.",Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates,9766229,R01DC004689,"['Acoustics', 'Address', 'Affect', 'Age', 'American', 'Articulation', 'Comparative Study', 'Development', 'Dysarthria', 'Employment', 'Evidence based practice', 'Frequencies', 'Funding', 'Genetic Transcription', 'Goals', 'Gold', 'Idiopathic Parkinson Disease', 'Individual', 'Instruction', 'Knowledge', 'Leisure Activities', 'Machine Learning', 'Measures', 'Methods', 'Mission', 'Modeling', 'Multiple Sclerosis', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Orthography', 'Outcome', 'Parkinson Disease', 'Procedures', 'Production', 'Publishing', 'Quality of life', 'Research', 'Secondary to', 'Societies', 'Speech', 'Techniques', 'Therapeutic', 'Variant', 'Work', 'base', 'clear speech', 'clinical implementation', 'comparative', 'experience', 'hearing impairment', 'improved', 'indexing', 'innovation', 'predictive modeling', 'public health relevance', 'sex', 'social', 'treatment optimization', 'treatment program']",NIDCD,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2019,517281,0.16469655201674321
"Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates     DESCRIPTION (provided by applicant): 90% of the 1.5 million Americans living with idiopathic Parkinson's disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria. Dysarthria has devastating consequences for life quality and participation in society due to its effects on employment, leisure activities and social relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by the gold standard perceptual construct of intelligibility is thus of vital importance. Owing to the scarcity of impartial comparative studies, the choice of one technique over others is often based on trial and error or reflects clinician bias, both of which are at odds with evidence-based practice. This project has sought to address this critical gap in knowledge regarding the comparative merits of dysarthria treatment techniques since its inception. Toward this end, published studies from the past funding cycle compared the acoustic and perceptual merits of three common, global dysarthria treatment techniques including 1) rate manipulation, 2) an increased vocal intensity and 3) clear speech in MS and PD as well as age and sex matched neurotypical talkers. Global treatment techniques by their very nature elicit co-occurring acoustic changes (e.g., duration, segmental articulation). Because an explanatory, acoustically-based model of intelligibility is lacking, the acoustic change(s) causing or explainin the improved perceptual outcomes of global treatment techniques are unknown. Determining the acoustic variables explaining intelligibility variation in dysarthria would not only tremendousy advance theoretical understanding of intelligibility but also would strengthen the scientific basis for treatment. Treatment focused on those acoustic variables explanatory for improved intelligibility may further accelerate progress in therapy. Importantly, research from the previous funding cycle suggests the promise of speech analysis-resynthesis for identifying segmental and suprasegmental acoustic variables explanatory for intelligibility in dysarthria. Building upon this work, the overarching goal of the continuation is to contribute towards development of an acoustically-based explanatory model of intelligibility. Our approach 1) employs established perceptual procedures and acoustic measures, 2) uses an innovative analysis-resynthesis technique that permits conclusions concerning the explanatory relationship between acoustic changes accompanying dysarthria therapy techniques and intelligibility, and 3) leverages methods from machine learning to build a predictive model of intelligibility from acoustics. The impact of this work is in its contribution to 1) advancing conceptual understanding of intelligibility, 2) strengthening the scientific basis for treatment, and 3) optimizing clinical implementation of dysarthria therapy techniques. PUBLIC HEALTH RELEVANCE: This project is directly relevant to the mission of NIDCD due to its focus on investigating the therapeutic techniques for maximizing intelligibility in dysarthra secondary to Parkinson's disease and Multiple Sclerosis. By advancing conceptual understanding of intelligibility, this research will strengthen the scientific basis for dysarthria treatments and will optimize their clinical implementation.",Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates,9549036,R01DC004689,"['Acoustics', 'Address', 'Affect', 'Age', 'American', 'Articulation', 'Comparative Study', 'Development', 'Dysarthria', 'Employment', 'Evidence based practice', 'Frequencies', 'Funding', 'Genetic Transcription', 'Goals', 'Gold', 'Idiopathic Parkinson Disease', 'Individual', 'Instruction', 'Knowledge', 'Leisure Activities', 'Machine Learning', 'Measures', 'Methods', 'Mission', 'Modeling', 'Multiple Sclerosis', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Orthography', 'Outcome', 'Parkinson Disease', 'Procedures', 'Production', 'Publishing', 'Quality of life', 'Research', 'Secondary to', 'Societies', 'Speech', 'Techniques', 'Therapeutic', 'Variant', 'Work', 'base', 'clear speech', 'clinical implementation', 'comparative', 'experience', 'hearing impairment', 'improved', 'indexing', 'innovation', 'predictive modeling', 'public health relevance', 'sex', 'social', 'treatment optimization', 'treatment program']",NIDCD,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2018,517281,0.16469655201674321
"Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates     DESCRIPTION (provided by applicant): 90% of the 1.5 million Americans living with idiopathic Parkinson's disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria. Dysarthria has devastating consequences for life quality and participation in society due to its effects on employment, leisure activities and social relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by the gold standard perceptual construct of intelligibility is thus of vital importance. Owing to the scarcity of impartial comparative studies, the choice of one technique over others is often based on trial and error or reflects clinician bias, both of which are at odds with evidence-based practice. This project has sought to address this critical gap in knowledge regarding the comparative merits of dysarthria treatment techniques since its inception. Toward this end, published studies from the past funding cycle compared the acoustic and perceptual merits of three common, global dysarthria treatment techniques including 1) rate manipulation, 2) an increased vocal intensity and 3) clear speech in MS and PD as well as age and sex matched neurotypical talkers. Global treatment techniques by their very nature elicit co-occurring acoustic changes (e.g., duration, segmental articulation). Because an explanatory, acoustically-based model of intelligibility is lacking, the acoustic change(s) causing or explainin the improved perceptual outcomes of global treatment techniques are unknown. Determining the acoustic variables explaining intelligibility variation in dysarthria would not only tremendousy advance theoretical understanding of intelligibility but also would strengthen the scientific basis for treatment. Treatment focused on those acoustic variables explanatory for improved intelligibility may further accelerate progress in therapy. Importantly, research from the previous funding cycle suggests the promise of speech analysis-resynthesis for identifying segmental and suprasegmental acoustic variables explanatory for intelligibility in dysarthria. Building upon this work, the overarching goal of the continuation is to contribute towards development of an acoustically-based explanatory model of intelligibility. Our approach 1) employs established perceptual procedures and acoustic measures, 2) uses an innovative analysis-resynthesis technique that permits conclusions concerning the explanatory relationship between acoustic changes accompanying dysarthria therapy techniques and intelligibility, and 3) leverages methods from machine learning to build a predictive model of intelligibility from acoustics. The impact of this work is in its contribution to 1) advancing conceptual understanding of intelligibility, 2) strengthening the scientific basis for treatment, and 3) optimizing clinical implementation of dysarthria therapy techniques. PUBLIC HEALTH RELEVANCE: This project is directly relevant to the mission of NIDCD due to its focus on investigating the therapeutic techniques for maximizing intelligibility in dysarthra secondary to Parkinson's disease and Multiple Sclerosis. By advancing conceptual understanding of intelligibility, this research will strengthen the scientific basis for dysarthria treatments and will optimize their clinical implementation.",Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates,9334170,R01DC004689,"['Acoustics', 'Address', 'Affect', 'Age', 'American', 'Articulation', 'Clinical', 'Comparative Study', 'Development', 'Dysarthria', 'Employment', 'Evidence based practice', 'Frequencies', 'Funding', 'Genetic Transcription', 'Goals', 'Gold', 'Idiopathic Parkinson Disease', 'Individual', 'Instruction', 'Knowledge', 'Leisure Activities', 'Machine Learning', 'Measures', 'Methods', 'Mission', 'Modeling', 'Multiple Sclerosis', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Orthography', 'Outcome', 'Parkinson Disease', 'Procedures', 'Production', 'Publishing', 'Quality of life', 'Research', 'Secondary to', 'Societies', 'Speech', 'Techniques', 'Therapeutic', 'Variant', 'Work', 'base', 'clear speech', 'comparative', 'experience', 'hearing impairment', 'improved', 'indexing', 'innovation', 'predictive modeling', 'public health relevance', 'sex', 'social', 'treatment program']",NIDCD,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2017,517281,0.16469655201674321
"Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates     DESCRIPTION (provided by applicant): 90% of the 1.5 million Americans living with idiopathic Parkinson's disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria. Dysarthria has devastating consequences for life quality and participation in society due to its effects on employment, leisure activities and social relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by the gold standard perceptual construct of intelligibility is thus of vital importance. Owing to the scarcity of impartial comparative studies, the choice of one technique over others is often based on trial and error or reflects clinician bias, both of which are at odds with evidence-based practice. This project has sought to address this critical gap in knowledge regarding the comparative merits of dysarthria treatment techniques since its inception. Toward this end, published studies from the past funding cycle compared the acoustic and perceptual merits of three common, global dysarthria treatment techniques including 1) rate manipulation, 2) an increased vocal intensity and 3) clear speech in MS and PD as well as age and sex matched neurotypical talkers. Global treatment techniques by their very nature elicit co-occurring acoustic changes (e.g., duration, segmental articulation). Because an explanatory, acoustically-based model of intelligibility is lacking, the acoustic change(s) causing or explainin the improved perceptual outcomes of global treatment techniques are unknown. Determining the acoustic variables explaining intelligibility variation in dysarthria would not only tremendousy advance theoretical understanding of intelligibility but also would strengthen the scientific basis for treatment. Treatment focused on those acoustic variables explanatory for improved intelligibility may further accelerate progress in therapy. Importantly, research from the previous funding cycle suggests the promise of speech analysis-resynthesis for identifying segmental and suprasegmental acoustic variables explanatory for intelligibility in dysarthria. Building upon this work, the overarching goal of the continuation is to contribute towards development of an acoustically-based explanatory model of intelligibility. Our approach 1) employs established perceptual procedures and acoustic measures, 2) uses an innovative analysis-resynthesis technique that permits conclusions concerning the explanatory relationship between acoustic changes accompanying dysarthria therapy techniques and intelligibility, and 3) leverages methods from machine learning to build a predictive model of intelligibility from acoustics. The impact of this work is in its contribution to 1) advancing conceptual understanding of intelligibility, 2) strengthening the scientific basis for treatment, and 3) optimizing clinical implementation of dysarthria therapy techniques. PUBLIC HEALTH RELEVANCE: This project is directly relevant to the mission of NIDCD due to its focus on investigating the therapeutic techniques for maximizing intelligibility in dysarthra secondary to Parkinson's disease and Multiple Sclerosis. By advancing conceptual understanding of intelligibility, this research will strengthen the scientific basis for dysarthria treatments and will optimize their clinical implementation.",Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates,9120824,R01DC004689,"['Acoustics', 'Address', 'Affect', 'Age', 'American', 'Clinical', 'Comparative Study', 'Development', 'Dysarthria', 'Employment', 'Evidence based practice', 'Frequencies', 'Funding', 'Genetic Transcription', 'Goals', 'Gold', 'Health', 'Idiopathic Parkinson Disease', 'Individual', 'Instruction', 'Joints', 'Knowledge', 'Leisure Activities', 'Life', 'Machine Learning', 'Measures', 'Methods', 'Mission', 'Modeling', 'Multiple Sclerosis', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Orthography', 'Outcome', 'Parkinson Disease', 'Procedures', 'Production', 'Publishing', 'Quality of life', 'Research', 'Secondary to', 'Societies', 'Speech', 'Techniques', 'Therapeutic', 'Variant', 'Work', 'base', 'clear speech', 'comparative', 'experience', 'hearing impairment', 'improved', 'indexing', 'innovation', 'predictive modeling', 'sex', 'social', 'treatment program']",NIDCD,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2016,504687,0.16469655201674321
"Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates     DESCRIPTION (provided by applicant): 90% of the 1.5 million Americans living with idiopathic Parkinson's disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria. Dysarthria has devastating consequences for life quality and participation in society due to its effects on employment, leisure activities and social relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by the gold standard perceptual construct of intelligibility is thus of vital importance. Owing to the scarcity of impartial comparative studies, the choice of one technique over others is often based on trial and error or reflects clinician bias, both of which are at odds with evidence-based practice. This project has sought to address this critical gap in knowledge regarding the comparative merits of dysarthria treatment techniques since its inception. Toward this end, published studies from the past funding cycle compared the acoustic and perceptual merits of three common, global dysarthria treatment techniques including 1) rate manipulation, 2) an increased vocal intensity and 3) clear speech in MS and PD as well as age and sex matched neurotypical talkers. Global treatment techniques by their very nature elicit co-occurring acoustic changes (e.g., duration, segmental articulation). Because an explanatory, acoustically-based model of intelligibility is lacking, the acoustic change(s) causing or explainin the improved perceptual outcomes of global treatment techniques are unknown. Determining the acoustic variables explaining intelligibility variation in dysarthria would not only tremendousy advance theoretical understanding of intelligibility but also would strengthen the scientific basis for treatment. Treatment focused on those acoustic variables explanatory for improved intelligibility may further accelerate progress in therapy. Importantly, research from the previous funding cycle suggests the promise of speech analysis-resynthesis for identifying segmental and suprasegmental acoustic variables explanatory for intelligibility in dysarthria. Building upon this work, the overarching goal of the continuation is to contribute towards development of an acoustically-based explanatory model of intelligibility. Our approach 1) employs established perceptual procedures and acoustic measures, 2) uses an innovative analysis-resynthesis technique that permits conclusions concerning the explanatory relationship between acoustic changes accompanying dysarthria therapy techniques and intelligibility, and 3) leverages methods from machine learning to build a predictive model of intelligibility from acoustics. The impact of this work is in its contribution to 1) advancing conceptual understanding of intelligibility, 2) strengthening the scientific basis for treatment, and 3) optimizing clinical implementation of dysarthria therapy techniques.         PUBLIC HEALTH RELEVANCE: This project is directly relevant to the mission of NIDCD due to its focus on investigating the therapeutic techniques for maximizing intelligibility in dysarthra secondary to Parkinson's disease and Multiple Sclerosis. By advancing conceptual understanding of intelligibility, this research will strengthen the scientific basis for dysarthria treatments and will optimize their clinical implementation.            ",Therapeutic Approaches to Dysarthria:  Acoustic and Perceptual Correlates,8955720,R01DC004689,"['Acoustics', 'Address', 'Affect', 'Age', 'American', 'Clinical', 'Comparative Study', 'Development', 'Dysarthria', 'Employment', 'Evidence based practice', 'Frequencies', 'Funding', 'Genetic Transcription', 'Goals', 'Gold', 'Individual', 'Instruction', 'Joints', 'Knowledge', 'Leisure Activities', 'Life', 'Machine Learning', 'Measures', 'Methods', 'Mission', 'Modeling', 'Multiple Sclerosis', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Orthography', 'Outcome', 'Parkinson Disease', 'Procedures', 'Production', 'Publishing', 'Quality of life', 'Research', 'Secondary to', 'Societies', 'Speech', 'Techniques', 'Therapeutic', 'Variant', 'Work', 'base', 'clear speech', 'comparative', 'experience', 'hearing impairment', 'improved', 'indexing', 'innovation', 'predictive modeling', 'public health relevance', 'sex', 'social', 'treatment program']",NIDCD,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2015,544862,0.16469655201674321
"PHYSICIAN-COMPUTER INTERACTION USING GRAPHICS AND SPEECH Recent advances in speech-recognition technology have made spoken language a viable interface medium for human-computer communication, promising increased acceptance of medical computer systems by physicians and other health workers. However, speech recognition techniques have not yet advanced to the point where understanding of unconstrained speech is possible. We accordingly propose a staged research plan to study the way computer graphics can be bused to constrain the possible meanings of an utterance, thereby promoting physician-computer communication that is both effective and practical.  We will create a prototype speech-and-graphics interface to ONCOCIN, a medical advice system previously developed in our laboratory.  We will use this prototype in experiments to discover how we might expand the interaction language to support more fully the kinds of phrases physicians would want to speak to ONCOCIN's graphical datasheet.  The results of these experiments will be used to extend the number and kind of phrases the ONCOCIN interface can accept.  We will then explore the use of graphics to constrain the task of speech-recognition of limited dictated text.  Using the same techniques employed for the graphical datasheet, we will design a structured progress note for reporting the status of cancer patients and implement it as a graphical interface.  In a parallel endeavor, we will perform experiments to study the language used in creating progress notes by simulating a graphical progress note entry system.  The research we propose benefits from our use of ONCOCIN, a medical expert system that already has a sophisticated graphical interface and is in limited clinical use.  The research gains additional impetus from our collaboration with Speech Systems incorporated, a company that is providing us with advanced speech-recognition technology.  n/a",PHYSICIAN-COMPUTER INTERACTION USING GRAPHICS AND SPEECH,3373988,R01LM004864,"['cancer information system', ' computer graphics /printing', ' computer simulation', ' physicians', ' speech']",NLM,STANFORD UNIVERSITY,R01,1990,132070,0.1182680707249924
"PHYSICIAN-COMPUTER INTERACTION USING GRAPHICS AND SPEECH Recent advances in speech-recognition technology have made spoken language a viable interface medium for human-computer communication, promising increased acceptance of medical computer systems by physicians and other health workers. However, speech recognition techniques have not yet advanced to the point where understanding of unconstrained speech is possible. We accordingly propose a staged research plan to study the way computer graphics can be bused to constrain the possible meanings of an utterance, thereby promoting physician-computer communication that is both effective and practical.  We will create a prototype speech-and-graphics interface to ONCOCIN, a medical advice system previously developed in our laboratory.  We will use this prototype in experiments to discover how we might expand the interaction language to support more fully the kinds of phrases physicians would want to speak to ONCOCIN's graphical datasheet.  The results of these experiments will be used to extend the number and kind of phrases the ONCOCIN interface can accept.  We will then explore the use of graphics to constrain the task of speech-recognition of limited dictated text.  Using the same techniques employed for the graphical datasheet, we will design a structured progress note for reporting the status of cancer patients and implement it as a graphical interface.  In a parallel endeavor, we will perform experiments to study the language used in creating progress notes by simulating a graphical progress note entry system.  The research we propose benefits from our use of ONCOCIN, a medical expert system that already has a sophisticated graphical interface and is in limited clinical use.  The research gains additional impetus from our collaboration with Speech Systems incorporated, a company that is providing us with advanced speech-recognition technology.  n/a",PHYSICIAN-COMPUTER INTERACTION USING GRAPHICS AND SPEECH,3373987,R01LM004864,"['cancer information system', ' computer graphics /printing', ' computer simulation', ' physicians', ' speech']",NLM,STANFORD UNIVERSITY,R01,1989,119574,0.1182680707249924
"PHYSICIAN-COMPUTER INTERACTION USING GRAPHICS AND SPEECH Recent advances in speech-recognition technology have made spoken language a viable interface medium for human-computer communication, promising increased acceptance of medical computer systems by physicians and other health workers. However, speech recognition techniques have not yet advanced to the point where understanding of unconstrained speech is possible. We accordingly propose a staged research plan to study the way computer graphics can be bused to constrain the possible meanings of an utterance, thereby promoting physician-computer communication that is both effective and practical.  We will create a prototype speech-and-graphics interface to ONCOCIN, a medical advice system previously developed in our laboratory.  We will use this prototype in experiments to discover how we might expand the interaction language to support more fully the kinds of phrases physicians would want to speak to ONCOCIN's graphical datasheet.  The results of these experiments will be used to extend the number and kind of phrases the ONCOCIN interface can accept.  We will then explore the use of graphics to constrain the task of speech-recognition of limited dictated text.  Using the same techniques employed for the graphical datasheet, we will design a structured progress note for reporting the status of cancer patients and implement it as a graphical interface.  In a parallel endeavor, we will perform experiments to study the language used in creating progress notes by simulating a graphical progress note entry system.  The research we propose benefits from our use of ONCOCIN, a medical expert system that already has a sophisticated graphical interface and is in limited clinical use.  The research gains additional impetus from our collaboration with Speech Systems incorporated, a company that is providing us with advanced speech-recognition technology.  n/a",PHYSICIAN-COMPUTER INTERACTION USING GRAPHICS AND SPEECH,3373985,R01LM004864,"['cancer information system', ' computer graphics /printing', ' computer simulation', ' physicians', ' speech']",NLM,STANFORD UNIVERSITY,R01,1988,117814,0.1182680707249924
"Analysis and Remediation of Language Production DESCRIPTION (provided by applicant): Aphasia strikes approximately one in 250 Americans. The reduced ability to communicate with language represents, in most cases, a catastrophic loss of self-sufficiency and a source of profound social isolation. No treatment for aphasia reported to date has reliably brought about changes in language production that migrate from highly constrained laboratory tasks such as single picture description to more challenging and socially functional tasks such as the production of entire narratives. The current climate in health care limits access to speech therapy, and thus it is imperative to develop approaches to treatment which allow patients to supplement 1:1 clinical treatment with intensive independent home practice. We have developed two computer programs to address the need for effective aphasia treatments that can be used semi-independently. One is a communication system (CS), which allows aphasic users to record spoken sentences a single word or phrase at a time, to replay these words or phrases, and to build them into sentences and narratives by manipulating visual icons on a computer screen. The other program is a language therapy system (TS) incorporating speech recognition and natural language understanding technology, which allows the computer to 'understand' the patient's spoken sentence and to provide feedback about whether it correctly describes a picture on the screen. This allows independent home practice of spoken language. The goals of this project are: (1) to replicate pilot results showing measurably more structured language production by aphasic patients using the CS, and to link these effects to characteristics of subjects' language processing impairments (Exp. 1); (2) to assess the impact of enhancing the CS with word-finding support for more severely impaired patients (Exp. 2); (3) to replicate the positive outcomes in pilot studies which used the TS and CS to improve aphasic patients' spoken language production, and to use the TS to train subjects on grammatical structures that provide tests of specific hypotheses about the impact of impaired short term memory on aphasic production (Exp. 3); and (4) to use data automatically collected by the CS to investigate the nature of the underlying disruption and to motivate the most effective approaches to remediation (Exp. 4). Information obtained from these studies will provide a basis for the further development of novel, theoretically motivated approaches to aphasia treatment. n/a",Analysis and Remediation of Language Production,7188572,R01DC005629,"['Address', 'American', 'Aphasia', 'Characteristics', 'Climate', 'Clinical Treatment', 'Communication', 'Computer software', 'Computers', 'Condition', 'Data', 'Development', 'Disruption', 'Elements', 'Employee Strikes', 'Evaluation', 'Facility Construction Funding Category', 'Feedback', 'Funding', 'Goals', 'Head', 'Healthcare', 'Home environment', 'Impairment', 'Knowledge', 'Laboratories', 'Language', 'Language Therapy', 'Linguistics', 'Link', 'Measures', 'Monitor', 'Natural Language Processing', 'Nature', 'Numbers', 'Outcome', 'Patients', 'Performance', 'Pilot Projects', 'Play', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Reporting', 'Role', 'Semantics', 'Short-Term Memory', 'Social isolation', 'Source', 'Speech', 'Speech Therapy', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Visual', 'analytical method', 'aphasic', 'base', 'computer program', 'improved', 'language processing', 'lexical', 'lexical retrieval', 'novel', 'phonology', 'programs', 'remediation', 'size', 'speech recognition', 'syntax', 'treatment effect']",NIDCD,UNIVERSITY OF MARYLAND BALTIMORE,R01,2007,337888,0.10264854993659459
"MRI & CT Studies of the Developing Vocal Tract  I. Project Summary/Abstract:  The postnatal growth of the oral and pharyngeal structures and their respective cavities - that define the vocal tract (VT) - entails changes in size, shape, and relative proportions. While prenatal and postnatal growth comprise a developmental continuum, current theory strongly associates the postnatal functions of the VT structures to guide its growth, particularly during infancy and early childhood. Acoustic theory affirms a relationship between the anatomy of the developing VT and the spectrum of speech sounds observed during development. However, although the available anatomic information remains inadequate to explain this relationship in detail, our VT Development Lab has made steady progress since 2000 towards its mission to quantitatively characterize sex-specific anatomic changes of the supralaryngeal speech system during development by: i) Establishing a unique imaging database consisting of 1116 MRI and CT studies across the lifespan that is representative of both sexes. ii) Compiling a large set of measurements that capture the concurrent growth of the head, face and VT structures throughout the lifespan; and creating composite 3D models where the spatial relation between select VT structures is maintained and from which measurements on relational growth will be secured. iii) Characterizing systematically the individual and relational growth of VT structures (ex. Vorperian et al. 1999, 2005), and synthesizing findings on the acoustic output of the developing VT, specifically the development of vowel acoustic space (Vorperian & Kent, 2007).  This proposed project combines imaging, acoustic analysis of speech, acoustic reflection or acoustic pharyngometry data, and VT modeling in an investigation on the development of the supra-laryngeal speech apparatus throughout the lifespan. The goals of this proposed research are to: (1) Expand the imaging and the measurements databases to include atypically developing cases, supplement measurements that are guided by embryologic origin of structures, and measurements from 3D models that capture additional information on the relational growth of VT structures. Also, to secure speech recordings and acoustic pharyngometry data mostly from individuals who will be imaged. (2) Assess perspectives on anatomic- acoustic relationships and structure/function interaction by statistically characterizing sex specific growth models and growth type of the VT structures in typically and atypically developing individuals; and assess their relative and relational growth while taking into account the structures' tissue type (bony, soft, cartilage and cavity), embryologic origin, plane of growth, and/or functional use. (3) Use complementary imaging and acoustic reflection data to configure developmental VT models and correlate model computed formant values with age specific acoustic data to make inferences regarding VT acoustic characteristics to developmental changes of specific anatomic structures. The findings will provide a coherent and much needed picture on the development of VT structures from embryo to geriatrics. Such information is foundational for both theoretical constructs and clinical application in multiple disciplines that deal with craniofacial structures and functions.  II. Project Narrative/Relevance of this research to public health  Imaging and acoustic methods are used to provide detailed information on the structural development of the oral and pharyngeal structures in both typically and atypically developing individuals. Anatomic data from typically developing individuals of both sexes will establish normative references for the growth of the oral and pharyngeal structures, along with their respective cavities and the vocal tract itself. This information is essential and valuable to a number of clinical disciplines - particularly those concerned with craniofacial anomalies - that are concerned with behaviors such as feeding, swallowing, speech production, and respiration. In addition, this information can be used to determine the efficacy of intervention strategies that effect developmental changes in anatomy, physiology, or acoustic output.",MRI & CT Studies of the Developing Vocal Tract,8499279,R01DC006282,"['Accounting', 'Achievement', 'Acoustics', 'Address', 'Adult', 'Advocate', 'Age', 'Anatomic structures', 'Anatomy', 'Area', 'Behavior', 'Birth', 'Cartilage', 'Characteristics', 'Childhood', 'Chromosome abnormality', 'Clinical', 'Data', 'Databases', 'Deglutition', 'Development', 'Dimensions', 'Discipline', 'Down Syndrome', 'Embryo', 'Face', 'Female', 'Genetic', 'Geriatrics', 'Goals', 'Growth', 'Head', 'Head and neck structure', 'Histocompatibility Testing', 'Image', 'Individual', 'Intervention', 'Investigation', 'Larynx', 'Length', 'Literature', 'Longevity', 'Magnetic Resonance Imaging', 'Measurement', 'Methods', 'Mission', 'Modeling', 'Morphology', 'Oropharyngeal', 'Output', 'Participant', 'Patients', 'Pattern', 'Pharyngeal structure', 'Physiological', 'Physiology', 'Principal Component Analysis', 'Process', 'Production', 'Property', 'Public Health', 'Publishing', 'Relative (related person)', 'Reporting', 'Research', 'Respiration', 'Robin bird', 'Sampling', 'Secure', 'Series', 'Shapes', 'Shprintzen syndrome', 'Simulate', 'Solid', 'Specific qualifier value', 'Speech', 'Speech Acoustics', 'Speech Development', 'Speech Disorders', 'Speech Sound', 'Statistical Methods', 'Structure', 'Structure-Activity Relationship', 'System', 'Techniques', 'Tissues', 'Work', 'X-Ray Computed Tomography', 'abstracting', 'acoustic imaging', 'base', 'clinical application', 'craniofacial', 'craniofacial complex', 'early childhood', 'feeding', 'improved', 'infancy', 'male', 'novel strategies', 'postnatal', 'prenatal', 'sex', 'soft tissue', 'theories', 'three-dimensional modeling', 'trend']",NIDCD,UNIVERSITY OF WISCONSIN-MADISON,R01,2013,505217,0.13669648342691598
"MRI & CT Studies of the Developing Vocal Tract  I. Project Summary/Abstract:  The postnatal growth of the oral and pharyngeal structures and their respective cavities - that define the vocal tract (VT) - entails changes in size, shape, and relative proportions. While prenatal and postnatal growth comprise a developmental continuum, current theory strongly associates the postnatal functions of the VT structures to guide its growth, particularly during infancy and early childhood. Acoustic theory affirms a relationship between the anatomy of the developing VT and the spectrum of speech sounds observed during development. However, although the available anatomic information remains inadequate to explain this relationship in detail, our VT Development Lab has made steady progress since 2000 towards its mission to quantitatively characterize sex-specific anatomic changes of the supralaryngeal speech system during development by: i) Establishing a unique imaging database consisting of 1116 MRI and CT studies across the lifespan that is representative of both sexes. ii) Compiling a large set of measurements that capture the concurrent growth of the head, face and VT structures throughout the lifespan; and creating composite 3D models where the spatial relation between select VT structures is maintained and from which measurements on relational growth will be secured. iii) Characterizing systematically the individual and relational growth of VT structures (ex. Vorperian et al. 1999, 2005), and synthesizing findings on the acoustic output of the developing VT, specifically the development of vowel acoustic space (Vorperian & Kent, 2007).  This proposed project combines imaging, acoustic analysis of speech, acoustic reflection or acoustic pharyngometry data, and VT modeling in an investigation on the development of the supra-laryngeal speech apparatus throughout the lifespan. The goals of this proposed research are to: (1) Expand the imaging and the measurements databases to include atypically developing cases, supplement measurements that are guided by embryologic origin of structures, and measurements from 3D models that capture additional information on the relational growth of VT structures. Also, to secure speech recordings and acoustic pharyngometry data mostly from individuals who will be imaged. (2) Assess perspectives on anatomic- acoustic relationships and structure/function interaction by statistically characterizing sex specific growth models and growth type of the VT structures in typically and atypically developing individuals; and assess their relative and relational growth while taking into account the structures' tissue type (bony, soft, cartilage and cavity), embryologic origin, plane of growth, and/or functional use. (3) Use complementary imaging and acoustic reflection data to configure developmental VT models and correlate model computed formant values with age specific acoustic data to make inferences regarding VT acoustic characteristics to developmental changes of specific anatomic structures. The findings will provide a coherent and much needed picture on the development of VT structures from embryo to geriatrics. Such information is foundational for both theoretical constructs and clinical application in multiple disciplines that deal with craniofacial structures and functions.  II. Project Narrative/Relevance of this research to public health  Imaging and acoustic methods are used to provide detailed information on the structural development of the oral and pharyngeal structures in both typically and atypically developing individuals. Anatomic data from typically developing individuals of both sexes will establish normative references for the growth of the oral and pharyngeal structures, along with their respective cavities and the vocal tract itself. This information is essential and valuable to a number of clinical disciplines - particularly those concerned with craniofacial anomalies - that are concerned with behaviors such as feeding, swallowing, speech production, and respiration. In addition, this information can be used to determine the efficacy of intervention strategies that effect developmental changes in anatomy, physiology, or acoustic output.",MRI & CT Studies of the Developing Vocal Tract,8284439,R01DC006282,"['Accounting', 'Achievement', 'Acoustics', 'Address', 'Adult', 'Advocate', 'Age', 'Anatomic structures', 'Anatomy', 'Area', 'Behavior', 'Birth', 'Cartilage', 'Characteristics', 'Childhood', 'Chromosome abnormality', 'Clinical', 'Data', 'Databases', 'Deglutition', 'Development', 'Dimensions', 'Discipline', 'Down Syndrome', 'Embryo', 'Face', 'Female', 'Genetic', 'Geriatrics', 'Goals', 'Growth', 'Head', 'Head and neck structure', 'Histocompatibility Testing', 'Image', 'Individual', 'Intervention', 'Investigation', 'Larynx', 'Length', 'Literature', 'Longevity', 'Magnetic Resonance Imaging', 'Measurement', 'Methods', 'Mission', 'Modeling', 'Morphology', 'Oropharyngeal', 'Output', 'Participant', 'Patients', 'Pattern', 'Pharyngeal structure', 'Physiological', 'Physiology', 'Principal Component Analysis', 'Process', 'Production', 'Property', 'Public Health', 'Publishing', 'Relative (related person)', 'Reporting', 'Research', 'Respiration', 'Robin bird', 'Sampling', 'Secure', 'Series', 'Shapes', 'Shprintzen syndrome', 'Simulate', 'Solid', 'Specific qualifier value', 'Speech', 'Speech Acoustics', 'Speech Development', 'Speech Disorders', 'Speech Sound', 'Statistical Methods', 'Structure', 'Structure-Activity Relationship', 'System', 'Techniques', 'Tissues', 'Work', 'X-Ray Computed Tomography', 'abstracting', 'acoustic imaging', 'base', 'clinical application', 'craniofacial', 'craniofacial complex', 'early childhood', 'feeding', 'improved', 'infancy', 'male', 'novel strategies', 'postnatal', 'prenatal', 'sex', 'soft tissue', 'theories', 'three-dimensional modeling', 'trend']",NIDCD,UNIVERSITY OF WISCONSIN-MADISON,R01,2012,544364,0.13669648342691598
"MRI & CT Studies of the Developing Vocal Tract    DESCRIPTION (provided by applicant): The postnatal growth of the oral and pharyngeal structures and their respective cavities - that define the vocal tract (VT) - entails changes in size, shape, and relative proportions. While prenatal and postnatal growth comprise a developmental continuum, current theory strongly associates the postnatal functions of the VT structures to guide its growth, particularly during infancy and early childhood. Acoustic theory affirms a relationship between the anatomy of the developing VT and the spectrum of speech sounds observed during development. However, although the available anatomic information remains inadequate to explain this relationship in detail, our VT Development Lab has made steady progress since 2000 towards its mission to quantitatively characterize sex-specific anatomic changes of the supralaryngeal speech system during development by: i) Establishing a unique imaging database consisting of 1116 MRI and CT studies across the lifespan that is representative of both sexes. ii) Compiling a large set of measurements that capture the concurrent growth of the head, face and VT structures throughout the lifespan; and creating composite 3D models where the spatial relation between select VT structures is maintained and from which measurements on relational growth will be secured. iii) Characterizing systematically the individual and relational growth of VT structures (ex. Vorperian et al. 1999, 2005), and synthesizing findings on the acoustic output of the developing VT, specifically the development of vowel acoustic space (Vorperian & Kent, 2007). This proposed project combines imaging, acoustic analysis of speech, acoustic reflection or acoustic pharyngometry data, and VT modeling in an investigation on the development of the supra-laryngeal speech apparatus throughout the lifespan. The goals of this proposed research are to: (1) Expand the imaging and the measurements databases to include atypically developing cases, supplement measurements that are guided by embryologic origin of structures, and measurements from 3D models that capture additional information on the relational growth of VT structures. Also, to secure speech recordings and acoustic pharyngometry data mostly from individuals who will be imaged. (2) Assess perspectives on anatomic- acoustic relationships and structure/function interaction by statistically characterizing sex specific growth models and growth type of the VT structures in typically and atypically developing individuals; and assess their relative and relational growth while taking into account the structures' tissue type (bony, soft, cartilage and cavity), embryologic origin, plane of growth, and/or functional use. (3) Use complementary imaging and acoustic reflection data to configure developmental VT models and correlate model computed formant values with age specific acoustic data to make inferences regarding VT acoustic characteristics to developmental changes of specific anatomic structures. The findings will provide a coherent and much needed picture on the development of VT structures from embryo to geriatrics. Such information is foundational for both theoretical constructs and clinical application in multiple disciplines that deal with craniofacial structures and functions. PUBLIC HEALTH RELEVANCE: Imaging and acoustic methods are used to provide detailed information on the structural development of the oral and pharyngeal structures in both typically and atypically developing individuals. Anatomic data from typically developing individuals of both sexes will establish normative references for the growth of the oral and pharyngeal structures, along with their respective cavities and the vocal tract itself. This information is essential and valuable to a number of clinical disciplines - particularly those concerned with craniofacial anomalies - that are concerned with behaviors such as feeding, swallowing, speech production, and respiration. In addition, this information can be used to determine the efficacy of intervention strategies that effect developmental changes in anatomy, physiology, or acoustic output.           II. Project Narrative/Relevance of this research to public health  Imaging and acoustic methods are used to provide detailed information on the structural development of the oral and pharyngeal structures in both typically and atypically developing individuals. Anatomic data from typically developing individuals of both sexes will establish normative references for the growth of the oral and pharyngeal structures, along with their respective cavities and the vocal tract itself. This information is essential and valuable to a number of clinical disciplines - particularly those concerned with craniofacial anomalies - that are concerned with behaviors such as feeding, swallowing, speech production, and respiration. In addition, this information can be used to determine the efficacy of intervention strategies that effect developmental changes in anatomy, physiology, or acoustic output.",MRI & CT Studies of the Developing Vocal Tract,8096591,R01DC006282,"['Accounting', 'Achievement', 'Acoustics', 'Address', 'Adult', 'Advocate', 'Age', 'Anatomic structures', 'Anatomy', 'Area', 'Behavior', 'Birth', 'Cartilage', 'Characteristics', 'Childhood', 'Chromosome abnormality', 'Clinical', 'Data', 'Databases', 'Deglutition', 'Development', 'Dimensions', 'Discipline', 'Down Syndrome', 'Embryo', 'Face', 'Female', 'Genetic', 'Geriatrics', 'Goals', 'Growth', 'Head', 'Head and neck structure', 'Health', 'Histocompatibility Testing', 'Image', 'Individual', 'Intervention', 'Investigation', 'Larynx', 'Length', 'Literature', 'Longevity', 'Magnetic Resonance Imaging', 'Measurement', 'Methods', 'Mission', 'Modeling', 'Morphology', 'Oropharyngeal', 'Output', 'Participant', 'Patients', 'Pattern', 'Pharyngeal structure', 'Physiological', 'Physiology', 'Principal Component Analysis', 'Process', 'Production', 'Property', 'Public Health', 'Publishing', 'Relative (related person)', 'Reporting', 'Research', 'Respiration', 'Robin bird', 'Sampling', 'Secure', 'Series', 'Shapes', 'Shprintzen syndrome', 'Simulate', 'Solid', 'Specific qualifier value', 'Speech', 'Speech Acoustics', 'Speech Development', 'Speech Disorders', 'Speech Sound', 'Statistical Methods', 'Structure', 'Structure-Activity Relationship', 'System', 'Techniques', 'Tissues', 'Work', 'X-Ray Computed Tomography', 'acoustic imaging', 'base', 'clinical application', 'craniofacial', 'craniofacial complex', 'early childhood', 'feeding', 'improved', 'infancy', 'male', 'novel strategies', 'postnatal', 'prenatal', 'sex', 'soft tissue', 'theories', 'three-dimensional modeling', 'trend']",NIDCD,UNIVERSITY OF WISCONSIN-MADISON,R01,2011,543307,0.1435485489385355
"MRI & CT Studies of the Developing Vocal Tract    DESCRIPTION (provided by applicant): The postnatal growth of the oral and pharyngeal structures and their respective cavities - that define the vocal tract (VT) - entails changes in size, shape, and relative proportions. While prenatal and postnatal growth comprise a developmental continuum, current theory strongly associates the postnatal functions of the VT structures to guide its growth, particularly during infancy and early childhood. Acoustic theory affirms a relationship between the anatomy of the developing VT and the spectrum of speech sounds observed during development. However, although the available anatomic information remains inadequate to explain this relationship in detail, our VT Development Lab has made steady progress since 2000 towards its mission to quantitatively characterize sex-specific anatomic changes of the supralaryngeal speech system during development by: i) Establishing a unique imaging database consisting of 1116 MRI and CT studies across the lifespan that is representative of both sexes. ii) Compiling a large set of measurements that capture the concurrent growth of the head, face and VT structures throughout the lifespan; and creating composite 3D models where the spatial relation between select VT structures is maintained and from which measurements on relational growth will be secured. iii) Characterizing systematically the individual and relational growth of VT structures (ex. Vorperian et al. 1999, 2005), and synthesizing findings on the acoustic output of the developing VT, specifically the development of vowel acoustic space (Vorperian & Kent, 2007). This proposed project combines imaging, acoustic analysis of speech, acoustic reflection or acoustic pharyngometry data, and VT modeling in an investigation on the development of the supra-laryngeal speech apparatus throughout the lifespan. The goals of this proposed research are to: (1) Expand the imaging and the measurements databases to include atypically developing cases, supplement measurements that are guided by embryologic origin of structures, and measurements from 3D models that capture additional information on the relational growth of VT structures. Also, to secure speech recordings and acoustic pharyngometry data mostly from individuals who will be imaged. (2) Assess perspectives on anatomic- acoustic relationships and structure/function interaction by statistically characterizing sex specific growth models and growth type of the VT structures in typically and atypically developing individuals; and assess their relative and relational growth while taking into account the structures' tissue type (bony, soft, cartilage and cavity), embryologic origin, plane of growth, and/or functional use. (3) Use complementary imaging and acoustic reflection data to configure developmental VT models and correlate model computed formant values with age specific acoustic data to make inferences regarding VT acoustic characteristics to developmental changes of specific anatomic structures. The findings will provide a coherent and much needed picture on the development of VT structures from embryo to geriatrics. Such information is foundational for both theoretical constructs and clinical application in multiple disciplines that deal with craniofacial structures and functions. PUBLIC HEALTH RELEVANCE: Imaging and acoustic methods are used to provide detailed information on the structural development of the oral and pharyngeal structures in both typically and atypically developing individuals. Anatomic data from typically developing individuals of both sexes will establish normative references for the growth of the oral and pharyngeal structures, along with their respective cavities and the vocal tract itself. This information is essential and valuable to a number of clinical disciplines - particularly those concerned with craniofacial anomalies - that are concerned with behaviors such as feeding, swallowing, speech production, and respiration. In addition, this information can be used to determine the efficacy of intervention strategies that effect developmental changes in anatomy, physiology, or acoustic output.           II. Project Narrative/Relevance of this research to public health  Imaging and acoustic methods are used to provide detailed information on the structural development of the oral and pharyngeal structures in both typically and atypically developing individuals. Anatomic data from typically developing individuals of both sexes will establish normative references for the growth of the oral and pharyngeal structures, along with their respective cavities and the vocal tract itself. This information is essential and valuable to a number of clinical disciplines - particularly those concerned with craniofacial anomalies - that are concerned with behaviors such as feeding, swallowing, speech production, and respiration. In addition, this information can be used to determine the efficacy of intervention strategies that effect developmental changes in anatomy, physiology, or acoustic output.",MRI & CT Studies of the Developing Vocal Tract,7893080,R01DC006282,"['Accounting', 'Achievement', 'Acoustics', 'Address', 'Adult', 'Advocate', 'Age', 'Anatomic structures', 'Anatomy', 'Area', 'Behavior', 'Birth', 'Cartilage', 'Characteristics', 'Childhood', 'Chromosome abnormality', 'Clinical', 'Data', 'Databases', 'Deglutition', 'Development', 'Dimensions', 'Discipline', 'Down Syndrome', 'Embryo', 'Face', 'Female', 'Genetic', 'Geriatrics', 'Goals', 'Growth', 'Head', 'Head and neck structure', 'Histocompatibility Testing', 'Image', 'Individual', 'Intervention', 'Investigation', 'Larynx', 'Length', 'Literature', 'Longevity', 'Magnetic Resonance Imaging', 'Measurement', 'Methods', 'Mission', 'Modeling', 'Morphology', 'Oropharyngeal', 'Output', 'Participant', 'Patients', 'Pattern', 'Pharyngeal structure', 'Physiological', 'Physiology', 'Principal Component Analysis', 'Process', 'Production', 'Property', 'Public Health', 'Publishing', 'Relative (related person)', 'Reporting', 'Research', 'Respiration', 'Robin bird', 'Sampling', 'Secure', 'Series', 'Shapes', 'Shprintzen syndrome', 'Simulate', 'Solid', 'Specific qualifier value', 'Speech', 'Speech Acoustics', 'Speech Development', 'Speech Disorders', 'Speech Sound', 'Statistical Methods', 'Structure', 'Structure-Activity Relationship', 'System', 'Techniques', 'Tissues', 'Work', 'X-Ray Computed Tomography', 'acoustic imaging', 'base', 'clinical application', 'craniofacial', 'craniofacial complex', 'early childhood', 'feeding', 'improved', 'infancy', 'male', 'novel strategies', 'postnatal', 'prenatal', 'public health relevance', 'sex', 'soft tissue', 'theories', 'three-dimensional modeling', 'trend']",NIDCD,UNIVERSITY OF WISCONSIN-MADISON,R01,2010,547763,0.1435485489385355
"MRI & CT Studies of the Developing Vocal Tract    DESCRIPTION (provided by applicant): The postnatal growth of the oral and pharyngeal structures and their respective cavities - that define the vocal tract (VT) - entails changes in size, shape, and relative proportions. While prenatal and postnatal growth comprise a developmental continuum, current theory strongly associates the postnatal functions of the VT structures to guide its growth, particularly during infancy and early childhood. Acoustic theory affirms a relationship between the anatomy of the developing VT and the spectrum of speech sounds observed during development. However, although the available anatomic information remains inadequate to explain this relationship in detail, our VT Development Lab has made steady progress since 2000 towards its mission to quantitatively characterize sex-specific anatomic changes of the supralaryngeal speech system during development by: i) Establishing a unique imaging database consisting of 1116 MRI and CT studies across the lifespan that is representative of both sexes. ii) Compiling a large set of measurements that capture the concurrent growth of the head, face and VT structures throughout the lifespan; and creating composite 3D models where the spatial relation between select VT structures is maintained and from which measurements on relational growth will be secured. iii) Characterizing systematically the individual and relational growth of VT structures (ex. Vorperian et al. 1999, 2005), and synthesizing findings on the acoustic output of the developing VT, specifically the development of vowel acoustic space (Vorperian & Kent, 2007). This proposed project combines imaging, acoustic analysis of speech, acoustic reflection or acoustic pharyngometry data, and VT modeling in an investigation on the development of the supra-laryngeal speech apparatus throughout the lifespan. The goals of this proposed research are to: (1) Expand the imaging and the measurements databases to include atypically developing cases, supplement measurements that are guided by embryologic origin of structures, and measurements from 3D models that capture additional information on the relational growth of VT structures. Also, to secure speech recordings and acoustic pharyngometry data mostly from individuals who will be imaged. (2) Assess perspectives on anatomic- acoustic relationships and structure/function interaction by statistically characterizing sex specific growth models and growth type of the VT structures in typically and atypically developing individuals; and assess their relative and relational growth while taking into account the structures' tissue type (bony, soft, cartilage and cavity), embryologic origin, plane of growth, and/or functional use. (3) Use complementary imaging and acoustic reflection data to configure developmental VT models and correlate model computed formant values with age specific acoustic data to make inferences regarding VT acoustic characteristics to developmental changes of specific anatomic structures. The findings will provide a coherent and much needed picture on the development of VT structures from embryo to geriatrics. Such information is foundational for both theoretical constructs and clinical application in multiple disciplines that deal with craniofacial structures and functions. PUBLIC HEALTH RELEVANCE: Imaging and acoustic methods are used to provide detailed information on the structural development of the oral and pharyngeal structures in both typically and atypically developing individuals. Anatomic data from typically developing individuals of both sexes will establish normative references for the growth of the oral and pharyngeal structures, along with their respective cavities and the vocal tract itself. This information is essential and valuable to a number of clinical disciplines - particularly those concerned with craniofacial anomalies - that are concerned with behaviors such as feeding, swallowing, speech production, and respiration. In addition, this information can be used to determine the efficacy of intervention strategies that effect developmental changes in anatomy, physiology, or acoustic output.           II. Project Narrative/Relevance of this research to public health  Imaging and acoustic methods are used to provide detailed information on the structural development of the oral and pharyngeal structures in both typically and atypically developing individuals. Anatomic data from typically developing individuals of both sexes will establish normative references for the growth of the oral and pharyngeal structures, along with their respective cavities and the vocal tract itself. This information is essential and valuable to a number of clinical disciplines - particularly those concerned with craniofacial anomalies - that are concerned with behaviors such as feeding, swallowing, speech production, and respiration. In addition, this information can be used to determine the efficacy of intervention strategies that effect developmental changes in anatomy, physiology, or acoustic output.",MRI & CT Studies of the Developing Vocal Tract,7737447,R01DC006282,"['Accounting', 'Achievement', 'Acoustics', 'Address', 'Adult', 'Advocate', 'Age', 'Anatomic structures', 'Anatomy', 'Area', 'Behavior', 'Birth', 'Cartilage', 'Characteristics', 'Childhood', 'Chromosome abnormality', 'Clinical', 'Data', 'Databases', 'Deglutition', 'Development', 'Dimensions', 'Discipline', 'Down Syndrome', 'Embryo', 'Face', 'Female', 'Genetic', 'Geriatrics', 'Goals', 'Growth', 'Head', 'Head and neck structure', 'Histocompatibility Testing', 'Image', 'Individual', 'Intervention', 'Investigation', 'Larynx', 'Length', 'Literature', 'Longevity', 'Magnetic Resonance Imaging', 'Measurement', 'Methods', 'Mission', 'Modeling', 'Morphology', 'Oropharyngeal', 'Output', 'Participant', 'Patients', 'Pattern', 'Pharyngeal structure', 'Physiological', 'Physiology', 'Principal Component Analysis', 'Process', 'Production', 'Property', 'Public Health', 'Publishing', 'Relative (related person)', 'Reporting', 'Research', 'Respiration', 'Robin bird', 'Sampling', 'Secure', 'Series', 'Shapes', 'Shprintzen syndrome', 'Simulate', 'Solid', 'Specific qualifier value', 'Speech', 'Speech Acoustics', 'Speech Development', 'Speech Disorders', 'Speech Sound', 'Statistical Methods', 'Structure', 'Structure-Activity Relationship', 'System', 'Techniques', 'Tissues', 'Work', 'X-Ray Computed Tomography', 'acoustic imaging', 'base', 'clinical application', 'craniofacial', 'craniofacial complex', 'early childhood', 'feeding', 'improved', 'infancy', 'male', 'novel strategies', 'postnatal', 'prenatal', 'public health relevance', 'sex', 'soft tissue', 'theories', 'three-dimensional modeling', 'trend']",NIDCD,UNIVERSITY OF WISCONSIN-MADISON,R01,2009,542532,0.1435485489385355
"Diagnostic Markers for Childhood Apraxia Speech DESCRIPTION (provided by applicant): Childhood Apraxia of Speech is a highly controversial disorder due to a lack of consensus on the features that define it and the etiologic conditions that explain its origin. The term Suspected Apraxia of Speech (sAOS) has been proposed as an interim term for this putative clinical entity. The point prevalence of sAOS in young children has been estimated at approximately 0.1%. The long-term objective of this proposal is to develop a valid, reliable, and efficient means to classify children as positive for sAOS. In addition to the contributions to theoretical explication of AOS, the software-based diagnostic tools resulting from this work will allow any certified speech-language pathologist to determine if a child's speech includes prosodic features that fall within a 95% confidence interval supporting the diagnosis of sAOS. The aim for this first period of planned programmatic research is to develop automated diagnostic markers for sAOS with clinically adequate sensitivity and specificity (> 90% positive and negative likelihood ratios).  The four specific aims are: (a) to automate and improve the sensitivity and specificity of two existing (manually derived) prosodic markers, (b) to develop four additional automatic, prosody-based diagnostic markers, (c) to derive a single diagnostic index based on a statistical derivative from the six individual markers, and (d) to validate the composite diagnostic marker using classification data obtained from expert clinical researchers.  Procedures are divided into four phases. In Year 1, automated versions of existing markers will be developed that determine speech-event locations using automatic speech recognition (ASR). Based on two pilot studies, this technique is expected to yield results equivalent to published data. The sensitivity of the markers will be improved by methods including normalizing by speaking rate and vowel identity. In Year 2, new automated markers will be created based on ASR and speech-signal processing techniques. These markers will measure variation in interstress timing, linguistic rhythm, speaking rate, and glottal-source characteristics. In the first part of Year 3, results from all six markers will be combined into a single diagnostic index using multi-layer perceptrons. In the latter part of Year 3, per-child errors will be evaluated to determine relationships between specific prosodic factors and the diagnosis of sAOS, providing insight into the features and definition of sAOS. n/a",Diagnostic Markers for Childhood Apraxia Speech,7035268,R21DC006722,"['apraxias', 'artificial intelligence', 'biomarker', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'diagnosis design /evaluation', 'human data', 'speech disorder diagnosis', 'speech disorders', 'speech recognition']",NIDCD,OREGON HEALTH AND SCIENCE UNIVERSITY,R21,2006,159900,0.12267258843586433
"Diagnostic Markers for Childhood Apraxia Speech DESCRIPTION (provided by applicant): Childhood Apraxia of Speech is a highly controversial disorder due to a lack of consensus on the features that define it and the etiologic conditions that explain its origin. The term Suspected Apraxia of Speech (sAOS) has been proposed as an interim term for this putative clinical entity. The point prevalence of sAOS in young children has been estimated at approximately 0.1%. The long-term objective of this proposal is to develop a valid, reliable, and efficient means to classify children as positive for sAOS. In addition to the contributions to theoretical explication of AOS, the software-based diagnostic tools resulting from this work will allow any certified speech-language pathologist to determine if a child's speech includes prosodic features that fall within a 95% confidence interval supporting the diagnosis of sAOS. The aim for this first period of planned programmatic research is to develop automated diagnostic markers for sAOS with clinically adequate sensitivity and specificity (> 90% positive and negative likelihood ratios).  The four specific aims are: (a) to automate and improve the sensitivity and specificity of two existing (manually derived) prosodic markers, (b) to develop four additional automatic, prosody-based diagnostic markers, (c) to derive a single diagnostic index based on a statistical derivative from the six individual markers, and (d) to validate the composite diagnostic marker using classification data obtained from expert clinical researchers.  Procedures are divided into four phases. In Year 1, automated versions of existing markers will be developed that determine speech-event locations using automatic speech recognition (ASR). Based on two pilot studies, this technique is expected to yield results equivalent to published data. The sensitivity of the markers will be improved by methods including normalizing by speaking rate and vowel identity. In Year 2, new automated markers will be created based on ASR and speech-signal processing techniques. These markers will measure variation in interstress timing, linguistic rhythm, speaking rate, and glottal-source characteristics. In the first part of Year 3, results from all six markers will be combined into a single diagnostic index using multi-layer perceptrons. In the latter part of Year 3, per-child errors will be evaluated to determine relationships between specific prosodic factors and the diagnosis of sAOS, providing insight into the features and definition of sAOS. n/a",Diagnostic Markers for Childhood Apraxia Speech,6881272,R21DC006722,"['apraxias', 'artificial intelligence', 'biomarker', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'diagnosis design /evaluation', 'human data', 'speech disorder diagnosis', 'speech disorders', 'speech recognition']",NIDCD,OREGON HEALTH AND SCIENCE UNIVERSITY,R21,2005,164000,0.12267258843586433
"Diagnostic Markers for Childhood Apraxia Speech DESCRIPTION (provided by applicant): Childhood Apraxia of Speech is a highly controversial disorder due to a lack of consensus on the features that define it and the etiologic conditions that explain its origin. The term Suspected Apraxia of Speech (sAOS) has been proposed as an interim term for this putative clinical entity. The point prevalence of sAOS in young children has been estimated at approximately 0.1%. The long-term objective of this proposal is to develop a valid, reliable, and efficient means to classify children as positive for sAOS. In addition to the contributions to theoretical explication of AOS, the software-based diagnostic tools resulting from this work will allow any certified speech-language pathologist to determine if a child's speech includes prosodic features that fall within a 95% confidence interval supporting the diagnosis of sAOS. The aim for this first period of planned programmatic research is to develop automated diagnostic markers for sAOS with clinically adequate sensitivity and specificity (> 90% positive and negative likelihood ratios).  The four specific aims are: (a) to automate and improve the sensitivity and specificity of two existing (manually derived) prosodic markers, (b) to develop four additional automatic, prosody-based diagnostic markers, (c) to derive a single diagnostic index based on a statistical derivative from the six individual markers, and (d) to validate the composite diagnostic marker using classification data obtained from expert clinical researchers.  Procedures are divided into four phases. In Year 1, automated versions of existing markers will be developed that determine speech-event locations using automatic speech recognition (ASR). Based on two pilot studies, this technique is expected to yield results equivalent to published data. The sensitivity of the markers will be improved by methods including normalizing by speaking rate and vowel identity. In Year 2, new automated markers will be created based on ASR and speech-signal processing techniques. These markers will measure variation in interstress timing, linguistic rhythm, speaking rate, and glottal-source characteristics. In the first part of Year 3, results from all six markers will be combined into a single diagnostic index using multi-layer perceptrons. In the latter part of Year 3, per-child errors will be evaluated to determine relationships between specific prosodic factors and the diagnosis of sAOS, providing insight into the features and definition of sAOS. n/a",Diagnostic Markers for Childhood Apraxia Speech,6782453,R21DC006722,"['apraxias', 'artificial intelligence', 'biomarker', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'diagnosis design /evaluation', 'human data', 'speech disorder diagnosis', 'speech disorders', 'speech recognition']",NIDCD,OREGON HEALTH AND SCIENCE UNIVERSITY,R21,2004,164000,0.12267258843586433
"Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes Project Summary The trained ear of the speech-language pathologist is the gold standard assessment tool for clinical practice in motor speech disorders. However, perceptual judgments are vulnerable to bias and their relationship with estimates of listener intelligibility  the final arbiter of speech goodness  is indeterminate. Interpretable, objective, and robust outcome measures that provide targets for treatment are urgently needed in order to provide more precise care and reliably monitor patient progress. Based on theoretical models of speech perception, in our previous grants we have developed a novel set of outcome measures that provide a multidimensional intelligibility profile (MIP) by using custom speech stimuli and a new coding strategy that allows us to capture the types of errors that listeners make when listening to dysarthric speech. This has led to a more complete intelligibility profile that codifies these errors at different levels of granularity, from global to discrete. Simultaneously, we have also developed a computational model for evaluation of dysarthric speech capable of reliably estimating a limited set of intelligibility measures directly from the speech acoustics. To date, both the outcome measures and the objective model have been evaluated on cross-sectional data only. In this renewal application, our principal goal is to evaluate specific hypotheses regarding expected changes in this multidimensional intelligibility profile as a result of different intervention instruction conditions (loud, clear, slow). A secondary goal of the proposal is to further refine our objective model to predict the complete intelligibility profile and to evaluate its ability to detect intelligibility changes within individual speakers. This is critical for clinicians who currently have no objective ways to assess the value of their interventions. With the aim of improving the standard of care through technology, the long- term goal of this proposal is to develop stand-alone objective outcome measures for dysarthria that can provide clinicians with reliable treatment targets. Such applications have the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury. Furthermore, these applications also have the potential to reduce health disparities by partially automating clinical intervention and providing easier access to these services to those in remote areas or in underdeveloped countries. Project Narrative There is an urgent need in the field of speech-language pathology for objective outcome measures of speech intelligibility that provide clinicians with actionable information regarding treatment targets. This proposal seeks to leverage theoretical advances in speech intelligibility to evaluate the sensitivity of a novel multidimensional intelligibility profile that quantifies the perceptual effects of speech change. Using listener transcriptions of dysarthric speech, along with a suite of automated acoustic metrics, the predictive model uses machine-learning algorithms to learn the relationship between speech acoustics and listener percepts. Ultimately, this model will allow clinicians to predict the outcomes of an intervention strategy to assess its utility for a patient. This has the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury.",Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes,9911475,R01DC006859,"['Acoustics', 'Area', 'Caring', 'Clinical', 'Clinical Assessment Tool', 'Code', 'Computer Simulation', 'Country', 'Custom', 'Data', 'Dysarthria', 'Ear', 'Educational Intervention', 'Evaluation', 'Genetic Transcription', 'Goals', 'Gold', 'Grant', 'Health Services Accessibility', 'Individual', 'Intervention', 'Judgment', 'Language', 'Learning', 'Loudness', 'Measures', 'Modeling', 'Motor', 'Nervous System Trauma', 'Outcome', 'Outcome Measure', 'Pathologist', 'Patient Monitoring', 'Patients', 'Perception', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Intelligibility', 'Speech Pathology', 'Speech Perception', 'Speech-Language Pathology', 'Stimulus', 'Technology', 'Theoretical model', 'Training', 'base', 'clinical practice', 'health disparity', 'improved', 'machine learning algorithm', 'nervous system disorder', 'novel', 'outcome prediction', 'predictive modeling', 'standard of care']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2019,208745,0.3830421198083381
"Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes The trained ear of the speech-language pathologist is the gold standard assessment tool for clinical practice in motor speech disorders. However, perceptual judgments are vulnerable to bias and their relationship with estimates of listener intelligibility  the final arbiter of speech goodness  is indeterminate. Interpretable, objective, and robust outcome measures that provide targets for treatment are urgently needed in order to provide more precise care and reliably monitor patient progress. Based on theoretical models of speech perception, in our previous grants we have developed a novel set of outcome measures that provide a multi- dimensional intelligibility profile (MIP) by using custom speech stimuli and a new coding strategy that allows us to capture the types of errors that listeners make when listening to dysarthric speech. This has led to a more complete intelligibility profile that codifies these errors at different levels of granularity, from global to discrete. Simultaneously, we have also developed a computational model for evaluation of dysarthric speech capable of reliably estimating a limited set of intelligibility measures directly from the speech acoustics. To date, both the outcome measures and the objective model have been evaluated on cross-sectional data only. In this renewal application, our principal goal is to evaluate specific hypotheses regarding expected changes in this multidimensional intelligibility profile as a result of different intervention instruction conditions (loud, clear, slow). A secondary goal of the proposal is to further refine our objective model to predict the complete intelligibility profile and to evaluate its ability to detect intelligibility changes within individual speakers. This is critical for clinicians who currently have no objective ways to assess the value of their interventions. With the aim of improving the standard of care through technology, the long-term goal of this proposal is to develop stand-alone objective outcome measures for dysarthria that can provide clinicians with reliable treatment targets. Such applications have the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury. Furthermore, these applications also have the potential to reduce health disparities by partially automating clinical intervention and providing easier access to these services to those in remote areas or in underdeveloped countries.  There is an urgent need in the field of speech-language pathology for objective outcome measures of speech intelligibility that provide clinicians with actionable information regarding treatment targets. This proposal seeks to leverage theoretical advances in speech intelligibility to evaluate the sensitivity of a novel multidimensional intelligibility profile that quantifies the perceptual effects of speech change. Using listener transcriptions of dysarthric speech, along with a suite of automated acoustic metrics, the predictive model uses machine-learning algorithms to learn the relationship between speech acoustics and listener percepts. Ultimately, this model will allow clinicians to predict the outcomes of an intervention strategy to assess its utility for a patient. This has the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury.",Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes,9850868,R01DC006859,"['Acoustics', 'Adopted', 'Affect', 'Area', 'Attention', 'Caring', 'Clinical', 'Clinical Assessment Tool', 'Code', 'Cognitive', 'Communication impairment', 'Complex', 'Computer Models', 'Country', 'Cues', 'Custom', 'Data', 'Dimensions', 'Disease Progression', 'Dysarthria', 'Ear', 'Educational Intervention', 'Evaluation', 'Frequencies', 'Genetic Transcription', 'Goals', 'Gold', 'Grant', 'Health Services Accessibility', 'Individual', 'Instruction', 'Intervention', 'Judgment', 'Knowledge', 'Language', 'Learning', 'Loudness', 'Measures', 'Modeling', 'Motor', 'Nervous System Trauma', 'Noise', 'Outcome', 'Outcome Measure', 'Participant', 'Pathologist', 'Patient Monitoring', 'Patients', 'Pattern', 'Perception', 'Periodicity', 'Population', 'Process', 'Research', 'Sampling', 'Severities', 'Signal Transduction', 'Source', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Intelligibility', 'Speech Pathology', 'Speech Perception', 'Speech-Language Pathology', 'Stimulus', 'Stream', 'Technology', 'Testing', 'Theoretical model', 'Time', 'Training', 'Update', 'Validation', 'Work', 'base', 'clinical practice', 'health disparity', 'improved', 'lexical', 'machine learning algorithm', 'nervous system disorder', 'novel', 'optimal treatments', 'outcome prediction', 'phrases', 'predictive modeling', 'recruit', 'signal processing', 'speech in noise', 'standard of care', 'tool']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2020,307097,0.3837763745099469
"Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes The trained ear of the speech-language pathologist is the gold standard assessment tool for clinical practice in motor speech disorders. However, perceptual judgments are vulnerable to bias and their relationship with estimates of listener intelligibility  the final arbiter of speech goodness  is indeterminate. Interpretable, objective, and robust outcome measures that provide targets for treatment are urgently needed in order to provide more precise care and reliably monitor patient progress. Based on theoretical models of speech perception, in our previous grants we have developed a novel set of outcome measures that provide a multi- dimensional intelligibility profile (MIP) by using custom speech stimuli and a new coding strategy that allows us to capture the types of errors that listeners make when listening to dysarthric speech. This has led to a more complete intelligibility profile that codifies these errors at different levels of granularity, from global to discrete. Simultaneously, we have also developed a computational model for evaluation of dysarthric speech capable of reliably estimating a limited set of intelligibility measures directly from the speech acoustics. To date, both the outcome measures and the objective model have been evaluated on cross-sectional data only. In this renewal application, our principal goal is to evaluate specific hypotheses regarding expected changes in this multidimensional intelligibility profile as a result of different intervention instruction conditions (loud, clear, slow). A secondary goal of the proposal is to further refine our objective model to predict the complete intelligibility profile and to evaluate its ability to detect intelligibility changes within individual speakers. This is critical for clinicians who currently have no objective ways to assess the value of their interventions. With the aim of improving the standard of care through technology, the long-term goal of this proposal is to develop stand-alone objective outcome measures for dysarthria that can provide clinicians with reliable treatment targets. Such applications have the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury. Furthermore, these applications also have the potential to reduce health disparities by partially automating clinical intervention and providing easier access to these services to those in remote areas or in underdeveloped countries.  There is an urgent need in the field of speech-language pathology for objective outcome measures of speech intelligibility that provide clinicians with actionable information regarding treatment targets. This proposal seeks to leverage theoretical advances in speech intelligibility to evaluate the sensitivity of a novel multidimensional intelligibility profile that quantifies the perceptual effects of speech change. Using listener transcriptions of dysarthric speech, along with a suite of automated acoustic metrics, the predictive model uses machine-learning algorithms to learn the relationship between speech acoustics and listener percepts. Ultimately, this model will allow clinicians to predict the outcomes of an intervention strategy to assess its utility for a patient. This has the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury.",Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes,9631443,R01DC006859,"['Acoustics', 'Adopted', 'Affect', 'Area', 'Attention', 'Caring', 'Clinical', 'Clinical Assessment Tool', 'Code', 'Cognitive', 'Communication impairment', 'Complex', 'Computer Simulation', 'Country', 'Cues', 'Custom', 'Data', 'Dimensions', 'Disease Progression', 'Dysarthria', 'Ear', 'Educational Intervention', 'Evaluation', 'Frequencies', 'Genetic Transcription', 'Goals', 'Gold', 'Grant', 'Health Services Accessibility', 'Individual', 'Instruction', 'Intervention', 'Judgment', 'Knowledge', 'Language', 'Learning', 'Loudness', 'Measures', 'Modeling', 'Motor', 'Nervous System Trauma', 'Noise', 'Outcome', 'Outcome Measure', 'Participant', 'Pathologist', 'Patient Monitoring', 'Patients', 'Pattern', 'Perception', 'Periodicity', 'Population', 'Process', 'Research', 'Sampling', 'Severities', 'Signal Transduction', 'Source', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Intelligibility', 'Speech Pathology', 'Speech Perception', 'Speech-Language Pathology', 'Stimulus', 'Stream', 'Technology', 'Testing', 'Theoretical model', 'Time', 'Training', 'Update', 'Validation', 'Work', 'base', 'clinical practice', 'health disparity', 'improved', 'lexical', 'machine learning algorithm', 'nervous system disorder', 'novel', 'optimal treatments', 'outcome prediction', 'phrases', 'predictive modeling', 'recruit', 'signal processing', 'speech in noise', 'standard of care', 'tool']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2019,308669,0.3837763745099469
"Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes The trained ear of the speech-language pathologist is the gold standard assessment tool for clinical practice in motor speech disorders. However, perceptual judgments are vulnerable to bias and their relationship with estimates of listener intelligibility  the final arbiter of speech goodness  is indeterminate. Interpretable, objective, and robust outcome measures that provide targets for treatment are urgently needed in order to provide more precise care and reliably monitor patient progress. Based on theoretical models of speech perception, in our previous grants we have developed a novel set of outcome measures that provide a multi- dimensional intelligibility profile (MIP) by using custom speech stimuli and a new coding strategy that allows us to capture the types of errors that listeners make when listening to dysarthric speech. This has led to a more complete intelligibility profile that codifies these errors at different levels of granularity, from global to discrete. Simultaneously, we have also developed a computational model for evaluation of dysarthric speech capable of reliably estimating a limited set of intelligibility measures directly from the speech acoustics. To date, both the outcome measures and the objective model have been evaluated on cross-sectional data only. In this renewal application, our principal goal is to evaluate specific hypotheses regarding expected changes in this multidimensional intelligibility profile as a result of different intervention instruction conditions (loud, clear, slow). A secondary goal of the proposal is to further refine our objective model to predict the complete intelligibility profile and to evaluate its ability to detect intelligibility changes within individual speakers. This is critical for clinicians who currently have no objective ways to assess the value of their interventions. With the aim of improving the standard of care through technology, the long-term goal of this proposal is to develop stand-alone objective outcome measures for dysarthria that can provide clinicians with reliable treatment targets. Such applications have the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury. Furthermore, these applications also have the potential to reduce health disparities by partially automating clinical intervention and providing easier access to these services to those in remote areas or in underdeveloped countries.  There is an urgent need in the field of speech-language pathology for objective outcome measures of speech intelligibility that provide clinicians with actionable information regarding treatment targets. This proposal seeks to leverage theoretical advances in speech intelligibility to evaluate the sensitivity of a novel multidimensional intelligibility profile that quantifies the perceptual effects of speech change. Using listener transcriptions of dysarthric speech, along with a suite of automated acoustic metrics, the predictive model uses machine-learning algorithms to learn the relationship between speech acoustics and listener percepts. Ultimately, this model will allow clinicians to predict the outcomes of an intervention strategy to assess its utility for a patient. This has the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury.",Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes,9432492,R01DC006859,"['Acoustics', 'Adopted', 'Affect', 'Algorithms', 'Area', 'Attention', 'Caring', 'Clinical', 'Clinical Assessment Tool', 'Code', 'Cognitive', 'Communication impairment', 'Complex', 'Computer Simulation', 'Country', 'Cues', 'Custom', 'Data', 'Dimensions', 'Disease Progression', 'Dysarthria', 'Ear', 'Educational Intervention', 'Evaluation', 'Frequencies', 'Genetic Transcription', 'Goals', 'Gold', 'Grant', 'Health Services Accessibility', 'Individual', 'Instruction', 'Intervention', 'Judgment', 'Knowledge', 'Language', 'Learning', 'Loudness', 'Machine Learning', 'Measures', 'Modeling', 'Motor', 'Nervous System Trauma', 'Noise', 'Outcome', 'Outcome Measure', 'Participant', 'Pathologist', 'Patient Monitoring', 'Patients', 'Pattern', 'Perception', 'Periodicity', 'Population', 'Process', 'Research', 'Sampling', 'Severities', 'Signal Transduction', 'Source', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Intelligibility', 'Speech Pathology', 'Speech Perception', 'Speech-Language Pathology', 'Stimulus', 'Stream', 'Technology', 'Testing', 'Theoretical model', 'Time', 'Training', 'Update', 'Validation', 'Work', 'base', 'clinical practice', 'health disparity', 'improved', 'lexical', 'nervous system disorder', 'novel', 'optimal treatments', 'outcome prediction', 'phrases', 'predictive modeling', 'recruit', 'signal processing', 'standard of care', 'tool']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2018,310124,0.3837763745099469
"Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes The trained ear of the speech-language pathologist is the gold standard assessment tool for clinical practice in motor speech disorders. However, perceptual judgments are vulnerable to bias and their relationship with estimates of listener intelligibility  the final arbiter of speech goodness  is indeterminate. Interpretable, objective, and robust outcome measures that provide targets for treatment are urgently needed in order to provide more precise care and reliably monitor patient progress. Based on theoretical models of speech perception, in our previous grants we have developed a novel set of outcome measures that provide a multi- dimensional intelligibility profile (MIP) by using custom speech stimuli and a new coding strategy that allows us to capture the types of errors that listeners make when listening to dysarthric speech. This has led to a more complete intelligibility profile that codifies these errors at different levels of granularity, from global to discrete. Simultaneously, we have also developed a computational model for evaluation of dysarthric speech capable of reliably estimating a limited set of intelligibility measures directly from the speech acoustics. To date, both the outcome measures and the objective model have been evaluated on cross-sectional data only. In this renewal application, our principal goal is to evaluate specific hypotheses regarding expected changes in this multidimensional intelligibility profile as a result of different intervention instruction conditions (loud, clear, slow). A secondary goal of the proposal is to further refine our objective model to predict the complete intelligibility profile and to evaluate its ability to detect intelligibility changes within individual speakers. This is critical for clinicians who currently have no objective ways to assess the value of their interventions. With the aim of improving the standard of care through technology, the long-term goal of this proposal is to develop stand-alone objective outcome measures for dysarthria that can provide clinicians with reliable treatment targets. Such applications have the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury. Furthermore, these applications also have the potential to reduce health disparities by partially automating clinical intervention and providing easier access to these services to those in remote areas or in underdeveloped countries.  There is an urgent need in the field of speech-language pathology for objective outcome measures of speech intelligibility that provide clinicians with actionable information regarding treatment targets. This proposal seeks to leverage theoretical advances in speech intelligibility to evaluate the sensitivity of a novel multidimensional intelligibility profile that quantifies the perceptual effects of speech change. Using listener transcriptions of dysarthric speech, along with a suite of automated acoustic metrics, the predictive model uses machine-learning algorithms to learn the relationship between speech acoustics and listener percepts. Ultimately, this model will allow clinicians to predict the outcomes of an intervention strategy to assess its utility for a patient. This has the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury.",Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes,9312085,R01DC006859,"['Acoustics', 'Adopted', 'Affect', 'Algorithms', 'Area', 'Attention', 'Caring', 'Clinical', 'Clinical Assessment Tool', 'Code', 'Cognitive', 'Communication impairment', 'Complex', 'Computer Simulation', 'Country', 'Cues', 'Custom', 'Data', 'Dimensions', 'Disease Progression', 'Dysarthria', 'Ear', 'Educational Intervention', 'Evaluation', 'Frequencies', 'Genetic Transcription', 'Goals', 'Gold', 'Grant', 'Health Services Accessibility', 'Individual', 'Instruction', 'Intervention', 'Judgment', 'Knowledge', 'Language', 'Learning', 'Loudness', 'Machine Learning', 'Measures', 'Modeling', 'Motor', 'Nervous System Trauma', 'Noise', 'Outcome', 'Outcome Measure', 'Participant', 'Pathologist', 'Patient Monitoring', 'Patients', 'Pattern', 'Perception', 'Periodicity', 'Population', 'Process', 'Recruitment Activity', 'Research', 'Sampling', 'Severities', 'Signal Transduction', 'Source', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Intelligibility', 'Speech Pathology', 'Speech Perception', 'Speech-Language Pathology', 'Stimulus', 'Stream', 'Technology', 'Testing', 'Theoretical model', 'Time', 'Training', 'Update', 'Validation', 'Work', 'base', 'clinical practice', 'health disparity', 'improved', 'lexical', 'nervous system disorder', 'novel', 'outcome prediction', 'phrases', 'predictive modeling', 'signal processing', 'standard of care', 'tool']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2017,311471,0.3837763745099469
"Perception of Dysarthric Speech DESCRIPTION (provided by applicant): Reduced intelligibility is at the heart of the communication disorder associated with the dysarthrias and other speech production deficits, undermining quality of life. This research program aims to develop a comprehensive model of intelligibility deficits that offers an explanation for communication failure and success, and thereby identifies targets for remediation, as well as dependent variables that will serve as outcome measures. We have shown that when listeners encounter speech that is difficult to understand, they turn their attention to prosody to help them decide where words begin and end. However this strategy for lexical segmentation becomes challenged when the prosodic information itself is degraded, as in the dysarthrias. Further, the nature of the prosodic degradation predicts the ways in which word boundary identification is impaired. The differences in perceptual error patterns resulting from speech produced by two equally intelligible speakers are predictable and provide information both about the underlying motor deficit and the perceptual representations and strategies of the listener. The present proposal defines this relationship through the development of sensitive dependent variables that predict listener performance patterns and production characteristics. Specifically, we will refine a set of acoustic measures and establish their predictive relationship to perceptual performance (intelligibility and error patterns), using speakers with dysarthria and healthy controls. These automated acoustic measures include measures of based on the low-frequency modulations of the amplitude envelope and measures of fundamental frequency and average spectral variability. This set of acoustic measures will be used to classify speakers by traditional dysarthric subtypes as well as by groupings based on a perceptual-outcome clustering that will be developed using the error patterns obtained from listeners' transcription of each speaker's samples. The model will be tested and refined on a new more diverse group of speakers with intelligibility deficits. The causality of the relationships between acoustics and perception uncovered by these analyses will be tested through perceptual experiments using speech samples that are digitally manipulated to match the prosodic patterns that are associated with particular error types. The proposed project holds promise for immediate clinical impact by providing both sensitive and meaningful outcome measures and an overarching theoretical framework in which to interpret them.     PUBLIC HEALTH RELEVANCE: The overall goal of the current project is to develop a theoretically-derived model of intelligibility deficits that has immediate clinical impact by identifying targets for remediation and offering dependent variables that may be used to predict perceptual outcome and track changes in speech due to intervention or disease progression. By defining a set of objective measures that map to meaningful aspects of speech understanding, these dependent variables can be applied to any communication disorder for which intelligibility is reduced. Project Narrative/Relevance  The overall goal of the current project is to develop a theoretically-derived model of  intelligibility deficits that has immediate clinical impact by identifying targets for remediation  and offering dependent variables that may be used to predict perceptual outcome and track  changes in speech due to intervention or disease progression. By defining a set of objective  measures that map to meaningful aspects of speech understanding, these dependent  variables can be applied to any communication disorder for which intelligibility is reduced.",Perception of Dysarthric Speech,8721911,R01DC006859,"['Accounting', 'Acoustics', 'Age', 'Attention', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Cognitive', 'Collection', 'Communication', 'Communication Disability', 'Communication impairment', 'Comprehension', 'Control Groups', 'Cues', 'Data', 'Databases', 'Development', 'Digital Signal Processing', 'Disadvantaged', 'Discriminant Analysis', 'Disease', 'Disease Progression', 'Dysarthria', 'Environment', 'Etiology', 'Event', 'Failure', 'Frequencies', 'Funding', 'Gender', 'Genetic Transcription', 'Goals', 'Grouping', 'Hearing', 'Heart', 'Intervention', 'Lead', 'Learning', 'Maps', 'Measures', 'Mediating', 'Metric', 'Modeling', 'Nature', 'Outcome', 'Outcome Measure', 'Pattern', 'Perception', 'Perceptual learning', 'Performance', 'Periodicity', 'Population', 'Principal Component Analysis', 'Production', 'Quality of life', 'Reading', 'Regression Analysis', 'Research', 'Sampling', 'Scheme', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Speech Intelligibility', 'Speech Perception', 'Stimulus', 'System', 'Techniques', 'Testing', 'Time', 'Transcend', 'Uncertainty', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'clinical practice', 'design', 'digital', 'expectation', 'experience', 'flexibility', 'lexical', 'motor deficit', 'nervous system disorder', 'novel', 'programs', 'public health relevance', 'remediation', 'research study', 'success', 'theories']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2014,302331,0.21517235546230642
"Perception of Dysarthric Speech DESCRIPTION (provided by applicant): Reduced intelligibility is at the heart of the communication disorder associated with the dysarthrias and other speech production deficits, undermining quality of life. This research program aims to develop a comprehensive model of intelligibility deficits that offers an explanation for communication failure and success, and thereby identifies targets for remediation, as well as dependent variables that will serve as outcome measures. We have shown that when listeners encounter speech that is difficult to understand, they turn their attention to prosody to help them decide where words begin and end. However this strategy for lexical segmentation becomes challenged when the prosodic information itself is degraded, as in the dysarthrias. Further, the nature of the prosodic degradation predicts the ways in which word boundary identification is impaired. The differences in perceptual error patterns resulting from speech produced by two equally intelligible speakers are predictable and provide information both about the underlying motor deficit and the perceptual representations and strategies of the listener. The present proposal defines this relationship through the development of sensitive dependent variables that predict listener performance patterns and production characteristics. Specifically, we will refine a set of acoustic measures and establish their predictive relationship to perceptual performance (intelligibility and error patterns), using speakers with dysarthria and healthy controls. These automated acoustic measures include measures of based on the low-frequency modulations of the amplitude envelope and measures of fundamental frequency and average spectral variability. This set of acoustic measures will be used to classify speakers by traditional dysarthric subtypes as well as by groupings based on a perceptual-outcome clustering that will be developed using the error patterns obtained from listeners' transcription of each speaker's samples. The model will be tested and refined on a new more diverse group of speakers with intelligibility deficits. The causality of the relationships between acoustics and perception uncovered by these analyses will be tested through perceptual experiments using speech samples that are digitally manipulated to match the prosodic patterns that are associated with particular error types. The proposed project holds promise for immediate clinical impact by providing both sensitive and meaningful outcome measures and an overarching theoretical framework in which to interpret them.    PUBLIC HEALTH RELEVANCE: The overall goal of the current project is to develop a theoretically-derived model of intelligibility deficits that has immediate clinical impact by identifying targets for remediation and offering dependent variables that may be used to predict perceptual outcome and track changes in speech due to intervention or disease progression. By defining a set of objective measures that map to meaningful aspects of speech understanding, these dependent variables can be applied to any communication disorder for which intelligibility is reduced. Project Narrative/Relevance  The overall goal of the current project is to develop a theoretically-derived model of  intelligibility deficits that has immediate clinical impact by identifying targets for remediation  and offering dependent variables that may be used to predict perceptual outcome and track  changes in speech due to intervention or disease progression. By defining a set of objective  measures that map to meaningful aspects of speech understanding, these dependent  variables can be applied to any communication disorder for which intelligibility is reduced.",Perception of Dysarthric Speech,8497640,R01DC006859,"['Accounting', 'Acoustics', 'Age', 'Attention', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Cognitive', 'Collection', 'Communication', 'Communication Disability', 'Communication impairment', 'Comprehension', 'Control Groups', 'Cues', 'Data', 'Databases', 'Development', 'Digital Signal Processing', 'Disadvantaged', 'Discriminant Analysis', 'Disease', 'Disease Progression', 'Dysarthria', 'Environment', 'Etiology', 'Event', 'Failure', 'Frequencies', 'Funding', 'Gender', 'Genetic Transcription', 'Goals', 'Grouping', 'Health', 'Hearing', 'Heart', 'Intervention', 'Lead', 'Learning', 'Maps', 'Measures', 'Mediating', 'Metric', 'Modeling', 'Nature', 'Outcome', 'Outcome Measure', 'Pattern', 'Perception', 'Perceptual learning', 'Performance', 'Periodicity', 'Population', 'Principal Component Analysis', 'Production', 'Quality of life', 'Reading', 'Regression Analysis', 'Research', 'Sampling', 'Scheme', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Speech Intelligibility', 'Speech Perception', 'Stimulus', 'System', 'Techniques', 'Testing', 'Time', 'Transcend', 'Uncertainty', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'clinical practice', 'design', 'digital', 'expectation', 'experience', 'flexibility', 'lexical', 'motor deficit', 'nervous system disorder', 'novel', 'programs', 'remediation', 'research study', 'success', 'theories']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2013,287848,0.21517235546230642
"Perception of Dysarthric Speech DESCRIPTION (provided by applicant): Reduced intelligibility is at the heart of the communication disorder associated with the dysarthrias and other speech production deficits, undermining quality of life. This research program aims to develop a comprehensive model of intelligibility deficits that offers an explanation for communication failure and success, and thereby identifies targets for remediation, as well as dependent variables that will serve as outcome measures. We have shown that when listeners encounter speech that is difficult to understand, they turn their attention to prosody to help them decide where words begin and end. However this strategy for lexical segmentation becomes challenged when the prosodic information itself is degraded, as in the dysarthrias. Further, the nature of the prosodic degradation predicts the ways in which word boundary identification is impaired. The differences in perceptual error patterns resulting from speech produced by two equally intelligible speakers are predictable and provide information both about the underlying motor deficit and the perceptual representations and strategies of the listener. The present proposal defines this relationship through the development of sensitive dependent variables that predict listener performance patterns and production characteristics. Specifically, we will refine a set of acoustic measures and establish their predictive relationship to perceptual performance (intelligibility and error patterns), using speakers with dysarthria and healthy controls. These automated acoustic measures include measures of based on the low-frequency modulations of the amplitude envelope and measures of fundamental frequency and average spectral variability. This set of acoustic measures will be used to classify speakers by traditional dysarthric subtypes as well as by groupings based on a perceptual-outcome clustering that will be developed using the error patterns obtained from listeners' transcription of each speaker's samples. The model will be tested and refined on a new more diverse group of speakers with intelligibility deficits. The causality of the relationships between acoustics and perception uncovered by these analyses will be tested through perceptual experiments using speech samples that are digitally manipulated to match the prosodic patterns that are associated with particular error types. The proposed project holds promise for immediate clinical impact by providing both sensitive and meaningful outcome measures and an overarching theoretical framework in which to interpret them.    PUBLIC HEALTH RELEVANCE: The overall goal of the current project is to develop a theoretically-derived model of intelligibility deficits that has immediate clinical impact by identifying targets for remediation and offering dependent variables that may be used to predict perceptual outcome and track changes in speech due to intervention or disease progression. By defining a set of objective measures that map to meaningful aspects of speech understanding, these dependent variables can be applied to any communication disorder for which intelligibility is reduced. Project Narrative/Relevance  The overall goal of the current project is to develop a theoretically-derived model of  intelligibility deficits that has immediate clinical impact by identifying targets for remediation  and offering dependent variables that may be used to predict perceptual outcome and track  changes in speech due to intervention or disease progression. By defining a set of objective  measures that map to meaningful aspects of speech understanding, these dependent  variables can be applied to any communication disorder for which intelligibility is reduced.",Perception of Dysarthric Speech,8284444,R01DC006859,"['Accounting', 'Acoustics', 'Age', 'Attention', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Cognitive', 'Collection', 'Communication', 'Communication Disability', 'Communication impairment', 'Comprehension', 'Control Groups', 'Cues', 'Data', 'Databases', 'Development', 'Digital Signal Processing', 'Disadvantaged', 'Discriminant Analysis', 'Disease', 'Disease Progression', 'Dysarthria', 'Environment', 'Etiology', 'Event', 'Failure', 'Frequencies', 'Funding', 'Gender', 'Genetic Transcription', 'Goals', 'Grouping', 'Health', 'Hearing', 'Heart', 'Intervention', 'Lead', 'Learning', 'Maps', 'Measures', 'Mediating', 'Metric', 'Modeling', 'Nature', 'Outcome', 'Outcome Measure', 'Pattern', 'Perception', 'Perceptual learning', 'Performance', 'Periodicity', 'Population', 'Principal Component Analysis', 'Production', 'Quality of life', 'Reading', 'Regression Analysis', 'Research', 'Sampling', 'Scheme', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Speech Intelligibility', 'Speech Perception', 'Stimulus', 'System', 'Techniques', 'Testing', 'Time', 'Transcend', 'Uncertainty', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'clinical practice', 'design', 'digital', 'expectation', 'experience', 'flexibility', 'lexical', 'motor deficit', 'nervous system disorder', 'novel', 'programs', 'remediation', 'research study', 'success', 'theories']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2012,346751,0.21517235546230642
"Perception of Dysarthric Speech    DESCRIPTION (provided by applicant): Reduced intelligibility is at the heart of the communication disorder associated with the dysarthrias and other speech production deficits, undermining quality of life. This research program aims to develop a comprehensive model of intelligibility deficits that offers an explanation for communication failure and success, and thereby identifies targets for remediation, as well as dependent variables that will serve as outcome measures. We have shown that when listeners encounter speech that is difficult to understand, they turn their attention to prosody to help them decide where words begin and end. However this strategy for lexical segmentation becomes challenged when the prosodic information itself is degraded, as in the dysarthrias. Further, the nature of the prosodic degradation predicts the ways in which word boundary identification is impaired. The differences in perceptual error patterns resulting from speech produced by two equally intelligible speakers are predictable and provide information both about the underlying motor deficit and the perceptual representations and strategies of the listener. The present proposal defines this relationship through the development of sensitive dependent variables that predict listener performance patterns and production characteristics. Specifically, we will refine a set of acoustic measures and establish their predictive relationship to perceptual performance (intelligibility and error patterns), using speakers with dysarthria and healthy controls. These automated acoustic measures include measures of based on the low-frequency modulations of the amplitude envelope and measures of fundamental frequency and average spectral variability. This set of acoustic measures will be used to classify speakers by traditional dysarthric subtypes as well as by groupings based on a perceptual-outcome clustering that will be developed using the error patterns obtained from listeners' transcription of each speaker's samples. The model will be tested and refined on a new more diverse group of speakers with intelligibility deficits. The causality of the relationships between acoustics and perception uncovered by these analyses will be tested through perceptual experiments using speech samples that are digitally manipulated to match the prosodic patterns that are associated with particular error types. The proposed project holds promise for immediate clinical impact by providing both sensitive and meaningful outcome measures and an overarching theoretical framework in which to interpret them.      PUBLIC HEALTH RELEVANCE: The overall goal of the current project is to develop a theoretically-derived model of intelligibility deficits that has immediate clinical impact by identifying targets for remediation and offering dependent variables that may be used to predict perceptual outcome and track changes in speech due to intervention or disease progression. By defining a set of objective measures that map to meaningful aspects of speech understanding, these dependent variables can be applied to any communication disorder for which intelligibility is reduced.                Project Narrative/Relevance  The overall goal of the current project is to develop a theoretically-derived model of  intelligibility deficits that has immediate clinical impact by identifying targets for remediation  and offering dependent variables that may be used to predict perceptual outcome and track  changes in speech due to intervention or disease progression. By defining a set of objective  measures that map to meaningful aspects of speech understanding, these dependent  variables can be applied to any communication disorder for which intelligibility is reduced.",Perception of Dysarthric Speech,8354706,R01DC006859,"['Accounting', 'Acoustics', 'Age', 'Attention', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Cognitive', 'Collection', 'Communication', 'Communication Disability', 'Communication impairment', 'Comprehension', 'Control Groups', 'Cues', 'Data', 'Databases', 'Development', 'Digital Signal Processing', 'Disadvantaged', 'Discriminant Analysis', 'Disease', 'Disease Progression', 'Dysarthria', 'Environment', 'Etiology', 'Event', 'Failure', 'Frequencies', 'Funding', 'Gender', 'Genetic Transcription', 'Goals', 'Grouping', 'Hearing', 'Heart', 'Intervention', 'Lead', 'Learning', 'Maps', 'Measures', 'Mediating', 'Metric', 'Modeling', 'Nature', 'Outcome', 'Outcome Measure', 'Pattern', 'Perception', 'Perceptual learning', 'Performance', 'Periodicity', 'Population', 'Principal Component Analysis', 'Production', 'Quality of life', 'Reading', 'Regression Analysis', 'Research', 'Sampling', 'Scheme', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Speech Intelligibility', 'Speech Perception', 'Stimulus', 'System', 'Techniques', 'Testing', 'Time', 'Transcend', 'Uncertainty', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'clinical practice', 'design', 'digital', 'expectation', 'experience', 'flexibility', 'lexical', 'motor deficit', 'nervous system disorder', 'novel', 'programs', 'public health relevance', 'remediation', 'research study', 'success', 'theories']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2012,43129,0.21517235546230642
"Perception of Dysarthric Speech    DESCRIPTION (provided by applicant): Reduced intelligibility is at the heart of the communication disorder associated with the dysarthrias and other speech production deficits, undermining quality of life. This research program aims to develop a comprehensive model of intelligibility deficits that offers an explanation for communication failure and success, and thereby identifies targets for remediation, as well as dependent variables that will serve as outcome measures. We have shown that when listeners encounter speech that is difficult to understand, they turn their attention to prosody to help them decide where words begin and end. However this strategy for lexical segmentation becomes challenged when the prosodic information itself is degraded, as in the dysarthrias. Further, the nature of the prosodic degradation predicts the ways in which word boundary identification is impaired. The differences in perceptual error patterns resulting from speech produced by two equally intelligible speakers are predictable and provide information both about the underlying motor deficit and the perceptual representations and strategies of the listener. The present proposal defines this relationship through the development of sensitive dependent variables that predict listener performance patterns and production characteristics. Specifically, we will refine a set of acoustic measures and establish their predictive relationship to perceptual performance (intelligibility and error patterns), using speakers with dysarthria and healthy controls. These automated acoustic measures include measures of based on the low-frequency modulations of the amplitude envelope and measures of fundamental frequency and average spectral variability. This set of acoustic measures will be used to classify speakers by traditional dysarthric subtypes as well as by groupings based on a perceptual-outcome clustering that will be developed using the error patterns obtained from listeners' transcription of each speaker's samples. The model will be tested and refined on a new more diverse group of speakers with intelligibility deficits. The causality of the relationships between acoustics and perception uncovered by these analyses will be tested through perceptual experiments using speech samples that are digitally manipulated to match the prosodic patterns that are associated with particular error types. The proposed project holds promise for immediate clinical impact by providing both sensitive and meaningful outcome measures and an overarching theoretical framework in which to interpret them.      PUBLIC HEALTH RELEVANCE: The overall goal of the current project is to develop a theoretically-derived model of intelligibility deficits that has immediate clinical impact by identifying targets for remediation and offering dependent variables that may be used to predict perceptual outcome and track changes in speech due to intervention or disease progression. By defining a set of objective measures that map to meaningful aspects of speech understanding, these dependent variables can be applied to any communication disorder for which intelligibility is reduced.                Project Narrative/Relevance  The overall goal of the current project is to develop a theoretically-derived model of  intelligibility deficits that has immediate clinical impact by identifying targets for remediation  and offering dependent variables that may be used to predict perceptual outcome and track  changes in speech due to intervention or disease progression. By defining a set of objective  measures that map to meaningful aspects of speech understanding, these dependent  variables can be applied to any communication disorder for which intelligibility is reduced.",Perception of Dysarthric Speech,8080815,R01DC006859,"['Accounting', 'Acoustics', 'Age', 'Attention', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Cognitive', 'Collection', 'Communication', 'Communication Disability', 'Communication impairment', 'Comprehension', 'Control Groups', 'Cues', 'Data', 'Databases', 'Development', 'Digital Signal Processing', 'Disadvantaged', 'Discriminant Analysis', 'Disease', 'Disease Progression', 'Dysarthria', 'Environment', 'Etiology', 'Event', 'Failure', 'Frequencies', 'Funding', 'Gender', 'Genetic Transcription', 'Goals', 'Grouping', 'Hearing', 'Heart', 'Intervention', 'Lead', 'Learning', 'Maps', 'Measures', 'Mediating', 'Metric', 'Modeling', 'Nature', 'Outcome', 'Outcome Measure', 'Pattern', 'Perception', 'Perceptual learning', 'Performance', 'Periodicity', 'Population', 'Principal Component Analysis', 'Production', 'Quality of life', 'Reading', 'Regression Analysis', 'Research', 'Sampling', 'Scheme', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Speech Intelligibility', 'Speech Perception', 'Stimulus', 'System', 'Techniques', 'Testing', 'Time', 'Transcend', 'Uncertainty', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'clinical practice', 'design', 'digital', 'expectation', 'experience', 'flexibility', 'lexical', 'motor deficit', 'nervous system disorder', 'novel', 'programs', 'public health relevance', 'remediation', 'research study', 'success', 'theories']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2011,304231,0.21517235546230642
"Perception of Dysarthric Speech    DESCRIPTION (provided by applicant): Reduced intelligibility is at the heart of the communication disorder associated with the dysarthrias and other speech production deficits, undermining quality of life. This research program aims to develop a comprehensive model of intelligibility deficits that offers an explanation for communication failure and success, and thereby identifies targets for remediation, as well as dependent variables that will serve as outcome measures. We have shown that when listeners encounter speech that is difficult to understand, they turn their attention to prosody to help them decide where words begin and end. However this strategy for lexical segmentation becomes challenged when the prosodic information itself is degraded, as in the dysarthrias. Further, the nature of the prosodic degradation predicts the ways in which word boundary identification is impaired. The differences in perceptual error patterns resulting from speech produced by two equally intelligible speakers are predictable and provide information both about the underlying motor deficit and the perceptual representations and strategies of the listener. The present proposal defines this relationship through the development of sensitive dependent variables that predict listener performance patterns and production characteristics. Specifically, we will refine a set of acoustic measures and establish their predictive relationship to perceptual performance (intelligibility and error patterns), using speakers with dysarthria and healthy controls. These automated acoustic measures include measures of based on the low-frequency modulations of the amplitude envelope and measures of fundamental frequency and average spectral variability. This set of acoustic measures will be used to classify speakers by traditional dysarthric subtypes as well as by groupings based on a perceptual-outcome clustering that will be developed using the error patterns obtained from listeners' transcription of each speaker's samples. The model will be tested and refined on a new more diverse group of speakers with intelligibility deficits. The causality of the relationships between acoustics and perception uncovered by these analyses will be tested through perceptual experiments using speech samples that are digitally manipulated to match the prosodic patterns that are associated with particular error types. The proposed project holds promise for immediate clinical impact by providing both sensitive and meaningful outcome measures and an overarching theoretical framework in which to interpret them.      PUBLIC HEALTH RELEVANCE: The overall goal of the current project is to develop a theoretically-derived model of intelligibility deficits that has immediate clinical impact by identifying targets for remediation and offering dependent variables that may be used to predict perceptual outcome and track changes in speech due to intervention or disease progression. By defining a set of objective measures that map to meaningful aspects of speech understanding, these dependent variables can be applied to any communication disorder for which intelligibility is reduced.                Project Narrative/Relevance  The overall goal of the current project is to develop a theoretically-derived model of  intelligibility deficits that has immediate clinical impact by identifying targets for remediation  and offering dependent variables that may be used to predict perceptual outcome and track  changes in speech due to intervention or disease progression. By defining a set of objective  measures that map to meaningful aspects of speech understanding, these dependent  variables can be applied to any communication disorder for which intelligibility is reduced.",Perception of Dysarthric Speech,7985050,R01DC006859,"['Accounting', 'Acoustics', 'Age', 'Attention', 'Characteristics', 'Classification', 'Classification Scheme', 'Clinical', 'Cognitive', 'Collection', 'Communication', 'Communication Disability', 'Communication impairment', 'Comprehension', 'Control Groups', 'Cues', 'Data', 'Databases', 'Development', 'Digital Signal Processing', 'Disadvantaged', 'Disease', 'Disease Progression', 'Dysarthria', 'Environment', 'Etiology', 'Event', 'Failure', 'Frequencies', 'Funding', 'Gender', 'Genetic Transcription', 'Goals', 'Grouping', 'Hearing', 'Heart', 'Intervention', 'Lead', 'Learning', 'Maps', 'Measures', 'Mediating', 'Metric', 'Modeling', 'Nature', 'Outcome', 'Outcome Measure', 'Pattern', 'Perception', 'Perceptual learning', 'Performance', 'Periodicity', 'Population', 'Principal Component Analysis', 'Production', 'Quality of life', 'Reading', 'Regression Analysis', 'Research', 'Sampling', 'Scheme', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Speech Intelligibility', 'Speech Perception', 'Stimulus', 'System', 'Techniques', 'Testing', 'Time', 'Transcend', 'Uncertainty', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'clinical practice', 'design', 'digital', 'expectation', 'experience', 'flexibility', 'lexical', 'motor deficit', 'nervous system disorder', 'novel', 'programs', 'public health relevance', 'remediation', 'research study', 'success', 'theories']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2010,325596,0.21517235546230642
"A Direct Brain to Speech Generator for use in Humans    DESCRIPTION (provided by applicant): LONG TERM OBJECTIVES AND SPECIFIC AIMS: We aim to restore near conversational rate speech in locked-in individuals. In the Phase 1 study, neural recordings from the speech motor area in a 23 year old locked-in subject implanted with the Neurotrophic Electrode System since December 2004 have yielded neural data that have been mapped to phonemic representations and to imagined and actual movements. In the proposed work, we intend to incorporate sophisticated speech recognition algortithms, such as Artificial Neural Networks and Hidden Markov Models, in order to enable rapid pattern recognition for purposes of a real-time Speech Prosthetic development. In addition, Population Vector Analysis as performed for chronic motor studies may realize a method of converting individual neuronal firings into Phonemic or Articulatory Space for driving a Speech Synthesis Model. An additional patient will be implanted with the electrode system to expand and verify the work achieved with the initial subject. The resulting data will add much to understanding the cortical organization of speech production and accelerate the development of a speech prosthetic for locked-in individuals. The website development for data sharing purposes will be expanded and used by the collaborators and other interested parties.       RELEVANCE OF RESEARCH TO PUBLIC HEALTH: The creation of a Speech Prosthetic Device is much needed by locked-in patients suffering from ALS and brain stem stroke. The substantial research being performed in invasive neuroprosthetic studies is focused on enabling recovery of lost motor functions in paralyzed limbs or providing indirect communication through computer software. This work is helpful to locked-in patients; however, such patients have indicated that real-time spontaneous speech is a much more desirable final application. The purpose of this research is to develop a speech prosthetic device using the Neurotrophic Electrode Human Cortical Recording system with sophisticated pattern recognition models and software. The majority of neuroprosthetic studies are focused on enabling recovery of lost motor functions in paralyzed limbs or providing indirect communication through computer software, however we believe that real-time spontaneous speech would be much more desirable application to locked-in patients.          n/a",A Direct Brain to Speech Generator for use in Humans,7905400,R44DC007050,"['Area', 'Automobile Driving', 'Back', 'Biological Neural Networks', 'Brain', 'Brain Stem', 'Brain Stem Infarctions', 'Chronic', 'Classification', 'Communication', 'Computer software', 'Controlled Study', 'Data', 'Data Analyses', 'Development', 'Electrodes', 'Evaluation', 'FDA approved', 'Feeds', 'Frequencies', 'Human', 'Hybrids', 'Implant', 'Individual', 'Investigation', 'Learning', 'Left', 'Limb structure', 'Link', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Motor', 'Movement', 'Neurons', 'Output', 'Paralysed', 'Pathway interactions', 'Patients', 'Pattern', 'Pattern Recognition', 'Pattern Recognition Systems', 'Phase', 'Phase I Clinical Trials', 'Play', 'Population', 'Population Analysis', 'Process', 'Production', 'Prosthesis', 'Public Health', 'Recording of previous events', 'Recovery', 'Reporting', 'Research', 'Role', 'Signal Transduction', 'Site', 'Sorting - Cell Movement', 'Speech', 'Speech Synthesizers', 'Speed', 'Stream', 'Stroke', 'System', 'Time', 'Training', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'base', 'data acquisition', 'data sharing', 'design', 'detector', 'implantation', 'improved', 'interest', 'male', 'markov model', 'meetings', 'motor control', 'relating to nervous system', 'sound', 'speech recognition', 'tool', 'vector', 'web site']",NIDCD,"NEURAL SIGNALS, INC.",R44,2009,74630,0.3453263824327876
"A Direct Brain to Speech Generator for use in Humans    DESCRIPTION (provided by applicant): LONG TERM OBJECTIVES AND SPECIFIC AIMS: We aim to restore near conversational rate speech in locked-in individuals. In the Phase 1 study, neural recordings from the speech motor area in a 23 year old locked-in subject implanted with the Neurotrophic Electrode System since December 2004 have yielded neural data that have been mapped to phonemic representations and to imagined and actual movements. In the proposed work, we intend to incorporate sophisticated speech recognition algortithms, such as Artificial Neural Networks and Hidden Markov Models, in order to enable rapid pattern recognition for purposes of a real-time Speech Prosthetic development. In addition, Population Vector Analysis as performed for chronic motor studies may realize a method of converting individual neuronal firings into Phonemic or Articulatory Space for driving a Speech Synthesis Model. An additional patient will be implanted with the electrode system to expand and verify the work achieved with the initial subject. The resulting data will add much to understanding the cortical organization of speech production and accelerate the development of a speech prosthetic for locked-in individuals. The website development for data sharing purposes will be expanded and used by the collaborators and other interested parties.       RELEVANCE OF RESEARCH TO PUBLIC HEALTH: The creation of a Speech Prosthetic Device is much needed by locked-in patients suffering from ALS and brain stem stroke. The substantial research being performed in invasive neuroprosthetic studies is focused on enabling recovery of lost motor functions in paralyzed limbs or providing indirect communication through computer software. This work is helpful to locked-in patients; however, such patients have indicated that real-time spontaneous speech is a much more desirable final application. The purpose of this research is to develop a speech prosthetic device using the Neurotrophic Electrode Human Cortical Recording system with sophisticated pattern recognition models and software. The majority of neuroprosthetic studies are focused on enabling recovery of lost motor functions in paralyzed limbs or providing indirect communication through computer software, however we believe that real-time spontaneous speech would be much more desirable application to locked-in patients.          n/a",A Direct Brain to Speech Generator for use in Humans,7458646,R44DC007050,"['Amyotrophic Lateral Sclerosis', 'Area', 'Automobile Driving', 'Back', 'Biological Neural Networks', 'Brain', 'Brain Stem', 'Brain Stem Infarctions', 'Chronic', 'Class', 'Classification', 'Communication', 'Computer software', 'Condition', 'Controlled Study', 'Data', 'Data Analyses', 'Development', 'Electrodes', 'Evaluation', 'Feeds', 'Fire - disasters', 'Frequencies', 'Human', 'Hybrids', 'Implant', 'Individual', 'Invasive', 'Investigation', 'Learning', 'Left', 'Limb structure', 'Link', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Motor', 'Movement', 'Neurons', 'Numbers', 'Output', 'Paralysed', 'Pathway interactions', 'Patients', 'Pattern', 'Pattern Recognition', 'Pattern Recognition Systems', 'Phase', 'Phase I Clinical Trials', 'Play', 'Population', 'Population Analysis', 'Process', 'Production', 'Prosthesis', 'Public Health', 'Purpose', 'Rate', 'Recording of previous events', 'Recovery', 'Reporting', 'Research', 'Role', 'Signal Transduction', 'Site', 'Sorting - Cell Movement', 'Speech', 'Speech Synthesizers', 'Speed', 'Stream', 'Stroke', 'System', 'Time', 'Training', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'base', 'data acquisition', 'design', 'detector', 'implantation', 'improved', 'interest', 'male', 'markov model', 'motor control', 'relating to nervous system', 'sound', 'speech recognition', 'tool', 'vector']",NIDCD,"NEURAL SIGNALS, INC.",R44,2008,452155,0.3453263824327876
"A Direct Brain to Speech Generator for use in Humans    DESCRIPTION (provided by applicant): LONG TERM OBJECTIVES AND SPECIFIC AIMS: We aim to restore near conversational rate speech in locked-in individuals. In the Phase 1 study, neural recordings from the speech motor area in a 23 year old locked-in subject implanted with the Neurotrophic Electrode System since December 2004 have yielded neural data that have been mapped to phonemic representations and to imagined and actual movements. In the proposed work, we intend to incorporate sophisticated speech recognition algortithms, such as Artificial Neural Networks and Hidden Markov Models, in order to enable rapid pattern recognition for purposes of a real-time Speech Prosthetic development. In addition, Population Vector Analysis as performed for chronic motor studies may realize a method of converting individual neuronal firings into Phonemic or Articulatory Space for driving a Speech Synthesis Model. An additional patient will be implanted with the electrode system to expand and verify the work achieved with the initial subject. The resulting data will add much to understanding the cortical organization of speech production and accelerate the development of a speech prosthetic for locked-in individuals. The website development for data sharing purposes will be expanded and used by the collaborators and other interested parties.       RELEVANCE OF RESEARCH TO PUBLIC HEALTH: The creation of a Speech Prosthetic Device is much needed by locked-in patients suffering from ALS and brain stem stroke. The substantial research being performed in invasive neuroprosthetic studies is focused on enabling recovery of lost motor functions in paralyzed limbs or providing indirect communication through computer software. This work is helpful to locked-in patients; however, such patients have indicated that real-time spontaneous speech is a much more desirable final application. The purpose of this research is to develop a speech prosthetic device using the Neurotrophic Electrode Human Cortical Recording system with sophisticated pattern recognition models and software. The majority of neuroprosthetic studies are focused on enabling recovery of lost motor functions in paralyzed limbs or providing indirect communication through computer software, however we believe that real-time spontaneous speech would be much more desirable application to locked-in patients.          n/a",A Direct Brain to Speech Generator for use in Humans,7328424,R44DC007050,"['Amyotrophic Lateral Sclerosis', 'Area', 'Automobile Driving', 'Back', 'Biological Neural Networks', 'Brain', 'Brain Stem', 'Brain Stem Infarctions', 'Chronic', 'Class', 'Classification', 'Communication', 'Computer software', 'Condition', 'Controlled Study', 'Data', 'Data Analyses', 'Development', 'Electrodes', 'Evaluation', 'Feeds', 'Fire - disasters', 'Frequencies', 'Human', 'Hybrids', 'Implant', 'Individual', 'Invasive', 'Investigation', 'Learning', 'Left', 'Limb structure', 'Link', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Motor', 'Movement', 'Neurons', 'Numbers', 'Output', 'Paralysed', 'Pathway interactions', 'Patients', 'Pattern', 'Pattern Recognition', 'Pattern Recognition Systems', 'Phase', 'Phase I Clinical Trials', 'Play', 'Population', 'Population Analysis', 'Process', 'Production', 'Prosthesis', 'Public Health', 'Purpose', 'Rate', 'Recording of previous events', 'Recovery', 'Reporting', 'Research', 'Role', 'Signal Transduction', 'Site', 'Sorting - Cell Movement', 'Speech', 'Speech Synthesizers', 'Speed', 'Stream', 'Stroke', 'System', 'Time', 'Training', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'base', 'data acquisition', 'design', 'detector', 'implantation', 'improved', 'interest', 'male', 'markov model', 'motor control', 'relating to nervous system', 'sound', 'speech recognition', 'tool', 'vector']",NIDCD,"NEURAL SIGNALS, INC.",R44,2007,369975,0.3453263824327876
"Intonation in spontaneous English & Japanese dialogue    DESCRIPTION (provided by applicant): Spoken dialogue is 1 of the most basic forms of human language, ubiquitous across cultures. It differs in form from written language and read speech, with different vocabulary usage, different syntax, and an expanded use of visually available context. Most importantly, speakers and hearers of dialogue must rely on intonation, or the 'melody in speech.' When we talk, we manipulate the pitch, rate, phrasing, and volume of our speech. Patterns of intonation across a conversation can indicate complex discourse information not available from the words alone, such as what is already known by both speakers, what is newly introduced to 1 or both of them, which information they are finished discussing and what is still to be talked about. Although intonational structuring of discourse information is reported for numerous languages, a theory of the general cognitive mechanism underlying the universal use of intonation has yet to be established. The cross-linguistic research proposed here is crucial for the development of a general theory of intonation use in human language processing. The focus on analyses of unscripted conversational speech provides the most accurate information available about basic human language performance. Studying spontaneous speech has been considered an intractable problem, because it is hard to predict the specific words and sentence structures a speaker will use. We have piloted novel methodology that allows collection of multiple tokens of like utterances from the same speaker in varying intonational conditions. To understand how listeners respond in conversation, we use head-mounted eye-movement monitoring, an immediate, implicit measure of comprehension that allows the listener to speak and move while looking at the objects described by a conversational partner. The comparisons of English and Japanese, 2 languages that differ substantially in their syntax and intonation, test whether intonation is used differently in a language that provides melodic cues more or less reliably, and in different physical forms. Understanding how consistently intonation marks the information status of words and whether intonational cues facilitate listeners' comprehension of messages is important, not only for theories of language processing and development, but also for accurate speech identification and generation systems in artificial intelligence, and for the development of effective diagnoses and therapies for aphasic patients and others with communication loss.          n/a",Intonation in spontaneous English & Japanese dialogue,7452462,R01DC007090,"['Accounting', 'Acoustics', 'Address', 'Artificial Intelligence', 'Attention', 'Characteristics', 'Clinical Engineering', 'Cochlear Implants', 'Cognitive', 'Collection', 'Communication', 'Communication Aids for Disabled', 'Complex', 'Comprehension', 'Condition', 'Cues', 'Development', 'Devices', 'Diagnosis', 'Education', 'Eye Movements', 'Future', 'Generations', 'Goals', 'Head', 'Hearing', 'Human', 'Instruction', 'Japanese Population', 'Knowledge', 'Language', 'Lead', 'Linguistics', 'Link', 'Measures', 'Memory', 'Methodology', 'Modeling', 'Monitor', 'Names', 'Nature', 'Patients', 'Pattern', 'Performance', 'Phonetics', 'Population', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Rate', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Vocabulary', 'Writing', 'aphasic', 'base', 'clinical application', 'cognitive function', 'disability', 'efficacy research', 'improved', 'language processing', 'lexical', 'novel', 'oral communication', 'phonology', 'programs', 'research study', 'speech accuracy', 'syntax', 'theories', 'transmission process']",NIDCD,OHIO STATE UNIVERSITY,R01,2008,213837,0.2912649090064999
"Intonation in spontaneous English & Japanese dialogue    DESCRIPTION (provided by applicant): Spoken dialogue is 1 of the most basic forms of human language, ubiquitous across cultures. It differs in form from written language and read speech, with different vocabulary usage, different syntax, and an expanded use of visually available context. Most importantly, speakers and hearers of dialogue must rely on intonation, or the 'melody in speech.' When we talk, we manipulate the pitch, rate, phrasing, and volume of our speech. Patterns of intonation across a conversation can indicate complex discourse information not available from the words alone, such as what is already known by both speakers, what is newly introduced to 1 or both of them, which information they are finished discussing and what is still to be talked about. Although intonational structuring of discourse information is reported for numerous languages, a theory of the general cognitive mechanism underlying the universal use of intonation has yet to be established. The cross-linguistic research proposed here is crucial for the development of a general theory of intonation use in human language processing. The focus on analyses of unscripted conversational speech provides the most accurate information available about basic human language performance. Studying spontaneous speech has been considered an intractable problem, because it is hard to predict the specific words and sentence structures a speaker will use. We have piloted novel methodology that allows collection of multiple tokens of like utterances from the same speaker in varying intonational conditions. To understand how listeners respond in conversation, we use head-mounted eye-movement monitoring, an immediate, implicit measure of comprehension that allows the listener to speak and move while looking at the objects described by a conversational partner. The comparisons of English and Japanese, 2 languages that differ substantially in their syntax and intonation, test whether intonation is used differently in a language that provides melodic cues more or less reliably, and in different physical forms. Understanding how consistently intonation marks the information status of words and whether intonational cues facilitate listeners' comprehension of messages is important, not only for theories of language processing and development, but also for accurate speech identification and generation systems in artificial intelligence, and for the development of effective diagnoses and therapies for aphasic patients and others with communication loss.          n/a",Intonation in spontaneous English & Japanese dialogue,8100589,R01DC007090,"['Accounting', 'Acoustics', 'Address', 'Artificial Intelligence', 'Attention', 'Characteristics', 'Cochlear Implants', 'Cognitive', 'Collection', 'Communication', 'Communication Aids for Disabled', 'Complex', 'Comprehension', 'Cues', 'Development', 'Devices', 'Diagnosis', 'Education', 'Engineering', 'Eye Movements', 'Future', 'Generations', 'Goals', 'Head', 'Hearing', 'Human', 'Instruction', 'Japanese Population', 'Knowledge', 'Language', 'Lead', 'Linguistics', 'Link', 'Measures', 'Memory', 'Methodology', 'Modeling', 'Monitor', 'Names', 'Nature', 'Patients', 'Pattern', 'Performance', 'Phonetics', 'Population', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Vocabulary', 'Writing', 'aphasic', 'base', 'clinical application', 'cognitive function', 'disability', 'efficacy research', 'improved', 'language processing', 'lexical', 'novel', 'oral communication', 'phonology', 'phrases', 'programs', 'research study', 'speech accuracy', 'syntax', 'theories', 'transmission process']",NIDCD,OHIO STATE UNIVERSITY,R01,2010,211698,0.2912649090064999
"Intonation in spontaneous English & Japanese dialogue    DESCRIPTION (provided by applicant): Spoken dialogue is 1 of the most basic forms of human language, ubiquitous across cultures. It differs in form from written language and read speech, with different vocabulary usage, different syntax, and an expanded use of visually available context. Most importantly, speakers and hearers of dialogue must rely on intonation, or the 'melody in speech.' When we talk, we manipulate the pitch, rate, phrasing, and volume of our speech. Patterns of intonation across a conversation can indicate complex discourse information not available from the words alone, such as what is already known by both speakers, what is newly introduced to 1 or both of them, which information they are finished discussing and what is still to be talked about. Although intonational structuring of discourse information is reported for numerous languages, a theory of the general cognitive mechanism underlying the universal use of intonation has yet to be established. The cross-linguistic research proposed here is crucial for the development of a general theory of intonation use in human language processing. The focus on analyses of unscripted conversational speech provides the most accurate information available about basic human language performance. Studying spontaneous speech has been considered an intractable problem, because it is hard to predict the specific words and sentence structures a speaker will use. We have piloted novel methodology that allows collection of multiple tokens of like utterances from the same speaker in varying intonational conditions. To understand how listeners respond in conversation, we use head-mounted eye-movement monitoring, an immediate, implicit measure of comprehension that allows the listener to speak and move while looking at the objects described by a conversational partner. The comparisons of English and Japanese, 2 languages that differ substantially in their syntax and intonation, test whether intonation is used differently in a language that provides melodic cues more or less reliably, and in different physical forms. Understanding how consistently intonation marks the information status of words and whether intonational cues facilitate listeners' comprehension of messages is important, not only for theories of language processing and development, but also for accurate speech identification and generation systems in artificial intelligence, and for the development of effective diagnoses and therapies for aphasic patients and others with communication loss.          n/a",Intonation in spontaneous English & Japanese dialogue,7643857,R01DC007090,"['Accounting', 'Acoustics', 'Address', 'Artificial Intelligence', 'Attention', 'Characteristics', 'Cochlear Implants', 'Cognitive', 'Collection', 'Communication', 'Communication Aids for Disabled', 'Complex', 'Comprehension', 'Cues', 'Development', 'Devices', 'Diagnosis', 'Education', 'Engineering', 'Eye Movements', 'Future', 'Generations', 'Goals', 'Head', 'Hearing', 'Human', 'Instruction', 'Japanese Population', 'Knowledge', 'Language', 'Lead', 'Linguistics', 'Link', 'Measures', 'Memory', 'Methodology', 'Modeling', 'Monitor', 'Names', 'Nature', 'Patients', 'Pattern', 'Performance', 'Phonetics', 'Population', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Vocabulary', 'Writing', 'aphasic', 'base', 'clinical application', 'cognitive function', 'disability', 'efficacy research', 'improved', 'language processing', 'lexical', 'novel', 'oral communication', 'phonology', 'phrases', 'programs', 'research study', 'speech accuracy', 'syntax', 'theories', 'transmission process']",NIDCD,OHIO STATE UNIVERSITY,R01,2009,213837,0.2912649090064999
"Intonation in spontaneous English & Japanese dialogue    DESCRIPTION (provided by applicant): Spoken dialogue is 1 of the most basic forms of human language, ubiquitous across cultures. It differs in form from written language and read speech, with different vocabulary usage, different syntax, and an expanded use of visually available context. Most importantly, speakers and hearers of dialogue must rely on intonation, or the 'melody in speech.' When we talk, we manipulate the pitch, rate, phrasing, and volume of our speech. Patterns of intonation across a conversation can indicate complex discourse information not available from the words alone, such as what is already known by both speakers, what is newly introduced to 1 or both of them, which information they are finished discussing and what is still to be talked about. Although intonational structuring of discourse information is reported for numerous languages, a theory of the general cognitive mechanism underlying the universal use of intonation has yet to be established. The cross-linguistic research proposed here is crucial for the development of a general theory of intonation use in human language processing. The focus on analyses of unscripted conversational speech provides the most accurate information available about basic human language performance. Studying spontaneous speech has been considered an intractable problem, because it is hard to predict the specific words and sentence structures a speaker will use. We have piloted novel methodology that allows collection of multiple tokens of like utterances from the same speaker in varying intonational conditions. To understand how listeners respond in conversation, we use head-mounted eye-movement monitoring, an immediate, implicit measure of comprehension that allows the listener to speak and move while looking at the objects described by a conversational partner. The comparisons of English and Japanese, 2 languages that differ substantially in their syntax and intonation, test whether intonation is used differently in a language that provides melodic cues more or less reliably, and in different physical forms. Understanding how consistently intonation marks the information status of words and whether intonational cues facilitate listeners' comprehension of messages is important, not only for theories of language processing and development, but also for accurate speech identification and generation systems in artificial intelligence, and for the development of effective diagnoses and therapies for aphasic patients and others with communication loss.          n/a",Intonation in spontaneous English & Japanese dialogue,7253196,R01DC007090,"['Accounting', 'Acoustics', 'Address', 'Artificial Intelligence', 'Attention', 'Characteristics', 'Clinical Engineering', 'Cochlear Implants', 'Cognitive', 'Collection', 'Communication', 'Communication Aids for Disabled', 'Complex', 'Comprehension', 'Condition', 'Cues', 'Development', 'Devices', 'Diagnosis', 'Education', 'Eye Movements', 'Future', 'Generations', 'Goals', 'Head', 'Hearing', 'Human', 'Instruction', 'Japanese Population', 'Knowledge', 'Language', 'Lead', 'Linguistics', 'Link', 'Measures', 'Memory', 'Methodology', 'Modeling', 'Monitor', 'Names', 'Nature', 'Patients', 'Pattern', 'Performance', 'Phonetics', 'Population', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Rate', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Vocabulary', 'Writing', 'aphasic', 'base', 'clinical application', 'cognitive function', 'disability', 'efficacy research', 'improved', 'language processing', 'lexical', 'novel', 'oral communication', 'phonology', 'programs', 'research study', 'speech accuracy', 'syntax', 'theories', 'transmission process']",NIDCD,OHIO STATE UNIVERSITY,R01,2007,216654,0.2912649090064999
"Intonation in spontaneous English & Japanese dialogue    DESCRIPTION (provided by applicant): Spoken dialogue is 1 of the most basic forms of human language, ubiquitous across cultures. It differs in form from written language and read speech, with different vocabulary usage, different syntax, and an expanded use of visually available context. Most importantly, speakers and hearers of dialogue must rely on intonation, or the 'melody in speech.' When we talk, we manipulate the pitch, rate, phrasing, and volume of our speech. Patterns of intonation across a conversation can indicate complex discourse information not available from the words alone, such as what is already known by both speakers, what is newly introduced to 1 or both of them, which information they are finished discussing and what is still to be talked about. Although intonational structuring of discourse information is reported for numerous languages, a theory of the general cognitive mechanism underlying the universal use of intonation has yet to be established. The cross-linguistic research proposed here is crucial for the development of a general theory of intonation use in human language processing. The focus on analyses of unscripted conversational speech provides the most accurate information available about basic human language performance. Studying spontaneous speech has been considered an intractable problem, because it is hard to predict the specific words and sentence structures a speaker will use. We have piloted novel methodology that allows collection of multiple tokens of like utterances from the same speaker in varying intonational conditions. To understand how listeners respond in conversation, we use head-mounted eye-movement monitoring, an immediate, implicit measure of comprehension that allows the listener to speak and move while looking at the objects described by a conversational partner. The comparisons of English and Japanese, 2 languages that differ substantially in their syntax and intonation, test whether intonation is used differently in a language that provides melodic cues more or less reliably, and in different physical forms. Understanding how consistently intonation marks the information status of words and whether intonational cues facilitate listeners' comprehension of messages is important, not only for theories of language processing and development, but also for accurate speech identification and generation systems in artificial intelligence, and for the development of effective diagnoses and therapies for aphasic patients and others with communication loss.          n/a",Intonation in spontaneous English & Japanese dialogue,7144725,R01DC007090,"['clinical research', 'comprehension', 'language', 'speech']",NIDCD,OHIO STATE UNIVERSITY,R01,2006,223125,0.2912649090064999
"Dynamics of Vocal Tract Shaping     DESCRIPTION (provided by applicant): The long-term goal of this project is to wed state-of-the-art technology for imaging the vocal tract with a linguistically informed analysis of dynamic vocal tract constriction actions in order to understand the control and production of the compositional units of spoken language. We have pioneered the use of real time MRI for speech imaging to illuminate articulatory dynamics and to understand how these emerge lawfully from the combined effects of vocal tract constriction events distributed over space (subparts of the tract) and over time. This project has developed and refined a novel real time MRI acquisition ability, making possible current reconstruction rates of up to 96 frames per second, quadrupling current imaging speeds. Data show clear real- time movements of the lips, tongue, velum and epiglottis, providing exquisite information about the spatiotemporal properties of speech gestures in both the oral and pharyngeal portions of the vocal tract. The project has also developed novel noise-mitigated image-synchronized strategies to record speech in-situ during imaging, as well as image processing strategies for deriving linguistically meaningful measures from the data, demon- strating the utility of this approach for linguistic studies of speech communication in a variety of languages. Using our direct access to dynamic information on vocal tract shaping, we investigate vocal tract shaping in three-dimensions as the composition of spatiotemporally coordinated vocal tract action units. This project's specific aims go beyond the dynamic shaping of individual vowels and consonants-postures over time-to examine more complex structuring of articulation-namely, the local and global influences governing linguistic control, temporal coherence and multi-unit coordination. The advances in our technical approach enable a series of studies that leverage: (i) unprecedented high-speech imaging with dynamic rtMRI to consider the prosodic modulation of temporally rapid and temporally coherent speech units; (ii) innovative multi-plane 3D imaging capability to inform the computational identification of linguistic control regimes; and (iii) a large- scale rtMRI corpus ad concomitant machine learning advances to move toward a principled account of system-level co-variability in space and time, both within and among individuals. This symbiotic theory-driven and data-driven research strategy will yield significant innovations in understanding spoken communication. It is no exaggeration to say that the advent of real-time MRI for speech has initiated a dramatic scientific change in the nature of speech production research by allowing for models of production driven by rich quantitative articulatory data. The project is having broad impact through the free dissemination of the unique rtMRI data corpora, tools and models-already used worldwide for research and teaching-and societal out- reach through its website and lay media coverage. Understanding articulatory compositional structure and cross-linguistic potentialities also has critical translational significance impacting the assessment and remediation of speech disorders, as our collaborative work on glossectomy and apraxia has begun to demonstrate. PUBLIC HEALTH RELEVANCE: Real-time imaging of the moving vocal tract with MRI has made direct movies of speech production possible, allowing an investigation of the articulatory composition of speech in healthy adults and illuminating the articulatory dissolution and lack of coherence often found in spoken language disorders. This technology platform, coupled with a linguistically driven theoretical framework that understands speech as composed of articulatory units, provides a scientific foothold for evidence-driven assessment and remediation of speech breakdown in clinical populations, including articulatory remediation and training and deploying assistive technologies for the impaired (automatic speech recognition, machine speech synthesis), and has potential broad impact on the clinical needs of those with swallowing disorders, sleep apnea, or facing recovery of speech function after stroke or surgery. Further, because speech presents the only example of rapid, cognitively- controlled, internal movements of the body, the unique challenges of speech production imaging offer the wider biomedical imaging community traction for advances that improve temporal and spatial image resolution-advances with potential import for cardiac and other imaging.",Dynamics of Vocal Tract Shaping,9829092,R01DC007124,"['3-Dimensional', 'Acoustics', 'Adult', 'Air Movements', 'Apraxias', 'Articulation', 'Articulators', 'Beds', 'Cardiac', 'Clinical', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Coupled', 'Data', 'Deglutition Disorders', 'Development', 'Educational process of instructing', 'Engineering', 'Epiglottis structure', 'Event', 'German population', 'Gestures', 'Glossectomy', 'Goals', 'Human', 'Image', 'Imaging technology', 'Impairment', 'In Situ', 'Individual', 'International', 'Investigation', 'Knowledge', 'Language', 'Language Disorders', 'Larynx', 'Lateral', 'Linguistics', 'Lip structure', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Modeling', 'Motion', 'Movement', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Oral', 'Oropharyngeal', 'Pattern', 'Pharyngeal structure', 'Play', 'Population', 'Posture', 'Production', 'Property', 'Recovery', 'Research', 'Research Personnel', 'Resolution', 'Self-Help Devices', 'Series', 'Shapes', 'Sleep Apnea Syndromes', 'Speech', 'Speech Disorders', 'Speed', 'Structure', 'Surgical Flaps', 'System', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Tongue', 'Traction', 'Training', 'Variant', 'Work', 'automated speech recognition', 'base', 'bioimaging', 'cohesion', 'computerized tools', 'constriction', 'data tools', 'dexterity', 'high dimensionality', 'image processing', 'imaging capabilities', 'improved', 'innovation', 'instrument', 'movie', 'novel', 'outreach', 'phonology', 'post stroke', 'program dissemination', 'public health relevance', 'real-time images', 'reconstruction', 'remediation', 'shape analysis', 'sound', 'spatiotemporal', 'speech synthesis', 'technological innovation', 'theories', 'tongue apex', 'tool', 'web site']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,467515,0.3739352564220851
"Dynamics of Vocal Tract Shaping     DESCRIPTION (provided by applicant): The long-term goal of this project is to wed state-of-the-art technology for imaging the vocal tract with a linguistically informed analysis of dynamic vocal tract constriction actions in order to understand the control and production of the compositional units of spoken language. We have pioneered the use of real time MRI for speech imaging to illuminate articulatory dynamics and to understand how these emerge lawfully from the combined effects of vocal tract constriction events distributed over space (subparts of the tract) and over time. This project has developed and refined a novel real time MRI acquisition ability, making possible current reconstruction rates of up to 96 frames per second, quadrupling current imaging speeds. Data show clear real- time movements of the lips, tongue, velum and epiglottis, providing exquisite information about the spatiotemporal properties of speech gestures in both the oral and pharyngeal portions of the vocal tract. The project has also developed novel noise-mitigated image-synchronized strategies to record speech in-situ during imaging, as well as image processing strategies for deriving linguistically meaningful measures from the data, demon- strating the utility of this approach for linguistic studies of speech communication in a variety of languages. Using our direct access to dynamic information on vocal tract shaping, we investigate vocal tract shaping in three-dimensions as the composition of spatiotemporally coordinated vocal tract action units. This project's specific aims go beyond the dynamic shaping of individual vowels and consonants-postures over time-to examine more complex structuring of articulation-namely, the local and global influences governing linguistic control, temporal coherence and multi-unit coordination. The advances in our technical approach enable a series of studies that leverage: (i) unprecedented high-speech imaging with dynamic rtMRI to consider the prosodic modulation of temporally rapid and temporally coherent speech units; (ii) innovative multi-plane 3D imaging capability to inform the computational identification of linguistic control regimes; and (iii) a large- scale rtMRI corpus ad concomitant machine learning advances to move toward a principled account of system-level co-variability in space and time, both within and among individuals. This symbiotic theory-driven and data-driven research strategy will yield significant innovations in understanding spoken communication. It is no exaggeration to say that the advent of real-time MRI for speech has initiated a dramatic scientific change in the nature of speech production research by allowing for models of production driven by rich quantitative articulatory data. The project is having broad impact through the free dissemination of the unique rtMRI data corpora, tools and models-already used worldwide for research and teaching-and societal out- reach through its website and lay media coverage. Understanding articulatory compositional structure and cross-linguistic potentialities also has critical translational significance impacting the assessment and remediation of speech disorders, as our collaborative work on glossectomy and apraxia has begun to demonstrate. PUBLIC HEALTH RELEVANCE: Real-time imaging of the moving vocal tract with MRI has made direct movies of speech production possible, allowing an investigation of the articulatory composition of speech in healthy adults and illuminating the articulatory dissolution and lack of coherence often found in spoken language disorders. This technology platform, coupled with a linguistically driven theoretical framework that understands speech as composed of articulatory units, provides a scientific foothold for evidence-driven assessment and remediation of speech breakdown in clinical populations, including articulatory remediation and training and deploying assistive technologies for the impaired (automatic speech recognition, machine speech synthesis), and has potential broad impact on the clinical needs of those with swallowing disorders, sleep apnea, or facing recovery of speech function after stroke or surgery. Further, because speech presents the only example of rapid, cognitively- controlled, internal movements of the body, the unique challenges of speech production imaging offer the wider biomedical imaging community traction for advances that improve temporal and spatial image resolution-advances with potential import for cardiac and other imaging.",Dynamics of Vocal Tract Shaping,9605700,R01DC007124,"['3-Dimensional', 'Acoustics', 'Adult', 'Apraxias', 'Articulation', 'Articulators', 'Beds', 'Cardiac', 'Clinical', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Coupled', 'Data', 'Deglutition Disorders', 'Development', 'Dimensions', 'Educational process of instructing', 'Engineering', 'Epiglottis structure', 'Event', 'German population', 'Gestures', 'Glossectomy', 'Goals', 'Human', 'Image', 'Imaging technology', 'Impairment', 'In Situ', 'Individual', 'International', 'Investigation', 'Knowledge', 'Language', 'Language Disorders', 'Larynx', 'Lateral', 'Linguistics', 'Lip structure', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Modeling', 'Motion', 'Movement', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Oral', 'Oropharyngeal', 'Pattern', 'Pharyngeal structure', 'Play', 'Population', 'Posture', 'Production', 'Property', 'Recovery', 'Research', 'Research Personnel', 'Resolution', 'Self-Help Devices', 'Series', 'Shapes', 'Sleep Apnea Syndromes', 'Speech', 'Speech Disorders', 'Speed', 'Structure', 'Surgical Flaps', 'System', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Tongue', 'Traction', 'Training', 'Variant', 'Work', 'automated speech recognition', 'base', 'bioimaging', 'cohesion', 'computerized tools', 'constriction', 'dexterity', 'high dimensionality', 'image processing', 'imaging capabilities', 'improved', 'innovation', 'instrument', 'movie', 'novel', 'outreach', 'phonology', 'post stroke', 'program dissemination', 'public health relevance', 'real-time images', 'reconstruction', 'remediation', 'shape analysis', 'sound', 'spatiotemporal', 'speech synthesis', 'technological innovation', 'theories', 'tongue apex', 'tool', 'web site']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,467515,0.3739352564220851
"Dynamics of Vocal Tract Shaping     DESCRIPTION (provided by applicant): The long-term goal of this project is to wed state-of-the-art technology for imaging the vocal tract with a linguistically informed analysis of dynamic vocal tract constriction actions in order to understand the control and production of the compositional units of spoken language. We have pioneered the use of real time MRI for speech imaging to illuminate articulatory dynamics and to understand how these emerge lawfully from the combined effects of vocal tract constriction events distributed over space (subparts of the tract) and over time. This project has developed and refined a novel real time MRI acquisition ability, making possible current reconstruction rates of up to 96 frames per second, quadrupling current imaging speeds. Data show clear real- time movements of the lips, tongue, velum and epiglottis, providing exquisite information about the spatiotemporal properties of speech gestures in both the oral and pharyngeal portions of the vocal tract. The project has also developed novel noise-mitigated image-synchronized strategies to record speech in-situ during imaging, as well as image processing strategies for deriving linguistically meaningful measures from the data, demon- strating the utility of this approach for linguistic studies of speech communication in a variety of languages. Using our direct access to dynamic information on vocal tract shaping, we investigate vocal tract shaping in three-dimensions as the composition of spatiotemporally coordinated vocal tract action units. This project's specific aims go beyond the dynamic shaping of individual vowels and consonants-postures over time-to examine more complex structuring of articulation-namely, the local and global influences governing linguistic control, temporal coherence and multi-unit coordination. The advances in our technical approach enable a series of studies that leverage: (i) unprecedented high-speech imaging with dynamic rtMRI to consider the prosodic modulation of temporally rapid and temporally coherent speech units; (ii) innovative multi-plane 3D imaging capability to inform the computational identification of linguistic control regimes; and (iii) a large- scale rtMRI corpus ad concomitant machine learning advances to move toward a principled account of system-level co-variability in space and time, both within and among individuals. This symbiotic theory-driven and data-driven research strategy will yield significant innovations in understanding spoken communication. It is no exaggeration to say that the advent of real-time MRI for speech has initiated a dramatic scientific change in the nature of speech production research by allowing for models of production driven by rich quantitative articulatory data. The project is having broad impact through the free dissemination of the unique rtMRI data corpora, tools and models-already used worldwide for research and teaching-and societal out- reach through its website and lay media coverage. Understanding articulatory compositional structure and cross-linguistic potentialities also has critical translational significance impacting the assessment and remediation of speech disorders, as our collaborative work on glossectomy and apraxia has begun to demonstrate. PUBLIC HEALTH RELEVANCE: Real-time imaging of the moving vocal tract with MRI has made direct movies of speech production possible, allowing an investigation of the articulatory composition of speech in healthy adults and illuminating the articulatory dissolution and lack of coherence often found in spoken language disorders. This technology platform, coupled with a linguistically driven theoretical framework that understands speech as composed of articulatory units, provides a scientific foothold for evidence-driven assessment and remediation of speech breakdown in clinical populations, including articulatory remediation and training and deploying assistive technologies for the impaired (automatic speech recognition, machine speech synthesis), and has potential broad impact on the clinical needs of those with swallowing disorders, sleep apnea, or facing recovery of speech function after stroke or surgery. Further, because speech presents the only example of rapid, cognitively- controlled, internal movements of the body, the unique challenges of speech production imaging offer the wider biomedical imaging community traction for advances that improve temporal and spatial image resolution-advances with potential import for cardiac and other imaging.",Dynamics of Vocal Tract Shaping,9390471,R01DC007124,"['Acoustics', 'Adult', 'Apraxias', 'Articulation', 'Articulators', 'Beds', 'Cardiac', 'Clinical', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Coupled', 'Data', 'Deglutition Disorders', 'Development', 'Dimensions', 'Educational process of instructing', 'Engineering', 'Epiglottis structure', 'Event', 'German population', 'Gestures', 'Glossectomy', 'Goals', 'Human', 'Image', 'Imaging technology', 'Impairment', 'In Situ', 'Individual', 'International', 'Investigation', 'Knowledge', 'Language', 'Language Disorders', 'Larynx', 'Lateral', 'Linguistics', 'Lip structure', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Modeling', 'Motion', 'Movement', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Oral', 'Oropharyngeal', 'Pattern', 'Pharyngeal structure', 'Play', 'Population', 'Posture', 'Production', 'Property', 'Recovery', 'Research', 'Research Personnel', 'Resolution', 'Self-Help Devices', 'Series', 'Shapes', 'Sleep Apnea Syndromes', 'Speech', 'Speech Disorders', 'Speed', 'Stroke', 'Structure', 'Surgical Flaps', 'System', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Tongue', 'Traction', 'Training', 'Variant', 'Work', 'base', 'bioimaging', 'cohesion', 'computerized tools', 'constriction', 'dexterity', 'high dimensionality', 'image processing', 'imaging capabilities', 'improved', 'innovation', 'instrument', 'movie', 'novel', 'outreach', 'phonology', 'program dissemination', 'public health relevance', 'reconstruction', 'remediation', 'shape analysis', 'sound', 'spatiotemporal', 'speech recognition', 'technological innovation', 'theories', 'tongue apex', 'tool', 'web site']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,467515,0.3739352564220851
"Dynamics of Vocal Tract Shaping     DESCRIPTION (provided by applicant): The long-term goal of this project is to wed state-of-the-art technology for imaging the vocal tract with a linguistically informed analysis of dynamic vocal tract constriction actions in order to understand the control and production of the compositional units of spoken language. We have pioneered the use of real time MRI for speech imaging to illuminate articulatory dynamics and to understand how these emerge lawfully from the combined effects of vocal tract constriction events distributed over space (subparts of the tract) and over time. This project has developed and refined a novel real time MRI acquisition ability, making possible current reconstruction rates of up to 96 frames per second, quadrupling current imaging speeds. Data show clear real- time movements of the lips, tongue, velum and epiglottis, providing exquisite information about the spatiotemporal properties of speech gestures in both the oral and pharyngeal portions of the vocal tract. The project has also developed novel noise-mitigated image-synchronized strategies to record speech in-situ during imaging, as well as image processing strategies for deriving linguistically meaningful measures from the data, demon- strating the utility of this approach for linguistic studies of speech communication in a variety of languages. Using our direct access to dynamic information on vocal tract shaping, we investigate vocal tract shaping in three-dimensions as the composition of spatiotemporally coordinated vocal tract action units. This project's specific aims go beyond the dynamic shaping of individual vowels and consonants-postures over time-to examine more complex structuring of articulation-namely, the local and global influences governing linguistic control, temporal coherence and multi-unit coordination. The advances in our technical approach enable a series of studies that leverage: (i) unprecedented high-speech imaging with dynamic rtMRI to consider the prosodic modulation of temporally rapid and temporally coherent speech units; (ii) innovative multi-plane 3D imaging capability to inform the computational identification of linguistic control regimes; and (iii) a large- scale rtMRI corpus ad concomitant machine learning advances to move toward a principled account of system-level co-variability in space and time, both within and among individuals. This symbiotic theory-driven and data-driven research strategy will yield significant innovations in understanding spoken communication. It is no exaggeration to say that the advent of real-time MRI for speech has initiated a dramatic scientific change in the nature of speech production research by allowing for models of production driven by rich quantitative articulatory data. The project is having broad impact through the free dissemination of the unique rtMRI data corpora, tools and models-already used worldwide for research and teaching-and societal out- reach through its website and lay media coverage. Understanding articulatory compositional structure and cross-linguistic potentialities also has critical translational significance impacting the assessment and remediation of speech disorders, as our collaborative work on glossectomy and apraxia has begun to demonstrate. PUBLIC HEALTH RELEVANCE: Real-time imaging of the moving vocal tract with MRI has made direct movies of speech production possible, allowing an investigation of the articulatory composition of speech in healthy adults and illuminating the articulatory dissolution and lack of coherence often found in spoken language disorders. This technology platform, coupled with a linguistically driven theoretical framework that understands speech as composed of articulatory units, provides a scientific foothold for evidence-driven assessment and remediation of speech breakdown in clinical populations, including articulatory remediation and training and deploying assistive technologies for the impaired (automatic speech recognition, machine speech synthesis), and has potential broad impact on the clinical needs of those with swallowing disorders, sleep apnea, or facing recovery of speech function after stroke or surgery. Further, because speech presents the only example of rapid, cognitively- controlled, internal movements of the body, the unique challenges of speech production imaging offer the wider biomedical imaging community traction for advances that improve temporal and spatial image resolution-advances with potential import for cardiac and other imaging.",Dynamics of Vocal Tract Shaping,9177754,R01DC007124,"['Acoustics', 'Adult', 'Apraxias', 'Articulation', 'Articulators', 'Beds', 'Cardiac', 'Clinical', 'Communication', 'Communities', 'Complex', 'Coupled', 'Data', 'Deglutition Disorders', 'Development', 'Dimensions', 'Educational process of instructing', 'Engineering', 'Epiglottis structure', 'Event', 'German population', 'Gestures', 'Glossectomy', 'Goals', 'Human', 'Image', 'Imaging technology', 'Impairment', 'In Situ', 'Individual', 'International', 'Investigation', 'Knowledge', 'Language', 'Language Disorders', 'Larynx', 'Lateral', 'Linguistics', 'Lip structure', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Modeling', 'Motion', 'Movement', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Oral', 'Oropharyngeal', 'Pattern', 'Pharyngeal structure', 'Play', 'Population', 'Posture', 'Production', 'Property', 'Recovery', 'Research', 'Research Personnel', 'Resolution', 'Self-Help Devices', 'Series', 'Shapes', 'Sleep Apnea Syndromes', 'Speech', 'Speech Disorders', 'Speed', 'Stroke', 'Structure', 'Surgical Flaps', 'System', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Tongue', 'Traction', 'Training', 'Variant', 'Work', 'base', 'bioimaging', 'cognitive control', 'cohesion', 'computerized tools', 'constriction', 'dexterity', 'high dimensionality', 'image processing', 'imaging capabilities', 'improved', 'innovation', 'instrument', 'movie', 'novel', 'outreach', 'phonology', 'program dissemination', 'public health relevance', 'reconstruction', 'remediation', 'shape analysis', 'sound', 'spatiotemporal', 'speech recognition', 'technological innovation', 'theories', 'tongue apex', 'tool', 'web site']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,433810,0.3739352564220851
"Dynamics of Vocal Tract Shaping     DESCRIPTION (provided by applicant): The long-term goal of this project is to wed state-of-the-art technology for imaging the vocal tract with a linguistically informed analysis of dynamic vocal tract constriction actions in order to understand the control and production of the compositional units of spoken language. We have pioneered the use of real time MRI for speech imaging to illuminate articulatory dynamics and to understand how these emerge lawfully from the combined effects of vocal tract constriction events distributed over space (subparts of the tract) and over time. This project has developed and refined a novel real time MRI acquisition ability, making possible current reconstruction rates of up to 96 frames per second, quadrupling current imaging speeds. Data show clear real- time movements of the lips, tongue, velum and epiglottis, providing exquisite information about the spatiotemporal properties of speech gestures in both the oral and pharyngeal portions of the vocal tract. The project has also developed novel noise-mitigated image-synchronized strategies to record speech in-situ during imaging, as well as image processing strategies for deriving linguistically meaningful measures from the data, demon- strating the utility of this approach for linguistic studies of speech communication in a variety of languages. Using our direct access to dynamic information on vocal tract shaping, we investigate vocal tract shaping in three-dimensions as the composition of spatiotemporally coordinated vocal tract action units. This project's specific aims go beyond the dynamic shaping of individual vowels and consonants-postures over time-to examine more complex structuring of articulation-namely, the local and global influences governing linguistic control, temporal coherence and multi-unit coordination. The advances in our technical approach enable a series of studies that leverage: (i) unprecedented high-speech imaging with dynamic rtMRI to consider the prosodic modulation of temporally rapid and temporally coherent speech units; (ii) innovative multi-plane 3D imaging capability to inform the computational identification of linguistic control regimes; and (iii) a large- scale rtMRI corpus ad concomitant machine learning advances to move toward a principled account of system-level co-variability in space and time, both within and among individuals. This symbiotic theory-driven and data-driven research strategy will yield significant innovations in understanding spoken communication. It is no exaggeration to say that the advent of real-time MRI for speech has initiated a dramatic scientific change in the nature of speech production research by allowing for models of production driven by rich quantitative articulatory data. The project is having broad impact through the free dissemination of the unique rtMRI data corpora, tools and models-already used worldwide for research and teaching-and societal out- reach through its website and lay media coverage. Understanding articulatory compositional structure and cross-linguistic potentialities also has critical translational significance impacting the assessment and remediation of speech disorders, as our collaborative work on glossectomy and apraxia has begun to demonstrate.         PUBLIC HEALTH RELEVANCE: Real-time imaging of the moving vocal tract with MRI has made direct movies of speech production possible, allowing an investigation of the articulatory composition of speech in healthy adults and illuminating the articulatory dissolution and lack of coherence often found in spoken language disorders. This technology platform, coupled with a linguistically driven theoretical framework that understands speech as composed of articulatory units, provides a scientific foothold for evidence-driven assessment and remediation of speech breakdown in clinical populations, including articulatory remediation and training and deploying assistive technologies for the impaired (automatic speech recognition, machine speech synthesis), and has potential broad impact on the clinical needs of those with swallowing disorders, sleep apnea, or facing recovery of speech function after stroke or surgery. Further, because speech presents the only example of rapid, cognitively- controlled, internal movements of the body, the unique challenges of speech production imaging offer the wider biomedical imaging community traction for advances that improve temporal and spatial image resolution-advances with potential import for cardiac and other imaging.            ",Dynamics of Vocal Tract Shaping,9030116,R01DC007124,"['Accounting', 'Acoustics', 'Adult', 'Apraxias', 'Articulators', 'Beds', 'Cardiac', 'Clinical', 'Communication', 'Communities', 'Complex', 'Coupled', 'Data', 'Deglutition Disorders', 'Development', 'Dimensions', 'Educational process of instructing', 'Engineering', 'Epiglottis structure', 'Event', 'German population', 'Gestures', 'Glossectomy', 'Goals', 'Human', 'Image', 'Imaging technology', 'In Situ', 'Individual', 'International', 'Investigation', 'Joints', 'Knowledge', 'Language', 'Language Disorders', 'Larynx', 'Lateral', 'Linguistics', 'Lip structure', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Modeling', 'Motion', 'Movement', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Oral', 'Oropharyngeal', 'Pattern', 'Pharyngeal structure', 'Play', 'Population', 'Posture', 'Production', 'Property', 'Recovery', 'Research', 'Research Personnel', 'Resolution', 'Self-Help Devices', 'Series', 'Shapes', 'Sleep Apnea Syndromes', 'Speech', 'Speech Disorders', 'Speed', 'Stroke', 'Structure', 'Surgical Flaps', 'System', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Tongue', 'Traction', 'Training', 'Variant', 'Work', 'base', 'bioimaging', 'computerized tools', 'constriction', 'dexterity', 'image processing', 'improved', 'innovation', 'instrument', 'internal control', 'movie', 'novel', 'outreach', 'phonology', 'program dissemination', 'public health relevance', 'reconstruction', 'remediation', 'sound', 'spatiotemporal', 'speech recognition', 'technological innovation', 'theories', 'tongue apex', 'tool', 'web site']",NIDCD,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2016,433810,0.3739352564220851
"SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the        ability to understand conversation  under difficult listening conditions,        such  as  in highly reverberant rooms  or in  gatherings where several           persons are  talking simultaneously,  affects a substantial portion  of          elderly individuals.  This impairment may vary in severity, but only in          very few cases can it be overcome  by the use  of currently available            prosthetic devices.  Attempts to alleviate  this  impairment  have been          impeded by  the fact that  neither the precise  characteristics of  the          intact  process in  the young, nor the causes  of its  breakdown in the          old, are currently well understood.                                                                                                                               The proposed research represents a continuation of work aimed at                 investigating  the ability of  both elderly and young individuals to             understand speech under  non-optimal  listening conditions,  i.e.,               perceptual separation  of a  speech  target from simultaneously ongoing          irrelevant ""noise"".  The research  has two  main  objectives: (1) to             investigate,  in elderly and  in young listeners, the  perceptual                processes (in  particular,  spatial resolution and resolution of temporal        fluctuations) which  play a role  in the separation of  simultaneously           presented relevant and irrelevant  auditory signals; and (2) to study            a group  of elderly individuals over a  five year period, in order to            detect initial or  progressive deterioration of the ability to separate          simultaneous signals and  to determine the correlates of this                    deterioration.                                                                                                                                                    These objectives  will be achieved by  testing selected groups of elderly        and  young individuals on standard  and non-standard audiological tests          as  well as  psychophysical  tests.  Spatial hearing  will be assessed           in a simulated  free field.  Multidimensional  auditory performance              profiles of subjects  will  be  defined  through  principal  component           analysis  and other  multivariate  statistical methods.                                                                                                           The  major scientific  significance of the  proposed  study is  that it          will  provide a more precise  definition of auditory temporal and                spatial processes  that allow  for the perceptual separation  of speech          and  background noise and  will also identify precise auditory processes         affected by aging.  The clinical  significance of  the study is that  it         will establish a  multidimensional data  base of auditory  capabilities          in  elderly individuals with  mild to  moderate  sensorineural  hearing          loss,  and  may identify  auditory processes which, if  impaired, will           help predict  impending deterioration of  speech understanding  under            non-optimal  listening conditions.  This  work will have an impact on a          wide-spread impairment of verbal communication in the elderly.                    n/a",SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING,6012548,R01AG007998,"['adolescence (12-20)', ' aging', ' audiometry', ' auditory discrimination', ' behavioral /social science research tag', ' binaural hearing', ' human middle age (35-64)', ' human old age (65+)', ' human subject', ' longitudinal human study', ' noise', ' perception', ' psychoacoustics', ' sensorineural hearing loss', ' space perception', ' speech', ' speech recognition', ' young adult human (21-34)']",NIA,EAST BAY INSTITUTE FOR RESEARCH AND EDUC,R01,1999,38048,0.20193431495331235
"SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the        ability to understand conversation  under difficult listening conditions,        such  as  in highly reverberant rooms  or in  gatherings where several           persons are  talking simultaneously,  affects a substantial portion  of          elderly individuals.  This impairment may vary in severity, but only in          very few cases can it be overcome  by the use  of currently available            prosthetic devices.  Attempts to alleviate  this  impairment  have been          impeded by  the fact that  neither the precise  characteristics of  the          intact  process in  the young, nor the causes  of its  breakdown in the          old, are currently well understood.                                                                                                                               The proposed research represents a continuation of work aimed at                 investigating  the ability of  both elderly and young individuals to             understand speech under  non-optimal  listening conditions,  i.e.,               perceptual separation  of a  speech  target from simultaneously ongoing          irrelevant ""noise"".  The research  has two  main  objectives: (1) to             investigate,  in elderly and  in young listeners, the  perceptual                processes (in  particular,  spatial resolution and resolution of temporal        fluctuations) which  play a role  in the separation of  simultaneously           presented relevant and irrelevant  auditory signals; and (2) to study            a group  of elderly individuals over a  five year period, in order to            detect initial or  progressive deterioration of the ability to separate          simultaneous signals and  to determine the correlates of this                    deterioration.                                                                                                                                                    These objectives  will be achieved by  testing selected groups of elderly        and  young individuals on standard  and non-standard audiological tests          as  well as  psychophysical  tests.  Spatial hearing  will be assessed           in a simulated  free field.  Multidimensional  auditory performance              profiles of subjects  will  be  defined  through  principal  component           analysis  and other  multivariate  statistical methods.                                                                                                           The  major scientific  significance of the  proposed  study is  that it          will  provide a more precise  definition of auditory temporal and                spatial processes  that allow  for the perceptual separation  of speech          and  background noise and  will also identify precise auditory processes         affected by aging.  The clinical  significance of  the study is that  it         will establish a  multidimensional data  base of auditory  capabilities          in  elderly individuals with  mild to  moderate  sensorineural  hearing          loss,  and  may identify  auditory processes which, if  impaired, will           help predict  impending deterioration of  speech understanding  under            non-optimal  listening conditions.  This  work will have an impact on a          wide-spread impairment of verbal communication in the elderly.                    n/a",SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING,2633315,R01AG007998,"['adolescence (12-20)', ' aging', ' audiometry', ' auditory discrimination', ' behavioral /social science research tag', ' binaural hearing', ' human middle age (35-64)', ' human old age (65+)', ' human subject', ' longitudinal human study', ' noise', ' perception', ' psychoacoustics', ' sensorineural hearing loss', ' space perception', ' speech', ' speech recognition', ' young adult human (21-34)']",NIA,EAST BAY INSTITUTE FOR RESEARCH AND EDUC,R01,1998,160074,0.20193431495331235
"SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the        ability to understand conversation  under difficult listening conditions,        such  as  in highly reverberant rooms  or in  gatherings where several           persons are  talking simultaneously,  affects a substantial portion  of          elderly individuals.  This impairment may vary in severity, but only in          very few cases can it be overcome  by the use  of currently available            prosthetic devices.  Attempts to alleviate  this  impairment  have been          impeded by  the fact that  neither the precise  characteristics of  the          intact  process in  the young, nor the causes  of its  breakdown in the          old, are currently well understood.                                                                                                                               The proposed research represents a continuation of work aimed at                 investigating  the ability of  both elderly and young individuals to             understand speech under  non-optimal  listening conditions,  i.e.,               perceptual separation  of a  speech  target from simultaneously ongoing          irrelevant ""noise"".  The research  has two  main  objectives: (1) to             investigate,  in elderly and  in young listeners, the  perceptual                processes (in  particular,  spatial resolution and resolution of temporal        fluctuations) which  play a role  in the separation of  simultaneously           presented relevant and irrelevant  auditory signals; and (2) to study            a group  of elderly individuals over a  five year period, in order to            detect initial or  progressive deterioration of the ability to separate          simultaneous signals and  to determine the correlates of this                    deterioration.                                                                                                                                                    These objectives  will be achieved by  testing selected groups of elderly        and  young individuals on standard  and non-standard audiological tests          as  well as  psychophysical  tests.  Spatial hearing  will be assessed           in a simulated  free field.  Multidimensional  auditory performance              profiles of subjects  will  be  defined  through  principal  component           analysis  and other  multivariate  statistical methods.                                                                                                           The  major scientific  significance of the  proposed  study is  that it          will  provide a more precise  definition of auditory temporal and                spatial processes  that allow  for the perceptual separation  of speech          and  background noise and  will also identify precise auditory processes         affected by aging.  The clinical  significance of  the study is that  it         will establish a  multidimensional data  base of auditory  capabilities          in  elderly individuals with  mild to  moderate  sensorineural  hearing          loss,  and  may identify  auditory processes which, if  impaired, will           help predict  impending deterioration of  speech understanding  under            non-optimal  listening conditions.  This  work will have an impact on a          wide-spread impairment of verbal communication in the elderly.                    n/a",SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING,2001288,R01AG007998,"['adolescence (12-20)', ' aging', ' audiometry', ' auditory discrimination', ' behavioral /social science research tag', ' binaural hearing', ' human middle age (35-64)', ' human old age (65+)', ' human subject', ' longitudinal human study', ' noise', ' perception', ' psychoacoustics', ' sensorineural hearing loss', ' space perception', ' speech', ' speech recognition', ' young adult human (21-34)']",NIA,EAST BAY INSTITUTE FOR RESEARCH AND EDUC,R01,1997,154120,0.20193431495331235
"SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the  ability to understand conversation  under difficult listening conditions,  such  as  in highly reverberant rooms  or in  gatherings where several  persons are  talking simultaneously,  affects a substantial portion  of  elderly individuals.  This impairment may vary in severity, but only in  very few cases can it be overcome  by the use  of currently available  prosthetic devices.  Attempts to alleviate  this  impairment  have been  impeded by  the fact that  neither the precise  characteristics of  the  intact  process in  the young, nor the causes  of its  breakdown in the  old, are currently well understood.    The proposed research represents a continuation of work aimed at  investigating  the ability of  both elderly and young individuals to  understand speech under  non-optimal  listening conditions,  i.e.,  perceptual separation  of a  speech  target from simultaneously ongoing  irrelevant ""noise"".  The research  has two  main  objectives: (1) to  investigate,  in elderly and  in young listeners, the  perceptual  processes (in  particular,  spatial resolution and resolution of temporal  fluctuations) which  play a role  in the separation of  simultaneously  presented relevant and irrelevant  auditory signals; and (2) to study  a group  of elderly individuals over a  five year period, in order to  detect initial or  progressive deterioration of the ability to separate  simultaneous signals and  to determine the correlates of this  deterioration.    These objectives  will be achieved by  testing selected groups of elderly  and  young individuals on standard  and non-standard audiological tests  as  well as  psychophysical  tests.  Spatial hearing  will be assessed  in a simulated  free field.  Multidimensional  auditory performance  profiles of subjects  will  be  defined  through  principal  component  analysis  and other  multivariate  statistical methods.    The  major scientific  significance of the  proposed  study is  that it  will  provide a more precise  definition of auditory temporal and  spatial processes  that allow  for the perceptual separation  of speech  and  background noise and  will also identify precise auditory processes  affected by aging.  The clinical  significance of  the study is that  it  will establish a  multidimensional data  base of auditory  capabilities  in  elderly individuals with  mild to  moderate  sensorineural  hearing  loss,  and  may identify  auditory processes which, if  impaired, will  help predict  impending deterioration of  speech understanding  under  non-optimal  listening conditions.  This  work will have an impact on a  wide-spread impairment of verbal communication in the elderly.  n/a",SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING,2049983,R01AG007998,"['adolescence (12-20)', ' aging', ' audiometry', ' auditory discrimination', ' binaural hearing', ' human middle age (35-64)', ' human old age (65+)', ' human subject', ' longitudinal human study', ' noise', ' perception', ' psychoacoustics', ' sensorineural hearing loss', ' space perception', ' speech', ' speech recognition', ' young adult human (21-34)']",NIA,EAST BAY INSTITUTE FOR RESEARCH AND EDUC,R01,1995,145134,0.20193431495331235
"SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the ability to understand conversation  under difficult listening conditions, such  as  in highly reverberant rooms  or in  gatherings where several persons are  talking simultaneously,  affects a substantial portion  of elderly individuals.  This impairment may vary in severity, but only in very few cases can it be overcome  by the use  of currently available prosthetic devices.  Attempts to alleviate  this  impairment  have been impeded by  the fact that  neither the precise  characteristics of  the intact  process in  the young, nor the causes  of its  breakdown in the old, are currently well understood.  The proposed research represents a continuation of work aimed at investigating  the ability of  both elderly and young individuals to understand speech under  non-optimal  listening conditions,  i.e., perceptual separation  of a  speech  target from simultaneously ongoing irrelevant ""noise"".  The research  has two  main  objectives: (1) to investigate,  in elderly and  in young listeners, the  perceptual processes (in  particular,  spatial resolution and resolution of temporal fluctuations) which  play a role  in the separation of  simultaneously presented relevant and irrelevant  auditory signals; and (2) to study a group  of elderly individuals over a  five year period, in order to detect initial or  progressive deterioration of the ability to separate simultaneous signals and  to determine the correlates of this deterioration.  These objectives  will be achieved by  testing selected groups of elderly and  young individuals on standard  and non-standard audiological tests as  well as  psychophysical  tests.  Spatial hearing  will be assessed in a simulated  free field.  Multidimensional  auditory performance profiles of subjects  will  be  defined  through  principal  component analysis  and other  multivariate  statistical methods.  The  major scientific  significance of the  proposed  study is  that it will  provide a more precise  definition of auditory temporal and spatial processes  that allow  for the perceptual separation  of speech and  background noise and  will also identify precise auditory processes affected by aging.  The clinical  significance of  the study is that  it will establish a  multidimensional data  base of auditory  capabilities in  elderly individuals with  mild to  moderate  sensorineural  hearing loss,  and  may identify  auditory processes which, if  impaired, will help predict  impending deterioration of  speech understanding  under non-optimal  listening conditions.  This  work will have an impact on a wide-spread impairment of verbal communication in the elderly.  n/a",SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING,2049981,R01AG007998,"['adolescence (12-20)', ' aging', ' audiometry', ' auditory discrimination', ' binaural hearing', ' human middle age (35-64)', ' human old age (65+)', ' human subject', ' longitudinal human study', ' noise', ' perception', ' psychoacoustics', ' sensorineural hearing loss', ' space perception', ' speech', ' young adult human (21-34)']",NIA,EAST BAY INSTITUTE FOR RESEARCH AND EDUC,R01,1994,139939,0.20193431495331235
"Advanced Medical Speech Recognition DESCRIPTION (provided by applicant):    Electronic medical record systems (EMR) are held back because data entry is slow, expensive, and codified in a format unsuitable for relational database repositories. Medical Reporting Solutions has developed advanced technology, which can overcome these problems through new methods to encode knowledge in medical reports, and a domain specific speech recognition system capable of real-time natural language understanding.      Our research and development uses methods in corpus linguistics and sentential logic to represent the knowledge in free-text medical reports in an efficient, codeable manner. We have created a prototype speech recognizer capable of natural language processing in real-time employing our unique knowledge base. The knowledge base, while under development, is derived from hundreds of thousands of reports in the radiology domain.      Our project plan includes completing our radiology speech recognizer, substantially enlarging our semantic knowledge base to cover 60% of the domain, and extensively testing the system. We plan to test our recognizer by measuring the word error rate (WER), using three different recognition algorithms we believe will substantially enhance performance beyond the best commercial medical speech recognizers. n/a",Advanced Medical Speech Recognition,7199534,R43LM008328,"['behavioral /social science research tag', 'bioimaging /biomedical imaging', 'clinical research', 'human subject', 'language', 'mathematics', 'radiology', 'semantics', 'speech recognition', 'technology /technique development', 'time resolved data']",NLM,"LOGICAL SEMANTICS, INC.",R43,2005,100000,0.20745808412807212
"Advanced Medical Speech Recognition DESCRIPTION (provided by applicant):    Electronic medical record systems (EMR) are held back because data entry is slow, expensive, and codified in a format unsuitable for relational database repositories. Medical Reporting Solutions has developed advanced technology, which can overcome these problems through new methods to encode knowledge in medical reports, and a domain specific speech recognition system capable of real-time natural language understanding.      Our research and development uses methods in corpus linguistics and sentential logic to represent the knowledge in free-text medical reports in an efficient, codeable manner. We have created a prototype speech recognizer capable of natural language processing in real-time employing our unique knowledge base. The knowledge base, while under development, is derived from hundreds of thousands of reports in the radiology domain.      Our project plan includes completing our radiology speech recognizer, substantially enlarging our semantic knowledge base to cover 60% of the domain, and extensively testing the system. We plan to test our recognizer by measuring the word error rate (WER), using three different recognition algorithms we believe will substantially enhance performance beyond the best commercial medical speech recognizers. n/a",Advanced Medical Speech Recognition,6965478,R43LM008328,"['behavioral /social science research tag', 'bioimaging /biomedical imaging', 'clinical research', 'human subject', 'language', 'mathematics', 'radiology', 'semantics', 'speech recognition', 'technology /technique development', 'time resolved data']",NLM,"LOGICAL SEMANTICS, INC.",R43,2005,100000,0.20745808412807212
"Neuromotor Modeling of Adductor Spasmodic Dysphonia    DESCRIPTION (provided by applicant): Our goal is to create a laryngeal neuromotor model of adductor spasmodic dysphonia (SD), a chronic and often debilitating vocal disorder. We will achieve this goal by identifying specific motoneuron firing patterns that occur during vocal spasm by applying recently-developed multidimensional electromyographic technologies to laryngeal muscles. A neuromotor model of SD will serve to characterize the disorder at the motor nucleus. Neuromotor models, in turn, facilitate parallel research in fields of neuroimaging and drug treatment. Speech requires the coordinated control of multiple muscles by the central nervous system, from diaphragm to lips - with larynx playing a principal role in the phonatory process. Skeletal muscles are controlled by two mechanisms: the recruitment of motoneurons and modulation of motoneuron firing rates. Characterization of laryngeal muscle control at the level of the motoneuron is important toward our understanding of normal speech motor control and of neurologic speech motor disorders. Although studied for many decades, the cause of spasmodic dysphonia has remained elusive. Findings of neuroimaging, genetics, and physiology - including conventional electromyography - have been inconsistent and therefore inconclusive about the neural underpinnings of SD. Irrespective of the heterogeneity of findings using conventional modalities, the central participant in SD is the vocal spasm, and therefore we turn the fine lens offered by the imaging of multiple motoneuron firing activities upon these spasms. We hypothesize that vocal spasms are characterized by episodic increases in new motoneuron firing activity and that these new activities are disordered in firing rate characteristics with the existing pool and among themselves. We will test this hypothesis by obtaining motoneuron firing plots of an intralaryngeal muscle: thyroarytenoid. Motoneuron firing plots contain firing activities of multiple motoneurons simultaneously and in their correct temporal relations. Features of recruitment, correlation, synchronicity, and oscillation during vocal spasm will be compared to periods of non-spasm and to control features of a normal control population. Neuromuscular disorders that disrupt speech affect a sizeable population and often seriously impair the professional, social and family interactions of the affected individual. In applying recent advances in multi-dimensional physiologic and artificial intelligence technologies, this project will formulate models of vocal motor control at the level of the brainstem that improve our understanding of speech production in the normal population and of a voice disorder called spasmodic dysphonia. Knowledge gained from this research aims to assist the development of drug therapy to treat spasmodic dysphonia and other neuromuscular speech disorders.           n/a",Neuromotor Modeling of Adductor Spasmodic Dysphonia,7535517,R21DC008786,"['Affect', 'Artificial Intelligence', 'Biology', 'Brain', 'Brain Stem', 'Cell Nucleus', 'Characteristics', 'Chronic', 'Denervation', 'Development', 'Disease', 'Electromyography', 'Family', 'Genetic', 'Goals', 'Heterogeneity', 'Image', 'Individual', 'Knowledge', 'Laryngeal muscle structure', 'Larynx', 'Lip structure', 'Modality', 'Modeling', 'Motor', 'Motor Neurons', 'Muscle', 'Neuraxis', 'Neurologic', 'Neuromuscular Diseases', 'Participant', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Physiological', 'Physiology', 'Play', 'Population', 'Population Control', 'Process', 'Production', 'Research', 'Respiratory Diaphragm', 'Role', 'Skeletal Muscle', 'Spasm', 'Spastic Dysphonias', 'Speech', 'Speech Disorders', 'Technology', 'Testing', 'Thyroarytenoid Muscle', 'Voice Disorders', 'improved', 'lens', 'motor control', 'motor disorder', 'neuroimaging', 'neuromuscular', 'relating to nervous system', 'social']",NIDCD,NEW YORK MEDICAL COLLEGE,R21,2009,201958,0.2534016817654406
"Neuromotor Modeling of Adductor Spasmodic Dysphonia    DESCRIPTION (provided by applicant): Our goal is to create a laryngeal neuromotor model of adductor spasmodic dysphonia (SD), a chronic and often debilitating vocal disorder. We will achieve this goal by identifying specific motoneuron firing patterns that occur during vocal spasm by applying recently-developed multidimensional electromyographic technologies to laryngeal muscles. A neuromotor model of SD will serve to characterize the disorder at the motor nucleus. Neuromotor models, in turn, facilitate parallel research in fields of neuroimaging and drug treatment. Speech requires the coordinated control of multiple muscles by the central nervous system, from diaphragm to lips - with larynx playing a principal role in the phonatory process. Skeletal muscles are controlled by two mechanisms: the recruitment of motoneurons and modulation of motoneuron firing rates. Characterization of laryngeal muscle control at the level of the motoneuron is important toward our understanding of normal speech motor control and of neurologic speech motor disorders. Although studied for many decades, the cause of spasmodic dysphonia has remained elusive. Findings of neuroimaging, genetics, and physiology - including conventional electromyography - have been inconsistent and therefore inconclusive about the neural underpinnings of SD. Irrespective of the heterogeneity of findings using conventional modalities, the central participant in SD is the vocal spasm, and therefore we turn the fine lens offered by the imaging of multiple motoneuron firing activities upon these spasms. We hypothesize that vocal spasms are characterized by episodic increases in new motoneuron firing activity and that these new activities are disordered in firing rate characteristics with the existing pool and among themselves. We will test this hypothesis by obtaining motoneuron firing plots of an intralaryngeal muscle: thyroarytenoid. Motoneuron firing plots contain firing activities of multiple motoneurons simultaneously and in their correct temporal relations. Features of recruitment, correlation, synchronicity, and oscillation during vocal spasm will be compared to periods of non-spasm and to control features of a normal control population. Neuromuscular disorders that disrupt speech affect a sizeable population and often seriously impair the professional, social and family interactions of the affected individual. In applying recent advances in multi-dimensional physiologic and artificial intelligence technologies, this project will formulate models of vocal motor control at the level of the brainstem that improve our understanding of speech production in the normal population and of a voice disorder called spasmodic dysphonia. Knowledge gained from this research aims to assist the development of drug therapy to treat spasmodic dysphonia and other neuromuscular speech disorders.           n/a",Neuromotor Modeling of Adductor Spasmodic Dysphonia,7387178,R21DC008786,"['Affect', 'Artificial Intelligence', 'Biology', 'Brain', 'Brain Stem', 'Cell Nucleus', 'Characteristics', 'Chronic', 'Denervation', 'Development', 'Disease', 'Electromyography', 'Family', 'Fire - disasters', 'Genetic', 'Goals', 'Heterogeneity', 'Image', 'Individual', 'Knowledge', 'Laryngeal muscle structure', 'Larynx', 'Lip structure', 'Modality', 'Modeling', 'Motor', 'Motor Neurons', 'Muscle', 'Neuraxis', 'Neurologic', 'Neuromuscular Diseases', 'Participant', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Physiological', 'Physiology', 'Play', 'Population', 'Population Control', 'Process', 'Production', 'Rate', 'Research', 'Respiratory Diaphragm', 'Role', 'Skeletal Muscle', 'Spasm', 'Spastic Dysphonias', 'Speech', 'Speech Disorders', 'Technology', 'Testing', 'Thyroarytenoid Muscle', 'Voice Disorders', 'improved', 'lens', 'motor control', 'motor disorder', 'neuroimaging', 'relating to nervous system', 'social']",NIDCD,NEW YORK MEDICAL COLLEGE,R21,2008,247971,0.2534016817654406
"Capturing Patient-Provider Encounter through Text Speech and Dialogue Processing    DESCRIPTION (provided by applicant):       Complete and accurate collection of clinical data in the course of health care is a long-standing goal that has not been achieved either by manual record-keeping or through electronic record systems. This proposed project addresses the problem from the beginning of the clinical process, by aiming to improve the capture of relevant medical facts during the face-to-face interaction between a patient and provider. Instead of relying on the provider's fallible memory to record facts after the visit, the proposed system will ""listen"" to the conversation, use automatic speech recognition to produce an (imperfect) record of what was said, and apply a variety of text analysis and extraction methods to create a draft record of the encounter. Further, it will provide an interface that should permit patients and providers to examine the facts that were recorded and to correct and complete them, also using speech as the primary interface.      The projects aims are to develop and integrate the components needed to accomplish this goal, to create a testbed in collaboration with researchers at the environmental health clinic of a children's hospital in which experiments can guide system development and assess progress, and to conduct a series of evaluations that assess a series of objectives. First, the research will characterize the ability of the speech recognition, information extraction and information organization components to process the target conversations. Second, it will evaluate the hypothesis that this system can collect a more complete and accurate record than what is routinely collected. Subsequently, it will evaluate the time taken by clinicians to use the system, the extent to which the system is seen to disrupt the patient-provider encounter, the ability of patients to use the system to make additions and corrections to their records, and the subjective response of both patients and providers to use of the system.      Success in this effort should lead to better clinical care that is based on more complete and accurate data. In addition, clinical data are also becoming an important resource in the conduct of translational medicine research, where improved data are obviously highly valuable.          n/a",Capturing Patient-Provider Encounter through Text Speech and Dialogue Processing,7526253,R01LM009723,"['Address', 'Adopted', 'Arts', 'Beds', 'Boston', 'Caring', 'Childhood', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Collection', 'Communication', 'Computer Systems', 'Computer Systems Development', 'Computer software', 'Computers', 'Conflict (Psychology)', 'Data', 'Diagnosis', 'Discipline of Nursing', 'Disease', 'Documentation', 'Environment', 'Environmental Health', 'Evaluation', 'Feedback', 'Genetic Transcription', 'Goals', 'Health Personnel', 'Healthcare', 'Individual', 'Lead', 'Linguistics', 'Manuals', 'Medical', 'Medical Records', 'Memory', 'Methods', 'Modeling', 'Paper', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Privacy', 'Process', 'Provider', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Running', 'Series', 'Signs and Symptoms', 'Speech', 'Standards of Weights and Measures', 'Stream', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Transcript', 'Visit', 'Visual', 'Work', 'base', 'clinically significant', 'design', 'electronic recording system', 'experience', 'improved', 'information organization', 'patient safety', 'prototype', 'research study', 'response', 'speech recognition', 'success', 'translational medicine']",NLM,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2008,424544,0.15794347534632294
"Capturing Patient-Provider Encounter through Text Speech and Dialogue Processing    DESCRIPTION (provided by applicant):       Complete and accurate collection of clinical data in the course of health care is a long-standing goal that has not been achieved either by manual record-keeping or through electronic record systems. This proposed project addresses the problem from the beginning of the clinical process, by aiming to improve the capture of relevant medical facts during the face-to-face interaction between a patient and provider. Instead of relying on the provider's fallible memory to record facts after the visit, the proposed system will ""listen"" to the conversation, use automatic speech recognition to produce an (imperfect) record of what was said, and apply a variety of text analysis and extraction methods to create a draft record of the encounter. Further, it will provide an interface that should permit patients and providers to examine the facts that were recorded and to correct and complete them, also using speech as the primary interface.      The projects aims are to develop and integrate the components needed to accomplish this goal, to create a testbed in collaboration with researchers at the environmental health clinic of a children's hospital in which experiments can guide system development and assess progress, and to conduct a series of evaluations that assess a series of objectives. First, the research will characterize the ability of the speech recognition, information extraction and information organization components to process the target conversations. Second, it will evaluate the hypothesis that this system can collect a more complete and accurate record than what is routinely collected. Subsequently, it will evaluate the time taken by clinicians to use the system, the extent to which the system is seen to disrupt the patient-provider encounter, the ability of patients to use the system to make additions and corrections to their records, and the subjective response of both patients and providers to use of the system.      Success in this effort should lead to better clinical care that is based on more complete and accurate data. In addition, clinical data are also becoming an important resource in the conduct of translational medicine research, where improved data are obviously highly valuable.          n/a",Capturing Patient-Provider Encounter through Text Speech and Dialogue Processing,7691691,R01LM009723,"['Address', 'Adopted', 'Arts', 'Beds', 'Boston', 'Childhood', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Collection', 'Communication', 'Computer Systems', 'Computer Systems Development', 'Computer software', 'Computers', 'Conflict (Psychology)', 'Data', 'Diagnosis', 'Discipline of Nursing', 'Disease', 'Documentation', 'Environment', 'Environmental Health', 'Evaluation', 'Feedback', 'Genetic Transcription', 'Goals', 'Health Personnel', 'Healthcare', 'Individual', 'Lead', 'Linguistics', 'Manuals', 'Medical', 'Medical Records', 'Memory', 'Methods', 'Modeling', 'Paper', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Privacy', 'Process', 'Provider', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Running', 'Series', 'Signs and Symptoms', 'Speech', 'Stream', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Transcript', 'Visit', 'Visual', 'Work', 'base', 'clinical care', 'clinically significant', 'design', 'electronic recording system', 'experience', 'improved', 'information organization', 'patient safety', 'prototype', 'research study', 'response', 'speech recognition', 'success', 'translational medicine']",NLM,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2009,429079,0.15794347534632294
"Capturing Patient-Provider Encounter through Text Speech and Dialogue Processing    DESCRIPTION (provided by applicant):       Complete and accurate collection of clinical data in the course of health care is a long-standing goal that has not been achieved either by manual record-keeping or through electronic record systems. This proposed project addresses the problem from the beginning of the clinical process, by aiming to improve the capture of relevant medical facts during the face-to-face interaction between a patient and provider. Instead of relying on the provider's fallible memory to record facts after the visit, the proposed system will ""listen"" to the conversation, use automatic speech recognition to produce an (imperfect) record of what was said, and apply a variety of text analysis and extraction methods to create a draft record of the encounter. Further, it will provide an interface that should permit patients and providers to examine the facts that were recorded and to correct and complete them, also using speech as the primary interface.      The projects aims are to develop and integrate the components needed to accomplish this goal, to create a testbed in collaboration with researchers at the environmental health clinic of a children's hospital in which experiments can guide system development and assess progress, and to conduct a series of evaluations that assess a series of objectives. First, the research will characterize the ability of the speech recognition, information extraction and information organization components to process the target conversations. Second, it will evaluate the hypothesis that this system can collect a more complete and accurate record than what is routinely collected. Subsequently, it will evaluate the time taken by clinicians to use the system, the extent to which the system is seen to disrupt the patient-provider encounter, the ability of patients to use the system to make additions and corrections to their records, and the subjective response of both patients and providers to use of the system.      Success in this effort should lead to better clinical care that is based on more complete and accurate data. In addition, clinical data are also becoming an important resource in the conduct of translational medicine research, where improved data are obviously highly valuable.          n/a",Capturing Patient-Provider Encounter through Text Speech and Dialogue Processing,7934457,R01LM009723,"['Address', 'Adopted', 'Arts', 'Beds', 'Boston', 'Childhood', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Collection', 'Communication', 'Computer Systems', 'Computer Systems Development', 'Computer software', 'Computers', 'Conflict (Psychology)', 'Data', 'Diagnosis', 'Discipline of Nursing', 'Disease', 'Documentation', 'Environment', 'Environmental Health', 'Evaluation', 'Feedback', 'Genetic Transcription', 'Goals', 'Health Personnel', 'Healthcare', 'Individual', 'Lead', 'Linguistics', 'Manuals', 'Medical', 'Medical Records', 'Memory', 'Methods', 'Modeling', 'Paper', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Privacy', 'Process', 'Provider', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Running', 'Series', 'Signs and Symptoms', 'Speech', 'Stream', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Transcript', 'Visit', 'Visual', 'Work', 'base', 'clinical care', 'clinically significant', 'design', 'electronic recording system', 'experience', 'improved', 'information organization', 'patient safety', 'prototype', 'research study', 'response', 'speech recognition', 'success', 'translational medicine']",NLM,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2010,424820,0.15794347534632294
"Capturing Patient-Provider Encounter through Text Speech and Dialogue Processing Project Summary Complete and accurate collection of clinical data in the course of health care is a long-standing goal that has not been achieved either by manual record-keeping or through electronic record systems. This proposed project addresses the problem from the beginning of the clinical process, by aiming to improve the capture of relevant medical facts during the face-to-face interaction between a patient and provider. Instead of relying on the provider's fallible memory to record facts after the visit, the proposed system will ""listen"" to the conversation, use automatic speech recognition to produce an (imperfect) record of what was said, and apply a variety of text analysis and extraction methods to create a draft record of the encounter. Further, it will provide an interface that should permit patients and providers to examine the facts that were recorded and to correct and complete them, also using speech as the primary interface.  The projects aims are to develop and integrate the components needed to accomplish this goal, to create a testbed in collaboration with researchers at the environmental health clinic of a children's hos- pital in which experiments can guide system development and assess progress, and to conduct a series of evaluations that assess a series of objectives. First, the research will characterize the ability of the speech recognition, information extraction and information organization components to process the target conversations. Second, it will evaluate the hypothesis that this system can collect a more complete and accurate record than what is routinely collected. Subsequently, it will evaluate the time taken by clinicians to use the system, the extent to which the system is seen to disrupt the patient-provider encounter, the ability of patients to use the system to make additions and corrections to their records, and the subjective response of both patients and providers to use of the system.  Success in this effort should lead to better clinical care that is based on more complete and accurate data. In addition, clinical data are also becoming an important resource in the conduct of translational medicine research, where improved data are obviously highly valuable. Project Narrative A well-organized, complete and accurate record of clinical encounters can form the bedrock of data on which both clinical care and clinical and biomedical research can rest. This project applies state of the art and novel technologies to ""listen"" to and interpret encounters between patients and health care providers to create such records. Its success should lead to better health care and greater possibilities for using clinical data in medical research. n/a",Capturing Patient-Provider Encounter through Text Speech and Dialogue Processing,7934200,R01LM009723,"['Address', 'Adopted', 'Arts', 'Beds', 'Biomedical Research', 'Boston', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Collection', 'Communication', 'Computer Systems', 'Computer Systems Development', 'Computer software', 'Computers', 'Conflict (Psychology)', 'Data', 'Diagnosis', 'Discipline of Nursing', 'Disease', 'Documentation', 'Environment', 'Environmental Health', 'Evaluation', 'Feedback', 'Genetic Transcription', 'Goals', 'Health Personnel', 'Healthcare', 'Individual', 'Lead', 'Linguistics', 'Manuals', 'Medical', 'Medical Records', 'Medical Research', 'Memory', 'Methods', 'Modeling', 'Paper', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Privacy', 'Process', 'Provider', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Rest', 'Running', 'Series', 'Signs and Symptoms', 'Speech', 'Stream', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Transcript', 'Visit', 'Visual', 'Work', 'base', 'clinical care', 'clinically significant', 'design', 'electronic recording system', 'experience', 'improved', 'information organization', 'new technology', 'patient safety', 'prototype', 'research study', 'response', 'speech recognition', 'success', 'translational medicine']",NLM,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2009,153012,0.13120475791377975
"An Accessible, Effective Treatment for Sentence Deficit in Agrammatic Aphasia    DESCRIPTION (provided by applicant): The goal of our work is to develop an accessible, effective and cost effective speech and language treatment for aphasia. To achieve this goal, we will develop a commercial prototype of a computer program called Sentactics based on over 15 years of research at Northwestern University that demonstrates the effectiveness of the program to improve speech production and comprehension skills of individuals with aphasia. Preliminary work funded by the NIH resulted in an initial version of the Sentactics program. A clinical trial revealed that individuals with agrammatic aphasia who used Sentactics improved their speech and language production and comprehension skills significantly, with gains equivalent to subjects who were administered the clinical treatment from expert human clinicians. This study also demonstrated that individuals with aphasia find the Sentactics program to be highly engaging and fun to use. Our project aims to demonstrate the feasibility of a commercial prototype of an improved version of Sentactics that incorporates two new features: (a) a spoken language system that uses speech and natural language processing technologies to provide feedback to clients about the accuracy of their speech productions, and (b) a clinician oversight capability that enables individual clinicians to use the internet to monitor multiple simultaneous users of the program and communicate with individual users as needed. In Phase I of the project, we will demonstrate the feasibility of using the spoken language system to provide feedback to clients about the accuracy of their speech productions. In Phase II we will develop a commercial prototype of the program incorporating the spoken language system and clinician oversight functions. We will then field test the program with individuals with aphasia. We will compare changes in the speech production and comprehension abilities of individuals using the Sentactics program to individuals using a commercially available program. PUBLIC HEALTH RELEVANCE: Successful outcomes of this project would result in a commercial prototype of an accessible, affordable, effective and easy to use speech and language therapy program for use by individuals with aphasia in clinics or homes. This project addresses a great national need for inexpensive, intensive, extensive, effective treatments for millions of individuals with aphasia.                        Relevance Successful outcomes of this project would result in a commercial prototype of an accessible, affordable, effective and easy to use speech and language therapy program for use by individuals with aphasia in clinics or homes. This project addresses a great national need for inexpensive, intensive, extensive, effective treatments for millions of individuals with aphasia.  ","An Accessible, Effective Treatment for Sentence Deficit in Agrammatic Aphasia",7611673,R43DC009926,"['Address', 'Aphasia', 'Aphasiology', 'Client', 'Clinic', 'Clinical', 'Clinical Treatment', 'Clinical Trials', 'Communication', 'Comprehension', 'Computers', 'Couples', 'Data', 'Effectiveness', 'Evaluation', 'Feedback', 'Funding', 'Goals', 'Home environment', 'Hospitals', 'Human', 'Individual', 'Internet', 'Language', 'Language Therapy', 'Modeling', 'Monitor', 'Natural Language Processing', 'Numbers', 'Outcome', 'Participant', 'Patients', 'Phase', 'Polishes', 'Production', 'Program Effectiveness', 'Public Health', 'Research', 'Semantics', 'Series', 'Site', 'Speech', 'Speech Recognition Software', 'Stimulus', 'System', 'Technology', 'Testing', 'Training', 'United States National Institutes of Health', 'Universities', 'Vocabulary', 'Work', 'aphasic', 'base', 'cohort', 'computer program', 'cost effective', 'design', 'improved', 'iterative design', 'lexical', 'programs', 'prototype', 'research and development', 'response', 'skills', 'speech accuracy', 'speech recognition', 'syntax', 'treatment effect', 'usability', 'user-friendly', 'visual stimulus']",NIDCD,"MENTOR INTERACTIVE, INC.",R43,2008,99956,0.28521766196931175
"SPOKEN LANGUAGE SYSTEMS FOR WAYFINDING INFORMATION Spoken language systems, consisting of speech recognition, speech synthesis, natural language processing, and human-computer interfaces allow people to use speech to communicate with computers.  The system proposed here provides access to wayfinding/route information for the visually impaired.  The system proposed here is a spoken language system which can recognize the speech of any speaker and requires t=no custom hardware.  A speaker independent speech recognition system developed during phase I will be the base for the system.  Phase I has two primary goals:  1.  Demonstrate the feasibility of combining Hidden Markov Models, Segmental models, and Artificial neural networks to improve recognition performance.  We propose to tightly integrate these technologies in a succinct mathematical framework.  2.  Determine the needs of visually impaired persons for wayfinding information and use this to design a language model which constrains the recognition process without appearing to constrain the users' interactions.  If successful, the technology proposed here would permit the development of more powerful, cost-effective spoken language systems than are currently available for making wayfinding information available to persons with visual impairments.  n/a",SPOKEN LANGUAGE SYSTEMS FOR WAYFINDING INFORMATION,2164769,R43EY010712,"['artificial intelligence', ' blind aid', ' computer human interaction', ' computer system design /evaluation', ' human subject', ' language', ' language translation', ' mathematical model', ' model design /development', ' speech synthesizers', ' travel']",NEI,"PURESPEECH, INC.",R43,1994,80250,0.27369510047611434
"Speech Therapy Robot (STR) to assist in the administration of evidence based spee    DESCRIPTION (provided by applicant): This SBIR phase I project will develop a Speech Therapy Robot (STR) to assist in the administration of evidence-based speech and language therapy to provide individualized monitoring of multiple clients simultaneously in a school setting. STR will use biologically plausible artificial intelligence models to prototype a system that is affordable, easy to use, portable and extensible to work with any number of students (clients) with disorder. Most children make some mistakes as they learn to say new words, but a speech sound disorder results when mistakes continue past a certain age. Speech sound disorders include problems with articulation (making sounds) and phonological processes (sound patterns), and it is one of the largest disabilities in the United States. Children with speech disorders are evaluated by a speech-language pathologist (SLP) and treated via speech-language intervention within the child's classroom (classroom-based) or outside of the classroom (pull-out). Multiple studies have demonstrated that classroom-based service is beneficial over pull- out service, but currently it is not widely practiced because of the many challenges facing SLPs: 1) it requires collaboration with classroom teachers and administrators who are not trained in speech-language pathology, 2) it can create a larger client-SLP ratio, 3) a small client-nonclient student in-class ratio, 4) unable to provide adequate intervention, 5) there is a large variation in severity of disorder within clients, 6) longer session hours over pull-out service. The American Speech-Language-Hearing Association (ASHA) recommendations caseloads should not exceed 40, but the median caseload is 50 in elementary and secondary schools, with high of 80 clients. This heavy workload for an SLP limits their capacity to provide effective treatment. Thus a robotic system capable of reducing the workload and assisting SLPs to provide improved individual care is highly desired by those in this field. In this Phase I SBIR, we will develop novel biologically plausible models to address these challenges by developing a robot-assisted therapy system capable of real time monitoring and assessment of client and client-provider interaction during the session, to determine client engagement, performance and to give feedback to providers in real time for improved treatment delivery. The biological models attempt to mimic the expert diagnostic capabilities of a SLP and extend it for use by non-SLPs to work with multiple clients at the same time. The solution will not require specialized training to use, allowing teachers to easily use it in their classrooms. In Phase I, we will demonstrate the feasibility and accuracy of STR. STR is not just a minor improvement over existing technologies but a technology and application that do not exist today. In Phase II, we will extend the capabilities towards a fully biologically plausible system to mimic expert human performance levels to develop a robotic system for speech-language therapy, this will be followed by clinical trials to ensure accuracy, efficacy of STR to facilitate evidence-based therapy. Variations of the system can be used towards phonology, morphology/syntax, pragmatics, language, fluency and/or vocabulary.      PUBLIC HEALTH RELEVANCE: Overall the project provides direct relevance to public health by facilitating new insights through the development of a novel biologically plausible artificial intelligence system capable of real time monitoring and assessment of verbal therapy session content in real time to determine patient engagement, performance and give feedback to providers in real time to improve treatment delivery, in a school setting. The novel biologically plausible device will significantly impact the current known methods of in classroom evaluation, monitoring and treatment of speech disorders. The project can help in substantial improvement in patient client interaction, better treatment, lower burden on speech language pathologists, and will significantly impact the current known methods, technologies, treatments, and address critical barriers to progress in the field.           Overall the project provides direct relevance to public health by facilitating new insights through the development of a novel biologically plausible artificial intelligence system capable of real time monitoring and assessment of verbal therapy session content in real time to determine patient engagement, performance and give feedback to providers in real time to improve treatment delivery, in a school setting. The novel biologically plausible device will significantly impact the current known methods of in classroom evaluation, monitoring and treatment of speech disorders. The project can help in substantial improvement in patient client interaction, better treatment, lower burden on speech language pathologists, and will significantly impact the current known methods, technologies, treatments, and address critical barriers to progress in the field.         ",Speech Therapy Robot (STR) to assist in the administration of evidence based spee,8207025,R43LM011325,"['Accent', 'Address', 'Administrator', 'Affect', 'Age', 'Algorithms', 'American Speech-Language-Hearing Association', 'Antirrhinum', 'Area', 'Artificial Intelligence', 'Auditory system', 'Biological Models', 'Caring', 'Cerebellum', 'Child', 'Client', 'Clinical Trials', 'Cognitive', 'Collaborations', 'Communication impairment', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Disease', 'Early identification', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Feedback', 'Florida', 'Future', 'Glosso-Sterandryl', 'Goals', 'Hour', 'Human', 'Impairment', 'Individual', 'Intervention', 'Joints', 'Language', 'Language Pathology', 'Language Therapy', 'Learning', 'Liquid substance', 'Memory', 'Methods', 'Minor', 'Modality', 'Modeling', 'Monitor', 'Morphology', 'Neurons', 'Pathologist', 'Patients', 'Pattern', 'Performance', 'Phase', 'Phonetics', 'Process', 'Provider', 'Public Health', 'Recommendation', 'Robot', 'Robotics', 'Running', 'Schools', 'Secondary Schools', 'Services', 'Severities', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Solutions', 'Source', 'Speech', 'Speech Disorders', 'Speech Sound', 'Speech Therapy', 'Speech-Language Pathology', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Therapeutic Intervention', 'Time', 'Training', 'United States', 'Universities', 'Variant', 'Vocabulary', 'Voice', 'Work', 'Workload', 'base', 'biological systems', 'cost', 'design', 'disability', 'effective therapy', 'elementary school', 'evidence base', 'improved', 'innovation', 'insight', 'neural model', 'novel', 'phonology', 'prevent', 'professor', 'prototype', 'robot assistance', 'sound', 'speech processing', 'success', 'syntax', 'teacher']",NLM,"AVENTUSOFT, LLC",R43,2011,95949,0.18913097370631862
"Speech segregation to improve intelligility of reverberant-noisy speech Project Summary Hearing loss is one of the most prevalent chronic conditions, affecting 37.5 million Americans. Although signal amplification in modern hearing aids makes sound more audible to hearing impaired listeners, speech understanding in background interference remains the biggest challenge by hearing aid wearers. The proposed research seeks a monaural (one-microphone) solution to this challenge by developing supervised speech segregation based on deep learning. Unlike traditional speech enhancement, deep learning based speech segregation is driven by training data, and three components of a deep neural network (DNN) model are features, training targets, and network architectures. Recently, deep learning has achieved tremendous successes in a variety of real world applications. Our approach builds on the progress made in the PI's previous R01 project which demonstrated, for the first time, substantial speech intelligibility improvements for hearing-impaired listeners in noise. A main focus of the proposed work in this cycle is to combat room reverberation in addition to background interference. The proposed work is designed to achieve three specific aims. The first aim is to improve intelligibility of reverberant-noisy speech for hearing- impaired listeners. To achieve this aim, we will train DNNs to perform time-frequency masking. The second aim is to improve intelligibility of reverberant speech in the presence of competing speech. To achieve this aim, we will perform DNN training to estimate two ideal masks, one for the target talker and the other for the interfering talker. The third aim is to improve intelligibility of reverberant speech in combined speech and nonspeech interference. To achieve this aim, we will develop a two-stage DNN model where the first stage will be trained to remove nonspeech interference and the second stage to remove interfering speech. Eight speech intelligibility experiments involving both hearing-impaired and normal-hearing listeners will be conducted to systematically evaluate the developed system. The proposed project is expected to substantially close the speech intelligibility gap between hearing-impaired and normal-hearing listeners in daily conditions, with the ultimate goal of removing the gap altogether. Relevance A widely acknowledged deficit of hearing loss is reduced intelligibility of reverberant-noisy speech. How to improve speech intelligibility of hearing impaired listeners in everyday environments is a major technical challenge. This project directly addresses this challenge and the results from the project are expected to yield technical methods that can be translated to hearing prosthesis, potentially benefiting millions of individuals with hearing loss.",Speech segregation to improve intelligility of reverberant-noisy speech,9831633,R01DC012048,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'American', 'Auditory', 'Auditory Prosthesis', 'Chronic', 'Complex', 'Data', 'Environment', 'Formulation', 'Frequencies', 'Goals', 'Hearing', 'Hearing Aids', 'Individual', 'Investigation', 'Laboratories', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Network-based', 'Neural Network Simulation', 'Noise', 'Recurrence', 'Research', 'Signal Transduction', 'Source', 'Speech', 'Speech Intelligibility', 'Structure', 'Supervision', 'Surface', 'Symptoms', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'combat', 'deep learning', 'deep neural network', 'design', 'digital', 'experimental study', 'hearing impairment', 'improved', 'innovation', 'microphone', 'network architecture', 'normal hearing', 'real world application', 'segregation', 'signal processing', 'sound', 'speech in noise', 'success', 'supervised learning']",NIDCD,OHIO STATE UNIVERSITY,R01,2020,303452,0.3802640204333557
"Speech segregation to improve intelligility of reverberant-noisy speech Project Summary Hearing loss is one of the most prevalent chronic conditions, affecting 37.5 million Americans. Although signal amplification in modern hearing aids makes sound more audible to hearing impaired listeners, speech understanding in background interference remains the biggest challenge by hearing aid wearers. The proposed research seeks a monaural (one-microphone) solution to this challenge by developing supervised speech segregation based on deep learning. Unlike traditional speech enhancement, deep learning based speech segregation is driven by training data, and three components of a deep neural network (DNN) model are features, training targets, and network architectures. Recently, deep learning has achieved tremendous successes in a variety of real world applications. Our approach builds on the progress made in the PI's previous R01 project which demonstrated, for the first time, substantial speech intelligibility improvements for hearing-impaired listeners in noise. A main focus of the proposed work in this cycle is to combat room reverberation in addition to background interference. The proposed work is designed to achieve three specific aims. The first aim is to improve intelligibility of reverberant-noisy speech for hearing- impaired listeners. To achieve this aim, we will train DNNs to perform time-frequency masking. The second aim is to improve intelligibility of reverberant speech in the presence of competing speech. To achieve this aim, we will perform DNN training to estimate two ideal masks, one for the target talker and the other for the interfering talker. The third aim is to improve intelligibility of reverberant speech in combined speech and nonspeech interference. To achieve this aim, we will develop a two-stage DNN model where the first stage will be trained to remove nonspeech interference and the second stage to remove interfering speech. Eight speech intelligibility experiments involving both hearing-impaired and normal-hearing listeners will be conducted to systematically evaluate the developed system. The proposed project is expected to substantially close the speech intelligibility gap between hearing-impaired and normal-hearing listeners in daily conditions, with the ultimate goal of removing the gap altogether. Relevance A widely acknowledged deficit of hearing loss is reduced intelligibility of reverberant-noisy speech. How to improve speech intelligibility of hearing impaired listeners in everyday environments is a major technical challenge. This project directly addresses this challenge and the results from the project are expected to yield technical methods that can be translated to hearing prosthesis, potentially benefiting millions of individuals with hearing loss.",Speech segregation to improve intelligility of reverberant-noisy speech,9623341,R01DC012048,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'American', 'Auditory', 'Auditory Prosthesis', 'Chronic', 'Complex', 'Data', 'Environment', 'Formulation', 'Frequencies', 'Goals', 'Hearing', 'Hearing Aids', 'Individual', 'Investigation', 'Laboratories', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Network-based', 'Neural Network Simulation', 'Noise', 'Recurrence', 'Research', 'Signal Transduction', 'Source', 'Speech', 'Speech Intelligibility', 'Structure', 'Supervision', 'Surface', 'Symptoms', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'combat', 'deep learning', 'deep neural network', 'design', 'digital', 'experimental study', 'hearing impairment', 'improved', 'innovation', 'microphone', 'network architecture', 'normal hearing', 'real world application', 'segregation', 'signal processing', 'sound', 'speech in noise', 'success', 'supervised learning']",NIDCD,OHIO STATE UNIVERSITY,R01,2019,304865,0.3802640204333557
"Speech segregation to improve intelligility of reverberant-noisy speech Project Summary Hearing loss is one of the most prevalent chronic conditions, affecting 37.5 million Americans. Although signal amplification in modern hearing aids makes sound more audible to hearing impaired listeners, speech understanding in background interference remains the biggest challenge by hearing aid wearers. The proposed research seeks a monaural (one-microphone) solution to this challenge by developing supervised speech segregation based on deep learning. Unlike traditional speech enhancement, deep learning based speech segregation is driven by training data, and three components of a deep neural network (DNN) model are features, training targets, and network architectures. Recently, deep learning has achieved tremendous successes in a variety of real world applications. Our approach builds on the progress made in the PI's previous R01 project which demonstrated, for the first time, substantial speech intelligibility improvements for hearing-impaired listeners in noise. A main focus of the proposed work in this cycle is to combat room reverberation in addition to background interference. The proposed work is designed to achieve three specific aims. The first aim is to improve intelligibility of reverberant-noisy speech for hearing- impaired listeners. To achieve this aim, we will train DNNs to perform time-frequency masking. The second aim is to improve intelligibility of reverberant speech in the presence of competing speech. To achieve this aim, we will perform DNN training to estimate two ideal masks, one for the target talker and the other for the interfering talker. The third aim is to improve intelligibility of reverberant speech in combined speech and nonspeech interference. To achieve this aim, we will develop a two-stage DNN model where the first stage will be trained to remove nonspeech interference and the second stage to remove interfering speech. Eight speech intelligibility experiments involving both hearing-impaired and normal-hearing listeners will be conducted to systematically evaluate the developed system. The proposed project is expected to substantially close the speech intelligibility gap between hearing-impaired and normal-hearing listeners in daily conditions, with the ultimate goal of removing the gap altogether. Relevance A widely acknowledged deficit of hearing loss is reduced intelligibility of reverberant-noisy speech. How to improve speech intelligibility of hearing impaired listeners in everyday environments is a major technical challenge. This project directly addresses this challenge and the results from the project are expected to yield technical methods that can be translated to hearing prosthesis, potentially benefiting millions of individuals with hearing loss.",Speech segregation to improve intelligility of reverberant-noisy speech,9443223,R01DC012048,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'American', 'Auditory', 'Auricular prosthesis', 'Chronic', 'Complex', 'Data', 'Environment', 'Formulation', 'Frequencies', 'Goals', 'Hearing', 'Hearing Aids', 'Individual', 'Investigation', 'Laboratories', 'Learning', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Network-based', 'Neural Network Simulation', 'Noise', 'Recurrence', 'Research', 'Signal Transduction', 'Source', 'Speech', 'Speech Intelligibility', 'Structure', 'Supervision', 'Surface', 'Symptoms', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'combat', 'deep learning', 'deep neural network', 'design', 'digital', 'experimental study', 'hearing impairment', 'improved', 'innovation', 'network architecture', 'real world application', 'segregation', 'signal processing', 'sound', 'success']",NIDCD,OHIO STATE UNIVERSITY,R01,2018,292174,0.3802640204333557
"Computer-Based Pronunciation Analysis for Children with Speech Sound Disorders  Summary The long-term objective of the proposed work is to develop speech-production assessment and pronunciation- training tools for children with speech sound disorders. The technology resulting from research on computer- assisted pronunciation training has not yet been successfully extended to help children with speech sound disorders, primarily because of a lack of accuracy in phoneme-level analysis of the speech signal. The goal of the proposed exploratory research is to develop a set of algorithms that will constitute the core components of an effective pronunciation analysis system for children with speech sound disorders. The components of this system, when used in concert, will reliably identify and score the intelligibility of a phoneme within an isolated target word. The algorithms will also identify specific types of distortion errors (e.g. fronting, in which the /sh/ phoneme is realized as /s/). The tools resulting from the proposed work will provide immediate, relevant, and understandable feedback about pronunciation errors. The Specific Aims are to (1) Create individualized speech templates for use in objective analysis of pronunciation, (2) Automatically identify phoneme locations in speech recordings, and (3) Automatically score phoneme intelligibility for children with speech sound disorders. For Specific Aim 1, the template for evaluating a participant's spoken word will be selected from a large pool of templates of that word, and each template will be further individualized to match the general spectral characteristics of the participant. For Specific Aim 2, the primary challenge is to identify phoneme locations when the observed (spoken) phoneme sequence is different from the expected (target) phoneme sequence. A five-step process will be used to identify possible differences between the observed and expected phoneme sequence using several independent sources of information. Methods will include automatic classification of manner of articulation using a Hidden Markov Model, dynamic time warping, and a priori determination of likely phoneme errors. Specific Aim 3 will provide a measure of the intelligibility of a target phoneme and also identify distorted features. The scoring of intelligibility will be performed using a proposed Phoneme Intelligibility Analysis (PIA) module, which is phoneme-specific and composed of six sources of information, including an acoustic template of the target phoneme, likely phonetic substitutions, acoustic features used in analysis, thresholds of acceptability, statistics of phoneme duration in the given context, and evaluation metrics. The use of human perceptual data (intelligibility scores) as training data is an important and new component of the proposed approach.  Narrative The proposed work is relevant to the public health in that the software tools that result from this work will enable children with speech sound disorders to better communicate with the general population. Furthermore, these tools will assist teachers of such children in the task of pronunciation assessment, allowing the teachers to more effectively use their time.",Computer-Based Pronunciation Analysis for Children with Speech Sound Disorders,8336853,R21DC012139,"['Acoustics', 'Adult', 'Affect', 'Algorithms', 'American Speech-Language-Hearing Association', 'Characteristics', 'Child', 'Classification', 'Communication', 'Complement', 'Computer Assisted', 'Computers', 'Data', 'Data Set', 'Diagnosis', 'Disease', 'Evaluation', 'Feedback', 'General Population', 'Genetic Transcription', 'Goals', 'Human', 'Individual', 'Joints', 'Language', 'Location', 'Manuals', 'Measures', 'Methods', 'Metric', 'Nursery Schools', 'Output', 'Participant', 'Pathologist', 'Performance', 'Phonetics', 'Population', 'Probability', 'Process', 'Production', 'Public Health', 'Reading', 'Research', 'School-Age Population', 'Schools', 'Signal Transduction', 'Software Tools', 'Source', 'Speech', 'Speech Sound', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Work', 'Writing', 'base', 'disability', 'innovation', 'markov model', 'mathematical ability', 'peer', 'phonology', 'remediation', 'spelling', 'statistics', 'success', 'teacher', 'tool']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R21,2012,189097,0.21133119086333915
"Computer-Based Pronunciation Analysis for Children with Speech Sound Disorders    DESCRIPTION (provided by applicant): The long-term objective of the proposed work is to develop speech-production assessment and pronunciation- training tools for children with speech sound disorders. The technology resulting from research on computer- assisted pronunciation training has not yet been successfully extended to help children with speech sound disorders, primarily because of a lack of accuracy in phoneme-level analysis of the speech signal. The goal of the proposed exploratory research is to develop a set of algorithms that will constitute the core components of an effective pronunciation analysis system for children with speech sound disorders. The components of this system, when used in concert, will reliably identify and score the intelligibility of a phoneme within an isolated target word. The algorithms will also identify specific types of distortion errors (e.g. fronting, in which the /sh/ phoneme is realized as /s/). The tools resulting from the proposed work will provide immediate, relevant, and understandable feedback about pronunciation errors. The Specific Aims are to (1) Create individualized speech templates for use in objective analysis of pronunciation, (2) Automatically identify phoneme locations in speech recordings, and (3) Automatically score phoneme intelligibility for children with speech sound disorders. For Specific Aim 1, the template for evaluating a participant's spoken word will be selected from a large pool of templates of that word, and each template will be further individualized to match the general spectral characteristics of the participant. For Specific Aim 2, the primary challenge is to identify phoneme locations when the observed (spoken) phoneme sequence is different from the expected (target) phoneme sequence. A five-step process will be used to identify possible differences between the observed and expected phoneme sequence using several independent sources of information. Methods will include automatic classification of manner of articulation using a Hidden Markov Model, dynamic time warping, and a priori determination of likely phoneme errors. Specific Aim 3 will provide a measure of the intelligibility of a target phoneme and also identify distorted features. The scoring of intelligibility will be performed using a proposed Phoneme Intelligibility Analysis (PIA) module, which is phoneme-specific and composed of six sources of information, including an acoustic template of the target phoneme, likely phonetic substitutions, acoustic features used in analysis, thresholds of acceptability, statistics of phoneme duration in the given context, and evaluation metrics. The use of human perceptual data (intelligibility scores) as training data is an important and new component of the proposed approach.      PUBLIC HEALTH RELEVANCE: The proposed work is relevant to the public health in that the software tools that result from this work will enable children with speech sound disorders to better communicate with the general population. Furthermore, these tools will assist teachers of such children in the task of pronunciation assessment, allowing the teachers to more effectively use their time.              The proposed work is relevant to the public health in that the software tools that result from this work will enable children with speech sound disorders to better communicate with the general population. Furthermore, these tools will assist teachers of such children in the task of pronunciation assessment, allowing the teachers to more effectively use their time.            ",Computer-Based Pronunciation Analysis for Children with Speech Sound Disorders,8227504,R21DC012139,"['Acoustics', 'Adult', 'Affect', 'Algorithms', 'American Speech-Language-Hearing Association', 'Characteristics', 'Child', 'Classification', 'Communication', 'Complement', 'Computer Assisted', 'Computers', 'Data', 'Data Set', 'Diagnosis', 'Disease', 'Evaluation', 'Feedback', 'General Population', 'Genetic Transcription', 'Goals', 'Human', 'Individual', 'Joints', 'Language', 'Location', 'Manuals', 'Measures', 'Methods', 'Metric', 'Nursery Schools', 'Output', 'Participant', 'Pathologist', 'Performance', 'Phonetics', 'Population', 'Probability', 'Process', 'Production', 'Public Health', 'Reading', 'Research', 'School-Age Population', 'Schools', 'Signal Transduction', 'Software Tools', 'Source', 'Speech', 'Speech Sound', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Work', 'Writing', 'base', 'disability', 'innovation', 'markov model', 'mathematical ability', 'peer', 'phonology', 'remediation', 'spelling', 'statistics', 'success', 'teacher', 'tool']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R21,2011,227102,0.18572979925620384
"SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer. PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.",SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS,9390468,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'communication device', 'data mining', 'diagnostic accuracy', 'digital', 'experimental study', 'improved', 'improved functioning', 'innovation', 'jaw movement', 'motor impairment', 'movement analysis', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2018,686265,0.24384479847069565
"SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer. PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.",SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS,9185964,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'communication device', 'data mining', 'diagnostic accuracy', 'digital', 'experimental study', 'improved', 'improved functioning', 'innovation', 'jaw movement', 'motor impairment', 'movement analysis', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2017,581327,0.24384479847069565
"Speech Movement Classification for Assessing and Treating ALS DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer. PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.",Speech Movement Classification for Assessing and Treating ALS,9341526,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'communication device', 'data mining', 'diagnostic accuracy', 'digital', 'experimental study', 'improved', 'improved functioning', 'innovation', 'jaw movement', 'motor impairment', 'movement analysis', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2017,93248,0.24384479847069565
"SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer. PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.",SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS,8985675,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Equilibrium', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Life', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Staging', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'communication device', 'data mining', 'diagnostic accuracy', 'digital', 'forging', 'improved', 'innovation', 'jaw movement', 'motor impairment', 'movement analysis', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'research study', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2016,585316,0.24384479847069565
"SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer. PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.",SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS,8775639,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Communication Aids for Disabled', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Equilibrium', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Life', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Staging', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'data mining', 'diagnostic accuracy', 'digital', 'forging', 'improved', 'innovation', 'jaw movement', 'motor impairment', 'movement analysis', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'research study', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2015,584132,0.24384479847069565
"SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS     DESCRIPTION (provided by applicant): The purpose of this project is advance the assessment and treatment of speech motor impairments due to ALS using novel computer-based approaches. Recently developed speech movement tracking technology will be used to record movements of tongue, lips, and jaw in 50 persons with ALS and 50 healthy control participants. The speech movement data will be analyzed using custom machine learning algorithms to address three important translational needs in person with ALS: improved early detection of speech motor involvement, improved progress monitoring of speech motor decline, and improved options for maintaining oral communication. The established interdisciplinary team with expertise in data mining, speech- language pathology, clinical neurology, and spatial statistics are well positioned to conduct this research. If successful, the specific aims have the potential to transform clinical practice for speech-language pathologists, neurologists, and other related health care professionals. The propose research will enhance human health by making an impact on individuals with speech motor impairment due to ALS and potentially to a broad range of other speech motor due to stroke, traumatic brain injury, multiple sclerosis, Parkinson's disease, cerebral palsy, traumatic brain injury, and orofacial or laryngeal cancer.         PUBLIC HEALTH RELEVANCE: ALS is one of the most common motor neuron diseases. According to the National Institute of Neurological Disorders and Stroke, approximately 30,000 Americans are living with ALS (NINDS, 2003). Recent evidence suggests ALS incidence is increasing in the general population (Strong & Rosenfeld, 2003), particularly among Gulf War veterans who are nearly twice as likely to develop the disease as veterans not deployed to the Gulf (Haley, 2003). This project is focused on the development and validation of novel machine-learning based tools for improving the assessment and treatment of patients with speech motor impairments due to ALS. If successful, this research may (1) improve early detection and prognostic accuracy, (2) and address the critical need for objective outcome measures for ongoing experimental drug trials, and (3) provide information to develop a novel oral communication device for persons with moderate to severe speech impairment. These developments may ameliorate the socioeconomic burden of speech motor impairments as well as the quality of life for these patients, their families, and the people they closely interact wit.                    ",SPEECH MOVEMENT CLASSIFICATION FOR ASSESSING AND TREATING ALS,8613983,R01DC013547,"['Address', 'Age', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Cerebral Palsy', 'Classification', 'Clinical', 'Communication Aids for Disabled', 'Computers', 'Custom', 'Data', 'Data Set', 'Deglutition', 'Deterioration', 'Development', 'Device Designs', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Progression', 'Dysarthria', 'Early Diagnosis', 'Electromagnetics', 'Equilibrium', 'Family', 'Future', 'Gender', 'General Population', 'Goals', 'Gulf War', 'Health', 'Health Professional', 'Human', 'Impairment', 'Incidence', 'Individual', 'Jaw', 'Language', 'Life', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Monitor', 'Motor', 'Motor Neuron Disease', 'Movement', 'Multiple Sclerosis', 'Muscle', 'National Institute of Neurological Disorders and Stroke', 'Neurologist', 'Neurology', 'Oral', 'Outcome Measure', 'Parkinson Disease', 'Participant', 'Pathologist', 'Pathway interactions', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Severities', 'Speech', 'Speech Disorders', 'Speech Intelligibility', 'Speech Synthesizers', 'Speech-Language Pathology', 'Staging', 'Stroke', 'System', 'Technology', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Validation', 'Veterans', 'Wit', 'Work', 'base', 'clinical decision-making', 'clinical practice', 'data mining', 'diagnostic accuracy', 'digital', 'forging', 'improved', 'innovation', 'jaw movement', 'motor impairment', 'novel', 'novel strategies', 'oral communication', 'orofacial', 'prognostic', 'public health relevance', 'research study', 'socioeconomics', 'statistics', 'tool']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,R01,2014,642629,0.24384479847069565
"Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication     DESCRIPTION (provided by applicant): The goal of this project is to test the efficacy of a silent speech interface (SSI) as an alternative mode of oral communication for persons who are unable to use their voice (e.g., after laryngectomy, surgical removal of larynx due to the treatment of cancer). We have recently developed a real-time, interactive SSI based on a commercial electromagnetic articulograph. The SSI converts tongue and lip movement to text, and then plays back corresponding synthesized speech sounds with natural sounding voice in real-time. The SSI has potential to restore the patient's voice by identifying the patient's voice characteristics before laryngectomy. Preliminary tests on healthy participants demonstrated the feasibility of the SSI. In this project, we further evaluate the system by studying 15 participants after laryngectomy and 15 age- and gender-matched healthy controls. If successful, the SSI has the potential to transform clinical practice for speech-language pathologists other related health care professionals. The proposed research will enhance human health by making an impact on individuals after laryngectomy and potentially to a broad range of other speech and voice disorders. PUBLIC HEALTH RELEVANCE: Silent speech interfaces (SSIs) are a novel alternative mode of oral communication for persons who are unable to produce speech sounds (e.g., individuals who undergo a laryngectomy, removal of larynx due to the treatment of laryngeal cancer). SSIs recognize speech sounds from articulatory data and then drive text-to-speech synthesis, which produces speech with natural sounding voice, which is one of the advantages over the current treatment options for these individuals. SSIs hold potential to even restore the patient's own voice by identifying the patient's voice characteristics before laryngectomy. Although participants after laryngectomy will be the test case in this project, the clinical implications of SSIs extend to a larger population of persons with other speech and voice disorders.",Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication,9114061,R03DC013990,"['Address', 'Age', 'Algorithms', 'American Cancer Society', 'Back', 'Characteristics', 'Clinical', 'Data', 'Devices', 'Diagnosis', 'Electromagnetics', 'Equilibrium', 'Excision', 'Future', 'Gender', 'Goals', 'Health', 'Health Professional', 'Human', 'Individual', 'Joints', 'Language', 'Laryngeal Prosthesis', 'Laryngectomy', 'Larynx', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Motor', 'Movement', 'Operative Surgical Procedures', 'Output', 'Participant', 'Pathologist', 'Patients', 'Pattern', 'Performance', 'Persons', 'Play', 'Population', 'Research', 'Self-Help Devices', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'System', 'Testing', 'Text', 'Time', 'Tongue', 'Tracheoesophageal Speech', 'United States', 'Voice', 'Voice Disorders', 'base', 'cancer therapy', 'clinical practice', 'computer science', 'efficacy testing', 'improved', 'innovative technologies', 'kinematics', 'movement analysis', 'novel', 'oral communication', 'sound']",NIDCD,UNIVERSITY OF TEXAS DALLAS,R03,2016,153000,0.25181917821118155
"Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication     DESCRIPTION (provided by applicant): The goal of this project is to test the efficacy of a silent speech interface (SSI) as an alternative mode of oral communication for persons who are unable to use their voice (e.g., after laryngectomy, surgical removal of larynx due to the treatment of cancer). We have recently developed a real-time, interactive SSI based on a commercial electromagnetic articulograph. The SSI converts tongue and lip movement to text, and then plays back corresponding synthesized speech sounds with natural sounding voice in real-time. The SSI has potential to restore the patient's voice by identifying the patient's voice characteristics before laryngectomy. Preliminary tests on healthy participants demonstrated the feasibility of the SSI. In this project, we further evaluate the system by studying 15 participants after laryngectomy and 15 age- and gender-matched healthy controls. If successful, the SSI has the potential to transform clinical practice for speech-language pathologists other related health care professionals. The proposed research will enhance human health by making an impact on individuals after laryngectomy and potentially to a broad range of other speech and voice disorders.         PUBLIC HEALTH RELEVANCE: Silent speech interfaces (SSIs) are a novel alternative mode of oral communication for persons who are unable to produce speech sounds (e.g., individuals who undergo a laryngectomy, removal of larynx due to the treatment of laryngeal cancer). SSIs recognize speech sounds from articulatory data and then drive text-to-speech synthesis, which produces speech with natural sounding voice, which is one of the advantages over the current treatment options for these individuals. SSIs hold potential to even restore the patient's own voice by identifying the patient's voice characteristics before laryngectomy. Although participants after laryngectomy will be the test case in this project, the clinical implications of SSIs extend to a larger population of persons with other speech and voice disorders.                ",Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication,8957652,R03DC013990,"['Address', 'Age', 'Algorithms', 'American Cancer Society', 'Back', 'Characteristics', 'Clinical', 'Data', 'Devices', 'Diagnosis', 'Electromagnetics', 'Equilibrium', 'Excision', 'Future', 'Gender', 'Goals', 'Health', 'Health Professional', 'Human', 'Individual', 'Joints', 'Language', 'Laryngeal Prosthesis', 'Laryngectomy', 'Larynx', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Motor', 'Movement', 'Operative Surgical Procedures', 'Output', 'Participant', 'Pathologist', 'Patients', 'Pattern', 'Performance', 'Persons', 'Play', 'Population', 'Research', 'Self-Help Devices', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'System', 'Testing', 'Text', 'Time', 'Tongue', 'Tracheoesophageal Speech', 'United States', 'Voice', 'Voice Disorders', 'base', 'cancer therapy', 'clinical practice', 'computer science', 'efficacy testing', 'improved', 'innovative technologies', 'kinematics', 'movement analysis', 'novel', 'oral communication', 'public health relevance', 'sound']",NIDCD,UNIVERSITY OF TEXAS DALLAS,R03,2015,153000,0.25181917821118155
"Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication     DESCRIPTION (provided by applicant): The goal of this project is to test the efficacy of a silent speech interface (SSI) as an alternative mode of oral communication for persons who are unable to use their voice (e.g., after laryngectomy, surgical removal of larynx due to the treatment of cancer). We have recently developed a real-time, interactive SSI based on a commercial electromagnetic articulograph. The SSI converts tongue and lip movement to text, and then plays back corresponding synthesized speech sounds with natural sounding voice in real-time. The SSI has potential to restore the patient's voice by identifying the patient's voice characteristics before laryngectomy. Preliminary tests on healthy participants demonstrated the feasibility of the SSI. In this project, we further evaluate the system by studying 15 participants after laryngectomy and 15 age- and gender-matched healthy controls. If successful, the SSI has the potential to transform clinical practice for speech-language pathologists other related health care professionals. The proposed research will enhance human health by making an impact on individuals after laryngectomy and potentially to a broad range of other speech and voice disorders. PUBLIC HEALTH RELEVANCE: Silent speech interfaces (SSIs) are a novel alternative mode of oral communication for persons who are unable to produce speech sounds (e.g., individuals who undergo a laryngectomy, removal of larynx due to the treatment of laryngeal cancer). SSIs recognize speech sounds from articulatory data and then drive text-to-speech synthesis, which produces speech with natural sounding voice, which is one of the advantages over the current treatment options for these individuals. SSIs hold potential to even restore the patient's own voice by identifying the patient's voice characteristics before laryngectomy. Although participants after laryngectomy will be the test case in this project, the clinical implications of SSIs extend to a larger population of persons with other speech and voice disorders.",Real-Time Articulation-to-Speech Mapping for Enhancing Impaired Oral Communication,9319226,R03DC013990,"['Address', 'Age', 'Algorithms', 'American Cancer Society', 'Articulation', 'Back', 'Characteristics', 'Clinical', 'Data', 'Devices', 'Diagnosis', 'Electromagnetics', 'Excision', 'Future', 'Gender', 'Goals', 'Health', 'Health Professional', 'Human', 'Impairment', 'Individual', 'Language', 'Laryngeal Prosthesis', 'Laryngectomy', 'Larynx', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Motor', 'Movement', 'Operative Surgical Procedures', 'Output', 'Participant', 'Pathologist', 'Patients', 'Pattern', 'Performance', 'Persons', 'Play', 'Population', 'Research', 'Self-Help Devices', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'System', 'Testing', 'Text', 'Time', 'Tongue', 'Tracheoesophageal Speech', 'United States', 'Voice', 'Voice Disorders', 'base', 'cancer therapy', 'clinical practice', 'computer science', 'efficacy testing', 'improved', 'innovative technologies', 'kinematics', 'movement analysis', 'novel', 'oral communication', 'public health relevance', 'sound']",NIDCD,UNIVERSITY OF TEXAS DALLAS,R03,2017,153000,0.25181917821118155
"Automatic Voice-Based Assessment of Language Abilities     DESCRIPTION (provided by applicant): Since untreated language disorder - a disorder with a prevalence of at least 7% - can lead to serious behavioral and educational problems, large-scale early language assessment is urgently needed not only for early identification of language disorder but also for planning interventions and tracking progress. This is all the more so because a recent study found that 71% of children diagnosed with Specific Language Impairment (a type of language disorder) had not been previously identified. However, such large-scale efforts would pose a large burden on professional staff and on other scarce resources. As a result, clinicians, educators, and researchers have argued for the use of computer based assessment. Recently, progress has been made with computer based language assessment, but it has been limited to language comprehension (i.e., receptive vocabulary and grammar). Thus, computer based assessment of language production that is expressive language and particularly discourse skills, is still lacking. One contributing factor is that a key technology needed for this, Automatic Speech Recognition (ASR), is perceived as inadequate for accurate scoring of language tests since even the best ASR systems have word error rates in excess of 20%. However, this perception is based on a limited perspective of how ASR can be used for assessment, in which a general- purpose ASR system provides an (often inaccurate) transcript of the child's speech, which then would be scored automatically according to conventional rules. We take an alternative perspective, and propose an innovative approach that comprises two core concepts. The first is that of creating special-purpose, test-specific ASR systems whose search space is carefully matched to the space of responses a test may elicit. The second is that of integrating these systems with machine-learning based scoring algorithms whereby the latter operate not on the final, ""best"" transcript generated by the ASR system but on the rich layers of intermediate representations that the ASR system computes in the process of recognizing the input speech (""rich representation""). Earlier experiments in our lab with digit and narrative recall tests have demonstrated the feasibility of this approach. In the proposed project we will create computer-based scoring and test administration systems for tests in the expressive modality as well as in the vocabulary, grammar, and discourse domains; we will also create a system for a non-word repetition test. The systems will be applied to a diverse group of 300 children ages 3-9 with typical development and with neurodevelopmental disorders, and will be validated against conventional language measures. The automated language tests developed in the project cover core diagnostic criteria for language disorders but also create a technological foundation for the computerization of a much broader array of tests for voice based language and cognitive assessment. PUBLIC HEALTH RELEVANCE: There is a significant need for language assessment for early detection, diagnosis, screening, and progress tracking of language difficulties. However, assessment involves face-to-face sessions with a professional, which may not always be available and affordable. The project goal is to provide a technology solution, by designing, implementing, and evaluating computer-based systems for automated voice-based language assessment (both test administration and test scoring) for narrative recall, picture naming, sentence repetition, sentence completion, and nonword repetition.",Automatic Voice-Based Assessment of Language Abilities,9191358,R01DC013996,"['Adult', 'Age', 'Algorithms', 'American', 'American Sign Language', 'Assessment tool', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavioral', 'Characteristics', 'Child', 'Clinical', 'Communication', 'Comprehension', 'Computer Systems', 'Computers', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Digit structure', 'Disease', 'Early Diagnosis', 'Early identification', 'Emotional', 'Ensure', 'Face', 'Foundations', 'Friends', 'Funding', 'Goals', 'Hearing', 'High Prevalence', 'Impairment', 'Individual', 'Intervention', 'Language', 'Language Disorders', 'Language Tests', 'Lead', 'Learning', 'Machine Learning', 'Manuals', 'Masks', 'Measures', 'Methods', 'Modality', 'Morphology', 'Names', 'National Institute on Deafness and Other Communication Disorders', 'Natural Language Processing', 'Neurodevelopmental Disorder', 'Parents', 'Perception', 'Performance', 'Policies', 'Prevalence', 'Privatization', 'Process', 'Production', 'Quality of life', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Scoring Method', 'Semantics', 'Services', 'Societies', 'Speech', 'Supervision', 'System', 'Technology', 'Testing', 'Transcript', 'Translating', 'Vocabulary', 'Voice', 'autism spectrum disorder', 'base', 'cognitive testing', 'computerized', 'cost', 'design', 'experimental study', 'follow up assessment', 'innovation', 'innovative technologies', 'language comprehension', 'language disorder diagnosis', 'phonology', 'psychiatric symptom', 'public health relevance', 'response', 'school district', 'screening', 'service intervention', 'skills', 'social communication', 'specific language impairment', 'speech recognition', 'syntax', 'tool']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2017,602469,0.0882501657951834
"Automatic Voice-Based Assessment of Language Abilities     DESCRIPTION (provided by applicant): Since untreated language disorder - a disorder with a prevalence of at least 7% - can lead to serious behavioral and educational problems, large-scale early language assessment is urgently needed not only for early identification of language disorder but also for planning interventions and tracking progress. This is all the more so because a recent study found that 71% of children diagnosed with Specific Language Impairment (a type of language disorder) had not been previously identified. However, such large-scale efforts would pose a large burden on professional staff and on other scarce resources. As a result, clinicians, educators, and researchers have argued for the use of computer based assessment. Recently, progress has been made with computer based language assessment, but it has been limited to language comprehension (i.e., receptive vocabulary and grammar). Thus, computer based assessment of language production that is expressive language and particularly discourse skills, is still lacking. One contributing factor is that a key technology needed for this, Automatic Speech Recognition (ASR), is perceived as inadequate for accurate scoring of language tests since even the best ASR systems have word error rates in excess of 20%. However, this perception is based on a limited perspective of how ASR can be used for assessment, in which a general- purpose ASR system provides an (often inaccurate) transcript of the child's speech, which then would be scored automatically according to conventional rules. We take an alternative perspective, and propose an innovative approach that comprises two core concepts. The first is that of creating special-purpose, test-specific ASR systems whose search space is carefully matched to the space of responses a test may elicit. The second is that of integrating these systems with machine-learning based scoring algorithms whereby the latter operate not on the final, ""best"" transcript generated by the ASR system but on the rich layers of intermediate representations that the ASR system computes in the process of recognizing the input speech (""rich representation""). Earlier experiments in our lab with digit and narrative recall tests have demonstrated the feasibility of this approach. In the proposed project we will create computer-based scoring and test administration systems for tests in the expressive modality as well as in the vocabulary, grammar, and discourse domains; we will also create a system for a non-word repetition test. The systems will be applied to a diverse group of 300 children ages 3-9 with typical development and with neurodevelopmental disorders, and will be validated against conventional language measures. The automated language tests developed in the project cover core diagnostic criteria for language disorders but also create a technological foundation for the computerization of a much broader array of tests for voice based language and cognitive assessment.         PUBLIC HEALTH RELEVANCE: There is a significant need for language assessment for early detection, diagnosis, screening, and progress tracking of language difficulties. However, assessment involves face-to-face sessions with a professional, which may not always be available and affordable. The project goal is to provide a technology solution, by designing, implementing, and evaluating computer-based systems for automated voice-based language assessment (both test administration and test scoring) for narrative recall, picture naming, sentence repetition, sentence completion, and nonword repetition.                ",Automatic Voice-Based Assessment of Language Abilities,9020029,R01DC013996,"['Adult', 'Age', 'Algorithms', 'American', 'Assessment tool', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavioral', 'Characteristics', 'Child', 'Clinical', 'Communication', 'Comprehension', 'Computer Systems', 'Computers', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Digit structure', 'Disease', 'Early Diagnosis', 'Early identification', 'Emotional', 'Ensure', 'Face', 'Foundations', 'Friends', 'Funding', 'Goals', 'Hearing', 'High Prevalence', 'Impairment', 'Individual', 'Intervention', 'Language', 'Language Disorders', 'Language Tests', 'Lead', 'Learning', 'Machine Learning', 'Manuals', 'Masks', 'Measures', 'Methods', 'Modality', 'Morphology', 'Names', 'National Institute on Deafness and Other Communication Disorders', 'Natural Language Processing', 'Neurodevelopmental Disorder', 'Only Child', 'Parents', 'Perception', 'Performance', 'Policies', 'Prevalence', 'Process', 'Production', 'Quality of life', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Scoring Method', 'Semantics', 'Services', 'Sign Language', 'Societies', 'Speech', 'Supervision', 'System', 'Technology', 'Testing', 'Transcript', 'Translating', 'Vocabulary', 'Voice', 'Writing', 'autism spectrum disorder', 'base', 'cognitive testing', 'computerized', 'cost', 'design', 'follow-up', 'innovation', 'innovative technologies', 'language comprehension', 'language disorder diagnosis', 'phonology', 'psychiatric symptom', 'public health relevance', 'research study', 'response', 'school district', 'screening', 'service intervention', 'skills', 'social communication', 'specific language impairment', 'speech recognition', 'syntax', 'tool']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2016,638494,0.0882501657951834
"Automatic Voice-Based Assessment of Language Abilities     DESCRIPTION (provided by applicant): Since untreated language disorder - a disorder with a prevalence of at least 7% - can lead to serious behavioral and educational problems, large-scale early language assessment is urgently needed not only for early identification of language disorder but also for planning interventions and tracking progress. This is all the more so because a recent study found that 71% of children diagnosed with Specific Language Impairment (a type of language disorder) had not been previously identified. However, such large-scale efforts would pose a large burden on professional staff and on other scarce resources. As a result, clinicians, educators, and researchers have argued for the use of computer based assessment. Recently, progress has been made with computer based language assessment, but it has been limited to language comprehension (i.e., receptive vocabulary and grammar). Thus, computer based assessment of language production that is expressive language and particularly discourse skills, is still lacking. One contributing factor is that a key technology needed for this, Automatic Speech Recognition (ASR), is perceived as inadequate for accurate scoring of language tests since even the best ASR systems have word error rates in excess of 20%. However, this perception is based on a limited perspective of how ASR can be used for assessment, in which a general- purpose ASR system provides an (often inaccurate) transcript of the child's speech, which then would be scored automatically according to conventional rules. We take an alternative perspective, and propose an innovative approach that comprises two core concepts. The first is that of creating special-purpose, test-specific ASR systems whose search space is carefully matched to the space of responses a test may elicit. The second is that of integrating these systems with machine-learning based scoring algorithms whereby the latter operate not on the final, ""best"" transcript generated by the ASR system but on the rich layers of intermediate representations that the ASR system computes in the process of recognizing the input speech (""rich representation""). Earlier experiments in our lab with digit and narrative recall tests have demonstrated the feasibility of this approach. In the proposed project we will create computer-based scoring and test administration systems for tests in the expressive modality as well as in the vocabulary, grammar, and discourse domains; we will also create a system for a non-word repetition test. The systems will be applied to a diverse group of 300 children ages 3-9 with typical development and with neurodevelopmental disorders, and will be validated against conventional language measures. The automated language tests developed in the project cover core diagnostic criteria for language disorders but also create a technological foundation for the computerization of a much broader array of tests for voice based language and cognitive assessment. PUBLIC HEALTH RELEVANCE: There is a significant need for language assessment for early detection, diagnosis, screening, and progress tracking of language difficulties. However, assessment involves face-to-face sessions with a professional, which may not always be available and affordable. The project goal is to provide a technology solution, by designing, implementing, and evaluating computer-based systems for automated voice-based language assessment (both test administration and test scoring) for narrative recall, picture naming, sentence repetition, sentence completion, and nonword repetition.",Automatic Voice-Based Assessment of Language Abilities,9825536,R01DC013996,"['Adult', 'Age', 'Algorithms', 'American', 'American Sign Language', 'Assessment tool', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavioral', 'Characteristics', 'Child', 'Clinical', 'Communication', 'Comprehension', 'Computer Systems', 'Computers', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Digit structure', 'Disease', 'Early Diagnosis', 'Early identification', 'Emotional', 'Ensure', 'Face', 'Foundations', 'Friends', 'Funding', 'Goals', 'Hearing', 'High Prevalence', 'Impairment', 'Individual', 'Intervention', 'Language', 'Language Disorders', 'Language Tests', 'Lead', 'Learning', 'Machine Learning', 'Manuals', 'Masks', 'Measures', 'Methods', 'Modality', 'Morphology', 'Names', 'National Institute on Deafness and Other Communication Disorders', 'Natural Language Processing', 'Neurodevelopmental Disorder', 'Parents', 'Perception', 'Performance', 'Policies', 'Prevalence', 'Privatization', 'Process', 'Production', 'Quality of life', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Scoring Method', 'Semantics', 'Services', 'Societies', 'Speech', 'Supervision', 'System', 'Technology', 'Testing', 'Transcript', 'Translating', 'Vocabulary', 'Voice', 'autism spectrum disorder', 'automated speech recognition', 'base', 'cognitive testing', 'computerized', 'cost', 'design', 'experimental study', 'follow up assessment', 'innovation', 'innovative technologies', 'language comprehension', 'language disorder diagnosis', 'phonology', 'psychiatric symptom', 'public health relevance', 'response', 'school district', 'screening', 'service intervention', 'skills', 'social communication', 'specific language impairment', 'syntax', 'tool']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2020,574747,0.0882501657951834
"Automatic Voice-Based Assessment of Language Abilities     DESCRIPTION (provided by applicant): Since untreated language disorder - a disorder with a prevalence of at least 7% - can lead to serious behavioral and educational problems, large-scale early language assessment is urgently needed not only for early identification of language disorder but also for planning interventions and tracking progress. This is all the more so because a recent study found that 71% of children diagnosed with Specific Language Impairment (a type of language disorder) had not been previously identified. However, such large-scale efforts would pose a large burden on professional staff and on other scarce resources. As a result, clinicians, educators, and researchers have argued for the use of computer based assessment. Recently, progress has been made with computer based language assessment, but it has been limited to language comprehension (i.e., receptive vocabulary and grammar). Thus, computer based assessment of language production that is expressive language and particularly discourse skills, is still lacking. One contributing factor is that a key technology needed for this, Automatic Speech Recognition (ASR), is perceived as inadequate for accurate scoring of language tests since even the best ASR systems have word error rates in excess of 20%. However, this perception is based on a limited perspective of how ASR can be used for assessment, in which a general- purpose ASR system provides an (often inaccurate) transcript of the child's speech, which then would be scored automatically according to conventional rules. We take an alternative perspective, and propose an innovative approach that comprises two core concepts. The first is that of creating special-purpose, test-specific ASR systems whose search space is carefully matched to the space of responses a test may elicit. The second is that of integrating these systems with machine-learning based scoring algorithms whereby the latter operate not on the final, ""best"" transcript generated by the ASR system but on the rich layers of intermediate representations that the ASR system computes in the process of recognizing the input speech (""rich representation""). Earlier experiments in our lab with digit and narrative recall tests have demonstrated the feasibility of this approach. In the proposed project we will create computer-based scoring and test administration systems for tests in the expressive modality as well as in the vocabulary, grammar, and discourse domains; we will also create a system for a non-word repetition test. The systems will be applied to a diverse group of 300 children ages 3-9 with typical development and with neurodevelopmental disorders, and will be validated against conventional language measures. The automated language tests developed in the project cover core diagnostic criteria for language disorders but also create a technological foundation for the computerization of a much broader array of tests for voice based language and cognitive assessment. PUBLIC HEALTH RELEVANCE: There is a significant need for language assessment for early detection, diagnosis, screening, and progress tracking of language difficulties. However, assessment involves face-to-face sessions with a professional, which may not always be available and affordable. The project goal is to provide a technology solution, by designing, implementing, and evaluating computer-based systems for automated voice-based language assessment (both test administration and test scoring) for narrative recall, picture naming, sentence repetition, sentence completion, and nonword repetition.",Automatic Voice-Based Assessment of Language Abilities,9390046,R01DC013996,"['Adult', 'Age', 'Algorithms', 'American', 'American Sign Language', 'Assessment tool', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavioral', 'Characteristics', 'Child', 'Clinical', 'Communication', 'Comprehension', 'Computer Systems', 'Computers', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Digit structure', 'Disease', 'Early Diagnosis', 'Early identification', 'Emotional', 'Ensure', 'Face', 'Foundations', 'Friends', 'Funding', 'Goals', 'Hearing', 'High Prevalence', 'Impairment', 'Individual', 'Intervention', 'Language', 'Language Disorders', 'Language Tests', 'Lead', 'Learning', 'Machine Learning', 'Manuals', 'Masks', 'Measures', 'Methods', 'Modality', 'Morphology', 'Names', 'National Institute on Deafness and Other Communication Disorders', 'Natural Language Processing', 'Neurodevelopmental Disorder', 'Parents', 'Perception', 'Performance', 'Policies', 'Prevalence', 'Privatization', 'Process', 'Production', 'Quality of life', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Scoring Method', 'Semantics', 'Services', 'Societies', 'Speech', 'Supervision', 'System', 'Technology', 'Testing', 'Transcript', 'Translating', 'Vocabulary', 'Voice', 'autism spectrum disorder', 'base', 'cognitive testing', 'computerized', 'cost', 'design', 'experimental study', 'follow up assessment', 'innovation', 'innovative technologies', 'language comprehension', 'language disorder diagnosis', 'phonology', 'psychiatric symptom', 'public health relevance', 'response', 'school district', 'screening', 'service intervention', 'skills', 'social communication', 'specific language impairment', 'speech recognition', 'syntax', 'tool']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2018,578454,0.0882501657951834
"Automatic Voice-Based Assessment of Language Abilities     DESCRIPTION (provided by applicant): Since untreated language disorder - a disorder with a prevalence of at least 7% - can lead to serious behavioral and educational problems, large-scale early language assessment is urgently needed not only for early identification of language disorder but also for planning interventions and tracking progress. This is all the more so because a recent study found that 71% of children diagnosed with Specific Language Impairment (a type of language disorder) had not been previously identified. However, such large-scale efforts would pose a large burden on professional staff and on other scarce resources. As a result, clinicians, educators, and researchers have argued for the use of computer based assessment. Recently, progress has been made with computer based language assessment, but it has been limited to language comprehension (i.e., receptive vocabulary and grammar). Thus, computer based assessment of language production that is expressive language and particularly discourse skills, is still lacking. One contributing factor is that a key technology needed for this, Automatic Speech Recognition (ASR), is perceived as inadequate for accurate scoring of language tests since even the best ASR systems have word error rates in excess of 20%. However, this perception is based on a limited perspective of how ASR can be used for assessment, in which a general- purpose ASR system provides an (often inaccurate) transcript of the child's speech, which then would be scored automatically according to conventional rules. We take an alternative perspective, and propose an innovative approach that comprises two core concepts. The first is that of creating special-purpose, test-specific ASR systems whose search space is carefully matched to the space of responses a test may elicit. The second is that of integrating these systems with machine-learning based scoring algorithms whereby the latter operate not on the final, ""best"" transcript generated by the ASR system but on the rich layers of intermediate representations that the ASR system computes in the process of recognizing the input speech (""rich representation""). Earlier experiments in our lab with digit and narrative recall tests have demonstrated the feasibility of this approach. In the proposed project we will create computer-based scoring and test administration systems for tests in the expressive modality as well as in the vocabulary, grammar, and discourse domains; we will also create a system for a non-word repetition test. The systems will be applied to a diverse group of 300 children ages 3-9 with typical development and with neurodevelopmental disorders, and will be validated against conventional language measures. The automated language tests developed in the project cover core diagnostic criteria for language disorders but also create a technological foundation for the computerization of a much broader array of tests for voice based language and cognitive assessment. PUBLIC HEALTH RELEVANCE: There is a significant need for language assessment for early detection, diagnosis, screening, and progress tracking of language difficulties. However, assessment involves face-to-face sessions with a professional, which may not always be available and affordable. The project goal is to provide a technology solution, by designing, implementing, and evaluating computer-based systems for automated voice-based language assessment (both test administration and test scoring) for narrative recall, picture naming, sentence repetition, sentence completion, and nonword repetition.",Automatic Voice-Based Assessment of Language Abilities,9600692,R01DC013996,"['Adult', 'Age', 'Algorithms', 'American', 'American Sign Language', 'Assessment tool', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavioral', 'Characteristics', 'Child', 'Clinical', 'Communication', 'Comprehension', 'Computer Systems', 'Computers', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Digit structure', 'Disease', 'Early Diagnosis', 'Early identification', 'Emotional', 'Ensure', 'Face', 'Foundations', 'Friends', 'Funding', 'Goals', 'Hearing', 'High Prevalence', 'Impairment', 'Individual', 'Intervention', 'Language', 'Language Disorders', 'Language Tests', 'Lead', 'Learning', 'Machine Learning', 'Manuals', 'Masks', 'Measures', 'Methods', 'Modality', 'Morphology', 'Names', 'National Institute on Deafness and Other Communication Disorders', 'Natural Language Processing', 'Neurodevelopmental Disorder', 'Parents', 'Perception', 'Performance', 'Policies', 'Prevalence', 'Privatization', 'Process', 'Production', 'Quality of life', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Scoring Method', 'Semantics', 'Services', 'Societies', 'Speech', 'Supervision', 'System', 'Technology', 'Testing', 'Transcript', 'Translating', 'Vocabulary', 'Voice', 'autism spectrum disorder', 'automated speech recognition', 'base', 'cognitive testing', 'computerized', 'cost', 'design', 'experimental study', 'follow up assessment', 'innovation', 'innovative technologies', 'language comprehension', 'language disorder diagnosis', 'phonology', 'psychiatric symptom', 'public health relevance', 'response', 'school district', 'screening', 'service intervention', 'skills', 'social communication', 'specific language impairment', 'syntax', 'tool']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2019,568221,0.0882501657951834
"Neurophysiology of robust speech perception in human superior temporal gyrus DESCRIPTION (provided by applicant): Perceiving and following an individual speaker in a crowded, noisy environment is a commonplace task for listeners with normal hearing. The underlying neurophysiology, however, is complex, and the task remains a struggle for people with peripheral and central auditory pathway disorders. The lack of a detailed neurobiological model of mechanisms and functions underlying robust speech perception has hindered our understanding of how these processes become impaired in the suffering population. In our innovative approach, we will record from high-density micro and macro electrode arrays surgically implanted on the superior temporal gyrus of epilepsy patients as part of their clinical evaluation. This method offers an exceptionally detailed perspective of cortical population activity. We will build upon two recent complementary findings where we identified a highly selective, spatially distributed neural representation of phonetic features (Mesgarani et. al. Science, 2014), which at the same time is highly dynamic and can change rapidly to reflect the perceptual bias of the listener (Mesgarani & Chang, Nature 2012). While significant, these studies revealed several gaps in our understanding of this process, which we intend to address in this proposal. Specifically, we will resolve the following unanswered questions: 1) what is the neural mechanism for joint encoding of both phonetic and speaker features? 2) How does attention modulate phonetic and speaker feature selectivity of neural responses? And 3) what computational mechanisms can account for dynamic feature selectivity of responses in STG? Answering these questions will significantly advance our understanding of a remarkable human ability, and will be of great interest to researchers from many areas including neurologists, and sensory and cognitive neuroscientists. PUBLIC HEALTH RELEVANCE: Understanding the mechanisms underlying speech perception in challenging environments is a crucial step in determining how these processes deteriorate in various disorders of peripheral and central auditory pathways. Our studies will result in novel neurobiological models of robust speech perception that will serve as a necessary step toward designing innovative therapeutic measures.",Neurophysiology of robust speech perception in human superior temporal gyrus,9650571,R01DC014279,"['Acoustics', 'Address', 'Aphasia', 'Area', 'Attention', 'Brain', 'Central Auditory Diseases', 'Characteristics', 'Code', 'Cognitive', 'Communication impairment', 'Complex', 'Comprehension', 'Computer Simulation', 'Crowding', 'Cues', 'Custom', 'Discipline', 'Disease', 'Dyslexia', 'Electrodes', 'Engineering', 'Environment', 'Epilepsy', 'Feedback', 'Human', 'Impairment', 'Implant', 'Individual', 'Intention', 'Joints', 'Language Development', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Nature', 'Neurobiology', 'Neurologist', 'Neurons', 'Operative Surgical Procedures', 'Patients', 'Perception', 'Peripheral', 'Population', 'Process', 'Property', 'Prosthesis', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Science', 'Sensory', 'Signal Transduction', 'Site', 'Specificity', 'Speech', 'Speech Pathology', 'Speech Perception', 'Superior temporal gyrus', 'Surface', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Voice', 'Work', 'attentional modulation', 'auditory pathway', 'cognitive ability', 'computer framework', 'density', 'design', 'expectation', 'experimental study', 'indexing', 'innovation', 'interest', 'learning strategy', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'normal hearing', 'novel', 'public health relevance', 'receptive field', 'relating to nervous system', 'research clinical testing', 'response', 'selective attention', 'spatiotemporal', 'speech processing']",NIDCD,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2019,398995,0.23747334998948513
"Neurophysiology of robust speech perception in human superior temporal gyrus DESCRIPTION (provided by applicant): Perceiving and following an individual speaker in a crowded, noisy environment is a commonplace task for listeners with normal hearing. The underlying neurophysiology, however, is complex, and the task remains a struggle for people with peripheral and central auditory pathway disorders. The lack of a detailed neurobiological model of mechanisms and functions underlying robust speech perception has hindered our understanding of how these processes become impaired in the suffering population. In our innovative approach, we will record from high-density micro and macro electrode arrays surgically implanted on the superior temporal gyrus of epilepsy patients as part of their clinical evaluation. This method offers an exceptionally detailed perspective of cortical population activity. We will build upon two recent complementary findings where we identified a highly selective, spatially distributed neural representation of phonetic features (Mesgarani et. al. Science, 2014), which at the same time is highly dynamic and can change rapidly to reflect the perceptual bias of the listener (Mesgarani & Chang, Nature 2012). While significant, these studies revealed several gaps in our understanding of this process, which we intend to address in this proposal. Specifically, we will resolve the following unanswered questions: 1) what is the neural mechanism for joint encoding of both phonetic and speaker features? 2) How does attention modulate phonetic and speaker feature selectivity of neural responses? And 3) what computational mechanisms can account for dynamic feature selectivity of responses in STG? Answering these questions will significantly advance our understanding of a remarkable human ability, and will be of great interest to researchers from many areas including neurologists, and sensory and cognitive neuroscientists. PUBLIC HEALTH RELEVANCE: Understanding the mechanisms underlying speech perception in challenging environments is a crucial step in determining how these processes deteriorate in various disorders of peripheral and central auditory pathways. Our studies will result in novel neurobiological models of robust speech perception that will serve as a necessary step toward designing innovative therapeutic measures.",Neurophysiology of robust speech perception in human superior temporal gyrus,9231432,R01DC014279,"['Acoustics', 'Address', 'Aphasia', 'Area', 'Attention', 'Brain', 'Central Auditory Diseases', 'Characteristics', 'Code', 'Cognitive', 'Communication impairment', 'Complex', 'Comprehension', 'Computer Simulation', 'Crowding', 'Cues', 'Custom', 'Discipline', 'Disease', 'Dyslexia', 'Electrodes', 'Engineering', 'Environment', 'Epilepsy', 'Feedback', 'Hearing', 'Human', 'Impairment', 'Implant', 'Individual', 'Intention', 'Joints', 'Language Development', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Nature', 'Neurobiology', 'Neurologist', 'Neurons', 'Operative Surgical Procedures', 'Patients', 'Perception', 'Peripheral', 'Population', 'Process', 'Property', 'Prosthesis', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Science', 'Sensory', 'Signal Transduction', 'Site', 'Specificity', 'Speech', 'Speech Pathology', 'Speech Perception', 'Superior temporal gyrus', 'Surface', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Voice', 'Work', 'attentional modulation', 'auditory pathway', 'cognitive ability', 'computer framework', 'density', 'design', 'expectation', 'experimental study', 'indexing', 'innovation', 'interest', 'learning strategy', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'novel', 'public health relevance', 'receptive field', 'relating to nervous system', 'research clinical testing', 'response', 'selective attention', 'spatiotemporal', 'speech processing']",NIDCD,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2017,396360,0.23747334998948513
"Functional and computational characterization of the human auditory cortex According to NIDCD, 6 to 8 million people in the United States have some form of speech or communication disorder. Speech perception requires a listener to map variable acoustic signals onto a finite set of phonological categories known as phonemes, and to integrate those categories over time to form larger linguistic units such as syllables and words. It remains speculative where these different speech features are encoded and what cortical computations are needed for their calculation from an acoustic signal. A better understanding of what neural circuits are involved, how they are organized, and what computations they perform to support speech comprehension is critical for developing a detailed neurobiological model of speech perception. The major aim of this proposal is to use a joint framework to study the encoding of acoustic and linguistic features and the computational underpinnings of natural speech processing, using invasive surface and depth electrodes implanted in human neurosurgical patients. To study the cortical organization of acoustic features, we will characterize the encoding and anatomical organization of acoustic features in auditory cortical regions. To study the cortical organization of linguistic features, we will measure the encoding of phonetic, phonotactic, and semantic information using multivariate linear regression. To understand the underlying computational mechanisms, we will train convolutional neural network models to predict the neural responses to speech and use a novel method to express their computation as a set of linear transforms. By interpreting these models, we will uncover nonlinear computations used in different auditory areas and relate them to the encoding of acoustic and linguistic features. These complementary analyses will extend our knowledge of speech processing in the human auditory cortex and lead to new hypotheses about the mechanisms of various speech and language disorders. Together, the proposed research will greatly improve the current models of cortical speech processing, which are of great interest in many disciplines including neurolinguistics, speech pathology, speech prostheses, and speech technologies. Speech and language disorders are major health issues. Speech perception requires a listener to compute linguistic units from variable acoustic signals, and where and how these computations happen in the human auditory cortex remains speculative. Using invasive human electrophysiology, we propose to study the neural encoding and computational underpinnings of the acoustic and linguistic features that enable speech perception; investigating this process in various auditory areas at high resolution will extend our knowledge of human speech perception and produce new insights into the mechanisms of speech and language disorders.",Functional and computational characterization of the human auditory cortex,9973774,R01DC014279,"['Acoustics', 'Address', 'Anatomy', 'Animal Model', 'Aphasia', 'Area', 'Auditory', 'Auditory Perception', 'Auditory area', 'Brain', 'Categories', 'Characteristics', 'Communication impairment', 'Comprehension', 'Computer Models', 'Discipline', 'Dyslexia', 'Electrocorticogram', 'Electroencephalography', 'Electrophysiology (science)', 'Frequencies', 'Health', 'Human', 'Impairment', 'Implanted Electrodes', 'Intuition', 'Joints', 'Knowledge', 'Language Development', 'Language Disorders', 'Lead', 'Linear Regressions', 'Linguistics', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Neural Network Simulation', 'Neurobiology', 'Patients', 'Process', 'Property', 'Prosthesis', 'Research', 'Resolution', 'Response Latencies', 'Semantics', 'Signal Transduction', 'Site', 'Specificity', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Pathology', 'Speech Perception', 'Stimulus', 'Superior temporal gyrus', 'Surface', 'Technology', 'Time', 'To specify', 'Training', 'United States', 'Work', 'auditory processing', 'auditory stimulus', 'convolutional neural network', 'cortex mapping', 'improved', 'innovation', 'insight', 'interest', 'neural circuit', 'neural network', 'neurophysiology', 'neurotransmission', 'nonhuman primate', 'novel', 'phonology', 'predicting response', 'receptive field', 'relating to nervous system', 'response', 'sound', 'speech processing', 'temporal measurement']",NIDCD,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2020,563437,0.37421789618595
"Neurophysiology of robust speech perception in human superior temporal gyrus DESCRIPTION (provided by applicant): Perceiving and following an individual speaker in a crowded, noisy environment is a commonplace task for listeners with normal hearing. The underlying neurophysiology, however, is complex, and the task remains a struggle for people with peripheral and central auditory pathway disorders. The lack of a detailed neurobiological model of mechanisms and functions underlying robust speech perception has hindered our understanding of how these processes become impaired in the suffering population. In our innovative approach, we will record from high-density micro and macro electrode arrays surgically implanted on the superior temporal gyrus of epilepsy patients as part of their clinical evaluation. This method offers an exceptionally detailed perspective of cortical population activity. We will build upon two recent complementary findings where we identified a highly selective, spatially distributed neural representation of phonetic features (Mesgarani et. al. Science, 2014), which at the same time is highly dynamic and can change rapidly to reflect the perceptual bias of the listener (Mesgarani & Chang, Nature 2012). While significant, these studies revealed several gaps in our understanding of this process, which we intend to address in this proposal. Specifically, we will resolve the following unanswered questions: 1) what is the neural mechanism for joint encoding of both phonetic and speaker features? 2) How does attention modulate phonetic and speaker feature selectivity of neural responses? And 3) what computational mechanisms can account for dynamic feature selectivity of responses in STG? Answering these questions will significantly advance our understanding of a remarkable human ability, and will be of great interest to researchers from many areas including neurologists, and sensory and cognitive neuroscientists. PUBLIC HEALTH RELEVANCE: Understanding the mechanisms underlying speech perception in challenging environments is a crucial step in determining how these processes deteriorate in various disorders of peripheral and central auditory pathways. Our studies will result in novel neurobiological models of robust speech perception that will serve as a necessary step toward designing innovative therapeutic measures.",Neurophysiology of robust speech perception in human superior temporal gyrus,9024503,R01DC014279,"['Accounting', 'Acoustics', 'Address', 'Aphasia', 'Area', 'Attention', 'Brain', 'Central Auditory Diseases', 'Characteristics', 'Code', 'Cognitive', 'Communication impairment', 'Complex', 'Comprehension', 'Computer Simulation', 'Crowding', 'Cues', 'Discipline', 'Disease', 'Dyslexia', 'Electrodes', 'Engineering', 'Environment', 'Epilepsy', 'Feedback', 'Health', 'Hearing', 'Human', 'Implant', 'Individual', 'Intention', 'Joints', 'Language Development', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Nature', 'Neurobiology', 'Neurologist', 'Neurons', 'Patients', 'Perception', 'Peripheral', 'Phonetics', 'Population', 'Process', 'Property', 'Prosthesis', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Science', 'Sensory', 'Signal Transduction', 'Site', 'Specificity', 'Speech', 'Speech Pathology', 'Speech Perception', 'Superior temporal gyrus', 'Surface', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Voice', 'Work', 'attentional modulation', 'auditory pathway', 'cognitive ability', 'computer framework', 'density', 'design', 'expectation', 'indexing', 'innovation', 'interest', 'learning strategy', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'novel', 'receptive field', 'relating to nervous system', 'research clinical testing', 'research study', 'response', 'selective attention', 'spatiotemporal', 'speech processing']",NIDCD,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2016,395101,0.23747334998948513
"Neurophysiology of robust speech perception in human superior temporal gyrus     DESCRIPTION (provided by applicant): Perceiving and following an individual speaker in a crowded, noisy environment is a commonplace task for listeners with normal hearing. The underlying neurophysiology, however, is complex, and the task remains a struggle for people with peripheral and central auditory pathway disorders. The lack of a detailed neurobiological model of mechanisms and functions underlying robust speech perception has hindered our understanding of how these processes become impaired in the suffering population. In our innovative approach, we will record from high-density micro and macro electrode arrays surgically implanted on the superior temporal gyrus of epilepsy patients as part of their clinical evaluation. This method offers an exceptionally detailed perspective of cortical population activity. We will build upon two recent complementary findings where we identified a highly selective, spatially distributed neural representation of phonetic features (Mesgarani et. al. Science, 2014), which at the same time is highly dynamic and can change rapidly to reflect the perceptual bias of the listener (Mesgarani & Chang, Nature 2012). While significant, these studies revealed several gaps in our understanding of this process, which we intend to address in this proposal. Specifically, we will resolve the following unanswered questions: 1) what is the neural mechanism for joint encoding of both phonetic and speaker features? 2) How does attention modulate phonetic and speaker feature selectivity of neural responses? And 3) what computational mechanisms can account for dynamic feature selectivity of responses in STG? Answering these questions will significantly advance our understanding of a remarkable human ability, and will be of great interest to researchers from many areas including neurologists, and sensory and cognitive neuroscientists.         PUBLIC HEALTH RELEVANCE: Understanding the mechanisms underlying speech perception in challenging environments is a crucial step in determining how these processes deteriorate in various disorders of peripheral and central auditory pathways. Our studies will result in novel neurobiological models of robust speech perception that will serve as a necessary step toward designing innovative therapeutic measures.                ",Neurophysiology of robust speech perception in human superior temporal gyrus,8801902,R01DC014279,"['Accounting', 'Acoustics', 'Address', 'Aphasia', 'Area', 'Attention', 'Brain', 'Central Auditory Diseases', 'Characteristics', 'Code', 'Cognitive', 'Communication impairment', 'Complex', 'Comprehension', 'Computer Simulation', 'Crowding', 'Cues', 'Discipline', 'Disease', 'Dyslexia', 'Electrodes', 'Engineering', 'Environment', 'Epilepsy', 'Feedback', 'Hearing', 'Human', 'Implant', 'Individual', 'Intention', 'Joints', 'Language Development', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Nature', 'Neurobiology', 'Neurologist', 'Neurons', 'Patients', 'Perception', 'Peripheral', 'Phonetics', 'Population', 'Process', 'Property', 'Prosthesis', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Science', 'Sensory', 'Signal Transduction', 'Site', 'Specificity', 'Speech', 'Speech Pathology', 'Speech Perception', 'Superior temporal gyrus', 'Surface', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Voice', 'Work', 'attentional modulation', 'auditory pathway', 'cognitive ability', 'computer framework', 'density', 'design', 'expectation', 'indexing', 'innovation', 'interest', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'novel', 'public health relevance', 'receptive field', 'relating to nervous system', 'research clinical testing', 'research study', 'response', 'selective attention', 'spatiotemporal', 'speech processing']",NIDCD,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2015,408878,0.23747334998948513
"Neurophysiology of robust speech perception in human superior temporal gyrus DESCRIPTION (provided by applicant): Perceiving and following an individual speaker in a crowded, noisy environment is a commonplace task for listeners with normal hearing. The underlying neurophysiology, however, is complex, and the task remains a struggle for people with peripheral and central auditory pathway disorders. The lack of a detailed neurobiological model of mechanisms and functions underlying robust speech perception has hindered our understanding of how these processes become impaired in the suffering population. In our innovative approach, we will record from high-density micro and macro electrode arrays surgically implanted on the superior temporal gyrus of epilepsy patients as part of their clinical evaluation. This method offers an exceptionally detailed perspective of cortical population activity. We will build upon two recent complementary findings where we identified a highly selective, spatially distributed neural representation of phonetic features (Mesgarani et. al. Science, 2014), which at the same time is highly dynamic and can change rapidly to reflect the perceptual bias of the listener (Mesgarani & Chang, Nature 2012). While significant, these studies revealed several gaps in our understanding of this process, which we intend to address in this proposal. Specifically, we will resolve the following unanswered questions: 1) what is the neural mechanism for joint encoding of both phonetic and speaker features? 2) How does attention modulate phonetic and speaker feature selectivity of neural responses? And 3) what computational mechanisms can account for dynamic feature selectivity of responses in STG? Answering these questions will significantly advance our understanding of a remarkable human ability, and will be of great interest to researchers from many areas including neurologists, and sensory and cognitive neuroscientists. PUBLIC HEALTH RELEVANCE: Understanding the mechanisms underlying speech perception in challenging environments is a crucial step in determining how these processes deteriorate in various disorders of peripheral and central auditory pathways. Our studies will result in novel neurobiological models of robust speech perception that will serve as a necessary step toward designing innovative therapeutic measures.",Neurophysiology of robust speech perception in human superior temporal gyrus,9444452,R01DC014279,"['Acoustics', 'Address', 'Aphasia', 'Area', 'Attention', 'Brain', 'Central Auditory Diseases', 'Characteristics', 'Code', 'Cognitive', 'Communication impairment', 'Complex', 'Comprehension', 'Computer Simulation', 'Crowding', 'Cues', 'Custom', 'Discipline', 'Disease', 'Dyslexia', 'Electrodes', 'Engineering', 'Environment', 'Epilepsy', 'Feedback', 'Hearing', 'Human', 'Impairment', 'Implant', 'Individual', 'Intention', 'Joints', 'Language Development', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Nature', 'Neurobiology', 'Neurologist', 'Neurons', 'Operative Surgical Procedures', 'Patients', 'Perception', 'Peripheral', 'Population', 'Process', 'Property', 'Prosthesis', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Science', 'Sensory', 'Signal Transduction', 'Site', 'Specificity', 'Speech', 'Speech Pathology', 'Speech Perception', 'Superior temporal gyrus', 'Surface', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Voice', 'Work', 'attentional modulation', 'auditory pathway', 'cognitive ability', 'computer framework', 'density', 'design', 'expectation', 'experimental study', 'indexing', 'innovation', 'interest', 'learning strategy', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'novel', 'public health relevance', 'receptive field', 'relating to nervous system', 'research clinical testing', 'response', 'selective attention', 'spatiotemporal', 'speech processing']",NIDCD,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2018,397659,0.23747334998948513
"Using Machine Learning to Mitigate Reverberation Effects in Cochlear Implants     DESCRIPTION (provided by applicant): Cochlear implants (CIs) provide hearing for over 200,000 recipients worldwide {NIDCD, 2011 #637}. These devices successfully provide high levels of speech understanding in quiet listening conditions; however, more challenging conditions degrade speech comprehension for CI recipients to a much greater degree than for normal hearing listeners {Kokkinakis, 2011 #673;Nelson, 2003 #302}. CI listeners are especially affected by reverberant conditions with even a small level of reverberation degrading comprehension to a greater degree than a large amount of steady-state noise {Hazrati, 2012 #674}. Thus, a method that mitigates the effects of reverberation has the potential to greatly improve the quality of life for CI users. Previous attempts to solve the problem of speech in reverberation for cochlear implants have not been able to be implemented in real time. Our preliminary results suggest that successful mitigation of overlap masking can result in a substantial improvement in speech recognition even if self-masking is not mitigated and we have devised an approach that can be implemented in real time. In the proposed effort, we will first improve the classifier to detect reverberation based on our successful preliminary efforts. Next we will assess the mitigation algorithm, first in normal hearing listeners and then in listeners with cochlear implants. Finally, we will implement the algorithm in real time and again test it. PUBLIC HEALTH RELEVANCE: While cochlear implants (CIs) provide high levels of speech comprehension in quiet for over 200,000 profoundly deaf individuals worldwide, speech in quiet scenarios are rarely encountered outside of the home. The presence of noise or reverberation in the listening environment decreases speech comprehension for CI users much more rapidly than for normal-hearing listeners, and of these two conditions, reverberation has a greater negative impact. The proposed research aims to provide a robust reverberation mitigation algorithm for CI speech processors thereby improving the ability of CI users to interact in real-world listening environments.",Using Machine Learning to Mitigate Reverberation Effects in Cochlear Implants,9722203,R01DC014290,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'Categories', 'Cochlear Implants', 'Comprehension', 'Development', 'Devices', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Frequencies', 'Hearing', 'Home environment', 'Individual', 'Location', 'Machine Learning', 'Maps', 'Masks', 'Methods', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Noise', 'Performance', 'Problem Solving', 'Quality of life', 'Research', 'Signal Transduction', 'Source', 'Speech', 'Speech Perception', 'Stimulus', 'Surface', 'Testing', 'Time', 'base', 'deaf', 'improved', 'microphone', 'normal hearing', 'public health relevance', 'recruit', 'response', 'simulation', 'sound', 'speech processing', 'speech recognition', 'success']",NIDCD,DUKE UNIVERSITY,R01,2019,326399,0.3541285789991508
"Using Machine Learning to Mitigate Reverberation Effects in Cochlear Implants     DESCRIPTION (provided by applicant): Cochlear implants (CIs) provide hearing for over 200,000 recipients worldwide {NIDCD, 2011 #637}. These devices successfully provide high levels of speech understanding in quiet listening conditions; however, more challenging conditions degrade speech comprehension for CI recipients to a much greater degree than for normal hearing listeners {Kokkinakis, 2011 #673;Nelson, 2003 #302}. CI listeners are especially affected by reverberant conditions with even a small level of reverberation degrading comprehension to a greater degree than a large amount of steady-state noise {Hazrati, 2012 #674}. Thus, a method that mitigates the effects of reverberation has the potential to greatly improve the quality of life for CI users. Previous attempts to solve the problem of speech in reverberation for cochlear implants have not been able to be implemented in real time. Our preliminary results suggest that successful mitigation of overlap masking can result in a substantial improvement in speech recognition even if self-masking is not mitigated and we have devised an approach that can be implemented in real time. In the proposed effort, we will first improve the classifier to detect reverberation based on our successful preliminary efforts. Next we will assess the mitigation algorithm, first in normal hearing listeners and then in listeners with cochlear implants. Finally, we will implement the algorithm in real time and again test it. PUBLIC HEALTH RELEVANCE: While cochlear implants (CIs) provide high levels of speech comprehension in quiet for over 200,000 profoundly deaf individuals worldwide, speech in quiet scenarios are rarely encountered outside of the home. The presence of noise or reverberation in the listening environment decreases speech comprehension for CI users much more rapidly than for normal-hearing listeners, and of these two conditions, reverberation has a greater negative impact. The proposed research aims to provide a robust reverberation mitigation algorithm for CI speech processors thereby improving the ability of CI users to interact in real-world listening environments.",Using Machine Learning to Mitigate Reverberation Effects in Cochlear Implants,9513918,R01DC014290,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'Categories', 'Cochlear Implants', 'Comprehension', 'Development', 'Devices', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Frequencies', 'Hearing', 'Hearing Impaired Persons', 'Home environment', 'Individual', 'Location', 'Machine Learning', 'Maps', 'Masks', 'Methods', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Noise', 'Performance', 'Problem Solving', 'Quality of life', 'Research', 'Signal Transduction', 'Source', 'Speech', 'Speech Perception', 'Stimulus', 'Surface', 'Testing', 'Time', 'base', 'improved', 'public health relevance', 'recruit', 'response', 'simulation', 'sound', 'speech processing', 'speech recognition', 'success']",NIDCD,DUKE UNIVERSITY,R01,2018,326840,0.3541285789991508
"Using Machine Learning to Mitigate Reverberation Effects in Cochlear Implants     DESCRIPTION (provided by applicant): Cochlear implants (CIs) provide hearing for over 200,000 recipients worldwide {NIDCD, 2011 #637}. These devices successfully provide high levels of speech understanding in quiet listening conditions; however, more challenging conditions degrade speech comprehension for CI recipients to a much greater degree than for normal hearing listeners {Kokkinakis, 2011 #673;Nelson, 2003 #302}. CI listeners are especially affected by reverberant conditions with even a small level of reverberation degrading comprehension to a greater degree than a large amount of steady-state noise {Hazrati, 2012 #674}. Thus, a method that mitigates the effects of reverberation has the potential to greatly improve the quality of life for CI users. Previous attempts to solve the problem of speech in reverberation for cochlear implants have not been able to be implemented in real time. Our preliminary results suggest that successful mitigation of overlap masking can result in a substantial improvement in speech recognition even if self-masking is not mitigated and we have devised an approach that can be implemented in real time. In the proposed effort, we will first improve the classifier to detect reverberation based on our successful preliminary efforts. Next we will assess the mitigation algorithm, first in normal hearing listeners and then in listeners with cochlear implants. Finally, we will implement the algorithm in real time and again test it. PUBLIC HEALTH RELEVANCE: While cochlear implants (CIs) provide high levels of speech comprehension in quiet for over 200,000 profoundly deaf individuals worldwide, speech in quiet scenarios are rarely encountered outside of the home. The presence of noise or reverberation in the listening environment decreases speech comprehension for CI users much more rapidly than for normal-hearing listeners, and of these two conditions, reverberation has a greater negative impact. The proposed research aims to provide a robust reverberation mitigation algorithm for CI speech processors thereby improving the ability of CI users to interact in real-world listening environments.",Using Machine Learning to Mitigate Reverberation Effects in Cochlear Implants,9305035,R01DC014290,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'Categories', 'Cochlear Implants', 'Comprehension', 'Development', 'Devices', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Frequencies', 'Hearing', 'Hearing Impaired Persons', 'Home environment', 'Individual', 'Location', 'Machine Learning', 'Maps', 'Masks', 'Methods', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Noise', 'Performance', 'Problem Solving', 'Quality of life', 'Recruitment Activity', 'Research', 'Signal Transduction', 'Source', 'Speech', 'Speech Perception', 'Stimulus', 'Surface', 'Testing', 'Time', 'base', 'improved', 'public health relevance', 'response', 'simulation', 'sound', 'speech processing', 'speech recognition', 'success']",NIDCD,DUKE UNIVERSITY,R01,2017,293476,0.3541285789991508
"Using Machine Learning to Mitigate Reverberation Effects in Cochlear Implants     DESCRIPTION (provided by applicant): Cochlear implants (CIs) provide hearing for over 200,000 recipients worldwide {NIDCD, 2011 #637}. These devices successfully provide high levels of speech understanding in quiet listening conditions; however, more challenging conditions degrade speech comprehension for CI recipients to a much greater degree than for normal hearing listeners {Kokkinakis, 2011 #673;Nelson, 2003 #302}. CI listeners are especially affected by reverberant conditions with even a small level of reverberation degrading comprehension to a greater degree than a large amount of steady-state noise {Hazrati, 2012 #674}. Thus, a method that mitigates the effects of reverberation has the potential to greatly improve the quality of life for CI users. Previous attempts to solve the problem of speech in reverberation for cochlear implants have not been able to be implemented in real time. Our preliminary results suggest that successful mitigation of overlap masking can result in a substantial improvement in speech recognition even if self-masking is not mitigated and we have devised an approach that can be implemented in real time. In the proposed effort, we will first improve the classifier to detect reverberation based on our successful preliminary efforts. Next we will assess the mitigation algorithm, first in normal hearing listeners and then in listeners with cochlear implants. Finally, we will implement the algorithm in real time and again test it. PUBLIC HEALTH RELEVANCE: While cochlear implants (CIs) provide high levels of speech comprehension in quiet for over 200,000 profoundly deaf individuals worldwide, speech in quiet scenarios are rarely encountered outside of the home. The presence of noise or reverberation in the listening environment decreases speech comprehension for CI users much more rapidly than for normal-hearing listeners, and of these two conditions, reverberation has a greater negative impact. The proposed research aims to provide a robust reverberation mitigation algorithm for CI speech processors thereby improving the ability of CI users to interact in real-world listening environments.",Using Machine Learning to Mitigate Reverberation Effects in Cochlear Implants,9100672,R01DC014290,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'Categories', 'Cochlear Implants', 'Comprehension', 'Development', 'Devices', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Frequencies', 'Health', 'Hearing', 'Hearing Impaired Persons', 'Home environment', 'Individual', 'Location', 'Machine Learning', 'Maps', 'Masks', 'Methods', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Noise', 'Performance', 'Problem Solving', 'Process', 'Quality of life', 'Recruitment Activity', 'Research', 'Signal Transduction', 'Source', 'Speech', 'Speech Perception', 'Stimulus', 'Surface', 'Testing', 'Time', 'base', 'improved', 'response', 'simulation', 'sound', 'speech processing', 'speech recognition', 'success']",NIDCD,DUKE UNIVERSITY,R01,2016,293885,0.3541285789991508
"Using Machine Learning to Mitigate Reverberation Effects in Cochlear Implants     DESCRIPTION (provided by applicant): Cochlear implants (CIs) provide hearing for over 200,000 recipients worldwide {NIDCD, 2011 #637}. These devices successfully provide high levels of speech understanding in quiet listening conditions; however, more challenging conditions degrade speech comprehension for CI recipients to a much greater degree than for normal hearing listeners {Kokkinakis, 2011 #673;Nelson, 2003 #302}. CI listeners are especially affected by reverberant conditions with even a small level of reverberation degrading comprehension to a greater degree than a large amount of steady-state noise {Hazrati, 2012 #674}. Thus, a method that mitigates the effects of reverberation has the potential to greatly improve the quality of life for CI users. Previous attempts to solve the problem of speech in reverberation for cochlear implants have not been able to be implemented in real time. Our preliminary results suggest that successful mitigation of overlap masking can result in a substantial improvement in speech recognition even if self-masking is not mitigated and we have devised an approach that can be implemented in real time. In the proposed effort, we will first improve the classifier to detect reverberation based on our successful preliminary efforts. Next we will assess the mitigation algorithm, first in normal hearing listeners and then in listeners with cochlear implants. Finally, we will implement the algorithm in real time and again test it.          PUBLIC HEALTH RELEVANCE: While cochlear implants (CIs) provide high levels of speech comprehension in quiet for over 200,000 profoundly deaf individuals worldwide, speech in quiet scenarios are rarely encountered outside of the home. The presence of noise or reverberation in the listening environment decreases speech comprehension for CI users much more rapidly than for normal-hearing listeners, and of these two conditions, reverberation has a greater negative impact. The proposed research aims to provide a robust reverberation mitigation algorithm for CI speech processors thereby improving the ability of CI users to interact in real-world listening environments.            ",Using Machine Learning to Mitigate Reverberation Effects in Cochlear Implants,8963088,R01DC014290,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'Categories', 'Cochlear Implants', 'Comprehension', 'Development', 'Devices', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Frequencies', 'Hearing', 'Hearing Impaired Persons', 'Home environment', 'Individual', 'Location', 'Machine Learning', 'Maps', 'Masks', 'Methods', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Noise', 'Performance', 'Problem Solving', 'Process', 'Quality of life', 'Recruitment Activity', 'Research', 'Signal Transduction', 'Source', 'Speech', 'Speech Perception', 'Stimulus', 'Surface', 'Testing', 'Time', 'base', 'improved', 'public health relevance', 'response', 'simulation', 'sound', 'speech processing', 'speech recognition', 'success']",NIDCD,DUKE UNIVERSITY,R01,2015,289263,0.3541285789991508
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases     DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand. PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9619075,R01DC014498,"['Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'American Sign Language', 'Articulation', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Databases', 'Detection', 'Devices', 'Dimensions', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Human', 'Image', 'Individual', 'Intervention', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deaf', 'deafness', 'design', 'experience', 'experimental study', 'face perception', 'innovation', 'instructor', 'interest', 'machine learning algorithm', 'prevent', 'public health relevance', 'reconstruction', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2019,318386,0.15564656380450534
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases     DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand. PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9402599,R01DC014498,"['Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'American Sign Language', 'Articulation', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Databases', 'Detection', 'Devices', 'Dimensions', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Intervention', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deafness', 'design', 'experience', 'experimental study', 'face perception', 'innovation', 'instructor', 'interest', 'prevent', 'public health relevance', 'reconstruction', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2018,318898,0.15564656380450534
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases     DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand. PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9199411,R01DC014498,"['Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'American Sign Language', 'Articulation', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Controlled Study', 'Databases', 'Detection', 'Devices', 'Dimensions', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Intervention', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deafness', 'design', 'experience', 'experimental study', 'face perception', 'innovation', 'instructor', 'interest', 'prevent', 'public health relevance', 'reconstruction', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2017,319382,0.15564656380450534
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases     DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand.         PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.            ",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9054574,R01DC014498,"['Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Controlled Study', 'Databases', 'Detection', 'Devices', 'Educational process of instructing', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Intervention', 'Joints', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deafness', 'design', 'experience', 'face perception', 'innovation', 'instructor', 'interest', 'prevent', 'public health relevance', 'reconstruction', 'research study', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2016,331310,0.15564656380450534
"Optimization of Personalized Speech Synthesis Our voices are not identical, they are our identities. The human voice is a powerful signal that conveys ones age, gender, size, ethnicity, and personality, among other attributes. Yet, until now, users of augmentative and alternative communication (AAC) devices, screen reading technologies and other text-to-speech (TTS) applications have relied on a limited set of mass-produced, generic-sounding synthetic voices. This mismatch in vocal identity impacts educational outcomes, infringes on personal safety, and hinders social integration. Conventional methods for building a synthetic voice require a voice actor to record an extensive dataset of studio-quality recordings which are used to train a computational model and generate the output voice. The process is time and labor intensive and thus inaccessible to everyday consumers let alone those with speech impairment. VocaliD Incs award-winning technology offers an unprecedented means to build custom crafted synthetic voices that reflect the recipient by combining his/her own residual vocalizations with recordings of a matched speaker from our crowdsourced Human Voicebank. We have discovered that even a single vowel contains enough ""vocal DNA"" to seed the personalization process. VocaliDs custom voices sound like the recipient in age, personality and vocal identity and have the clarity of everyday talkers. Having made significant progress towards improving the intelligibility and naturalness of our custom voices under Phase II, our voices are within a few percentage points of natural human speech in terms of intelligibility and rated as highly natural sounding by unfamiliar listeners. However, several persistent issues limit the commercial potential of our current methods. First, our new methods are computationally intensive and thus cannot be utilized on current assistive communication devices. Optimization of the methods to reduce latency and thereby improve usability is critical (Aim 1). Another unintended consequence of advances in clarity and naturalness of our voices is the potential for misappropriation. To counteract this, we propose developing a multi-speaker model to create unique new voices and mask the identity of a given speech donor (Aim 2). Last, although the new models are capable of more prosodic variation, current methods rely heavily on exemplars in the training data. Our customers indicate a need and desire for greater control of subtle yet meaningful differences in prosody. (Aim 3) These additional tasks will further bolster the product and likelihood of commercial success for the AAC market and beyond. VocaliDs breakthrough technology powers the first-ever custom synthetic voices that are made using only brief samples of recipient vocalizations combined with recordings of matched speaker(s) from our crowdsourced voicebank. This Phase II SBIR Administrative Supplement proposal addresses the challenges of creating a scalable and efficient method for achieving high quality, natural sounding, and controllable personalized voices.",Optimization of Personalized Speech Synthesis,9966587,R44DC014607,"['Acoustics', 'Address', 'Administrative Supplement', 'Age', 'Architecture', 'Augmentative and Alternative Communication', 'Award', 'Child', 'Computer Simulation', 'Computing Methodologies', 'Custom', 'DNA', 'Data', 'Data Set', 'Ethnic Origin', 'Gender', 'Generations', 'Gestures', 'Human', 'Impairment', 'Intervention', 'Learning', 'Learning Module', 'Manuals', 'Masks', 'Methods', 'Modeling', 'Outcome', 'Output', 'Pattern', 'Performance', 'Personality', 'Phase', 'Process', 'Quality of life', 'Reading', 'Residual state', 'Safety', 'Sampling', 'Science', 'Seeds', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Speech', 'System', 'Technology', 'Text', 'Time', 'To specify', 'Training', 'Variant', 'Voice', 'Voice Quality', 'Wheelchairs', 'Woman', 'base', 'communication device', 'crowdsourcing', 'deep learning', 'digital', 'flexibility', 'improved', 'man', 'novel', 'social integration', 'sound', 'speech synthesis', 'success', 'usability', 'vocalization']",NIDCD,"VOCALID, INC.",R44,2019,601315,0.133346684217709
"Identifying the neural structures and dynamics that regulate phonological structure The systematic patterning of language is a fundamental property of cognition. One aspect of this patterning, constraints on the combination of speech sounds to form words (phonotactic structure), has been implicated in constraining diverse processes related to language acquisition, perception, and production, bilingual language use, memory and even influences non-linguistic processes including the memorability of novel words and consumer reaction to novel brand names. This patterning changes in a variety of common acquired and developmental communication disorders. We will explore two explanations for these effects. One, developed in linguistic theory, argues that language users discover a set of abstract, language-specific rules or constraints that shape language use. The other view, developed in connectionist and dynamic systems theory, argues that phonotactic constraints emerge from top-down lexical influences on speech perception. Discriminating between these approaches is difficult because both explain behavioral data well. It is essential to discriminate between these accounts for two reasons. This question offers an excellent opportunity to resolve the debate over whether abstract linguistic rules/constraints create or simply describe the patterning of language. The resolution of this question has fundamental implications for the way linguistic formalism and connectionist simulations relate to human processing. At a more immediate level, this research offers the opportunity to identify a common core mechanism (either the leveraged use of abstract linguistic rules or top-down lexical influences) that explains and unites diverse linguistic and cognitive phenomena. Past efforts to resolve these issues have failed because of fundamental inferential limitations of behavioral and BOLD imaging paradigms. Accordingly, we have developed new tools and research strategies that allow us to identify patterns of directed interaction between brain regions (effective connectivity), and use these analyses to draw much stronger inferences about the dynamic processes that shape cognition. Observers in the field have argued that our methods have already provided definitive evidence to resolve the decades old debate over the role of top- down processes in speech processing. This proposal would extend those methods, and introduce innovative neural decoding analyses that we will use to characterize the categories (e.g. rules, words, abstract phonological representations needed to support rule application) that are encoded in localized brain activity. Using these methods, we will determine whether top-down lexical processes that we have shown produce phonotactic phenomena related to the processing of patterns that occur in speaker's language generalize to unfamiliar patterns. We will also use them to identify the substrates of rule- versus word-mediated processing, to provide a baseline for interpreted the representations and dynamic processes that support phonotactic effects in natural language processing. The human ability to learn and use language is the result of a complex set of dynamic brain processes that can break down as the result of disease, injury or developmental disorders. This work uses new non-invasive techniques to observe and understand some of the most fundamental brain processes that shape language learning and use so that we may better understand how they break down. This understanding should one day guide the development of better ways to treat diverse language disorders.",Identifying the neural structures and dynamics that regulate phonological structure,9894782,R01DC015455,"['Address', 'Affect', 'Algebra', 'Behavioral', 'Brain', 'Brain region', 'Categories', 'Cognition', 'Cognitive', 'Common Core', 'Communication impairment', 'Competence', 'Complex', 'Data', 'Development', 'Developmental Communication Disorders', 'Disease', 'Evaluation', 'Frequencies', 'Goals', 'Heart', 'Human', 'Image', 'Imaging Techniques', 'Injury', 'Intuition', 'Judgment', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Language Disorders', 'Learning', 'Linguistics', 'Mediating', 'Mediation', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Neighborhoods', 'Pathology', 'Pattern', 'Perception', 'Performance', 'Process', 'Production', 'Property', 'Reaction', 'Research', 'Resolution', 'Role', 'Shapes', 'Speech', 'Speech Perception', 'Speech Sound', 'Structure', 'Structure of supramarginal gyrus', 'Systems Theory', 'Techniques', 'Testing', 'Work', 'bilingualism', 'developmental disease', 'dynamic system', 'experience', 'innovation', 'lexical', 'lexical processing', 'models and simulation', 'novel', 'phonology', 'relating to nervous system', 'simulation', 'sound', 'spatiotemporal', 'speech processing', 'theories', 'tool', 'word learning']",NIDCD,MASSACHUSETTS GENERAL HOSPITAL,R01,2020,517151,0.10500104650411568
"Identifying the neural structures and dynamics that regulate phonological structure The systematic patterning of language is a fundamental property of cognition. One aspect of this patterning, constraints on the combination of speech sounds to form words (phonotactic structure), has been implicated in constraining diverse processes related to language acquisition, perception, and production, bilingual language use, memory and even influences non-linguistic processes including the memorability of novel words and consumer reaction to novel brand names. This patterning changes in a variety of common acquired and developmental communication disorders. We will explore two explanations for these effects. One, developed in linguistic theory, argues that language users discover a set of abstract, language-specific rules or constraints that shape language use. The other view, developed in connectionist and dynamic systems theory, argues that phonotactic constraints emerge from top-down lexical influences on speech perception. Discriminating between these approaches is difficult because both explain behavioral data well. It is essential to discriminate between these accounts for two reasons. This question offers an excellent opportunity to resolve the debate over whether abstract linguistic rules/constraints create or simply describe the patterning of language. The resolution of this question has fundamental implications for the way linguistic formalism and connectionist simulations relate to human processing. At a more immediate level, this research offers the opportunity to identify a common core mechanism (either the leveraged use of abstract linguistic rules or top-down lexical influences) that explains and unites diverse linguistic and cognitive phenomena. Past efforts to resolve these issues have failed because of fundamental inferential limitations of behavioral and BOLD imaging paradigms. Accordingly, we have developed new tools and research strategies that allow us to identify patterns of directed interaction between brain regions (effective connectivity), and use these analyses to draw much stronger inferences about the dynamic processes that shape cognition. Observers in the field have argued that our methods have already provided definitive evidence to resolve the decades old debate over the role of top- down processes in speech processing. This proposal would extend those methods, and introduce innovative neural decoding analyses that we will use to characterize the categories (e.g. rules, words, abstract phonological representations needed to support rule application) that are encoded in localized brain activity. Using these methods, we will determine whether top-down lexical processes that we have shown produce phonotactic phenomena related to the processing of patterns that occur in speaker's language generalize to unfamiliar patterns. We will also use them to identify the substrates of rule- versus word-mediated processing, to provide a baseline for interpreted the representations and dynamic processes that support phonotactic effects in natural language processing. The human ability to learn and use language is the result of a complex set of dynamic brain processes that can break down as the result of disease, injury or developmental disorders. This work uses new non-invasive techniques to observe and understand some of the most fundamental brain processes that shape language learning and use so that we may better understand how they break down. This understanding should one day guide the development of better ways to treat diverse language disorders.",Identifying the neural structures and dynamics that regulate phonological structure,9674437,R01DC015455,"['Address', 'Affect', 'Algebra', 'Behavioral', 'Brain', 'Brain region', 'Categories', 'Cognition', 'Cognitive', 'Common Core', 'Communication impairment', 'Competence', 'Complex', 'Data', 'Development', 'Developmental Communication Disorders', 'Disease', 'Evaluation', 'Frequencies', 'Goals', 'Heart', 'Human', 'Image', 'Imaging Techniques', 'Injury', 'Intuition', 'Judgment', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Language Disorders', 'Learning', 'Linguistics', 'Mediating', 'Mediation', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Neighborhoods', 'Pathology', 'Pattern', 'Perception', 'Performance', 'Process', 'Production', 'Property', 'Reaction', 'Research', 'Resolution', 'Role', 'Shapes', 'Speech', 'Speech Perception', 'Speech Sound', 'Structure', 'Structure of supramarginal gyrus', 'Systems Theory', 'Techniques', 'Testing', 'Work', 'bilingualism', 'developmental disease', 'dynamic system', 'experience', 'innovation', 'lexical', 'lexical processing', 'models and simulation', 'novel', 'phonology', 'relating to nervous system', 'simulation', 'sound', 'spatiotemporal', 'speech processing', 'theories', 'tool', 'word learning']",NIDCD,MASSACHUSETTS GENERAL HOSPITAL,R01,2019,557010,0.10500104650411568
"Identifying the neural structures and dynamics that regulate phonological structure The systematic patterning of language is a fundamental property of cognition. One aspect of this patterning, constraints on the combination of speech sounds to form words (phonotactic structure), has been implicated in constraining diverse processes related to language acquisition, perception, and production, bilingual language use, memory and even influences non-linguistic processes including the memorability of novel words and consumer reaction to novel brand names. This patterning changes in a variety of common acquired and developmental communication disorders. We will explore two explanations for these effects. One, developed in linguistic theory, argues that language users discover a set of abstract, language-specific rules or constraints that shape language use. The other view, developed in connectionist and dynamic systems theory, argues that phonotactic constraints emerge from top-down lexical influences on speech perception. Discriminating between these approaches is difficult because both explain behavioral data well. It is essential to discriminate between these accounts for two reasons. This question offers an excellent opportunity to resolve the debate over whether abstract linguistic rules/constraints create or simply describe the patterning of language. The resolution of this question has fundamental implications for the way linguistic formalism and connectionist simulations relate to human processing. At a more immediate level, this research offers the opportunity to identify a common core mechanism (either the leveraged use of abstract linguistic rules or top-down lexical influences) that explains and unites diverse linguistic and cognitive phenomena. Past efforts to resolve these issues have failed because of fundamental inferential limitations of behavioral and BOLD imaging paradigms. Accordingly, we have developed new tools and research strategies that allow us to identify patterns of directed interaction between brain regions (effective connectivity), and use these analyses to draw much stronger inferences about the dynamic processes that shape cognition. Observers in the field have argued that our methods have already provided definitive evidence to resolve the decades old debate over the role of top- down processes in speech processing. This proposal would extend those methods, and introduce innovative neural decoding analyses that we will use to characterize the categories (e.g. rules, words, abstract phonological representations needed to support rule application) that are encoded in localized brain activity. Using these methods, we will determine whether top-down lexical processes that we have shown produce phonotactic phenomena related to the processing of patterns that occur in speaker's language generalize to unfamiliar patterns. We will also use them to identify the substrates of rule- versus word-mediated processing, to provide a baseline for interpreted the representations and dynamic processes that support phonotactic effects in natural language processing. The human ability to learn and use language is the result of a complex set of dynamic brain processes that can break down as the result of disease, injury or developmental disorders. This work uses new non-invasive techniques to observe and understand some of the most fundamental brain processes that shape language learning and use so that we may better understand how they break down. This understanding should one day guide the development of better ways to treat diverse language disorders.",Identifying the neural structures and dynamics that regulate phonological structure,9461502,R01DC015455,"['Address', 'Affect', 'Algebra', 'Behavioral', 'Brain', 'Brain region', 'Categories', 'Cognition', 'Cognitive', 'Common Core', 'Communication impairment', 'Competence', 'Complex', 'Data', 'Development', 'Developmental Communication Disorders', 'Disease', 'Evaluation', 'Frequencies', 'Goals', 'Heart', 'Human', 'Image', 'Imaging Techniques', 'Injury', 'Intuition', 'Judgment', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Language Disorders', 'Learning', 'Linguistics', 'Mediating', 'Mediation', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Neighborhoods', 'Pathology', 'Pattern', 'Perception', 'Performance', 'Process', 'Production', 'Property', 'Reaction', 'Research', 'Resolution', 'Role', 'Shapes', 'Speech', 'Speech Perception', 'Speech Sound', 'Structure', 'Structure of supramarginal gyrus', 'Systems Theory', 'Techniques', 'Testing', 'Work', 'bilingualism', 'developmental disease', 'dynamic system', 'experience', 'innovation', 'lexical', 'lexical processing', 'models and simulation', 'novel', 'phonology', 'relating to nervous system', 'simulation', 'sound', 'spatiotemporal', 'speech processing', 'theories', 'tool', 'word learning']",NIDCD,MASSACHUSETTS GENERAL HOSPITAL,R01,2018,607626,0.10500104650411568
"Identifying the neural structures and dynamics that regulate phonological structure The systematic patterning of language is a fundamental property of cognition. One aspect of this patterning, constraints on the combination of speech sounds to form words (phonotactic structure), has been implicated in constraining diverse processes related to language acquisition, perception, and production, bilingual language use, memory and even influences non-linguistic processes including the memorability of novel words and consumer reaction to novel brand names. This patterning changes in a variety of common acquired and developmental communication disorders. We will explore two explanations for these effects. One, developed in linguistic theory, argues that language users discover a set of abstract, language-specific rules or constraints that shape language use. The other view, developed in connectionist and dynamic systems theory, argues that phonotactic constraints emerge from top-down lexical influences on speech perception. Discriminating between these approaches is difficult because both explain behavioral data well. It is essential to discriminate between these accounts for two reasons. This question offers an excellent opportunity to resolve the debate over whether abstract linguistic rules/constraints create or simply describe the patterning of language. The resolution of this question has fundamental implications for the way linguistic formalism and connectionist simulations relate to human processing. At a more immediate level, this research offers the opportunity to identify a common core mechanism (either the leveraged use of abstract linguistic rules or top-down lexical influences) that explains and unites diverse linguistic and cognitive phenomena. Past efforts to resolve these issues have failed because of fundamental inferential limitations of behavioral and BOLD imaging paradigms. Accordingly, we have developed new tools and research strategies that allow us to identify patterns of directed interaction between brain regions (effective connectivity), and use these analyses to draw much stronger inferences about the dynamic processes that shape cognition. Observers in the field have argued that our methods have already provided definitive evidence to resolve the decades old debate over the role of top- down processes in speech processing. This proposal would extend those methods, and introduce innovative neural decoding analyses that we will use to characterize the categories (e.g. rules, words, abstract phonological representations needed to support rule application) that are encoded in localized brain activity. Using these methods, we will determine whether top-down lexical processes that we have shown produce phonotactic phenomena related to the processing of patterns that occur in speaker's language generalize to unfamiliar patterns. We will also use them to identify the substrates of rule- versus word-mediated processing, to provide a baseline for interpreted the representations and dynamic processes that support phonotactic effects in natural language processing. The human ability to learn and use language is the result of a complex set of dynamic brain processes that can break down as the result of disease, injury or developmental disorders. This work uses new non-invasive techniques to observe and understand some of the most fundamental brain processes that shape language learning and use so that we may better understand how they break down. This understanding should one day guide the development of better ways to treat diverse language disorders.",Identifying the neural structures and dynamics that regulate phonological structure,9311162,R01DC015455,"['Address', 'Affect', 'Algebra', 'Behavioral', 'Brain', 'Brain region', 'Categories', 'Cognition', 'Cognitive', 'Common Core', 'Communication impairment', 'Competence', 'Complex', 'Data', 'Development', 'Developmental Communication Disorders', 'Disease', 'Evaluation', 'Frequencies', 'Goals', 'Heart', 'Human', 'Image', 'Imaging Techniques', 'Injury', 'Intuition', 'Judgment', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Language Disorders', 'Learning', 'Linguistics', 'Mediating', 'Mediation', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Neighborhoods', 'Pathology', 'Pattern', 'Perception', 'Performance', 'Process', 'Production', 'Property', 'Reaction', 'Research', 'Resolution', 'Role', 'Shapes', 'Speech', 'Speech Perception', 'Speech Sound', 'Structure', 'Structure of supramarginal gyrus', 'Systems Theory', 'Techniques', 'Testing', 'Work', 'bilingualism', 'developmental disease', 'dynamic system', 'experience', 'innovation', 'lexical', 'lexical processing', 'models and simulation', 'novel', 'phonology', 'relating to nervous system', 'simulation', 'sound', 'spatiotemporal', 'speech processing', 'theories', 'tool', 'word learning']",NIDCD,MASSACHUSETTS GENERAL HOSPITAL,R01,2017,608789,0.10500104650411568
"The Cortical Dynamics of Motor Activity During Speech Production Speech is critical for human communication, but the cortical control of the fundamental movements of speech production is not clearly understood. Current knowledge primarily stems from functional imaging and lesion studies, which lack the ability to observe rapid changes in activity. Advances in electrocorticography (ECoG) now enable investigation of rapid changes over multiple cortical regions. Early ECoG studies have found ventral motor cortex (M1) activity broadly related to large vocal pitch change and loosely correlated with production of speech sounds, or phonemes. I have demonstrated that even single instances of phonemes can be identified during word production from patterns in activity in M1. However, motor control studies show M1 activity primarily represents movements. Correspondingly, the movements of speech, such as those of the laryngeal musculature involved in vocal pitch, are well understood. Similarly, phonology literature suggests the basic units of speech production are simple articulator movements, or gestures, such as tongue tip closure. Despite strong evidence for movement representation in M1, speech movements have yet to be been investigated with ECoG. The objective of this proposal is to elucidate how the motor cortices control speech movements of articulators and vocal pitch. The research aims of this proposal are to determine the cortical representation of articulatory gestures and vocal pitch during speech and non-speech movements. I will decode high-gamma activity (70-200 Hz) of M1, premotor cortex (PM) and inferior frontal gyrus (IFG) to estimate representation of these movements. Decoding accuracy will quantify the extent to which gestures are represented in each cortical area during speech and non-speech movements. Preliminary results reveal distinct cortical signatures in M1 related to gesture production. Completion of these goals will result in a model of cortical representation of speech movements. This work will enable substantiation of neurophysiological theories, lead to practical applications for patients, and enable investigations of motor activity during higher- order processes of speech. Moreover, we enable comparison of speech to other motor control processes. Results will improve understanding of the cortical representation of the larynx, which remains poorly understood in human physiology research. This study examines cortical activity of motor areas during speech production, with the goal of identifying the cortical representation of articulatory movements and vocal pitch. This research will address fundamental questions about speech motor control during production and phonation, which will enable evaluation of neurophysiological theories and lead to practical applications for patients.",The Cortical Dynamics of Motor Activity During Speech Production,9191603,F32DC015708,"['Acoustics', 'Address', 'Algorithms', 'Area', 'Articulators', 'Communication', 'Controlled Study', 'Craniotomy', 'Electrocorticogram', 'Electrophysiology (science)', 'Evaluation', 'Excision', 'Functional Imaging', 'Gestures', 'Goals', 'Human', 'Inferior frontal gyrus', 'Investigation', 'Knowledge', 'Laryngeal muscle structure', 'Larynx', 'Lead', 'Lesion', 'Lip structure', 'Literature', 'Machine Learning', 'Maps', 'Mechanics', 'Mentors', 'Modeling', 'Motor Activity', 'Motor Cortex', 'Movement', 'Patients', 'Pattern', 'Phonation', 'Physiology', 'Process', 'Production', 'Reading', 'Research', 'Research Personnel', 'Speech', 'Speech Sound', 'Technology', 'Testing', 'Time', 'Training', 'Variant', 'Work', 'analog', 'awake', 'design', 'improved', 'innovation', 'motor control', 'neural prosthesis', 'neurophysiology', 'neurosurgery', 'patient population', 'phonology', 'phrases', 'practical application', 'relating to nervous system', 'restoration', 'speech processing', 'stem', 'theories', 'tongue apex', 'tool', 'tumor']",NIDCD,NORTHWESTERN UNIVERSITY AT CHICAGO,F32,2016,53358,0.18914320636513848
"Multi-Measure Speech Perception in Noise (MMSPIN) Chart: More Scores, Fewer Tests Project summary. This Phase I will establish the feasibility of increasing audiological diagnostic information by carrying out word- and phoneme-level analyses of open set responses during speech audiometry and by obtaining subjective hearing measures. Speech audiometry is used in characterizing functional hearing in settings of hearing screening, diagnosis, hearing aid fitting, counseling, aural rehabilitation/training, occupational fitness, and research. A typical procedure used with word and sentence tests in background noise is to ask the client/patient to repeat back what was just said (i.e., give an open set response). Responses are then scored in terms of words or keywords correct/incorrect. This method discards potentially diagnostic information in response errors, because noise can reveal systematic phonetic feature or phoneme confusions, and with background babble, intrusions from the babble. Other response patterns attributable to cognitive or memory declines may manifest in the paucity or verbosity of response words. Specific types of phoneme perception errors are thought to be associated with extent and configuration of hearing loss; and different types of noise maskers (i.e., energetic and informational maskers) present different types of perceptual problems that vary in severity across individuals. In order to utilize response errors, computational methods are needed to establish their relationships to the stimulus. This is because response errors may incorporate incorrect stimulus-to-response phoneme substitutions, as well as insertions or deletions of phonemes or words relative to the stimulus. We have developed sequence alignment methods to mine errors during speech audiometry, which we propose to evaluate using our system (Multi-Measure SPIN Chart: MMSPIN Chart). MMSPIN chart will be further developed and installed in the George Washington University Speech & Hearing Center (Aim 1). Audiologists will use the system during QuickSin sentence and NU-6 word testing with 200 clients (18-85 years of age) who give permission to access their entire clinic records (Aim 2). Their conventional speech audiometry will be augmented by obtaining subjective hearing accuracy judgments and hearing self-efficacy measures. These subjective judgments are designed to expose discrepancies with objective performance and to reveal individual differences in social cognition associated with hearing loss, both of which may account for the large individual differences in performance and intervention outcomes not accounted for by the audiogram. Evaluation of results in Aim 3 will include developing group and individual profile models comprising objective and subjective clinical data. With our clinician partners, we will develop formats for communicating MMSPIN Chart results to clients. In Aim 4, we will present results in a public lecture for audiologists and solicit opinions about how our new results may best impact clinical practice. Our approach can deliver more informative and efficient speech audiometry using existing test materials and can pave the way to more sensitive speech audiometry, including tests that are adaptive to specific levels of speech processing difficulty. Narrative. The typical approach to speech audiometry is to elicit open set responses that are scored in terms of words/keywords correct, discarding information in response errors. This Phase I will establish the feasibility of increasing diagnostic information provided to audiologists by carrying out word- and phoneme-level analyses of open set responses and obtaining subjective hearing measures in conjunction with speech audiometry. The goal is to improve clinical efficiency and effectiveness and to improve patient outcomes.","Multi-Measure Speech Perception in Noise (MMSPIN) Chart: More Scores, Fewer Tests",9408539,R43DC015749,"['Adult', 'Age', 'Age-Years', 'Attitude to Health', 'Audiometry', 'Authorization documentation', 'Back', 'Classification', 'Client', 'Clinic', 'Clinical', 'Clinical Data', 'Computers', 'Computing Methodologies', 'Confusion', 'Counseling', 'Data', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Effectiveness', 'Evaluation', 'Factor Analysis', 'Focus Groups', 'Goals', 'Hearing', 'Hearing Aids', 'Impaired cognition', 'Individual', 'Individual Differences', 'Intervention', 'Judgment', 'Machine Learning', 'Materials Testing', 'Measures', 'Memory Loss', 'Methods', 'Modeling', 'Noise', 'Occupational', 'Outcome', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Perception', 'Performance', 'Phase', 'Procedures', 'Pure-Tone Audiometry', 'Questionnaires', 'Records', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Role', 'Self Efficacy', 'Sequence Alignment', 'Severities', 'Speech', 'Speech Audiometry', 'Speech Perception', 'Stimulus', 'Supervision', 'System', 'Technology', 'Test Result', 'Testing', 'Time', 'Training', 'Universities', 'Vision', 'Voice', 'Washington', 'Work', 'clinical practice', 'comparison group', 'data modeling', 'design', 'fitness', 'hearing impairment', 'hearing screening', 'improved', 'lectures', 'permissiveness', 'phonology', 'response', 'satisfaction', 'social cognition', 'speech processing', 'speech recognition', 'touchscreen']",NIDCD,"SEEHEAR, LLC",R43,2017,147919,0.2577474862845149
"Prolonging Functional Speech in Persons with Amyotrophic Lateral Sclerosis: A Real-Time Virtual Vocal Tract People with ALS eventually and inevitably experience serious speech impairment due to progressive deterioration of brain cells that control movements of the tongue, lips and jaw. Despite the devastating consequences of this speech impairment on quality of life and survival, few options are available to assist impaired oral communication, and many existing speech-generating technologies are slow to operate and cost prohibitive. This project seeks to improve quality of life for persons with impaired speech due to ALS by testing the effectiveness of a low-cost, speech-generating device (a virtual vocal tract) that could significantly prolong the ability of these patients to communicate orally. If successful, these techniques could be extended for use by patients' with a broad range of speech motor impairments. The virtual vocal track uses machine learning algorithms to predict what a person is attempting to say, in real-time, based solely on lip movements. Users of the device are able to trigger the playback of a number of predetermined phrases by simply attempting to articulate what they want to say. Our previous work has shown the feasibility of this approach using cost-prohibitive laboratory systems such as electromagnetic articulography. Recent advances in 3D depth mapping camera technology allow these techniques to be tested for the first time using technologies, which are low-cost, portable and already being integrated into consumer devices such as laptops and cellphones. To this end, the system under development will be tested in 60 patients with ALS, representing a range of speech impairment from normal to severe speech intelligibility (15 normal, 15 mild, 15 moderate, 15 severe). During testing, participants will be cued to articulate the phrases in a random order as fast as is comfortable for them. The entire session will be recorded and the following variables will be measured offline: recognition accuracy, recognition latency, task time, % completion, and communication rate (words per minute). Users will rate the usability and acceptability of the virtual vocal tract immediately following device testing, using the System Usability Scale. Results of this testing will be used to address the following specific aims: (1) Determine the accuracy and latency of real-time phrase synthesis based on dysarthric speech using the virtual vocal tract, (2) Determine the usability and acceptability of real-time phrases produced using the virtual vocal tract, and (3) Identify the articulatory and speech factors that degrade recognition accuracy. People with ALS eventually and inevitably experience serious speech impairment due to progressive bulbar motor deterioration. Despite the devastating consequences of this impairment to quality of life, few options are available to assist or prolong impaired oral communication. The goal of the proposed work is to lay the groundwork for development of a low-cost speech-generating devicea virtual vocal tractthat can prolong functional oral communication for people with bulbar motor deterioration. This project seeks to testing the efficacy of a low-cost, speech-generating device (a virtual vocal tract) that records lip movements in real-time and triggers the playback of prerecorded phrases as users articulate what they want to say. If successful, the virtual device could provide an alternative means of oral communication for the large number of persons with unintelligible speech but still able to move their oral structures.",Prolonging Functional Speech in Persons with Amyotrophic Lateral Sclerosis: A Real-Time Virtual Vocal Tract,9719822,K24DC016312,"['3-Dimensional', 'Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Articulation', 'Articulators', 'Bypass', 'Cellular Phone', 'Cerebral Palsy', 'Characteristics', 'Communication', 'Complex', 'Cues', 'Data', 'Deterioration', 'Development', 'Devices', 'Disease', 'Dysarthria', 'Effectiveness', 'Electromagnetics', 'Ensure', 'Future', 'Generations', 'Goals', 'Impairment', 'Individual', 'Jaw', 'Laboratories', 'Learning', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Modification', 'Motion', 'Motor', 'Movement', 'Multiple Sclerosis', 'Oral', 'Output', 'Parkinson Disease', 'Participant', 'Patients', 'Performance', 'Persons', 'Play', 'Quality of life', 'Questionnaires', 'Records', 'Research', 'Research Personnel', 'Running', 'Severities', 'Speech', 'Speech Intelligibility', 'Speech Sound', 'Speed', 'Stroke', 'Structure', 'Surveys', 'System', 'Tablet Computer', 'Tablets', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Voice', 'Work', 'base', 'brain cell', 'clear speech', 'cost', 'efficacy testing', 'experience', 'experimental study', 'improved', 'innovation', 'jaw movement', 'laptop', 'machine learning algorithm', 'motor impairment', 'novel', 'oral communication', 'orofacial', 'phrases', 'portability', 'spatiotemporal', 'time use', 'usability', 'virtual', 'virtual vocal tract']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,K24,2019,189841,0.22290514792170246
"Prolonging Functional Speech in Persons with Amyotrophic Lateral Sclerosis: A Real-Time Virtual Vocal Tract People with ALS eventually and inevitably experience serious speech impairment due to progressive deterioration of brain cells that control movements of the tongue, lips and jaw. Despite the devastating consequences of this speech impairment on quality of life and survival, few options are available to assist impaired oral communication, and many existing speech-generating technologies are slow to operate and cost prohibitive. This project seeks to improve quality of life for persons with impaired speech due to ALS by testing the effectiveness of a low-cost, speech-generating device (a virtual vocal tract) that could significantly prolong the ability of these patients to communicate orally. If successful, these techniques could be extended for use by patients' with a broad range of speech motor impairments. The virtual vocal track uses machine learning algorithms to predict what a person is attempting to say, in real-time, based solely on lip movements. Users of the device are able to trigger the playback of a number of predetermined phrases by simply attempting to articulate what they want to say. Our previous work has shown the feasibility of this approach using cost-prohibitive laboratory systems such as electromagnetic articulography. Recent advances in 3D depth mapping camera technology allow these techniques to be tested for the first time using technologies, which are low-cost, portable and already being integrated into consumer devices such as laptops and cellphones. To this end, the system under development will be tested in 60 patients with ALS, representing a range of speech impairment from normal to severe speech intelligibility (15 normal, 15 mild, 15 moderate, 15 severe). During testing, participants will be cued to articulate the phrases in a random order as fast as is comfortable for them. The entire session will be recorded and the following variables will be measured offline: recognition accuracy, recognition latency, task time, % completion, and communication rate (words per minute). Users will rate the usability and acceptability of the virtual vocal tract immediately following device testing, using the System Usability Scale. Results of this testing will be used to address the following specific aims: (1) Determine the accuracy and latency of real-time phrase synthesis based on dysarthric speech using the virtual vocal tract, (2) Determine the usability and acceptability of real-time phrases produced using the virtual vocal tract, and (3) Identify the articulatory and speech factors that degrade recognition accuracy. People with ALS eventually and inevitably experience serious speech impairment due to progressive bulbar motor deterioration. Despite the devastating consequences of this impairment to quality of life, few options are available to assist or prolong impaired oral communication. The goal of the proposed work is to lay the groundwork for development of a low-cost speech-generating devicea virtual vocal tractthat can prolong functional oral communication for people with bulbar motor deterioration. This project seeks to testing the efficacy of a low-cost, speech-generating device (a virtual vocal tract) that records lip movements in real-time and triggers the playback of prerecorded phrases as users articulate what they want to say. If successful, the virtual device could provide an alternative means of oral communication for the large number of persons with unintelligible speech but still able to move their oral structures.",Prolonging Functional Speech in Persons with Amyotrophic Lateral Sclerosis: A Real-Time Virtual Vocal Tract,9964782,K24DC016312,"['3-Dimensional', 'Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Articulation', 'Articulators', 'Bypass', 'Cellular Phone', 'Cerebral Palsy', 'Characteristics', 'Communication', 'Complex', 'Cues', 'Data', 'Deterioration', 'Development', 'Devices', 'Disease', 'Dysarthria', 'Effectiveness', 'Electromagnetics', 'Ensure', 'Future', 'Generations', 'Goals', 'Impairment', 'Individual', 'Jaw', 'Laboratories', 'Learning', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Modification', 'Motion', 'Motor', 'Movement', 'Multiple Sclerosis', 'Oral', 'Output', 'Parkinson Disease', 'Participant', 'Patients', 'Performance', 'Persons', 'Play', 'Quality of life', 'Questionnaires', 'Records', 'Research', 'Research Personnel', 'Running', 'Severities', 'Speech', 'Speech Intelligibility', 'Speech Sound', 'Speed', 'Stroke', 'Structure', 'Surveys', 'System', 'Tablet Computer', 'Tablets', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Voice', 'Work', 'base', 'brain cell', 'clear speech', 'cost', 'effectiveness testing', 'efficacy testing', 'experience', 'experimental study', 'improved', 'innovation', 'jaw movement', 'laptop', 'machine learning algorithm', 'motor impairment', 'novel', 'oral communication', 'orofacial', 'phrases', 'portability', 'spatiotemporal', 'time use', 'usability', 'virtual', 'virtual vocal tract']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,K24,2020,189841,0.22290514792170246
"Prolonging Functional Speech in Persons with Amyotrophic Lateral Sclerosis: A Real-Time Virtual Vocal Tract People with ALS eventually and inevitably experience serious speech impairment due to progressive deterioration of brain cells that control movements of the tongue, lips and jaw. Despite the devastating consequences of this speech impairment on quality of life and survival, few options are available to assist impaired oral communication, and many existing speech-generating technologies are slow to operate and cost prohibitive. This project seeks to improve quality of life for persons with impaired speech due to ALS by testing the effectiveness of a low-cost, speech-generating device (a virtual vocal tract) that could significantly prolong the ability of these patients to communicate orally. If successful, these techniques could be extended for use by patients' with a broad range of speech motor impairments. The virtual vocal track uses machine learning algorithms to predict what a person is attempting to say, in real-time, based solely on lip movements. Users of the device are able to trigger the playback of a number of predetermined phrases by simply attempting to articulate what they want to say. Our previous work has shown the feasibility of this approach using cost-prohibitive laboratory systems such as electromagnetic articulography. Recent advances in 3D depth mapping camera technology allow these techniques to be tested for the first time using technologies, which are low-cost, portable and already being integrated into consumer devices such as laptops and cellphones. To this end, the system under development will be tested in 60 patients with ALS, representing a range of speech impairment from normal to severe speech intelligibility (15 normal, 15 mild, 15 moderate, 15 severe). During testing, participants will be cued to articulate the phrases in a random order as fast as is comfortable for them. The entire session will be recorded and the following variables will be measured offline: recognition accuracy, recognition latency, task time, % completion, and communication rate (words per minute). Users will rate the usability and acceptability of the virtual vocal tract immediately following device testing, using the System Usability Scale. Results of this testing will be used to address the following specific aims: (1) Determine the accuracy and latency of real-time phrase synthesis based on dysarthric speech using the virtual vocal tract, (2) Determine the usability and acceptability of real-time phrases produced using the virtual vocal tract, and (3) Identify the articulatory and speech factors that degrade recognition accuracy. People with ALS eventually and inevitably experience serious speech impairment due to progressive bulbar motor deterioration. Despite the devastating consequences of this impairment to quality of life, few options are available to assist or prolong impaired oral communication. The goal of the proposed work is to lay the groundwork for development of a low-cost speech-generating devicea virtual vocal tractthat can prolong functional oral communication for people with bulbar motor deterioration. This project seeks to testing the efficacy of a low-cost, speech-generating device (a virtual vocal tract) that records lip movements in real-time and triggers the playback of prerecorded phrases as users articulate what they want to say. If successful, the virtual device could provide an alternative means of oral communication for the large number of persons with unintelligible speech but still able to move their oral structures.",Prolonging Functional Speech in Persons with Amyotrophic Lateral Sclerosis: A Real-Time Virtual Vocal Tract,9525935,K24DC016312,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Articulation', 'Articulators', 'Bypass', 'Cellular Phone', 'Cerebral Palsy', 'Characteristics', 'Communication', 'Complex', 'Cues', 'Data', 'Deterioration', 'Development', 'Devices', 'Disease', 'Dysarthria', 'Effectiveness', 'Electromagnetics', 'Ensure', 'Future', 'Generations', 'Goals', 'Impairment', 'Individual', 'Jaw', 'Laboratories', 'Learning', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Modification', 'Motion', 'Motor', 'Movement', 'Multiple Sclerosis', 'Oral', 'Output', 'Parkinson Disease', 'Participant', 'Patients', 'Performance', 'Persons', 'Play', 'Quality of life', 'Questionnaires', 'Records', 'Research', 'Research Personnel', 'Running', 'Severities', 'Speech', 'Speech Intelligibility', 'Speech Sound', 'Speed', 'Stroke', 'Structure', 'Surveys', 'System', 'Tablet Computer', 'Tablets', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Voice', 'Work', 'base', 'brain cell', 'clear speech', 'cost', 'efficacy testing', 'experience', 'experimental study', 'improved', 'innovation', 'jaw movement', 'laptop', 'motor impairment', 'novel', 'oral communication', 'orofacial', 'phrases', 'portability', 'spatiotemporal', 'time use', 'usability', 'virtual', 'virtual vocal tract']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,K24,2018,189841,0.22290514792170246
"Prolonging Functional Speech in Persons with Amyotrophic Lateral Sclerosis: A Real-Time Virtual Vocal Tract People with ALS eventually and inevitably experience serious speech impairment due to progressive deterioration of brain cells that control movements of the tongue, lips and jaw. Despite the devastating consequences of this speech impairment on quality of life and survival, few options are available to assist impaired oral communication, and many existing speech-generating technologies are slow to operate and cost prohibitive. This project seeks to improve quality of life for persons with impaired speech due to ALS by testing the effectiveness of a low-cost, speech-generating device (a virtual vocal tract) that could significantly prolong the ability of these patients to communicate orally. If successful, these techniques could be extended for use by patients' with a broad range of speech motor impairments. The virtual vocal track uses machine learning algorithms to predict what a person is attempting to say, in real-time, based solely on lip movements. Users of the device are able to trigger the playback of a number of predetermined phrases by simply attempting to articulate what they want to say. Our previous work has shown the feasibility of this approach using cost-prohibitive laboratory systems such as electromagnetic articulography. Recent advances in 3D depth mapping camera technology allow these techniques to be tested for the first time using technologies, which are low-cost, portable and already being integrated into consumer devices such as laptops and cellphones. To this end, the system under development will be tested in 60 patients with ALS, representing a range of speech impairment from normal to severe speech intelligibility (15 normal, 15 mild, 15 moderate, 15 severe). During testing, participants will be cued to articulate the phrases in a random order as fast as is comfortable for them. The entire session will be recorded and the following variables will be measured offline: recognition accuracy, recognition latency, task time, % completion, and communication rate (words per minute). Users will rate the usability and acceptability of the virtual vocal tract immediately following device testing, using the System Usability Scale. Results of this testing will be used to address the following specific aims: (1) Determine the accuracy and latency of real-time phrase synthesis based on dysarthric speech using the virtual vocal tract, (2) Determine the usability and acceptability of real-time phrases produced using the virtual vocal tract, and (3) Identify the articulatory and speech factors that degrade recognition accuracy. People with ALS eventually and inevitably experience serious speech impairment due to progressive bulbar motor deterioration. Despite the devastating consequences of this impairment to quality of life, few options are available to assist or prolong impaired oral communication. The goal of the proposed work is to lay the groundwork for development of a low-cost speech-generating devicea virtual vocal tractthat can prolong functional oral communication for people with bulbar motor deterioration. This project seeks to testing the efficacy of a low-cost, speech-generating device (a virtual vocal tract) that records lip movements in real-time and triggers the playback of prerecorded phrases as users articulate what they want to say. If successful, the virtual device could provide an alternative means of oral communication for the large number of persons with unintelligible speech but still able to move their oral structures.",Prolonging Functional Speech in Persons with Amyotrophic Lateral Sclerosis: A Real-Time Virtual Vocal Tract,9370414,K24DC016312,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Articulation', 'Articulators', 'Bypass', 'Cellular Phone', 'Cerebral Palsy', 'Characteristics', 'Communication', 'Complex', 'Cues', 'Data', 'Deterioration', 'Development', 'Devices', 'Disease', 'Dysarthria', 'Effectiveness', 'Electromagnetics', 'Ensure', 'Future', 'Generations', 'Goals', 'Impairment', 'Individual', 'Jaw', 'Laboratories', 'Learning', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Modification', 'Motion', 'Motor', 'Movement', 'Multiple Sclerosis', 'Oral', 'Output', 'Parkinson Disease', 'Participant', 'Patients', 'Performance', 'Persons', 'Play', 'Quality of life', 'Questionnaires', 'Records', 'Research', 'Research Personnel', 'Running', 'Severities', 'Speech', 'Speech Intelligibility', 'Speech Sound', 'Speed', 'Stroke', 'Structure', 'Surveys', 'System', 'Tablet Computer', 'Tablets', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Voice', 'Work', 'base', 'brain cell', 'clear speech', 'cost', 'efficacy testing', 'experience', 'experimental study', 'improved', 'innovation', 'jaw movement', 'laptop', 'motor impairment', 'novel', 'oral communication', 'orofacial', 'phrases', 'portability', 'spatiotemporal', 'time use', 'usability', 'virtual']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,K24,2017,189841,0.22290514792170246
"Development of brain-computer interface methods to influence brain dynamics in stuttering Project summary Brain dynamics that drive variability within and between patients are an important, but poorly understood, element of many cognitive disorders. The long-term goal of this research project is to develop technology that will identify brain activity patterns associated with successful performance on a given task, and use this pattern as a target for brain-computer interface (BCI) training. The overarching hypothesis is that using BCI training to more often have a brain state that is spontaneously correlated to good performance will, in turn, improve overall performance. This approach could be developed into a powerful tool for rehabilitation and therapy for many neurological and psychiatric disorders. Here we will investigate persistent developmental stuttering (PDS) as a model to study brain dynamics associated with successful vs. unsuccessful performance. PDS is a speech disorder where fluent speech is punctuated to various degrees by stuttering. Individuals with PDS are otherwise neurologically in the normal range, which avoids complicating factors in most patient populations. Stuttering is intermittent; thus on some occasions the brain is in a state conducive to fluent speech and at other times it is not. We propose to use EEG activity shortly before speaking to predict whether somebody with PDS will stutter or speak fluently. Preliminary data are given to show proof of concept with traditional EEG analysis methods. This approach will be expanded by first using advanced methods such as common spatial pattern analysis and machine learning over multiple subject sessions to identify EEG signals that distinguish fluent vs. dysfluent trials (Aim 1). PDS subjects will then be trained to produce and maintain their EEG pattern that is most strongly associated with fluent speech by using BCI methods. We hypothesize that individuals will learn to modulate EEG features to be more consistent with fluent trials, which in turn will significantly reduce stuttering rate. After successful completion of this project we envision a new BCI-based intervention that can be used to encourage neural states conducive to fluent speech in those who stutter. The BCI intervention would complement traditional speech therapy using behavioral methods. The two-step approach of first identifying brain states associated with a patients best performance followed by BCI training to enter that state more often can be applied to rehabilitation in many other neurological and psychiatric disorders, such as Alzheimers disease, traumatic brain injury, and mood disorders, to name a few. Project narrative The goal of this project is to develop brain-computer interface technology to optimize brain function on an individual basis. This could have therapeutic applications to many neurological and psychiatric disorders, including stroke, Alzheimers disease, and traumatic brain injury.",Development of brain-computer interface methods to influence brain dynamics in stuttering,9659309,R21DC016353,"['Alzheimer&apos', 's Disease', 'Behavioral', 'Brain', 'Brain region', 'Cognition Disorders', 'Complement', 'Control Groups', 'Cues', 'Data', 'Development', 'Developmental Stuttering', 'Electroencephalography', 'Elements', 'Failure', 'Feedback', 'Frequencies', 'Goals', 'Individual', 'Intervention', 'Learning', 'Machine Learning', 'Memory', 'Mental disorders', 'Metaphor', 'Methods', 'Modeling', 'Mood Disorders', 'Names', 'Neurologic', 'Normal Range', 'Patients', 'Pattern', 'Performance', 'Physiology', 'Rehabilitation therapy', 'Research Project Grants', 'Sensitivity and Specificity', 'Severities', 'Signal Transduction', 'Speech', 'Speech Disorders', 'Speech Therapy', 'Stroke', 'Stuttering', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Traumatic Brain Injury', 'Work', 'base', 'brain computer interface', 'improved', 'indexing', 'motor control', 'nervous system disorder', 'patient population', 'relating to nervous system', 'response', 'tool', 'visual feedback']",NIDCD,UNIVERSITY OF TEXAS SAN ANTONIO,R21,2019,170514,0.0718088783687981
"Development of brain-computer interface methods to influence brain dynamics in stuttering Project summary Brain dynamics that drive variability within and between patients are an important, but poorly understood, element of many cognitive disorders. The long-term goal of this research project is to develop technology that will identify brain activity patterns associated with successful performance on a given task, and use this pattern as a target for brain-computer interface (BCI) training. The overarching hypothesis is that using BCI training to more often have a brain state that is spontaneously correlated to good performance will, in turn, improve overall performance. This approach could be developed into a powerful tool for rehabilitation and therapy for many neurological and psychiatric disorders. Here we will investigate persistent developmental stuttering (PDS) as a model to study brain dynamics associated with successful vs. unsuccessful performance. PDS is a speech disorder where fluent speech is punctuated to various degrees by stuttering. Individuals with PDS are otherwise neurologically in the normal range, which avoids complicating factors in most patient populations. Stuttering is intermittent; thus on some occasions the brain is in a state conducive to fluent speech and at other times it is not. We propose to use EEG activity shortly before speaking to predict whether somebody with PDS will stutter or speak fluently. Preliminary data are given to show proof of concept with traditional EEG analysis methods. This approach will be expanded by first using advanced methods such as common spatial pattern analysis and machine learning over multiple subject sessions to identify EEG signals that distinguish fluent vs. dysfluent trials (Aim 1). PDS subjects will then be trained to produce and maintain their EEG pattern that is most strongly associated with fluent speech by using BCI methods. We hypothesize that individuals will learn to modulate EEG features to be more consistent with fluent trials, which in turn will significantly reduce stuttering rate. After successful completion of this project we envision a new BCI-based intervention that can be used to encourage neural states conducive to fluent speech in those who stutter. The BCI intervention would complement traditional speech therapy using behavioral methods. The two-step approach of first identifying brain states associated with a patients best performance followed by BCI training to enter that state more often can be applied to rehabilitation in many other neurological and psychiatric disorders, such as Alzheimers disease, traumatic brain injury, and mood disorders, to name a few. Project narrative The goal of this project is to develop brain-computer interface technology to optimize brain function on an individual basis. This could have therapeutic applications to many neurological and psychiatric disorders, including stroke, Alzheimers disease, and traumatic brain injury.",Development of brain-computer interface methods to influence brain dynamics in stuttering,9530270,R21DC016353,"['Alzheimer&apos', 's Disease', 'Behavioral', 'Brain', 'Brain region', 'Cognition Disorders', 'Complement', 'Control Groups', 'Cues', 'Data', 'Development', 'Developmental Stuttering', 'Electroencephalography', 'Elements', 'Failure', 'Feedback', 'Frequencies', 'Goals', 'Individual', 'Intervention', 'Learning', 'Machine Learning', 'Memory', 'Mental disorders', 'Metaphor', 'Methods', 'Modeling', 'Mood Disorders', 'Names', 'Neurologic', 'Normal Range', 'Patients', 'Pattern', 'Performance', 'Physiology', 'Rehabilitation therapy', 'Research Project Grants', 'Sensitivity and Specificity', 'Severities', 'Signal Transduction', 'Speech', 'Speech Disorders', 'Speech Therapy', 'Stroke', 'Stuttering', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Traumatic Brain Injury', 'Work', 'base', 'brain computer interface', 'improved', 'indexing', 'motor control', 'nervous system disorder', 'patient population', 'relating to nervous system', 'response', 'tool', 'visual feedback']",NIDCD,UNIVERSITY OF TEXAS SAN ANTONIO,R21,2018,216658,0.0718088783687981
"Wearable silent speech technology to enhance impaired oral communication Project Summary/Abstract The long-term objectives of this project are to obtain a deeper understanding how articulatory movement patterns are mapped to speech particularly when there is no vocal fold vibration (silent speech) and then to develop a novel, wearable assistive technology called silent speech interface (SSI) to assist the impaired oral communication for individuals in need (e.g., individuals after laryngectomy, surgical removal of larynx to treat advanced laryngeal cancer). Designed for daily use, the SSI contains a wearable magnetic device and a small camera for tongue and lip motion tracking, respectively, and an articulation-to-speech synthesizer to output natural sounding speech that preserves the speakers voice characteristics. Specific Aims of the proposal include to (1) determine the articulatory patterns of normal (vocalized) and silent speech, produced by both healthy talkers and people after laryngectomy, (2) develop a wearable, wireless magnetic device for real-time tongue and lip motion tracking, and (3) synthesize speech from articulation directly. There are currently limited alternative communication options for people who have undergone laryngectomy. These options include esophageal speech, tracheo-esophageal speech, and use of an artificial larynx (or electrolarynx). These solutions are either invasive or difficult to use, and all of them result in a hoarse or mechanical/robotic sounding voice, which can be difficult to understand. In contrast, the SSI in this application is non-invasive, easy-to-use, and produces natural sounding speech and may even preserve the patients voice identity. We have exciting preliminary results that support the feasibility of the project including that (1) we have recently developed a wireless magnetic device for tongue motion, and (2) we have demonstrated real- time articulation-to-speech synthesis with a 90% word accuracy (judged by a human listener). In this project, we will further reduce the size of the wireless device and make it wearable and conduct articulation-to-speech algorithms by studying 30 participants after laryngectomy and 30 age- and gender-matched healthy controls. If successful, the proposed research will enhance human health by making an impact on individuals after laryngectomy and potentially to a broader range of other speech and voice disorders. In addition, the technology will have an impact to the speech science field by providing a fist-time-ever tool for potential large- scale tongue motion data collection and have a variety of broader implications including visual feedback-based secondary language training and speech therapy, which may benefit millions of people with motor speech deficits in the United States. Project Narrative Silent speech interfaces (SSI) is a novel assistive technology for enhancing the oral communication for people who are unable to produce speech sounds (e.g., individuals who undergo laryngectomy, removal of larynx to treat advanced laryngeal cancer). The proposed SSI is a wearable device for tongue motion tracking and produces synthesized, natural sounding speech that preserves the patients voice characteristics in real-time, which holds potential to enhance the speech health and quality of life of laryngectomees. The technology also has potential for a variety of broader applications including visual feedback-based secondary language training and speech therapy.",Wearable silent speech technology to enhance impaired oral communication,9994877,R01DC016621,"['Acoustics', 'Age', 'Alaryngeal Speech', 'Algorithms', 'Articular Range of Motion', 'Articulation', 'Articulators', 'Characteristics', 'Communication', 'Data', 'Data Collection', 'Development', 'Devices', 'Electrolarynx', 'Electromagnetics', 'Enhancement Technology', 'Esophageal Speech', 'Excision', 'Gender', 'Goals', 'Gold', 'Health', 'Hoarseness', 'Human', 'Impairment', 'Individual', 'Knowledge', 'Laryngeal Prosthesis', 'Laryngectomee', 'Laryngectomy', 'Larynx', 'Life', 'Lip structure', 'Machine Learning', 'Magnetism', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Mechanics', 'Mental Depression', 'Motion', 'Motor', 'Movement', 'Output', 'Participant', 'Patients', 'Pattern', 'Performance', 'Population', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Robotics', 'Science', 'Self-Help Devices', 'Speech', 'Speech Disorders', 'Speech Sound', 'Speech Synthesizers', 'Speech Therapy', 'Speed', 'Technology', 'Testing', 'Time', 'Tongue', 'Tracer', 'Tracheoesophageal Speech', 'United States', 'Voice', 'Voice Disorders', 'Voice Quality', 'Wireless Technology', 'alternative communication', 'auditory feedback', 'base', 'design', 'experimental study', 'improved', 'innovation', 'kinematics', 'language training', 'machine learning algorithm', 'millisecond', 'new technology', 'novel', 'oral communication', 'preservation', 'prototype', 'social', 'social exclusion', 'sound', 'speech synthesis', 'tool', 'vibration', 'visual feedback', 'vocal cord', 'wearable device']",NIDCD,"UNIVERSITY OF TEXAS, AUSTIN",R01,2020,580870,0.34194240204149806
"Wearable silent speech technology to enhance impaired oral communication Project Summary/Abstract The long-term objectives of this project are to obtain a deeper understanding how articulatory movement patterns are mapped to speech particularly when there is no vocal fold vibration (silent speech) and then to develop a novel, wearable assistive technology called silent speech interface (SSI) to assist the impaired oral communication for individuals in need (e.g., individuals after laryngectomy, surgical removal of larynx to treat advanced laryngeal cancer). Designed for daily use, the SSI contains a wearable magnetic device and a small camera for tongue and lip motion tracking, respectively, and an articulation-to-speech synthesizer to output natural sounding speech that preserves the speakers voice characteristics. Specific Aims of the proposal include to (1) determine the articulatory patterns of normal (vocalized) and silent speech, produced by both healthy talkers and people after laryngectomy, (2) develop a wearable, wireless magnetic device for real-time tongue and lip motion tracking, and (3) synthesize speech from articulation directly. There are currently limited alternative communication options for people who have undergone laryngectomy. These options include esophageal speech, tracheo-esophageal speech, and use of an artificial larynx (or electrolarynx). These solutions are either invasive or difficult to use, and all of them result in a hoarse or mechanical/robotic sounding voice, which can be difficult to understand. In contrast, the SSI in this application is non-invasive, easy-to-use, and produces natural sounding speech and may even preserve the patients voice identity. We have exciting preliminary results that support the feasibility of the project including that (1) we have recently developed a wireless magnetic device for tongue motion, and (2) we have demonstrated real- time articulation-to-speech synthesis with a 90% word accuracy (judged by a human listener). In this project, we will further reduce the size of the wireless device and make it wearable and conduct articulation-to-speech algorithms by studying 30 participants after laryngectomy and 30 age- and gender-matched healthy controls. If successful, the proposed research will enhance human health by making an impact on individuals after laryngectomy and potentially to a broader range of other speech and voice disorders. In addition, the technology will have an impact to the speech science field by providing a fist-time-ever tool for potential large- scale tongue motion data collection and have a variety of broader implications including visual feedback-based secondary language training and speech therapy, which may benefit millions of people with motor speech deficits in the United States. Project Narrative Silent speech interfaces (SSI) is a novel assistive technology for enhancing the oral communication for people who are unable to produce speech sounds (e.g., individuals who undergo laryngectomy, removal of larynx to treat advanced laryngeal cancer). The proposed SSI is a wearable device for tongue motion tracking and produces synthesized, natural sounding speech that preserves the patients voice characteristics in real-time, which holds potential to enhance the speech health and quality of life of laryngectomees. The technology also has potential for a variety of broader applications including visual feedback-based secondary language training and speech therapy.",Wearable silent speech technology to enhance impaired oral communication,9740858,R01DC016621,"['Acoustics', 'Age', 'Alaryngeal Speech', 'Algorithms', 'Articular Range of Motion', 'Articulation', 'Articulators', 'Characteristics', 'Communication', 'Data', 'Data Collection', 'Development', 'Devices', 'Electrolarynx', 'Electromagnetics', 'Enhancement Technology', 'Esophageal Speech', 'Excision', 'Gender', 'Goals', 'Gold', 'Health', 'Hoarseness', 'Human', 'Impairment', 'Individual', 'Knowledge', 'Laryngeal Prosthesis', 'Laryngectomee', 'Laryngectomy', 'Larynx', 'Life', 'Lip structure', 'Machine Learning', 'Magnetism', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Mechanics', 'Mental Depression', 'Motion', 'Motor', 'Movement', 'Output', 'Participant', 'Patients', 'Pattern', 'Performance', 'Population', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Robotics', 'Science', 'Self-Help Devices', 'Speech', 'Speech Disorders', 'Speech Sound', 'Speech Synthesizers', 'Speech Therapy', 'Speed', 'Technology', 'Testing', 'Time', 'Tongue', 'Tracer', 'Tracheoesophageal Speech', 'United States', 'Voice', 'Voice Disorders', 'Voice Quality', 'Wireless Technology', 'alternative communication', 'auditory feedback', 'base', 'design', 'experimental study', 'improved', 'innovation', 'kinematics', 'language training', 'machine learning algorithm', 'millisecond', 'new technology', 'novel', 'oral communication', 'preservation', 'prototype', 'social', 'social exclusion', 'sound', 'speech synthesis', 'tool', 'vibration', 'visual feedback', 'vocal cord', 'wearable device']",NIDCD,"UNIVERSITY OF TEXAS, AUSTIN",R01,2019,618564,0.34194240204149806
"Speech markers of cognitive impairment in Parkinson's disease ABSTRACT Dr. Kara Smith is a Movement Disorders neurologist at the University of Massachusetts Medical School (UMMS) whose goal is to become an independent investigator focused on early cognitive impairment in Parkinson disease (PD). Her long-term goal is to develop speech markers of cognitive impairment in PD. Cognitive impairment occurs in the majority of PD patients, leading to increased mortality and decreased quality of life. The current diagnostic tools are resource-intensive and have limited sensitivity. Treatments are often offered late in the course of cognitive decline and do not provide optimal benefit. Speech markers could improve detection, monitoring and treatment of cognitive impairment in PD. Speech markers could be monitored frequently and remotely via mobile technology, capturing sensitive, quantitative data about cognitive function in the context of patients daily life and in response to therapeutics. Dr. Smiths role as a clinical movement disorders specialist ideally positions her to lead the application of advanced speech and language research to feasible, patient-oriented tools for real-life clinical practice and clinical trials. Dr. Smith has assembled an expert interdisciplinary mentorship team ideally suited for the goals of this innovative proposal. Dr. Smith and her team have previously shown that a) speech acoustic markers are associated with cognitive function in non-demented PD patients, and b) PD patients with mild cognitive impairment had linguistic deficits including pauses within utterances and grammaticality. Building on these results, Dr. Smith proposes to study speech and language more comprehensively in PD patients with and without mild cognitive impairment and controls to confirm these preliminary results and identify additional biomarkers. The aims of this study will be 1) to develop algorithms using speech acoustic markers to categorize by cognitive status, 2) to identify linguistic markers associated with mild cognitive impairment in PD, and 3) to assess on-line syntactic processing in PD subjects with mild cognitive impairment. The overall goal of the proposal is to identify speech and language markers of early cognitive dysfunction that can be further refined, validated and implemented using mobile technology into a larger scale, longitudinal R01 proposal. Further work will also address the underlying neurobiological mechanisms of these speech markers. Dr. Smiths rigorous training plan includes a Masters degree, linguistics and speech motor physiology courses, and experience in signal processing and speech acoustic analysis. Through her training goals, she will advance her knowledge and skills in patient-centered outcomes measures and instrument validation. She will gain experience in research leadership, presentation and dissemination of scientific work, and in grant writing, culminating in an R01 proposal. This K23 award will be critical for Dr. Smith to establish an independent career as a PD clinician-scientist at the unique intersection of speech and language science and cognitive impairment. PUBLIC HEALTH RELEVANCE: Speech markers have the potential to improve diagnosis, monitoring and treatment of cognitive impairment in Parkinsons disease (PD). Although the majority of PD patients will develop cognitive impairment, the tools available to assess and treat this disabling complication are fraught with limitations. As a detailed and quantitative assessment tool, speech markers may increase sensitivity to early cognitive dysfunction and to changes over time compared with current measures. They may be automated and then implemented through mobile technology to increase patients access to cognitive symptom monitoring outside of the clinic setting. Dr. Smiths proposed career development plan has potential to fill a major gap in PD research by making inexpensive and easy-to-use cognitive assessment tools accessible to patients in rural and international settings, and by fueling clinical trials to discover new therapeutics capable of slowing cognitive decline in PD.",Speech markers of cognitive impairment in Parkinson's disease,9883772,K23DC016656,"['Acoustics', 'Address', 'Algorithms', 'American', 'Area', 'Articulation', 'Assessment tool', 'Award', 'Biological Markers', 'Biomedical Engineering', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Cognitive Therapy', 'Cognitive deficits', 'Complication', 'Comprehension', 'Data', 'Data Analyses', 'Dementia', 'Detection', 'Development', 'Development Plans', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Foundations', 'Future', 'Goals', 'Grant', 'Health Services Accessibility', 'Impaired cognition', 'Impairment', 'Individual', 'International', 'Knowledge', 'Language', 'Language Disorders', 'Lead', 'Leadership', 'Life', 'Linguistics', 'Location', 'Longitudinal Studies', 'Machine Learning', 'Massachusetts', 'Master&apos', 's Degree', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement Disorders', 'Nature', 'Nerve Degeneration', 'Neurobehavioral Manifestations', 'Neurobiology', 'Neurologist', 'Neuropsychological Tests', 'Outcome', 'Outcome Measure', 'Outcomes Research', 'Parkinson Disease', 'Parkinson&apos', 's Dementia', 'Participant', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physiology', 'Population', 'Positioning Attribute', 'Production', 'Proxy', 'Quality of Care', 'Quality of life', 'Research', 'Research Personnel', 'Resources', 'Role', 'Rural', 'Science', 'Scientist', 'Severities', 'Specialist', 'Speech', 'Speech Acoustics', 'Symptoms', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Universities', 'Validation', 'Work', 'Writing', 'career', 'career development', 'clinical movement disorder', 'clinical practice', 'cognitive change', 'cognitive control', 'cognitive function', 'cognitive impairment in Parkinson&apos', 's', 'cognitive performance', 'cognitive testing', 'common symptom', 'experience', 'handheld mobile device', 'improved', 'innovation', 'instrument', 'language processing', 'large scale data', 'lexical retrieval', 'machine learning method', 'medical schools', 'mild cognitive impairment', 'mobile computing', 'mortality', 'motor deficit', 'neurobiological mechanism', 'non-demented', 'novel', 'novel therapeutics', 'patient oriented', 'public health relevance', 'recruit', 'response', 'screening', 'signal processing', 'skills', 'syntax', 'tool']",NIDCD,UNIV OF MASSACHUSETTS MED SCH WORCESTER,K23,2020,189216,0.2779996673548657
"Speech markers of cognitive impairment in Parkinson's disease ABSTRACT Dr. Kara Smith is a Movement Disorders neurologist at the University of Massachusetts Medical School (UMMS) whose goal is to become an independent investigator focused on early cognitive impairment in Parkinson disease (PD). Her long-term goal is to develop speech markers of cognitive impairment in PD. Cognitive impairment occurs in the majority of PD patients, leading to increased mortality and decreased quality of life. The current diagnostic tools are resource-intensive and have limited sensitivity. Treatments are often offered late in the course of cognitive decline and do not provide optimal benefit. Speech markers could improve detection, monitoring and treatment of cognitive impairment in PD. Speech markers could be monitored frequently and remotely via mobile technology, capturing sensitive, quantitative data about cognitive function in the context of patients daily life and in response to therapeutics. Dr. Smiths role as a clinical movement disorders specialist ideally positions her to lead the application of advanced speech and language research to feasible, patient-oriented tools for real-life clinical practice and clinical trials. Dr. Smith has assembled an expert interdisciplinary mentorship team ideally suited for the goals of this innovative proposal. Dr. Smith and her team have previously shown that a) speech acoustic markers are associated with cognitive function in non-demented PD patients, and b) PD patients with mild cognitive impairment had linguistic deficits including pauses within utterances and grammaticality. Building on these results, Dr. Smith proposes to study speech and language more comprehensively in PD patients with and without mild cognitive impairment and controls to confirm these preliminary results and identify additional biomarkers. The aims of this study will be 1) to develop algorithms using speech acoustic markers to categorize by cognitive status, 2) to identify linguistic markers associated with mild cognitive impairment in PD, and 3) to assess on-line syntactic processing in PD subjects with mild cognitive impairment. The overall goal of the proposal is to identify speech and language markers of early cognitive dysfunction that can be further refined, validated and implemented using mobile technology into a larger scale, longitudinal R01 proposal. Further work will also address the underlying neurobiological mechanisms of these speech markers. Dr. Smiths rigorous training plan includes a Masters degree, linguistics and speech motor physiology courses, and experience in signal processing and speech acoustic analysis. Through her training goals, she will advance her knowledge and skills in patient-centered outcomes measures and instrument validation. She will gain experience in research leadership, presentation and dissemination of scientific work, and in grant writing, culminating in an R01 proposal. This K23 award will be critical for Dr. Smith to establish an independent career as a PD clinician-scientist at the unique intersection of speech and language science and cognitive impairment. PUBLIC HEALTH RELEVANCE: Speech markers have the potential to improve diagnosis, monitoring and treatment of cognitive impairment in Parkinsons disease (PD). Although the majority of PD patients will develop cognitive impairment, the tools available to assess and treat this disabling complication are fraught with limitations. As a detailed and quantitative assessment tool, speech markers may increase sensitivity to early cognitive dysfunction and to changes over time compared with current measures. They may be automated and then implemented through mobile technology to increase patients access to cognitive symptom monitoring outside of the clinic setting. Dr. Smiths proposed career development plan has potential to fill a major gap in PD research by making inexpensive and easy-to-use cognitive assessment tools accessible to patients in rural and international settings, and by fueling clinical trials to discover new therapeutics capable of slowing cognitive decline in PD.",Speech markers of cognitive impairment in Parkinson's disease,9666574,K23DC016656,"['Acoustics', 'Address', 'Algorithms', 'American', 'Area', 'Articulation', 'Assessment tool', 'Award', 'Biological Markers', 'Biomedical Engineering', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Cognitive Therapy', 'Cognitive deficits', 'Complication', 'Comprehension', 'Data', 'Data Analyses', 'Dementia', 'Detection', 'Development', 'Development Plans', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Foundations', 'Future', 'Goals', 'Grant', 'Health Services Accessibility', 'Impaired cognition', 'Impairment', 'Individual', 'International', 'Knowledge', 'Language', 'Language Disorders', 'Lead', 'Leadership', 'Life', 'Linguistics', 'Location', 'Longitudinal Studies', 'Machine Learning', 'Massachusetts', 'Master&apos', 's Degree', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement Disorders', 'Nature', 'Nerve Degeneration', 'Neurobehavioral Manifestations', 'Neurobiology', 'Neurologist', 'Neuropsychological Tests', 'Outcome', 'Outcome Measure', 'Outcomes Research', 'Parkinson Disease', 'Parkinson&apos', 's Dementia', 'Participant', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physiology', 'Population', 'Positioning Attribute', 'Production', 'Proxy', 'Quality of Care', 'Quality of life', 'Research', 'Research Personnel', 'Resources', 'Role', 'Rural', 'Science', 'Scientist', 'Severities', 'Specialist', 'Speech', 'Speech Acoustics', 'Symptoms', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Universities', 'Validation', 'Work', 'Writing', 'career', 'career development', 'clinical movement disorder', 'clinical practice', 'cognitive change', 'cognitive control', 'cognitive function', 'cognitive performance', 'cognitive testing', 'common symptom', 'experience', 'handheld mobile device', 'improved', 'innovation', 'instrument', 'language processing', 'learning strategy', 'lexical retrieval', 'medical schools', 'mild cognitive impairment', 'mobile computing', 'mortality', 'motor deficit', 'neurobiological mechanism', 'non-demented', 'novel', 'novel therapeutics', 'patient oriented', 'public health relevance', 'recruit', 'response', 'screening', 'signal processing', 'skills', 'syntax', 'tool']",NIDCD,UNIV OF MASSACHUSETTS MED SCH WORCESTER,K23,2019,188946,0.2779996673548657
"Structural and functional connectivity markers of developmental speech and language disorders ABSTRACT Developmental speech and language disorders affect an estimated 15% of children and have lifelong impacts on social and emotional development and employment. Two common neurodevelopmental disorders are developmental language disorder (DLD; also called specific language impairment) and developmental stuttering, affecting 7% and 5% of children respectively. Despite their prevalence and immense impact, little is known of the neural causes, correlates, and consequences of these common neurodevelopmental disorders; thus, effective treatment remains elusive. In the proposed project, we will study the neural underpinnings of these disorders using magnetic resonance imaging (MRI) to study structural and functional neural connectivity. Previous studies of connectivity in these populations are limited and show little consensus, likely due in part to small sample sizes. Theoretical accounts of both disorders implicate dysfunctional neural circuits through the basal ganglia. In the current proposal, we will test and compare the structural and functional integrity of neural pathways in large cohorts of people with DLD (N=80) and people who stutter (PWS; N=80) and compare them with similar data obtained in age- and sex-matched control groups of people with typical development (N=160). First, we will assess connectivity in speech/language-specific networks, using diffusion data to assess structural connectivity and resting-state data to assess functional connectivity. Results will indicate abnormalities in connectivity in large cohorts of PWS and people with DLD. In each disorder, we will also determine connectivity contributions to individual differences in behavior. This will reveal how different connectivity patterns are correlated to differences in severity along relevant dimensions (e.g., fluency, language measures), ideally resulting in neural correlates of the disorders. Finally, we will evaluate whole-brain functional connectivity differences between each disorder group and its matched control group using data-driven machine learning approaches. Results will indicate patterns of neural activity that differentiate these disorders from controls. The outcome of this proposal will be the characterization of underlying network differences in these populations, which will ideally lead to the development of targeted behavioral and neuro-modulatory treatments of these multifaceted and pervasive disorders. Research and training will take place at the University of Oxford, an ideal environment in which to pursue this line of research. The applicant will be mentored by world-leading researchers with the knowledge needed to guide him in this work, including expertise in the neural bases of developmental speech and language disorders, cutting-edge methodology in neuroimaging, and machine learning. Achieving these aims will illuminate the neural correlates of these speech and language disorders as well as prepare the applicant for an independent research career in this area. PROJECT NARRATIVE For a surprisingly large number of children, as many as 15%, speech and language development does not go smoothly and results in disorders such as stuttering or language delay. We wish to understand more about the causes of these common disorders and will assess whether the brain networks involved in speech and language development are impaired in people with developmental language disorder and stuttering. Using large existing neuroimaging datasets obtained in these disorders will allow us to address important questions about the causes and consequences of these developmental disorders.",Structural and functional connectivity markers of developmental speech and language disorders,9944309,F32DC017637,"['Address', 'Adolescent', 'Adult', 'Affect', 'Age', 'Area', 'Auditory', 'Basal Ganglia', 'Behavior', 'Behavioral', 'Brain', 'Cerebral cortex', 'Child', 'Classification', 'Clinical', 'Comprehension', 'Consensus', 'Control Groups', 'Corpus striatum structure', 'Data', 'Data Set', 'Development', 'Developmental Stuttering', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Disease', 'Emotional', 'Employment', 'Environment', 'Event', 'Functional Imaging', 'Future', 'Heterogeneity', 'Image', 'Impairment', 'Individual', 'Individual Differences', 'Inferior frontal gyrus', 'Investigation', 'Knowledge', 'Language', 'Language Delays', 'Language Development', 'Language Development Disorders', 'Language Disorders', 'Lead', 'Length', 'Life', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Mentors', 'Methodology', 'Motor', 'Network-based', 'Neural Pathways', 'Neurodevelopmental Disorder', 'Neurophysiology - biologic function', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Pattern', 'Population', 'Prevalence', 'Process', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Rest', 'Sample Size', 'Sampling', 'Scanning', 'Semantics', 'Severities', 'Speech', 'Speech Development', 'Speech Disorders', 'Structure', 'Stuttering', 'Superior temporal gyrus', 'Testing', 'Universities', 'Work', 'base', 'career', 'caudate nucleus', 'cohort', 'developmental disease', 'disorder control', 'economic impact', 'effective therapy', 'interest', 'language impairment', 'motor control', 'neural circuit', 'neural correlate', 'neural network', 'neural patterning', 'neuroimaging', 'neuroregulation', 'putamen', 'relating to nervous system', 'sex', 'social', 'specific language impairment', 'standardize measure']",NIDCD,UNIVERSITY OF OXFORD,F32,2020,69426,0.10667602324523698
"Structural and functional connectivity markers of developmental speech and language disorders ABSTRACT Developmental speech and language disorders affect an estimated 15% of children and have lifelong impacts on social and emotional development and employment. Two common neurodevelopmental disorders are developmental language disorder (DLD; also called specific language impairment) and developmental stuttering, affecting 7% and 5% of children respectively. Despite their prevalence and immense impact, little is known of the neural causes, correlates, and consequences of these common neurodevelopmental disorders; thus, effective treatment remains elusive. In the proposed project, we will study the neural underpinnings of these disorders using magnetic resonance imaging (MRI) to study structural and functional neural connectivity. Previous studies of connectivity in these populations are limited and show little consensus, likely due in part to small sample sizes. Theoretical accounts of both disorders implicate dysfunctional neural circuits through the basal ganglia. In the current proposal, we will test and compare the structural and functional integrity of neural pathways in large cohorts of people with DLD (N=80) and people who stutter (PWS; N=80) and compare them with similar data obtained in age- and sex-matched control groups of people with typical development (N=160). First, we will assess connectivity in speech/language-specific networks, using diffusion data to assess structural connectivity and resting-state data to assess functional connectivity. Results will indicate abnormalities in connectivity in large cohorts of PWS and people with DLD. In each disorder, we will also determine connectivity contributions to individual differences in behavior. This will reveal how different connectivity patterns are correlated to differences in severity along relevant dimensions (e.g., fluency, language measures), ideally resulting in neural correlates of the disorders. Finally, we will evaluate whole-brain functional connectivity differences between each disorder group and its matched control group using data-driven machine learning approaches. Results will indicate patterns of neural activity that differentiate these disorders from controls. The outcome of this proposal will be the characterization of underlying network differences in these populations, which will ideally lead to the development of targeted behavioral and neuro-modulatory treatments of these multifaceted and pervasive disorders. Research and training will take place at the University of Oxford, an ideal environment in which to pursue this line of research. The applicant will be mentored by world-leading researchers with the knowledge needed to guide him in this work, including expertise in the neural bases of developmental speech and language disorders, cutting-edge methodology in neuroimaging, and machine learning. Achieving these aims will illuminate the neural correlates of these speech and language disorders as well as prepare the applicant for an independent research career in this area. PROJECT NARRATIVE For a surprisingly large number of children, as many as 15%, speech and language development does not go smoothly and results in disorders such as stuttering or language delay. We wish to understand more about the causes of these common disorders and will assess whether the brain networks involved in speech and language development are impaired in people with developmental language disorder and stuttering. Using large existing neuroimaging datasets obtained in these disorders will allow us to address important questions about the causes and consequences of these developmental disorders.",Structural and functional connectivity markers of developmental speech and language disorders,9757857,F32DC017637,"['Address', 'Adolescent', 'Adult', 'Affect', 'Age', 'Area', 'Auditory', 'Basal Ganglia', 'Behavior', 'Behavioral', 'Brain', 'Cerebral cortex', 'Child', 'Classification', 'Clinical', 'Consensus', 'Control Groups', 'Corpus striatum structure', 'Data', 'Data Set', 'Development', 'Developmental Stuttering', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Disease', 'Economics', 'Emotional', 'Employment', 'Environment', 'Event', 'Functional Imaging', 'Future', 'Heterogeneity', 'Image', 'Impairment', 'Individual', 'Individual Differences', 'Inferior frontal gyrus', 'Investigation', 'Knowledge', 'Language', 'Language Delays', 'Language Development', 'Language Development Disorders', 'Language Disorders', 'Lead', 'Length', 'Life', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Mentors', 'Methodology', 'Motor', 'Network-based', 'Neural Pathways', 'Neurodevelopmental Disorder', 'Neurophysiology - biologic function', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Pattern', 'Population', 'Prevalence', 'Process', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Rest', 'Sample Size', 'Scanning', 'Semantics', 'Severities', 'Speech', 'Speech Development', 'Speech Disorders', 'Structure', 'Stuttering', 'Superior temporal gyrus', 'Testing', 'Universities', 'Work', 'base', 'career', 'caudate nucleus', 'cohort', 'developmental disease', 'disorder control', 'effective therapy', 'interest', 'language impairment', 'motor control', 'neural circuit', 'neural correlate', 'neural patterning', 'neuroimaging', 'neuroregulation', 'putamen', 'relating to nervous system', 'sex', 'social', 'specific language impairment', 'standardize measure']",NIDCD,UNIVERSITY OF OXFORD,F32,2019,65354,0.10667602324523698
"Studying the Laryngeal Mechanisms Underlying Dysphonia in Connected Speech Project Summary/Abstract  This proposal aims to employ the recent advancement of coupling fiberoptic endoscopes with high-speed videoendoscopy (HSV) systems to obtain HSV recordings during connected speech. The goal is to study vocal mechanisms underlying dysphonia in patients with neurogenic voice disorders. The long-term goal of this line of research is to create clinically applicable quantitative methods for functional measurement of vocal fold vibration in connected speech using innovative laryngeal imaging, an approach that could advance clinical voice assessment and treatment practice. In Aim 1, HSV-based measures of vocal fold kinematics will be developed and the influence of these measures on voice audio-perceptual qualities in the patients will be determined. Image processing techniques will be developed to extract such measures from the HSV data in connected speech. The extracted measures will be given as inputs to the statistical models to determine the source of the differences between the normal controls and the patients for different speech phonetic contexts and words. This aim provides an unbiased HSV-based method to predict voice quality. Developing such HSV-based methodology for functional laryngeal examination in connected speech can enhance clinical voice assessment. In addition, better understanding the influence of phonetic context would lead to optimizing the protocols for functional voice assessment through laryngeal imaging in connected speech. In Aim 2, machine learning approaches will be employed to discover hidden physics and unknown laryngeal mechanisms of voice production in the dysphonic patients. The findings of this project will help make necessary adjustments in biomechanical or physiological characteristics of vocal folds to enhance voice quality in patients with neurogenic voice disorders. Therefore, the outcome of this research will aid clinicians in properly selecting, and developing new treatment strategies (therapeutic, medicinal, or surgical), which are based on the gained knowledge of laryngeal mechanisms of dysphonia. The proposed research is in harmony with multiple priority areas of the NIDCD, described in the 2017-2021 Strategic Plan. Both aims support Priority 3 (improve methods of diagnosis, treatment, and prevention) through developing objective HSV-based measures and predicting the voice quality. Comparing laryngeal mechanisms in normal and disordered voices addresses Priority 1 (deepen our understanding of the normal function of the systems of human communication). Both aims propose to study laryngeal mechanisms in patients with neurogenic and functional voice disorders, which addresses Priority 2 (increase our knowledge about conditions that alter or diminish communication and health). Project Narrative  The goal of this proposal is to determine laryngeal mechanisms underlying dysphonia in connected speech, which will lead to development of clinically applicable quantitative methods for functional laryngeal examination in connected speech using laryngeal imaging. This can potentially result in enhancement of clinical voice assessment and development of new clinical voice management strategies to better help people with voice disorders.",Studying the Laryngeal Mechanisms Underlying Dysphonia in Connected Speech,9901502,K01DC017751,"['Acoustics', 'Address', 'Age', 'Area', 'Auditory', 'Behavior', 'Biomechanics', 'Categories', 'Characteristics', 'Clinical', 'Communication', 'Coupling', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Dysphonia', 'Endoscopes', 'Evaluation', 'Functional disorder', 'Goals', 'Gold', 'Health', 'Human', 'Image', 'Knowledge', 'Larynx', 'Lead', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mining', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Paralysed', 'Patients', 'Physics', 'Physiological', 'Prevention', 'Production', 'Protocols documentation', 'Research', 'Series', 'Severities', 'Source', 'Spastic Dysphonias', 'Speech', 'Speed', 'Statistical Data Interpretation', 'Statistical Models', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Tremor', 'Visual', 'Voice', 'Voice Disorders', 'Voice Disturbances', 'Voice Quality', 'base', 'clinical application', 'clinical development', 'clinical practice', 'clinically relevant', 'cohort', 'flexibility', 'image processing', 'imaging approach', 'improved', 'innovation', 'kinematics', 'sex', 'temporal measurement', 'time use', 'tool', 'treatment strategy', 'vibration', 'vocal cord', 'vocalization']",NIDCD,MICHIGAN STATE UNIVERSITY,K01,2020,137795,0.33373406403282097
"Studying the Laryngeal Mechanisms Underlying Dysphonia in Connected Speech Project Summary/Abstract  This proposal aims to employ the recent advancement of coupling fiberoptic endoscopes with high-speed videoendoscopy (HSV) systems to obtain HSV recordings during connected speech. The goal is to study vocal mechanisms underlying dysphonia in patients with neurogenic voice disorders. The long-term goal of this line of research is to create clinically applicable quantitative methods for functional measurement of vocal fold vibration in connected speech using innovative laryngeal imaging, an approach that could advance clinical voice assessment and treatment practice. In Aim 1, HSV-based measures of vocal fold kinematics will be developed and the influence of these measures on voice audio-perceptual qualities in the patients will be determined. Image processing techniques will be developed to extract such measures from the HSV data in connected speech. The extracted measures will be given as inputs to the statistical models to determine the source of the differences between the normal controls and the patients for different speech phonetic contexts and words. This aim provides an unbiased HSV-based method to predict voice quality. Developing such HSV-based methodology for functional laryngeal examination in connected speech can enhance clinical voice assessment. In addition, better understanding the influence of phonetic context would lead to optimizing the protocols for functional voice assessment through laryngeal imaging in connected speech. In Aim 2, machine learning approaches will be employed to discover hidden physics and unknown laryngeal mechanisms of voice production in the dysphonic patients. The findings of this project will help make necessary adjustments in biomechanical or physiological characteristics of vocal folds to enhance voice quality in patients with neurogenic voice disorders. Therefore, the outcome of this research will aid clinicians in properly selecting, and developing new treatment strategies (therapeutic, medicinal, or surgical), which are based on the gained knowledge of laryngeal mechanisms of dysphonia. The proposed research is in harmony with multiple priority areas of the NIDCD, described in the 2017-2021 Strategic Plan. Both aims support Priority 3 (improve methods of diagnosis, treatment, and prevention) through developing objective HSV-based measures and predicting the voice quality. Comparing laryngeal mechanisms in normal and disordered voices addresses Priority 1 (deepen our understanding of the normal function of the systems of human communication). Both aims propose to study laryngeal mechanisms in patients with neurogenic and functional voice disorders, which addresses Priority 2 (increase our knowledge about conditions that alter or diminish communication and health). Project Narrative  The goal of this proposal is to determine laryngeal mechanisms underlying dysphonia in connected speech, which will lead to development of clinically applicable quantitative methods for functional laryngeal examination in connected speech using laryngeal imaging. This can potentially result in enhancement of clinical voice assessment and development of new clinical voice management strategies to better help people with voice disorders.",Studying the Laryngeal Mechanisms Underlying Dysphonia in Connected Speech,9720522,K01DC017751,"['Acoustics', 'Address', 'Age', 'Area', 'Auditory', 'Behavior', 'Biomechanics', 'Categories', 'Characteristics', 'Clinical', 'Communication', 'Coupling', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Dysphonia', 'Endoscopes', 'Evaluation', 'Functional disorder', 'Goals', 'Gold', 'Health', 'Human', 'Image', 'Knowledge', 'Larynx', 'Lead', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mining', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Paralysed', 'Patients', 'Physics', 'Physiological', 'Prevention', 'Production', 'Protocols documentation', 'Research', 'Series', 'Severities', 'Source', 'Spastic Dysphonias', 'Speech', 'Speed', 'Statistical Data Interpretation', 'Statistical Models', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Tremor', 'Visual', 'Voice', 'Voice Disorders', 'Voice Disturbances', 'Voice Quality', 'base', 'clinical application', 'clinical development', 'clinical practice', 'clinically relevant', 'cohort', 'flexibility', 'image processing', 'imaging approach', 'improved', 'innovation', 'kinematics', 'sex', 'temporal measurement', 'time use', 'tool', 'treatment strategy', 'vibration', 'vocal cord', 'vocalization']",NIDCD,MICHIGAN STATE UNIVERSITY,K01,2019,137795,0.33373406403282097
"Revealing the organization and functional significance of neural timescales in auditory cortex Project Summary People are remarkably adept at making sense of the world through sound: understanding speech in a noisy restaurant, picking out the voice of a family member, or recognizing a familiar melody. Although we take these abilities for granted, they reflect impressive computational feats of biological engineering that are remarkably difficult to replicate in machine systems. The long-term goal of my research program is to develop computational and experimental methods to reverse-engineer how the brain codes natural sounds like speech and to exploit these advances to understand and aid in the treatment of hearing impairment. One of the central challenges of coding natural sounds is that they are structured at many different timescales from milliseconds to seconds and even minutes. How does the brain integrate across these diverse timescales to derive meaning from sound? Answering this question has been challenging because there are no general-purpose methods for measuring neural timescales in the brain. As a consequence, we know relatively little about how neural timescales are organized in auditory cortex and how this organization enables the coding of natural sounds. To overcome these limitations, we develop a simple experimental paradigm (the temporal context invariance or TCI paradigm) for estimating the temporal integration period of any sensory response: the time window during which stimuli alter the response. We apply the TCI method to human electrocorticography (ECoG) and animal physiology recordings to reveal the organization of neural timescales at both the region and single-cell level (Aim I). Pilot data from our analyses reveal that timescales are organized hierarchically, with higher-order regions showing substantially longer integration periods. To explore the functional significance of this timescale hierarchy, we couple TCI with computational techniques well-suited for characterizing natural sounds (Aim II). We test whether increased integration periods enable a more noise-robust representation of speech (Aim IIA), whether regions with longer integration periods code higher-order properties of natural sounds (Aim IIB&IIC), whether there are dedicated integration periods for important sounds categories like speech or music (Aim IID), and whether cortical integration periods can be explained by the duration of the features they respond to (Aim IIE). In the process of conducting this research, I will be trained in two critical areas: (1) ECoG, which is the only method with the spatial and temporal precision to understand how neural timescales are organized in the human brain (2) deep neural networks (DNN) which are the only models able to perform challenging perceptual tasks at human levels and predict neural responses in higher-order cortical regions. After completing this training, I will have a unique set of experimental (fMRI, ECoG, psychophysics) and computational skills (data-driven statistical modeling and hypothesis-driven DNN modeling), which will facilitate my transition to an independent investigator. Project Narrative Natural sounds like speech contain information at many different timescales (e.g. phonemes, syllables, words), but how the human brain extracts this information remains unclear. Understanding this process is critical to understanding how hearing impairment degrades speech perception. The proposed research will reveal the organization of neural timescales in the brain, and how this organization facilitates the coding of natural sounds like speech, which is a critical first step in understanding how this code is impaired by hearing loss.",Revealing the organization and functional significance of neural timescales in auditory cortex,9977571,K99DC018051,"['Address', 'Animals', 'Area', 'Auditory area', 'Biological', 'Brain', 'Categories', 'Cells', 'Code', 'Collaborations', 'Communication', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Data', 'Electrocorticogram', 'Engineering', 'Family member', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grant', 'Hearing', 'Human', 'Learning', 'Measures', 'Methods', 'Modeling', 'Music', 'Neural Network Simulation', 'Neurosciences', 'Noise', 'Physiology', 'Population', 'Process', 'Property', 'Psychophysics', 'Reaction Time', 'Research', 'Research Personnel', 'Research Training', 'Restaurants', 'Sensory', 'Speech', 'Speech Perception', 'Statistical Models', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Training', 'Voice', 'Work', 'clinically significant', 'deep neural network', 'experience', 'experimental study', 'hearing impairment', 'millisecond', 'neuromechanism', 'programs', 'relating to nervous system', 'response', 'skills', 'sound', 'theories']",NIDCD,COLUMBIA UNIV NEW YORK MORNINGSIDE,K99,2020,125442,0.20339312122892514
"The role of amplitude modulation in perceiving speech and music Project Summary/Abstract  My career goal is to become a leading researcher on cognitive neuroscience, with a special focus on the neural mechanisms underlying auditory perception, including how humans track and perceive the fleeting audi- tory information in speech and music. In this proposal, I outline a research program to investigate the acoustic and neural distinctions between speech and music, two specialized forms of auditory signals that are closely tied to the human mind. Despite our increasingly rich understanding of the perceptual and neural mechanisms for processing speech or music, surprisingly little is known about why and how they are treated as different au- ditory signals by the human mind and brain in the first place. Investigating these distinctions is foundational for a thorough understanding of how acoustic waveforms are transformed into meaningful information. The work will provide a more solid basis for understanding cognition and communication as well as treating people with communicative deficits, such as people with autism, Alzheimer's disease, and aphasia.  I hypothesize that the temporal structure reflected in the amplitude modulation (AM) of speech and music signals is a critical distinctive feature for the brain and engages to different processing pathways, as speech and music are known to have distinct AM rates. A series of studies, combining psychophysics, MEG (magne- toencephalography), fMRI (functional magnetic resonance imaging), and machine learning approaches, will use stimuli with AM rates across the modulation frequency ranges of speech and music to address this topic at the computational (the goals), algorithmic (the representations and operations), and implementational (neural mechanism) levels. (1) Does the AM rate of a sound affect whether it will be perceived as speech or music? (2) Does the AM rate of a stimulus optimize speech and music perceptual performance at different frequencies? (3) What are the underlying neural mechanisms and the associated brain regions implementing the differentiation of speech and music? Aim 1 investigates whether the AM rate of a sound conditions it to be processed as speech or music. By manipulating the AM rate of noise-vocoded speech and music recordings, I hypothesize that the sounds with slower or faster AM rates will likely to be perceived as music or speech, respectively, the perceptual judgment will be biased by the higher or lower spectral energy of neural oscillatory activity (meas- ured by MEG) while listening to the sounds, respectively, and the associated brain regions will be revealed by fMRI with machine learning decoding approaches. Aim 2 investigates whether the AM rate of stimuli optimizes speech and music perceptual performances at different rates. I hypothesize that the music perceptual perfor- mance is optimal at slower AM rates while the speech perceptual performance is optimal at faster AM rates, and the neural oscillatory entrainment at lower or higher frequency band has domain-specific function facilitat- ing speech or music perceptual performance. Project Narrative Speech and music are two specialized forms of auditory signal that are closely tied to human mind; however, despite our increasingly rich understanding of the perceptual and neural mechanisms of human processing of speech or music, surprisingly little is known how they are treated as different auditory signals by the human mind and brain at the first place. The current proposal aims to investigate the fundamental differences between speech and music at the acoustic, perceptual, and neural levels, by combining psychophysics, neuroimaging, and machine learning approaches. Investigating their distinctions is crucial for understanding how acoustic waveforms are transformed into meaningful information, and it will provide the basis for understanding and treating people with communicative deficits, such as people with autism, Alzheimer's disease, and aphasia.",The role of amplitude modulation in perceiving speech and music,9835650,F32DC018205,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Aphasia', 'Auditory', 'Auditory Perception', 'Auditory area', 'Behavioral', 'Brain', 'Brain region', 'Cognition', 'Communication', 'Data', 'Foundations', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Human', 'Judgment', 'Linguistics', 'Machine Learning', 'Magnetoencephalography', 'Measures', 'Mind', 'Music', 'Noise', 'Participant', 'Pathway interactions', 'Perception', 'Performance', 'Periodicity', 'Process', 'Psychophysics', 'Records', 'Research', 'Research Personnel', 'Role', 'Series', 'Signal Transduction', 'Solid', 'Speech', 'Speech Perception', 'Stimulus', 'Structure', 'System', 'Testing', 'Work', 'auditory processing', 'autism spectrum disorder', 'career', 'cognitive neuroscience', 'experimental study', 'insight', 'neuroimaging', 'neuromechanism', 'non-invasive imaging', 'operation', 'programs', 'relating to nervous system', 'sound', 'spectral energy', 'speech processing']",NIDCD,NEW YORK UNIVERSITY,F32,2019,60854,0.38316497377576125
"A holistic approach to identifying functional units of tongue motion during speech PROJECT SUMMARY  Oral cancers have the seventh highest incidence, with roughly 51,540 new cases and 10,030 cancer- related deaths expected to occur in 2018. Although a variety of treatment methods are available, the death rate is higher than that for most cancers with five-year rates of about 50 percent. The most frequently used treatment method, glossectomy surgery, involves the surgical removal of tumors and surrounding tissues, and the addition of grafted tissues, often followed by radiotherapy. Although tongue cancer and its treatment have debilitating effects on speech, the impact of varying degrees of resection and reconstruction on the formation of functional units in speech has remained poorly understood. In order to produce intelligible speech, a variety of local muscle groupings of the tonguei.e., functional unitsemerge and recede rapidly and nimbly in a highly coordinated fashion. Therefore, understanding the formation of functional units that are critical for speech production can provide substantial insights into normal, pathological, and adapted motor control strategies in controls and patients with tongue cancer for novel therapeutic, surgical, and rehabilitative strategies. One of the critical challenges in pre-operative surgical and treatment planning, as well as in post- operative evaluation for tongue cancer is the difficulty in developing objective and quantitative measures and in evaluating their functional outcome predictability. To address this, in this proposal, three integrated approaches will be used in in vivo tongue motion during speech to seamlessly identify the functional units and associated quantitative measures: multimodal MRI methods, multimodal deep learning, and biomechanical simulations. This will provide a convergent approach, thereby allowing us to (1) test hypotheses about the spatiotemporal basis of muscle coordination in a consilient way, and (2) develop objective quantitative measures that are required for understanding the complex biomechanical system as well as for predicting the functional outcomes after various reconstruction methods. The first proof of concept study published by the PI and the team identified the functional units of speech tasks using the sparse non-negative matrix factorization framework, in which the magnitude and angle of displacements from tagged MRI were used as our input quantities. With these advances in place, we will further incorporate muscle fiber anatomy from diffusion MRI and motion tracking from tagged MRI into our framework to yield physiologically and anatomically meaningful functional units. In addition, we will create a completely novel and integrated way of directly relating the functional units to tongue muscle anatomy, learning joint representation via a multimodal deep learning technique, and linking them to biomechanical simulations. Furthermore, 3D and 4D atlases will be utilized to identify objective and quantitative measures based on our functional units analysis. Taken together, the successful implementation of our integrated framework will identify functional units that can be used for research on tongue motion, for surgical planning, and for diagnosis, prognosis, and rehabilitation in a range of speech-related disorders. PROJECT NARRATIVE  Tongue cancer and its treatment affect tongue structure and function, yet little is known about how the changes in tongue structure due to varying degrees of resection and reconstruction affect the formation of functional units of tongue motion during speech. We propose to use novel integrated platform tools to identify functional units seamlessly with unprecedented resolution and precision. Upon success of this proposal, our integrated framework has the potential to aid in an increased understanding of speech motor control strategies in healthy controls and the patient group, thereby benefiting patients through improved diagnosis, treatment, and rehabilitative strategies.",A holistic approach to identifying functional units of tongue motion during speech,9937181,R01DC018511,"['3-Dimensional', 'Acoustics', 'Address', 'Affect', 'Aftercare', 'Anatomy', 'Atlases', 'Behavior', 'Biomechanics', 'Cessation of life', 'Clinical', 'Complex', 'Computing Methodologies', 'Data', 'Death Rate', 'Deglutition', 'Diagnosis', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Evaluation', 'Excision', 'Exhibits', 'Fiber', 'Geometry', 'Glossectomy', 'Goals', 'Grouping', 'Impairment', 'Incidence', 'Joints', 'Knowledge', 'Learning', 'Link', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Motion', 'Muscle', 'Muscle Fibers', 'Operative Surgical Procedures', 'Outcome', 'Pathologic', 'Patients', 'Physiological', 'Postoperative Period', 'Predictive Value', 'Procedures', 'Production', 'Proxy', 'Publishing', 'Radiation therapy', 'Rehabilitation therapy', 'Research', 'Resolution', 'Speech', 'Speech Intelligibility', 'Standardization', 'Structure', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Tissue Grafts', 'Tissues', 'Tongue', 'Weight', 'Work', 'base', 'biomechanical model', 'clinical practice', 'deep learning', 'functional outcomes', 'holistic approach', 'improved', 'in vivo', 'insight', 'malignant mouth neoplasm', 'malignant tongue neoplasm', 'motor control', 'multimodality', 'muscular structure', 'novel', 'novel therapeutics', 'outcome forecast', 'outcome prediction', 'reconstruction', 'rehabilitation strategy', 'signal processing', 'spatiotemporal', 'success', 'tool', 'treatment planning', 'treatment strategy', 'tumor']",NIDCD,MASSACHUSETTS GENERAL HOSPITAL,R01,2020,588403,0.22438060785646252
"Real-time deep learning to improve speech intelligibility in noise Project Summary/Abstract  One in eight Americans has hearing loss, and this constitutes a major health and economic burden (Blackwell et al., 2014). The primary complaint of hearing-impaired (HI) listeners is difficulty understanding speech when background noise is present (see Dillon, 2012). While hearing aids (HAs) have improved in recent years, they still provide little benefit in noisy environments. For decades, a means of improving the ability to understand speech in background noise appeared unattainable, despite substantial amounts of research by both universities and HA companies. This changed when deep learning provided the first demonstration of a single-microphone algorithm that improves intelligibly in noise for HI listeners (Healy et al., 2013, 2014, 2015). Although this algorithm provides massive intelligibility improvements (even allowing listeners to improve intelligibility from floor to ceiling levels), it is currently not implemented to operate in real time and is therefore not suitable for implementation into HAs and cochlear implants (CIs). What is needed, therefore, is a highly effective noise-reduction algorithm that is capable of operating in real time. This project aims to address this critical need.  The long-term goal of the currently proposed project is to alleviate HI listeners predominant hearing handicap, which is difficulty understanding speech in background noise. The first aim introduces a new algorithm, based on a novel foundational scheme, that is designed to provide substantial benefit for any HI listener in real time. This algorithm will be well suited for implementation into HAs, CIs, and other face-to-face communication applications. The effectiveness of this new algorithm will be quantified using both HI and normal-hearing (NH) listeners. The second aim expands upon this new algorithm by modifying it to accept a small amount of future time-frame information, which could improve its noise-reduction performance but will introduce a brief processing delay. The rationale is that different devices have different allowable latencies. Face-to-face communication devices (HAs, CIs, etc.) have strict low-latency requirements, but other important communication systems (e.g., telephones) have different requirements. It is possible that the addition of future time-frame information within these requirements (up to 150 ms) will result in even better speech intelligibility. But the magnitude of any potential benefit is unknown. This critical information will be established currently. Using both HI and NH listeners, we will measure intelligibility for noisy sentences that have been processed using various amounts of future time information.  This comprehensive fellowship training plan will provide individualized, mentored research training from world-class faculty in a highly supportive and productive environment. The proposed work will endow the applicant with the skills needed to transition to the next stage of his research career, transform our treatment of hearing loss, and substantially impact quality of life for millions of Americans. Project Narrative An estimated 37.5 million Americans have hearing loss, which commonly leads to difficulty understanding speech in background noise. The proposed study will test a new noise-reduction system and improve our treatment of hearing loss.",Real-time deep learning to improve speech intelligibility in noise,10155960,F32DC019314,"['Address', 'Algorithms', 'American', 'Area', 'Auditory', 'Cellular Phone', 'Characteristics', 'Cochlear Implants', 'Communication', 'Complex', 'Data', 'Devices', 'Diagnosis', 'Economic Burden', 'Effectiveness', 'Environment', 'Equilibrium', 'Etiology', 'Faculty', 'Fellowship', 'Floor', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Hearing', 'Hearing Aids', 'Human', 'Implant', 'Measures', 'Mentors', 'Mission', 'National Institute on Deafness and Other Communication Disorders', 'Noise', 'Performance', 'Phase', 'Prevention', 'Process', 'Quality of life', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Scheme', 'Seminal', 'Signal Transduction', 'Speech', 'Speech Intelligibility', 'Strategic Planning', 'System', 'Telephone', 'Testing', 'Time', 'Training', 'Translating', 'Universities', 'Videoconferencing', 'Work', 'artificial neural network', 'base', 'career', 'communication device', 'deep learning', 'deep neural network', 'design', 'experimental study', 'health economics', 'hearing impairment', 'hearing loss treatment', 'improved', 'microphone', 'network architecture', 'neural network', 'normal hearing', 'novel', 'novel strategies', 'operation', 'skills', 'speech in noise', 'wearable device']",NIDCD,OHIO STATE UNIVERSITY,F32,2020,76840,0.1725159797811016
"DEVELOPMENT OF ARTIFICIAL HEARING FOR THE DEAF The effort to develop artificial hearing systems for the deaf has recently been focussed on tactual vocoders, devices that transduce acoustic energy into vibratory or electrocutaneous signals which are then applied to the skin.  Current tactual vocoders, however, are based largely upon untested assumptions about optimal filter configurations for speech processing.  In a study which carefully controls device variables (through use of software filters and computer driven displays) while systematically varying filter configurations, perception of critical features of the speech code will be studied.  Using psychophysical techniques, the research seeks to describe the tactual perception of speech features based on signals processed through filter banks with linear, logarithmic and composite spacings for both 32 and 16 channel displays.  The studies will:  1. determine discrimination and identification of speech contrasts along various speech-simulating continua for each of the filter configurations;  2. provide an empirical basis for the design of tactual vocoders with optimal filter configurations; and  3. lay the foundations for a miniaturized tactual vocoder design to be implemented in succeeding phases of the work.  This work is directed toward the development and ultimate manufacture of a portable, wearable, cosmetically acceptable, artificial hearing prosthesis for the deaf.  n/a",DEVELOPMENT OF ARTIFICIAL HEARING FOR THE DEAF,3503959,R43NS021390,"['artificial intelligence', ' auditory discrimination', ' computer data analysis', ' deaf aid', ' deafness', ' electrodes', ' electrostimulus', ' electrotactile communication', ' form /pattern perception', ' human subject', ' somesthesis', ' speech', ' voice']",NINDS,ARTIFICIAL HEARING SYSTEMS CORPORATION,R43,1985,50000,0.27893507951824015
"Objectively Quantifying Speech Outcomes of Children with Cleft Palate Perceptual assessment of hypernasality is considered a critical component when evaluating the speech of children with cleft lip and/or palate (CLP). However, most speech-language pathologists (SLPs) do not receive formal training for perceptual evaluation of speech and, as a result, research shows that the subjective ratings are inherently biased to the perceiver and exhibit considerable variability. In this project, we aim to develop an artificial intelligence (AI) algorithm that automatically evaluates speech along four dimensions deemed to be critically important by the Americleft Speech Outcomes Group (ASOG), namely speech acceptability, articulation, hypernasality, and audible nasal emissions. The AI algorithm in this project is based on an existing database of speech collected as a part of an NIH-funded project to develop reliable speech outcomes by improving the reliability of perceptual ratings by training clinicians (NIDCR DE019-01235, PI: Kathy Chapman). This database contains speech samples from 125 5-7 year olds along with multiple perceptual rating for each speech sample. The clinicians participating in this study were successfully trained using a new protocol from the Americleft Speech Outcomes Group and they exhibit excellent inter-clinician reliability.  In SA1 we will develop an AI algorithm that automatically learns the relationship between a comprehensive set of speech acoustics and the average of the ASOG-trained expert ratings for each of the four perceptual dimensions. This approach is based on technology that the PIs have successfully used to evaluate dysarthric speech. Unique to these algorithms is modeling of perceptual judgments of trained experts using tools from statistical signal processing and AI. The output of the algorithms will map to a clinically- relevant scale, rather than to norm-referenced values that may or may not be meaningful. In SA2, we will evaluate the tool on new data by collecting new speech samples using a mobile app at a partner clinic using the same protocol as in the original study. Every collected sample will be further evaluated by ASOG trained clinicians. We will use this data to evaluate the accuracy of the AI model by comparing the model's predictions with the average of ASOG-trained experts. Preliminary results show promise that the proposed approach will yield a successful tool for accurately characterizing perceptual dimensions in the speech of children with CLP. These results indicate that a number of acoustic features that have been developed previously by the PIs accurately capture differences in hypernasality and articulation between the speech of three children with CLP (with varying severity). Furthermore, we show the success of our approach on a different, but related, task: objective evaluation of dysarthric speech. We show that an algorithm that automatically rates hypernasality performs on par with the judgment of human evaluators. The results of the proposed research will form the basis for a subsequent R01 proposal for the development and evaluation of a clinical tool to objectively quantify and track speech production in children with CLP. Project Narrative Perceptual assessment of the speech of children with cleft lip and/or palate is commonly used as a key clinical indicator upon which follow-on decisions are made about intervention. However, studies show that the inter- clinician reliability can be low. As an alternative, we propose a new objective outcome tool based on signal processing and artificial intelligence that automatically assesses speech acceptability, articulation, hypernasality, and audible nasal emissions directly from speech; the output of the tool is on a scale defined by the Americleft Speech Outcomes Group and can be used by clinicians to objectively measure progress.",Objectively Quantifying Speech Outcomes of Children with Cleft Palate,9765280,R21DE026252,"['7 year old', 'Acoustics', 'Age', 'Algorithms', 'American', 'Articulation', 'Artificial Intelligence', 'Behavior Therapy', 'Child', 'Cleft Palate', 'Cleft lip with or without cleft palate', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Dimensions', 'Ear', 'Ensure', 'Environment', 'Evaluation', 'Exhibits', 'Four-dimensional', 'Frequencies', 'Funding', 'Gold', 'Human', 'Individual', 'International', 'Intervention', 'Judgment', 'Language', 'Learning', 'Maps', 'Measures', 'Methods', 'Modeling', 'National Institute of Dental and Craniofacial Research', 'Nose', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Output', 'Pathologist', 'Perception', 'Performance', 'Population', 'Positioning Attribute', 'Production', 'Protocols documentation', 'Proxy', 'Reference Values', 'Reporting', 'Research', 'Sampling', 'Series', 'Severities', 'Signal Transduction', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Technology', 'Time', 'Training', 'United States National Institutes of Health', 'Utah', 'Validation', 'Validity and Reliability', 'Visit', 'Work', 'base', 'cleft lip and palate', 'clinically relevant', 'craniofacial', 'impression', 'improved outcome', 'learning algorithm', 'mobile application', 'novel', 'predictive modeling', 'signal processing', 'success', 'tool']",NIDCR,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R21,2019,225535,0.3812343868242706
"Objectively Quantifying Speech Outcomes of Children with Cleft Palate Perceptual assessment of hypernasality is considered a critical component when evaluating the speech of children with cleft lip and/or palate (CLP). However, most speech-language pathologists (SLPs) do not receive formal training for perceptual evaluation of speech and, as a result, research shows that the subjective ratings are inherently biased to the perceiver and exhibit considerable variability. In this project, we aim to develop an artificial intelligence (AI) algorithm that automatically evaluates speech along four dimensions deemed to be critically important by the Americleft Speech Outcomes Group (ASOG), namely speech acceptability, articulation, hypernasality, and audible nasal emissions. The AI algorithm in this project is based on an existing database of speech collected as a part of an NIH-funded project to develop reliable speech outcomes by improving the reliability of perceptual ratings by training clinicians (NIDCR DE019-01235, PI: Kathy Chapman). This database contains speech samples from 125 5-7 year olds along with multiple perceptual rating for each speech sample. The clinicians participating in this study were successfully trained using a new protocol from the Americleft Speech Outcomes Group and they exhibit excellent inter-clinician reliability.  In SA1 we will develop an AI algorithm that automatically learns the relationship between a comprehensive set of speech acoustics and the average of the ASOG-trained expert ratings for each of the four perceptual dimensions. This approach is based on technology that the PIs have successfully used to evaluate dysarthric speech. Unique to these algorithms is modeling of perceptual judgments of trained experts using tools from statistical signal processing and AI. The output of the algorithms will map to a clinically- relevant scale, rather than to norm-referenced values that may or may not be meaningful. In SA2, we will evaluate the tool on new data by collecting new speech samples using a mobile app at a partner clinic using the same protocol as in the original study. Every collected sample will be further evaluated by ASOG trained clinicians. We will use this data to evaluate the accuracy of the AI model by comparing the model's predictions with the average of ASOG-trained experts. Preliminary results show promise that the proposed approach will yield a successful tool for accurately characterizing perceptual dimensions in the speech of children with CLP. These results indicate that a number of acoustic features that have been developed previously by the PIs accurately capture differences in hypernasality and articulation between the speech of three children with CLP (with varying severity). Furthermore, we show the success of our approach on a different, but related, task: objective evaluation of dysarthric speech. We show that an algorithm that automatically rates hypernasality performs on par with the judgment of human evaluators. The results of the proposed research will form the basis for a subsequent R01 proposal for the development and evaluation of a clinical tool to objectively quantify and track speech production in children with CLP. Project Narrative Perceptual assessment of the speech of children with cleft lip and/or palate is commonly used as a key clinical indicator upon which follow-on decisions are made about intervention. However, studies show that the inter- clinician reliability can be low. As an alternative, we propose a new objective outcome tool based on signal processing and artificial intelligence that automatically assesses speech acceptability, articulation, hypernasality, and audible nasal emissions directly from speech; the output of the tool is on a scale defined by the Americleft Speech Outcomes Group and can be used by clinicians to objectively measure progress.",Objectively Quantifying Speech Outcomes of Children with Cleft Palate,9601604,R21DE026252,"['7 year old', 'Acoustics', 'Age', 'Algorithms', 'American', 'Articulation', 'Artificial Intelligence', 'Behavior Therapy', 'Child', 'Cleft Lip', 'Cleft Palate', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Dimensions', 'Ear', 'Ensure', 'Environment', 'Evaluation', 'Exhibits', 'Four-dimensional', 'Frequencies', 'Funding', 'Gold', 'Human', 'Individual', 'International', 'Intervention', 'Judgment', 'Language', 'Learning', 'Maps', 'Measures', 'Methods', 'Modeling', 'National Institute of Dental and Craniofacial Research', 'Nose', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Output', 'Palate', 'Pathologist', 'Perception', 'Performance', 'Population', 'Positioning Attribute', 'Production', 'Protocols documentation', 'Proxy', 'Reference Values', 'Reporting', 'Research', 'Sampling', 'Series', 'Severities', 'Signal Transduction', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Technology', 'Time', 'Training', 'United States National Institutes of Health', 'Utah', 'Validation', 'Validity and Reliability', 'Visit', 'Work', 'base', 'cleft lip and palate', 'clinically relevant', 'craniofacial', 'impression', 'improved outcome', 'mobile application', 'novel', 'predictive modeling', 'signal processing', 'success', 'tool']",NIDCR,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R21,2018,203708,0.3812343868242706
"VOICE-ACTIVATED PHONE FOR THE SPINAL CORD-INJURED The long term objective of this research in Phase I and Phase II is to develop a speaker-independent, voice-activated telephone interface for use by quadriplegics and others, with the innovative capability of handling speech produced by people who require ventilators.  Specific objectives include:  (1) collecting and analyzing ""ventilator speech"" produced by people with spinal cord injuries at different phases of rehabilitation; (2) modifying Emerson & Stern's existing SOLILOQUY R Voice Interface Software to handle breath-related interference and ignore non-speech interjections, without compromising SOLILOQUY's current speaker-independence; and (3) determining if an adequate telephone interface can be affixed to a halo orthotic.  These objectives will be met by (1) recording telephone numbers spoken by patients at Sharp Hospital's Rehabilitation Center; (2) using a series of in-house software tools for speech analysis; (3) iteratively improving SOLILOQUY's microphone recognition algorithms and testing the results; (4) making similar changes to telephone-based algorithms; and (5) testing a cervical orthosis-mounted phone interface.  Other voice-activated phones are speaker-dependent, requiring users to ""re- train"" the system every time their voice changes.  No other system has SOLILOQUY's linguistic ability to ignore a non-speech sound by treating it as extraneous speech.  n/a",VOICE-ACTIVATED PHONE FOR THE SPINAL CORD-INJURED,3500036,R43HD029318,"['artificial intelligence', ' assistive device /technology', ' audiotape', ' biomedical equipment development', ' clinical biomedical equipment', ' computer program /software', ' human subject', ' information systems', ' person with disability', ' speech', ' speech disorders', ' spinal cord injury', ' telecommunications', ' vocalization', ' voice']",NICHD,EMERSON AND STERN ASSOCIATES,R43,1992,50000,0.2147118469691347
"Decoding inner speech: An AI approach to transcribing thoughts via EEG & EMG ABSTRACT  Losing the capacity to communicate through language has a significant negative impact on a persons autonomy, social interactions, occupation, mental health, and overall quality of life. Many people lose the capacity to speak and write but keep their thinking intact.  Inner speech is internally and willfully generated, non-articulated verbal thoughts (e.g., reading in silence). Changes in the activation patterns of the brains language-related areas co-occur with inner speech and can be detected with electroencephalography (EEG). Furthermore, while inner speech doesnt lead to any discernible voice sound or articulation, co-occurring low amplitude electrical discharges in the articulatory muscles can be detected with electromyography (EMG). The information about ongoing inner speech reflected in electrophysiological signals (EEG and EMG) can be used to transcribe inner speech into text or voice.  Machine learning algorithms have been used for this purpose, however, the resulting systems have low accuracy and/or are constrained by very small vocabularies (~10 words). Furthermore, these systems need to be trained anew for each user, which significantly increases individual data-collection time. The development of ready-to-use/minimal-training (fine tuning) systems requires large training datasets that algorithms can use to learn high-level features capable of being transferred between individuals. Unfortunately, to date there are no available datasets that are large enough to train these systems.  To tackle these issues, I have assembled a multidisciplinary team of collaborators from Google AI, Yale linguistics, and Yale Psychiatry to develop a state-of-the-art deep neural network to transcribe inner speech to text using EEG and EMG signals. This system will incorporate some of the latest advances in artificial intelligence and data processing developed by Google AI. It will be designed to transcribe phonemes, thus, in principle, will be able to transcribe any word. Furthermore, we will collect the largest (x120 times) multi-subject (n=150) electrophysiological (EEG+EMG) inner speech dataset to date (300 hrs. in total) to train the first ready- to-use/minimal-training inner speech transcriber system.  The technology resulting from this study has the potential to radically improve the quality of life of thousands of patients by providing them with a fast method of communicating their verbal thoughts. Furthermore, by combining this system with one of the many text-to-speech AIs that are currently available, our system could potentially restore the patients capacity to produce conversational speech. PROJECT NARRATIVE People that have lost their capacity for verbal communication struggle with isolation, mental illness, and poor quality of life. Artificial intelligence offers an opportunity to translate verbal thoughts into text or synthesized voice and restore verbal communication in impaired people. In this study, we introduce a state-of-the art artificial intelligence system designed to transduce the electrophysiological activity (electroencephalography [EEG] and electromyography [EMG]) accompanying verbal thoughts into text.",Decoding inner speech: An AI approach to transcribing thoughts via EEG & EMG,10058047,R21EB029607,"['ALS2 gene', 'Algorithms', 'American', 'Architecture', 'Area', 'Articulation', 'Artificial Intelligence', 'Attention', 'Brain', 'Clinical', 'Communication', 'Complex', 'Data', 'Data Collection', 'Data Set', 'Development', 'Electroencephalography', 'Electromyography', 'Electrophysiology (science)', 'Expert Systems', 'Fostering', 'Gender', 'Genetic Transcription', 'Hand', 'Impairment', 'Individual', 'Language', 'Lead', 'Learning', 'Letters', 'Linguistics', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Mental Health', 'Mental disorders', 'Methods', 'Modeling', 'Muscle', 'Occupations', 'Output', 'Patients', 'Pattern', 'Performance', 'Persons', 'Psyche structure', 'Psychiatry', 'Quality of life', 'Reading', 'Signal Transduction', 'Social Interaction', 'Speech', 'Stimulus', 'Sum', 'System', 'Technology', 'Text', 'Thinking', 'Time', 'Training', 'Translating', 'Vocabulary', 'Voice', 'Writing', 'algorithm training', 'blind', 'computerized data processing', 'deep learning', 'deep neural network', 'design', 'digital', 'healthy volunteer', 'improved', 'innovation', 'large datasets', 'machine learning algorithm', 'multidisciplinary', 'performance tests', 'sound', 'speech accuracy']",NIBIB,YALE UNIVERSITY,R21,2020,523600,0.206619122113239
"DEVELOPMENT OF AN IMPROVED TACTUAL VOCODER DESCRIPTION:  (Adapted  from the Applicant's  Abstract.) Intelligent Hearing  Systems  proposes development  of awearable tactual  vocoder, an  artificial  hearing system worn on the skin, to assist the deaf in learning to communicate  verbally.    Previous  commerciallyavailable tactileaids  have  had three  important  drawbacks:  such systems 1) have been based upon inflexible analog  circuitry, and consequently have not been programmable to accommodate special  needs  of individual  users or  programs  of  rehabilitation,  2)  have  been  ineffective as transmitters of fundamental  frequency (pitch), one of  the key  factors in the success of surgically implanted artificial hearing systems, and 3)have  not  been  capable  of  transmitting  rapid  spectral  cues (format  transitions), one of the most important characteristics of the speech code.  The proposed  IntelliVoc  (Intelligent  Vocoder)  device  would  address the  failures of previously available systems by 1) providing a miniaturized system  that  can  be programmed  by  a  speech and  hearing clinician  to adapt  the  systems's characteristics (channel allocation, dynamic range, frequency coding  etc.)  to the  individual needs  of users  or programs  of rehabilitation,  2)  utilizing  a new  wideband vibrator to  transmit pitch intonation  contours  directly as  frequency of vibration to  the skin, and 3) employing a realtime  Linear Predictive Coding (LPC) signal  processing approach to deliver formants  and formant transitions to the skin.  n/a",DEVELOPMENT OF AN IMPROVED TACTUAL VOCODER,3508314,R44HD031502,"['artificial intelligence', ' biomedical equipment development', ' clinical biomedical equipment', ' deaf aid', ' electrotactile communication', ' human subject', ' psychophysics', ' skin', ' sound frequency', ' vibration', ' vibration perception']",NICHD,INTELLIGENT HEARING SYSTEMS,R44,1993,251261,0.18198973168973115
"Development of Speech Perception and Brain Plasticity    DESCRIPTION (provided by applicant): Language is a hallmark of human beings. In the last 50 years, debates on language have given way to a new view of the process by which humans acquire language. One catalyst for theoretical change has been empirical studies on infants. In the last decade, researchers have not only charted when infants acquire knowledge about the properties of their native language, but how they do so, and this has caused a revision in linguistic and psychological theories. New research focuses on the phonetic units of speech, the consonants and vowels that form building blocks for words. Key advances from this laboratory are cross- language data showing that infants learn from exposure to language in the earliest periods of development and that this alters speech perception to assist language learning. Moreover, our studies show that early speech predicts later language, and that the clarity of mothers' infant-directed speech is linked to infants' speech perception abilities. Finally, brain measures on infants and adults listening to language suggest that, during early development, the infant brain ""neurally commits"" to the patterns of native language speech and that this both promotes future language learning as well as the decline in nonnative speech perception that occurs at the end of the first year of life. This work on early speech perception is impacting child development, neuroscience, neurobiology, and computational modeling. The early speech measures developed as a part of this project are being used in the study of developmental disabilities including autism, and may provide an early marker of the disability. The data prompted an extension of the Native Language Magnet model to incorporate neural commitment as the mechanism for developmental change. This theoretical position provides the background and framework for the studies in this proposal. Four converging lines of research are proposed to test the theory and further advance our knowledge of infant speech development: (a) speech perception development and its impact on language; (b) the brain correlates of early speech and language development, (c) the role of language input to children, and (d) brain plasticity and the ""critical period"" for language acquisition. The research will produce data that address theories of speech and language development and more general theories of the interface between biology and culture.           n/a",Development of Speech Perception and Brain Plasticity,8044743,R01HD037954,"['Acoustics', 'Address', 'Adult', 'Age-Months', 'Age-Years', 'Audiotape', 'Autistic Disorder', 'Biology', 'Birth', 'Brain', 'Characteristics', 'Child', 'Child Development', 'Commit', 'Computer Simulation', 'Cues', 'Data', 'Development', 'Developmental Disabilities', 'Event-Related Potentials', 'Evolution', 'Exposure to', 'Funding', 'Future', 'Hour', 'Human', 'Individual', 'Infant', 'Infant Development', 'Intervention Studies', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Learning', 'Life', 'Linguistics', 'Link', 'Longevity', 'Machine Learning', 'Magnetoencephalography', 'Measures', 'Modeling', 'Mothers', 'Nature', 'Neurobiology', 'Neurosciences', 'Newborn Infant', 'Pattern', 'Perception', 'Phase', 'Phonetics', 'Play', 'Positioning Attribute', 'Process', 'Property', 'Psychological Theory', 'Research', 'Research Personnel', 'Role', 'Social Interaction', 'Speech', 'Speech Development', 'Speech Perception', 'Techniques', 'Television', 'Testing', 'Time', 'Work', 'catalyst', 'critical period', 'design', 'disability', 'foreign language', 'infancy', 'language processing', 'relating to nervous system', 'skills', 'speech processing', 'theories']",NICHD,UNIVERSITY OF WASHINGTON,R01,2011,526418,0.2098458625146422
"Development of Speech Perception and Brain Plasticity    DESCRIPTION (provided by applicant): Language is a hallmark of human beings. In the last 50 years, debates on language have given way to a new view of the process by which humans acquire language. One catalyst for theoretical change has been empirical studies on infants. In the last decade, researchers have not only charted when infants acquire knowledge about the properties of their native language, but how they do so, and this has caused a revision in linguistic and psychological theories. New research focuses on the phonetic units of speech, the consonants and vowels that form building blocks for words. Key advances from this laboratory are cross- language data showing that infants learn from exposure to language in the earliest periods of development and that this alters speech perception to assist language learning. Moreover, our studies show that early speech predicts later language, and that the clarity of mothers' infant-directed speech is linked to infants' speech perception abilities. Finally, brain measures on infants and adults listening to language suggest that, during early development, the infant brain ""neurally commits"" to the patterns of native language speech and that this both promotes future language learning as well as the decline in nonnative speech perception that occurs at the end of the first year of life. This work on early speech perception is impacting child development, neuroscience, neurobiology, and computational modeling. The early speech measures developed as a part of this project are being used in the study of developmental disabilities including autism, and may provide an early marker of the disability. The data prompted an extension of the Native Language Magnet model to incorporate neural commitment as the mechanism for developmental change. This theoretical position provides the background and framework for the studies in this proposal. Four converging lines of research are proposed to test the theory and further advance our knowledge of infant speech development: (a) speech perception development and its impact on language; (b) the brain correlates of early speech and language development, (c) the role of language input to children, and (d) brain plasticity and the ""critical period"" for language acquisition. The research will produce data that address theories of speech and language development and more general theories of the interface between biology and culture.           n/a",Development of Speech Perception and Brain Plasticity,7768396,R01HD037954,"['Acoustics', 'Address', 'Adult', 'Age-Months', 'Age-Years', 'Audiotape', 'Autistic Disorder', 'Biology', 'Birth', 'Brain', 'Characteristics', 'Child', 'Child Development', 'Commit', 'Computer Simulation', 'Cues', 'Data', 'Development', 'Developmental Disabilities', 'Event-Related Potentials', 'Evolution', 'Exposure to', 'Funding', 'Future', 'Hour', 'Human', 'Individual', 'Infant', 'Infant Development', 'Intervention', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Learning', 'Life', 'Linguistics', 'Link', 'Longevity', 'Machine Learning', 'Magnetoencephalography', 'Measures', 'Modeling', 'Mothers', 'Nature', 'Neurobiology', 'Neurosciences', 'Newborn Infant', 'Pattern', 'Perception', 'Phase', 'Phonetics', 'Play', 'Positioning Attribute', 'Process', 'Property', 'Psychological Theory', 'Research', 'Research Design', 'Research Personnel', 'Role', 'Social Interaction', 'Speech', 'Speech Development', 'Speech Perception', 'Techniques', 'Television', 'Testing', 'Time', 'Work', 'catalyst', 'critical period', 'disability', 'foreign language', 'infancy', 'language processing', 'relating to nervous system', 'skills', 'theories']",NICHD,UNIVERSITY OF WASHINGTON,R01,2010,530965,0.2098458625146422
"Development of Speech Perception and Brain Plasticity    DESCRIPTION (provided by applicant): Language is a hallmark of human beings. In the last 50 years, debates on language have given way to a new view of the process by which humans acquire language. One catalyst for theoretical change has been empirical studies on infants. In the last decade, researchers have not only charted when infants acquire knowledge about the properties of their native language, but how they do so, and this has caused a revision in linguistic and psychological theories. New research focuses on the phonetic units of speech, the consonants and vowels that form building blocks for words. Key advances from this laboratory are cross- language data showing that infants learn from exposure to language in the earliest periods of development and that this alters speech perception to assist language learning. Moreover, our studies show that early speech predicts later language, and that the clarity of mothers' infant-directed speech is linked to infants' speech perception abilities. Finally, brain measures on infants and adults listening to language suggest that, during early development, the infant brain ""neurally commits"" to the patterns of native language speech and that this both promotes future language learning as well as the decline in nonnative speech perception that occurs at the end of the first year of life. This work on early speech perception is impacting child development, neuroscience, neurobiology, and computational modeling. The early speech measures developed as a part of this project are being used in the study of developmental disabilities including autism, and may provide an early marker of the disability. The data prompted an extension of the Native Language Magnet model to incorporate neural commitment as the mechanism for developmental change. This theoretical position provides the background and framework for the studies in this proposal. Four converging lines of research are proposed to test the theory and further advance our knowledge of infant speech development: (a) speech perception development and its impact on language; (b) the brain correlates of early speech and language development, (c) the role of language input to children, and (d) brain plasticity and the ""critical period"" for language acquisition. The research will produce data that address theories of speech and language development and more general theories of the interface between biology and culture.           n/a",Development of Speech Perception and Brain Plasticity,7587262,R01HD037954,"['Acoustics', 'Address', 'Adult', 'Age-Months', 'Age-Years', 'Audiotape', 'Autistic Disorder', 'Biology', 'Birth', 'Brain', 'Characteristics', 'Child', 'Child Development', 'Commit', 'Computer Simulation', 'Cues', 'Data', 'Development', 'Developmental Disabilities', 'Event-Related Potentials', 'Evolution', 'Exposure to', 'Funding', 'Future', 'Hour', 'Human', 'Individual', 'Infant', 'Infant Development', 'Intervention', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Learning', 'Life', 'Linguistics', 'Link', 'Longevity', 'Machine Learning', 'Magnetoencephalography', 'Measures', 'Modeling', 'Mothers', 'Nature', 'Neurobiology', 'Neurosciences', 'Newborn Infant', 'Pattern', 'Perception', 'Phase', 'Phonetics', 'Play', 'Positioning Attribute', 'Process', 'Property', 'Psychological Theory', 'Research', 'Research Design', 'Research Personnel', 'Role', 'Social Interaction', 'Speech', 'Speech Development', 'Speech Perception', 'Techniques', 'Television', 'Testing', 'Time', 'Work', 'catalyst', 'critical period', 'disability', 'foreign language', 'infancy', 'language processing', 'relating to nervous system', 'skills', 'theories']",NICHD,UNIVERSITY OF WASHINGTON,R01,2009,537826,0.2098458625146422
"Development of Speech Perception and Brain Plasticity    DESCRIPTION (provided by applicant): Language is a hallmark of human beings. In the last 50 years, debates on language have given way to a new view of the process by which humans acquire language. One catalyst for theoretical change has been empirical studies on infants. In the last decade, researchers have not only charted when infants acquire knowledge about the properties of their native language, but how they do so, and this has caused a revision in linguistic and psychological theories. New research focuses on the phonetic units of speech, the consonants and vowels that form building blocks for words. Key advances from this laboratory are cross- language data showing that infants learn from exposure to language in the earliest periods of development and that this alters speech perception to assist language learning. Moreover, our studies show that early speech predicts later language, and that the clarity of mothers' infant-directed speech is linked to infants' speech perception abilities. Finally, brain measures on infants and adults listening to language suggest that, during early development, the infant brain ""neurally commits"" to the patterns of native language speech and that this both promotes future language learning as well as the decline in nonnative speech perception that occurs at the end of the first year of life. This work on early speech perception is impacting child development, neuroscience, neurobiology, and computational modeling. The early speech measures developed as a part of this project are being used in the study of developmental disabilities including autism, and may provide an early marker of the disability. The data prompted an extension of the Native Language Magnet model to incorporate neural commitment as the mechanism for developmental change. This theoretical position provides the background and framework for the studies in this proposal. Four converging lines of research are proposed to test the theory and further advance our knowledge of infant speech development: (a) speech perception development and its impact on language; (b) the brain correlates of early speech and language development, (c) the role of language input to children, and (d) brain plasticity and the ""critical period"" for language acquisition. The research will produce data that address theories of speech and language development and more general theories of the interface between biology and culture.           n/a",Development of Speech Perception and Brain Plasticity,7386008,R01HD037954,"['Acoustics', 'Address', 'Adult', 'Age-Months', 'Age-Years', 'Appendix', 'Audiotape', 'Autistic Disorder', 'Biology', 'Birth', 'Brain', 'Characteristics', 'Child', 'Child Development', 'Commit', 'Computer Simulation', 'Cues', 'Data', 'Development', 'Developmental Disabilities', 'Event-Related Potentials', 'Evolution', 'Exposure to', 'Funding', 'Future', 'Hour', 'Human', 'Individual', 'Infant', 'Infant Development', 'Intervention Studies', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Learning', 'Life', 'Linguistics', 'Link', 'Longevity', 'Machine Learning', 'Magnetoencephalography', 'Measures', 'Modeling', 'Mothers', 'Nature', 'Neurobiology', 'Neurosciences', 'Newborn Infant', 'Other Finding', 'Pattern', 'Perception', 'Phase', 'Phonetics', 'Play', 'Positioning Attribute', 'Process', 'Property', 'Psychological Theory', 'Research', 'Research Design', 'Research Personnel', 'Role', 'Social Interaction', 'Speech', 'Speech Development', 'Speech Perception', 'Techniques', 'Television', 'Testing', 'Time', 'Work', 'catalyst', 'critical developmental period', 'disability', 'foreign language', 'infancy', 'language processing', 'relating to nervous system', 'skills', 'theories']",NICHD,UNIVERSITY OF WASHINGTON,R01,2008,539966,0.2098458625146422
"Development of Speech Perception and Brain Plasticity    DESCRIPTION (provided by applicant): Language is a hallmark of human beings. In the last 50 years, debates on language have given way to a new view of the process by which humans acquire language. One catalyst for theoretical change has been empirical studies on infants. In the last decade, researchers have not only charted when infants acquire knowledge about the properties of their native language, but how they do so, and this has caused a revision in linguistic and psychological theories. New research focuses on the phonetic units of speech, the consonants and vowels that form building blocks for words. Key advances from this laboratory are cross- language data showing that infants learn from exposure to language in the earliest periods of development and that this alters speech perception to assist language learning. Moreover, our studies show that early speech predicts later language, and that the clarity of mothers' infant-directed speech is linked to infants' speech perception abilities. Finally, brain measures on infants and adults listening to language suggest that, during early development, the infant brain ""neurally commits"" to the patterns of native language speech and that this both promotes future language learning as well as the decline in nonnative speech perception that occurs at the end of the first year of life. This work on early speech perception is impacting child development, neuroscience, neurobiology, and computational modeling. The early speech measures developed as a part of this project are being used in the study of developmental disabilities including autism, and may provide an early marker of the disability. The data prompted an extension of the Native Language Magnet model to incorporate neural commitment as the mechanism for developmental change. This theoretical position provides the background and framework for the studies in this proposal. Four converging lines of research are proposed to test the theory and further advance our knowledge of infant speech development: (a) speech perception development and its impact on language; (b) the brain correlates of early speech and language development, (c) the role of language input to children, and (d) brain plasticity and the ""critical period"" for language acquisition. The research will produce data that address theories of speech and language development and more general theories of the interface between biology and culture.           n/a",Development of Speech Perception and Brain Plasticity,8063408,R01HD037954,"['Acoustics', 'Address', 'Adult', 'Age-Months', 'Age-Years', 'Audiotape', 'Autistic Disorder', 'Biology', 'Birth', 'Brain', 'Characteristics', 'Child', 'Child Development', 'Commit', 'Computer Simulation', 'Cues', 'Data', 'Development', 'Developmental Disabilities', 'Event-Related Potentials', 'Evolution', 'Exposure to', 'Funding', 'Future', 'Hour', 'Human', 'Individual', 'Infant', 'Infant Development', 'Intervention', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Learning', 'Life', 'Linguistics', 'Link', 'Longevity', 'Machine Learning', 'Magnetoencephalography', 'Measures', 'Modeling', 'Mothers', 'Nature', 'Neurobiology', 'Neurosciences', 'Newborn Infant', 'Pattern', 'Perception', 'Phase', 'Phonetics', 'Play', 'Positioning Attribute', 'Process', 'Property', 'Psychological Theory', 'Research', 'Research Design', 'Research Personnel', 'Role', 'Social Interaction', 'Speech', 'Speech Development', 'Speech Perception', 'Techniques', 'Television', 'Testing', 'Time', 'Work', 'catalyst', 'critical period', 'disability', 'foreign language', 'infancy', 'language processing', 'relating to nervous system', 'skills', 'theories']",NICHD,UNIVERSITY OF WASHINGTON,R01,2010,214173,0.2098458625146422
"Development of Speech Perception and Brain Plasticity    DESCRIPTION (provided by applicant): Language is a hallmark of human beings. In the last 50 years, debates on language have given way to a new view of the process by which humans acquire language. One catalyst for theoretical change has been empirical studies on infants. In the last decade, researchers have not only charted when infants acquire knowledge about the properties of their native language, but how they do so, and this has caused a revision in linguistic and psychological theories. New research focuses on the phonetic units of speech, the consonants and vowels that form building blocks for words. Key advances from this laboratory are cross- language data showing that infants learn from exposure to language in the earliest periods of development and that this alters speech perception to assist language learning. Moreover, our studies show that early speech predicts later language, and that the clarity of mothers' infant-directed speech is linked to infants' speech perception abilities. Finally, brain measures on infants and adults listening to language suggest that, during early development, the infant brain ""neurally commits"" to the patterns of native language speech and that this both promotes future language learning as well as the decline in nonnative speech perception that occurs at the end of the first year of life. This work on early speech perception is impacting child development, neuroscience, neurobiology, and computational modeling. The early speech measures developed as a part of this project are being used in the study of developmental disabilities including autism, and may provide an early marker of the disability. The data prompted an extension of the Native Language Magnet model to incorporate neural commitment as the mechanism for developmental change. This theoretical position provides the background and framework for the studies in this proposal. Four converging lines of research are proposed to test the theory and further advance our knowledge of infant speech development: (a) speech perception development and its impact on language; (b) the brain correlates of early speech and language development, (c) the role of language input to children, and (d) brain plasticity and the ""critical period"" for language acquisition. The research will produce data that address theories of speech and language development and more general theories of the interface between biology and culture.           n/a",Development of Speech Perception and Brain Plasticity,7212358,R01HD037954,"['Acoustics', 'Address', 'Adult', 'Age-Months', 'Age-Years', 'Appendix', 'Audiotape', 'Autistic Disorder', 'Biology', 'Birth', 'Brain', 'Characteristics', 'Child', 'Child Development', 'Commit', 'Computer Simulation', 'Cues', 'Data', 'Development', 'Developmental Disabilities', 'Event-Related Potentials', 'Evolution', 'Exposure to', 'Funding', 'Future', 'Hour', 'Human', 'Individual', 'Infant', 'Infant Development', 'Intervention Studies', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Learning', 'Life', 'Linguistics', 'Link', 'Longevity', 'Machine Learning', 'Magnetoencephalography', 'Measures', 'Modeling', 'Mothers', 'Nature', 'Neurobiology', 'Neurosciences', 'Newborn Infant', 'Other Finding', 'Pattern', 'Perception', 'Phase', 'Phonetics', 'Play', 'Positioning Attribute', 'Process', 'Property', 'Psychological Theory', 'Research', 'Research Design', 'Research Personnel', 'Role', 'Social Interaction', 'Speech', 'Speech Development', 'Speech Perception', 'Techniques', 'Television', 'Testing', 'Time', 'Work', 'catalyst', 'critical developmental period', 'disability', 'foreign language', 'infancy', 'language processing', 'relating to nervous system', 'skills', 'theories']",NICHD,UNIVERSITY OF WASHINGTON,R01,2007,573458,0.2098458625146422
"Automated Speech Analysis: A Marker of Drug Intoxication & Treatment Outcome     DESCRIPTION (provided by applicant): A major limitation of existing assessments of clinically-relevant mental states related to drug use, abuse, and treatment is that self-report measures rely on the capacity and motivation to accurately report one's internal experiences. A potential alternative is presented by emerging computer-based natural language processing methods that can extract fine-grained semantic, structural, and syntactic features from free speech1, potentially providing a unique 'window into the mind.' These methods are widely used in industry2, yet remain largely unknown in clinical research. To begin to assess the potential of these advanced analytic methods in clinical research, we recently partnered with IBM computer science researchers to test computer-based analysis of speech semantic structure. In preliminary work, we were able to demonstrate that such methods could detect acute drug intoxication3 and accurately predicted the development of psychosis in clinical risk states4. Here, we propose to build on these highly promising initial findings, conducting three secondary data analyses to rapidly and cost-effectively advance this novel direction. Projects 1 and 2 will extend our preliminary work on speech markers of mental state changes during acute drug intoxication. In Project 1, we will assess speech semantic, structural, and syntactic features as markers of mental state changes due to MDMA (0, 0.75, 1.5 mg/kg; oral). In Project 2, we will extend these findings to another drug, assessing speech markers of intoxication with LSD (0, 70 g; intravenous). These projects are possible because we have access to existing transcripts of free speech from within-subject, controlled laboratory studies of the effects of MDMA (N = 77) and LSD (N = 19). Potential future uses for these methods could include rapid characterization of the effects of emerging drugs and, potentially, detection of acute drug intoxication in the absence of biochemical confirmation. Project 3 will assess the use of speech analysis as a prognostic marker in substance abuse treatment. Specifically, we will use speech transcripts (N = 50) from a currently ongoing study to assess whether features extracted from baseline free speech can predict treatment outcome in cocaine users undergoing 12 weeks of CBT relapse prevention. Self-report5,6 and manual coding of speech7-9 suggest that motivation to change may be a predictor of treatment outcome for substance use disorders: we expect that the fine-grained computational methods we will employ will allow the development of more accurate predictive models. The capacity to use automated methods to detect mental states from free speech has wide ranging, potentially transformative implications for addiction medicine and psychiatry more broadly4,10. Results of the proposed secondary analyses projects will efficiently advance understanding of how automated speech analysis, a non-invasive and cost- effective assessment method, could be used in clinical practice and research about drug abuse. More broadly, results may contribute to the empirical basis for the development of automated, objective, speech- based diagnostic and prognostic tests in psychiatry. PUBLIC HEALTH RELEVANCE: Free speech, a unique `window into the mind', represents a rich source of information that can be mined for clinically-relevant information. This application proposes to apply novel computer science speech analysis methods in three secondary data analysis projects to investigate 1) speech markers of mental states during acute drug intoxication; and 2) speech as a prognostic marker in cognitive behavioral treatment for drug abuse. Results will rapidly and cost-effectively advance understanding of how automated speech analysis could be used in clinical practice and research about drug use, abuse, and treatment.",Automated Speech Analysis: A Marker of Drug Intoxication & Treatment Outcome,9232130,R03DA040855,"['Acute', 'Behavior', 'Biochemical', 'Bypass', 'Cereals', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Cocaine', 'Cocaine Abuse', 'Cocaine Users', 'Code', 'Cognitive Therapy', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnostic tests', 'Disease', 'Double-Blind Method', 'Drug abuse', 'Drug usage', 'Funding', 'Funding Mechanisms', 'Future', 'Human', 'Individual', 'Industry', 'Intoxication', 'Intravenous', 'Laboratory Study', 'Language', 'Lysergic Acid Diethylamide', 'Machine Learning', 'Manuals', 'Measures', 'Medicine', 'Mental disorders', 'Methods', 'Mind', 'Moods', 'Motivation', 'Natural Language Processing', 'Oral', 'Patient Self-Report', 'Patient risk', 'Patients', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Placebos', 'Prognostic Marker', 'Psychiatry', 'Psychotic Disorders', 'Randomized', 'Reporting', 'Research', 'Research Personnel', 'Research Proposals', 'Sampling', 'Scientist', 'Semantics', 'Source', 'Speech', 'Structure', 'Study Subject', 'Substance Use Disorder', 'Technology', 'Testing', 'Transcript', 'Treatment outcome', 'Work', 'addiction', 'analytical method', 'base', 'clinical practice', 'clinical risk', 'clinically relevant', 'cocaine use', 'computer science', 'computerized', 'cost', 'cost effective', 'disorder later incidence prevention', 'ecstasy', 'experience', 'high risk', 'innovation', 'mental state', 'natural language', 'novel', 'outcome prediction', 'predictive modeling', 'prognostic assays', 'programs', 'public health relevance', 'secondary analysis', 'substance abuse treatment', 'syntax']",NIDA,NEW YORK STATE PSYCHIATRIC INSTITUTE,R03,2017,81000,0.33083597087032107
"Automated Speech Analysis: A Marker of Drug Intoxication & Treatment Outcome     DESCRIPTION (provided by applicant): A major limitation of existing assessments of clinically-relevant mental states related to drug use, abuse, and treatment is that self-report measures rely on the capacity and motivation to accurately report one's internal experiences. A potential alternative is presented by emerging computer-based natural language processing methods that can extract fine-grained semantic, structural, and syntactic features from free speech1, potentially providing a unique 'window into the mind.' These methods are widely used in industry2, yet remain largely unknown in clinical research. To begin to assess the potential of these advanced analytic methods in clinical research, we recently partnered with IBM computer science researchers to test computer-based analysis of speech semantic structure. In preliminary work, we were able to demonstrate that such methods could detect acute drug intoxication3 and accurately predicted the development of psychosis in clinical risk states4. Here, we propose to build on these highly promising initial findings, conducting three secondary data analyses to rapidly and cost-effectively advance this novel direction. Projects 1 and 2 will extend our preliminary work on speech markers of mental state changes during acute drug intoxication. In Project 1, we will assess speech semantic, structural, and syntactic features as markers of mental state changes due to MDMA (0, 0.75, 1.5 mg/kg; oral). In Project 2, we will extend these findings to another drug, assessing speech markers of intoxication with LSD (0, 70 g; intravenous). These projects are possible because we have access to existing transcripts of free speech from within-subject, controlled laboratory studies of the effects of MDMA (N = 77) and LSD (N = 19). Potential future uses for these methods could include rapid characterization of the effects of emerging drugs and, potentially, detection of acute drug intoxication in the absence of biochemical confirmation. Project 3 will assess the use of speech analysis as a prognostic marker in substance abuse treatment. Specifically, we will use speech transcripts (N = 50) from a currently ongoing study to assess whether features extracted from baseline free speech can predict treatment outcome in cocaine users undergoing 12 weeks of CBT relapse prevention. Self-report5,6 and manual coding of speech7-9 suggest that motivation to change may be a predictor of treatment outcome for substance use disorders: we expect that the fine-grained computational methods we will employ will allow the development of more accurate predictive models. The capacity to use automated methods to detect mental states from free speech has wide ranging, potentially transformative implications for addiction medicine and psychiatry more broadly4,10. Results of the proposed secondary analyses projects will efficiently advance understanding of how automated speech analysis, a non-invasive and cost- effective assessment method, could be used in clinical practice and research about drug abuse. More broadly, results may contribute to the empirical basis for the development of automated, objective, speech- based diagnostic and prognostic tests in psychiatry.         PUBLIC HEALTH RELEVANCE: Free speech, a unique `window into the mind', represents a rich source of information that can be mined for clinically-relevant information. This application proposes to apply novel computer science speech analysis methods in three secondary data analysis projects to investigate 1) speech markers of mental states during acute drug intoxication; and 2) speech as a prognostic marker in cognitive behavioral treatment for drug abuse. Results will rapidly and cost-effectively advance understanding of how automated speech analysis could be used in clinical practice and research about drug use, abuse, and treatment.            ",Automated Speech Analysis: A Marker of Drug Intoxication & Treatment Outcome,9017684,R03DA040855,"['Acute', 'Behavior', 'Biochemical', 'Bypass', 'Cereals', 'Characteristics', 'Clinical', 'Clinical Research', 'Cocaine', 'Cocaine Abuse', 'Cocaine Users', 'Code', 'Cognitive Therapy', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnostic tests', 'Disease', 'Double-Blind Method', 'Drug abuse', 'Drug usage', 'Funding', 'Funding Mechanisms', 'Future', 'Human', 'Individual', 'Industry', 'Intoxication', 'Intravenous', 'Laboratory Study', 'Language', 'Lysergic Acid Diethylamide', 'Machine Learning', 'Manuals', 'Measures', 'Medicine', 'Mental disorders', 'Methods', 'Mind', 'Mining', 'Moods', 'Motivation', 'Natural Language Processing', 'Oral', 'Outcome', 'Patient Self-Report', 'Patients', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Placebos', 'Prognostic Marker', 'Psychiatry', 'Psychotic Disorders', 'Randomized', 'Reporting', 'Research', 'Research Personnel', 'Research Proposals', 'Sampling', 'Scientist', 'Semantics', 'Source', 'Speech', 'Structure', 'Study Subject', 'Substance Use Disorder', 'Technology', 'Testing', 'Transcript', 'Treatment outcome', 'Work', 'addiction', 'base', 'clinical practice', 'clinical risk', 'clinically relevant', 'cocaine use', 'computer science', 'computerized', 'cost', 'cost effective', 'disorder later incidence prevention', 'ecstasy', 'experience', 'high risk', 'innovation', 'mental state', 'natural language', 'novel', 'outcome prediction', 'predictive modeling', 'prognostic assays', 'programs', 'public health relevance', 'research in practice', 'substance abuse treatment', 'syntax']",NIDA,NEW YORK STATE PSYCHIATRIC INSTITUTE,R03,2016,81000,0.33083597087032107
"Longitudinal Analysis of Spoken Language Characteristics in the Nun Study DESCRIPTION (provided by applicant): Verbal communication is one of the most complex and vital human behaviors negatively affected by neurodegenerative disease, and previous research strongly suggests that linguistic characteristics are promising as early clinical indicators. The Nun Study data set offers a rare opportunity to investigate the application of linguistic methods to the assessment of late life cognitive deficits and the development of neurodegenerative disease. We propose to investigate rate of decline in linguistic ability using previously unanalyzed spoken autobiography audio samples repeated over four waves of assessment. Aim 1 will capitalize on work already done to digitize audio recordings of the Nun Study longitudinal spoken autobiography samples. Verbatim transcripts will be produced and will be time-aligned with the audio from each speech sample. Aim 2 will use computational linguistic methods to calculate measures of syntactic complexity and propositional content, and will evaluate the rate of change on these measures over the 41/2 year follow-up period. Mean rates of change will be estimated in those sisters with repeated speech samples available, and will be compared between those sisters with and without dementia to determine if the rate of decline in linguistic ability is associated with diagnostic status. This project leverages the extensive NIH resources already invested in the Nun Study to 1) analyze an under-utilized aspect of this valuable data set, 2) expand upon the study's early findings in the application of linguistic analysis to the study of aging and disease-development, and 3) update the analysis methodology applied to the autobiography samples by making use of automated methods from the field of computational linguistics to combine information about speech content with information from the audio recording. PUBLIC HEALTH RELEVANCE: The Nun Study data set offers a rare opportunity to study speech quality in the development of dementia. This study will investigate rate of decline in linguistic ability using previously unanalyzed spoken autobiography audio samples from the Nun Study. Rate of change will be estimated for those sisters with repeated speech samples available, and will be compared between sisters with and without dementia to determine if the rate of decline in linguistic ability is associated with diagnostic status.",Longitudinal Analysis of Spoken Language Characteristics in the Nun Study,8572958,R03AG045476,"['Affect', 'Aging', 'Alzheimer&apos', 's Disease', 'American', 'Autobiography', 'Automated Annotation', 'Behavior', 'Behavioral', 'Biological Markers', 'Biomedical Research', 'Brain imaging', 'Cerebrospinal Fluid', 'Characteristics', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Collection', 'Communication', 'Complex', 'Data Analyses', 'Data Set', 'Dementia', 'Development', 'Diagnostic', 'Disease', 'Elderly', 'Evaluation', 'Future', 'Health', 'Human', 'Incidence', 'Investigation', 'Language', 'Lesion', 'Life', 'Linguistics', 'Link', 'Longitudinal Studies', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Neurodegenerative Disorders', 'Neuropsychology', 'Positioning Attribute', 'Proteins', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Sampling', 'Sister', 'Source', 'Speech', 'Symptoms', 'Testing', 'Time', 'Training', 'Transcript', 'United States National Institutes of Health', 'Update', 'Work', 'analog', 'base', 'brain tissue', 'cognitive change', 'computerized', 'digital', 'follow-up', 'healthy aging', 'longitudinal analysis', 'repository', 'syntax', 'usability']",NIA,UNIVERSITY OF MINNESOTA,R03,2013,76000,0.1557702983151446
"Technology-supported, measurement-based supervision for Motivational Interviewing Millions of Americans receive evidence-based counseling for substance use problems each year. Many evidence-based treatments for substance abuse are talk based therapies, such as motivational interviewing (MI), but the existing research-based methodology for evaluating counseling quality is to record sessions and use human rating teams to evaluate them. However, using humans as the assessment tool via behavioral coding is prohibitive in cost and time, can be error prone, and is virtually never used in the real world. Technology is needed that can analyze the speech patterns and spoken language of counseling sessions, provide automatic and intuitive quality scores, and summarize these in actionable feedback. Rapid, performance-based quality metrics could support training, ongoing supervision, and quality assurance for millions of evidence-based counseling sessions for substance abuse each year. Lyssn.io is a start-up targeting the development of implementation-focused technology to support evidence-based counseling. Our goal is to develop innovative health technology solutions that are objective, scalable, and cost efficient. Lyssn.io includes expertise in speech signal processing, machine learning, user-centered design, software engineering, and clinical expertise in evidence-based counseling. Previous NIH-funded research laid a computational foundation for generating MI quality metrics from speech and language features in MI sessions, and led to a prototype of a clinical software support tool, the Counselor Observer Ratings Expert for MI (CORE-MI). The current Fast-Track SBIR proposal includes Phase I, which will focus on understanding clinical workflows, assessing usability, and initial validation of machine learning of MI fidelity measures in the opioid treatment program at Evergreen Treatment Services (ETS) clinic in Seattle, WA. Phase II will focus on robust validation of the speech and language technologies underlying the CORE-MI tool, and development of scalable supervision protocols that integrate CORE-MI supported feedback for counselors. Finally, we will conduct a quasi-experimental evaluation of CORE-MI supported supervision and training at a second ETS clinic in the Puget Sound, focusing on acceptability, usability, and adoption, the impact on supervision, improved MI fidelity and preliminary evidence of increased client retention. The successful execution of this project will break the reliance on human judgment for providing performance-based feedback to MI and will massively expand the capacity to train, supervise, and provide quality assurance in MI for substance abuse. Most evidence-based treatments for substance abuse are in-person psychotherapy and counseling interventions, such as motivational interviewing. There are currently no methods for evaluating the quality of such counseling interventions in the real world to support training, supervision, and quality assurance. Building on an existing prototype, Lyssn.io  a technology start-up focused on scalable and cost-efficient human-centered technologies  will enhance and evaluate a cloud-based, HIPAA-compliant clinical support software tool that uses automated speech recognition and machine learning in an community based opioid replacement clinic.","Technology-supported, measurement-based supervision for Motivational Interviewing",9847959,R44DA046243,"['Adherence', 'Administrator', 'Adoption', 'Alcohol or Other Drugs use', 'Algorithms', 'American', 'Assessment tool', 'Behavior Therapy', 'Behavioral', 'Benchmarking', 'Client', 'Clinic', 'Clinical', 'Code', 'Communities', 'Computer software', 'Consumption', 'Counseling', 'Dependence', 'Development', 'E-learning', 'Enhancement Technology', 'Environment', 'Evaluation', 'Evidence based treatment', 'Feasibility Studies', 'Feedback', 'Foundations', 'Funding', 'Goals', 'Health Insurance Portability and Accountability Act', 'Health Technology', 'Human', 'Intervention', 'Intuition', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mission', 'National Institute of Drug Abuse', 'Needs Assessment', 'Online Systems', 'Opioid', 'Outcome', 'Pattern', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Process', 'Professional counselor', 'Protocols documentation', 'Provider', 'Psychology', 'Psychotherapy', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Site', 'Small Business Innovation Research Grant', 'Software Engineering', 'Software Tools', 'Speech', 'Stream', 'Substance Use Disorder', 'Substance abuse problem', 'Suicide', 'Supervision', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Training Support', 'United States National Institutes of Health', 'Universities', 'Update', 'Utah', 'Validation', 'Work', 'addiction', 'automated speech recognition', 'automobile accident', 'base', 'clinical practice', 'cloud based', 'community setting', 'cost', 'cost efficient', 'dashboard', 'design', 'encryption', 'evidence base', 'experience', 'gun homicide', 'implementation research', 'improved', 'innovation', 'motivational enhancement therapy', 'novel', 'opioid abuse', 'opioid treatment program', 'overdose death', 'prediction algorithm', 'protocol development', 'prototype', 'quality assurance', 'research and development', 'signal processing', 'skills', 'sound', 'speech processing', 'substance abuse treatment', 'support tools', 'technological innovation', 'tool', 'tool development', 'treatment services', 'usability', 'user centered design', 'virtual', 'visual feedback']",NIDA,"LYSSN.IO, INC.",R44,2020,394059,0.13836613837083095
"Technology-supported, measurement-based supervision for Motivational Interviewing Millions of Americans receive evidence-based counseling for substance use problems each year. Many evidence-based treatments for substance abuse are talk based therapies, such as motivational interviewing (MI), but the existing research-based methodology for evaluating counseling quality is to record sessions and use human rating teams to evaluate them. However, using humans as the assessment tool via behavioral coding is prohibitive in cost and time, can be error prone, and is virtually never used in the real world. Technology is needed that can analyze the speech patterns and spoken language of counseling sessions, provide automatic and intuitive quality scores, and summarize these in actionable feedback. Rapid, performance-based quality metrics could support training, ongoing supervision, and quality assurance for millions of evidence-based counseling sessions for substance abuse each year. Lyssn.io is a start-up targeting the development of implementation-focused technology to support evidence-based counseling. Our goal is to develop innovative health technology solutions that are objective, scalable, and cost efficient. Lyssn.io includes expertise in speech signal processing, machine learning, user-centered design, software engineering, and clinical expertise in evidence-based counseling. Previous NIH-funded research laid a computational foundation for generating MI quality metrics from speech and language features in MI sessions, and led to a prototype of a clinical software support tool, the Counselor Observer Ratings Expert for MI (CORE-MI). The current Fast-Track SBIR proposal includes Phase I, which will focus on understanding clinical workflows, assessing usability, and initial validation of machine learning of MI fidelity measures in the opioid treatment program at Evergreen Treatment Services (ETS) clinic in Seattle, WA. Phase II will focus on robust validation of the speech and language technologies underlying the CORE-MI tool, and development of scalable supervision protocols that integrate CORE-MI supported feedback for counselors. Finally, we will conduct a quasi-experimental evaluation of CORE-MI supported supervision and training at a second ETS clinic in the Puget Sound, focusing on acceptability, usability, and adoption, the impact on supervision, improved MI fidelity and preliminary evidence of increased client retention. The successful execution of this project will break the reliance on human judgment for providing performance-based feedback to MI and will massively expand the capacity to train, supervise, and provide quality assurance in MI for substance abuse. Most evidence-based treatments for substance abuse are in-person psychotherapy and counseling interventions, such as motivational interviewing. There are currently no methods for evaluating the quality of such counseling interventions in the real world to support training, supervision, and quality assurance. Building on an existing prototype, Lyssn.io  a technology start-up focused on scalable and cost-efficient human-centered technologies  will enhance and evaluate a cloud-based, HIPAA-compliant clinical support software tool that uses automated speech recognition and machine learning in an community based opioid replacement clinic.","Technology-supported, measurement-based supervision for Motivational Interviewing",9741887,R44DA046243,"['Adherence', 'Administrator', 'Adoption', 'Alcohol or Other Drugs use', 'Algorithms', 'American', 'Assessment tool', 'Behavior Therapy', 'Behavioral', 'Benchmarking', 'Client', 'Clinic', 'Clinical', 'Code', 'Communities', 'Computer software', 'Consumption', 'Counseling', 'Dependence', 'Development', 'E-learning', 'Enhancement Technology', 'Environment', 'Evaluation', 'Evidence based treatment', 'Feasibility Studies', 'Feedback', 'Foundations', 'Funding', 'Goals', 'Health Insurance Portability and Accountability Act', 'Health Technology', 'Human', 'Intervention', 'Intuition', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mission', 'National Institute of Drug Abuse', 'Needs Assessment', 'Online Systems', 'Opioid', 'Outcome', 'Pattern', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Process', 'Professional counselor', 'Protocols documentation', 'Provider', 'Psychology', 'Psychotherapy', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Site', 'Small Business Innovation Research Grant', 'Software Engineering', 'Software Tools', 'Speech', 'Stream', 'Substance Use Disorder', 'Substance abuse problem', 'Suicide', 'Supervision', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Training Support', 'United States National Institutes of Health', 'Universities', 'Update', 'Utah', 'Validation', 'Work', 'addiction', 'automated speech recognition', 'automobile accident', 'base', 'clinical practice', 'cloud based', 'community setting', 'cost', 'cost efficient', 'dashboard', 'design', 'evidence base', 'experience', 'gun homicide', 'implementation research', 'improved', 'innovation', 'motivational enhancement therapy', 'novel', 'opioid abuse', 'opioid treatment program', 'overdose death', 'prediction algorithm', 'protocol development', 'prototype', 'quality assurance', 'research and development', 'signal processing', 'skills', 'sound', 'speech processing', 'substance abuse treatment', 'support tools', 'technological innovation', 'tool', 'tool development', 'treatment services', 'usability', 'user centered design', 'virtual', 'visual feedback']",NIDA,"LYSSN.IO, INC.",R44,2019,553403,0.13836613837083095
"Technology-supported, measurement-based supervision for Motivational Interviewing Millions of Americans receive evidence-based counseling for substance use problems each year. Many  evidence-based treatments for substance abuse are talk based therapies, such as motivational  interviewing (MI), but the existing research-based methodology for evaluating counseling quality is  to record sessions and use human rating teams to evaluate them. However, using humans as the  assessment tool via behavioral coding is prohibitive in cost and time, can be error prone, and is  virtually never used in the real world. Technology is needed that can analyze the speech patterns and spoken language of counseling  sessions, provide automatic and intuitive quality scores, and summarize these in actionable  feedback. Rapid, performance-based quality metrics could support training, ongoing supervision, and  quality assurance for millions of evidence-based counseling sessions for substance abuse each year. Lyssn.io is a start-up targeting the development of implementation-focused technology to support  evidence-based counseling.  Our goal is to develop innovative health technology solutions that are  objective, scalable, and cost efficient. Lyssn.io includes expertise in speech signal processing,  machine learning, user-centered design, software engineering, and clinical expertise in evidence-based counseling.  Previous NIH-funded research laid a computational foundation for generating MI quality metrics from  speech and language features in MI sessions, and led to a prototype of a clinical software support  tool, the Counselor Observer Ratings Expert for MI (CORE-MI). The current Fast-Track SBIR proposal includes Phase I, which will focus on understanding clinical  workflows, assessing usability, and initial validation of machine learning of MI fidelity measures  in the opioid treatment program at Evergreen Treatment Services (ETS) clinic in Seattle, WA. Phase  II will focus on robust validation of the speech and language technologies underlying the CORE-MI  tool, and development of scalable supervision protocols that integrate CORE-MI supported feedback  for counselors. Finally, we will conduct a quasi-experimental evaluation of CORE-MI supported  supervision and training at a second ETS clinic in the Puget Sound, focusing on acceptability,  usability, and adoption, the impact on supervision, improved MI fidelity and preliminary evidence  of increased client retention.  The successful execution of this project will break the reliance on  human judgment for providing performance-based feedback to MI and will massively expand the  capacity to train, supervise, and provide quality assurance in MI for substance abuse. Most evidence-based treatments for substance abuse are in-person psychotherapy and counseling  interventions, such as motivational interviewing. There are currently no methods for evaluating the  quality of such counseling interventions in the real world to support training, supervision, and  quality assurance. Building on an existing prototype, Lyssn.io  a technology start-up focused on  scalable and cost-efficient human-centered technologies  will enhance and evaluate a cloud-based, HIPAA-compliant clinical support software tool that uses automated speech recognition and machine learning in an community  based opioid replacement clinic.","Technology-supported, measurement-based supervision for Motivational Interviewing",9930480,R44DA046243,"['Administrator', 'Adoption', 'Alcohol or Other Drugs use', 'American', 'Assessment tool', 'Behavioral', 'Client', 'Clinic', 'Clinical', 'Code', 'Communities', 'Computer software', 'Counseling', 'Development', 'Evaluation', 'Evidence based treatment', 'Feedback', 'Foundations', 'Funding', 'Goals', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Health Technology', 'Human', 'Intervention', 'Interview', 'Intuition', 'Judgment', 'Language', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Opioid', 'Patients', 'Pattern', 'Performance', 'Persons', 'Phase', 'Primary Health Care', 'Professional counselor', 'Protocols documentation', 'Provider', 'Psychotherapy', 'Research', 'Small Business Innovation Research Grant', 'Software Engineering', 'Software Tools', 'Speech', 'Substance abuse problem', 'Supervision', 'Technology', 'Time', 'Training', 'Training Support', 'Transcript', 'United States National Institutes of Health', 'Validation', 'automated speech recognition', 'base', 'cloud based', 'commercialization', 'cost', 'cost efficient', 'design', 'evidence base', 'improved', 'innovation', 'motivational enhancement therapy', 'opioid treatment program', 'primary care setting', 'prototype', 'quality assurance', 'signal processing', 'sound', 'substance abuse treatment', 'support tools', 'technology validation', 'tool development', 'treatment services', 'usability', 'user centered design', 'virtual']",NIDA,"LYSSN.IO, INC.",R44,2020,134649,0.13836613837083095
"A Specialized Automatic Speech Recognition and Conversational Platform to Enable Socially Assistive Robots for Persons with Mild-to-Moderate Alzheimer's Disease and Related Dementia Abstract 1 in 3 seniors in the United States dies with dementia, of which Alzheimers disease (AD) is the most common form. AD patients suffer from decreased ability to meaningfully communicate and interact, which causes significant stress and burden for both professional caregivers and family members. Socially assistive robots (SARs) have been designed to promote therapeutic interaction and communication. Unfortunately, artificial intelligence (AI) has long been challenged by the speech of elderly persons, who exhibit age-related voice tremors, hesitations, imprecise production of consonants, increased variability of fundamental frequency, and other barriers that can be exacerbated by the neurological changes associated with AD, further complicated by common environmental noises such as the ceiling fan, television, etc. Because of the resulting poor real-world speech and language understanding by available SAR technologies, scarce human caregivers are often required to guide AD patients through SAR interactions, limiting SARs to small deployments, mostly as part of research studies. Unlike existing approaches relying purely on AI, care.coach is developing a SAR-like avatar that converses with elderly and AD patients through truly natural speech. Each avatar is controlled by a 24x7 team of trained human staff who can cost-effectively monitor and engage 12 or more patients sequentially (2 simultaneously) through the audio/visual feeds from the patients avatar device. The staff communicate with each patient by sending text commands which are converted into the avatars voice through a speech synthesis engine. The staff contribute to the system their human abilities for speech and natural language processing (NLP) and for generating free-form conversational responses to help patients build personal relationships with the avatar. The staff are guided by a software-driven expert system embedded into their work interface, which is programmed with evidence-based prompting and protocols to support healthy behaviors and self-care. This SBIR Fast-Track project will leverage the unique data generated by our human- in-the-loop platform to develop new ASR capabilities, enabling fully automatic conversational protocols to engage and support AD patients without human intervention. We aim in Phase I to leverage our unique prior work dataset to train an automatic speech recognition (ASR) engine to enable the understanding of certain types of elderly and AD patient speech more successfully than any currently available engine. We aim in Phase II to incorporate this new engine along with an NLP module into our existing human-in-the-loop avatar system, recruiting a population of AD patients to further train and validate with during a 2-year human subjects study so that we can demonstrate full automation of a significant portion of our avatar conversations with mild- to-moderate level AD patients. Thus, we will improve the commercial scalability of our avatars, while validating our new ASR/NLP engine as the most accurate platform for enabling the next generation of AD-focused SARs. Narrative Artificial intelligence (AI) has long been challenged by the speech of elderly persons, and especially persons with dementia, due to age-related voice tremors, hesitations, imprecise production of consonants, increased variability of fundamental frequency, and other barriers. Unlike existing approaches to socially assistive robots (SARs) relying purely on limited AI for conversation, care.coach has been commercializing a SAR-like avatar that converses with elderly and AD patients through truly natural speech, powered by a 24x7 team of trained human staff. The unique data sets that our solution enables us to gather at commercial scale will be leveraged in this SBIR project to develop an automatic speech recognition (ASR) and natural language processing (NLP) engine that is best-in-class for AD applications, improving the commercial scalability of our avatars by reducing our dependence on human staff, while serving as a new AI platform for enabling the next generation of AD- focused, conversational SARs.",A Specialized Automatic Speech Recognition and Conversational Platform to Enable Socially Assistive Robots for Persons with Mild-to-Moderate Alzheimer's Disease and Related Dementia,9777444,R44AG062014,"['Age', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease related dementia', 'Artificial Intelligence', 'Automation', 'Behavior', 'Caregivers', 'Caring', 'Clinical Research', 'Communication', 'Community Hospitals', 'Computer software', 'Computers', 'Contracts', 'Data', 'Data Set', 'Delirium', 'Dementia', 'Dependence', 'Devices', 'Disease', 'Elderly', 'Exhibits', 'Expert Systems', 'Family', 'Family member', 'Feeds', 'Frequencies', 'Generations', 'Genetic Transcription', 'Goals', 'Home environment', 'Hospitals', 'Hour', 'Human', 'Hybrids', 'Individual', 'Institutes', 'Intervention', 'Jamaica', 'Label', 'Language', 'Licensing', 'Loneliness', 'Manuals', 'Measures', 'Medical center', 'Modeling', 'Monitor', 'Natural Language Processing', 'Network-based', 'Neural Network Simulation', 'Neurologic', 'Noise', 'Patients', 'Persons', 'Phase', 'Population', 'Price', 'Production', 'Protocols documentation', 'Reaction', 'Self Care', 'Semantics', 'Small Business Innovation Research Grant', 'Social Interaction', 'Social support', 'Speech', 'Stress', 'Study Subject', 'System', 'Techniques', 'Technology', 'Television', 'Text', 'Therapeutic', 'Training', 'Tremor', 'United States', 'Universities', 'Validation', 'Visual', 'Voice', 'Washington', 'Work', 'World Health Organization', 'age related', 'aging population', 'automated speech recognition', 'care providers', 'cost', 'deep neural network', 'depressive symptoms', 'design', 'evidence base', 'falls', 'feeding', 'health plan', 'human subject', 'human-in-the-loop', 'improved', 'next generation', 'older patient', 'patient engagement', 'patient response', 'phrases', 'recruit', 'research study', 'response', 'restraint', 'satisfaction', 'skills', 'social assistive robot', 'speech recognition', 'speech synthesis', 'success', 'usability']",NIA,CARE.COACH CORPORATION,R44,2019,349998,-0.015271133771053878
"A Specialized Automatic Speech Recognition and Conversational Platform to Enable Socially Assistive Robots for Persons with Mild-to-Moderate Alzheimer's Disease and Related Dementia Abstract 1 in 3 seniors in the United States dies with dementia, of which Alzheimers disease (AD) is the most common form. AD patients suffer from decreased ability to meaningfully communicate and interact, which causes significant stress and burden for both professional caregivers and family members. Socially assistive robots (SARs) have been designed to promote therapeutic interaction and communication. Unfortunately, artificial intelligence (AI) has long been challenged by the speech of elderly persons, who exhibit age-related voice tremors, hesitations, imprecise production of consonants, increased variability of fundamental frequency, and other barriers that can be exacerbated by the neurological changes associated with AD, further complicated by common environmental noises such as the ceiling fan, television, etc. Because of the resulting poor real-world speech and language understanding by available SAR technologies, scarce human caregivers are often required to guide AD patients through SAR interactions, limiting SARs to small deployments, mostly as part of research studies. Unlike existing approaches relying purely on AI, care.coach is developing a SAR-like avatar that converses with elderly and AD patients through truly natural speech. Each avatar is controlled by a 24x7 team of trained human staff who can cost-effectively monitor and engage 12 or more patients sequentially (2 simultaneously) through the audio/visual feeds from the patients avatar device. The staff communicate with each patient by sending text commands which are converted into the avatars voice through a speech synthesis engine. The staff contribute to the system their human abilities for speech and natural language processing (NLP) and for generating free-form conversational responses to help patients build personal relationships with the avatar. The staff are guided by a software-driven expert system embedded into their work interface, which is programmed with evidence-based prompting and protocols to support healthy behaviors and self-care. This SBIR Fast-Track project will leverage the unique data generated by our human- in-the-loop platform to develop new ASR capabilities, enabling fully automatic conversational protocols to engage and support AD patients without human intervention. We aim in Phase I to leverage our unique prior work dataset to train an automatic speech recognition (ASR) engine to enable the understanding of certain types of elderly and AD patient speech more successfully than any currently available engine. We aim in Phase II to incorporate this new engine along with an NLP module into our existing human-in-the-loop avatar system, recruiting a population of AD patients to further train and validate with during a 2-year human subjects study so that we can demonstrate full automation of a significant portion of our avatar conversations with mild- to-moderate level AD patients. Thus, we will improve the commercial scalability of our avatars, while validating our new ASR/NLP engine as the most accurate platform for enabling the next generation of AD-focused SARs. Narrative Artificial intelligence (AI) has long been challenged by the speech of elderly persons, and especially persons with dementia, due to age-related voice tremors, hesitations, imprecise production of consonants, increased variability of fundamental frequency, and other barriers. Unlike existing approaches to socially assistive robots (SARs) relying purely on limited AI for conversation, care.coach has been commercializing a SAR-like avatar that converses with elderly and AD patients through truly natural speech, powered by a 24x7 team of trained human staff. The unique data sets that our solution enables us to gather at commercial scale will be leveraged in this SBIR project to develop an automatic speech recognition (ASR) and natural language processing (NLP) engine that is best-in-class for AD applications, improving the commercial scalability of our avatars by reducing our dependence on human staff, while serving as a new AI platform for enabling the next generation of AD- focused, conversational SARs.",A Specialized Automatic Speech Recognition and Conversational Platform to Enable Socially Assistive Robots for Persons with Mild-to-Moderate Alzheimer's Disease and Related Dementia,10230460,R44AG062014,"['Age', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease patient', 'Alzheimer&apos', 's disease related dementia', 'Artificial Intelligence', 'Automation', 'Behavior', 'Caregivers', 'Caring', 'Clinical Research', 'Communication', 'Community Hospitals', 'Computer software', 'Computers', 'Contracts', 'Data', 'Data Set', 'Delirium', 'Dementia', 'Dependence', 'Devices', 'Disease', 'Elderly', 'Exhibits', 'Expert Systems', 'Family', 'Family member', 'Feeds', 'Frequencies', 'Generations', 'Genetic Transcription', 'Goals', 'Home environment', 'Hospitals', 'Hour', 'Human', 'Hybrids', 'Individual', 'Institutes', 'Intervention', 'Jamaica', 'Label', 'Language', 'Licensing', 'Loneliness', 'Manuals', 'Measures', 'Medical center', 'Modeling', 'Monitor', 'Natural Language Processing', 'Network-based', 'Neural Network Simulation', 'Neurologic', 'Noise', 'Patients', 'Persons', 'Phase', 'Population', 'Price', 'Production', 'Protocols documentation', 'Reaction', 'Self Care', 'Semantics', 'Small Business Innovation Research Grant', 'Social Interaction', 'Social support', 'Speech', 'Stress', 'Study Subject', 'System', 'Techniques', 'Technology', 'Television', 'Text', 'Therapeutic', 'Training', 'Tremor', 'United States', 'Universities', 'Validation', 'Visual', 'Voice', 'Washington', 'Work', 'World Health Organization', 'age related', 'aging population', 'automated speech recognition', 'care providers', 'cost', 'deep neural network', 'depressive symptoms', 'design', 'evidence base', 'falls', 'health plan', 'human subject', 'human-in-the-loop', 'improved', 'next generation', 'older patient', 'patient engagement', 'patient response', 'phrases', 'recruit', 'research study', 'response', 'restraint', 'satisfaction', 'skills', 'social assistive robot', 'speech recognition', 'speech synthesis', 'success', 'usability']",NIA,CARE.COACH CORPORATION,R44,2020,1502690,-0.015271133771053878
"Computational Speech Analysis in Alzheimer's Disease and Other Neurocognitive Disorders Abstract: Early and accurate diagnosis of neurocognitive disorders (NCDs) is critical for planning, treatment, and research referral, but demands time and expertise often unavailable to primary care providers. Speech and language are often impaired early in the disease course of several NCDs. Previous research has demonstrated the diagnostic potential of computer speech analysis (CSA), with differences between healthy controls and disorders such as mild cognitive impairment (MCI) and Alzheimer's disease. However, there are several additional steps that must be taken to make CSA a diagnostically viable screening tool. This proposal includes a career development plan providing the applicant with training, mentorship, and experience in the following areas in order to bring CSA techniques into clinical practice: 1) computational linguistics and paralinguistics, 2) longitudinal markers of disease, and 3) design of novel technology for dissemination. As part of this training, academic and professional skills, including ethics in research, will also be expanded. Uniquely qualified mentorship and advisory teams have been selected to ensure the success of the proposed training and research. The proposed study is a prospective, longitudinal, observational, cohort investigation of two distinct research groups. The first group is a highly selected and well-characterized research cohort of healthy control, Alzheimer's disease, and MCI subjects (Group A). In Group A, the performance and reproducibility of a machine learning algorithm will be improved to distinguish Alzheimer's disease and MCI from healthy controls using CSA. Multiple regression and voxel-based morphometry will be used to better understand what may drive group differences in CSA measures in Group A as well. Clinical applications of this algorithm will then be assessed in a clinic-based cohort of patients with different NCDs (Group B) in order reduce spectrum bias likely present in prior studies. As sub-aims in both groups, possible further improvement of the algorithmic outcomes with longitudinal CSA measures will also be examined. The overall objective is to develop intuitive, reliable and reproducible CSA-based clinical measures by correlating them with established neuropsychiatric and imaging markers, determining their efficacy in clinical populations, and determining how they change over time. As a result, this research will validate specific speech traits as useful diagnostic markers of neurocognitive disease and explain why those markers differ between patient groups, both of which are major steps towards the design of novel and easily implemented tools in the screening of NCDs such as Alzheimer's disease. PROJECT NARRATIVE Computational speech analysis (CSA) has shown promise as a cost-effective, rapid screening for patients with neurocognitive disorders (NCDs) by objectively and automatically quantifying speech and language use; however, critical steps must be taken before these measures can become clinically useful. I have training and experience in the neurology of speech and language, but require additional training in computational linguistics and paralinguistics, longitudinal markers of disease including neuroimaging and neuropsychological measures, and design of novel technology for dissemination in order to bring CSA into clinical practice. In this project, we propose to investigate the utility of using CSA measures in two distinct patient groups, including a highly characterized group of research participants that includes healthy controls, Alzheimer's disease patients, and mild cognitive impairment patients (Group A), and a group of consented clinic patients with different NCDs (Group B) and to follow these two groups in prospective, longitudinal studies to correlate spontaneous speech measures with standardized linguistic, neuropsychological, and biological measures.",Computational Speech Analysis in Alzheimer's Disease and Other Neurocognitive Disorders,9975566,K23AG063900,"['Accent', 'Address', 'Adult', 'Advisory Committees', 'Algorithms', 'Alzheimer disease screening', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease diagnosis', 'Alzheimer&apos', 's disease pathology', 'Alzheimer&apos', 's disease patient', 'Area', 'Biological', 'Brain', 'Caregivers', 'Characteristics', 'Classification', 'Clinic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Communities', 'Computational Linguistics', 'Computers', 'Consent', 'Cross-Sectional Studies', 'Dementia', 'Development', 'Development Plans', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Diagnostic Sensitivity', 'Disease', 'Disease Marker', 'Early Diagnosis', 'Effectiveness', 'Enrollment', 'Ensure', 'Ethics', 'Evaluation', 'Fostering', 'Frontotemporal Dementia', 'Goals', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Intuition', 'Investigation', 'Knowledge', 'Language', 'Language Tests', 'Lead', 'Lewy Body Dementia', 'Linguistics', 'Liquid substance', 'Longitudinal Studies', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Memory', 'Mentorship', 'Neurocognitive', 'Neurology', 'Neuropsychological Tests', 'Neuropsychology', 'Outcome', 'Participant', 'Patients', 'Performance', 'Population', 'Positioning Attribute', 'Preparation', 'Prevalence', 'Primary Health Care', 'Quality of life', 'Reproducibility', 'Research', 'Screening procedure', 'Speech', 'Standardization', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Variant', 'accurate diagnosis', 'aging population', 'base', 'care providers', 'career development', 'clinical application', 'clinical practice', 'cohort', 'cost effective', 'design', 'diagnostic biomarker', 'experience', 'healthy aging', 'imaging biomarker', 'improved', 'machine learning algorithm', 'mild cognitive impairment', 'morphometry', 'neurocognitive disorder', 'neuroimaging', 'neuropsychiatry', 'new technology', 'novel', 'novel diagnostics', 'novel therapeutics', 'patient screening', 'primary outcome', 'prospective', 'recruit', 'screening', 'skills', 'success', 'technology development', 'tool', 'trait']",NIA,UNIVERSITY OF COLORADO DENVER,K23,2020,188384,0.151441291513793
"SCH: INT: Collaborative Research: Exploiting Voice Assistant Systems for Early Detection of Cognitive Decline Early detection of the cognitive decline involved in Alzheimer's Disease and Related Dementias (ADRD)  in older adults living alone is essential for developing, planning, and initiating interventions and support  systems to improve patients' everyday function and quality of life. Conventional, clinic-based methods for  early diagnosis are expensive, time-consuming, and impracticalfor large-scale screening. This project aims to develop a low-cost, passive, and practical home-based assessment method using Voice Assistant  Systems (VAS) for early detection of cognitive decline, including a set of novel data mining techniques for  sparse time-series speech. The project has three specific aims: 1. Using a recurrent neural network  (RNN) and a softmax regression model, we will develop a transfer learning technique to investigate the  link between the speech from in-lab VAS tasks and cognitive decline. The Pitt corpus from the  DementiaBank database will be used to optimize the RNN parameters and thereby overcome the limited  data problem of VAS. The softmax regression model will allow us to align the feature distributions from the  previous speech data and in-lab VAS speech. 2. We will develop a novel ""many-to-difference"" prediction  model with a symmetric RNN structure to predict the cognitive difference at two ends of a time period from  the sparse time-series data. The proposed model is different from previous ones as the learning focus is  shifted from the short-term pattern differences across users to the pattern difference over time for an  individual user. The proposed model accommodates well for the highly dynamic nature of the inputs and  maximally removes individual characteristics from the prediction result. To analyze the sparse time-series  speech, a new data sampling technique will be used to address the imbalanced data problem, and a data  quality metric will be developed for the proposed model. 3. The team will conduct an 18-month in-lab  evaluation and a 28-month in-home evaluation with a focus on whether the VAS tasks and features from  the in-lab evaluation and the repetition features of the in-home VAS data can measure and predict  cognitive decline in the in-home participants over time. The proposed methods will be integrated into an  interactive system to enable efficient communication on cognitive decline among patients, caregivers, and  clinicians. If successful, the outcomes of this project will provide an opportun ity to provide supportive  evidence to clinicians for the early detection of cognitive impairment outside of a clinic-based setting. This project aims to develop a low-cost, passive, and practical cognitive assessment method using Voice  Assistant Systems (VAS) for early detection of cognitive decline. If successful, the proposed system may  be widely disseminated for the early diagnosis of cognitive impairment to complement existing diagnostic  modalities that could ultimately enable long-term patient and caregiver planning to maintain individual's  independence at home.",SCH: INT: Collaborative Research: Exploiting Voice Assistant Systems for Early Detection of Cognitive Decline,9977514,R01AG067416,"['Address', 'Alzheimer&apos', 's disease related dementia', 'Caregivers', 'Characteristics', 'Clinic', 'Cognitive', 'Communication', 'Complement', 'Consumption', 'Data', 'Databases', 'Diagnostic', 'Early Diagnosis', 'Elderly', 'Evaluation', 'Home environment', 'Impaired cognition', 'Individual', 'Instruction', 'Intervention', 'Learning', 'Link', 'Measures', 'Methods', 'Modality', 'Modeling', 'Nature', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Psychological Transfer', 'Quality of life', 'Research', 'Sampling', 'Series', 'Speech', 'Structure', 'Support System', 'System', 'Techniques', 'Time', 'Voice', 'base', 'cognitive testing', 'cost', 'data mining', 'data quality', 'improved', 'novel', 'predictive modeling', 'recurrent neural network', 'screening']",NIA,UNIVERSITY OF MASSACHUSETTS BOSTON,R01,2019,300000,0.17763188678783476
"SCH: INT: Collaborative Research: Exploiting Voice Assistant Systems for Early Detection of Cognitive Decline Early detection of the cognitive decline involved in Alzheimer's Disease and Related Dementias (ADRD)  in older adults living alone is essential for developing, planning, and initiating interventions and support  systems to improve patients' everyday function and quality of life. Conventional, clinic-based methods for  early diagnosis are expensive, time-consuming, and impractical for large-scale screening. This project aims to develop a low-cost, passive, and practical home-based assessment method using Voice Assistant  Systems (VAS) for early detection of cognitive decline, including a set of novel data mining techniques for  sparse time-series speech. The project has three specific aims: 1. Using a recurrent neural network  (RNN) and a softmax regression model, we will develop a transfer learning technique to investigate the  link between the speech from in-lab VAS tasks and cognitive decline. The Pitt corpus from the  DementiaBank database will be used to optimize the RNN parameters and thereby overcome the limited  data problem of VAS. The softmax regression model will allow us to align the feature distributions from the  previous speech data and in-lab VAS speech. 2. We will develop a novel ""many-to-difference"" prediction  model with a symmetric RNN structure to predict the cognitive difference at two ends of a time period from  the sparse time-series data. The proposed model is different from previous ones as the learning focus is  shifted from the short-term pattern differences across users to the pattern difference over time for an  individual user. The proposed model accommodates well for the highly dynamic nature of the inputs and  maximally removes individual characteristics from the prediction result. To analyze the sparse time-series  speech, a new data sampling technique will be used to address the imbalanced data problem, and a data  quality metric will be developed for the proposed model. 3. The team will conduct an 18-month in-lab  evaluation and a 28-month in-home evaluation with a focus on whether the VAS tasks and features from  the in-lab evaluation and the repetition features of the in-home VAS data can measure and predict  cognitive decline in the in-home participants over time. The proposed methods will be integrated into an  interactive system to enable efficient communication on cognitive decline among patients, caregivers, and  clinicians. If successful, the outcomes of this project will provide an opportunity to provide supportive  evidence to clinicians for the early detection of cognitive impairment outside of a clinic-based setting. This project aims to develop a low-cost, passive, and practical cognitive assessment method using Voice  Assistant Systems (VAS) for early detection of cognitive decline. If successful, the proposed system may  be widely disseminated for the early diagnosis of cognitive impairment to complement existing diagnostic  modalities that could ultimately enable long-term patient and caregiver planning to maintain individual's  independence at home.",SCH: INT: Collaborative Research: Exploiting Voice Assistant Systems for Early Detection of Cognitive Decline,10019452,R01AG067416,"['Address', 'Alzheimer&apos', 's disease related dementia', 'Caregivers', 'Characteristics', 'Clinic', 'Cognition', 'Cognitive', 'Communication', 'Complement', 'Consumption', 'Custom', 'Data', 'Databases', 'Diagnostic', 'Early Diagnosis', 'Elderly', 'Evaluation', 'Home environment', 'Human', 'Impaired cognition', 'Individual', 'Intervention', 'Knowledge', 'Learning', 'Link', 'Measures', 'Methods', 'Modality', 'Modeling', 'Nature', 'Neural Network Simulation', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Population', 'Psychological Transfer', 'Quality of life', 'Research', 'Sampling', 'Series', 'Speech', 'Structure', 'Support System', 'System', 'Techniques', 'Time', 'Visit', 'Voice', 'base', 'cognitive testing', 'cost', 'data mining', 'data quality', 'improved', 'novel', 'predictive modeling', 'recurrent neural network', 'screening']",NIA,UNIVERSITY OF MASSACHUSETTS BOSTON,R01,2020,296138,0.17763188678783476
"Investigating neural mechanisms for flexible, robust speech perception with fMRI     DESCRIPTION (provided by applicant): The brain is bombarded by sensory information from the world, and must extract certain pieces of useful information using limited neural resources. This means that the brain must be efficient, throwing away information that is not needed in order to focus on the most important part of the sensory information from the world. However, information that is uninformative in one situation may be highly informative in another, and thus this efficiency must be matched by flexibility. One domain where this is particularly true is speech perception, where a noisy, ambiguous sensory signal is mapped onto underlying linguistic units like phonemes, words, and sentences. This mapping changes substantially depending on who is talking. One way the brain might deal with this is to learn talker-specific representations which optimize the efficiency with which speech sounds are processed, and deploy or ""swap out"" those representations whenever the talker changes, learning new representations for new talkers as necessary. While there is some evidence that listeners do use such a strategy, little is known about the underlying neural mechanisms. This proposal seeks to clarify these mechanisms through two specific aims. First, functional magnetic resonance imaging (fMRI) will image the brains of listeners while they are hearing words from two talkers with different accents, mixed together. By comparing the areas that are active when the talker switches with areas that are active during periods of learning about each accent (as measured by behavioral responses), the circuits by which listeners learn and deploy talker-specific representations will be elucidated. Second, using multi-voxel pattern analysis techniques, the neural representations of identical speech sounds which have different interpretations depending on the talker will be measured to determine how deeply talker-specific knowledge affects the processing of speech sounds. If talker-specific knowledge is being used to optimize the efficiency of perceptual processing at a low level, then within-category differences should result in more similar patterns of activity, while across-category differences should result in more distinct patterns of activity.         PUBLIC HEALTH RELEVANCE: The ability to flexibly adjust the processing and representation of speech sounds depending on who is talking is absolutely fundamental to the effective and fluent comprehension of spoken language, and impairments in this ability would make daily life very difficult. Completion of the proposed research has the potential to lead to new views on neurological disorders which impact language, like Williams Syndrome and Specific Language Impairment, and possibly new classifications of these disorders. Furthermore, by linking robust speech comprehension to more general perceptual adaptation and learning, the proposed work has the potential to shed light on the underlying pathology of disorders such as Autism Spectrum Disorders, which have been hypothesized to involve deficiencies in the integration of top-down expectations and bottom-up sensory information, both in language processing (Stewart & Ota, 2008; Yu, 2010) and general perception (Pellicano & Burr, 2012).                ","Investigating neural mechanisms for flexible, robust speech perception with fMRI",8992857,F31HD082893,"['Accent', 'Acoustics', 'Address', 'Affect', 'Area', 'Auditory', 'Auditory area', 'Behavior', 'Behavioral', 'Brain', 'Brain imaging', 'Breeding', 'Categories', 'Code', 'Cognition', 'Complement', 'Comprehension', 'Computer Simulation', 'Disease', 'Environment', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Hearing', 'Knowledge', 'Language', 'Lead', 'Learning', 'Life', 'Light', 'Linguistics', 'Link', 'Machine Learning', 'Maps', 'Measures', 'Neurons', 'Pathology', 'Pattern', 'Peach', 'Perception', 'Play', 'Process', 'Production', 'Property', 'Research', 'Research Personnel', 'Resources', 'Role', 'Sensory', 'Sensory Process', 'Shapes', 'Signal Transduction', 'Speech', 'Speech Perception', 'Speech Sound', 'Staging', 'Stimulus', 'Structure', 'System', 'Techniques', 'Thick', 'Training', 'Uncertainty', 'Voice', 'Williams Syndrome', 'Work', 'autism spectrum disorder', 'base', 'behavioral response', 'cognitive neuroscience', 'disease classification', 'expectation', 'experience', 'flexibility', 'improved', 'insight', 'language impairment', 'language processing', 'nervous system disorder', 'neural circuit', 'neural patterning', 'neuromechanism', 'neurophysiology', 'novel', 'public health relevance', 'relating to nervous system', 'skills', 'sound', 'specific language impairment', 'speech processing', 'statistics', 'transmission process']",NICHD,UNIVERSITY OF ROCHESTER,F31,2016,20746,0.17627002706727485
"Investigating neural mechanisms for flexible, robust speech perception with fMRI     DESCRIPTION (provided by applicant): The brain is bombarded by sensory information from the world, and must extract certain pieces of useful information using limited neural resources. This means that the brain must be efficient, throwing away information that is not needed in order to focus on the most important part of the sensory information from the world. However, information that is uninformative in one situation may be highly informative in another, and thus this efficiency must be matched by flexibility. One domain where this is particularly true is speech perception, where a noisy, ambiguous sensory signal is mapped onto underlying linguistic units like phonemes, words, and sentences. This mapping changes substantially depending on who is talking. One way the brain might deal with this is to learn talker-specific representations which optimize the efficiency with which speech sounds are processed, and deploy or ""swap out"" those representations whenever the talker changes, learning new representations for new talkers as necessary. While there is some evidence that listeners do use such a strategy, little is known about the underlying neural mechanisms. This proposal seeks to clarify these mechanisms through two specific aims. First, functional magnetic resonance imaging (fMRI) will image the brains of listeners while they are hearing words from two talkers with different accents, mixed together. By comparing the areas that are active when the talker switches with areas that are active during periods of learning about each accent (as measured by behavioral responses), the circuits by which listeners learn and deploy talker-specific representations will be elucidated. Second, using multi-voxel pattern analysis techniques, the neural representations of identical speech sounds which have different interpretations depending on the talker will be measured to determine how deeply talker-specific knowledge affects the processing of speech sounds. If talker-specific knowledge is being used to optimize the efficiency of perceptual processing at a low level, then within-category differences should result in more similar patterns of activity, while across-category differences should result in more distinct patterns of activity.         PUBLIC HEALTH RELEVANCE: The ability to flexibly adjust the processing and representation of speech sounds depending on who is talking is absolutely fundamental to the effective and fluent comprehension of spoken language, and impairments in this ability would make daily life very difficult. Completion of the proposed research has the potential to lead to new views on neurological disorders which impact language, like Williams Syndrome and Specific Language Impairment, and possibly new classifications of these disorders. Furthermore, by linking robust speech comprehension to more general perceptual adaptation and learning, the proposed work has the potential to shed light on the underlying pathology of disorders such as Autism Spectrum Disorders, which have been hypothesized to involve deficiencies in the integration of top-down expectations and bottom-up sensory information, both in language processing (Stewart & Ota, 2008; Yu, 2010) and general perception (Pellicano & Burr, 2012).                ","Investigating neural mechanisms for flexible, robust speech perception with fMRI",8831930,F31HD082893,"['Accent', 'Acoustics', 'Address', 'Affect', 'Area', 'Auditory', 'Auditory area', 'Behavior', 'Behavioral', 'Brain', 'Brain imaging', 'Breeding', 'Categories', 'Code', 'Cognition', 'Complement', 'Comprehension', 'Computer Simulation', 'Disease', 'Environment', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Hearing', 'Impairment', 'Knowledge', 'Language', 'Lead', 'Learning', 'Life', 'Light', 'Linguistics', 'Link', 'Machine Learning', 'Maps', 'Measures', 'Neurons', 'Pathology', 'Pattern', 'Peach', 'Perception', 'Play', 'Process', 'Production', 'Property', 'Research', 'Research Personnel', 'Resources', 'Role', 'Sensory', 'Sensory Process', 'Shapes', 'Signal Transduction', 'Speech', 'Speech Perception', 'Speech Sound', 'Staging', 'Stimulus', 'Structure', 'System', 'Techniques', 'Thick', 'Training', 'Uncertainty', 'Voice', 'Williams Syndrome', 'Work', 'autism spectrum disorder', 'base', 'behavioral response', 'cognitive neuroscience', 'disease classification', 'expectation', 'experience', 'flexibility', 'improved', 'insight', 'language processing', 'nervous system disorder', 'neural circuit', 'neural patterning', 'neuromechanism', 'neurophysiology', 'novel', 'public health relevance', 'relating to nervous system', 'skills', 'sound', 'specific language impairment', 'speech processing', 'statistics', 'transmission process']",NICHD,UNIVERSITY OF ROCHESTER,F31,2015,28238,0.17627002706727485
"Subthalamic and corticosubthalamic coding of speech production Speech production and control is disrupted in a number of neurological diseases that involve the basal ganglia. Notably, hypophonia and hypokinetic dysarthria (characterized by decreased motor gain) are prevalent in patients with Parkinson's disease (PD). Deep brain stimulation (DBS) of the subthalamic nucleus (STN) produces predictable improvements in other motor symptoms of PD but does not result in consistent improvement in speech and can negatively impact language function. These observations and other accumulating evidence indicate an important role for the basal ganglia in speech. However, a major impediment to developing treatments for speech deficits in movement disorders and reducing speech-related side effects of DBS is the absence of a neurophysiological model for basal ganglia participation in speech production. Testing how general tenets of basal ganglia organization and function apply to the speech motor system presents both unique challenges for clinical neuroscientists and significant opportunities to advance the cognitive neuroscience of speech production. Our overall goals are to determine how motor and linguistic speech information is encoded at multiple levels of granularity within the STN-cortical network, and to determine the relationship between neural activity within the STN-cortical network and the gain of vocal output. Despite the fact that electrophysiological data obtained during DBS surgery offers the unique opportunity to directly assess basal ganglia neuronal activity during speech, this paradigm remains remarkably unexplored. Our central hypothesis is that the STN contributes at multiple levels to the hierarchical control of speech production. Using a completely novel approach, we will rigorously test this hypothesis by simultaneously recording STN units, STN and cortical local field potentials (LFP), and spoken acoustics while PD subjects perform a speech task during DBS surgery. To test for encoding at different levels of granularity, we will explore the extent to which neuronal activity in the STN codes for articulatory and linguistic features associated with different levels of representation within the speech production system (Aim 1). To test for a role in voice modulation, we will explore the extent to which the STN codes for measures of gain, such as volume, pitch and fluency (Aim 2). Additionally, we will directly assess the causal role of STN function in speech production by delivering disruptive stimulation to the STN (Aim 3). A major strength of our project is the complimentary nature of extensive, multi-disciplinary expertise from team members at the University of Pittsburgh, Johns Hopkins University and Carnegie Mellon University. This combined expertise allows us to employ a novel combination of classical analytic methods and more recent machine learning methods for supervised and exploratory analyses to document the neural dynamics of STN and cortical activity during speech production. Speech production and control is disrupted in a number of neurological diseases that involve the basal ganglia, for instance hypophonia and hypokinetic dysarthria are prevalent in patients with Parkinson's disease (PD). We will use a novel experimental approach and combination of analytic techniques to elucidate the contribution of neural activity in the subthalamic nucleus to the hierarchical control of speech production, in subjects with PD undergoing deep brain stimulation surgery.",Subthalamic and corticosubthalamic coding of speech production,9492650,U01NS098969,"['Acoustics', 'Address', 'Adverse effects', 'Basal Ganglia', 'Clinical', 'Code', 'Cues', 'Data', 'Deep Brain Stimulation', 'Dysarthria', 'Electrophysiology (science)', 'Event', 'Goals', 'Language', 'Linguistics', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Motor', 'Movement', 'Movement Disorders', 'Nature', 'Neurobiology', 'Neurons', 'Neurosciences', 'Operative Surgical Procedures', 'Output', 'Parkinson Disease', 'Patients', 'Pattern', 'Performance', 'Phase', 'Production', 'Role', 'STN stimulation', 'Speech', 'Stimulus', 'Stream', 'Structure of subthalamic nucleus', 'Supervision', 'System', 'Techniques', 'Testing', 'Time', 'Universities', 'Voice', 'analytical method', 'base', 'cognitive neuroscience', 'gain of function', 'kinematics', 'learning strategy', 'member', 'microstimulation', 'motor symptom', 'multidisciplinary', 'nervous system disorder', 'neurophysiology', 'novel', 'novel strategies', 'predictive test', 'relating to nervous system', 'response']",NINDS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2018,875182,0.3693002564845749
"Subthalamic and corticosubthalamic coding of speech production Speech production and control is disrupted in a number of neurological diseases that involve the basal ganglia. Notably, hypophonia and hypokinetic dysarthria (characterized by decreased motor gain) are prevalent in patients with Parkinson's disease (PD). Deep brain stimulation (DBS) of the subthalamic nucleus (STN) produces predictable improvements in other motor symptoms of PD but does not result in consistent improvement in speech and can negatively impact language function. These observations and other accumulating evidence indicate an important role for the basal ganglia in speech. However, a major impediment to developing treatments for speech deficits in movement disorders and reducing speech-related side effects of DBS is the absence of a neurophysiological model for basal ganglia participation in speech production. Testing how general tenets of basal ganglia organization and function apply to the speech motor system presents both unique challenges for clinical neuroscientists and significant opportunities to advance the cognitive neuroscience of speech production. Our overall goals are to determine how motor and linguistic speech information is encoded at multiple levels of granularity within the STN-cortical network, and to determine the relationship between neural activity within the STN-cortical network and the gain of vocal output. Despite the fact that electrophysiological data obtained during DBS surgery offers the unique opportunity to directly assess basal ganglia neuronal activity during speech, this paradigm remains remarkably unexplored. Our central hypothesis is that the STN contributes at multiple levels to the hierarchical control of speech production. Using a completely novel approach, we will rigorously test this hypothesis by simultaneously recording STN units, STN and cortical local field potentials (LFP), and spoken acoustics while PD subjects perform a speech task during DBS surgery. To test for encoding at different levels of granularity, we will explore the extent to which neuronal activity in the STN codes for articulatory and linguistic features associated with different levels of representation within the speech production system (Aim 1). To test for a role in voice modulation, we will explore the extent to which the STN codes for measures of gain, such as volume, pitch and fluency (Aim 2). Additionally, we will directly assess the causal role of STN function in speech production by delivering disruptive stimulation to the STN (Aim 3). A major strength of our project is the complimentary nature of extensive, multi-disciplinary expertise from team members at the University of Pittsburgh, Johns Hopkins University and Carnegie Mellon University. This combined expertise allows us to employ a novel combination of classical analytic methods and more recent machine learning methods for supervised and exploratory analyses to document the neural dynamics of STN and cortical activity during speech production. Speech production and control is disrupted in a number of neurological diseases that involve the basal ganglia, for instance hypophonia and hypokinetic dysarthria are prevalent in patients with Parkinson's disease (PD). We will use a novel experimental approach and combination of analytic techniques to elucidate the contribution of neural activity in the subthalamic nucleus to the hierarchical control of speech production, in subjects with PD undergoing deep brain stimulation surgery.",Subthalamic and corticosubthalamic coding of speech production,9355730,U01NS098969,"['Acoustics', 'Address', 'Adverse effects', 'Basal Ganglia', 'Clinical', 'Code', 'Cues', 'Data', 'Deep Brain Stimulation', 'Dysarthria', 'Electrophysiology (science)', 'Event', 'Goals', 'Language', 'Linguistics', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Motor', 'Movement', 'Movement Disorders', 'Nature', 'Neurobiology', 'Neurons', 'Neurosciences', 'Operative Surgical Procedures', 'Output', 'Parkinson Disease', 'Patients', 'Pattern', 'Performance', 'Phase', 'Production', 'Role', 'STN stimulation', 'Speech', 'Stimulus', 'Stream', 'Structure of subthalamic nucleus', 'Supervision', 'System', 'Techniques', 'Testing', 'Time', 'Universities', 'Voice', 'analytical method', 'base', 'cognitive neuroscience', 'gain of function', 'kinematics', 'learning strategy', 'member', 'microstimulation', 'motor symptom', 'multidisciplinary', 'nervous system disorder', 'neurophysiology', 'novel', 'novel strategies', 'relating to nervous system', 'response']",NINDS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2017,900000,0.3693002564845749
"Subthalamic and corticosubthalamic coding of speech production Speech production and control is disrupted in a number of neurological diseases that involve the basal ganglia. Notably, hypophonia and hypokinetic dysarthria (characterized by decreased motor gain) are prevalent in patients with Parkinson's disease (PD). Deep brain stimulation (DBS) of the subthalamic nucleus (STN) produces predictable improvements in other motor symptoms of PD but does not result in consistent improvement in speech and can negatively impact language function. These observations and other accumulating evidence indicate an important role for the basal ganglia in speech. However, a major impediment to developing treatments for speech deficits in movement disorders and reducing speech-related side effects of DBS is the absence of a neurophysiological model for basal ganglia participation in speech production. Testing how general tenets of basal ganglia organization and function apply to the speech motor system presents both unique challenges for clinical neuroscientists and significant opportunities to advance the cognitive neuroscience of speech production. Our overall goals are to determine how motor and linguistic speech information is encoded at multiple levels of granularity within the STN-cortical network, and to determine the relationship between neural activity within the STN-cortical network and the gain of vocal output. Despite the fact that electrophysiological data obtained during DBS surgery offers the unique opportunity to directly assess basal ganglia neuronal activity during speech, this paradigm remains remarkably unexplored. Our central hypothesis is that the STN contributes at multiple levels to the hierarchical control of speech production. Using a completely novel approach, we will rigorously test this hypothesis by simultaneously recording STN units, STN and cortical local field potentials (LFP), and spoken acoustics while PD subjects perform a speech task during DBS surgery. To test for encoding at different levels of granularity, we will explore the extent to which neuronal activity in the STN codes for articulatory and linguistic features associated with different levels of representation within the speech production system (Aim 1). To test for a role in voice modulation, we will explore the extent to which the STN codes for measures of gain, such as volume, pitch and fluency (Aim 2). Additionally, we will directly assess the causal role of STN function in speech production by delivering disruptive stimulation to the STN (Aim 3). A major strength of our project is the complimentary nature of extensive, multi-disciplinary expertise from team members at the University of Pittsburgh, Johns Hopkins University and Carnegie Mellon University. This combined expertise allows us to employ a novel combination of classical analytic methods and more recent machine learning methods for supervised and exploratory analyses to document the neural dynamics of STN and cortical activity during speech production. Speech production and control is disrupted in a number of neurological diseases that involve the basal ganglia, for instance hypophonia and hypokinetic dysarthria are prevalent in patients with Parkinson's disease (PD). We will use a novel experimental approach and combination of analytic techniques to elucidate the contribution of neural activity in the subthalamic nucleus to the hierarchical control of speech production, in subjects with PD undergoing deep brain stimulation surgery.",Subthalamic and corticosubthalamic coding of speech production,9205890,U01NS098969,"['Acoustics', 'Address', 'Adverse effects', 'Basal Ganglia', 'Clinical', 'Code', 'Cues', 'Data', 'Deep Brain Stimulation', 'Dysarthria', 'Electrophysiology (science)', 'Event', 'Goals', 'Language', 'Linguistics', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Motor', 'Movement', 'Movement Disorders', 'Nature', 'Neurobiology', 'Neurons', 'Neurosciences', 'Operative Surgical Procedures', 'Output', 'Parkinson Disease', 'Patients', 'Pattern', 'Performance', 'Phase', 'Phonetics', 'Production', 'Role', 'STN stimulation', 'Speech', 'Stimulus', 'Stream', 'Structure of subthalamic nucleus', 'System', 'Techniques', 'Testing', 'Time', 'Universities', 'Voice', 'base', 'cognitive neuroscience', 'gain of function', 'kinematics', 'learning strategy', 'member', 'microstimulation', 'motor symptom', 'nervous system disorder', 'neurophysiology', 'novel', 'novel strategies', 'relating to nervous system', 'response']",NINDS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2016,900000,0.3693002564845749
"Functional Architecture of Speech Motor Cortex PROJECT SUMMARY Speaking is one of the most complex actions that we perform, yet nearly all of us learn do it effortlessly. The ability to communicate through speech is often described as the unique and defining trait of human behavior. Despite its importance, the basic neural mechanisms that govern our ability to speak fluently remain unresolved. This proposal addresses two fundamental questions at the crossroads of linguistics, systems neuroscience, and biomedical engineering: 1) How are the kinematic and acoustic targets of articulation represented in human speech motor cortex?, 2) What are the coordinated patterns of cortical activation that gives rise to fluent, continuous speech?, and 3) How does prefrontal cortex govern the cognitive inhibitory control of speech (e.g. stopping)? Our studies should greatly advance understanding of how the speech motor cortex encodes the precise control of articulation during speech production as well as determine whether this control system can be harnessed for novel rehabilitative strategies. Three potential areas of impact are: Neurobiology of Language, where results will shed light on neurophysiologic mechanisms of speech motor control; Human Neurophysiology, where insight gained may suggest novel methods for machine learning-based analyses of distributed population neural activity; and Translational NeuroEngineering, where utilization of novel cortical recording technologies at unparalleled spatiotemporal resolution and duration. We propose to investigate the functional organization of the speech motor cortex during controlled vowel and syllable productions, but also from natural, continuous speech. Our methods utilizing safe, high-density, large-scale intracranial electrode recordings in humans represent a significant advancement over current noninvasive neuroimaging approaches. To accomplish this, we must innovate new, integrative approaches to speech motor control research. We have assembled a team with significant multi- disciplinary strengths in neurosurgery, neurology, ethics, computational modeling, machine learning, neuroscience, engineering, and linguistics. The most debilitating aspect of profound paralysis due to trauma, stroke, or disease is loss of the ability to speak, which leads to profound social isolation. Our research leverages foundational knowledge gained during research piloted under a NIH New Innovator (DP2) award. We wish to broaden the impact of our research in the neurobiology of speech motor control. PROJECT NARRATIVE Discovering the neural mechanisms of speech production has major implications for understanding a large number of communication disorders including mutism, stuttering, apraxia of speech, and aphasia. The proposed research will also have immediate impact on clinical brain mapping procedures for speech.",Functional Architecture of Speech Motor Cortex,9548753,U01NS098971,"['Acoustics', 'Acute', 'Address', 'Affect', 'Animal Model', 'Aphasia', 'Apraxias', 'Architecture', 'Archives', 'Area', 'Articulation', 'Articulators', 'Auditory', 'Award', 'Behavior', 'Behavioral', 'Biomedical Engineering', 'Brain', 'Brain Mapping', 'Brain region', 'Chronic', 'Clinical', 'Cognitive', 'Communication impairment', 'Communities', 'Complex', 'Computer Simulation', 'Correlation Studies', 'Data', 'Dementia', 'Devices', 'Dimensions', 'Disease', 'Electrocorticogram', 'Electrodes', 'Engineering', 'Ethics', 'Face', 'Foundations', 'Functional disorder', 'Gestures', 'Goals', 'Human', 'Image', 'Imagery', 'Implant', 'Individual', 'Jaw', 'Knowledge', 'Language', 'Larynx', 'Learning', 'Left', 'Light', 'Linguistics', 'Lip structure', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Monitor', 'Motor Cortex', 'Movement', 'Mutism', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Neurosciences', 'Paralysed', 'Pattern', 'Physiology', 'Play', 'Population', 'Population Dynamics', 'Positioning Attribute', 'Prefrontal Cortex', 'Procedures', 'Production', 'Property', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Signal Transduction', 'Site', 'Social isolation', 'Speech', 'Speech Sound', 'Stroke', 'Stuttering', 'System', 'Technology', 'Time', 'Tongue', 'Trauma', 'Ultrasonography', 'United States National Institutes of Health', 'base', 'cognitive control', 'data sharing', 'density', 'experience', 'implantation', 'innovation', 'insight', 'kinematics', 'motor control', 'multidisciplinary', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'neurosurgery', 'novel', 'relating to nervous system', 'relational database', 'response', 'sound', 'spatiotemporal', 'synergism', 'temporal measurement', 'tool', 'trait']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2018,175477,0.3316350446686326
"Functional Architecture of Speech Motor Cortex PROJECT SUMMARY Speaking is one of the most complex actions that we perform, yet nearly all of us learn do it effortlessly. The ability to communicate through speech is often described as the unique and defining trait of human behavior. Despite its importance, the basic neural mechanisms that govern our ability to speak fluently remain unresolved. This proposal addresses two fundamental questions at the crossroads of linguistics, systems neuroscience, and biomedical engineering: 1) How are the kinematic and acoustic targets of articulation represented in human speech motor cortex?, 2) What are the coordinated patterns of cortical activation that gives rise to fluent, continuous speech?, and 3) How does prefrontal cortex govern the cognitive inhibitory control of speech (e.g. stopping)? Our studies should greatly advance understanding of how the speech motor cortex encodes the precise control of articulation during speech production as well as determine whether this control system can be harnessed for novel rehabilitative strategies. Three potential areas of impact are: Neurobiology of Language, where results will shed light on neurophysiologic mechanisms of speech motor control; Human Neurophysiology, where insight gained may suggest novel methods for machine learning-based analyses of distributed population neural activity; and Translational NeuroEngineering, where utilization of novel cortical recording technologies at unparalleled spatiotemporal resolution and duration. We propose to investigate the functional organization of the speech motor cortex during controlled vowel and syllable productions, but also from natural, continuous speech. Our methods utilizing safe, high-density, large-scale intracranial electrode recordings in humans represent a significant advancement over current noninvasive neuroimaging approaches. To accomplish this, we must innovate new, integrative approaches to speech motor control research. We have assembled a team with significant multi- disciplinary strengths in neurosurgery, neurology, ethics, computational modeling, machine learning, neuroscience, engineering, and linguistics. The most debilitating aspect of profound paralysis due to trauma, stroke, or disease is loss of the ability to speak, which leads to profound social isolation. Our research leverages foundational knowledge gained during research piloted under a NIH New Innovator (DP2) award. We wish to broaden the impact of our research in the neurobiology of speech motor control. PROJECT NARRATIVE Discovering the neural mechanisms of speech production has major implications for understanding a large number of communication disorders including mutism, stuttering, apraxia of speech, and aphasia. The proposed research will also have immediate impact on clinical brain mapping procedures for speech.",Functional Architecture of Speech Motor Cortex,9356341,U01NS098971,"['Acoustics', 'Acute', 'Address', 'Affect', 'Animal Model', 'Aphasia', 'Apraxias', 'Architecture', 'Archives', 'Area', 'Articulation', 'Articulators', 'Auditory', 'Award', 'Behavior', 'Behavioral', 'Biomedical Engineering', 'Brain', 'Brain Mapping', 'Brain region', 'Chronic', 'Clinical', 'Cognitive', 'Communication impairment', 'Communities', 'Complex', 'Computer Simulation', 'Correlation Studies', 'Data', 'Dementia', 'Devices', 'Dimensions', 'Disease', 'Electrocorticogram', 'Electrodes', 'Engineering', 'Ethics', 'Face', 'Foundations', 'Functional disorder', 'Gestures', 'Goals', 'Human', 'Image', 'Imagery', 'Implant', 'Individual', 'Jaw', 'Knowledge', 'Language', 'Larynx', 'Learning', 'Left', 'Light', 'Linguistics', 'Lip structure', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Monitor', 'Motor Cortex', 'Movement', 'Mutism', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Neurosciences', 'Paralysed', 'Pattern', 'Physiology', 'Play', 'Population', 'Population Dynamics', 'Positioning Attribute', 'Prefrontal Cortex', 'Procedures', 'Production', 'Property', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Signal Transduction', 'Site', 'Social isolation', 'Speech', 'Speech Sound', 'Stroke', 'Stuttering', 'System', 'Technology', 'Time', 'Tongue', 'Trauma', 'Ultrasonography', 'United States National Institutes of Health', 'base', 'cognitive control', 'data sharing', 'density', 'experience', 'implantation', 'innovation', 'insight', 'kinematics', 'motor control', 'multidisciplinary', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'neurosurgery', 'novel', 'relating to nervous system', 'relational database', 'response', 'sound', 'spatiotemporal', 'synergism', 'temporal measurement', 'tool', 'trait']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2017,199885,0.3316350446686326
"Functional Architecture of Speech Motor Cortex PROJECT SUMMARY Speaking is one of the most complex actions that we perform, yet nearly all of us learn do it effortlessly. The ability to communicate through speech is often described as the unique and defining trait of human behavior. Despite its importance, the basic neural mechanisms that govern our ability to speak fluently remain unresolved. This proposal addresses two fundamental questions at the crossroads of linguistics, systems neuroscience, and biomedical engineering: 1) How are the kinematic and acoustic targets of articulation represented in human speech motor cortex?, 2) What are the coordinated patterns of cortical activation that gives rise to fluent, continuous speech?, and 3) How does prefrontal cortex govern the cognitive inhibitory control of speech (e.g. stopping)? Our studies should greatly advance understanding of how the speech motor cortex encodes the precise control of articulation during speech production as well as determine whether this control system can be harnessed for novel rehabilitative strategies. Three potential areas of impact are: Neurobiology of Language, where results will shed light on neurophysiologic mechanisms of speech motor control; Human Neurophysiology, where insight gained may suggest novel methods for machine learning-based analyses of distributed population neural activity; and Translational NeuroEngineering, where utilization of novel cortical recording technologies at unparalleled spatiotemporal resolution and duration. We propose to investigate the functional organization of the speech motor cortex during controlled vowel and syllable productions, but also from natural, continuous speech. Our methods utilizing safe, high-density, large-scale intracranial electrode recordings in humans represent a significant advancement over current noninvasive neuroimaging approaches. To accomplish this, we must innovate new, integrative approaches to speech motor control research. We have assembled a team with significant multi- disciplinary strengths in neurosurgery, neurology, ethics, computational modeling, machine learning, neuroscience, engineering, and linguistics. The most debilitating aspect of profound paralysis due to trauma, stroke, or disease is loss of the ability to speak, which leads to profound social isolation. Our research leverages foundational knowledge gained during research piloted under a NIH New Innovator (DP2) award. We wish to broaden the impact of our research in the neurobiology of speech motor control. PROJECT NARRATIVE Discovering the neural mechanisms of speech production has major implications for understanding a large number of communication disorders including mutism, stuttering, apraxia of speech, and aphasia. The proposed research will also have immediate impact on clinical brain mapping procedures for speech.",Functional Architecture of Speech Motor Cortex,9205946,U01NS098971,"['Acoustics', 'Acute', 'Address', 'Affect', 'Animal Model', 'Aphasia', 'Apraxias', 'Architecture', 'Area', 'Articulators', 'Auditory', 'Award', 'Behavior', 'Behavioral', 'Biomedical Engineering', 'Brain', 'Brain Mapping', 'Brain region', 'Chronic', 'Clinical', 'Cognitive', 'Communication impairment', 'Communities', 'Complex', 'Computer Simulation', 'Correlation Studies', 'Data', 'Dementia', 'Devices', 'Dimensions', 'Disease', 'Electrodes', 'Engineering', 'Ethics', 'Face', 'Functional disorder', 'Gestures', 'Goals', 'Human', 'Image Analysis', 'Imagery', 'Implant', 'Individual', 'Jaw', 'Joints', 'Knowledge', 'Language', 'Larynx', 'Learning', 'Left', 'Light', 'Linguistics', 'Lip structure', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methods', 'Monitor', 'Motor Cortex', 'Movement', 'Mutism', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Neurosciences', 'Paralysed', 'Pattern', 'Physiology', 'Play', 'Population', 'Population Dynamics', 'Positioning Attribute', 'Prefrontal Cortex', 'Procedures', 'Production', 'Property', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Signal Transduction', 'Site', 'Social isolation', 'Speech', 'Speech Sound', 'Stroke', 'Stuttering', 'System', 'Technology', 'Time', 'Tongue', 'Trauma', 'Ultrasonography', 'United States National Institutes of Health', 'base', 'cognitive control', 'cortex mapping', 'data sharing', 'density', 'experience', 'implantation', 'innovation', 'insight', 'kinematics', 'motor control', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'neurosurgery', 'novel', 'relating to nervous system', 'relational database', 'research study', 'response', 'sound', 'spatiotemporal', 'temporal measurement', 'tool', 'trait']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2016,199295,0.3316350446686326
"Functional Architecture of Speech Motor Cortex PROJECT SUMMARY Speaking is one of the most complex actions that we perform, yet nearly all of us learn do it effortlessly. The ability to communicate through speech is often described as the unique and defining trait of human behavior. Despite its importance, the basic neural mechanisms that govern our ability to speak fluently remain unresolved. This proposal addresses two fundamental questions at the crossroads of linguistics, systems neuroscience, and biomedical engineering: 1) How are the kinematic and acoustic targets of articulation represented in human speech motor cortex?, 2) What are the coordinated patterns of cortical activation that gives rise to fluent, continuous speech?, and 3) How does prefrontal cortex govern the cognitive inhibitory control of speech (e.g. stopping)? Our studies should greatly advance understanding of how the speech motor cortex encodes the precise control of articulation during speech production as well as determine whether this control system can be harnessed for novel rehabilitative strategies. Three potential areas of impact are: Neurobiology of Language, where results will shed light on neurophysiologic mechanisms of speech motor control; Human Neurophysiology, where insight gained may suggest novel methods for machine learning-based analyses of distributed population neural activity; and Translational NeuroEngineering, where utilization of novel cortical recording technologies at unparalleled spatiotemporal resolution and duration. We propose to investigate the functional organization of the speech motor cortex during controlled vowel and syllable productions, but also from natural, continuous speech. Our methods utilizing safe, high-density, large-scale intracranial electrode recordings in humans represent a significant advancement over current noninvasive neuroimaging approaches. To accomplish this, we must innovate new, integrative approaches to speech motor control research. We have assembled a team with significant multi- disciplinary strengths in neurosurgery, neurology, ethics, computational modeling, machine learning, neuroscience, engineering, and linguistics. The most debilitating aspect of profound paralysis due to trauma, stroke, or disease is loss of the ability to speak, which leads to profound social isolation. Our research leverages foundational knowledge gained during research piloted under a NIH New Innovator (DP2) award. We wish to broaden the impact of our research in the neurobiology of speech motor control. PROJECT NARRATIVE Discovering the neural mechanisms of speech production has major implications for understanding a large number of communication disorders including mutism, stuttering, apraxia of speech, and aphasia. The proposed research will also have immediate impact on clinical brain mapping procedures for speech.",Functional Architecture of Speech Motor Cortex,9645876,U01NS098971,"['Acoustics', 'Acute', 'Address', 'Affect', 'Animal Model', 'Aphasia', 'Apraxias', 'Architecture', 'Archives', 'Area', 'Articulation', 'Articulators', 'Auditory', 'Award', 'Behavior', 'Behavioral', 'Biomedical Engineering', 'Brain', 'Brain Mapping', 'Brain region', 'Chronic', 'Clinical', 'Cognitive', 'Communication impairment', 'Communities', 'Complex', 'Computer Simulation', 'Correlation Studies', 'Data', 'Dementia', 'Devices', 'Dimensions', 'Disease', 'Electrocorticogram', 'Electrodes', 'Engineering', 'Ethics', 'Face', 'Foundations', 'Functional disorder', 'Gestures', 'Goals', 'Human', 'Image', 'Imagery', 'Implant', 'Individual', 'Jaw', 'Knowledge', 'Language', 'Larynx', 'Learning', 'Left', 'Light', 'Linguistics', 'Lip structure', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Monitor', 'Motor Cortex', 'Movement', 'Mutism', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Neurosciences', 'Paralysed', 'Pattern', 'Physiology', 'Play', 'Population', 'Population Dynamics', 'Positioning Attribute', 'Prefrontal Cortex', 'Procedures', 'Production', 'Property', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Signal Transduction', 'Site', 'Social isolation', 'Speech', 'Speech Sound', 'Stroke', 'Stuttering', 'System', 'Technology', 'Time', 'Tongue', 'Trauma', 'Ultrasonography', 'United States National Institutes of Health', 'base', 'cognitive control', 'data sharing', 'density', 'experience', 'implantation', 'innovation', 'insight', 'kinematics', 'motor control', 'multidisciplinary', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'neurosurgery', 'novel', 'relating to nervous system', 'relational database', 'response', 'sound', 'spatiotemporal', 'synergism', 'temporal measurement', 'tool', 'trait']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2018,189567,0.3316350446686326
"Longitudinal Voice Patterns in Bipolar Disorder DESCRIPTION (provided by applicant): The proposed research study will identify changes in acoustic speech parameters, using innovative cell phone based technology, in order to predict clinically significant mood state transitions in individuals with bipolar disorder. The central hypothesis is that there are quantitative changes in acoustic speech patterns that occur in advance of clinically observed mood changes. These changes is speech patterns can be identified using computational methods over longitudinal monitoring of ecologically gathered voice data that requires minimal input from the individual being observed. These computationally determined changes are imperceptible to human observation but are hypothesized to predict clinically significant mood transitions. To test this hypothesis we will study 50 rapid cycling individuals with bipolar I and II disorder and 10 healthy controls for 6 months by recording their acoustic characteristics of speech (not lexical content) while using a mobile ""smart- phone"". In this manner we are gathering data free of observer bias. We will also gather weekly clinical assessments with standardized instruments (Hamilton Depression Rating Scale and Young Mania Rating Scale) in which we will record their physical voice patterns as well. Bipolar disorder is an ideal disorder for the initial study of speech patterns in the assessment of psychopathology. It is an illness with pathological disruptions of emotion, cognitive and motor capacity. There is a periodicity of the illness pattern that oscillates between manic energized states with charged emotions and pressured rapid speech to depressed emotional phases with retarded movements and inhibited quality and quantity of speech. The successful management of patients with bipolar disorder requires ongoing clinical monitoring of mental states. Currently there are few technologies that address the challenge of monitoring individuals long-term in an ecological manner. Speech pattern recognition technology would allow for unobtrusive monitoring that can be seamlessly integrated into daily routine of mobile phone usage to predict future changes in illness states. The proposed study tests a highly innovative approach by developing a practical solution to assist in the longitudinal management of bipolar patients. Computational algorithms of analyzed speech patterns will use statistic (Gausian Mixture Models and Support Vector Machines) and dynamic (Hidden Markov Models) modeling. This project has the potential of transformative advances in the management of psychiatric disease, as speech patterns, and changes therein, are highly likely to be reflective of current and emerging psychopathology. If successful this technology will provide for the prioritization of patients for medical and psychiatric care based on computational detection of change patterns in voice and speech before they are clinically observable. PUBLIC HEALTH RELEVANCE: This is a study that detects measurable changes in speech patterns using computer-based analyses and correlates these changes with pathological variation in clinically assessed mood states. Changes in speech patterns are likely to precede and predict clinically significant mood state changes (to mania or depression). The overall goal is to use computational methods for the early detection of mood changes that will provide the opportunity for early clinical intervention.",Longitudinal Voice Patterns in Bipolar Disorder,8843048,R34MH100404,"['Accent', 'Acoustics', 'Address', 'Algorithms', 'Anxiety Disorders', 'Behavior', 'Bipolar Depression', 'Bipolar Disorder', 'Bipolar I', 'Bipolar II', 'Car Phone', 'Cellular Phone', 'Characteristics', 'Charge', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Cognitive', 'Computational algorithm', 'Computer Simulation', 'Computers', 'Computing Methodologies', 'Data', 'Data Collection', 'Depressed mood', 'Detection', 'Devices', 'Disease', 'Early Diagnosis', 'Emotional', 'Emotions', 'Environmental Monitoring', 'Frequencies', 'Future', 'Goals', 'Hamilton Rating Scale for Depression', 'Health', 'Human', 'Individual', 'Intervention', 'Interview', 'Knowledge', 'Longitudinal Studies', 'Loudness', 'Machine Learning', 'Manic', 'Measurable', 'Measures', 'Medical', 'Mental Depression', 'Mental disorders', 'Modeling', 'Monitor', 'Mood Disorders', 'Moods', 'Motor', 'Movement', 'Neurocognitive', 'Observer Variation', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Pattern Recognition', 'Perception', 'Periodicity', 'Personality', 'Phase', 'Population', 'Prevention', 'Process', 'Property', 'Psychiatric therapeutic procedure', 'Psychopathology', 'Psychotic Disorders', 'Recording of previous events', 'Recruitment Activity', 'Secure', 'Sensory', 'Shapes', 'Solutions', 'Speech', 'Speech Acoustics', 'Stress', 'Structure', 'Technology', 'Telephone', 'Testing', 'Time', 'Variant', 'Voice', 'Work', 'base', 'bipolar mania', 'bipolar patients', 'clinically significant', 'digital', 'heuristics', 'innovation', 'insight', 'instrument', 'lexical', 'markov model', 'mental state', 'pressure', 'programs', 'psychologic', 'research study', 'speech processing', 'statistics', 'tool']",NIMH,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R34,2015,155500,0.32898300296776034
"Longitudinal Voice Patterns in Bipolar Disorder     DESCRIPTION (provided by applicant): The proposed research study will identify changes in acoustic speech parameters, using innovative cell phone based technology, in order to predict clinically significant mood state transitions in individuals with bipolar disorder. The central hypothesis is that there are quantitative changes in acoustic speech patterns that occur in advance of clinically observed mood changes. These changes is speech patterns can be identified using computational methods over longitudinal monitoring of ecologically gathered voice data that requires minimal input from the individual being observed. These computationally determined changes are imperceptible to human observation but are hypothesized to predict clinically significant mood transitions. To test this hypothesis we will study 50 rapid cycling individuals with bipolar I and II disorder and 10 healthy controls for 6 months by recording their acoustic characteristics of speech (not lexical content) while using a mobile ""smart- phone"". In this manner we are gathering data free of observer bias. We will also gather weekly clinical assessments with standardized instruments (Hamilton Depression Rating Scale and Young Mania Rating Scale) in which we will record their physical voice patterns as well. Bipolar disorder is an ideal disorder for the initial study of speech patterns in the assessment of psychopathology. It is an illness with pathological disruptions of emotion, cognitive and motor capacity. There is a periodicity of the illness pattern that oscillates between manic energized states with charged emotions and pressured rapid speech to depressed emotional phases with retarded movements and inhibited quality and quantity of speech. The successful management of patients with bipolar disorder requires ongoing clinical monitoring of mental states. Currently there are few technologies that address the challenge of monitoring individuals long-term in an ecological manner. Speech pattern recognition technology would allow for unobtrusive monitoring that can be seamlessly integrated into daily routine of mobile phone usage to predict future changes in illness states. The proposed study tests a highly innovative approach by developing a practical solution to assist in the longitudinal management of bipolar patients. Computational algorithms of analyzed speech patterns will use statistic (Gausian Mixture Models and Support Vector Machines) and dynamic (Hidden Markov Models) modeling. This project has the potential of transformative advances in the management of psychiatric disease, as speech patterns, and changes therein, are highly likely to be reflective of current and emerging psychopathology. If successful this technology will provide for the prioritization of patients for medical and psychiatric care based on computational detection of change patterns in voice and speech before they are clinically observable.         PUBLIC HEALTH RELEVANCE: This is a study that detects measurable changes in speech patterns using computer-based analyses and correlates these changes with pathological variation in clinically assessed mood states. Changes in speech patterns are likely to precede and predict clinically significant mood state changes (to mania or depression). The overall goal is to use computational methods for the early detection of mood changes that will provide the opportunity for early clinical intervention.                ",Longitudinal Voice Patterns in Bipolar Disorder,8494970,R34MH100404,"['Accent', 'Acoustics', 'Address', 'Algorithms', 'Anxiety Disorders', 'Behavior', 'Bipolar Disorder', 'Car Phone', 'Cellular Phone', 'Characteristics', 'Charge', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Cognitive', 'Computational algorithm', 'Computer Simulation', 'Computers', 'Computing Methodologies', 'Data', 'Data Collection', 'Depressed mood', 'Detection', 'Devices', 'Disease', 'Early Diagnosis', 'Emotional', 'Emotions', 'Environmental Monitoring', 'Frequencies', 'Future', 'Goals', 'Hamilton Rating Scale for Depression', 'Health', 'Human', 'Individual', 'Intervention', 'Interview', 'Knowledge', 'Longitudinal Studies', 'Loudness', 'Machine Learning', 'Manic', 'Measurable', 'Measures', 'Medical', 'Mental Depression', 'Mental disorders', 'Modeling', 'Monitor', 'Mood Disorders', 'Moods', 'Motor', 'Movement', 'Neurocognitive', 'Observer Variation', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Pattern Recognition', 'Perception', 'Periodicity', 'Personality', 'Phase', 'Population', 'Prevention', 'Process', 'Property', 'Psychiatric therapeutic procedure', 'Psychopathology', 'Psychotic Disorders', 'Recording of previous events', 'Recruitment Activity', 'Secure', 'Sensory', 'Shapes', 'Solutions', 'Speech', 'Speech Acoustics', 'Stress', 'Structure', 'Technology', 'Telephone', 'Testing', 'Time', 'Variant', 'Voice', 'Work', 'base', 'clinically significant', 'digital', 'heuristics', 'innovation', 'insight', 'instrument', 'lexical', 'markov model', 'mental state', 'pressure', 'programs', 'psychologic', 'public health relevance', 'research study', 'speech processing', 'statistics', 'tool']",NIMH,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R34,2013,272125,0.32898300296776034
"Longitudinal Voice Patterns in Bipolar Disorder     DESCRIPTION (provided by applicant): The proposed research study will identify changes in acoustic speech parameters, using innovative cell phone based technology, in order to predict clinically significant mood state transitions in individuals with bipolar disorder. The central hypothesis is that there are quantitative changes in acoustic speech patterns that occur in advance of clinically observed mood changes. These changes is speech patterns can be identified using computational methods over longitudinal monitoring of ecologically gathered voice data that requires minimal input from the individual being observed. These computationally determined changes are imperceptible to human observation but are hypothesized to predict clinically significant mood transitions. To test this hypothesis we will study 50 rapid cycling individuals with bipolar I and II disorder and 10 healthy controls for 6 months by recording their acoustic characteristics of speech (not lexical content) while using a mobile ""smart- phone"". In this manner we are gathering data free of observer bias. We will also gather weekly clinical assessments with standardized instruments (Hamilton Depression Rating Scale and Young Mania Rating Scale) in which we will record their physical voice patterns as well. Bipolar disorder is an ideal disorder for the initial study of speech patterns in the assessment of psychopathology. It is an illness with pathological disruptions of emotion, cognitive and motor capacity. There is a periodicity of the illness pattern that oscillates between manic energized states with charged emotions and pressured rapid speech to depressed emotional phases with retarded movements and inhibited quality and quantity of speech. The successful management of patients with bipolar disorder requires ongoing clinical monitoring of mental states. Currently there are few technologies that address the challenge of monitoring individuals long-term in an ecological manner. Speech pattern recognition technology would allow for unobtrusive monitoring that can be seamlessly integrated into daily routine of mobile phone usage to predict future changes in illness states. The proposed study tests a highly innovative approach by developing a practical solution to assist in the longitudinal management of bipolar patients. Computational algorithms of analyzed speech patterns will use statistic (Gausian Mixture Models and Support Vector Machines) and dynamic (Hidden Markov Models) modeling. This project has the potential of transformative advances in the management of psychiatric disease, as speech patterns, and changes therein, are highly likely to be reflective of current and emerging psychopathology. If successful this technology will provide for the prioritization of patients for medical and psychiatric care based on computational detection of change patterns in voice and speech before they are clinically observable.         PUBLIC HEALTH RELEVANCE: This is a study that detects measurable changes in speech patterns using computer-based analyses and correlates these changes with pathological variation in clinically assessed mood states. Changes in speech patterns are likely to precede and predict clinically significant mood state changes (to mania or depression). The overall goal is to use computational methods for the early detection of mood changes that will provide the opportunity for early clinical intervention.                ",Longitudinal Voice Patterns in Bipolar Disorder,8658149,R34MH100404,"['Accent', 'Acoustics', 'Address', 'Algorithms', 'Anxiety Disorders', 'Behavior', 'Bipolar Depression', 'Bipolar Disorder', 'Bipolar I', 'Bipolar II', 'Car Phone', 'Cellular Phone', 'Characteristics', 'Charge', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Cognitive', 'Computational algorithm', 'Computer Simulation', 'Computers', 'Computing Methodologies', 'Data', 'Data Collection', 'Depressed mood', 'Detection', 'Devices', 'Disease', 'Early Diagnosis', 'Emotional', 'Emotions', 'Environmental Monitoring', 'Frequencies', 'Future', 'Goals', 'Hamilton Rating Scale for Depression', 'Health', 'Human', 'Individual', 'Intervention', 'Interview', 'Knowledge', 'Longitudinal Studies', 'Loudness', 'Machine Learning', 'Manic', 'Measurable', 'Measures', 'Medical', 'Mental Depression', 'Mental disorders', 'Modeling', 'Monitor', 'Mood Disorders', 'Moods', 'Motor', 'Movement', 'Neurocognitive', 'Observer Variation', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Pattern Recognition', 'Perception', 'Periodicity', 'Personality', 'Phase', 'Population', 'Prevention', 'Process', 'Property', 'Psychiatric therapeutic procedure', 'Psychopathology', 'Psychotic Disorders', 'Recording of previous events', 'Recruitment Activity', 'Secure', 'Sensory', 'Shapes', 'Solutions', 'Speech', 'Speech Acoustics', 'Stress', 'Structure', 'Technology', 'Telephone', 'Testing', 'Time', 'Variant', 'Voice', 'Work', 'base', 'bipolar mania', 'clinically significant', 'digital', 'heuristics', 'innovation', 'insight', 'instrument', 'lexical', 'markov model', 'mental state', 'pressure', 'programs', 'psychologic', 'public health relevance', 'research study', 'speech processing', 'statistics', 'tool']",NIMH,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R34,2014,272125,0.32898300296776034
"Automated linguistic analyses of semantics and syntax in speech output in the psychosis prodrome:  A novel paradigm to evaluate subtle thought disorder.     DESCRIPTION (provided by applicant): In an effort to intervene before psychosis onset and prevent morbidity, a major recent focus in schizophrenia research has been the identification of young people during a putative prodromal period, so as to develop safe and effective interventions to modify disease course. Over the past decades, studies at Columbia and elsewhere have evaluated clinical high-risk (CHR) individuals across a range of cognitive processes in an effort to identify core deficits of schizophrenia evident before psychosis onset. Subtle thought disorder, manifest in disturbance of language production, is a feature that predates rather than follows, psychosis onset in CHR individuals, and therefore may be an indicator of schizophrenia liability. Subtle thought disorder in schizophrenia and its risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. Here, we propose to instead use a novel automated machine-learning approach to speech analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language. It also evaluates syntax through ""part-of-speech"" tagging. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture subtle thought disorder and discriminate psychosis outcome among CHR individuals. Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we were able to identify a classifier with high accuracy for psychosis onset in a small CHR cohort at Columbia, which included semantic coherence from phrase to phrase, shortened phrase length, and decreased use of determiner pronouns (""which"", ""what"", ""that""). These features were correlated with prodromal symptoms but outperformed them in terms of classification accuracy. They also discriminated schizophrenia from normal speech. While promising, these automated methods of analysis require validation in a second CHR cohort. In this proposal, in collaboration with IBM, we will validate these automated methods using a large archive of speech data from the UCLA CHR cohort. This dataset has several advantages. First, the UCLA CHR cohort has a high prevalence of psychosis transition, important as machine learning is sensitive to group size. Second, it has undergone prior manual linguistic analysis, identifying features of language production that predicted psychosis outcome; hence, automated and manual methods can be directly compared. Third, there are speech data available from healthy controls and recent-onset psychosis patients (for validation). Fourth, several participants have multiple speech assays (such that stability of the classifier can be examined). Beyond validation of methods, we will maximize group size and combine speech data from Columbia and UCLA to characterize a common classifier of psychosis outcome. Automated methods for language analysis may improve prediction of psychosis onset and inform remediation strategies for its prevention. PUBLIC HEALTH RELEVANCE: Subtle thought disorder is an early core feature of schizophrenia evident before psychosis onset: it has traditionally been evaluated using clinical ratings or labor-intensive manual linguistic analyses. This proposal will apply novel computer-based speech analysis methods to existing datasets to identify abnormal semantic and syntactic features of language production that can predict or classify psychosis outcome among youths at clinical high risk for psychosis. Improved characterization of thought disorder can inform targeted preventive interventions for young people at risk for schizophrenia and related psychotic disorders, so as to reduce the morbidity of psychosis.",Automated linguistic analyses of semantics and syntax in speech output in the psychosis prodrome:  A novel paradigm to evaluate subtle thought disorder.,9231498,R03MH108933,"['Address', 'Age', 'Archives', 'Artificial Intelligence', 'Biological Assay', 'Cereals', 'Classification', 'Clinical', 'Collaborations', 'Computers', 'Data', 'Data Set', 'Data Sources', 'Deltastab', 'Development', 'Diagnosis', 'Disease', 'Elderly', 'Emotional', 'Friends', 'High Prevalence', 'Human', 'Impairment', 'Individual', 'Language', 'Lead', 'Length', 'Linguistics', 'Machine Learning', 'Manuals', 'Mental disorders', 'Methods', 'Morbidity - disease rate', 'Natural Language Processing', 'Occupations', 'Outcome', 'Output', 'Participant', 'Patients', 'Population', 'Poverty', 'Prevention', 'Preventive Intervention', 'Production', 'Psychiatrist', 'Psychiatry', 'Psychotic Disorders', 'Research', 'Risk', 'Role', 'Sample Size', 'Sampling', 'Schizophrenia', 'Scientist', 'Semantics', 'Site', 'Source', 'Speech', 'Structure', 'Symptoms', 'Testing', 'Text', 'Thinking', 'Training', 'Validation', 'Youth', 'analytical method', 'base', 'cognitive process', 'cohort', 'demographics', 'disabling symptom', 'effective intervention', 'hazard', 'high risk', 'improved', 'indexing', 'novel', 'phrases', 'prevent', 'public health relevance', 'remediation', 'standard of care', 'syntax', 'targeted treatment', 'tool']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R03,2017,35453,0.13010636642951487
"Automated linguistic analyses of semantics and syntax in speech output in the psychosis prodrome:  A novel paradigm to evaluate subtle thought disorder.     DESCRIPTION (provided by applicant): In an effort to intervene before psychosis onset and prevent morbidity, a major recent focus in schizophrenia research has been the identification of young people during a putative prodromal period, so as to develop safe and effective interventions to modify disease course. Over the past decades, studies at Columbia and elsewhere have evaluated clinical high-risk (CHR) individuals across a range of cognitive processes in an effort to identify core deficits of schizophrenia evident before psychosis onset. Subtle thought disorder, manifest in disturbance of language production, is a feature that predates rather than follows, psychosis onset in CHR individuals, and therefore may be an indicator of schizophrenia liability. Subtle thought disorder in schizophrenia and its risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. Here, we propose to instead use a novel automated machine-learning approach to speech analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language. It also evaluates syntax through ""part-of-speech"" tagging. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture subtle thought disorder and discriminate psychosis outcome among CHR individuals. Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we were able to identify a classifier with high accuracy for psychosis onset in a small CHR cohort at Columbia, which included semantic coherence from phrase to phrase, shortened phrase length, and decreased use of determiner pronouns (""which"", ""what"", ""that""). These features were correlated with prodromal symptoms but outperformed them in terms of classification accuracy. They also discriminated schizophrenia from normal speech. While promising, these automated methods of analysis require validation in a second CHR cohort. In this proposal, in collaboration with IBM, we will validate these automated methods using a large archive of speech data from the UCLA CHR cohort. This dataset has several advantages. First, the UCLA CHR cohort has a high prevalence of psychosis transition, important as machine learning is sensitive to group size. Second, it has undergone prior manual linguistic analysis, identifying features of language production that predicted psychosis outcome; hence, automated and manual methods can be directly compared. Third, there are speech data available from healthy controls and recent-onset psychosis patients (for validation). Fourth, several participants have multiple speech assays (such that stability of the classifier can be examined). Beyond validation of methods, we will maximize group size and combine speech data from Columbia and UCLA to characterize a common classifier of psychosis outcome. Automated methods for language analysis may improve prediction of psychosis onset and inform remediation strategies for its prevention.         PUBLIC HEALTH RELEVANCE: Subtle thought disorder is an early core feature of schizophrenia evident before psychosis onset: it has traditionally been evaluated using clinical ratings or labor-intensive manual linguistic analyses. This proposal will apply novel computer-based speech analysis methods to existing datasets to identify abnormal semantic and syntactic features of language production that can predict or classify psychosis outcome among youths at clinical high risk for psychosis. Improved characterization of thought disorder can inform targeted preventive interventions for young people at risk for schizophrenia and related psychotic disorders, so as to reduce the morbidity of psychosis.            ",Automated linguistic analyses of semantics and syntax in speech output in the psychosis prodrome:  A novel paradigm to evaluate subtle thought disorder.,9017082,R03MH108933,"['Address', 'Age', 'Archives', 'Artificial Intelligence', 'Biological Assay', 'Cereals', 'Classification', 'Clinical', 'Collaborations', 'Computers', 'Data', 'Data Set', 'Data Sources', 'Deltastab', 'Development', 'Diagnosis', 'Disease', 'Elderly', 'Emotional', 'Friends', 'High Prevalence', 'Human', 'Impairment', 'Individual', 'Language', 'Lead', 'Length', 'Linguistics', 'Machine Learning', 'Manuals', 'Mental disorders', 'Methods', 'Morbidity - disease rate', 'Natural Language Processing', 'Occupations', 'Outcome', 'Output', 'Participant', 'Patients', 'Population', 'Poverty', 'Prevention', 'Preventive Intervention', 'Production', 'Psychiatrist', 'Psychiatry', 'Psychotic Disorders', 'Research', 'Risk', 'Role', 'Sample Size', 'Sampling', 'Schizophrenia', 'Scientist', 'Semantics', 'Site', 'Source', 'Speech', 'Structure', 'Symptoms', 'Testing', 'Text', 'Thinking', 'Training', 'Validation', 'Youth', 'base', 'cognitive process', 'cohort', 'demographics', 'disabling symptom', 'effective intervention', 'hazard', 'high risk', 'improved', 'indexing', 'novel', 'phrases', 'prevent', 'public health relevance', 'remediation', 'standard of care', 'syntax', 'targeted treatment', 'tool']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R03,2016,81000,0.13010636642951487
"Molecular profiling of the zebra finch brain Project Summary Avian model organisms, including songbirds (zebra finches, canaries, starlings), parrots, chicken, quail, and pigeons have contributed much to our understanding of brain function and disorders that affect neural development, function, and cognition. Furthermore, many bird groups are being increasingly recognized as having enlarged brains that are capable of advanced cognitive and learning skills that rival and even surpass those in mammals. Despite these contributions, we still lack a clear understanding of how the molecular brain organization in birds compares to that in mammals, including humans. To address this gap, we utilized resource building funds from the NINDS and NIGMS to develop the Zebra finch Expression Brain Atlas (ZEBrA), currently the largest in situ hybridization database of brain gene expression for any avian species. ZEBrA is a publicly accessible website with a database containing >3,500 high resolution digital images of brain sections from adult male zebra finches that are aligned to a reference histological atlas, and hybridized to reveal the brain-wide expression of >720 genes of relevance for brain development, physiology, plasticity, and vocal learning. Notably, nearly 200 of these genes have been linked to speech and/or neural disorders in humans, and/or constitute shared molecular specializations of analogous brain regions for vocal production and learning in birds and humans. Many expression patterns in ZEBrA have also revealed previously unsuspected subdomains that are not visible with conventional histological techniques, as well as enrichments in discrete nuclei within circuits that underlie specific behaviors (e.g., vocal production and learning). Despite these findings, a quantitative analysis of the ZEBrA database has not yet been performed, which has hindered our ability to perform accurate comparative analyses with similar resources from mammals (e.g. Allen Institute's Mouse Brain Atlas - MBA). We propose here to use image analysis methods to extract equivalent regional gene expression data from both databases. The outcomes in finch will define regional molecular profiles of major brain areas and specialized nuclei of the vocal control and learning circuitry, the latter a cognitive trait of high relevance to human speech and language. The ZEBrA and MBA data will also be compared to derive insights into how avian and mammalian brains relate or diverge molecularly. Such insights will help to further validate the use of avian species as informative model organisms for understanding the molecular basis of brain function and cognitive skills, as well as the genetic basis of brain disorders of high relevance to humans. Project Narrative Although avian species have long been important model organisms for studying brain structure-function relationships and disorders that affect neural development, cognition, and behavior, a clear understanding of the molecular organization of the avian brain, as it relates to those of mammals, including humans, has been lacking. Here we propose a comprehensive analysis of the Zebra finch Expression Brain Atlas (ZEBrA) with major goals of defining the molecular profiles of major avian brain areas and specialized nuclei of the vocal learning circuitry, and comparing these profiles to those generated for the Allen Institute's Mouse Brain Atlas. Expected outcomes will include a better understanding of similarities and divergences in the molecular brain architecture of birds and mammals, as well as novel insights into the genetic basis of complex traits like speech, language, and related cognitive functions of high relevance to humans.",Molecular profiling of the zebra finch brain,10062755,R03NS115145,"['Address', 'Adult', 'Affect', 'Algorithms', 'Anatomy', 'Animal Model', 'Apraxias', 'Architecture', 'Area', 'Atlases', 'Behavior', 'Biology', 'Birds', 'Brain', 'Brain Diseases', 'Brain imaging', 'Brain region', 'Cell Density', 'Cell Nucleus', 'Cell Size', 'Cells', 'Chickens', 'Cognition', 'Cognitive', 'Columbidae', 'Comparative Study', 'Complex', 'Corpus striatum structure', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evolution', 'Expressed Sequence Tags', 'FOXP2 gene', 'Family Psittacidae', 'Finches', 'Funding', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genetic', 'Genome', 'Goals', 'Gonadal Steroid Hormones', 'Health', 'Histologic', 'Histological Techniques', 'Human', 'Hybrids', 'Image', 'Image Analysis', 'Impairment', 'In Situ Hybridization', 'Institutes', 'Knowledge', 'Language', 'Language Development', 'Language Disorders', 'Learning', 'Learning Skill', 'Link', 'Mammals', 'Mental disorders', 'Methods', 'Mind', 'Molecular', 'Molecular Profiling', 'Monoclonal Antibody R24', 'Motor', 'Mus', 'Mutation', 'National Institute of General Medical Sciences', 'National Institute of Neurological Disorders and Stroke', 'Neurobiology', 'Neurons', 'Nuclear', 'Outcome', 'Pattern', 'Phase', 'Physiology', 'Positioning Attribute', 'Principal Component Analysis', 'Production', 'Property', 'Quail', 'Resolution', 'Resources', 'Rodent', 'Sampling', 'Serinus', 'Sleep', 'Songbirds', 'Speech', 'Speech Disorders', 'Structure', 'Structure-Activity Relationship', 'Sturnus vulgaris', 'Stuttering', 'System', 'Transcript', 'United States National Institutes of Health', 'Vertebrates', 'adult neurogenesis', 'base', 'behavioral phenotyping', 'cDNA Library', 'cell type', 'cognitive function', 'cognitive skill', 'comparative', 'differential expression', 'digital imaging', 'information model', 'insight', 'interest', 'male', 'nervous system disorder', 'neurodevelopment', 'nonhuman primate', 'novel', 'relating to nervous system', 'sexual dimorphism', 'stem', 'trait', 'vocal control', 'vocal learning', 'web site', 'zebra finch']",NINDS,OREGON HEALTH & SCIENCE UNIVERSITY,R03,2020,154000,0.07618953042325541
"Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach Thought disorder in psychotic disorders and their risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. We propose instead to use a novel automated linguistic corpus-based approach to language analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language, and leads to measures of semantic coherence from one phrase to the next. It also evaluates syntactic complexity through part-of-speech tagging and analysis of speech graphs. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture thought disorder.  Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we identified a classifier with high accuracy for psychosis onset in a small CHR cohort, which included decreased semantic coherence from phrase to phrase, and decreased syntactic complexity, including shortened phrase length and decreased use of determiner pronouns (which, what, that). These features correlated with prodromal symptoms but outperformed them in classification accuracy. They also discriminated schizophrenia from normal speech. We further cross-validated this automated approach in a second small CHR cohort, identifying a semantics/syntax classifier that classified psychosis outcome in both cohorts, and discriminated speech in recent-onset psychosis patients from normal speech.  These automated linguistic analytic methods hold great promise, but their use thus far has been circumscribed to only a few small studies that aim to discriminate schizophrenia from the norm, and in our own work, predict psychosis. There is a critical gap in our understanding of the linguistic mechanisms that underlie thought disorder. To address this gap, in response to PAR-16-136, we propose to use the RDoC construct of language production, and its linguistic corpus-based analytic paradigm, to study thought disorder dimensionally and transdiagnostically, in a large cohort of 150 putatively healthy volunteers, 150 CHR patients, and 150 recent-onset psychosis patients. We expect that latent semantic analysis will yield measures of semantic coherence that index positive thought disorder (tangentiality, derailment), whereas part-of-speech (POS) tagging/speech graphs will yields measures of syntactic complexity that index negative thought disorder (concreteness, poverty of content).  This large language dataset will be obtained from two PSYSCAN/HARMONY sites, such that these language data will be available for secondary analyses with PSYSCAN/HARMONY imaging and EEG data to study language production at the circuit and physiological levels. This large language and clinical dataset will also be archived at NIH for further linguistic analyses by other investigators. Language offers a privileged view into the mind: it is the basis by which we infer others' thoughts. In collaboration with computer scientists at IBM, we will use advanced computational speech analytic approaches to identify the linguistic basis  semantics and syntax  that underlie language production along a spectrum from normal to gradations of thought disorder. Our large international language dataset on 450 individuals will be archived at NIH as a resource for further linguistic analyses.",Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach,9859468,R01MH115332,"['Address', 'Age', 'Archives', 'Artificial Intelligence', 'Canis familiaris', 'Categories', 'Classification', 'Clinical', 'Cognition', 'Collaborations', 'Communication', 'Computers', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electroencephalography', 'Felis catus', 'Genetic', 'Grain', 'Graph', 'Human', 'Image', 'Individual', 'International', 'Interview', 'Language', 'Length', 'Linguistics', 'Manuals', 'Measures', 'Mediator of activation protein', 'Metaphor', 'Methods', 'Mind', 'Mood Disorders', 'Morbidity - disease rate', 'Morphology', 'NIH Program Announcements', 'National Institute of Mental Health', 'Natural Language Processing', 'Outcome', 'Output', 'Patients', 'Physiological', 'Poverty', 'Production', 'Psychotic Disorders', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Risk', 'Schizophrenia', 'Scientific Advances and Accomplishments', 'Scientist', 'Semantics', 'Series', 'Site', 'Speech', 'Sum', 'Symptoms', 'Techniques', 'Text', 'Thinking', 'United States National Institutes of Health', 'Work', 'Yogurt', 'analytical method', 'base', 'cohort', 'data archive', 'functional disability', 'functional outcomes', 'healthy volunteer', 'high risk', 'indexing', 'natural language', 'neural correlate', 'novel', 'phrases', 'relating to nervous system', 'response', 'secondary analysis', 'syntax', 'vector']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2020,533776,0.24589174615065448
"Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach Contact PD/PI: Corcoran, Cheryl M  Thought disorder in psychotic disorders and their risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. We propose instead to use a novel automated linguistic corpus-based approach to language analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language, and leads to measures of semantic coherence from one phrase to the next. It also evaluates syntactic complexity through part-of-speech tagging and analysis of speech graphs. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture thought disorder.  Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we identified a classifier with high accuracy for psychosis onset in a small CHR cohort, which included decreased semantic coherence from phrase to phrase, and decreased syntactic complexity, including shortened phrase length and decreased use of determiner pronouns (which, what, that). These features correlated with prodromal symptoms but outperformed them in classification accuracy. They also discriminated schizophrenia from normal speech. We further cross-validated this automated approach in a second small CHR cohort, identifying a semantics/syntax classifier that classified psychosis outcome in both cohorts, and discriminated speech in recent-onset psychosis patients from normal speech.  These automated linguistic analytic methods hold great promise, but their use thus far has been circumscribed to only a few small studies that aim to discriminate schizophrenia from the norm, and in our own work, predict psychosis. There is a critical gap in our understanding of the linguistic mechanisms that underlie thought disorder. To address this gap, in response to PAR-16-136, we propose to use the RDoC construct of language production, and its linguistic corpus-based analytic paradigm, to study thought disorder dimensionally and transdiagnostically, in a large cohort of 150 putatively healthy volunteers, 150 CHR patients, and 150 recent-onset psychosis patients. We expect that latent semantic analysis will yield measures of semantic coherence that index positive thought disorder (tangentiality, derailment), whereas part-of-speech (POS) tagging/speech graphs will yields measures of syntactic complexity that index negative thought disorder (concreteness, poverty of content).  This large language dataset will be obtained from two PSYSCAN/HARMONY sites, such that these language data will be available for secondary analyses with PSYSCAN/HARMONY imaging and EEG data to study language production at the circuit and physiological levels. This large language and clinical dataset will also be archived at NIH for further linguistic analyses by other investigators. Project Summary/Abstract Page 7 Contact PD/PI: Corcoran, Cheryl M Language offers a privileged view into the mind: it is the basis by which we infer others' thoughts. In collaboration with computer scientists at IBM, we will use advanced computational speech analytic approaches to identify the linguistic basis  semantics and syntax  that underlie language production along a spectrum from normal to gradations of thought disorder. Our large international language dataset on 450 individuals will be archived at NIH as a resource for further linguistic analyses. Project Narrative Page 8",Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach,9903990,R01MH115332,"['Address', 'Archives', 'Artificial Intelligence', 'Classification', 'Clinical', 'Collaborations', 'Computers', 'Data', 'Data Set', 'Dimensions', 'Disease', 'Electroencephalography', 'Grain', 'Graph', 'Human', 'Image', 'Individual', 'International', 'Language', 'Length', 'Linguistics', 'Manuals', 'Measures', 'Methods', 'Mind', 'Outcome', 'Patients', 'Physiological', 'Poverty', 'Production', 'Psychotic Disorders', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Risk', 'Schizophrenia', 'Scientist', 'Semantics', 'Site', 'Speech', 'Symptoms', 'Text', 'Thinking', 'United States National Institutes of Health', 'Work', 'analytical method', 'base', 'cohort', 'healthy volunteer', 'indexing', 'novel', 'phrases', 'response', 'secondary analysis', 'syntax']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2019,89238,0.24589174615065448
"Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach Thought disorder in psychotic disorders and their risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. We propose instead to use a novel automated linguistic corpus-based approach to language analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language, and leads to measures of semantic coherence from one phrase to the next. It also evaluates syntactic complexity through part-of-speech tagging and analysis of speech graphs. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture thought disorder.  Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we identified a classifier with high accuracy for psychosis onset in a small CHR cohort, which included decreased semantic coherence from phrase to phrase, and decreased syntactic complexity, including shortened phrase length and decreased use of determiner pronouns (which, what, that). These features correlated with prodromal symptoms but outperformed them in classification accuracy. They also discriminated schizophrenia from normal speech. We further cross-validated this automated approach in a second small CHR cohort, identifying a semantics/syntax classifier that classified psychosis outcome in both cohorts, and discriminated speech in recent-onset psychosis patients from normal speech.  These automated linguistic analytic methods hold great promise, but their use thus far has been circumscribed to only a few small studies that aim to discriminate schizophrenia from the norm, and in our own work, predict psychosis. There is a critical gap in our understanding of the linguistic mechanisms that underlie thought disorder. To address this gap, in response to PAR-16-136, we propose to use the RDoC construct of language production, and its linguistic corpus-based analytic paradigm, to study thought disorder dimensionally and transdiagnostically, in a large cohort of 150 putatively healthy volunteers, 150 CHR patients, and 150 recent-onset psychosis patients. We expect that latent semantic analysis will yield measures of semantic coherence that index positive thought disorder (tangentiality, derailment), whereas part-of-speech (POS) tagging/speech graphs will yields measures of syntactic complexity that index negative thought disorder (concreteness, poverty of content).  This large language dataset will be obtained from two PSYSCAN/HARMONY sites, such that these language data will be available for secondary analyses with PSYSCAN/HARMONY imaging and EEG data to study language production at the circuit and physiological levels. This large language and clinical dataset will also be archived at NIH for further linguistic analyses by other investigators. Language offers a privileged view into the mind: it is the basis by which we infer others' thoughts. In collaboration with computer scientists at IBM, we will use advanced computational speech analytic approaches to identify the linguistic basis  semantics and syntax  that underlie language production along a spectrum from normal to gradations of thought disorder. Our large international language dataset on 450 individuals will be archived at NIH as a resource for further linguistic analyses.",Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach,9669113,R01MH115332,"['Address', 'Affective', 'Age', 'Archives', 'Artificial Intelligence', 'Canis familiaris', 'Categories', 'Classification', 'Clinical', 'Cognition', 'Collaborations', 'Communication', 'Computers', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electroencephalography', 'Felis catus', 'Genetic', 'Grain', 'Graph', 'Human', 'Image', 'Individual', 'International', 'Interview', 'Language', 'Length', 'Linguistics', 'Manuals', 'Measures', 'Mediator of activation protein', 'Metaphor', 'Methods', 'Mind', 'Morbidity - disease rate', 'Morphology', 'NIH Program Announcements', 'National Institute of Mental Health', 'Natural Language Processing', 'Outcome', 'Output', 'Patients', 'Physiological', 'Poverty', 'Production', 'Psychotic Disorders', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Risk', 'Schizophrenia', 'Scientific Advances and Accomplishments', 'Scientist', 'Semantics', 'Series', 'Site', 'Speech', 'Sum', 'Symptoms', 'Text', 'Thinking', 'United States National Institutes of Health', 'Work', 'Yogurt', 'analytical method', 'base', 'cohort', 'data archive', 'functional disability', 'healthy volunteer', 'high risk', 'indexing', 'natural language', 'neural correlate', 'novel', 'phrases', 'relating to nervous system', 'response', 'secondary analysis', 'syntax', 'vector']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2019,468909,0.24589174615065448
"Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach Thought disorder in psychotic disorders and their risk states has typically been evaluated using clinical rating scales, and occasionally labor-intensive manual methods of linguistic analysis. We propose instead to use a novel automated linguistic corpus-based approach to language analysis informed by artificial intelligence. The method derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to language, and leads to measures of semantic coherence from one phrase to the next. It also evaluates syntactic complexity through part-of-speech tagging and analysis of speech graphs. These analyses yield fine-grained indices of speech semantics and syntax that may more accurately capture thought disorder.  Using these automated methods of speech analysis, in collaboration with computer scientists from IBM, we identified a classifier with high accuracy for psychosis onset in a small CHR cohort, which included decreased semantic coherence from phrase to phrase, and decreased syntactic complexity, including shortened phrase length and decreased use of determiner pronouns (which, what, that). These features correlated with prodromal symptoms but outperformed them in classification accuracy. They also discriminated schizophrenia from normal speech. We further cross-validated this automated approach in a second small CHR cohort, identifying a semantics/syntax classifier that classified psychosis outcome in both cohorts, and discriminated speech in recent-onset psychosis patients from normal speech.  These automated linguistic analytic methods hold great promise, but their use thus far has been circumscribed to only a few small studies that aim to discriminate schizophrenia from the norm, and in our own work, predict psychosis. There is a critical gap in our understanding of the linguistic mechanisms that underlie thought disorder. To address this gap, in response to PAR-16-136, we propose to use the RDoC construct of language production, and its linguistic corpus-based analytic paradigm, to study thought disorder dimensionally and transdiagnostically, in a large cohort of 150 putatively healthy volunteers, 150 CHR patients, and 150 recent-onset psychosis patients. We expect that latent semantic analysis will yield measures of semantic coherence that index positive thought disorder (tangentiality, derailment), whereas part-of-speech (POS) tagging/speech graphs will yields measures of syntactic complexity that index negative thought disorder (concreteness, poverty of content).  This large language dataset will be obtained from two PSYSCAN/HARMONY sites, such that these language data will be available for secondary analyses with PSYSCAN/HARMONY imaging and EEG data to study language production at the circuit and physiological levels. This large language and clinical dataset will also be archived at NIH for further linguistic analyses by other investigators. Language offers a privileged view into the mind: it is the basis by which we infer others' thoughts. In collaboration with computer scientists at IBM, we will use advanced computational speech analytic approaches to identify the linguistic basis  semantics and syntax  that underlie language production along a spectrum from normal to gradations of thought disorder. Our large international language dataset on 450 individuals will be archived at NIH as a resource for further linguistic analyses.",Using the RDoC Approach to Understand Thought Disorder: A Linguistic Corpus-Based Approach,9435649,R01MH115332,"['Address', 'Affective', 'Age', 'Archives', 'Artificial Intelligence', 'Canis familiaris', 'Categories', 'Classification', 'Clinical', 'Cognition', 'Collaborations', 'Communication', 'Computers', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electroencephalography', 'Felis catus', 'Genetic', 'Grain', 'Graph', 'Human', 'Image', 'Individual', 'International', 'Interview', 'Language', 'Length', 'Linguistics', 'Manuals', 'Measures', 'Mediator of activation protein', 'Metaphor', 'Methods', 'Mind', 'Morbidity - disease rate', 'Morphology', 'NIH Program Announcements', 'National Institute of Mental Health', 'Natural Language Processing', 'Outcome', 'Output', 'Patient risk', 'Patients', 'Physiological', 'Poverty', 'Production', 'Psychotic Disorders', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Risk', 'Schizophrenia', 'Scientific Advances and Accomplishments', 'Scientist', 'Semantics', 'Series', 'Site', 'Speech', 'Sum', 'Symptoms', 'Text', 'Thinking', 'United States National Institutes of Health', 'Work', 'Yogurt', 'analytical method', 'base', 'cohort', 'data archive', 'functional disability', 'healthy volunteer', 'high risk', 'indexing', 'natural language', 'neural correlate', 'novel', 'phrases', 'relating to nervous system', 'response', 'secondary analysis', 'syntax', 'vector']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2018,513989,0.24589174615065448
"Using Speech Acoustics to Reveal Motor Disruptions in Psychosis Project summary The goal of this project is to investigate the feasibility of using speech acoustics as a clinical biomarker in individuals at clinical high risk (CHR) for developing psychosis. There is evidence that disruptions to cortico-cerebellar circuits in individuals experiencing attenuated psychosis symptoms impact motor control of the face and limbs. This proposal would be the first study to examine whether these motor disruptions in high-risk populations also affect the complex motor control required for speech. In Aim 1 an instrumental approach will be used to investigate the acoustic correlates of psychosis risk. Specifically, speech data will be collected to investigate fine-grained acoustic properties of vowels and consonants in simple repetition tasks as well as during more naturalistic conversational speech. The speech of CHR young adults will be compared to age-matched healthy controls to discover if there are group differences in the speech acoustics that allow us to classify speech samples into healthy and clinical groups. To enable fast, reliable analysis, machine learning-based algorithms will be used to measure the acoustics speech properties of interest. In Aim 2, the speech properties measured in Aim 1 will be compared to other behavioral measures, in order to discover if they correlate with several measures of cerebellar dysfunction (posture control, procedural learning, and motor timing) that are known to occur in CHR individuals. These measures will provide convergent validity for these novel speech measures. Cognitive capabilities which are related and unrelated to speech and motor control will also be assessed, to provide specificity and divergent validity to these measures. In Aim 3, the links between speech features and changes in symptom severity will be assessed at two time points, connecting changes in speech motor control to longitudinal changes in the progression of the symptoms over 12 months. These investigations may reveal speech as a novel and easily-collected biomarker enabling early detection of psychosis risk. Project narrative The goal of this proposal is to determine whether speech patterns can signal vulnerability to psychotic disorders such as schizophrenia. Speech samples will be collected from high-risk and matched healthy control participants in order to: determine if there are abnormalities in the acoustics of vowels and consonants; evaluate if these properties map on to dysfunction of the cerebellum (a brain region impacted in the development of psychosis that also plays a role in speech motor control); and relate these properties to symptom severity and illness progression. This study will lay the groundwork for the use of speech as an inexpensive, non-invasive, and mechanistically-relevant metric that will ultimately support earlier identification and facilitate timely treatment.",Using Speech Acoustics to Reveal Motor Disruptions in Psychosis,9898478,R21MH119677,"['Acoustics', 'Address', 'Affect', 'Age', 'Algorithms', 'Articulators', 'Attenuated', 'Behavioral', 'Biological Markers', 'Brain region', 'Cerebellar Diseases', 'Cerebellum', 'Clinical', 'Cognitive', 'Complex', 'Computers', 'Control Groups', 'Cueing for speech', 'Data', 'Development', 'Diagnosis', 'Dyskinetic syndrome', 'Early Diagnosis', 'Early Intervention', 'Early identification', 'Ensure', 'Equilibrium', 'Etiology', 'Face', 'Fingers', 'Functional disorder', 'Goals', 'Grain', 'Individual', 'Intervention', 'Interview', 'Investigation', 'Larynx', 'Learning', 'Limb structure', 'Linguistics', 'Link', 'Lip structure', 'Literature', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Motor', 'Movement', 'Musculoskeletal Equilibrium', 'National Institute of Mental Health', 'Neurologic', 'Participant', 'Pathogenicity', 'Pattern', 'Play', 'Population', 'Positioning Attribute', 'Posture', 'Process', 'Production', 'Property', 'Psychotic Disorders', 'Research', 'Research Personnel', 'Risk', 'Role', 'Roter', 'Sampling', 'Schizophrenia', 'Scientist', 'Sensory', 'Severities', 'Severity of illness', 'Signal Transduction', 'Specificity', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'Structure', 'Symptoms', 'System', 'Testing', 'Time', 'Tongue', 'Variant', 'Youth', 'automated algorithm', 'base', 'behavior measurement', 'clinical biomarkers', 'clinical predictors', 'experience', 'high risk', 'high risk population', 'improved', 'indexing', 'individualized medicine', 'interest', 'motor behavior', 'motor control', 'motor deficit', 'motor disorder', 'motor learning', 'novel', 'novel marker', 'potential biomarker', 'relating to nervous system', 'sensory integration', 'tool', 'vocal control', 'young adult']",NIMH,NORTHWESTERN UNIVERSITY,R21,2020,185000,0.35173740302829687
"Using Speech Acoustics to Reveal Motor Disruptions in Psychosis Project summary The goal of this project is to investigate the feasibility of using speech acoustics as a clinical biomarker in individuals at clinical high risk (CHR) for developing psychosis. There is evidence that disruptions to cortico-cerebellar circuits in individuals experiencing attenuated psychosis symptoms impact motor control of the face and limbs. This proposal would be the first study to examine whether these motor disruptions in high-risk populations also affect the complex motor control required for speech. In Aim 1 an instrumental approach will be used to investigate the acoustic correlates of psychosis risk. Specifically, speech data will be collected to investigate fine-grained acoustic properties of vowels and consonants in simple repetition tasks as well as during more naturalistic conversational speech. The speech of CHR young adults will be compared to age-matched healthy controls to discover if there are group differences in the speech acoustics that allow us to classify speech samples into healthy and clinical groups. To enable fast, reliable analysis, machine learning-based algorithms will be used to measure the acoustics speech properties of interest. In Aim 2, the speech properties measured in Aim 1 will be compared to other behavioral measures, in order to discover if they correlate with several measures of cerebellar dysfunction (posture control, procedural learning, and motor timing) that are known to occur in CHR individuals. These measures will provide convergent validity for these novel speech measures. Cognitive capabilities which are related and unrelated to speech and motor control will also be assessed, to provide specificity and divergent validity to these measures. In Aim 3, the links between speech features and changes in symptom severity will be assessed at two time points, connecting changes in speech motor control to longitudinal changes in the progression of the symptoms over 12 months. These investigations may reveal speech as a novel and easily-collected biomarker enabling early detection of psychosis risk. Project narrative The goal of this proposal is to determine whether speech patterns can signal vulnerability to psychotic disorders such as schizophrenia. Speech samples will be collected from high-risk and matched healthy control participants in order to: determine if there are abnormalities in the acoustics of vowels and consonants; evaluate if these properties map on to dysfunction of the cerebellum (a brain region impacted in the development of psychosis that also plays a role in speech motor control); and relate these properties to symptom severity and illness progression. This study will lay the groundwork for the use of speech as an inexpensive, non-invasive, and mechanistically-relevant metric that will ultimately support earlier identification and facilitate timely treatment.",Using Speech Acoustics to Reveal Motor Disruptions in Psychosis,9746454,R21MH119677,"['Acoustics', 'Address', 'Affect', 'Age', 'Algorithms', 'Articulators', 'Attenuated', 'Behavior', 'Behavioral', 'Biological Markers', 'Brain region', 'Cerebellar Diseases', 'Cerebellum', 'Clinical', 'Cognitive', 'Complex', 'Computers', 'Control Groups', 'Cueing for speech', 'Data', 'Development', 'Diagnosis', 'Dyskinetic syndrome', 'Early Diagnosis', 'Early Intervention', 'Early identification', 'Ensure', 'Equilibrium', 'Etiology', 'Face', 'Fingers', 'Functional disorder', 'Goals', 'Grain', 'Individual', 'Intervention', 'Interview', 'Investigation', 'Larynx', 'Learning', 'Limb structure', 'Linguistics', 'Link', 'Lip structure', 'Literature', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Motor', 'Movement', 'Musculoskeletal Equilibrium', 'National Institute of Mental Health', 'Neurologic', 'Participant', 'Pathogenicity', 'Pattern', 'Play', 'Population', 'Positioning Attribute', 'Posture', 'Process', 'Production', 'Property', 'Psychotic Disorders', 'Research', 'Research Personnel', 'Risk', 'Role', 'Roter', 'Sampling', 'Schizophrenia', 'Scientist', 'Sensory', 'Severities', 'Severity of illness', 'Signal Transduction', 'Specificity', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'Structure', 'Symptoms', 'System', 'Testing', 'Time', 'Tongue', 'Variant', 'Youth', 'base', 'behavior measurement', 'clinical biomarkers', 'clinical predictors', 'experience', 'high risk', 'high risk population', 'improved', 'indexing', 'individualized medicine', 'interest', 'motor control', 'motor deficit', 'motor disorder', 'motor learning', 'novel', 'novel marker', 'potential biomarker', 'relating to nervous system', 'sensory integration', 'tool', 'vocal control', 'young adult']",NIMH,NORTHWESTERN UNIVERSITY,R21,2019,239000,0.35173740302829687
"Predictors of Speech Quality after Tongue Cancer Surgery    DESCRIPTION (provided by applicant): Oral, head and neck cancer represents 3% of all cancers in the United States and is the 6th most common cancer worldwide. Recent studies have shown a five-fold increase in the incidence of squamous cell carcinomas of the oral portion of the tongue among young men and a six-fold increase among young women. Glossectomy is the surgical removal of a cancerous tumor. The amount of tongue tissue removed is not negotiable, because the surgeon must remove the entire tumor plus about 1 cm of tissue around it. The reconstructive phase focuses on maximizing the patient's post-operative quality of life, including the critical function of speech. Patients with poor speech avoid social situations, develop self-consciousness, and even depression. Surgeons must choose a reconstruction method, such as primary closure (suture the wound), or replacement with a 'flap' of tissue taken from elsewhere in the body. Larger tumors (above 4 cm in largest dimension) almost always require some type of flap. Among early stage tumors, however, there is no consensus as to the optimal closure procedure. We propose an objective study of patients for whom precise tumor-node-metastasis staging and specific surgical treatments have been established. We will study partial- to hemiglossectomy with primary closure, secondary healing and radial forearm free flap. We will focus on discovering the relationships between reconstruction technique and successful glossectomy speech as mediated by characteristics of tongue motion, kinematic properties of the residual muscles, and configurations of the vocal tract. We have research aims organized to address the following hypotheses. 1(a): Glossectomy speech quality will correlate with tongue motion pattern. 1(b): Tongue motion components that affect speech will vary by surgical type. 2(a): Glossectomy speech quality reflects the success of altered muscle mechanics. 2(b): Muscle mechanics that relate to speech quality will vary by surgical type. 3: A vocal tract model that maps geometry to acoustics can provide targeted speech therapy and can inform surgical decisions. In Aim 3 we will use the knowledge gained in Aims 1 and 2 along with both a 1D and a 3D acoustic model to represent post-glossectomy vocal tracts and then modify them to reflect changes in size and shape of flap or resected region. Results of the project will include a unique atlas of normal tongue deformation and patient comparisons, a better understanding of the impacts of tongue cancer surgery on the tongue and speech, and a tool to help understand speech outcomes of surgical modifications. PUBLIC HEALTH RELEVANCE: Tongue cancer surgery is a life saving procedure, but it typically leaves patients physically and mentally damaged, with speech communication difficulties that are in play every day. This research is an attempt to improve the speech outcome of tongue cancer surgery by minimizing the impact of the tongue reconstruction and understanding the contributions of patient and surgical factors, so patients can communicate more clearly. This will in turn diminish their own feelings of inadequacy, depression, and withdrawal from society.           Tongue cancer surgery is a life saving procedure, but it typically leaves patients physically and mentally damaged, with speech communication difficulties that are in play every day. This research is an attempt to improve the speech outcome of tongue cancer surgery by minimizing the impact of the tongue reconstruction and understanding the contributions of patient and surgical factors, so patients can communicate more clearly. This will in turn diminish their own feelings of inadequacy, depression, and withdrawal from society.",Predictors of Speech Quality after Tongue Cancer Surgery,8254419,R01CA133015,"['Acoustics', 'Address', 'Affect', 'Anatomy', 'Area', 'Articular Range of Motion', 'Atlases', 'Behavior', 'Cancer Patient', 'Cancerous', 'Characteristics', 'Cicatrix', 'Cine Magnetic Resonance Imaging', 'Communication', 'Computers', 'Conscious', 'Consensus', 'Data', 'Diagnostic Neoplasm Staging', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Discipline', 'Distal', 'Excision', 'Exhibits', 'Fatigue', 'Feeling', 'Fiber', 'Forearm', 'Gestures', 'Glossectomy', 'Head Cancer', 'Healed', 'Health', 'Image', 'Incidence', 'Individual', 'Inferior', 'Knowledge', 'Left', 'Life', 'Link', 'Location', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Mechanics', 'Mediating', 'Mental Depression', 'Methods', 'Modeling', 'Modification', 'Morphology', 'Motion', 'Muscle', 'Neck Cancer', 'Neoplasm Metastasis', 'Noise', 'Operative Surgical Procedures', 'Outcome', 'Output', 'Patients', 'Pattern', 'Phase', 'Play', 'Positioning Attribute', 'Postoperative Period', 'Principal Component Analysis', 'Procedures', 'Property', 'Quality of life', 'Radial', 'Research', 'Resected', 'Residual state', 'Resolution', 'Resources', 'Scientist', 'Shapes', 'Site', 'Societies', 'Specialist', 'Speech', 'Speech Acoustics', 'Speech Pathologist', 'Speech Therapy', 'Staging', 'Stretching', 'Structure', 'Surface', 'Surgeon', 'Surgical Flaps', 'Surgical sutures', 'Techniques', 'Testing', 'Time', 'Tissues', 'Tongue', 'Tumor stage', 'United States', 'Withdrawal', 'Work', 'base', 'cancer surgery', 'cell motility', 'constriction', 'disorder control', 'healing', 'image processing', 'imaging modality', 'improved', 'kinematics', 'malignant mouth neoplasm', 'malignant tongue neoplasm', 'mouth squamous cell carcinoma', 'novel', 'programs', 'reconstruction', 'response', 'social', 'sound', 'speech accuracy', 'success', 'tongue root', 'tool', 'tumor', 'wound', 'young man', 'young woman']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2012,495517,0.21637604999249266
"Predictors of Speech Quality after Tongue Cancer Surgery    DESCRIPTION (provided by applicant): Oral, head and neck cancer represents 3% of all cancers in the United States and is the 6th most common cancer worldwide. Recent studies have shown a five-fold increase in the incidence of squamous cell carcinomas of the oral portion of the tongue among young men and a six-fold increase among young women. Glossectomy is the surgical removal of a cancerous tumor. The amount of tongue tissue removed is not negotiable, because the surgeon must remove the entire tumor plus about 1 cm of tissue around it. The reconstructive phase focuses on maximizing the patient's post-operative quality of life, including the critical function of speech. Patients with poor speech avoid social situations, develop self-consciousness, and even depression. Surgeons must choose a reconstruction method, such as primary closure (suture the wound), or replacement with a 'flap' of tissue taken from elsewhere in the body. Larger tumors (above 4 cm in largest dimension) almost always require some type of flap. Among early stage tumors, however, there is no consensus as to the optimal closure procedure. We propose an objective study of patients for whom precise tumor-node-metastasis staging and specific surgical treatments have been established. We will study partial- to hemiglossectomy with primary closure, secondary healing and radial forearm free flap. We will focus on discovering the relationships between reconstruction technique and successful glossectomy speech as mediated by characteristics of tongue motion, kinematic properties of the residual muscles, and configurations of the vocal tract. We have research aims organized to address the following hypotheses. 1(a): Glossectomy speech quality will correlate with tongue motion pattern. 1(b): Tongue motion components that affect speech will vary by surgical type. 2(a): Glossectomy speech quality reflects the success of altered muscle mechanics. 2(b): Muscle mechanics that relate to speech quality will vary by surgical type. 3: A vocal tract model that maps geometry to acoustics can provide targeted speech therapy and can inform surgical decisions. In Aim 3 we will use the knowledge gained in Aims 1 and 2 along with both a 1D and a 3D acoustic model to represent post-glossectomy vocal tracts and then modify them to reflect changes in size and shape of flap or resected region. Results of the project will include a unique atlas of normal tongue deformation and patient comparisons, a better understanding of the impacts of tongue cancer surgery on the tongue and speech, and a tool to help understand speech outcomes of surgical modifications. PUBLIC HEALTH RELEVANCE: Tongue cancer surgery is a life saving procedure, but it typically leaves patients physically and mentally damaged, with speech communication difficulties that are in play every day. This research is an attempt to improve the speech outcome of tongue cancer surgery by minimizing the impact of the tongue reconstruction and understanding the contributions of patient and surgical factors, so patients can communicate more clearly. This will in turn diminish their own feelings of inadequacy, depression, and withdrawal from society.           Tongue cancer surgery is a life saving procedure, but it typically leaves patients physically and mentally damaged, with speech communication difficulties that are in play every day. This research is an attempt to improve the speech outcome of tongue cancer surgery by minimizing the impact of the tongue reconstruction and understanding the contributions of patient and surgical factors, so patients can communicate more clearly. This will in turn diminish their own feelings of inadequacy, depression, and withdrawal from society.",Predictors of Speech Quality after Tongue Cancer Surgery,8067183,R01CA133015,"['Acoustics', 'Address', 'Affect', 'Anatomy', 'Area', 'Articular Range of Motion', 'Atlases', 'Behavior', 'Cancer Patient', 'Cancerous', 'Characteristics', 'Cicatrix', 'Cine Magnetic Resonance Imaging', 'Communication', 'Computers', 'Conscious', 'Consensus', 'Data', 'Diagnostic Neoplasm Staging', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Discipline', 'Distal', 'Excision', 'Exhibits', 'Fatigue', 'Feeling', 'Fiber', 'Forearm', 'Gestures', 'Glossectomy', 'Head Cancer', 'Healed', 'Health', 'Image', 'Incidence', 'Individual', 'Inferior', 'Knowledge', 'Left', 'Life', 'Link', 'Location', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Mechanics', 'Mediating', 'Mental Depression', 'Methods', 'Modeling', 'Modification', 'Morphology', 'Motion', 'Muscle', 'Neck Cancer', 'Neoplasm Metastasis', 'Noise', 'Operative Surgical Procedures', 'Outcome', 'Output', 'Patients', 'Pattern', 'Phase', 'Play', 'Positioning Attribute', 'Postoperative Period', 'Principal Component Analysis', 'Procedures', 'Property', 'Quality of life', 'Radial', 'Research', 'Resected', 'Residual state', 'Resolution', 'Resources', 'Scientist', 'Shapes', 'Site', 'Societies', 'Specialist', 'Speech', 'Speech Acoustics', 'Speech Pathologist', 'Speech Therapy', 'Staging', 'Stretching', 'Structure', 'Surface', 'Surgeon', 'Surgical Flaps', 'Surgical sutures', 'Techniques', 'Testing', 'Time', 'Tissues', 'Tongue', 'Tumor stage', 'United States', 'Withdrawal', 'Woman', 'Work', 'base', 'cancer surgery', 'cell motility', 'constriction', 'disorder control', 'healing', 'image processing', 'imaging modality', 'improved', 'kinematics', 'malignant mouth neoplasm', 'malignant tongue neoplasm', 'men', 'mouth squamous cell carcinoma', 'novel', 'programs', 'reconstruction', 'response', 'social', 'sound', 'speech accuracy', 'success', 'tongue root', 'tool', 'tumor', 'wound']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2011,498480,0.21637604999249266
"Predictors of Speech Quality after Tongue Cancer Surgery    DESCRIPTION (provided by applicant): Oral, head and neck cancer represents 3% of all cancers in the United States and is the 6th most common cancer worldwide. Recent studies have shown a five-fold increase in the incidence of squamous cell carcinomas of the oral portion of the tongue among young men and a six-fold increase among young women. Glossectomy is the surgical removal of a cancerous tumor. The amount of tongue tissue removed is not negotiable, because the surgeon must remove the entire tumor plus about 1 cm of tissue around it. The reconstructive phase focuses on maximizing the patient's post-operative quality of life, including the critical function of speech. Patients with poor speech avoid social situations, develop self-consciousness, and even depression. Surgeons must choose a reconstruction method, such as primary closure (suture the wound), or replacement with a 'flap' of tissue taken from elsewhere in the body. Larger tumors (above 4 cm in largest dimension) almost always require some type of flap. Among early stage tumors, however, there is no consensus as to the optimal closure procedure. We propose an objective study of patients for whom precise tumor-node-metastasis staging and specific surgical treatments have been established. We will study partial- to hemiglossectomy with primary closure, secondary healing and radial forearm free flap. We will focus on discovering the relationships between reconstruction technique and successful glossectomy speech as mediated by characteristics of tongue motion, kinematic properties of the residual muscles, and configurations of the vocal tract. We have research aims organized to address the following hypotheses. 1(a): Glossectomy speech quality will correlate with tongue motion pattern. 1(b): Tongue motion components that affect speech will vary by surgical type. 2(a): Glossectomy speech quality reflects the success of altered muscle mechanics. 2(b): Muscle mechanics that relate to speech quality will vary by surgical type. 3: A vocal tract model that maps geometry to acoustics can provide targeted speech therapy and can inform surgical decisions. In Aim 3 we will use the knowledge gained in Aims 1 and 2 along with both a 1D and a 3D acoustic model to represent post-glossectomy vocal tracts and then modify them to reflect changes in size and shape of flap or resected region. Results of the project will include a unique atlas of normal tongue deformation and patient comparisons, a better understanding of the impacts of tongue cancer surgery on the tongue and speech, and a tool to help understand speech outcomes of surgical modifications. PUBLIC HEALTH RELEVANCE: Tongue cancer surgery is a life saving procedure, but it typically leaves patients physically and mentally damaged, with speech communication difficulties that are in play every day. This research is an attempt to improve the speech outcome of tongue cancer surgery by minimizing the impact of the tongue reconstruction and understanding the contributions of patient and surgical factors, so patients can communicate more clearly. This will in turn diminish their own feelings of inadequacy, depression, and withdrawal from society.           Tongue cancer surgery is a life saving procedure, but it typically leaves patients physically and mentally damaged, with speech communication difficulties that are in play every day. This research is an attempt to improve the speech outcome of tongue cancer surgery by minimizing the impact of the tongue reconstruction and understanding the contributions of patient and surgical factors, so patients can communicate more clearly. This will in turn diminish their own feelings of inadequacy, depression, and withdrawal from society.",Predictors of Speech Quality after Tongue Cancer Surgery,7841779,R01CA133015,"['Acoustics', 'Address', 'Affect', 'Anatomy', 'Area', 'Articular Range of Motion', 'Atlases', 'Behavior', 'Cancer Patient', 'Cancerous', 'Characteristics', 'Cicatrix', 'Cine Magnetic Resonance Imaging', 'Communication', 'Computers', 'Conscious', 'Consensus', 'Data', 'Diagnostic Neoplasm Staging', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Discipline', 'Distal', 'Excision', 'Exhibits', 'Fatigue', 'Feeling', 'Fiber', 'Figs - dietary', 'Forearm', 'Gestures', 'Glossectomy', 'Head and Neck Cancer', 'Healed', 'Image', 'Incidence', 'Individual', 'Inferior', 'Knowledge', 'Left', 'Life', 'Link', 'Location', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Mechanics', 'Mediating', 'Mental Depression', 'Methods', 'Modeling', 'Modification', 'Morphology', 'Motion', 'Muscle', 'Neoplasm Metastasis', 'Noise', 'Operative Surgical Procedures', 'Oral', 'Outcome', 'Output', 'Patients', 'Pattern', 'Phase', 'Play', 'Positioning Attribute', 'Postoperative Period', 'Principal Component Analysis', 'Procedures', 'Property', 'Quality of life', 'Radial', 'Research', 'Resected', 'Residual state', 'Resolution', 'Resources', 'Scientist', 'Shapes', 'Site', 'Societies', 'Specialist', 'Speech', 'Speech Acoustics', 'Speech Pathologist', 'Speech Therapy', 'Staging', 'Stretching', 'Structure', 'Surface', 'Surgeon', 'Surgical Flaps', 'Surgical sutures', 'Techniques', 'Testing', 'Time', 'Tissues', 'Tongue', 'Tumor stage', 'United States', 'Withdrawal', 'Woman', 'Work', 'base', 'cancer surgery', 'cell motility', 'constriction', 'disorder control', 'healing', 'image processing', 'imaging modality', 'improved', 'kinematics', 'malignant mouth neoplasm', 'malignant tongue neoplasm', 'men', 'mouth squamous cell carcinoma', 'novel', 'programs', 'public health relevance', 'reconstruction', 'response', 'social', 'sound', 'speech accuracy', 'success', 'three-dimensional modeling', 'tongue root', 'tool', 'tumor', 'wound']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2010,516286,0.21637604999249266
"Predictors of Speech Quality after Tongue Cancer Surgery    DESCRIPTION (provided by applicant): Oral, head and neck cancer represents 3% of all cancers in the United States and is the 6th most common cancer worldwide. Recent studies have shown a five-fold increase in the incidence of squamous cell carcinomas of the oral portion of the tongue among young men and a six-fold increase among young women. Glossectomy is the surgical removal of a cancerous tumor. The amount of tongue tissue removed is not negotiable, because the surgeon must remove the entire tumor plus about 1 cm of tissue around it. The reconstructive phase focuses on maximizing the patient's post-operative quality of life, including the critical function of speech. Patients with poor speech avoid social situations, develop self-consciousness, and even depression. Surgeons must choose a reconstruction method, such as primary closure (suture the wound), or replacement with a 'flap' of tissue taken from elsewhere in the body. Larger tumors (above 4 cm in largest dimension) almost always require some type of flap. Among early stage tumors, however, there is no consensus as to the optimal closure procedure. We propose an objective study of patients for whom precise tumor-node-metastasis staging and specific surgical treatments have been established. We will study partial- to hemiglossectomy with primary closure, secondary healing and radial forearm free flap. We will focus on discovering the relationships between reconstruction technique and successful glossectomy speech as mediated by characteristics of tongue motion, kinematic properties of the residual muscles, and configurations of the vocal tract. We have research aims organized to address the following hypotheses. 1(a): Glossectomy speech quality will correlate with tongue motion pattern. 1(b): Tongue motion components that affect speech will vary by surgical type. 2(a): Glossectomy speech quality reflects the success of altered muscle mechanics. 2(b): Muscle mechanics that relate to speech quality will vary by surgical type. 3: A vocal tract model that maps geometry to acoustics can provide targeted speech therapy and can inform surgical decisions. In Aim 3 we will use the knowledge gained in Aims 1 and 2 along with both a 1D and a 3D acoustic model to represent post-glossectomy vocal tracts and then modify them to reflect changes in size and shape of flap or resected region. Results of the project will include a unique atlas of normal tongue deformation and patient comparisons, a better understanding of the impacts of tongue cancer surgery on the tongue and speech, and a tool to help understand speech outcomes of surgical modifications.  PUBLIC HEALTH RELEVANCE: Tongue cancer surgery is a life saving procedure, but it typically leaves patients physically and mentally damaged, with speech communication difficulties that are in play every day. This research is an attempt to improve the speech outcome of tongue cancer surgery by minimizing the impact of the tongue reconstruction and understanding the contributions of patient and surgical factors, so patients can communicate more clearly. This will in turn diminish their own feelings of inadequacy, depression, and withdrawal from society.         ",Predictors of Speech Quality after Tongue Cancer Surgery,8467993,R01CA133015,"['Acoustics', 'Address', 'Affect', 'Anatomy', 'Area', 'Articular Range of Motion', 'Atlases', 'Behavior', 'Cancer Patient', 'Cancerous', 'Characteristics', 'Cicatrix', 'Cine Magnetic Resonance Imaging', 'Communication', 'Computers', 'Conscious', 'Consensus', 'Data', 'Diagnostic Neoplasm Staging', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Discipline', 'Distal', 'Excision', 'Exhibits', 'Fatigue', 'Feeling', 'Fiber', 'Forearm', 'Gestures', 'Glossectomy', 'Head Cancer', 'Healed', 'Health', 'Image', 'Incidence', 'Individual', 'Inferior', 'Knowledge', 'Left', 'Life', 'Link', 'Location', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Mechanics', 'Mediating', 'Mental Depression', 'Methods', 'Modeling', 'Modification', 'Morphology', 'Motion', 'Muscle', 'Neck Cancer', 'Neoplasm Metastasis', 'Noise', 'Operative Surgical Procedures', 'Outcome', 'Output', 'Patients', 'Pattern', 'Phase', 'Play', 'Positioning Attribute', 'Postoperative Period', 'Principal Component Analysis', 'Procedures', 'Property', 'Quality of life', 'Radial', 'Research', 'Resected', 'Residual state', 'Resolution', 'Resources', 'Scientist', 'Shapes', 'Site', 'Societies', 'Specialist', 'Speech', 'Speech Acoustics', 'Speech Pathologist', 'Speech Therapy', 'Staging', 'Stretching', 'Structure', 'Surface', 'Surgeon', 'Surgical Flaps', 'Surgical sutures', 'Techniques', 'Testing', 'Time', 'Tissues', 'Tongue', 'Tumor stage', 'United States', 'Withdrawal', 'Work', 'base', 'cancer surgery', 'cell motility', 'constriction', 'disorder control', 'healing', 'image processing', 'imaging modality', 'improved', 'kinematics', 'malignant mouth neoplasm', 'malignant tongue neoplasm', 'mouth squamous cell carcinoma', 'novel', 'programs', 'reconstruction', 'response', 'social', 'sound', 'speech accuracy', 'success', 'tongue root', 'tool', 'tumor', 'wound', 'young man', 'young woman']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2013,462567,0.23088168734157108
"Predictors of Speech Quality after Tongue Cancer Surgery    DESCRIPTION (provided by applicant): Oral, head and neck cancer represents 3% of all cancers in the United States and is the 6th most common cancer worldwide. Recent studies have shown a five-fold increase in the incidence of squamous cell carcinomas of the oral portion of the tongue among young men and a six-fold increase among young women. Glossectomy is the surgical removal of a cancerous tumor. The amount of tongue tissue removed is not negotiable, because the surgeon must remove the entire tumor plus about 1 cm of tissue around it. The reconstructive phase focuses on maximizing the patient's post-operative quality of life, including the critical function of speech. Patients with poor speech avoid social situations, develop self-consciousness, and even depression. Surgeons must choose a reconstruction method, such as primary closure (suture the wound), or replacement with a 'flap' of tissue taken from elsewhere in the body. Larger tumors (above 4 cm in largest dimension) almost always require some type of flap. Among early stage tumors, however, there is no consensus as to the optimal closure procedure. We propose an objective study of patients for whom precise tumor-node-metastasis staging and specific surgical treatments have been established. We will study partial- to hemiglossectomy with primary closure, secondary healing and radial forearm free flap. We will focus on discovering the relationships between reconstruction technique and successful glossectomy speech as mediated by characteristics of tongue motion, kinematic properties of the residual muscles, and configurations of the vocal tract. We have research aims organized to address the following hypotheses. 1(a): Glossectomy speech quality will correlate with tongue motion pattern. 1(b): Tongue motion components that affect speech will vary by surgical type. 2(a): Glossectomy speech quality reflects the success of altered muscle mechanics. 2(b): Muscle mechanics that relate to speech quality will vary by surgical type. 3: A vocal tract model that maps geometry to acoustics can provide targeted speech therapy and can inform surgical decisions. In Aim 3 we will use the knowledge gained in Aims 1 and 2 along with both a 1D and a 3D acoustic model to represent post-glossectomy vocal tracts and then modify them to reflect changes in size and shape of flap or resected region. Results of the project will include a unique atlas of normal tongue deformation and patient comparisons, a better understanding of the impacts of tongue cancer surgery on the tongue and speech, and a tool to help understand speech outcomes of surgical modifications. PUBLIC HEALTH RELEVANCE: Tongue cancer surgery is a life saving procedure, but it typically leaves patients physically and mentally damaged, with speech communication difficulties that are in play every day. This research is an attempt to improve the speech outcome of tongue cancer surgery by minimizing the impact of the tongue reconstruction and understanding the contributions of patient and surgical factors, so patients can communicate more clearly. This will in turn diminish their own feelings of inadequacy, depression, and withdrawal from society.           Tongue cancer surgery is a life saving procedure, but it typically leaves patients physically and mentally damaged, with speech communication difficulties that are in play every day. This research is an attempt to improve the speech outcome of tongue cancer surgery by minimizing the impact of the tongue reconstruction and understanding the contributions of patient and surgical factors, so patients can communicate more clearly. This will in turn diminish their own feelings of inadequacy, depression, and withdrawal from society.",Predictors of Speech Quality after Tongue Cancer Surgery,7731742,R01CA133015,"['Acoustics', 'Address', 'Affect', 'Anatomy', 'Area', 'Articular Range of Motion', 'Atlases', 'Behavior', 'Cancer Patient', 'Cancerous', 'Characteristics', 'Cicatrix', 'Cine Magnetic Resonance Imaging', 'Communication', 'Computers', 'Conscious', 'Consensus', 'Data', 'Diagnostic Neoplasm Staging', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Discipline', 'Distal', 'Excision', 'Exhibits', 'Fatigue', 'Feeling', 'Fiber', 'Figs - dietary', 'Forearm', 'Gestures', 'Glossectomy', 'Head and Neck Cancer', 'Healed', 'Image', 'Incidence', 'Individual', 'Inferior', 'Knowledge', 'Left', 'Life', 'Link', 'Location', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Mechanics', 'Mediating', 'Methods', 'Modeling', 'Modification', 'Morphology', 'Motion', 'Muscle', 'Neoplasm Metastasis', 'Noise', 'Operative Surgical Procedures', 'Oral', 'Outcome', 'Output', 'Patients', 'Pattern', 'Phase', 'Play', 'Positioning Attribute', 'Postoperative Period', 'Principal Component Analysis', 'Procedures', 'Property', 'Quality of life', 'Radial', 'Research', 'Resected', 'Residual state', 'Resolution', 'Resources', 'Scientist', 'Shapes', 'Site', 'Societies', 'Specialist', 'Speech', 'Speech Acoustics', 'Speech Pathologist', 'Speech Therapy', 'Staging', 'Stretching', 'Structure', 'Surface', 'Surgeon', 'Surgical Flaps', 'Surgical sutures', 'Techniques', 'Testing', 'Time', 'Tissues', 'Tongue', 'Tumor stage', 'United States', 'Withdrawal', 'Woman', 'Work', 'base', 'cancer surgery', 'cell motility', 'constriction', 'depression', 'disorder control', 'healing', 'image processing', 'imaging modality', 'improved', 'kinematics', 'malignant mouth neoplasm', 'malignant tongue neoplasm', 'men', 'mouth squamous cell carcinoma', 'novel', 'programs', 'public health relevance', 'reconstruction', 'response', 'social', 'sound', 'speech accuracy', 'success', 'three-dimensional modeling', 'tongue root', 'tool', 'tumor', 'wound']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2009,546577,0.21637604999249266
"Acoustic speech analysis of patients with decompensated heart failure Abstract Heart failure (HF) is the most common cause of hospitalization among Americans over 65 years of age. A major goal of HF management is to maintain stability by predicting and preventing episodes of acute decompensated HF (ADHF). Body weight is currently used for non-invasive, at-home monitoring of volume status, and as an early warning for decompensation, by patients with HF. However, HF-related weight changes occur relatively close to the onset of symptoms and may not be detected in time to prevent an episode of decompensation. In contrast, the vocal folds consist of thin tissue layers that are impacted by body hydration levels and may therefore be especially sensitive to HF-related fluid retention. The amount of laryngeal edema required to change voice acoustics is expected to be small relative to the large amount of systemic edema needed to significantly increase body weight. Therefore, we hypothesize that clinicians will be able to detect and track HF-related volume overload more closely by monitoring voice metrics in addition to weight. The goal of this project is to investigate the potential of voice and speech characteristics as correlates of clinical improvement during treatment for ADHF. In a pilot study, acoustic voice and speech measures from ten HF patients undergoing diuresis for HF were analyzed. Several promising voice measures were identified for additional investigation, including measures reflecting higher pitch and increased vocal stability following successful ADHF treatment. In this project, the study size will be expanded to facilitate more rigorous and extensive statistical analysis techniques, including machine learning and multi-level modeling analyses. Additionally, the potential of a neck-surface vibration sensor for noise-robust, confidential monitoring of ADHF status will be evaluated. The vibration sensor has been used extensively for voice monitoring in our lab because it is minimally affected by environmental noise and does not record intelligible speech, making it ideal for recording voice while preserving patient privacy in noisy environments. The performance of microphone-based recordings in predicting patients heart failure status will be compared with that of vibration-sensor-based recordings, with the hypothesis that the vibration sensor will work as well as, or better than, a conventional audio microphone. Completion of this project will result in a novel, voice-based method using wearable sensor technology to provide a monitoring system for HF patients recovering from decompensation, and will aid us in future work as we move towards developing an early warning system for patients at risk of ADHF. Project Narrative The proposed project will investigate how voice and speech production change in response to treatment of acute decompensated heart failure (ADHF), which is characterized by an abnormal accumulation of fluid throughout the body. Excess fluid in the vocal folds and lungs is hypothesized to cause a number of changes in voice and speech, which can be detected with acoustic analysis of recordings from both a conventional microphone and a novel neck-skin vibration sensor. This research may provide clinicians and patients with a method of detecting impending ADHF earlier and more easily than is currently possible, which could save lives, reduce healthcare costs, and improve patients quality of life.",Acoustic speech analysis of patients with decompensated heart failure,9744304,F31HL143824,"['Acoustics', 'Acute', 'Admission activity', 'Affect', 'Age-Years', 'American', 'Biological Markers', 'Blood', 'Body Weight', 'Body Weight Changes', 'Characteristics', 'Clinical', 'Clinical Management', 'Data', 'Diagnosis', 'Diuresis', 'EFRAC', 'Edema', 'Enrollment', 'Environment', 'Exhibits', 'Future', 'Goals', 'Gold', 'Health Care Costs', 'Heart', 'Heart failure', 'Home environment', 'Hospitalization', 'Hour', 'Human', 'Hydration status', 'Investigation', 'Laboratories', 'Laryngeal Edema', 'Life', 'Liquid substance', 'Lung', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Monitor', 'Neck', 'Noise', 'Patient Monitoring System', 'Patients', 'Performance', 'Pilot Projects', 'Production', 'Protocols documentation', 'Publishing', 'Pump', 'Quality of life', 'Records', 'Research', 'Respiration', 'Risk', 'Signal Transduction', 'Skin', 'Speech', 'Speech Acoustics', 'Speech Intelligibility', 'Statistical Data Interpretation', 'Surface', 'Symptoms', 'System', 'Techniques', 'Thinness', 'Time', 'Tissues', 'Vascular blood supply', 'Voice', 'Weight', 'Weight Gain', 'Work', 'base', 'cohort', 'experience', 'improved', 'microphone', 'mortality', 'multilevel analysis', 'novel', 'patient privacy', 'preservation', 'pressure', 'prevent', 'respiratory', 'response', 'sensor', 'treatment response', 'vibration', 'vocal cord', 'wearable sensor technology']",NHLBI,HARVARD MEDICAL SCHOOL,F31,2019,32658,0.11010386926879819

text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"SIMULATION OF MENTAL PROCESSES IN LIGHT OF BRAIN This proposal presents a set of principles of human information processing  that have been developed by the investigator and describes a program of  collaborative research designed to assess the adequacy of these principles  for modeling cognitive processes in normal and disordered individuals.  In  brief, the principles state the human cognition occurs through a graded,  stochastic, adaptive, interactive and distributed process.  The ultimate  goal is the further development and understanding of the set of principles  and a further exploration of the implications of the principles for normal  and disordered cognition.  The principles serve as the focus of specific  research projects on the following topics:  general laws and regularities  of information processing; the role of context in visual information  processing; the mechanisms of attention and neuromodulatory deficits of  attention; and the mechanisms of attention and neuromodulatory deficits of  attention; and the mechanisms of accessing representations of the sounds  and meanings of printed words.  Each project involves the comparison of  the results of computer simulation models based on the principles to the  results of psychological experiments designed to produce evidence relevant  to the assessment of the adequacy of the principles.  The proposal also  describes plans for the further incorporation of principles emerging from  neuroscience and for the dissemination of concepts and simulation tools  that are crucial to facilitate exploration of the principles by other  researchers.  n/a",SIMULATION OF MENTAL PROCESSES IN LIGHT OF BRAIN,2239797,K05MH000385,"['artificial intelligence', ' cognition', ' computational neuroscience', ' human subject', ' learning', ' neural information processing', ' psychological models', ' schizophrenia']",NIMH,CARNEGIE-MELLON UNIVERSITY,K05,1995,101370,-0.03516805097846879
"SIMULATION OF MENTAL PROCESSES IN LIGHT OF BRAIN This proposal presents a set of principles of human information processing that have been developed by the investigator and describes a program of collaborative research designed to assess the adequacy of these principles for modeling cognitive processes in normal and disordered individuals.  In brief, the principles state the human cognition occurs through a graded, stochastic, adaptive, interactive and distributed process.  The ultimate goal is the further development and understanding of the set of principles and a further exploration of the implications of the principles for normal and disordered cognition.  The principles serve as the focus of specific research projects on the following topics:  general laws and regularities of information processing; the role of context in visual information processing; the mechanisms of attention and neuromodulatory deficits of attention; and the mechanisms of attention and neuromodulatory deficits of attention; and the mechanisms of accessing representations of the sounds and meanings of printed words.  Each project involves the comparison of the results of computer simulation models based on the principles to the results of psychological experiments designed to produce evidence relevant to the assessment of the adequacy of the principles.  The proposal also describes plans for the further incorporation of principles emerging from neuroscience and for the dissemination of concepts and simulation tools that are crucial to facilitate exploration of the principles by other researchers.  n/a",SIMULATION OF MENTAL PROCESSES IN LIGHT OF BRAIN,2239796,K05MH000385,"['artificial intelligence', ' cognition', ' computational neuroscience', ' human subject', ' learning', ' neural information processing', ' psychological models', ' schizophrenia']",NIMH,CARNEGIE-MELLON UNIVERSITY,K05,1994,101626,-0.03516805097846879
"SIMULATION OF MENTAL PROCESSES IN LIGHT OF THE BRAIN This proposal presents a set of principles of human information processing that have been developed by the investigator and describes a program of collaborative research designed to assess the adequacy of these principles for modeling cognitive processes in normal and disordered individuals.  In brief, the principles state the human cognition occurs through a graded, stochastic, adaptive, interactive and distributed process.  The ultimate goal is the further development and understanding of the set of principles and a further exploration of the implications of the principles for normal and disordered cognition.  The principles serve as the focus of specific research projects on the following topics:  general laws and regularities of information processing; the role of context in visual information processing; the mechanisms of attention and neuromodulatory deficits of attention; and the mechanisms of attention and neuromodulatory deficits of attention; and the mechanisms of accessing representations of the sounds and meanings of printed words.  Each project involves the comparison of the results of computer simulation models based on the principles to the results of psychological experiments designed to produce evidence relevant to the assessment of the adequacy of the principles.  The proposal also describes plans for the further incorporation of principles emerging from neuroscience and for the dissemination of concepts and simulation tools that are crucial to facilitate exploration of the principles by other researchers.  n/a",SIMULATION OF MENTAL PROCESSES IN LIGHT OF THE BRAIN,3075758,K05MH000385,"['artificial intelligence', ' cognition', ' computational neuroscience', ' human subject', ' learning', ' neural information processing', ' psychological models', ' schizophrenia']",NIMH,CARNEGIE-MELLON UNIVERSITY,K05,1993,98848,-0.03516805097846879
"SIMULATION OF MENTAL PROCESSES IN LIGHT OF THE BRAIN I propose to continue my explorations of parallel distributed processing models of cognitive processes.  These models are based on the idea that information processing activity grows out of the interactions of large numbers of simple processing units, and that learning is based on the adjustment of the strengths of the connections among the units.  The work is aimed primarily at modeling psychological phenomena in a computationally adequate way, though it draws its inspiration from and is ultimately applicable to our developing understanding of how information processing actually takes place in the brain.  Two main areas of research are described, one in the area of language processing and one in the area of network learning mechanisms.  In the area of language processing, I plan to carry out a series of studies on sentence comprehension designed to test and elaborate three basic principles of language understanding, and to develop a simulation model based on these principles.  In the area of network learning mechanisms, I describe a new network learning algorithm, and describe plans to try to bring the further development of this and related algorithms together with neurophysiological investigations of a) adaptation to changes in environmental input and b) of the laws that govern synaptic change as studied through experiments on long-term potentiation.  n/a",SIMULATION OF MENTAL PROCESSES IN LIGHT OF THE BRAIN,3069747,K02MH000385,"['artificial intelligence', ' cognition', ' computer simulation', ' learning', ' long term potentiation', ' neural information processing', ' neuroanatomy', ' neurophysiology', ' perception', ' psychological models', ' reading disorder']",NIMH,CARNEGIE-MELLON UNIVERSITY,K02,1991,100897,-0.03313198821903901
"SIMULATION OF MENTAL PROCESSES IN LIGHT OF THE BRAIN I propose to continue my explorations of parallel distributed processing models of cognitive processes.  These models are based on the idea that information processing activity grows out of the interactions of large numbers of simple processing units, and that learning is based on the adjustment of the strengths of the connections among the units.  The work is aimed primarily at modeling psychological phenomena in a computationally adequate way, though it draws its inspiration from and is ultimately applicable to our developing understanding of how information processing actually takes place in the brain.  Two main areas of research are described, one in the area of language processing and one in the area of network learning mechanisms.  In the area of language processing, I plan to carry out a series of studies on sentence comprehension designed to test and elaborate three basic principles of language understanding, and to develop a simulation model based on these principles.  In the area of network learning mechanisms, I describe a new network learning algorithm, and describe plans to try to bring the further development of this and related algorithms together with neurophysiological investigations of a) adaptation to changes in environmental input and b) of the laws that govern synaptic change as studied through experiments on long-term potentiation.  n/a",SIMULATION OF MENTAL PROCESSES IN LIGHT OF THE BRAIN,3069746,K02MH000385,"['artificial intelligence', ' cognition', ' computer simulation', ' learning', ' neural information processing', ' neuroanatomy', ' neurophysiology', ' perception', ' psychological models', ' reading disorder']",NIMH,CARNEGIE-MELLON UNIVERSITY,K02,1990,99733,-0.03313198821903901
"SIMULATION OF MENTAL PROCESSES IN LIGHT OF THE BRAIN I propose to continue my explorations of parallel distributed processing models of cognitive processes.  These models are based on the idea that information processing activity grows out of the interactions of large numbers of simple processing units, and that learning is based on the adjustment of the strengths of the connections among the units.  The work is aimed primarily at modeling psychological phenomena in a computationally adequate way, though it draws its inspiration from and is ultimately applicable to our developing understanding of how information processing actually takes place in the brain.  Two main areas of research are described, one in the area of language processing and one in the area of network learning mechanisms.  In the area of language processing, I plan to carry out a series of studies on sentence comprehension designed to test and elaborate three basic principles of language understanding, and to develop a simulation model based on these principles.  In the area of network learning mechanisms, I describe a new network learning algorithm, and describe plans to try to bring the further development of this and related algorithms together with neurophysiological investigations of a) adaptation to changes in environmental input and b) of the laws that govern synaptic change as studied through experiments on long-term potentiation.  n/a",SIMULATION OF MENTAL PROCESSES IN LIGHT OF THE BRAIN,3069745,K02MH000385,"['artificial intelligence', ' cognition', ' computer simulation', ' learning', ' neural information processing', ' neuroanatomy', ' neurophysiology', ' perception', ' psychological models', ' reading disorder']",NIMH,CARNEGIE-MELLON UNIVERSITY,K02,1989,60264,-0.03313198821903901
"SIMULATION OF MENTAL PROCESSES IN LIGHT OF THE BRAIN I propose to continue my explorations of parallel distributed processing models of cognitive processes.  These models are based on the idea that information processing activity grows out of the interactions of large numbers of simple processing units, and that learning is based on the adjustment of the strengths of the connections among the units.  The work is aimed primarily at modeling psychological phenomena in a computationally adequate way, though it draws its inspiration from and is ultimately applicable to our developing understanding of how information processing actually takes place in the brain.  Two main areas of research are described, one in the area of language processing and one in the area of network learning mechanisms.  In the area of language processing, I plan to carry out a series of studies on sentence comprehension designed to test and elaborate three basic principles of language understanding, and to develop a simulation model based on these principles.  In the area of network learning mechanisms, I describe a new network learning algorithm, and describe plans to try to bring the further development of this and related algorithms together with neurophysiological investigations of a) adaptation to changes in environmental input and b) of the laws that govern synaptic change as studied through experiments on long-term potentiation.  n/a",SIMULATION OF MENTAL PROCESSES IN LIGHT OF THE BRAIN,3069744,K02MH000385,"['artificial intelligence', ' cognition', ' computer simulation', ' learning', ' neural information processing', ' neuroanatomy', ' neurophysiology', ' perception', ' psychological models', ' reading disorder']",NIMH,CARNEGIE-MELLON UNIVERSITY,K02,1988,60264,-0.03313198821903901
"SIMULATION OF MENTAL PROCESSIS IN LIGHT OF THE BRAIN I propose to continue my explorations of parallel distributed processing models of cognitive processes.  These models are based on the idea that information processing activity grows out of the interactions of large numbers of simple processing units, and that learning is based on the adjustment of the strengths of the connections among the units.  The work is aimed primarily at modeling psychological phenomena in a computationally adequate way, though it draws its inspiration from and is ultimately applicable to our developing understanding of how information processing actually takes place in the brain.  Two main areas of research are described, one in the area of language processing and one in the area of network learning mechanisms.  In the area of language processing, I plan to carry out a series of studies on sentence comprehension designed to test and elaborate three basic principles of language understanding, and to develop a simulation model based on these principles.  In the area of network learning mechanisms, I describe a new network learning algorithm, and describe plans to try to bring the further development of this and related algorithms together with neurophysiological investigations of a) adaptation to changes in environmental input and b) of the laws that govern synaptic change as studied through experiments on long-term potentiation.  n/a",SIMULATION OF MENTAL PROCESSIS IN LIGHT OF THE BRAIN,3069743,K02MH000385,"['artificial intelligence', ' cognition', ' computer simulation', ' learning', ' neural information processing', ' neuroanatomy', ' neurophysiology', ' perception', ' psychological models', ' reading disorder']",NIMH,CARNEGIE-MELLON UNIVERSITY,K02,1987,60264,-0.03313198821903901
"Spectromicroscopy for biochemical analysis of sperm   Description (provided by applicant): We propose to develop the capabilities of       x-ray spectromicroscopy to allow it to be applied to biochemical imaging of          normal and abnormal sperm. This will be done by focusing ""soft"" X rays (in this      case, x rays with a photon energy of 200-800 eV) to the smallest far-field           focus of electromagnetic radiation of any wavelength, and scanning a dry or          frozen hydrated specimen through that focal spot to form an image. Images at a       series of photon energies will then be combined to deliver                           chemical-state-sensitive x-ray absorption spectra from an entire sperm at            better than 50 nm spatial resolution. From this data, major biochemical              constituents in individual sperm will be determined by fitting to reference          spectra of isolated compounds; furthermore, expected and possibly unexpected         spatial correlations between components will be studied using principal              component analysis methods. This work will be carried out using x-ray                microscopes developed by our group at Stony Brook which operate at a soft x-ray      undulator beamline at the National Synchrotron Light Source at Brookhaven            National Laboratory. The microscopes will be improved by the addition of better      order sorting optics for quatitative spectroscopy, and by equipping a cryo           microscope with a more accurate piezo stage and a higher efficiency detector         for studies of frozen hydrated specimens without excessive radiation damage.         These instrumentation developments will be guided by our goal of studying the        correlation between morphological and biochemical variations in sperm, in order      to better understand the causes of male infertility. By obtaining x-ray              spectromicroscopic data on different sperm morphologies in infertile males, we       hope to guide the in vitro fertilization (IVP) clinician in the choice of sperm      use for intracytoplasmic sperm injection (ICSI) so as to improve the success         rate of the procedure. Spectromicroscopic data anaysis software will be made         available for free downloading by other researchers as it is developed.                                                                                                   n/a",Spectromicroscopy for biochemical analysis of sperm,6732037,R01EB000479,"['X ray microscopy', 'X ray spectrometry', 'biochemistry', 'bioimaging /biomedical imaging', 'computer program /software', 'computer system design /evaluation', 'cryoscopy', 'fertility', 'human tissue', 'male', 'morphometry', 'sperm', 'technology /technique development']",NIBIB,STATE UNIVERSITY NEW YORK STONY BROOK,R01,2004,241938,-0.00999457245611705
"Spectromicroscopy for biochemical analysis of sperm   Description (provided by applicant): We propose to develop the capabilities of       x-ray spectromicroscopy to allow it to be applied to biochemical imaging of          normal and abnormal sperm. This will be done by focusing ""soft"" X rays (in this      case, x rays with a photon energy of 200-800 eV) to the smallest far-field           focus of electromagnetic radiation of any wavelength, and scanning a dry or          frozen hydrated specimen through that focal spot to form an image. Images at a       series of photon energies will then be combined to deliver                           chemical-state-sensitive x-ray absorption spectra from an entire sperm at            better than 50 nm spatial resolution. From this data, major biochemical              constituents in individual sperm will be determined by fitting to reference          spectra of isolated compounds; furthermore, expected and possibly unexpected         spatial correlations between components will be studied using principal              component analysis methods. This work will be carried out using x-ray                microscopes developed by our group at Stony Brook which operate at a soft x-ray      undulator beamline at the National Synchrotron Light Source at Brookhaven            National Laboratory. The microscopes will be improved by the addition of better      order sorting optics for quatitative spectroscopy, and by equipping a cryo           microscope with a more accurate piezo stage and a higher efficiency detector         for studies of frozen hydrated specimens without excessive radiation damage.         These instrumentation developments will be guided by our goal of studying the        correlation between morphological and biochemical variations in sperm, in order      to better understand the causes of male infertility. By obtaining x-ray              spectromicroscopic data on different sperm morphologies in infertile males, we       hope to guide the in vitro fertilization (IVP) clinician in the choice of sperm      use for intracytoplasmic sperm injection (ICSI) so as to improve the success         rate of the procedure. Spectromicroscopic data anaysis software will be made         available for free downloading by other researchers as it is developed.                                                                                                   n/a",Spectromicroscopy for biochemical analysis of sperm,6624086,R01EB000479,"['X ray microscopy', ' X ray spectrometry', ' biochemistry', ' bioimaging /biomedical imaging', ' computer program /software', ' computer system design /evaluation', ' cryoscopy', ' fertility', ' human tissue', ' male', ' morphometry', ' sperm', ' technology /technique development']",NIBIB,STATE UNIVERSITY NEW YORK STONY BROOK,R01,2003,241938,-0.00999457245611705
"Spectromicroscopy for biochemical analysis of sperm   Description (provided by applicant): We propose to develop the capabilities of       x-ray spectromicroscopy to allow it to be applied to biochemical imaging of          normal and abnormal sperm. This will be done by focusing ""soft"" X rays (in this      case, x rays with a photon energy of 200-800 eV) to the smallest far-field           focus of electromagnetic radiation of any wavelength, and scanning a dry or          frozen hydrated specimen through that focal spot to form an image. Images at a       series of photon energies will then be combined to deliver                           chemical-state-sensitive x-ray absorption spectra from an entire sperm at            better than 50 nm spatial resolution. From this data, major biochemical              constituents in individual sperm will be determined by fitting to reference          spectra of isolated compounds; furthermore, expected and possibly unexpected         spatial correlations between components will be studied using principal              component analysis methods. This work will be carried out using x-ray                microscopes developed by our group at Stony Brook which operate at a soft x-ray      undulator beamline at the National Synchrotron Light Source at Brookhaven            National Laboratory. The microscopes will be improved by the addition of better      order sorting optics for quatitative spectroscopy, and by equipping a cryo           microscope with a more accurate piezo stage and a higher efficiency detector         for studies of frozen hydrated specimens without excessive radiation damage.         These instrumentation developments will be guided by our goal of studying the        correlation between morphological and biochemical variations in sperm, in order      to better understand the causes of male infertility. By obtaining x-ray              spectromicroscopic data on different sperm morphologies in infertile males, we       hope to guide the in vitro fertilization (IVP) clinician in the choice of sperm      use for intracytoplasmic sperm injection (ICSI) so as to improve the success         rate of the procedure. Spectromicroscopic data anaysis software will be made         available for free downloading by other researchers as it is developed.                                                                                                   n/a",Spectromicroscopy for biochemical analysis of sperm,6472301,R01EB000479,"['X ray microscopy', ' X ray spectrometry', ' biochemistry', ' bioimaging /biomedical imaging', ' computer program /software', ' computer system design /evaluation', ' cryoscopy', ' fertility', ' human tissue', ' male', ' morphometry', ' sperm', ' technology /technique development']",NIBIB,STATE UNIVERSITY NEW YORK STONY BROOK,R01,2002,241938,-0.00999457245611705
"MicroSeer, Analysis software for microscopy imagery  DESCRIPTION (provided by applicant): This project will create new pattern recognition software to improve the analysis and interpretation of in vivo  biomedical imagery. Currently, researchers can get remarkably detailed images of living cells and their  constituent proteins using molecular genetic and microscopy-based approaches in conjunction with  sophisticated microscopy hardware. Available image analysis techniques and software, however, lag behind  the power of this new imaging equipment to visualize the microscopic world.  This phase I SBIR project will apply existing technology in spatial analysis of satellite image data to microscopy data, create new statistical techniques specific to the study of spatial association of proteins in  cells, and create software that implements these statistics for use in the analysis of spatial association timeslice in vivo biomedical imagery.   n/a","MicroSeer, Analysis software for microscopy imagery",6581125,R43EB000575,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer data analysis', ' computer graphics /printing', ' computer program /software', ' computer system design /evaluation', ' image processing', ' statistics /biometry', ' structural biology']",NIBIB,BIOMEDWARE,R43,2003,177261,0.02082661832689914
"Vector Quantization for Image Pattern Recognition    DESCRIPTION (provided by applicant):    This Phase-I SBIR application addresses the increasingly significant challenges faced by pathologists and clinicians in manually inspecting microscope slides. Microscopic inspection suffers from being labor-intensive, subjective, expensive and limited by the need for physical access to the glass slide specimen of interest. The obstacle to automated microscopic inspection has been the inability to efficiently digitize entire microscope specimens at high resolutions. Aperio has developed the ScanScope (R), a novel microscope slide scanner that makes it practical - for the first time - to rapidly create virtual microscope slides at high resolutions. Virtual slides set the stage for automating microscopic inspection using automated pattern recognition. This research aims to adapt and optimize Aperio's existing and novel algorithms for vector quantization (VQ) to the problem of automatic pattern recognition in virtual slides. VQ is a general mathematical technique for encoding bitstreams using a vocabulary. The primary aim is to demonstrate the feasibility of using VQ for pattern recognition in a practical and well-characterized application: automatically finding virtually all micrometastasis clusters in cytology specimens. This proposed research represents a first attempt to automate pattern recognition in virtual slides using VQ.         n/a",Vector Quantization for Image Pattern Recognition,6695147,R43EB001617,"['artificial intelligence', ' automated data processing', ' bioimaging /biomedical imaging', ' cell line', ' computer system design /evaluation', ' cytology', ' digital imaging', ' high throughput technology', ' metastasis', ' microscopy', ' nomenclature']",NIBIB,"APERIO TECHNOLOGIES, INC.",R43,2003,97269,-0.010469855749125867
"AUTORADIOGRAPHIC IMAGE PROCESSING CENTER We propose a center dedicated to research and development of computerized image analysis technology for us with samples produced by modern neuroanatomical techniques.  Included among these techniques are receptor and radiolabeled tracer autoradiography which reveal detailed functional and distributional information about neural activity and mechanisms.  Center research falls into two broad categories:  parcellation, or rapid and reproducible identification of the boundaries of subareas in histological images, and quantification, or the measurement of the relative or absolute values of underlying substance values from optical images of these regions.  The parcellation project utilizes expert system technology for knowledge based image analysis.  Quantification projects include the development of a computerized system for the determination of local thickness of variation, of computer techniques for the detection of local variations within brain regions, and for the quantification of autoradiograms.  The center will operate service facilities that will supply image analysis software for microcomputer image analysis systems used in neuroscience, and will also provide access to image analysis hardware for researchers in the Northeast Corridor and throughout the country.  The image analysis hardware and software assists in the acquisition of accurate and precise information from CNS samples, focusing on the detection, quantification, and mapping of the experimental effects seen in autoradiographic and other images and in the accurate collation of different images (e.g., autoradiographic and histological images).  The center will also disseminate information about computerized image analysis methods for neuroscience by training researchers, publishing papers, and offering seminars and short courses.  n/a",AUTORADIOGRAPHIC IMAGE PROCESSING CENTER,3103953,P41RR001638,"['autoradiography', ' biomedical equipment resource', ' brain mapping', ' image processing', ' neuroanatomy']",NCRR,DREXEL UNIVERSITY,P41,1993,2015,-0.019920391918046492
"AUTORADIOGRAPHIC IMAGE PROCESSING CENTER We propose a center dedicated to research and development of computerized image analysis technology for us with samples produced by modern neuroanatomical techniques.  Included among these techniques are receptor and radiolabeled tracer autoradiography which reveal detailed functional and distributional information about neural activity and mechanisms.  Center research falls into two broad categories:  parcellation, or rapid and reproducible identification of the boundaries of subareas in histological images, and quantification, or the measurement of the relative or absolute values of underlying substance values from optical images of these regions.  The parcellation project utilizes expert system technology for knowledge based image analysis.  Quantification projects include the development of a computerized system for the determination of local thickness of variation, of computer techniques for the detection of local variations within brain regions, and for the quantification of autoradiograms.  The center will operate service facilities that will supply image analysis software for microcomputer image analysis systems used in neuroscience, and will also provide access to image analysis hardware for researchers in the Northeast Corridor and throughout the country.  The image analysis hardware and software assists in the acquisition of accurate and precise information from CNS samples, focusing on the detection, quantification, and mapping of the experimental effects seen in autoradiographic and other images and in the accurate collation of different images (e.g., autoradiographic and histological images).  The center will also disseminate information about computerized image analysis methods for neuroscience by training researchers, publishing papers, and offering seminars and short courses.  n/a",AUTORADIOGRAPHIC IMAGE PROCESSING CENTER,3103958,P41RR001638,"['autoradiography', ' biomedical equipment resource', ' brain mapping', ' image processing', ' neuroanatomy']",NCRR,DREXEL UNIVERSITY,P41,1992,485022,-0.019920391918046492
"AUTORADIOGRAPHIC IMAGE PROCESSING CENTER We propose a center dedicated to research and development of computerized image analysis technology for us with samples produced by modern neuroanatomical techniques.  Included among these techniques are receptor and radiolabeled tracer autoradiography which reveal detailed functional and distributional information about neural activity and mechanisms.  Center research falls into two broad categories:  parcellation, or rapid and reproducible identification of the boundaries of subareas in histological images, and quantification, or the measurement of the relative or absolute values of underlying substance values from optical images of these regions.  The parcellation project utilizes expert system technology for knowledge based image analysis.  Quantification projects include the development of a computerized system for the determination of local thickness of variation, of computer techniques for the detection of local variations within brain regions, and for the quantification of autoradiograms.  The center will operate service facilities that will supply image analysis software for microcomputer image analysis systems used in neuroscience, and will also provide access to image analysis hardware for researchers in the Northeast Corridor and throughout the country.  The image analysis hardware and software assists in the acquisition of accurate and precise information from CNS samples, focusing on the detection, quantification, and mapping of the experimental effects seen in autoradiographic and other images and in the accurate collation of different images (e.g., autoradiographic and histological images).  The center will also disseminate information about computerized image analysis methods for neuroscience by training researchers, publishing papers, and offering seminars and short courses.  n/a",AUTORADIOGRAPHIC IMAGE PROCESSING CENTER,3103957,P41RR001638,"['autoradiography', ' biomedical equipment resource', ' brain mapping', ' image processing', ' neuroanatomy']",NCRR,DREXEL UNIVERSITY,P41,1991,459979,-0.019920391918046492
"AUTORADIOGRAPHIC IMAGE PROCESSING CENTER We propose a center dedicated to research and development of computerized image analysis technology for us with samples produced by modern neuroanatomical techniques.  Included among these techniques are receptor and radiolabeled tracer autoradiography which reveal detailed functional and distributional information about neural activity and mechanisms.  Center research falls into two broad categories:  parcellation, or rapid and reproducible identification of the boundaries of subareas in histological images, and quantification, or the measurement of the relative or absolute values of underlying substance values from optical images of these regions.  The parcellation project utilizes expert system technology for knowledge based image analysis.  Quantification projects include the development of a computerized system for the determination of local thickness of variation, of computer techniques for the detection of local variations within brain regions, and for the quantification of autoradiograms.  The center will operate service facilities that will supply image analysis software for microcomputer image analysis systems used in neuroscience, and will also provide access to image analysis hardware for researchers in the Northeast Corridor and throughout the country.  The image analysis hardware and software assists in the acquisition of accurate and precise information from CNS samples, focusing on the detection, quantification, and mapping of the experimental effects seen in autoradiographic and other images and in the accurate collation of different images (e.g., autoradiographic and histological images).  The center will also disseminate information about computerized image analysis methods for neuroscience by training researchers, publishing papers, and offering seminars and short courses.  n/a",AUTORADIOGRAPHIC IMAGE PROCESSING CENTER,3103956,P41RR001638,"['autoradiography', ' biomedical equipment resource', ' brain mapping', ' image processing', ' neuroanatomy']",NCRR,DREXEL UNIVERSITY,P41,1990,436578,-0.019920391918046492
"AUTORADIOGRAPHIC IMAGE PROCESSING CENTER We propose a center dedicated to research and development of computerized image analysis technology for us with samples produced by modern neuroanatomical techniques.  Included among these techniques are receptor and radiolabeled tracer autoradiography which reveal detailed functional and distributional information about neural activity and mechanisms.  Center research falls into two broad categories:  parcellation, or rapid and reproducible identification of the boundaries of subareas in histological images, and quantification, or the measurement of the relative or absolute values of underlying substance values from optical images of these regions.  The parcellation project utilizes expert system technology for knowledge based image analysis.  Quantification projects include the development of a computerized system for the determination of local thickness of variation, of computer techniques for the detection of local variations within brain regions, and for the quantification of autoradiograms.  The center will operate service facilities that will supply image analysis software for microcomputer image analysis systems used in neuroscience, and will also provide access to image analysis hardware for researchers in the Northeast Corridor and throughout the country.  The image analysis hardware and software assists in the acquisition of accurate and precise information from CNS samples, focusing on the detection, quantification, and mapping of the experimental effects seen in autoradiographic and other images and in the accurate collation of different images (e.g., autoradiographic and histological images).  The center will also disseminate information about computerized image analysis methods for neuroscience by training researchers, publishing papers, and offering seminars and short courses.  n/a",AUTORADIOGRAPHIC IMAGE PROCESSING CENTER,3103955,P41RR001638,"['autoradiography', ' biomedical equipment resource', ' brain mapping', ' image processing', ' neuroanatomy']",NCRR,DREXEL UNIVERSITY,P41,1989,456828,-0.019920391918046492
"AUTORADIOGRAPHIC IMAGE PROCESSING CENTER We propose a center dedicated to research and development of computerized image analysis technology for us with samples produced by modern neuroanatomical techniques.  Included among these techniques are receptor and radiolabeled tracer autoradiography which reveal detailed functional and distributional information about neural activity and mechanisms.  Center research falls into two broad categories:  parcellation, or rapid and reproducible identification of the boundaries of subareas in histological images, and quantification, or the measurement of the relative or absolute values of underlying substance values from optical images of these regions.  The parcellation project utilizes expert system technology for knowledge based image analysis.  Quantification projects include the development of a computerized system for the determination of local thickness of variation, of computer techniques for the detection of local variations within brain regions, and for the quantification of autoradiograms.  The center will operate service facilities that will supply image analysis software for microcomputer image analysis systems used in neuroscience, and will also provide access to image analysis hardware for researchers in the Northeast Corridor and throughout the country.  The image analysis hardware and software assists in the acquisition of accurate and precise information from CNS samples, focusing on the detection, quantification, and mapping of the experimental effects seen in autoradiographic and other images and in the accurate collation of different images (e.g., autoradiographic and histological images).  The center will also disseminate information about computerized image analysis methods for neuroscience by training researchers, publishing papers, and offering seminars and short courses.  n/a",AUTORADIOGRAPHIC IMAGE PROCESSING CENTER,3103949,P41RR001638,"['autoradiography', ' biomedical equipment resource', ' brain mapping', ' image processing', ' neuroanatomy']",NCRR,DREXEL UNIVERSITY,P41,1988,591931,-0.019920391918046492
"Simulation Tools for 3D and 4D CT and Dosimetry Abstract Photon-counting CT (PCCT) is a major technological advance in CT imaging. Using photon-counting instead of current energy-integrating detectors, PCCT can offer superior performance in terms of spatial resolution, artifact reduction, and most notably, material decomposition. PCCT’s energy differentiation utility offers an ability to more precisely distinguish different materials and optimize and expand the use of contrast agents in CT. With these abilities, PCCT can significantly facilitate quantitative imaging, reduce radiation exposure, and enable revolutionary new applications in functional and physiological imaging beyond existing CT techniques. To realize the full potential of PCCT in clinical practice, the technology needs comprehensive assessments and application-based optimizations. Effective design and deployment of PCCT depends on many design and use choices that should be made in view of the eventual clinical utility. Making these choices requires large scale trials on actual patients. However, such trials are challenging, considering the need to make many decisions prior to prototyping, the limited numbers of prototype PCCT scanners available today, and the often-unknown ground-truth in the patient images. Even for existing prototype systems, many decisions require repetitive trials with multiple acquisitions. This is both unethical and impractical considering radiation safety concerns and costs. These challenges can be overcome by utilizing virtual imaging trials (VITs) using computerized patients and imaging models. VITs provide an efficient means with which to determine the most effective and optimized design and use of imaging technologies with complete control over the study design. In our prior funded project, we developed a VIT framework to evaluate standard energy-integrating detector CT technologies. In this project, we expand the applicability of this framework to photon-counting detector CT. Specifically, we enhance our computational XCAT phantoms to model the necessary higher-resolution detail including normal and abnormal tissue heterogeneities and intra-organ contrast perfusion diversity across populations (Aim 1). To image the phantoms, we develop the first PCCT simulator capable of mimicking existing and emerging prototypes (Aim 2). The enhanced VIT framework will provide the essential foundation with which to comprehensively evaluate and optimize PCCT technologies and applications. In Aim 3, we assess and optimize the use of PCCT for morphological, textural, and compositional quantification in select oncologic and cardiac applications, two leading health detriments in the US where PCCT can offer a notable impact. The results will be the first of their kind in comprehensively evaluating the task-based merits and capabilities of PCCT, determining optimum dose per patient size for PCCT imaging of patients for cancerous lesions and cardiac plaque/stenoses, and helping to establish the effective utility of PCCT in clinical care. The purpose of this project is to develop and utilize a virtual framework to comprehensively evaluate and optimize emerging photon-counting devices and applications in CT imaging. The results will be the first of their kind evaluating the task-based merits and capabilities of photon-counting CT and will help establish its effectual utility in oncologic and cardiac care.",Simulation Tools for 3D and 4D CT and Dosimetry,10051026,R01EB001838,"['3-Dimensional', 'Abdomen', 'Anatomy', 'Cancerous', 'Cardiac', 'Caring', 'Clinic', 'Clinical', 'Computer software', 'Contrast Media', 'Data', 'Detection', 'Development', 'Devices', 'Disease', 'Dose', 'Ensure', 'Ethics', 'Evaluation', 'Foundations', 'Functional Imaging', 'Funding', 'Health', 'Heterogeneity', 'Human', 'Image', 'Imaging Phantoms', 'Imaging technology', 'Industry', 'Lesion', 'Manufacturer Name', 'Methods', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Noise', 'Organ', 'Pathologic', 'Patient imaging', 'Patients', 'Performance', 'Perfusion', 'Photons', 'Population', 'Radiation', 'Radiation Dose Unit', 'Radiation exposure', 'Research Design', 'Resolution', 'Resources', 'Role', 'Safety', 'Scientist', 'Series', 'Specimen', 'Stenosis', 'System', 'Task Performances', 'Techniques', 'Technology', 'Texture', 'Tissue Model', 'Tissues', 'Work', 'X-Ray Computed Tomography', 'analytical method', 'base', 'cardiac plaque', 'clinical application', 'clinical care', 'clinical practice', 'computerized', 'computerized tools', 'cost', 'cost efficient', 'deep learning', 'design', 'detector', 'dosimetry', 'experimental study', 'human imaging', 'human subject', 'improved', 'insight', 'learning strategy', 'photon-counting detector', 'prototype', 'quantitative imaging', 'simulation', 'soft tissue', 'tool', 'virtual', 'virtual imaging']",NIBIB,DUKE UNIVERSITY,R01,2020,545403,-0.05800946459357755
"ARRAY PROCESSOR FOR MEDICAL IMAGE PROCESSING A powerful array processor to be attached to our existing VAX 11/780 computer is requested.  This instrument will be dedicated to research in medical imaging and image processing.  Specifically, four projects are proposed, two each in digital radiology and nuclear medicine.  In digital radiology, the goal of the proposed research is to deblur subtraction angiograms by using sophisticated image processing algorithms, thereby facilitating the visualization of atherosclerotic lesions in the coronary arteries.  Two distinct processing algorithms will be investigated.  Both of these algorithms are well known in the image processing leterature, but have not yet been applied to large medical images for lack of sufficient computer power.  The nuclear medicine projects have as their goal earlier detection and more accurate characterization of small neoplasms.  In one of these projects, three-dimensional images will be reconstructed from coded-aperture data.  This approach allows true tomographic reconstruction without any motion of the detector or aperture.  The other nuclear medicine project involves automated feature extraction and pattern recognition of liver scans.  In addition, it will include an attempt to optimize the design of nuclear imaging systems for most accurate extraction of the relevant features.  n/a",ARRAY PROCESSOR FOR MEDICAL IMAGE PROCESSING,3519184,S10RR002395,"['artificial intelligence', ' clinical biomedical equipment', ' computers', ' nuclear medicine', ' radiodiagnosis', ' tomography']",NCRR,UNIVERSITY OF ARIZONA,S10,1985,92500,-0.09366652664501635
"COMPUTATIONAL MODEL OF SELF ORGANIZING AUDITORY MAPS The overall goal of this project is to develop a comprehensive computer          model of neural coding of auditory space in the auditory thalamocortical         system.  A self-organizing neural network model of auditory cortical maps        will be studied.  The output of the network will be that of a simulated          planar cortex that will afford direct comparisons of simulated virtual           space receptive fields (VSRF) with those of actual VSRFs of direction-           sensitive neurons in primary auditory cortex (AI).  These computational          simulations will greatly benefit from the ability to synthesize virtual          auditory space from measured Head-Related Transfer Functions (HRTFs). The        same stimuli used for microelectrode recordings in auditory cortex of cat        will be used in the development and stimulation of the neural-network            models.  These network models assume that primary auditory cortex is             subject to experience-dependent changes.  Currently available                    computational models of neural signal processing in the auditory periphery       and brain stem will be used to provide a neural representation of binaural       stimuli to the self-organizing thalamocortical model.  The simulation of         self-organizing processes operating on input data with intrinsic structure       leads to the emergence of topographical maps.  These maps afford the             opportunity to examine overlays of functional organization.  Currently,          the ability to corroborate the emergence of a spatial auditory map with a        detailed map of an auditory cortical field is quite limited, but the             simulated development of maps will demonstrate how global topographic            order can emerge, in principle, from local cooperative and competitive           interactions within the cortical field.  It is anticipated that these            simulation studies will help guide neurophysiology research with regard to       deciphering the neural code of auditory space.  Specifically, the                simulations may suggest where to probe the cortex with microelectrodes and       with what types of stimulation.  Interactions between the tonotopic              frequency organization and orthogonal iso-frequency organization will be         investigated computationally.  This computational modeling work may              provide a better understanding of the representation of complex sounds in        general at higher levels in the auditory system.  Given the nature of the        model to re-organize, effects of cochlear lesions can be studied and thus        aid in the study of sensorineural hearing impairment.                             n/a",COMPUTATIONAL MODEL OF SELF ORGANIZING AUDITORY MAPS,2414667,R03DC002804,"['artificial intelligence', ' auditory pathways', ' cats', ' computational neuroscience', ' computer simulation', ' computer system design /evaluation', ' model design /development', ' neuroanatomy', ' sensorineural hearing loss', ' thalamocortical tract']",NIDCD,UNIVERSITY OF WISCONSIN MADISON,R03,1997,33508,-0.02737105637503113
"Manipulating Neural Tissue with Ultra-Short Laser Pulses    DESCRIPTION (provided by applicant):  We seek to renew our joint discovery and technology based bioengineering research partnership (BRP).  Our program advances the use of amplified ultrashort laser pulses as a tool to manipulate living and histological tissue.  This unique technology, pioneered by the BRP team, permits tissue to be cut on the micrometer scale by light-fueled plasma-meditated ablation.  Cutting occurs without thermal damage and can be combined with spectroscopic feedback to identify and limit the region being perturbed.  We advance this technology in new ways, including the use of temporal focusing for deep ablation, and apply it to three key test beds.  The first is construction of the angiotome, i.e., a complete vectorized map of cortical vasculature; here we use plasma mediated ablation to automate a form of block-face imaging along with novel algorithms to filter and vectorize features in the data.  The second is the study of neuronal viability in response to perturbations of subcortical blood flow; here we use plasma-mediated ablation to block flow in individual targeted microvessels that lie below the cortical surface.  The third test bed is the automation of cranial and spinal surgery; we combine plasma-mediated ablation with laser-induced breakdown spectroscopy to cut bone yet avoid injury to soft tissue.  These investigations are particularly relevant to diagnosing and understanding microinfarctions, i.e., damage to the brain as the result of damage to single cortical vessels, that accumulate with age and trauma.        Program Director (Last, first, middle): Kleinfeld, David, Kaufhold, John and Squier, Jeffrey. These studies advance the use of light to image and manipulate the flow of blood in the brain. We make use of rats and mice as model systems for our experiments. The new capabilities from the proposed work hold two promises for advances in medicine: One is an understanding of micro- infarctions - lesions of individual blood vessels in cortex that lead to the death of brain cells- that accrue with age and trauma. The second is an understanding of the normal patterns of blood flow throughout the brain, which provide the basis for interpreting medical images of brain function.",Manipulating Neural Tissue with Ultra-Short Laser Pulses,8729873,R01EB003832,"['3-Dimensional', 'Ablation', 'Age', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Automation', 'Beds', 'Biological Models', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Cephalic', 'Cessation of life', 'Craniotomy', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Dimensions', 'Face', 'Feedback', 'Fluorescence', 'Functional Imaging', 'Future', 'Generations', 'Goals', 'Health', 'Histocompatibility Testing', 'Histology', 'Image', 'Impaired cognition', 'Individual', 'Infarction', 'Injury', 'Investigation', 'Joints', 'Label', 'Laminectomy', 'Lasers', 'Lateral', 'Lead', 'Learning', 'Lesion', 'Life', 'Light', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Mediating', 'Medical Imaging', 'Medicine', 'Metabolism', 'Methodology', 'Mitochondria', 'Modeling', 'Mus', 'Neocortex', 'Neurons', 'Operative Surgical Procedures', 'Optics', 'Organelles', 'Pattern', 'Physiologic pulse', 'Plasma', 'Process', 'Rattus', 'Reperfusion Therapy', 'Research', 'Resolution', 'Role', 'Scanning', 'Shapes', 'Spectrum Analysis', 'Speed', 'Spinal', 'Stroke', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Trauma', 'Ursidae Family', 'Veins', 'Work', 'arteriole', 'base', 'bone', 'brain cell', 'capillary', 'design', 'hemodynamics', 'meetings', 'millimeter', 'neurosurgery', 'novel', 'programs', 'reconstruction', 'relating to nervous system', 'research study', 'response', 'second harmonic', 'soft tissue', 'tool', 'two-photon']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2014,812810,-0.014014694665780878
"Manipulating Neural Tissue with Ultra-Short Laser Pulses    DESCRIPTION (provided by applicant):  We seek to renew our joint discovery and technology based bioengineering research partnership (BRP).  Our program advances the use of amplified ultrashort laser pulses as a tool to manipulate living and histological tissue.  This unique technology, pioneered by the BRP team, permits tissue to be cut on the micrometer scale by light-fueled plasma-meditated ablation.  Cutting occurs without thermal damage and can be combined with spectroscopic feedback to identify and limit the region being perturbed.  We advance this technology in new ways, including the use of temporal focusing for deep ablation, and apply it to three key test beds.  The first is construction of the angiotome, i.e., a complete vectorized map of cortical vasculature; here we use plasma mediated ablation to automate a form of block-face imaging along with novel algorithms to filter and vectorize features in the data.  The second is the study of neuronal viability in response to perturbations of subcortical blood flow; here we use plasma-mediated ablation to block flow in individual targeted microvessels that lie below the cortical surface.  The third test bed is the automation of cranial and spinal surgery; we combine plasma-mediated ablation with laser-induced breakdown spectroscopy to cut bone yet avoid injury to soft tissue.  These investigations are particularly relevant to diagnosing and understanding microinfarctions, i.e., damage to the brain as the result of damage to single cortical vessels, that accumulate with age and trauma.      PUBLIC HEALTH RELEVANCE:  These studies advance the use of light to image and manipulate the flow of blood in the brain.  We make use of rats and mice as model systems for our experiments.  The new capabilities from the proposed work hold two promises for advances in medicine:  One is an understanding of micro- infarctions - lesions of individual blood vessels in cortex that lead to the death of brain cells- that accrue with age and trauma.  The second is an understanding of the normal patterns of blood flow throughout the brain, which provide the basis for interpreting medical images of brain function.              Program Director (Last, first, middle): Kleinfeld, David, Kaufhold, John and Squier, Jeffrey. These studies advance the use of light to image and manipulate the flow of blood in the brain. We make use of rats and mice as model systems for our experiments. The new capabilities from the proposed work hold two promises for advances in medicine: One is an understanding of micro- infarctions - lesions of individual blood vessels in cortex that lead to the death of brain cells- that accrue with age and trauma. The second is an understanding of the normal patterns of blood flow throughout the brain, which provide the basis for interpreting medical images of brain function.",Manipulating Neural Tissue with Ultra-Short Laser Pulses,8537158,R01EB003832,"['3-Dimensional', 'Ablation', 'Age', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Automation', 'Beds', 'Biological Models', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Cephalic', 'Cessation of life', 'Craniotomy', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Dimensions', 'Face', 'Feedback', 'Fluorescence', 'Functional Imaging', 'Future', 'Generations', 'Goals', 'Health', 'Histocompatibility Testing', 'Histology', 'Image', 'Impaired cognition', 'Individual', 'Infarction', 'Injury', 'Investigation', 'Joints', 'Label', 'Laminectomy', 'Lasers', 'Lateral', 'Lead', 'Learning', 'Lesion', 'Life', 'Light', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Mediating', 'Medical Imaging', 'Medicine', 'Metabolism', 'Methodology', 'Mitochondria', 'Modeling', 'Mus', 'Neocortex', 'Neurons', 'Operative Surgical Procedures', 'Optics', 'Organelles', 'Pattern', 'Physiologic pulse', 'Plasma', 'Process', 'Rattus', 'Reperfusion Therapy', 'Research', 'Resolution', 'Role', 'Scanning', 'Shapes', 'Spectrum Analysis', 'Speed', 'Spinal', 'Stroke', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Trauma', 'Ursidae Family', 'Veins', 'Work', 'arteriole', 'base', 'bone', 'brain cell', 'capillary', 'design', 'hemodynamics', 'meetings', 'millimeter', 'neurosurgery', 'novel', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'research study', 'response', 'second harmonic', 'soft tissue', 'tool', 'two-photon']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,826572,-0.01509783583514028
"Manipulating Neural Tissue with Ultra-Short Laser Pulses    DESCRIPTION (provided by applicant):  We seek to renew our joint discovery and technology based bioengineering research partnership (BRP).  Our program advances the use of amplified ultrashort laser pulses as a tool to manipulate living and histological tissue.  This unique technology, pioneered by the BRP team, permits tissue to be cut on the micrometer scale by light-fueled plasma-meditated ablation.  Cutting occurs without thermal damage and can be combined with spectroscopic feedback to identify and limit the region being perturbed.  We advance this technology in new ways, including the use of temporal focusing for deep ablation, and apply it to three key test beds.  The first is construction of the angiotome, i.e., a complete vectorized map of cortical vasculature; here we use plasma mediated ablation to automate a form of block-face imaging along with novel algorithms to filter and vectorize features in the data.  The second is the study of neuronal viability in response to perturbations of subcortical blood flow; here we use plasma-mediated ablation to block flow in individual targeted microvessels that lie below the cortical surface.  The third test bed is the automation of cranial and spinal surgery; we combine plasma-mediated ablation with laser-induced breakdown spectroscopy to cut bone yet avoid injury to soft tissue.  These investigations are particularly relevant to diagnosing and understanding microinfarctions, i.e., damage to the brain as the result of damage to single cortical vessels, that accumulate with age and trauma.      PUBLIC HEALTH RELEVANCE:  These studies advance the use of light to image and manipulate the flow of blood in the brain.  We make use of rats and mice as model systems for our experiments.  The new capabilities from the proposed work hold two promises for advances in medicine:  One is an understanding of micro- infarctions - lesions of individual blood vessels in cortex that lead to the death of brain cells- that accrue with age and trauma.  The second is an understanding of the normal patterns of blood flow throughout the brain, which provide the basis for interpreting medical images of brain function.              Program Director (Last, first, middle): Kleinfeld, David, Kaufhold, John and Squier, Jeffrey. These studies advance the use of light to image and manipulate the flow of blood in the brain. We make use of rats and mice as model systems for our experiments. The new capabilities from the proposed work hold two promises for advances in medicine: One is an understanding of micro- infarctions - lesions of individual blood vessels in cortex that lead to the death of brain cells- that accrue with age and trauma. The second is an understanding of the normal patterns of blood flow throughout the brain, which provide the basis for interpreting medical images of brain function.",Manipulating Neural Tissue with Ultra-Short Laser Pulses,8319531,R01EB003832,"['3-Dimensional', 'Ablation', 'Age', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Automation', 'Beds', 'Biological Models', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Cephalic', 'Cessation of life', 'Craniotomy', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Dimensions', 'Face', 'Feedback', 'Fluorescence', 'Functional Imaging', 'Future', 'Generations', 'Goals', 'Health', 'Histocompatibility Testing', 'Histology', 'Image', 'Impaired cognition', 'Individual', 'Infarction', 'Injury', 'Investigation', 'Joints', 'Label', 'Laminectomy', 'Lasers', 'Lateral', 'Lead', 'Learning', 'Lesion', 'Life', 'Light', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Mediating', 'Medical Imaging', 'Medicine', 'Metabolism', 'Methodology', 'Mitochondria', 'Modeling', 'Mus', 'Neocortex', 'Neurons', 'Operative Surgical Procedures', 'Optics', 'Organelles', 'Pattern', 'Physiologic pulse', 'Plasma', 'Process', 'Rattus', 'Reperfusion Therapy', 'Research', 'Resolution', 'Role', 'Scanning', 'Shapes', 'Spectrum Analysis', 'Speed', 'Spinal', 'Stroke', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Trauma', 'Ursidae Family', 'Veins', 'Work', 'arteriole', 'base', 'bone', 'brain cell', 'capillary', 'design', 'hemodynamics', 'meetings', 'millimeter', 'neurosurgery', 'novel', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'research study', 'response', 'second harmonic', 'soft tissue', 'tool', 'two-photon']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,892777,-0.01509783583514028
"Manipulating Neural Tissue with Ultra-Short Laser Pulses    DESCRIPTION (provided by applicant):  We seek to renew our joint discovery and technology based bioengineering research partnership (BRP).  Our program advances the use of amplified ultrashort laser pulses as a tool to manipulate living and histological tissue.  This unique technology, pioneered by the BRP team, permits tissue to be cut on the micrometer scale by light-fueled plasma-meditated ablation.  Cutting occurs without thermal damage and can be combined with spectroscopic feedback to identify and limit the region being perturbed.  We advance this technology in new ways, including the use of temporal focusing for deep ablation, and apply it to three key test beds.  The first is construction of the angiotome, i.e., a complete vectorized map of cortical vasculature; here we use plasma mediated ablation to automate a form of block-face imaging along with novel algorithms to filter and vectorize features in the data.  The second is the study of neuronal viability in response to perturbations of subcortical blood flow; here we use plasma-mediated ablation to block flow in individual targeted microvessels that lie below the cortical surface.  The third test bed is the automation of cranial and spinal surgery; we combine plasma-mediated ablation with laser-induced breakdown spectroscopy to cut bone yet avoid injury to soft tissue.  These investigations are particularly relevant to diagnosing and understanding microinfarctions, i.e., damage to the brain as the result of damage to single cortical vessels, that accumulate with age and trauma.      PUBLIC HEALTH RELEVANCE:  These studies advance the use of light to image and manipulate the flow of blood in the brain.  We make use of rats and mice as model systems for our experiments.  The new capabilities from the proposed work hold two promises for advances in medicine:  One is an understanding of micro- infarctions - lesions of individual blood vessels in cortex that lead to the death of brain cells- that accrue with age and trauma.  The second is an understanding of the normal patterns of blood flow throughout the brain, which provide the basis for interpreting medical images of brain function.              Program Director (Last, first, middle): Kleinfeld, David, Kaufhold, John and Squier, Jeffrey. These studies advance the use of light to image and manipulate the flow of blood in the brain. We make use of rats and mice as model systems for our experiments. The new capabilities from the proposed work hold two promises for advances in medicine: One is an understanding of micro- infarctions - lesions of individual blood vessels in cortex that lead to the death of brain cells- that accrue with age and trauma. The second is an understanding of the normal patterns of blood flow throughout the brain, which provide the basis for interpreting medical images of brain function.",Manipulating Neural Tissue with Ultra-Short Laser Pulses,8134363,R01EB003832,"['3-Dimensional', 'Ablation', 'Age', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Automation', 'Beds', 'Biological Models', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Cephalic', 'Cessation of life', 'Craniotomy', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Dimensions', 'Face', 'Feedback', 'Fluorescence', 'Functional Imaging', 'Future', 'Generations', 'Goals', 'Health', 'Histocompatibility Testing', 'Histology', 'Image', 'Impaired cognition', 'Individual', 'Infarction', 'Injury', 'Investigation', 'Joints', 'Label', 'Laminectomy', 'Lasers', 'Lateral', 'Lead', 'Learning', 'Lesion', 'Life', 'Light', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Mediating', 'Medical Imaging', 'Medicine', 'Metabolism', 'Methodology', 'Mitochondria', 'Modeling', 'Mus', 'Neocortex', 'Neurons', 'Operative Surgical Procedures', 'Optics', 'Organelles', 'Pattern', 'Physiologic pulse', 'Plasma', 'Process', 'Rattus', 'Reperfusion Therapy', 'Research', 'Resolution', 'Role', 'Scanning', 'Shapes', 'Spectrum Analysis', 'Speed', 'Spinal', 'Stroke', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Trauma', 'Ursidae Family', 'Veins', 'Work', 'arteriole', 'base', 'bone', 'brain cell', 'capillary', 'design', 'hemodynamics', 'meetings', 'millimeter', 'neurosurgery', 'novel', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'research study', 'response', 'second harmonic', 'soft tissue', 'tool', 'two-photon']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,932015,-0.01509783583514028
"Manipulating Neural Tissue with Ultra-Short Laser Pulses    DESCRIPTION (provided by applicant):  We seek to renew our joint discovery and technology based bioengineering research partnership (BRP).  Our program advances the use of amplified ultrashort laser pulses as a tool to manipulate living and histological tissue.  This unique technology, pioneered by the BRP team, permits tissue to be cut on the micrometer scale by light-fueled plasma-meditated ablation.  Cutting occurs without thermal damage and can be combined with spectroscopic feedback to identify and limit the region being perturbed.  We advance this technology in new ways, including the use of temporal focusing for deep ablation, and apply it to three key test beds.  The first is construction of the angiotome, i.e., a complete vectorized map of cortical vasculature; here we use plasma mediated ablation to automate a form of block-face imaging along with novel algorithms to filter and vectorize features in the data.  The second is the study of neuronal viability in response to perturbations of subcortical blood flow; here we use plasma-mediated ablation to block flow in individual targeted microvessels that lie below the cortical surface.  The third test bed is the automation of cranial and spinal surgery; we combine plasma-mediated ablation with laser-induced breakdown spectroscopy to cut bone yet avoid injury to soft tissue.  These investigations are particularly relevant to diagnosing and understanding microinfarctions, i.e., damage to the brain as the result of damage to single cortical vessels, that accumulate with age and trauma.      PUBLIC HEALTH RELEVANCE:  These studies advance the use of light to image and manipulate the flow of blood in the brain.  We make use of rats and mice as model systems for our experiments.  The new capabilities from the proposed work hold two promises for advances in medicine:  One is an understanding of micro- infarctions - lesions of individual blood vessels in cortex that lead to the death of brain cells- that accrue with age and trauma.  The second is an understanding of the normal patterns of blood flow throughout the brain, which provide the basis for interpreting medical images of brain function.              Program Director (Last, first, middle): Kleinfeld, David, Kaufhold, John and Squier, Jeffrey. These studies advance the use of light to image and manipulate the flow of blood in the brain. We make use of rats and mice as model systems for our experiments. The new capabilities from the proposed work hold two promises for advances in medicine: One is an understanding of micro- infarctions - lesions of individual blood vessels in cortex that lead to the death of brain cells- that accrue with age and trauma. The second is an understanding of the normal patterns of blood flow throughout the brain, which provide the basis for interpreting medical images of brain function.",Manipulating Neural Tissue with Ultra-Short Laser Pulses,7988297,R01EB003832,"['3-Dimensional', 'Ablation', 'Age', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Automation', 'Beds', 'Biological Models', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Cephalic', 'Cessation of life', 'Craniotomy', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Dimensions', 'Feedback', 'Fluorescence', 'Functional Imaging', 'Future', 'Generations', 'Goals', 'Health', 'Histocompatibility Testing', 'Histology', 'Image', 'Impaired cognition', 'Individual', 'Infarction', 'Injury', 'Investigation', 'Joints', 'Label', 'Laminectomy', 'Lasers', 'Lateral', 'Lead', 'Learning', 'Lesion', 'Life', 'Light', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Mediating', 'Medical Imaging', 'Medicine', 'Metabolism', 'Methodology', 'Mitochondria', 'Modeling', 'Mus', 'Neocortex', 'Neurons', 'Operative Surgical Procedures', 'Optics', 'Organelles', 'Pattern', 'Physiologic pulse', 'Plasma', 'Process', 'Rattus', 'Reperfusion Therapy', 'Research', 'Resolution', 'Role', 'Scanning', 'Shapes', 'Spectrum Analysis', 'Speed', 'Spinal', 'Stroke', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Trauma', 'Ursidae Family', 'Veins', 'Work', 'arteriole', 'base', 'bone', 'brain cell', 'capillary', 'design', 'hemodynamics', 'meetings', 'millimeter', 'neurosurgery', 'novel', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'research study', 'response', 'second harmonic', 'soft tissue', 'tool', 'two-photon']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2010,994058,-0.01509783583514028
"PATTERN SIGNATURES OF LARYNGEAL MYOELECTRIC SIGNALS This study aims to demonstrate the utility and feasibility of                    decomposing vocal tract electromyographic signals to their constituent           motor unit action potentials. This will provide a picture of the                 myoelectric activities of the many motor units in a local area of the            muscle in their appropriate magnitude and temporal relations. This has           not been previously accomplished for vocal tract musculature and has             only recently become viable because specialized recording needle                 electrodes and amplifiers have become commercially available. The                availability of these constituent signals will permit the observation of         underlying neural pathology during the manifestation of disorders                affecting voice and speech, thus greatly enhancing our understanding of          neuromotor disease physiology.  The thyroarytenoid and cricothyroid              muscles of spasmodic dysphonia subjects will be used as models for the           study. An emerging class of powerful signal processing technologies,             such as time-frequency wavelet transforms and singular value                     decomposition, will be applied to the composite electromyographic                waveforms to provide new representations of the dynamic frequency                content of the signal. Since the nervous system encodes its information          to the muscle in terms of frequency, these transforms may reveal pattern         signatures of disordered pathology that may be embedded in the                   myoelectric signals. These technologies have been successfully applied           in modern applications of space technology, ""smart weapons"", and radar           arrays where signals measured at the periphery are described as                  containing multivariate, poor-context frequency information in the               presence of noise. Although signals and objectives are highly analogous,         transfer of these modern processing technologies to laryn-gologic                research has been greatly lacking. The constituent neural signals                obtained by decomposition together with the new frequency                        representations provide the framework for identifying pattern signatures         of the controlling system using peripheral measurements. Once pattern            signatures of neuromuscular pathologies become identified in the                 myoelectric signals of laryngeal muscles, processes that identify and            correlate pattern signatures obtained from ore remote (and less                  invasive) recording fields, such as surface array microdetectors (e.g.           electrodes, accelerometers, magnetometers, ultra-sound), makes it                possible to achieve equal success as in analogous applications. This             ability, in turn, will have a profound impact on early detection of              diseases that manifest with vocal symptoms, such as Parkinson's and ALS,         and for the pre-surgical testing of the propensity for potentially life-         threatening laryngeal tonic closure (spasm) due to intubation trauma.             n/a",PATTERN SIGNATURES OF LARYNGEAL MYOELECTRIC SIGNALS,6137898,R21DC003952,"['action potentials', ' clinical research', ' electromyography', ' human subject', ' larynx', ' larynx muscle', ' neural recruitment', ' neuromuscular disorder', ' neuromuscular function', ' spasm', ' speech disorders']",NIDCD,NEW YORK MEDICAL COLLEGE,R21,2000,76748,-0.04649112696237824
"PATTERN SIGNATURES OF LARYNGEAL MYOELECTRIC SIGNALS This study aims to demonstrate the utility and feasibility of                    decomposing vocal tract electromyographic signals to their constituent           motor unit action potentials. This will provide a picture of the                 myoelectric activities of the many motor units in a local area of the            muscle in their appropriate magnitude and temporal relations. This has           not been previously accomplished for vocal tract musculature and has             only recently become viable because specialized recording needle                 electrodes and amplifiers have become commercially available. The                availability of these constituent signals will permit the observation of         underlying neural pathology during the manifestation of disorders                affecting voice and speech, thus greatly enhancing our understanding of          neuromotor disease physiology.  The thyroarytenoid and cricothyroid              muscles of spasmodic dysphonia subjects will be used as models for the           study. An emerging class of powerful signal processing technologies,             such as time-frequency wavelet transforms and singular value                     decomposition, will be applied to the composite electromyographic                waveforms to provide new representations of the dynamic frequency                content of the signal. Since the nervous system encodes its information          to the muscle in terms of frequency, these transforms may reveal pattern         signatures of disordered pathology that may be embedded in the                   myoelectric signals. These technologies have been successfully applied           in modern applications of space technology, ""smart weapons"", and radar           arrays where signals measured at the periphery are described as                  containing multivariate, poor-context frequency information in the               presence of noise. Although signals and objectives are highly analogous,         transfer of these modern processing technologies to laryn-gologic                research has been greatly lacking. The constituent neural signals                obtained by decomposition together with the new frequency                        representations provide the framework for identifying pattern signatures         of the controlling system using peripheral measurements. Once pattern            signatures of neuromuscular pathologies become identified in the                 myoelectric signals of laryngeal muscles, processes that identify and            correlate pattern signatures obtained from ore remote (and less                  invasive) recording fields, such as surface array microdetectors (e.g.           electrodes, accelerometers, magnetometers, ultra-sound), makes it                possible to achieve equal success as in analogous applications. This             ability, in turn, will have a profound impact on early detection of              diseases that manifest with vocal symptoms, such as Parkinson's and ALS,         and for the pre-surgical testing of the propensity for potentially life-         threatening laryngeal tonic closure (spasm) due to intubation trauma.             n/a",PATTERN SIGNATURES OF LARYNGEAL MYOELECTRIC SIGNALS,2749290,R21DC003952,"['action potentials', ' clinical research', ' electromyography', ' human subject', ' larynx', ' larynx muscle', ' neural recruitment', ' neuromuscular disorder', ' neuromuscular function', ' spasm', ' speech disorders']",NIDCD,NEW YORK MEDICAL COLLEGE,R21,1999,76210,-0.04649112696237824
"A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION Radiation therapy planning utilizes knowledge of the properties of radiation beams in tissue, and relies on calculation of radiation dose in the tumor region and in other locations of the patient's body.  Manual optimization of treatment plans for an individual patient based on dose computations with variation of parameters can only consider a few of the possible options.  We have developed an expert system for planning radiation therapy as an alternative approach.  The proposed work will extend this system to address several research topics in the area of knowledge-based systems for planning therapy that have arisen out of the previous developments.  The search space of radiation treatment plans has both discrete and continuous dimensions, representing dozens to hundreds of adjustable parameters in a typical treatment plan.  The solution space is therefore extremely large.  Radiation dose calculation is necessary for each plan the search considers, in order to evaluate the proposed plan.  This computation is clearly the rate limiting step. Therefore it is important to investigate how the efficiency of search for a solution to a planning problem can be increased by modifying the search strategy.  The expert system we have developed will be used to achieve two main objectives:  1. to collect data on search strategies for performing optimization in a complex planning domain, in which we can vary the parameters of the search algorithm systematically to see if there are optimal answers to trade-offs between e.g., depth-first or breadth-first, pruning algorithms, broad search or fine tuning of plans, and other issues.  2. to systematically explore the possibilities for configuring radiation beams for different classes of tumor sites, which would likely lead to unusual but more effective treatment configurations than are normally considered in clinical practice.  This offers the potential to learn how to dramatically improve the accuracy and effectiveness of radiation treatment and thereby increase the cure rate for many cancers.  Other issues to be investigated include: how graphical tools can aid in the above, i.e., tools for visualization of large data sets; how the user interface can be built to smoothly integrate the automated search process into the clinical work flow; how to integrate the system with other information sources (e.g., an anatomy browser), and how to verify the correctness of the program as a whole.  n/a",A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION,2237617,R01LM004174,"['artificial intelligence', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer simulation', ' human data', ' neoplasm /cancer radiation therapy', ' patient care planning', ' radiation therapy dosage']",NLM,UNIVERSITY OF WASHINGTON,R01,1994,191252,-0.01523869010198783
"A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION Radiation therapy planning utilizes knowledge of the properties of radiation beams in tissue, and relies on calculation of radiation dose in the tumor region and in other locations of the patient's body.  Manual optimization of treatment plans for an individual patient based on dose computations with variation of parameters can only consider a few of the possible options.  We have developed an expert system for planning radiation therapy as an alternative approach.  The proposed work will extend this system to address several research topics in the area of knowledge-based systems for planning therapy that have arisen out of the previous developments.  The search space of radiation treatment plans has both discrete and continuous dimensions, representing dozens to hundreds of adjustable parameters in a typical treatment plan.  The solution space is therefore extremely large.  Radiation dose calculation is necessary for each plan the search considers, in order to evaluate the proposed plan.  This computation is clearly the rate limiting step. Therefore it is important to investigate how the efficiency of search for a solution to a planning problem can be increased by modifying the search strategy.  The expert system we have developed will be used to achieve two main objectives:  1. to collect data on search strategies for performing optimization in a complex planning domain, in which we can vary the parameters of the search algorithm systematically to see if there are optimal answers to trade-offs between e.g., depth-first or breadth-first, pruning algorithms, broad search or fine tuning of plans, and other issues.  2. to systematically explore the possibilities for configuring radiation beams for different classes of tumor sites, which would likely lead to unusual but more effective treatment configurations than are normally considered in clinical practice.  This offers the potential to learn how to dramatically improve the accuracy and effectiveness of radiation treatment and thereby increase the cure rate for many cancers.  Other issues to be investigated include: how graphical tools can aid in the above, i.e., tools for visualization of large data sets; how the user interface can be built to smoothly integrate the automated search process into the clinical work flow; how to integrate the system with other information sources (e.g., an anatomy browser), and how to verify the correctness of the program as a whole.  n/a",A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION,3373571,R01LM004174,"['artificial intelligence', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer simulation', ' human data', ' neoplasm /cancer radiation therapy', ' patient care planning', ' radiation therapy dosage']",NLM,UNIVERSITY OF WASHINGTON,R01,1993,191863,-0.01523869010198783
"A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION Radiation therapy for cancer is one of the first areas in which computers were widely used in support of clinical practice, dating back at least twenty years.  The initial application, still the major one today, was the calculation of radiation dose in the tumor region and in other locations of the patient's body.  However, knowledge about clinical implications of radiation treatment has been difficult to systematize in a form suitable for computer processing.  The number and range of treatment options is large.  Treatment optimization based on dose computations with variation of parameters can only handle a few of the possible options.  As an alternative approach to systematizing clinical knowledge of radiation effects, this project will develop an expert system for planning radiation therapy.  The problem of planning radiation therapy presents challenges in the design of expert systems.  Efficiency and flexibility are important because of the large problem space.  In addition, some of the knowledge of radiation treatment planning is most naturally expressed in the form of constraints, rather than production rules.  Most important, decision-making normally relies on a treatment simulation system to provide dose and geometric information about proposed treatment strategies.  To meet these requirements this project will develop and investigate the properties of an expert system that integrates with a treatmet simulation system.  The two systems will be coupled using a message passing technique previously developed for the simulation system internal operation.  The expert system will use both frames and production rules as appropriate tothe problem domain.  Constraint mechanisms will be introduced which allow constraints to be strengthened or weakened as the system preceeds toward a solution.  The simulation system will be coupled to the expert system as a means of checking constraint satisfaction and graphically displaying the solution(s).  The integration of a simulation system with appropriate knowledge representation techniques should provide new insight into methods of constraint satisfaction, and also extend understanding of the design and behavior of hybrid systems.  n/a",A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION,3373575,R01LM004174,"['computer assisted medical decision making', ' computer assisted patient care', ' computer simulation', ' neoplasm /cancer radiation therapy', ' patient care planning', ' radiation dosage']",NLM,UNIVERSITY OF WASHINGTON,R01,1991,194979,0.010683163851677653
"A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION Radiation therapy for cancer is one of the first areas in which computers were widely used in support of clinical practice, dating back at least twenty years.  The initial application, still the major one today, was the calculation of radiation dose in the tumor region and in other locations of the patient's body.  However, knowledge about clinical implications of radiation treatment has been difficult to systematize in a form suitable for computer processing.  The number and range of treatment options is large.  Treatment optimization based on dose computations with variation of parameters can only handle a few of the possible options.  As an alternative approach to systematizing clinical knowledge of radiation effects, this project will develop an expert system for planning radiation therapy.  The problem of planning radiation therapy presents challenges in the design of expert systems.  Efficiency and flexibility are important because of the large problem space.  In addition, some of the knowledge of radiation treatment planning is most naturally expressed in the form of constraints, rather than production rules.  Most important, decision-making normally relies on a treatment simulation system to provide dose and geometric information about proposed treatment strategies.  To meet these requirements this project will develop and investigate the properties of an expert system that integrates with a treatmet simulation system.  The two systems will be coupled using a message passing technique previously developed for the simulation system internal operation.  The expert system will use both frames and production rules as appropriate tothe problem domain.  Constraint mechanisms will be introduced which allow constraints to be strengthened or weakened as the system preceeds toward a solution.  The simulation system will be coupled to the expert system as a means of checking constraint satisfaction and graphically displaying the solution(s).  The integration of a simulation system with appropriate knowledge representation techniques should provide new insight into methods of constraint satisfaction, and also extend understanding of the design and behavior of hybrid systems.  n/a",A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION,3373574,R01LM004174,"['computer assisted medical decision making', ' computer assisted patient care', ' computer simulation', ' neoplasm /cancer radiation therapy', ' patient care planning', ' radiation dosage']",NLM,UNIVERSITY OF WASHINGTON,R01,1990,189122,0.010683163851677653
"A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION Radiation therapy for cancer is one of the first areas in which computers were widely used in support of clinical practice, dating back at least twenty years.  The initial application, still the major one today, was the calculation of radiation dose in the tumor region and in other locations of the patient's body.  However, knowledge about clinical implications of radiation treatment has been difficult to systematize in a form suitable for computer processing.  The number and range of treatment options is large.  Treatment optimization based on dose computations with variation of parameters can only handle a few of the possible options.  As an alternative approach to systematizing clinical knowledge of radiation effects, this project will develop an expert system for planning radiation therapy.  The problem of planning radiation therapy presents challenges in the design of expert systems.  Efficiency and flexibility are important because of the large problem space.  In addition, some of the knowledge of radiation treatment planning is most naturally expressed in the form of constraints, rather than production rules.  Most important, decision-making normally relies on a treatment simulation system to provide dose and geometric information about proposed treatment strategies.  To meet these requirements this project will develop and investigate the properties of an expert system that integrates with a treatmet simulation system.  The two systems will be coupled using a message passing technique previously developed for the simulation system internal operation.  The expert system will use both frames and production rules as appropriate tothe problem domain.  Constraint mechanisms will be introduced which allow constraints to be strengthened or weakened as the system preceeds toward a solution.  The simulation system will be coupled to the expert system as a means of checking constraint satisfaction and graphically displaying the solution(s).  The integration of a simulation system with appropriate knowledge representation techniques should provide new insight into methods of constraint satisfaction, and also extend understanding of the design and behavior of hybrid systems.  n/a",A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION,3373570,R01LM004174,"['computer assisted medical decision making', ' computer assisted patient care', ' computer simulation', ' neoplasm /cancer radiation therapy', ' patient care planning', ' radiation dosage']",NLM,UNIVERSITY OF WASHINGTON,R01,1989,163123,0.010683163851677653
"A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION Radiation therapy for cancer is one of the first areas in which computers were widely used in support of clinical practice, dating back at least twenty years.  The initial application, still the major one today, was the calculation of radiation dose in the tumor region and in other locations of the patient's body.  However, knowledge about clinical implications of radiation treatment has been difficult to systematize in a form suitable for computer processing.  The number and range of treatment options is large.  Treatment optimization based on dose computations with variation of parameters can only handle a few of the possible options.  As an alternative approach to systematizing clinical knowledge of radiation effects, this project will develop an expert system for planning radiation therapy.  The problem of planning radiation therapy presents challenges in the design of expert systems.  Efficiency and flexibility are important because of the large problem space.  In addition, some of the knowledge of radiation treatment planning is most naturally expressed in the form of constraints, rather than production rules.  Most important, decision-making normally relies on a treatment simulation system to provide dose and geometric information about proposed treatment strategies.  To meet these requirements this project will develop and investigate the properties of an expert system that integrates with a treatmet simulation system.  The two systems will be coupled using a message passing technique previously developed for the simulation system internal operation.  The expert system will use both frames and production rules as appropriate tothe problem domain.  Constraint mechanisms will be introduced which allow constraints to be strengthened or weakened as the system preceeds toward a solution.  The simulation system will be coupled to the expert system as a means of checking constraint satisfaction and graphically displaying the solution(s).  The integration of a simulation system with appropriate knowledge representation techniques should provide new insight into methods of constraint satisfaction, and also extend understanding of the design and behavior of hybrid systems.  n/a",A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION,3373573,R01LM004174,"['computer assisted diagnosis', ' computer simulation', ' decision making', ' human therapy evaluation', ' information system analysis', ' neoplasm /cancer radiation therapy', ' radiation dosage', ' radiobiology']",NLM,UNIVERSITY OF WASHINGTON,R01,1986,95464,0.010683163851677653
"A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION Radiation therapy for cancer is one of the first areas in which computers were widely used in support of clinical practice, dating back at least twenty years.  The initial application, still the major one today, was the calculation of radiation dose in the tumor region and in other locations of the patient's body.  However, knowledge about clinical implications of radiation treatment has been difficult to systematize in a form suitable for computer processing.  The number and range of treatment options is large.  Treatment optimization based on dose computations with variation of parameters can only handle a few of the possible options.  As an alternative approach to systematizing clinical knowledge of radiation effects, this project will develop an expert system for planning radiation therapy.  The problem of planning radiation therapy presents challenges in the design of expert systems.  Efficiency and flexibility are important because of the large problem space.  In addition, some of the knowledge of radiation treatment planning is most naturally expressed in the form of constraints, rather than production rules.  Most important, decision-making normally relies on a treatment simulation system to provide dose and geometric information about proposed treatment strategies.  To meet these requirements this project will develop and investigate the properties of an expert system that integrates with a treatmet simulation system.  The two systems will be coupled using a message passing technique previously developed for the simulation system internal operation.  The expert system will use both frames and production rules as appropriate tothe problem domain.  Constraint mechanisms will be introduced which allow constraints to be strengthened or weakened as the system preceeds toward a solution.  The simulation system will be coupled to the expert system as a means of checking constraint satisfaction and graphically displaying the solution(s).  The integration of a simulation system with appropriate knowledge representation techniques should provide new insight into methods of constraint satisfaction, and also extend understanding of the design and behavior of hybrid systems.  n/a",A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION,3373572,R01LM004174,"['computer assisted diagnosis', ' computer simulation', ' decision making', ' human therapy evaluation', ' information system analysis', ' neoplasm /cancer radiation therapy', ' radiation dosage', ' radiobiology']",NLM,UNIVERSITY OF WASHINGTON,R01,1985,92557,0.010683163851677653
"Reconstruction Hardware for Real-Time Moving Tabel MRI    DESCRIPTION (provided by applicant):    The overall goal of this project is to design and integrate instrumentation allowing the next generation of real-time image formation in MRI. The motivation comes from an existing project for forming MR images during continuous motion of the patient table. When applied to peripheral MR angiography the target is the formation of a moving table angiogram in which the table motion is precisely matched to the transit of the contrast bolus through the patient. However, implementation of this requires that a number of mathematical processes be done at high speed, including: (i) time-resolved 3D MRI of an extended field of view (FOV); (ii) determination of localized time-dependent parameters such as bolus arrival time and bolus velocity; (iii) variable ordering of phase encodings over the course of an MRI scan, allowing optimized local resolution; (iv) gradient warping correction for MR acquisition done using a moving patient table; (v) multi-coil reconstruction using the SENSE technique, allowing improved lateral resolution for a given acquisition time; (vi) MR acquisition done using a variable table velocity When implemented, these methods will allow the formation of peripheral MR angiograms with optimized, patient-specific table motion, maximum efficiency, and high spatial resolution. Specific aims are: 1. Construction of the Next Generation Real-Time Image MR Reconstruction System. The funding will allow the construction of a system enabling the real-time performance of the mathematical algorithms which perform the above processes. System design will allow the data acquisition and reconstruction to be modified in real time. 2. Incorporation of the New System into the Project of Moving Table MRA. Once the hardware is integrated into a useable system it will be interfaced to a clinical MRI scanner at Mayo and used in the formation of peripheral contrast-enhanced MR angiograms using continuous motion of the patient table through the scanner gantry.         n/a",Reconstruction Hardware for Real-Time Moving Tabel MRI,6890990,R33EB004281,"['angiography', 'artificial intelligence', 'bioengineering /biomedical engineering', 'bioimaging /biomedical imaging', 'biomedical equipment development', 'clinical research', 'computer program /software', 'human subject', 'image processing', 'magnetic resonance imaging', 'mathematics', 'physics', 'technology /technique development', 'time resolved data']",NIBIB,MAYO CLINIC,R33,2005,330397,0.023746100604907432
"Reconstruction Hardware for Real-Time Moving Table MRI    DESCRIPTION (provided by applicant):    The overall goal of this project is to design and integrate instrumentation allowing the next generation of real-time image formation in MRI. The motivation comes from an existing project for forming MR images during continuous motion of the patient table. When applied to peripheral MR angiography the target is the formation of a moving table angiogram in which the table motion is precisely matched to the transit of the contrast bolus through the patient. However, implementation of this requires that a number of mathematical processes be done at high speed, including: (i) time-resolved 3D MRI of an extended field of view (FOV); (ii) determination of localized time-dependent parameters such as bolus arrival time and bolus velocity; (iii) variable ordering of phase encodings over the course of an MRI scan, allowing optimized local resolution; (iv) gradient warping correction for MR acquisition done using a moving patient table; (v) multi-coil reconstruction using the SENSE technique, allowing improved lateral resolution for a given acquisition time; (vi) MR acquisition done using a variable table velocity When implemented, these methods will allow the formation of peripheral MR angiograms with optimized, patient-specific table motion, maximum efficiency, and high spatial resolution. Specific aims are: 1. Construction of the Next Generation Real-Time Image MR Reconstruction System. The funding will allow the construction of a system enabling the real-time performance of the mathematical algorithms which perform the above processes. System design will allow the data acquisition and reconstruction to be modified in real time. 2. Incorporation of the New System into the Project of Moving Table MRA. Once the hardware is integrated into a useable system it will be interfaced to a clinical MRI scanner at Mayo and used in the formation of peripheral contrast-enhanced MR angiograms using continuous motion of the patient table through the scanner gantry.         n/a",Reconstruction Hardware for Real-Time Moving Table MRI,6783945,R33EB004281,"['angiography', 'artificial intelligence', 'bioengineering /biomedical engineering', 'bioimaging /biomedical imaging', 'biomedical equipment development', 'clinical research', 'computer program /software', 'human subject', 'image processing', 'magnetic resonance imaging', 'mathematics', 'physics', 'technology /technique development', 'time resolved data']",NIBIB,MAYO CLINIC,R33,2004,361371,0.023746100604907432
"VIRTUAL SIMULATION OF TEMPORAL BONE DISSECTION The broad, long-term objectives of this work are to integrate new and emerging hardware and software to create a robust, realistic virtual environment for synthesizing the techniques involved in temporal bone dissection.  To extend the functional use of the system, the integration of high-resolution data sets that are emerging from developments in in-vivo imaging protocols is investigated.  Recommendations to link the system with data from the National Temporal Bone Registry are made. The specific aims of this proposal are to develop, evaluate, and refine a concise, concentrated, and cost-effective simulation environment for a temporal bone dissection simulator.  The proposed simulation of the techniques of temporal bone dissection in a realistic, synthesized, interactive session will serve as an anatomical tutorial and a surgical technique simulator. Multimodal volumetric data will be integrated with haptic (force feedback) interfaces that will provide realistic feel to the resistances experienced in temporal bone dissection.  In addition, surgical simulate sounds of the instruments will be synthesized, thus providing an environment that integrates visual, aural, and haptic cues to the user.  The system design will encompass resident training, with extensibility to preoperative assessment, and surgical planning.  Extensibility for the system to serve as an interface for digital data sets acquired through the National Temporal Bone Registry, thus establishing a method for remote interaction, will be made. This work includes controlled trials with residents to evaluate the efficacy of the system for use in resident training. Specific tasks that are required in temporal bone dissection are tracked and recorded.  These tasks include time to task, identification, and quality of exposure.  Resident knowledge of logical and sequential progression through the techniques are determined.  An expert system that will provide active assessment of the user's technique is developed.  n/a",VIRTUAL SIMULATION OF TEMPORAL BONE DISSECTION,6379562,R21DC004515,"['bioimaging /biomedical imaging', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' dissection', ' ear surgery', ' educational resource design /development', ' human data', ' image processing', ' musculoskeletal imaging /visualization /scanning', ' sensory feedback', ' technology /technique development']",NIDCD,RESEARCH INST NATIONWIDE CHILDREN'S HOSP,R21,2001,65745,-0.0515556499188113
"VIRTUAL SIMULATION OF TEMPORAL BONE DISSECTION The broad, long-term objectives of this work are to integrate new and emerging hardware and software to create a robust, realistic virtual environment for synthesizing the techniques involved in temporal bone dissection.  To extend the functional use of the system, the integration of high-resolution data sets that are emerging from developments in in-vivo imaging protocols is investigated.  Recommendations to link the system with data from the National Temporal Bone Registry are made. The specific aims of this proposal are to develop, evaluate, and refine a concise, concentrated, and cost-effective simulation environment for a temporal bone dissection simulator.  The proposed simulation of the techniques of temporal bone dissection in a realistic, synthesized, interactive session will serve as an anatomical tutorial and a surgical technique simulator. Multimodal volumetric data will be integrated with haptic (force feedback) interfaces that will provide realistic feel to the resistances experienced in temporal bone dissection.  In addition, surgical simulate sounds of the instruments will be synthesized, thus providing an environment that integrates visual, aural, and haptic cues to the user.  The system design will encompass resident training, with extensibility to preoperative assessment, and surgical planning.  Extensibility for the system to serve as an interface for digital data sets acquired through the National Temporal Bone Registry, thus establishing a method for remote interaction, will be made. This work includes controlled trials with residents to evaluate the efficacy of the system for use in resident training. Specific tasks that are required in temporal bone dissection are tracked and recorded.  These tasks include time to task, identification, and quality of exposure.  Resident knowledge of logical and sequential progression through the techniques are determined.  An expert system that will provide active assessment of the user's technique is developed.  n/a",VIRTUAL SIMULATION OF TEMPORAL BONE DISSECTION,6143951,R21DC004515,"['bioimaging /biomedical imaging', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' dissection', ' ear surgery', ' educational resource design /development', ' human data', ' image processing', ' musculoskeletal imaging /visualization /scanning', ' sensory feedback', ' technology /technique development']",NIDCD,OHIO STATE UNIVERSITY,R21,2000,63988,-0.0515556499188113
"COMPUTER CONSULTANT FOR OPTIMAL NMR IMAGING Medical magnetic resonance imaging (MRI) is a new expensive imaging modality which avoids ionizing radiation and provides excellent diagnostic images.  The images produced depend on both the properties of the patient's tissues and the radiologist controlled parameters of the MRI scanner.  When scanner parameters are selected with clearcut objectives accounting for the patients's history, anatomic relationships and the MR properties of relevant tissues, accurate diagnostic images result. There are expert MRI radiologists who understand these tradeoffs, and use heuristic reasoning to simplify the mathematical and clinical complexity.  Their expertise can be encoded into a knowledge-based expert computer system.  Specifically, we propose to accomplish the following research:  1. Using existing methods of artificial intelligence and knowledge engineering, construct a first prototype expert system. It will be designed to incorporate the following functionality: o  Be a useful consultant for twenty representative situations involving the anatomy of the head, such as multiple sclerorsis, acoustic neuroma and pituitary adenoma. o  In a given clinical setting, produce a set of MR imaging parameters optimal for determining if a specific disease process is present. o  Be able to explain its reasoning in terms comprehensible to radiologists.  2.  Explore different inference strategies for reasoning about complex MRI physics interactions, such as constraint algorithms and mixed forward/backward chaining approaches.  3.  Build a user interface uniquely suited to the needs of radiologists engaged in MR imaging.  4.  Expand the knowledge of the prototype by incorporating changes in MR imaging technology and gradually including more anatomical regions.  5.  Develop automated knowledge acquisition strategies which help maintain the accuracy and completeness of the knowledge base.  This represents new research in artificial intelligence.  n/a",COMPUTER CONSULTANT FOR OPTIMAL NMR IMAGING,3474461,R29LM004707,"['artificial intelligence', ' diagnosis quality /standard', ' magnetic resonance imaging', ' noninvasive diagnosis', ' physics']",NLM,CARNEGIE-MELLON UNIVERSITY,R29,1991,98576,-0.027415992947388338
"COMPUTER CONSULTANT FOR OPTIMAL NMR IMAGING Medical magnetic resonance imaging (MRI) is a new expensive imaging modality which avoids ionizing radiation and provides excellent diagnostic images.  The images produced depend on both the properties of the patient's tissues and the radiologist controlled parameters of the MRI scanner.  When scanner parameters are selected with clearcut objectives accounting for the patients's history, anatomic relationships and the MR properties of relevant tissues, accurate diagnostic images result. There are expert MRI radiologists who understand these tradeoffs, and use heuristic reasoning to simplify the mathematical and clinical complexity.  Their expertise can be encoded into a knowledge-based expert computer system.  Specifically, we propose to accomplish the following research:  1. Using existing methods of artificial intelligence and knowledge engineering, construct a first prototype expert system. It will be designed to incorporate the following functionality: o  Be a useful consultant for twenty representative situations involving the anatomy of the head, such as multiple sclerorsis, acoustic neuroma and pituitary adenoma. o  In a given clinical setting, produce a set of MR imaging parameters optimal for determining if a specific disease process is present. o  Be able to explain its reasoning in terms comprehensible to radiologists.  2.  Explore different inference strategies for reasoning about complex MRI physics interactions, such as constraint algorithms and mixed forward/backward chaining approaches.  3.  Build a user interface uniquely suited to the needs of radiologists engaged in MR imaging.  4.  Expand the knowledge of the prototype by incorporating changes in MR imaging technology and gradually including more anatomical regions.  5.  Develop automated knowledge acquisition strategies which help maintain the accuracy and completeness of the knowledge base.  This represents new research in artificial intelligence.  n/a",COMPUTER CONSULTANT FOR OPTIMAL NMR IMAGING,3474460,R29LM004707,"['artificial intelligence', ' diagnosis quality /standard', ' magnetic resonance imaging', ' noninvasive diagnosis', ' physics']",NLM,CARNEGIE-MELLON UNIVERSITY,R29,1990,94893,-0.027415992947388338
"COMPUTER CONSULTANT FOR OPTIMAL NMR IMAGING Medical magnetic resonance imaging (MRI) is a new expensive imaging modality which avoids ionizing radiation and provides excellent diagnostic images.  The images produced depend on both the properties of the patient's tissues and the radiologist controlled parameters of the MRI scanner.  When scanner parameters are selected with clearcut objectives accounting for the patients's history, anatomic relationships and the MR properties of relevant tissues, accurate diagnostic images result. There are expert MRI radiologists who understand these tradeoffs, and use heuristic reasoning to simplify the mathematical and clinical complexity.  Their expertise can be encoded into a knowledge-based expert computer system.  Specifically, we propose to accomplish the following research:  1. Using existing methods of artificial intelligence and knowledge engineering, construct a first prototype expert system. It will be designed to incorporate the following functionality: o  Be a useful consultant for twenty representative situations involving the anatomy of the head, such as multiple sclerorsis, acoustic neuroma and pituitary adenoma. o  In a given clinical setting, produce a set of MR imaging parameters optimal for determining if a specific disease process is present. o  Be able to explain its reasoning in terms comprehensible to radiologists.  2.  Explore different inference strategies for reasoning about complex MRI physics interactions, such as constraint algorithms and mixed forward/backward chaining approaches.  3.  Build a user interface uniquely suited to the needs of radiologists engaged in MR imaging.  4.  Expand the knowledge of the prototype by incorporating changes in MR imaging technology and gradually including more anatomical regions.  5.  Develop automated knowledge acquisition strategies which help maintain the accuracy and completeness of the knowledge base.  This represents new research in artificial intelligence.  n/a",COMPUTER CONSULTANT FOR OPTIMAL NMR IMAGING,3474459,R29LM004707,"['artificial intelligence', ' diagnosis quality /standard', ' magnetic resonance imaging', ' noninvasive diagnosis', ' physics']",NLM,CARNEGIE-MELLON UNIVERSITY,R29,1989,90196,-0.027415992947388338
"COMPUTER CONSULTANT FOR OPTIMAL NMR IMAGING Medical magnetic resonance imaging (MRI) is a new expensive imaging modality which avoids ionizing radiation and provides excellent diagnostic images.  The images produced depend on both the properties of the patient's tissues and the radiologist controlled parameters of the MRI scanner.  When scanner parameters are selected with clearcut objectives accounting for the patients's history, anatomic relationships and the MR properties of relevant tissues, accurate diagnostic images result. There are expert MRI radiologists who understand these tradeoffs, and use heuristic reasoning to simplify the mathematical and clinical complexity.  Their expertise can be encoded into a knowledge-based expert computer system.  Specifically, we propose to accomplish the following research:  1. Using existing methods of artificial intelligence and knowledge engineering, construct a first prototype expert system. It will be designed to incorporate the following functionality: o  Be a useful consultant for twenty representative situations involving the anatomy of the head, such as multiple sclerorsis, acoustic neuroma and pituitary adenoma. o  In a given clinical setting, produce a set of MR imaging parameters optimal for determining if a specific disease process is present. o  Be able to explain its reasoning in terms comprehensible to radiologists.  2.  Explore different inference strategies for reasoning about complex MRI physics interactions, such as constraint algorithms and mixed forward/backward chaining approaches.  3.  Build a user interface uniquely suited to the needs of radiologists engaged in MR imaging.  4.  Expand the knowledge of the prototype by incorporating changes in MR imaging technology and gradually including more anatomical regions.  5.  Develop automated knowledge acquisition strategies which help maintain the accuracy and completeness of the knowledge base.  This represents new research in artificial intelligence.  n/a",COMPUTER CONSULTANT FOR OPTIMAL NMR IMAGING,3474458,R29LM004707,"['artificial intelligence', ' diagnosis quality /standard', ' magnetic resonance imaging', ' noninvasive diagnosis', ' physics']",NLM,CARNEGIE-MELLON UNIVERSITY,R29,1988,77754,-0.027415992947388338
"COMPUTER CONSULTANT FOR OPTIMAL NMR IMAGING Medical magnetic resonance imaging (MRI) is a new expensive imaging modality which avoids ionizing radiation and provides excellent diagnostic images.  The images produced depend on both the properties of the patient's tissues and the radiologist controlled parameters of the MRI scanner.  When scanner parameters are selected with clearcut objectives accounting for the patients's history, anatomic relationships and the MR properties of relevant tissues, accurate diagnostic images result. There are expert MRI radiologists who understand these tradeoffs, and use heuristic reasoning to simplify the mathematical and clinical complexity.  Their expertise can be encoded into a knowledge-based expert computer system.  Specifically, we propose to accomplish the following research:  1. Using existing methods of artificial intelligence and knowledge engineering, construct a first prototype expert system. It will be designed to incorporate the following functionality: o  Be a useful consultant for twenty representative situations involving the anatomy of the head, such as multiple sclerorsis, acoustic neuroma and pituitary adenoma. o  In a given clinical setting, produce a set of MR imaging parameters optimal for determining if a specific disease process is present. o  Be able to explain its reasoning in terms comprehensible to radiologists.  2.  Explore different inference strategies for reasoning about complex MRI physics interactions, such as constraint algorithms and mixed forward/backward chaining approaches.  3.  Build a user interface uniquely suited to the needs of radiologists engaged in MR imaging.  4.  Expand the knowledge of the prototype by incorporating changes in MR imaging technology and gradually including more anatomical regions.  5.  Develop automated knowledge acquisition strategies which help maintain the accuracy and completeness of the knowledge base.  This represents new research in artificial intelligence.  n/a",COMPUTER CONSULTANT FOR OPTIMAL NMR IMAGING,3474457,R29LM004707,"['artificial intelligence', ' diagnosis quality /standard', ' noninvasive diagnosis', ' physics']",NLM,CARNEGIE-MELLON UNIVERSITY,R29,1987,111415,-0.027415992947388338
"THE ROLE OF THE HAPTIC SYSTEM IN COMMUNICATION The major research effort in this project is directed at the fundamental problem of the processing of complex tactile stimulus patterns in real-time for use as correlates of environmental information by handicapped or normal persons working under a sensory overload. To determine what characteristics of patterns provide reliable and rapidly processed units of information, a computer-controlled vibrotactile matrix has been constructed to permit presentation of a wide variety of frequencies, amplitudes, and time relations of tactile stimuli over a spatial display of 64 independently controlled vibrators.  Present explorations of promising pattern dimensions involve serial presentations of patterns for discrimination and identification, but future work will involve pattern production and modification in dynamic simulated environmental representations.  A variety of basic problems that have appeared as by-products of the main effort also receive attention, viz., threshold and ""loudness"" summation in the presence of multiple contactors, spatial mislocalizations as space-time trade-offs, judgments of texture and distance on the skin, and the influence of mechanical skin characteristics on basic psychophysical functions.   n/a",THE ROLE OF THE HAPTIC SYSTEM IN COMMUNICATION,3393333,R01NS004755,"['artificial intelligence', ' communication disorder aid', ' cutaneous sensory nerve', ' electrophysiology', ' human subject', ' neural information processing', ' paralinguistic behavior', ' person with disability', ' psychophysics', ' sensory thresholds', ' sequential perception', ' somesthesis', ' space perception', ' time perception', ' touch', ' vibration perception']",NINDS,PRINCETON UNIVERSITY,R01,1985,184951,-0.03713625451710655
"SYMBOLIC SIMULATION OF DNA METABOLISM Using established methods of knowledge representation and rule- based expert systems I propose to develop a symbolic simulation of DNA metabolism.  Rule-based expert systems permit simulations of DNA metabolism which can predict the activity of both individual enzymes and of entire pathways under a wide variety of physiological conditions.  Both classical production rules and an assumption-based truth maintenance system will be used.  Rule-based methods have the intrinsic property of being able to explain conclusions using English sentences or graphic representations of the pattern of inference.  The enzymes, substrates and products will also be represented in a frame-based knowledge system.  Knowledge based systems facilitate the description of product-precursor relationships between different metabolites as well as the complete pathway.  Physiological conditions such as the presence of salts, nucleotides, temperature, ionic strength, pH, etc. will be presented graphically as active images of thermometers, gauges and switches.  All environmental and substrate conditions can be changed by using a mouse pointing device.  In response to changes of any parameter, a series of rules defining the specificity and reactivity of enzymes are automatically invoked and specific conclusions are made. The methods for specifying the actions of enzymes are well defined, allowing ready simulation of other enzymes.  Simulations of individual enzymatic steps can be coupled together to generate a discrete event simulation of the entire metabolic pathway.  n/a",SYMBOLIC SIMULATION OF DNA METABOLISM,3374064,R01LM004957,"['DNA', ' artificial intelligence', ' chemical models', ' computer simulation', ' decision making', ' enzyme mechanism', ' enzyme substrate', ' nucleic acid metabolism']",NLM,STANFORD UNIVERSITY,R01,1990,203276,-0.05491960511997714
"SYMBOLIC SIMULATION OF DNA METABOLISM Using established methods of knowledge representation and rule- based expert systems I propose to develop a symbolic simulation of DNA metabolism.  Rule-based expert systems permit simulations of DNA metabolism which can predict the activity of both individual enzymes and of entire pathways under a wide variety of physiological conditions.  Both classical production rules and an assumption-based truth maintenance system will be used.  Rule-based methods have the intrinsic property of being able to explain conclusions using English sentences or graphic representations of the pattern of inference.  The enzymes, substrates and products will also be represented in a frame-based knowledge system.  Knowledge based systems facilitate the description of product-precursor relationships between different metabolites as well as the complete pathway.  Physiological conditions such as the presence of salts, nucleotides, temperature, ionic strength, pH, etc. will be presented graphically as active images of thermometers, gauges and switches.  All environmental and substrate conditions can be changed by using a mouse pointing device.  In response to changes of any parameter, a series of rules defining the specificity and reactivity of enzymes are automatically invoked and specific conclusions are made. The methods for specifying the actions of enzymes are well defined, allowing ready simulation of other enzymes.  Simulations of individual enzymatic steps can be coupled together to generate a discrete event simulation of the entire metabolic pathway.  n/a",SYMBOLIC SIMULATION OF DNA METABOLISM,3374063,R01LM004957,"['DNA', ' artificial intelligence', ' chemical models', ' computer simulation', ' decision making', ' enzyme mechanism', ' enzyme substrate', ' nucleic acid metabolism']",NLM,STANFORD UNIVERSITY,R01,1989,178695,-0.05491960511997714
"SYMBOLIC SIMULATION OF DNA METABOLISM Using established methods of knowledge representation and rule- based expert systems I propose to develop a symbolic simulation of DNA metabolism.  Rule-based expert systems permit simulations of DNA metabolism which can predict the activity of both individual enzymes and of entire pathways under a wide variety of physiological conditions.  Both classical production rules and an assumption-based truth maintenance system will be used.  Rule-based methods have the intrinsic property of being able to explain conclusions using English sentences or graphic representations of the pattern of inference.  The enzymes, substrates and products will also be represented in a frame-based knowledge system.  Knowledge based systems facilitate the description of product-precursor relationships between different metabolites as well as the complete pathway.  Physiological conditions such as the presence of salts, nucleotides, temperature, ionic strength, pH, etc. will be presented graphically as active images of thermometers, gauges and switches.  All environmental and substrate conditions can be changed by using a mouse pointing device.  In response to changes of any parameter, a series of rules defining the specificity and reactivity of enzymes are automatically invoked and specific conclusions are made. The methods for specifying the actions of enzymes are well defined, allowing ready simulation of other enzymes.  Simulations of individual enzymatic steps can be coupled together to generate a discrete event simulation of the entire metabolic pathway.  n/a",SYMBOLIC SIMULATION OF DNA METABOLISM,3374059,R01LM004957,"['DNA', ' artificial intelligence', ' chemical models', ' computer simulation', ' decision making', ' enzyme mechanism', ' enzyme substrate', ' nucleic acid metabolism']",NLM,STANFORD UNIVERSITY,R01,1988,146913,-0.05491960511997714
"FAST BROWSING OF PACS IMAGE ARCHIVE VIA NETWORK THIS IS A SHANNON AWARD PROVIDING PARTIAL SUPPORT FOR THE RESEARCH               PROJECTS THAT FALL SHORT OF THE ASSIGNED INSTITUTE'S FUNDING RANGE BUT           ARE IN THE MARGIN OF EXCELLENCE. THE SHANNON AWARD IS INTENDED TO PROVIDE        SUPPORT TO TEST THE FEASIBILITY OF THE APPROACH; DEVELOP FURTHER TESTS           AND REFINE RESEARCH TECHNIQUES; PERFORM SECONDARY ANALYSIS OR AVAILABLE          DATA SETS; OR CONDUCT DISCRETE PROJECTS THAT CAN DEMONSTRATE THE PI'S            RESEARCH CAPABILITIES OR LEND ADDITIONAL WEIGHT TO AN ALREADY MERITORIOUS        APPLICATION. THE ABSTRACT BELOW IS TAKEN FROM THE ORIGINAL DOCUMENT              SUBMITTED BY THE PRINCIPAL INVESTIGATOR.                                                                                                                          DESCRIPTION:  (adapted from the application abstract)  The advent of             large  picture archiving and communication systems (PACS) will likely            result in a  conversion of clinical radiology to become nearly completely        digital. Improved methods for access to a large collection of on-line            image data can  improve medical research and education.  Although the            techniques for fast text  search are well established, similar tool              development for rapidly searching  through years of digitally archived           images is still in its infancy. Development of a fast browsing technology        for retrieving archived images over  a network, both local and remote            (international) accesses is proposed.                                                                                                                             In this project, three key technologies will be merged:  smart query,            hierarchical archive, and photo indexing using embedded  zerotree wavelet        transform (EZTWT) code.  The first two technologies provides query               constraint  and automatic image migration management, and are supported          in other PACS- related projects at UCLA.  The third and newest component,        EZTWT, is an  encoding method designed for maximum network utility,              providing the receiver  highest image quality for a given transmission           time.  The implications for  high-performance teleradiology under                bandwidth limitations are significant.  Diagnostic viewing can start             before transmission of an image in full resolution and quality is                complete.  This encoding scheme will be modified to  allow variable size         iconic representation of the original, achieving minimum  network delay          and fast reconstruction, thus making browsing through a larger  selection        of images convenient and manageable.                                                                                                                              In addition, the modified EZTWT can achieve high efficiency for images           with  clearly separated anatomical and background regions.  The space            saving in very  high resolution radiographs, such as mammograms, can be          typically 7MB per  4Kx4K image (44%) without loss in anatomical                  information.  This coding  technique will be tested in conjunction with          new pattern recognition techniques to achieve efficient mammogram storage        and telecommunication.                                                            n/a",FAST BROWSING OF PACS IMAGE ARCHIVE VIA NETWORK,2329566,R55LM005712,"['abstracting', ' archives', ' artificial intelligence', ' bioimaging /biomedical imaging', ' computer assisted diagnosis', ' computer assisted instruction', ' computer graphics /printing', ' digital imaging', ' informatics', ' online computer', ' radiology', ' telemedicine']",NLM,MASSACHUSETTS GENERAL HOSPITAL,R55,1996,100000,-0.03138802615355293
"MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES The long term goal of our research is to understand the flow of                  information from the genome to the phenotype of organisms. In this               proposal, we will attempt to use Bayesian networks and near-optimal              sequence alignments to represent protein secondary structures and motifs.        A Bayesian network describes the likelihood of amino acids at each               position in a motif as well as the dependence of amino acids in one              position on the amino acids at other position. Hence, Bayesian networks          can describe both the conservation of amino acids at single positions and        the conservation of correlations between two positions simultaneously.                                                                                            Conserved amino acids result from evolutionary selection for a specific          amino acid or type of amino acid at one position in a protein structure.         These positions often have important functional or structural                    requirements. Correlated changes between amino acids generally result from       side-chain side-chain interactions between pairs of amino acids in a             protein's structure. The types of correlations we have represented with          Bayesian networks include electrostatic charges, hydrophobicity, hydrogen-       bond donor and acceptor and inversely correlated packing volumes among           others. These Bayesian networks can be used to 1) discover side-chain            side--chain interactions within protei motifs and 2) to search sequence          databases for motifs showing both correlations and conserved amino acids.                                                                                         Near-optimal alignments between two sequences can display regions that           have been more highly conserved or less highly conserved using the               information contained in only two sequences. The most highly conserved           region correspond to the most highly structured regions and the most             highly variable regions correspond to loops and coils and other                  hypervariable regions. We propose to use near-optimal alignments to              display conserved secondary structures of proteins and hypervariable             regions. We will use secondary-structure specific amino acid substitution        matrices to provide specificity.                                                                                                                                  The goals of this proposal are to 1) build a database of Bayesian networks       that represent protein motifs, 2) test these networks for their ability to       detect motifs using test sets and crossvalidation methods, 3) compare            these networks with other methods for searching protein databases , 4)           build an integrated set of Bayesian networks to predict protein secondary        structure, 5) compare the prediction of protein secondary structure with         existing method 6) build a near-optimal sequence alignment workbench, and        7) predict structured and unstructured regions in proteins from near-            optimal alignments.                                                               n/a",MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES,6146063,R01LM005716,"['artificial intelligence', ' biochemical evolution', ' computer assisted sequence analysis', ' hydrogen bond', ' hydropathy', ' ionic bond', ' model design /development', ' molecular biology information system', ' physical model', ' protein sequence', ' protein structure function', ' structural biology']",NLM,STANFORD UNIVERSITY,R01,1999,56010,-0.03747261015956343
"MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES The long term goal of our research is to understand the flow of                  information from the genome to the phenotype of organisms. In this               proposal, we will attempt to use Bayesian networks and near-optimal              sequence alignments to represent protein secondary structures and motifs.        A Bayesian network describes the likelihood of amino acids at each               position in a motif as well as the dependence of amino acids in one              position on the amino acids at other position. Hence, Bayesian networks          can describe both the conservation of amino acids at single positions and        the conservation of correlations between two positions simultaneously.                                                                                            Conserved amino acids result from evolutionary selection for a specific          amino acid or type of amino acid at one position in a protein structure.         These positions often have important functional or structural                    requirements. Correlated changes between amino acids generally result from       side-chain side-chain interactions between pairs of amino acids in a             protein's structure. The types of correlations we have represented with          Bayesian networks include electrostatic charges, hydrophobicity, hydrogen-       bond donor and acceptor and inversely correlated packing volumes among           others. These Bayesian networks can be used to 1) discover side-chain            side--chain interactions within protei motifs and 2) to search sequence          databases for motifs showing both correlations and conserved amino acids.                                                                                         Near-optimal alignments between two sequences can display regions that           have been more highly conserved or less highly conserved using the               information contained in only two sequences. The most highly conserved           region correspond to the most highly structured regions and the most             highly variable regions correspond to loops and coils and other                  hypervariable regions. We propose to use near-optimal alignments to              display conserved secondary structures of proteins and hypervariable             regions. We will use secondary-structure specific amino acid substitution        matrices to provide specificity.                                                                                                                                  The goals of this proposal are to 1) build a database of Bayesian networks       that represent protein motifs, 2) test these networks for their ability to       detect motifs using test sets and crossvalidation methods, 3) compare            these networks with other methods for searching protein databases , 4)           build an integrated set of Bayesian networks to predict protein secondary        structure, 5) compare the prediction of protein secondary structure with         existing method 6) build a near-optimal sequence alignment workbench, and        7) predict structured and unstructured regions in proteins from near-            optimal alignments.                                                               n/a",MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES,2771696,R01LM005716,"['artificial intelligence', ' biochemical evolution', ' chemical information system', ' computer assisted sequence analysis', ' hydrogen bond', ' hydropathy', ' ionic bond', ' model design /development', ' physical model', ' protein sequence', ' protein structure function', ' structural biology']",NLM,STANFORD UNIVERSITY,R01,1998,222416,-0.03747261015956343
"MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES The long term goal of our research is to understand the flow of                  information from the genome to the phenotype of organisms. In this               proposal, we will attempt to use Bayesian networks and near-optimal              sequence alignments to represent protein secondary structures and motifs.        A Bayesian network describes the likelihood of amino acids at each               position in a motif as well as the dependence of amino acids in one              position on the amino acids at other position. Hence, Bayesian networks          can describe both the conservation of amino acids at single positions and        the conservation of correlations between two positions simultaneously.                                                                                            Conserved amino acids result from evolutionary selection for a specific          amino acid or type of amino acid at one position in a protein structure.         These positions often have important functional or structural                    requirements. Correlated changes between amino acids generally result from       side-chain side-chain interactions between pairs of amino acids in a             protein's structure. The types of correlations we have represented with          Bayesian networks include electrostatic charges, hydrophobicity, hydrogen-       bond donor and acceptor and inversely correlated packing volumes among           others. These Bayesian networks can be used to 1) discover side-chain            side--chain interactions within protei motifs and 2) to search sequence          databases for motifs showing both correlations and conserved amino acids.                                                                                         Near-optimal alignments between two sequences can display regions that           have been more highly conserved or less highly conserved using the               information contained in only two sequences. The most highly conserved           region correspond to the most highly structured regions and the most             highly variable regions correspond to loops and coils and other                  hypervariable regions. We propose to use near-optimal alignments to              display conserved secondary structures of proteins and hypervariable             regions. We will use secondary-structure specific amino acid substitution        matrices to provide specificity.                                                                                                                                  The goals of this proposal are to 1) build a database of Bayesian networks       that represent protein motifs, 2) test these networks for their ability to       detect motifs using test sets and crossvalidation methods, 3) compare            these networks with other methods for searching protein databases , 4)           build an integrated set of Bayesian networks to predict protein secondary        structure, 5) compare the prediction of protein secondary structure with         existing method 6) build a near-optimal sequence alignment workbench, and        7) predict structured and unstructured regions in proteins from near-            optimal alignments.                                                               n/a",MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES,2519669,R01LM005716,"['artificial intelligence', ' biochemical evolution', ' chemical information system', ' computer assisted sequence analysis', ' hydrogen bond', ' hydropathy', ' ionic bond', ' model design /development', ' physical model', ' protein sequence', ' protein structure function', ' structural biology']",NLM,STANFORD UNIVERSITY,R01,1997,214055,-0.03747261015956343
"MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES The long term goal of our research is to understand the flow of  information from the genome to the phenotype of organisms. In this  proposal, we will attempt to use Bayesian networks and near-optimal  sequence alignments to represent protein secondary structures and motifs.  A Bayesian network describes the likelihood of amino acids at each  position in a motif as well as the dependence of amino acids in one  position on the amino acids at other position. Hence, Bayesian networks  can describe both the conservation of amino acids at single positions and  the conservation of correlations between two positions simultaneously.    Conserved amino acids result from evolutionary selection for a specific  amino acid or type of amino acid at one position in a protein structure.  These positions often have important functional or structural  requirements. Correlated changes between amino acids generally result from  side-chain side-chain interactions between pairs of amino acids in a  protein's structure. The types of correlations we have represented with  Bayesian networks include electrostatic charges, hydrophobicity, hydrogen-  bond donor and acceptor and inversely correlated packing volumes among  others. These Bayesian networks can be used to 1) discover side-chain  side--chain interactions within protei motifs and 2) to search sequence  databases for motifs showing both correlations and conserved amino acids.    Near-optimal alignments between two sequences can display regions that  have been more highly conserved or less highly conserved using the  information contained in only two sequences. The most highly conserved  region correspond to the most highly structured regions and the most  highly variable regions correspond to loops and coils and other  hypervariable regions. We propose to use near-optimal alignments to  display conserved secondary structures of proteins and hypervariable  regions. We will use secondary-structure specific amino acid substitution  matrices to provide specificity.    The goals of this proposal are to 1) build a database of Bayesian networks  that represent protein motifs, 2) test these networks for their ability to  detect motifs using test sets and crossvalidation methods, 3) compare  these networks with other methods for searching protein databases , 4)  build an integrated set of Bayesian networks to predict protein secondary  structure, 5) compare the prediction of protein secondary structure with  existing method 6) build a near-optimal sequence alignment workbench, and  7) predict structured and unstructured regions in proteins from near-  optimal alignments.  n/a",MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES,2238097,R01LM005716,"['artificial intelligence', ' biochemical evolution', ' chemical information system', ' computer assisted sequence analysis', ' hydrogen bond', ' hydropathy', ' ionic bond', ' model design /development', ' physical model', ' protein sequence', ' protein structure function', ' structural biology']",NLM,STANFORD UNIVERSITY,R01,1995,204452,-0.03747261015956343
"MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES The long term goal of our research is to understand the flow of information from the genome to the phenotype of organisms. In this proposal, we will attempt to use Bayesian networks and near-optimal sequence alignments to represent protein secondary structures and motifs. A Bayesian network describes the likelihood of amino acids at each position in a motif as well as the dependence of amino acids in one position on the amino acids at other position. Hence, Bayesian networks can describe both the conservation of amino acids at single positions and the conservation of correlations between two positions simultaneously.  Conserved amino acids result from evolutionary selection for a specific amino acid or type of amino acid at one position in a protein structure. These positions often have important functional or structural requirements. Correlated changes between amino acids generally result from side-chain side-chain interactions between pairs of amino acids in a protein's structure. The types of correlations we have represented with Bayesian networks include electrostatic charges, hydrophobicity, hydrogen- bond donor and acceptor and inversely correlated packing volumes among others. These Bayesian networks can be used to 1) discover side-chain side--chain interactions within protei motifs and 2) to search sequence databases for motifs showing both correlations and conserved amino acids.  Near-optimal alignments between two sequences can display regions that have been more highly conserved or less highly conserved using the information contained in only two sequences. The most highly conserved region correspond to the most highly structured regions and the most highly variable regions correspond to loops and coils and other hypervariable regions. We propose to use near-optimal alignments to display conserved secondary structures of proteins and hypervariable regions. We will use secondary-structure specific amino acid substitution matrices to provide specificity.  The goals of this proposal are to 1) build a database of Bayesian networks that represent protein motifs, 2) test these networks for their ability to detect motifs using test sets and crossvalidation methods, 3) compare these networks with other methods for searching protein databases , 4) build an integrated set of Bayesian networks to predict protein secondary structure, 5) compare the prediction of protein secondary structure with existing method 6) build a near-optimal sequence alignment workbench, and 7) predict structured and unstructured regions in proteins from near- optimal alignments.  n/a",MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES,2238096,R01LM005716,"['artificial intelligence', ' biochemical evolution', ' chemical information system', ' computer assisted sequence analysis', ' hydrogen bond', ' hydropathy', ' ionic bond', ' model design /development', ' physical model', ' protein sequence', ' protein structure function', ' structural biology']",NLM,STANFORD UNIVERSITY,R01,1994,198777,-0.03747261015956343
"Comprehensive Positioning Management System    DESCRIPTION (provided by applicant):    We plan to develop and refine a comprehensive positional management system (CPM System) for positional manifestations of vestibule disorders for widespread application at medical and research facilities. Utilizing hardware that can carry out automated positioning of the subject in a precise manner (for testing or treatment), coupled with state-of-the-art 3-D eye movement recording and analysis methods, the system will display the data, provide interpretive guidelines and assist in carrying out complex maneuvers. During phase I we will modify our prototype system these improvements to provide sophisticated motion control along 3 axes and eye movement analysis capabilities.  Basic software will be developed for interactive display and control with the ability to review data from the basic management-guidance system. We will also demonstrate the value of the improved system by performing feasibility studies in a small cohort of subjects with vertigo/imbalance problems (n=20). We will validate performance by determining the added value that the CPM System brings as compared to diagnoses and treatment using the best commercially available equipment.  During phase 2, we anticipate that engineering modifications will be made based upon phase 1 findings. We will also incorporate state-of-the-art systems for rotational and graviception tests; and a knowledge-based expert system.  Production models will be built and installed in several beta sites for conducting human studies in a larger subject population to obtain more definitive validation and user acceptance testing.         n/a",Comprehensive Positioning Management System,6644649,R43DC006157,"['actigraphy', ' bioengineering /biomedical engineering', ' biomedical equipment development', ' body movement', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' eye movements', ' human subject', ' infrared radiation', ' motion perception', ' space perception', ' vertigo']",NIDCD,VESTICON,R43,2003,175924,-0.053793260059251446
"STRUCTURE BASED VISUAL ACCESS TO BIOMEDICAL INFORMATION DESCRIPTION (Taken from application abstract):  The long term goal of the        work described in this proposal is to develop on-line combined spatial and       symbolic methods for representing, storing, retrieving and visualizing           anatomical information, both as a means for understanding human biological       structure, and as a visual gateway into the rapidly increasing array of          on-line text-based information sources in biomedicine.  In this proposal we      will address some of the fundamental problems involved in combining spatial      and symbolic anatomic information, and will test solutions to these problems     in a World Wide Web based 3-D anatomy information system for the thoracic        viscera.  Clinicians, researchers and students will be able to use this          system to retrieve specific anatomic knowledge, in the form of                   dynamically-generated interactive 3-D scenes and corresponding symbolic          information, without the need to consult hard copy atlases or to navigate        through irrelevant computer-based images before finding the needed               information.  In order to build this system we will need to address              fundamental problems in spatial modeling, organization of these models in a      knowledge based spatial database system, and access to the models via an         on-line user interface and spatial query processor.  The specific aims for       this proposal are:  1) develop a knowledge base that organizes and               integrates spatial and symbolic models of anatomy, 2) implement an anatomy       information system that combines knowledge-based spatial and symbolic            retrieval with dynamically generated 3-D scenes, 3) develop methods for          smoothly rendering and interacting with the scene in real-time, and 4)           evaluate the system by integrating other spatial data and by providing it to     anatomy medical students and radiation treatment planners.  Accomplishment       of these aims will lead to a useful information system for the anatomy of        the thoracic viscera that can be enhanced with new technology such as high       performance graphics and virtual reality.  Moreover, the information             framework and methods we establish in this project will be generalizable not     only to gross anatomy in different regions of the body and to different          anatomical databases, but also to the management of structural information       pertaining to cellular and molecular biology, as well as developmental and       neurobiology.                                                                     n/a",STRUCTURE BASED VISUAL ACCESS TO BIOMEDICAL INFORMATION,2897386,R01LM006316,"['DVD /CD ROM', ' anatomy', ' artificial intelligence', ' computer human interaction', ' computer system design /evaluation', ' information retrieval', ' information systems']",NLM,UNIVERSITY OF WASHINGTON,R01,1999,296940,-0.024854437161902106
"STRUCTURE BASED VISUAL ACCESS TO BIOMEDICAL INFORMATION DESCRIPTION (Taken from application abstract):  The long term goal of the        work described in this proposal is to develop on-line combined spatial and       symbolic methods for representing, storing, retrieving and visualizing           anatomical information, both as a means for understanding human biological       structure, and as a visual gateway into the rapidly increasing array of          on-line text-based information sources in biomedicine.  In this proposal we      will address some of the fundamental problems involved in combining spatial      and symbolic anatomic information, and will test solutions to these problems     in a World Wide Web based 3-D anatomy information system for the thoracic        viscera.  Clinicians, researchers and students will be able to use this          system to retrieve specific anatomic knowledge, in the form of                   dynamically-generated interactive 3-D scenes and corresponding symbolic          information, without the need to consult hard copy atlases or to navigate        through irrelevant computer-based images before finding the needed               information.  In order to build this system we will need to address              fundamental problems in spatial modeling, organization of these models in a      knowledge based spatial database system, and access to the models via an         on-line user interface and spatial query processor.  The specific aims for       this proposal are:  1) develop a knowledge base that organizes and               integrates spatial and symbolic models of anatomy, 2) implement an anatomy       information system that combines knowledge-based spatial and symbolic            retrieval with dynamically generated 3-D scenes, 3) develop methods for          smoothly rendering and interacting with the scene in real-time, and 4)           evaluate the system by integrating other spatial data and by providing it to     anatomy medical students and radiation treatment planners.  Accomplishment       of these aims will lead to a useful information system for the anatomy of        the thoracic viscera that can be enhanced with new technology such as high       performance graphics and virtual reality.  Moreover, the information             framework and methods we establish in this project will be generalizable not     only to gross anatomy in different regions of the body and to different          anatomical databases, but also to the management of structural information       pertaining to cellular and molecular biology, as well as developmental and       neurobiology.                                                                     n/a",STRUCTURE BASED VISUAL ACCESS TO BIOMEDICAL INFORMATION,2702873,R01LM006316,"['DVD /CD ROM', ' anatomy', ' artificial intelligence', ' computer human interaction', ' computer system design /evaluation', ' information retrieval', ' information systems']",NLM,UNIVERSITY OF WASHINGTON,R01,1998,296107,-0.024854437161902106
"STRUCTURE BASED VISUAL ACCESS TO BIOMEDICAL INFORMATION DESCRIPTION (Taken from application abstract):  The long term goal of the        work described in this proposal is to develop on-line combined spatial and       symbolic methods for representing, storing, retrieving and visualizing           anatomical information, both as a means for understanding human biological       structure, and as a visual gateway into the rapidly increasing array of          on-line text-based information sources in biomedicine.  In this proposal we      will address some of the fundamental problems involved in combining spatial      and symbolic anatomic information, and will test solutions to these problems     in a World Wide Web based 3-D anatomy information system for the thoracic        viscera.  Clinicians, researchers and students will be able to use this          system to retrieve specific anatomic knowledge, in the form of                   dynamically-generated interactive 3-D scenes and corresponding symbolic          information, without the need to consult hard copy atlases or to navigate        through irrelevant computer-based images before finding the needed               information.  In order to build this system we will need to address              fundamental problems in spatial modeling, organization of these models in a      knowledge based spatial database system, and access to the models via an         on-line user interface and spatial query processor.  The specific aims for       this proposal are:  1) develop a knowledge base that organizes and               integrates spatial and symbolic models of anatomy, 2) implement an anatomy       information system that combines knowledge-based spatial and symbolic            retrieval with dynamically generated 3-D scenes, 3) develop methods for          smoothly rendering and interacting with the scene in real-time, and 4)           evaluate the system by integrating other spatial data and by providing it to     anatomy medical students and radiation treatment planners.  Accomplishment       of these aims will lead to a useful information system for the anatomy of        the thoracic viscera that can be enhanced with new technology such as high       performance graphics and virtual reality.  Moreover, the information             framework and methods we establish in this project will be generalizable not     only to gross anatomy in different regions of the body and to different          anatomical databases, but also to the management of structural information       pertaining to cellular and molecular biology, as well as developmental and       neurobiology.                                                                     n/a",STRUCTURE BASED VISUAL ACCESS TO BIOMEDICAL INFORMATION,2032431,R01LM006316,"['DVD /CD ROM', ' anatomy', ' artificial intelligence', ' computer human interaction', ' computer system design /evaluation', ' information retrieval', ' information systems']",NLM,UNIVERSITY OF WASHINGTON,R01,1997,298924,-0.024854437161902106
"THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES DESCRIPTION (Taken from application abstract):  Recent advances in               understanding neuronal functions have led to an expanded scientific interest     in defining the organization of the nervous system so the spatial correlates     of these functions can be identified.  This interest has been formalized as      the Human Brain Project, an ambitious multi disciplinary effort to map the       nervous system from the organismal to the macromolecular levels.  One of the     greatest challenges of this effort is to preserve the complex                    three-dimensional relationships that occur between neuronal structures.          This problem will require new methods for data acquisition as well as data       visualization.                                                                                                                                                    The project described here is an interdisciplinary effort to derive              three-dimensional reconstructions of synaptic architecture from stereo           electron micrographs acquired from multiple viewpoints.  The collaboration       combines advanced ultrastructural visualization techniques with massively        parallel computational methods and an innovative set of pattern recognition,     stereo correspondence and depth mapping algorithms.  Our goal is to              integrate structural information from numerous images into a single,             high-accuracy three-dimensional reconstruction of the synaptic cytoskeleton.     The immediate result of this collaboration will be an improved understanding     of the spatial relationships between synaptic macromolecules.  More              importantly, the project will produce a set of computational tools that can      be applied to stereo image data sets of various areas of the nervous system,     from the macroscopic to the molecular level.  Finally, our studies will          advance the state-of-the-art of parallel computation and interactive             reconstruction methods that can provide novel solutions to difficult             problems of neuroscience visualization.                                           n/a",THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES,6185220,R01LM006326,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain mapping', ' cell cell interaction', ' computational neuroscience', ' computer program /software', ' computer system design /evaluation', ' cytoskeleton', ' data collection', ' electron microscopy', ' image processing', ' imaging /visualization /scanning', ' macromolecule', ' method development', ' nerve endings', ' neurons', ' parallel processing', ' stereophotography', ' structural biology', ' synapses']",NLM,UNIVERSITY OF MARYLAND BALTIMORE,R01,2000,117821,-0.0009407585392557735
"THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES DESCRIPTION (Taken from application abstract):  Recent advances in               understanding neuronal functions have led to an expanded scientific interest     in defining the organization of the nervous system so the spatial correlates     of these functions can be identified.  This interest has been formalized as      the Human Brain Project, an ambitious multi disciplinary effort to map the       nervous system from the organismal to the macromolecular levels.  One of the     greatest challenges of this effort is to preserve the complex                    three-dimensional relationships that occur between neuronal structures.          This problem will require new methods for data acquisition as well as data       visualization.                                                                                                                                                    The project described here is an interdisciplinary effort to derive              three-dimensional reconstructions of synaptic architecture from stereo           electron micrographs acquired from multiple viewpoints.  The collaboration       combines advanced ultrastructural visualization techniques with massively        parallel computational methods and an innovative set of pattern recognition,     stereo correspondence and depth mapping algorithms.  Our goal is to              integrate structural information from numerous images into a single,             high-accuracy three-dimensional reconstruction of the synaptic cytoskeleton.     The immediate result of this collaboration will be an improved understanding     of the spatial relationships between synaptic macromolecules.  More              importantly, the project will produce a set of computational tools that can      be applied to stereo image data sets of various areas of the nervous system,     from the macroscopic to the molecular level.  Finally, our studies will          advance the state-of-the-art of parallel computation and interactive             reconstruction methods that can provide novel solutions to difficult             problems of neuroscience visualization.                                           n/a",THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES,2897389,R01LM006326,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain mapping', ' cell cell interaction', ' computational neuroscience', ' computer program /software', ' computer system design /evaluation', ' cytoskeleton', ' data collection', ' electron microscopy', ' image processing', ' imaging /visualization /scanning', ' macromolecule', ' method development', ' nerve endings', ' neurons', ' parallel processing', ' stereophotography', ' structural biology', ' synapses']",NLM,UNIVERSITY OF MARYLAND BALTIMORE,R01,1999,107225,-0.0009407585392557735
"THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES DESCRIPTION (Taken from application abstract):  Recent advances in               understanding neuronal functions have led to an expanded scientific interest     in defining the organization of the nervous system so the spatial correlates     of these functions can be identified.  This interest has been formalized as      the Human Brain Project, an ambitious multi disciplinary effort to map the       nervous system from the organismal to the macromolecular levels.  One of the     greatest challenges of this effort is to preserve the complex                    three-dimensional relationships that occur between neuronal structures.          This problem will require new methods for data acquisition as well as data       visualization.                                                                                                                                                    The project described here is an interdisciplinary effort to derive              three-dimensional reconstructions of synaptic architecture from stereo           electron micrographs acquired from multiple viewpoints.  The collaboration       combines advanced ultrastructural visualization techniques with massively        parallel computational methods and an innovative set of pattern recognition,     stereo correspondence and depth mapping algorithms.  Our goal is to              integrate structural information from numerous images into a single,             high-accuracy three-dimensional reconstruction of the synaptic cytoskeleton.     The immediate result of this collaboration will be an improved understanding     of the spatial relationships between synaptic macromolecules.  More              importantly, the project will produce a set of computational tools that can      be applied to stereo image data sets of various areas of the nervous system,     from the macroscopic to the molecular level.  Finally, our studies will          advance the state-of-the-art of parallel computation and interactive             reconstruction methods that can provide novel solutions to difficult             problems of neuroscience visualization.                                           n/a",THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES,2771703,R01LM006326,"['artificial intelligence', ' brain mapping', ' cell cell interaction', ' computational neuroscience', ' computer program /software', ' computer system design /evaluation', ' cytoskeleton', ' data collection', ' electron microscopy', ' image processing', ' imaging /visualization /scanning', ' macromolecule', ' method development', ' nerve endings', ' neurons', ' parallel processing', ' stereophotography', ' structural biology', ' synapses']",NLM,UNIVERSITY OF MARYLAND BALTIMORE,R01,1998,118988,-0.0009407585392557735
"THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES DESCRIPTION (Taken from application abstract):  Recent advances in               understanding neuronal functions have led to an expanded scientific interest     in defining the organization of the nervous system so the spatial correlates     of these functions can be identified.  This interest has been formalized as      the Human Brain Project, an ambitious multi disciplinary effort to map the       nervous system from the organismal to the macromolecular levels.  One of the     greatest challenges of this effort is to preserve the complex                    three-dimensional relationships that occur between neuronal structures.          This problem will require new methods for data acquisition as well as data       visualization.                                                                                                                                                    The project described here is an interdisciplinary effort to derive              three-dimensional reconstructions of synaptic architecture from stereo           electron micrographs acquired from multiple viewpoints.  The collaboration       combines advanced ultrastructural visualization techniques with massively        parallel computational methods and an innovative set of pattern recognition,     stereo correspondence and depth mapping algorithms.  Our goal is to              integrate structural information from numerous images into a single,             high-accuracy three-dimensional reconstruction of the synaptic cytoskeleton.     The immediate result of this collaboration will be an improved understanding     of the spatial relationships between synaptic macromolecules.  More              importantly, the project will produce a set of computational tools that can      be applied to stereo image data sets of various areas of the nervous system,     from the macroscopic to the molecular level.  Finally, our studies will          advance the state-of-the-art of parallel computation and interactive             reconstruction methods that can provide novel solutions to difficult             problems of neuroscience visualization.                                           n/a",THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES,2453331,R01LM006326,"['artificial intelligence', ' brain mapping', ' cell cell interaction', ' computational neuroscience', ' computer program /software', ' computer system design /evaluation', ' cytoskeleton', ' data collection', ' electron microscopy', ' image processing', ' imaging /visualization /scanning', ' macromolecule', ' method development', ' nerve endings', ' neurons', ' parallel processing', ' stereophotography', ' structural biology', ' synapses']",NLM,UNIVERSITY OF MARYLAND BALTIMORE,R01,1997,128043,-0.0009407585392557735
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,7068069,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2006,19532,-0.039633402373947135
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,6937143,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2005,20000,-0.039633402373947135
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,6803809,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2004,242276,-0.039633402373947135
"BIOINFORMATICS, DISORDERED PROTEINS, AND FUNCTION A major objective of bioinformatics is to predict protein structure/function from sequence; the most successful current methods use sequence homology.  These current homology-based methods explicitly or implicitly assume the paradigm sequence yields structure yields function.  Since protein function depends on flexibility, movement, or even lack of structure (disorder) in some cases, combining various motion predictions with structure predictions should improve prediction of function.  Algorithms for predicting flexibility from sequence and local disorder have been developed, as have algorithms to predict sequences that snitch between two structured states.  Sequence complexity might also be an indirect measure of mobility and still other measures could be discovered by the study of known examples.  We propose to determine the interplay of homology-based predictions that use sequence information only with explicit representations of both structure information and motion information as determined from amino acid sequence.  Structural information will be represented by helix, sheet, etc., predictions, and by hydrophobic moment calculations.  Motion information will be represented by flexibility prediction, switch sequence prediction, order / disorder prediction, sequence complexity, and new measures if any are discovered in the course of this work.  Since different functions would involve motional parameters to different extents, the plan is to apply this combined approach on a sequence family basis.  Novel comparisons, called Attribute Profiles, are proposed for the representation of structure and motion information.  Sets of Gribskov/Eisenberg 1D Profiles and associated Attribute Profiles will be combined into single predictions using neural network data models.  Prediction outcomes will be compared with experimental observations and evaluated using the jackknife method.  If successful, this work will improve prediction of protein function from amino acid sequence, which is important given the significant numbers of protein sequences with undetermined functions coming out of the various genome projects.  Motion and disorder are important pieces that need to be added to the characterization of protein structure/function.  For example, taxol has been shown recently to bind to a disordered loop in Bcl-2.  Alzheimer disease, transmissible spongiform encephalopathies, Parkinson disease and infectious agents such as Staphylococcal aureus, foot-and-mouth disease virus, and (perhaps) HIV depend critically on disordered regions of protein.  Thus, understanding the interplay of sequence, flexibility, order / disorder, complexity, structure and function clearly relates to human health.  n/a","BIOINFORMATICS, DISORDERED PROTEINS, AND FUNCTION",6538213,R01LM006916,"['artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' conformation', ' hydropathy', ' informatics', ' protein sequence', ' protein structure function', ' structural biology']",NLM,WASHINGTON STATE UNIVERSITY,R01,2002,342080,-0.020373983020732886
"BIOINFORMATICS, DISORDERED PROTEINS, AND FUNCTION A major objective of bioinformatics is to predict protein structure/function from sequence; the most successful current methods use sequence homology.  These current homology-based methods explicitly or implicitly assume the paradigm sequence yields structure yields function.  Since protein function depends on flexibility, movement, or even lack of structure (disorder) in some cases, combining various motion predictions with structure predictions should improve prediction of function.  Algorithms for predicting flexibility from sequence and local disorder have been developed, as have algorithms to predict sequences that snitch between two structured states.  Sequence complexity might also be an indirect measure of mobility and still other measures could be discovered by the study of known examples.  We propose to determine the interplay of homology-based predictions that use sequence information only with explicit representations of both structure information and motion information as determined from amino acid sequence.  Structural information will be represented by helix, sheet, etc., predictions, and by hydrophobic moment calculations.  Motion information will be represented by flexibility prediction, switch sequence prediction, order / disorder prediction, sequence complexity, and new measures if any are discovered in the course of this work.  Since different functions would involve motional parameters to different extents, the plan is to apply this combined approach on a sequence family basis.  Novel comparisons, called Attribute Profiles, are proposed for the representation of structure and motion information.  Sets of Gribskov/Eisenberg 1D Profiles and associated Attribute Profiles will be combined into single predictions using neural network data models.  Prediction outcomes will be compared with experimental observations and evaluated using the jackknife method.  If successful, this work will improve prediction of protein function from amino acid sequence, which is important given the significant numbers of protein sequences with undetermined functions coming out of the various genome projects.  Motion and disorder are important pieces that need to be added to the characterization of protein structure/function.  For example, taxol has been shown recently to bind to a disordered loop in Bcl-2.  Alzheimer disease, transmissible spongiform encephalopathies, Parkinson disease and infectious agents such as Staphylococcal aureus, foot-and-mouth disease virus, and (perhaps) HIV depend critically on disordered regions of protein.  Thus, understanding the interplay of sequence, flexibility, order / disorder, complexity, structure and function clearly relates to human health.  n/a","BIOINFORMATICS, DISORDERED PROTEINS, AND FUNCTION",6391288,R01LM006916,"['artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' conformation', ' hydropathy', ' informatics', ' protein sequence', ' protein structure function', ' structural biology']",NLM,WASHINGTON STATE UNIVERSITY,R01,2001,332118,-0.020373983020732886
"BIOINFORMATICS, DISORDERED PROTEINS, AND FUNCTION A major objective of bioinformatics is to predict protein structure/function from sequence; the most successful current methods use sequence homology.  These current homology-based methods explicitly or implicitly assume the paradigm sequence yields structure yields function.  Since protein function depends on flexibility, movement, or even lack of structure (disorder) in some cases, combining various motion predictions with structure predictions should improve prediction of function.  Algorithms for predicting flexibility from sequence and local disorder have been developed, as have algorithms to predict sequences that snitch between two structured states.  Sequence complexity might also be an indirect measure of mobility and still other measures could be discovered by the study of known examples.  We propose to determine the interplay of homology-based predictions that use sequence information only with explicit representations of both structure information and motion information as determined from amino acid sequence.  Structural information will be represented by helix, sheet, etc., predictions, and by hydrophobic moment calculations.  Motion information will be represented by flexibility prediction, switch sequence prediction, order / disorder prediction, sequence complexity, and new measures if any are discovered in the course of this work.  Since different functions would involve motional parameters to different extents, the plan is to apply this combined approach on a sequence family basis.  Novel comparisons, called Attribute Profiles, are proposed for the representation of structure and motion information.  Sets of Gribskov/Eisenberg 1D Profiles and associated Attribute Profiles will be combined into single predictions using neural network data models.  Prediction outcomes will be compared with experimental observations and evaluated using the jackknife method.  If successful, this work will improve prediction of protein function from amino acid sequence, which is important given the significant numbers of protein sequences with undetermined functions coming out of the various genome projects.  Motion and disorder are important pieces that need to be added to the characterization of protein structure/function.  For example, taxol has been shown recently to bind to a disordered loop in Bcl-2.  Alzheimer disease, transmissible spongiform encephalopathies, Parkinson disease and infectious agents such as Staphylococcal aureus, foot-and-mouth disease virus, and (perhaps) HIV depend critically on disordered regions of protein.  Thus, understanding the interplay of sequence, flexibility, order / disorder, complexity, structure and function clearly relates to human health.  n/a","BIOINFORMATICS, DISORDERED PROTEINS, AND FUNCTION",6033672,R01LM006916,"['artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' conformation', ' hydropathy', ' informatics', ' protein sequence', ' protein structure function', ' structural biology']",NLM,WASHINGTON STATE UNIVERSITY,R01,2000,309826,-0.020373983020732886
"Coding of auditory space in the avian brain DESCRIPTION (provided by applicant): All auditory information used for sound localization ascends through the brainstem auditory nuclei. We will use physiological and theoretical approaches to understand how multidimensional features of sound, relevant to sound localization, are processed and encoded in the avian auditory brainstem. A primary advantage of using barn owls for the exploration of auditory processing is the substantial body of behavioral, anatomical and neurophysiological work that has elucidated the mechanisms of sound localization. The main cues owls use to compute sound direction are the interaural level difference (ILD) and the interaural time difference (ITD). Unlike mammals, owls use ILD to determine the vertical coordinate of the sound source and ITD to determine the horizontal coordinate. Two independent brainstem pathways process ITD and ILD and converge in the midbrain, where a spatiotopic map of auditory space emerges. Activity of neurons in the map precedes, and stimulation evokes, a head-orienting response towards the sound source. Thus, in barn owls, the neural algorithm for sound localization can be viewed as a system in which two input variables (ITD and ILD) are processed in parallel in order to control two output variables (horizontal and vertical coordinates of head saccades). We have used theoretical models to describe the neural responses that encode spatial information in the owl's auditory system. This approach has guided our experiments and aided the interpretation of our findings. Behavioral experiments in humans have used a similar approach to sound localization. However, due to a lack of neural data in humans, the predictive power of models of sound localization with regard to the neural bases of behavior has been a persistent question. Our studies in barn owls address this issue by investigating the mechanism of neural computations that are fundamental to models developed for human sound localization. This proposal is organized around three primary questions: 1) What are the computational primitives of auditory-space processing in the owl's brainstem? 2) How is spectrotemporal information encoded, transmitted and processed in parallel with spatial information? 3) What fundamental changes in information coding occur at the crossroads between the auditory midbrain and forebrain? We will address these questions using a wide range of strategies and techniques - intracellular in vivo recordings, cell-attached recording in vivo, multi-neuron tetrode recording, and modeling - which will make our approach interdisciplinary and of broad scope. Our research seeks to understand the function of the auditory brainstem and midbrain. In doing so, we will identify the types of information that are available to upstream nuclei and show how this information is encoded. A comprehensive approach to information processing in the auditory brainstem has the potential to provide new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to robotics and neural prosthetics. Cochlear implants that provide encoding of stimuli in ways that are more biologically relevant can be built, while devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves. By its differences and similarities with other species, the avian brain provides an excellent model system to define fundamental properties of neural processing and neural coding. The proposed research will test hypotheses based on human models, of how the auditory system computes  sound direction. This approach has the potential of providing new avenues for better understanding disorders  of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering,  mathematical formulations of brain processes can advance technology related to artificial intelligence and  neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea  could aid patients with compromised or non-functional auditory nerves.",Coding of auditory space in the avian brain,8521234,R01DC007690,"['Acoustic Nerve', 'Acoustics', 'Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Auditory', 'Auditory Physiology', 'Auditory system', 'Barn Owls', 'Behavior', 'Behavioral', 'Biological Models', 'Biology', 'Birds', 'Brain', 'Brain Stem', 'Cell Nucleus', 'Cells', 'Cochlea', 'Cochlear Implants', 'Cochlear nucleus', 'Code', 'Complex', 'Cues', 'Data', 'Devices', 'Dimensions', 'Disease', 'Drug Formulations', 'Ear', 'Engineering', 'Frequencies', 'Goals', 'Head', 'Hearing', 'Human', 'Impaired cognition', 'Inferior Colliculus', 'Lateral', 'Left', 'Link', 'Location', 'Mammals', 'Maps', 'Midbrain structure', 'Modeling', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Patients', 'Pattern', 'Physiological', 'Population', 'Process', 'Property', 'Prosencephalon', 'Research', 'Robotics', 'Saccades', 'Scheme', 'Signal Transduction', 'Sound Localization', 'Source', 'Specialist', 'Staging', 'Stimulus', 'Strigiformes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thalamic Nuclei', 'Thalamic structure', 'Theoretical model', 'Time', 'Work', 'auditory nuclei', 'auditory pathway', 'base', 'in vivo', 'information processing', 'interdisciplinary approach', 'neural prosthesis', 'neuromechanism', 'neurophysiology', 'operation', 'receptive field', 'relating to nervous system', 'research study', 'response', 'sound', 'theories']",NIDCD,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2013,324389,-0.011738157064701304
"Coding of auditory space in the avian brain    DESCRIPTION (provided by applicant): All auditory information used for sound localization ascends through the brainstem auditory nuclei. We will use physiological and theoretical approaches to understand how multidimensional features of sound, relevant to sound localization, are processed and encoded in the avian auditory brainstem. A primary advantage of using barn owls for the exploration of auditory processing is the substantial body of behavioral, anatomical and neurophysiological work that has elucidated the mechanisms of sound localization. The main cues owls use to compute sound direction are the interaural level difference (ILD) and the interaural time difference (ITD). Unlike mammals, owls use ILD to determine the vertical coordinate of the sound source and ITD to determine the horizontal coordinate. Two independent brainstem pathways process ITD and ILD and converge in the midbrain, where a spatiotopic map of auditory space emerges. Activity of neurons in the map precedes, and stimulation evokes, a head-orienting response towards the sound source. Thus, in barn owls, the neural algorithm for sound localization can be viewed as a system in which two input variables (ITD and ILD) are processed in parallel in order to control two output variables (horizontal and vertical coordinates of head saccades). We have used theoretical models to describe the neural responses that encode spatial information in the owl's auditory system. This approach has guided our experiments and aided the interpretation of our findings. Behavioral experiments in humans have used a similar approach to sound localization. However, due to a lack of neural data in humans, the predictive power of models of sound localization with regard to the neural bases of behavior has been a persistent question. Our studies in barn owls address this issue by investigating the mechanism of neural computations that are fundamental to models developed for human sound localization. This proposal is organized around three primary questions: 1) What are the computational primitives of auditory-space processing in the owl's brainstem? 2) How is spectrotemporal information encoded, transmitted and processed in parallel with spatial information? 3) What fundamental changes in information coding occur at the crossroads between the auditory midbrain and forebrain? We will address these questions using a wide range of strategies and techniques - intracellular in vivo recordings, cell-attached recording in vivo, multi-neuron tetrode recording, and modeling - which will make our approach interdisciplinary and of broad scope. Our research seeks to understand the function of the auditory brainstem and midbrain. In doing so, we will identify the types of information that are available to upstream nuclei and show how this information is encoded. A comprehensive approach to information processing in the auditory brainstem has the potential to provide new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to robotics and neural prosthetics. Cochlear implants that provide encoding of stimuli in ways that are more biologically relevant can be built, while devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves. By its differences and similarities with other species, the avian brain provides an excellent model system to define fundamental properties of neural processing and neural coding.        PUBLIC HEALTH RELEVANCE: The proposed research will test hypotheses based on human models, of how the auditory system computes sound direction. This approach has the potential of providing new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to artificial intelligence and neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves.           The proposed research will test hypotheses based on human models, of how the auditory system computes sound direction. This approach has the potential of providing new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to artificial intelligence and neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves.",Coding of auditory space in the avian brain,8723146,R01DC007690,"['Acoustic Nerve', 'Acoustics', 'Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Auditory', 'Auditory Physiology', 'Auditory system', 'Barn Owls', 'Behavior', 'Behavioral', 'Biological Models', 'Biology', 'Birds', 'Brain', 'Brain Stem', 'Cell Nucleus', 'Cells', 'Cochlea', 'Cochlear Implants', 'Cochlear nucleus', 'Code', 'Complex', 'Cues', 'Data', 'Devices', 'Dimensions', 'Disease', 'Drug Formulations', 'Ear', 'Engineering', 'Frequencies', 'Goals', 'Head', 'Hearing', 'Human', 'Impaired cognition', 'Inferior Colliculus', 'Lateral', 'Left', 'Link', 'Location', 'Mammals', 'Maps', 'Midbrain structure', 'Modeling', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Patients', 'Pattern', 'Physiological', 'Population', 'Process', 'Property', 'Prosencephalon', 'Research', 'Robotics', 'Saccades', 'Scheme', 'Signal Transduction', 'Sound Localization', 'Source', 'Specialist', 'Staging', 'Stimulus', 'Strigiformes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thalamic Nuclei', 'Thalamic structure', 'Theoretical model', 'Time', 'Work', 'auditory nuclei', 'auditory pathway', 'base', 'in vivo', 'information processing', 'interdisciplinary approach', 'neural prosthesis', 'neuromechanism', 'neurophysiology', 'operation', 'public health relevance', 'receptive field', 'relating to nervous system', 'research study', 'response', 'sound', 'theories']",NIDCD,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2014,341462,-0.013187565122884092
"Coding of auditory space in the avian brain    DESCRIPTION (provided by applicant): All auditory information used for sound localization ascends through the brainstem auditory nuclei. We will use physiological and theoretical approaches to understand how multidimensional features of sound, relevant to sound localization, are processed and encoded in the avian auditory brainstem. A primary advantage of using barn owls for the exploration of auditory processing is the substantial body of behavioral, anatomical and neurophysiological work that has elucidated the mechanisms of sound localization. The main cues owls use to compute sound direction are the interaural level difference (ILD) and the interaural time difference (ITD). Unlike mammals, owls use ILD to determine the vertical coordinate of the sound source and ITD to determine the horizontal coordinate. Two independent brainstem pathways process ITD and ILD and converge in the midbrain, where a spatiotopic map of auditory space emerges. Activity of neurons in the map precedes, and stimulation evokes, a head-orienting response towards the sound source. Thus, in barn owls, the neural algorithm for sound localization can be viewed as a system in which two input variables (ITD and ILD) are processed in parallel in order to control two output variables (horizontal and vertical coordinates of head saccades). We have used theoretical models to describe the neural responses that encode spatial information in the owl's auditory system. This approach has guided our experiments and aided the interpretation of our findings. Behavioral experiments in humans have used a similar approach to sound localization. However, due to a lack of neural data in humans, the predictive power of models of sound localization with regard to the neural bases of behavior has been a persistent question. Our studies in barn owls address this issue by investigating the mechanism of neural computations that are fundamental to models developed for human sound localization. This proposal is organized around three primary questions: 1) What are the computational primitives of auditory-space processing in the owl's brainstem? 2) How is spectrotemporal information encoded, transmitted and processed in parallel with spatial information? 3) What fundamental changes in information coding occur at the crossroads between the auditory midbrain and forebrain? We will address these questions using a wide range of strategies and techniques - intracellular in vivo recordings, cell-attached recording in vivo, multi-neuron tetrode recording, and modeling - which will make our approach interdisciplinary and of broad scope. Our research seeks to understand the function of the auditory brainstem and midbrain. In doing so, we will identify the types of information that are available to upstream nuclei and show how this information is encoded. A comprehensive approach to information processing in the auditory brainstem has the potential to provide new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to robotics and neural prosthetics. Cochlear implants that provide encoding of stimuli in ways that are more biologically relevant can be built, while devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves. By its differences and similarities with other species, the avian brain provides an excellent model system to define fundamental properties of neural processing and neural coding.       PUBLIC HEALTH RELEVANCE: The proposed research will test hypotheses based on human models, of how the auditory system computes sound direction. This approach has the potential of providing new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to artificial intelligence and neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves.           The proposed research will test hypotheses based on human models, of how the auditory system computes sound direction. This approach has the potential of providing new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to artificial intelligence and neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves.",Coding of auditory space in the avian brain,8305642,R01DC007690,"['Acoustic Nerve', 'Acoustics', 'Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Auditory', 'Auditory Physiology', 'Auditory system', 'Barn Owls', 'Behavior', 'Behavioral', 'Biological Models', 'Biology', 'Birds', 'Brain', 'Brain Stem', 'Cell Nucleus', 'Cells', 'Cochlea', 'Cochlear Implants', 'Cochlear nucleus', 'Code', 'Complex', 'Cues', 'Data', 'Devices', 'Dimensions', 'Disease', 'Drug Formulations', 'Ear', 'Engineering', 'Frequencies', 'Goals', 'Head', 'Hearing', 'Human', 'Impaired cognition', 'Inferior Colliculus', 'Lateral', 'Left', 'Link', 'Location', 'Mammals', 'Maps', 'Midbrain structure', 'Modeling', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Patients', 'Pattern', 'Physiological', 'Population', 'Process', 'Property', 'Prosencephalon', 'Research', 'Robotics', 'Saccades', 'Scheme', 'Signal Transduction', 'Sound Localization', 'Source', 'Specialist', 'Staging', 'Stimulus', 'Strigiformes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thalamic Nuclei', 'Thalamic structure', 'Theoretical model', 'Time', 'Work', 'auditory nuclei', 'auditory pathway', 'base', 'in vivo', 'information processing', 'interdisciplinary approach', 'neural prosthesis', 'neuromechanism', 'neurophysiology', 'operation', 'public health relevance', 'receptive field', 'relating to nervous system', 'research study', 'response', 'sound', 'theories']",NIDCD,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2012,341462,-0.013187565122884092
"Coding of auditory space in the avian brain    DESCRIPTION (provided by applicant): All auditory information used for sound localization ascends through the brainstem auditory nuclei. We will use physiological and theoretical approaches to understand how multidimensional features of sound, relevant to sound localization, are processed and encoded in the avian auditory brainstem. A primary advantage of using barn owls for the exploration of auditory processing is the substantial body of behavioral, anatomical and neurophysiological work that has elucidated the mechanisms of sound localization. The main cues owls use to compute sound direction are the interaural level difference (ILD) and the interaural time difference (ITD). Unlike mammals, owls use ILD to determine the vertical coordinate of the sound source and ITD to determine the horizontal coordinate. Two independent brainstem pathways process ITD and ILD and converge in the midbrain, where a spatiotopic map of auditory space emerges. Activity of neurons in the map precedes, and stimulation evokes, a head-orienting response towards the sound source. Thus, in barn owls, the neural algorithm for sound localization can be viewed as a system in which two input variables (ITD and ILD) are processed in parallel in order to control two output variables (horizontal and vertical coordinates of head saccades). We have used theoretical models to describe the neural responses that encode spatial information in the owl's auditory system. This approach has guided our experiments and aided the interpretation of our findings. Behavioral experiments in humans have used a similar approach to sound localization. However, due to a lack of neural data in humans, the predictive power of models of sound localization with regard to the neural bases of behavior has been a persistent question. Our studies in barn owls address this issue by investigating the mechanism of neural computations that are fundamental to models developed for human sound localization. This proposal is organized around three primary questions: 1) What are the computational primitives of auditory-space processing in the owl's brainstem? 2) How is spectrotemporal information encoded, transmitted and processed in parallel with spatial information? 3) What fundamental changes in information coding occur at the crossroads between the auditory midbrain and forebrain? We will address these questions using a wide range of strategies and techniques - intracellular in vivo recordings, cell-attached recording in vivo, multi-neuron tetrode recording, and modeling - which will make our approach interdisciplinary and of broad scope. Our research seeks to understand the function of the auditory brainstem and midbrain. In doing so, we will identify the types of information that are available to upstream nuclei and show how this information is encoded. A comprehensive approach to information processing in the auditory brainstem has the potential to provide new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to robotics and neural prosthetics. Cochlear implants that provide encoding of stimuli in ways that are more biologically relevant can be built, while devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves. By its differences and similarities with other species, the avian brain provides an excellent model system to define fundamental properties of neural processing and neural coding.       PUBLIC HEALTH RELEVANCE: The proposed research will test hypotheses based on human models, of how the auditory system computes sound direction. This approach has the potential of providing new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to artificial intelligence and neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves.           The proposed research will test hypotheses based on human models, of how the auditory system computes sound direction. This approach has the potential of providing new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to artificial intelligence and neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves.",Coding of auditory space in the avian brain,8196019,R01DC007690,"['Acoustic Nerve', 'Acoustics', 'Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Auditory', 'Auditory Physiology', 'Auditory system', 'Barn Owls', 'Behavior', 'Behavioral', 'Biological Models', 'Biology', 'Birds', 'Brain', 'Brain Stem', 'Cell Nucleus', 'Cells', 'Cochlea', 'Cochlear Implants', 'Cochlear nucleus', 'Code', 'Complex', 'Cues', 'Data', 'Devices', 'Dimensions', 'Disease', 'Drug Formulations', 'Ear', 'Engineering', 'Frequencies', 'Goals', 'Head', 'Hearing', 'Human', 'Impaired cognition', 'Inferior Colliculus', 'Lateral', 'Left', 'Link', 'Location', 'Mammals', 'Maps', 'Midbrain structure', 'Modeling', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Patients', 'Pattern', 'Physiological', 'Population', 'Process', 'Property', 'Prosencephalon', 'Research', 'Robotics', 'Saccades', 'Scheme', 'Signal Transduction', 'Sound Localization', 'Source', 'Specialist', 'Staging', 'Stimulus', 'Strigiformes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thalamic Nuclei', 'Thalamic structure', 'Theoretical model', 'Time', 'Work', 'auditory nuclei', 'auditory pathway', 'base', 'in vivo', 'information processing', 'interdisciplinary approach', 'neural prosthesis', 'neuromechanism', 'neurophysiology', 'operation', 'public health relevance', 'receptive field', 'relating to nervous system', 'research study', 'response', 'sound', 'theories']",NIDCD,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2011,24900,-0.013187565122884092
"Coding of auditory space in the avian brain    DESCRIPTION (provided by applicant): All auditory information used for sound localization ascends through the brainstem auditory nuclei. We will use physiological and theoretical approaches to understand how multidimensional features of sound, relevant to sound localization, are processed and encoded in the avian auditory brainstem. A primary advantage of using barn owls for the exploration of auditory processing is the substantial body of behavioral, anatomical and neurophysiological work that has elucidated the mechanisms of sound localization. The main cues owls use to compute sound direction are the interaural level difference (ILD) and the interaural time difference (ITD). Unlike mammals, owls use ILD to determine the vertical coordinate of the sound source and ITD to determine the horizontal coordinate. Two independent brainstem pathways process ITD and ILD and converge in the midbrain, where a spatiotopic map of auditory space emerges. Activity of neurons in the map precedes, and stimulation evokes, a head-orienting response towards the sound source. Thus, in barn owls, the neural algorithm for sound localization can be viewed as a system in which two input variables (ITD and ILD) are processed in parallel in order to control two output variables (horizontal and vertical coordinates of head saccades). We have used theoretical models to describe the neural responses that encode spatial information in the owl's auditory system. This approach has guided our experiments and aided the interpretation of our findings. Behavioral experiments in humans have used a similar approach to sound localization. However, due to a lack of neural data in humans, the predictive power of models of sound localization with regard to the neural bases of behavior has been a persistent question. Our studies in barn owls address this issue by investigating the mechanism of neural computations that are fundamental to models developed for human sound localization. This proposal is organized around three primary questions: 1) What are the computational primitives of auditory-space processing in the owl's brainstem? 2) How is spectrotemporal information encoded, transmitted and processed in parallel with spatial information? 3) What fundamental changes in information coding occur at the crossroads between the auditory midbrain and forebrain? We will address these questions using a wide range of strategies and techniques - intracellular in vivo recordings, cell-attached recording in vivo, multi-neuron tetrode recording, and modeling - which will make our approach interdisciplinary and of broad scope. Our research seeks to understand the function of the auditory brainstem and midbrain. In doing so, we will identify the types of information that are available to upstream nuclei and show how this information is encoded. A comprehensive approach to information processing in the auditory brainstem has the potential to provide new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to robotics and neural prosthetics. Cochlear implants that provide encoding of stimuli in ways that are more biologically relevant can be built, while devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves. By its differences and similarities with other species, the avian brain provides an excellent model system to define fundamental properties of neural processing and neural coding.       PUBLIC HEALTH RELEVANCE: The proposed research will test hypotheses based on human models, of how the auditory system computes sound direction. This approach has the potential of providing new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to artificial intelligence and neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves.           The proposed research will test hypotheses based on human models, of how the auditory system computes sound direction. This approach has the potential of providing new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to artificial intelligence and neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves.",Coding of auditory space in the avian brain,8265019,R01DC007690,"['Acoustic Nerve', 'Acoustics', 'Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Auditory', 'Auditory Physiology', 'Auditory system', 'Barn Owls', 'Behavior', 'Behavioral', 'Biological Models', 'Biology', 'Birds', 'Brain', 'Brain Stem', 'Cell Nucleus', 'Cells', 'Cochlea', 'Cochlear Implants', 'Cochlear nucleus', 'Code', 'Complex', 'Cues', 'Data', 'Devices', 'Dimensions', 'Disease', 'Drug Formulations', 'Ear', 'Engineering', 'Frequencies', 'Goals', 'Head', 'Hearing', 'Human', 'Impaired cognition', 'Inferior Colliculus', 'Lateral', 'Left', 'Link', 'Location', 'Mammals', 'Maps', 'Midbrain structure', 'Modeling', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Patients', 'Pattern', 'Physiological', 'Population', 'Process', 'Property', 'Prosencephalon', 'Research', 'Robotics', 'Saccades', 'Scheme', 'Signal Transduction', 'Sound Localization', 'Source', 'Specialist', 'Staging', 'Stimulus', 'Strigiformes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thalamic Nuclei', 'Thalamic structure', 'Theoretical model', 'Time', 'Work', 'auditory nuclei', 'auditory pathway', 'base', 'in vivo', 'information processing', 'interdisciplinary approach', 'neural prosthesis', 'neuromechanism', 'neurophysiology', 'operation', 'public health relevance', 'receptive field', 'relating to nervous system', 'research study', 'response', 'sound', 'theories']",NIDCD,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2011,341462,-0.013187565122884092
"Coding of auditory space in the avian brain    DESCRIPTION (provided by applicant): All auditory information used for sound localization ascends through the brainstem auditory nuclei. We will use physiological and theoretical approaches to understand how multidimensional features of sound, relevant to sound localization, are processed and encoded in the avian auditory brainstem. A primary advantage of using barn owls for the exploration of auditory processing is the substantial body of behavioral, anatomical and neurophysiological work that has elucidated the mechanisms of sound localization. The main cues owls use to compute sound direction are the interaural level difference (ILD) and the interaural time difference (ITD). Unlike mammals, owls use ILD to determine the vertical coordinate of the sound source and ITD to determine the horizontal coordinate. Two independent brainstem pathways process ITD and ILD and converge in the midbrain, where a spatiotopic map of auditory space emerges. Activity of neurons in the map precedes, and stimulation evokes, a head-orienting response towards the sound source. Thus, in barn owls, the neural algorithm for sound localization can be viewed as a system in which two input variables (ITD and ILD) are processed in parallel in order to control two output variables (horizontal and vertical coordinates of head saccades). We have used theoretical models to describe the neural responses that encode spatial information in the owl's auditory system. This approach has guided our experiments and aided the interpretation of our findings. Behavioral experiments in humans have used a similar approach to sound localization. However, due to a lack of neural data in humans, the predictive power of models of sound localization with regard to the neural bases of behavior has been a persistent question. Our studies in barn owls address this issue by investigating the mechanism of neural computations that are fundamental to models developed for human sound localization. This proposal is organized around three primary questions: 1) What are the computational primitives of auditory-space processing in the owl's brainstem? 2) How is spectrotemporal information encoded, transmitted and processed in parallel with spatial information? 3) What fundamental changes in information coding occur at the crossroads between the auditory midbrain and forebrain? We will address these questions using a wide range of strategies and techniques - intracellular in vivo recordings, cell-attached recording in vivo, multi-neuron tetrode recording, and modeling - which will make our approach interdisciplinary and of broad scope. Our research seeks to understand the function of the auditory brainstem and midbrain. In doing so, we will identify the types of information that are available to upstream nuclei and show how this information is encoded. A comprehensive approach to information processing in the auditory brainstem has the potential to provide new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to robotics and neural prosthetics. Cochlear implants that provide encoding of stimuli in ways that are more biologically relevant can be built, while devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves. By its differences and similarities with other species, the avian brain provides an excellent model system to define fundamental properties of neural processing and neural coding.       PUBLIC HEALTH RELEVANCE: The proposed research will test hypotheses based on human models, of how the auditory system computes sound direction. This approach has the potential of providing new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to artificial intelligence and neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves.           The proposed research will test hypotheses based on human models, of how the auditory system computes sound direction. This approach has the potential of providing new avenues for better understanding disorders of the central auditory system and cognitive impairments involving hearing. By linking biology and engineering, mathematical formulations of brain processes can advance technology related to artificial intelligence and neural prosthetics; smarter cochlear implants can be built, as well as devices acting downstream of the cochlea could aid patients with compromised or non-functional auditory nerves.",Coding of auditory space in the avian brain,8040464,R01DC007690,"['Acoustic Nerve', 'Acoustics', 'Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Auditory', 'Auditory Physiology', 'Auditory system', 'Barn Owls', 'Behavior', 'Behavioral', 'Biological Models', 'Biology', 'Birds', 'Brain', 'Brain Stem', 'Cell Nucleus', 'Cells', 'Cochlea', 'Cochlear Implants', 'Cochlear nucleus', 'Code', 'Complex', 'Cues', 'Data', 'Devices', 'Dimensions', 'Disease', 'Drug Formulations', 'Ear', 'Engineering', 'Frequencies', 'Goals', 'Head', 'Hearing', 'Human', 'Impaired cognition', 'Inferior Colliculus', 'Lateral', 'Left', 'Link', 'Location', 'Mammals', 'Maps', 'Midbrain structure', 'Modeling', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Patients', 'Pattern', 'Physiological', 'Population', 'Process', 'Property', 'Prosencephalon', 'Research', 'Robotics', 'Saccades', 'Scheme', 'Signal Transduction', 'Sound Localization', 'Source', 'Specialist', 'Staging', 'Stimulus', 'Strigiformes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thalamic Nuclei', 'Thalamic structure', 'Theoretical model', 'Time', 'Work', 'auditory nuclei', 'auditory pathway', 'base', 'in vivo', 'information processing', 'interdisciplinary approach', 'neural prosthesis', 'neuromechanism', 'neurophysiology', 'operation', 'receptive field', 'relating to nervous system', 'research study', 'response', 'sound', 'theories']",NIDCD,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2010,352750,-0.013187565122884092
"Biomechanical Analysis in Strabismus Surgery We propose 3 interrelated aims to define the biomechanics of the eye rotating (extraocular) muscles (EOMs) & optic nerve (ON) in health & visual disease, understand novel EOM actions, & characterize mechanical effects that may contribute to severe myopia. We aim to improve treatment of strabismus, misalignment of visual directions of the eyes; glaucoma & non-arteritic anterior ischemic optic neuropathy (NA-AION), both common blinding ON diseases; & high axial myopia, an ocular elongation & distortion that has become a worldwide epidemic & major cause of blindness. We propose a novel & critical nexus linking the EOMs, ON, & structure of the eye's scleral wall that we will explore using modern imaging & artificial intelligence techniques. Aim I will clarify the kinematic (motion) properties of the human eye, testing by multipositional magnetic resonance imaging (MRI) of the eyeball & EOMs the hypothesis that translational (linear) movement contributes importantly to ocular alignment. MRI will be performed during horizontal convergence & vertical eye rotation in normal people, & in patients who have common forms of strabismus including convergence insufficiency, eye crossing (esotropia), & outward ocular deviation (exotropia), both before & after corrective EOM surgery. Clarification of ocular translation is necessary to understand normal ocular motility and treat its disorders. Aim II will characterize the mechanical loading on the ON caused by eye movements. We will characterize the mechanical effects of ON tractional loading on the eyeball during horizontal & vertical eye rotations at 2 scales in living people, to test the hypothesis that such ON loading deforms it & adjacent retina & blood vessels as loading translates the eye. We propose that the resulting deformation during eye movements may create repetitive strain injury contributing to glaucoma, NA-AION, & axial myopia. In groups of subjects with the foregoing diseases, & in an equal group of matched healthy subjects, we will study mechanical effects of eye movement within the living eye by imaging its internal micro structure & blood vessels with optical coherence tomography, & outside the eyeball in the eye socket using MRI. Effects of tethering during eye movement will be studied ex vivo by precision 3D optical imaging of fresh human eye bank specimens subjected to mechanical tension on the ON that mimic effects of the eye movements imaged in the living subjects. Aim III will model the biomechanics of ocular kinematics. The constitutive mechanical properties of the non-muscular ocular & eye socket tissues will be described by finite element models (FEMs) using modern engineering methods for computational simulation to predict ocular kinematics, as well as local mechanical strains in the ON & sclera that may cause glaucoma, NA-AION, & the ocular deformities underlying extreme nearsightedness. We will determine if FEMs employing normal tissue properties can simulate normal ocular translation during horizontal & vertical rotations & convergence. By FEM simulation, we will also test the hypothesis that ocular loading by eye movement might contribute to: normal vergence, strabismus, & the effects of strabismus surgery. Relevance. Strabismus is a common clinical disorder that can cause double vision in adults and vision loss in children. Strabismus is often treated by surgical manipulation of the eye muscles, although current knowledge of their structure and function is incomplete. Proposed functional imaging and biomechanical studies of the properties of the eye muscles, eyeball, and optic nerve will improve understanding of the causes and treatment of strabismus, optic nerve diseases, and nearsightedness.",Biomechanical Analysis in Strabismus Surgery,9972266,R01EY008313,"['3-Dimensional', 'Accounting', 'Adult', 'Agreement', 'Algorithms', 'Anatomy', 'Anterior Ischemic Optic Neuropathy', 'Artificial Intelligence', 'Behavior', 'Biological Specimen Banks', 'Biomechanics', 'Blindness', 'Blood Vessels', 'Child', 'Choroid', 'Clinical', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Connective Tissue', 'Convergence Insufficiency', 'Cumulative Trauma Disorders', 'Deformity', 'Degenerative Myopia', 'Diplopia', 'Disease', 'Duct (organ) structure', 'Elements', 'Engineering', 'Epidemic', 'Equilibrium', 'Esotropia', 'Etiology', 'Exotropia', 'Eye', 'Eye Banks', 'Eye Movements', 'Failure', 'Functional Imaging', 'Gap Junctions', 'Glaucoma', 'Health', 'Human', 'Image', 'Individual', 'Knowledge', 'Lasers', 'Link', 'Magnetic Resonance Imaging', 'Matched Group', 'Measurement', 'Measures', 'Mechanics', 'Modeling', 'Modernization', 'Motion', 'Movement', 'Muscle', 'Muscle Contraction', 'Myopia', 'Normal tissue morphology', 'Ocular orbit', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Optics', 'Patients', 'Physiologic Intraocular Pressure', 'Play', 'Primary Open Angle Glaucoma', 'Property', 'Retina', 'Role', 'Rotation', 'Scanning', 'Sclera', 'Strabismus', 'Stress', 'Structure', 'Techniques', 'Testing', 'Therapeutic', 'Tissues', 'Traction', 'Translating', 'Translations', 'Validation', 'Variant', 'Visual', 'anatomic imaging', 'biomechanical model', 'cell motility', 'crosslink', 'digital imaging', 'ex vivo imaging', 'human tissue', 'improved', 'in vivo imaging', 'in vivo optical imaging', 'kinematics', 'mechanical load', 'mechanical properties', 'model development', 'models and simulation', 'monocular', 'neglect', 'novel', 'ocular imaging', 'optic nerve disorder', 'optical imaging', 'orbit muscle', 'predictive modeling', 'quantitative imaging', 'retina blood vessel structure', 'simulation']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,622245,-0.06295835324565227
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,8062031,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Health', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2011,359403,-0.038438094877081885
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,8068069,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2010,51400,-0.038438094877081885
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7828142,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2010,338802,-0.038438094877081885
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7577491,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2009,342223,-0.038438094877081885
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7431959,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Condition', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Numbers', 'Patients', 'Play', 'Population', 'Process', 'Public Health', 'Rate', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'particle', 'size', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2008,376423,-0.038438094877081885
"Machine Learning and Visualization in Structural Biology    DESCRIPTION:       This project's main objective is to develop computerized tools that assist x-ray crystallographers in rapidly determining the three-dimensional structure of a protein. More specifically, this project addresses the following task: given a 3D electron-density map from crystallography and the sequence of the protein, find the most likely layout (i.e. ""trace"") of the protein sequence in 3D. The project will create both automated methods based on statistical machine-learning and computer-vision techniques, as well as visualization tools that support humans doing this layout. These two approaches complement each other and are synergistic.      This project's first specific aim is to develop and empirically evaluate algorithms that interpret crystallographic electron-density maps. The second specific aim is to incorporate structural-biology domain knowledge (secondary-structure prediction and potential-energy calculations) into the project's algorithms for interpreting density maps. The third specific aim is to tightly integrate partial model-construction with phase estimation updates to improve the recognition of 3D protein structures in x-ray reflection data; crystallographers will be able to intervene whenever they desire to help ""steer"" this iterative process. The final specific aim is to develop intuitive and effective modalities - including virtual reality and the use of speech/audio - for the efficient use of crystallographer's time in manual model fitting and validation.      Structural biology has wide relevance to biomedicine, since protein function generally follows from protein form (i.e., its structure). This project's techniques will speed-up the process of determining protein 3D structures, especially from low-quality (i.e., low-resolution) x-ray data, and will be applicable to other structural-biology tasks. Being able to accurately interpret low-resolution data promises to allow higher through put structure determination. The broader impact will include a better understanding of the power of modern theories and algorithms in machine learning and visualization in solving biological problems.         n/a",Machine Learning and Visualization in Structural Biology,7599114,R01LM008796,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Arts', 'Biological', 'Complement', 'Complex', 'Computer Vision Systems', 'Coupling', 'Crystallography', 'Data', 'Devices', 'Engineering', 'Error Sources', 'Evaluation', 'Freedom', 'Human', 'Image', 'Imagery', 'Knowledge', 'Left', 'Machine Learning', 'Manuals', 'Maps', 'Methods', 'Modality', 'Modeling', 'Nature', 'Peptide Sequence Determination', 'Phase', 'Potential Energy', 'Process', 'Proteins', 'Research Personnel', 'Resolution', 'Right-On', 'Shapes', 'Speech', 'Speed', 'Structure', 'Surface', 'Techniques', 'Testing', 'Time', 'Update', 'Validation', 'Vertebral column', 'Work', 'base', 'computerized tools', 'cost', 'data format', 'density', 'design', 'electron density', 'improved', 'method development', 'open source', 'programs', 'protein function', 'protein structure', 'structural biology', 'theories', 'three dimensional structure', 'tool', 'usability', 'virtual reality']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2009,306728,0.0005444904522553315
"Machine Learning and Visualization in Structural Biology    DESCRIPTION:       This project's main objective is to develop computerized tools that assist x-ray crystallographers in rapidly determining the three-dimensional structure of a protein. More specifically, this project addresses the following task: given a 3D electron-density map from crystallography and the sequence of the protein, find the most likely layout (i.e. ""trace"") of the protein sequence in 3D. The project will create both automated methods based on statistical machine-learning and computer-vision techniques, as well as visualization tools that support humans doing this layout. These two approaches complement each other and are synergistic.      This project's first specific aim is to develop and empirically evaluate algorithms that interpret crystallographic electron-density maps. The second specific aim is to incorporate structural-biology domain knowledge (secondary-structure prediction and potential-energy calculations) into the project's algorithms for interpreting density maps. The third specific aim is to tightly integrate partial model-construction with phase estimation updates to improve the recognition of 3D protein structures in x-ray reflection data; crystallographers will be able to intervene whenever they desire to help ""steer"" this iterative process. The final specific aim is to develop intuitive and effective modalities - including virtual reality and the use of speech/audio - for the efficient use of crystallographer's time in manual model fitting and validation.      Structural biology has wide relevance to biomedicine, since protein function generally follows from protein form (i.e., its structure). This project's techniques will speed-up the process of determining protein 3D structures, especially from low-quality (i.e., low-resolution) x-ray data, and will be applicable to other structural-biology tasks. Being able to accurately interpret low-resolution data promises to allow higher through put structure determination. The broader impact will include a better understanding of the power of modern theories and algorithms in machine learning and visualization in solving biological problems.         n/a",Machine Learning and Visualization in Structural Biology,7391736,R01LM008796,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Arts', 'Biological', 'Complement', 'Complex', 'Computer Vision Systems', 'Condition', 'Coupling', 'Crystallography', 'Data', 'Devices', 'Engineering', 'Error Sources', 'Evaluation', 'Facility Construction Funding Category', 'Freedom', 'Human', 'Image', 'Imagery', 'Knowledge', 'Left', 'Machine Learning', 'Manuals', 'Maps', 'Methods', 'Modality', 'Modeling', 'Nature', 'Object Attachment', 'Peptide Sequence Determination', 'Phase', 'Potential Energy', 'Process', 'Proteins', 'Research Personnel', 'Resolution', 'Right-On', 'Shapes', 'Speech', 'Speed', 'Structure', 'Surface', 'Techniques', 'Testing', 'Time', 'Update', 'Validation', 'Vertebral column', 'Work', 'base', 'computerized tools', 'cost', 'density', 'design', 'desire', 'electron density', 'improved', 'method development', 'open source', 'programs', 'protein function', 'protein structure', 'structural biology', 'theories', 'three dimensional structure', 'tool', 'usability', 'virtual reality']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2008,306970,0.0005444904522553315
"Machine Learning and Visualization in Structural Biology    DESCRIPTION:       This project's main objective is to develop computerized tools that assist x-ray crystallographers in rapidly determining the three-dimensional structure of a protein. More specifically, this project addresses the following task: given a 3D electron-density map from crystallography and the sequence of the protein, find the most likely layout (i.e. ""trace"") of the protein sequence in 3D. The project will create both automated methods based on statistical machine-learning and computer-vision techniques, as well as visualization tools that support humans doing this layout. These two approaches complement each other and are synergistic.      This project's first specific aim is to develop and empirically evaluate algorithms that interpret crystallographic electron-density maps. The second specific aim is to incorporate structural-biology domain knowledge (secondary-structure prediction and potential-energy calculations) into the project's algorithms for interpreting density maps. The third specific aim is to tightly integrate partial model-construction with phase estimation updates to improve the recognition of 3D protein structures in x-ray reflection data; crystallographers will be able to intervene whenever they desire to help ""steer"" this iterative process. The final specific aim is to develop intuitive and effective modalities - including virtual reality and the use of speech/audio - for the efficient use of crystallographer's time in manual model fitting and validation.      Structural biology has wide relevance to biomedicine, since protein function generally follows from protein form (i.e., its structure). This project's techniques will speed-up the process of determining protein 3D structures, especially from low-quality (i.e., low-resolution) x-ray data, and will be applicable to other structural-biology tasks. Being able to accurately interpret low-resolution data promises to allow higher through put structure determination. The broader impact will include a better understanding of the power of modern theories and algorithms in machine learning and visualization in solving biological problems.         n/a",Machine Learning and Visualization in Structural Biology,7216865,R01LM008796,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Arts', 'Biological', 'Complement', 'Complex', 'Computer Vision Systems', 'Condition', 'Coupling', 'Crystallography', 'Data', 'Devices', 'Engineering', 'Error Sources', 'Evaluation', 'Facility Construction Funding Category', 'Freedom', 'Human', 'Image', 'Imagery', 'Knowledge', 'Left', 'Machine Learning', 'Manuals', 'Maps', 'Methods', 'Modality', 'Modeling', 'Nature', 'Object Attachment', 'Peptide Sequence Determination', 'Phase', 'Potential Energy', 'Process', 'Proteins', 'Research Personnel', 'Resolution', 'Right-On', 'Shapes', 'Speech', 'Speed', 'Structure', 'Surface', 'Techniques', 'Testing', 'Time', 'Update', 'Validation', 'Vertebral column', 'Work', 'base', 'computerized tools', 'cost', 'density', 'design', 'desire', 'electron density', 'improved', 'method development', 'open source', 'programs', 'protein function', 'protein structure', 'structural biology', 'theories', 'three dimensional structure', 'tool', 'usability', 'virtual reality']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2007,313156,0.0005444904522553315
"Machine Learning and Visualization in Structural Biology    DESCRIPTION:       This project's main objective is to develop computerized tools that assist x-ray crystallographers in rapidly determining the three-dimensional structure of a protein. More specifically, this project addresses the following task: given a 3D electron-density map from crystallography and the sequence of the protein, find the most likely layout (i.e. ""trace"") of the protein sequence in 3D. The project will create both automated methods based on statistical machine-learning and computer-vision techniques, as well as visualization tools that support humans doing this layout. These two approaches complement each other and are synergistic.      This project's first specific aim is to develop and empirically evaluate algorithms that interpret crystallographic electron-density maps. The second specific aim is to incorporate structural-biology domain knowledge (secondary-structure prediction and potential-energy calculations) into the project's algorithms for interpreting density maps. The third specific aim is to tightly integrate partial model-construction with phase estimation updates to improve the recognition of 3D protein structures in x-ray reflection data; crystallographers will be able to intervene whenever they desire to help ""steer"" this iterative process. The final specific aim is to develop intuitive and effective modalities - including virtual reality and the use of speech/audio - for the efficient use of crystallographer's time in manual model fitting and validation.      Structural biology has wide relevance to biomedicine, since protein function generally follows from protein form (i.e., its structure). This project's techniques will speed-up the process of determining protein 3D structures, especially from low-quality (i.e., low-resolution) x-ray data, and will be applicable to other structural-biology tasks. Being able to accurately interpret low-resolution data promises to allow higher through put structure determination. The broader impact will include a better understanding of the power of modern theories and algorithms in machine learning and visualization in solving biological problems.         n/a",Machine Learning and Visualization in Structural Biology,6958317,R01LM008796,"['computer human interaction', 'computer simulation', 'computers', 'density', 'electron density', 'human', 'learning', 'model', 'protein sequence', 'protein structure', 'proteins', 'structural biology', 'vision']",NLM,UNIVERSITY OF WISCONSIN MADISON,R01,2006,322749,0.0005444904522553315
"Optimizing measurement of mindfulness meditation using brain pattern classification. PROJECT SUMMARY/ABSTRACT  	The candidate Dr. Helen Weng is seeking a K08 Mentored Clinical Scientist Research Career Development Award for her immediate career goal to gain mentorship and training to develop the EMBODY Task, a more objective and precise fMRI measure of meditation practice that uses multivariate brain pattern classification methods. Dr. Weng earned her doctorate in clinical psychology researching the neurobiological mechanisms of compassion meditation with Dr. Richard Davidson, an expert in contemplative neuroscience. She is currently a postdoctoral scholar at the Osher Center for Integrative Medicine (OCIM) at the University of California, San Francisco (UCSF) in a NCCIH T32-funded fellowship, where she is studying mindful body awareness using neuroscientific and psychophysiological methods. Dr. Weng is rigorously trained in functional magnetic resonance imaging (fMRI) to study contemplative neuroscience; however, she is primarily trained in using standard univariate analyses, which collapse across spatial and temporal information within fMRI data. She recognized the potential limitations of these methods to study the complex and fluctuating mental states that occur during meditation practice, and aims to develop novel fMRI tasks that use multivariate methods (which utilize spatial and temporal variability within fMRI data) to study brain states trained by meditation. Obtaining this K08 to develop the EMBODY Task would support Dr. Weng’s long-term career goal of becoming an independent investigator of the neurobiological mechanisms that underlie the therapeutic benefits of mindfulness-based interventions (MBIs). 	Dr. Weng’s career development plan includes training in the cognitive neuroscience of attention, fMRI pattern classification, and clinical trial methods to study meditation training. Dr. Weng has assembled a stellar mentoring team including top experts in cognitive neuroscience, mindfulness-based interventions, and pattern classification. Her primary sponsor and mentor, Dr. Adam Gazzaley (Director of the UCSF Neuroscience Imaging Center), is a world-class expert in the cognitive neuroscience of attention and neuroplasticity induced by app-based trainings. Her co-mentor, Dr. Rick Hecht (Research Director at OCIM) is a leading expert in clinical trials of MBIs. Her team also includes experts in fMRI pattern classification methodology (Dr. Bin Yu, Chancellor’s Professor, UC Berkeley; Dr. Melina Uncapher, UCSF) and cognitive neuroscience of internal attention (Dr. David Ziegler, UCSF). Dr. Weng’s training environment is at UCSF, a top NIH-funded research institution, and she will have access to state-of-the-art neuroimaging facilities at the Neuroscience Imaging Center and resources from OCIM, a flagship center for studying meditation. She will regularly meet and communicate with her mentorship team, and complete coursework, fMRI workshops, and training programs to fulfill her training goals. Dr. Weng’s K08 research proposal aims to develop the EMBODY Task, a novel fMRI task that uses brain pattern classification to decode the focus of attention during a period of breath meditation. This task will more accurately measure mental states during meditation by using precise information from individuals’ unique neural signatures. The task will distinguish neural patterns associated with attention (breath attention) and inattention (mind wandering) to the breath, and use these patterns to identify fluctuating brain states during ten minutes of breath meditation. This will produce objective brain-derived metrics of breath attention such as percentage time spent in breath attention, which can then be used to objectively assess outcomes of meditation training. She will pilot, develop, and validate this task within long-term practitioners, meditation novices, and a separate group of novices who go through mobile app-based meditation training. Dr. Weng will use this data to prepare a future R61/R33 proposal that will use the EMBODY Task to more rigorously test the mechanisms of action of how mindfulness-based interventions improve clinical outcomes by training meditation skills. PROJECT NARRATIVE Chronic pain is an enormous public health issue that affects 100 million Americans and costs more than $600 billion per year in treatments and lost productivity; and major depression is one of the most common mental disorders in the United States (6.7% prevalence in 2014) and carries the heaviest burden of disability among mental disorders (World Health Organization, 2010). Mindfulness-Based Interventions improve symptoms in chronic pain and depression; however, we do not currently understand what meditation skills learned from mindfulness-based interventions improve symptoms in chronic pain and depression, and this information is needed to improve treatment. This proposal will develop a more accurate and precise measure of meditation practice using brain imaging methods (functional MRI) that can identify attention states during meditation practice.",Optimizing measurement of mindfulness meditation using brain pattern classification.,9415417,K08AT009385,"['Affect', 'Algorithms', 'American', 'Attention', 'Awareness', 'Body measure procedure', 'Brain', 'Brain imaging', 'California', 'Categories', 'Chronic', 'Classification', 'Clinical', 'Clinical Psychology', 'Clinical Trials', 'Cognitive', 'Compassion', 'Complex', 'Data', 'Development Plans', 'Educational workshop', 'Emotions', 'Environment', 'Esthesia', 'Exercise', 'Fellowship', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Grain', 'Health', 'Image', 'Individual', 'Institution', 'Insula of Reil', 'Integrative Medicine', 'Intervention', 'K-Series Research Career Programs', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Mathematics', 'Measurement', 'Measures', 'Medial', 'Meditation', 'Mental disorders', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Mind', 'Neuronal Plasticity', 'Neurosciences', 'Outcome', 'Patient Self-Report', 'Pattern', 'Performance', 'Persons', 'Pilot Projects', 'Population', 'Prefrontal Cortex', 'Prevalence', 'Procedures', 'Productivity', 'Psychophysiology', 'Public Health', 'Research', 'Research Personnel', 'Research Proposals', 'Resources', 'San Francisco', 'Scientist', 'Signal Transduction', 'Stimulus', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Training', 'Training Programs', 'United States', 'United States National Institutes of Health', 'Universities', 'World Health Organization', 'base', 'behavior measurement', 'career', 'career development', 'chronic depression', 'chronic pain', 'cognitive neuroscience', 'cost', 'design', 'disability burden', 'health training', 'imaging modality', 'improved', 'inattention', 'mental representation', 'mental state', 'mindfulness', 'mindfulness intervention', 'mindfulness meditation', 'mobile application', 'neural patterning', 'neurobiological mechanism', 'neuroimaging', 'novel', 'professor', 'relating to nervous system', 'skills', 'symptomatic improvement']",NCCIH,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K08,2018,195853,-0.0601780629808183
"Optimizing measurement of mindfulness meditation using brain pattern classification. PROJECT SUMMARY/ABSTRACT  	The candidate Dr. Helen Weng is seeking a K08 Mentored Clinical Scientist Research Career Development Award for her immediate career goal to gain mentorship and training to develop the EMBODY Task, a more objective and precise fMRI measure of meditation practice that uses multivariate brain pattern classification methods. Dr. Weng earned her doctorate in clinical psychology researching the neurobiological mechanisms of compassion meditation with Dr. Richard Davidson, an expert in contemplative neuroscience. She is currently a postdoctoral scholar at the Osher Center for Integrative Medicine (OCIM) at the University of California, San Francisco (UCSF) in a NCCIH T32-funded fellowship, where she is studying mindful body awareness using neuroscientific and psychophysiological methods. Dr. Weng is rigorously trained in functional magnetic resonance imaging (fMRI) to study contemplative neuroscience; however, she is primarily trained in using standard univariate analyses, which collapse across spatial and temporal information within fMRI data. She recognized the potential limitations of these methods to study the complex and fluctuating mental states that occur during meditation practice, and aims to develop novel fMRI tasks that use multivariate methods (which utilize spatial and temporal variability within fMRI data) to study brain states trained by meditation. Obtaining this K08 to develop the EMBODY Task would support Dr. Weng’s long-term career goal of becoming an independent investigator of the neurobiological mechanisms that underlie the therapeutic benefits of mindfulness-based interventions (MBIs). 	Dr. Weng’s career development plan includes training in the cognitive neuroscience of attention, fMRI pattern classification, and clinical trial methods to study meditation training. Dr. Weng has assembled a stellar mentoring team including top experts in cognitive neuroscience, mindfulness-based interventions, and pattern classification. Her primary sponsor and mentor, Dr. Adam Gazzaley (Director of the UCSF Neuroscience Imaging Center), is a world-class expert in the cognitive neuroscience of attention and neuroplasticity induced by app-based trainings. Her co-mentor, Dr. Rick Hecht (Research Director at OCIM) is a leading expert in clinical trials of MBIs. Her team also includes experts in fMRI pattern classification methodology (Dr. Bin Yu, Chancellor’s Professor, UC Berkeley; Dr. Melina Uncapher, UCSF) and cognitive neuroscience of internal attention (Dr. David Ziegler, UCSF). Dr. Weng’s training environment is at UCSF, a top NIH-funded research institution, and she will have access to state-of-the-art neuroimaging facilities at the Neuroscience Imaging Center and resources from OCIM, a flagship center for studying meditation. She will regularly meet and communicate with her mentorship team, and complete coursework, fMRI workshops, and training programs to fulfill her training goals. Dr. Weng’s K08 research proposal aims to develop the EMBODY Task, a novel fMRI task that uses brain pattern classification to decode the focus of attention during a period of breath meditation. This task will more accurately measure mental states during meditation by using precise information from individuals’ unique neural signatures. The task will distinguish neural patterns associated with attention (breath attention) and inattention (mind wandering) to the breath, and use these patterns to identify fluctuating brain states during ten minutes of breath meditation. This will produce objective brain-derived metrics of breath attention such as percentage time spent in breath attention, which can then be used to objectively assess outcomes of meditation training. She will pilot, develop, and validate this task within long-term practitioners, meditation novices, and a separate group of novices who go through mobile app-based meditation training. Dr. Weng will use this data to prepare a future R61/R33 proposal that will use the EMBODY Task to more rigorously test the mechanisms of action of how mindfulness-based interventions improve clinical outcomes by training meditation skills. PROJECT NARRATIVE Chronic pain is an enormous public health issue that affects 100 million Americans and costs more than $600 billion per year in treatments and lost productivity; and major depression is one of the most common mental disorders in the United States (6.7% prevalence in 2014) and carries the heaviest burden of disability among mental disorders (World Health Organization, 2010). Mindfulness-Based Interventions improve symptoms in chronic pain and depression; however, we do not currently understand what meditation skills learned from mindfulness-based interventions improve symptoms in chronic pain and depression, and this information is needed to improve treatment. This proposal will develop a more accurate and precise measure of meditation practice using brain imaging methods (functional MRI) that can identify attention states during meditation practice.",Optimizing measurement of mindfulness meditation using brain pattern classification.,9625081,K08AT009385,"['Affect', 'American', 'Attention', 'Awareness', 'Body measure procedure', 'Brain', 'Brain imaging', 'California', 'Categories', 'Chronic', 'Classification', 'Clinical', 'Clinical Psychology', 'Clinical Trials', 'Cognitive', 'Compassion', 'Complex', 'Data', 'Development Plans', 'Educational workshop', 'Emotions', 'Environment', 'Esthesia', 'Exercise', 'Fellowship', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Grain', 'Health', 'Image', 'Individual', 'Institution', 'Insula of Reil', 'Integrative Medicine', 'Intervention', 'K-Series Research Career Programs', 'Major Depressive Disorder', 'Mathematics', 'Measurement', 'Measures', 'Medial', 'Meditation', 'Mental disorders', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Mind', 'Neuronal Plasticity', 'Neurosciences', 'Outcome', 'Patient Self-Report', 'Pattern', 'Performance', 'Persons', 'Pilot Projects', 'Population', 'Prefrontal Cortex', 'Prevalence', 'Procedures', 'Productivity', 'Psychophysiology', 'Public Health', 'Research', 'Research Personnel', 'Research Proposals', 'Resources', 'San Francisco', 'Scientist', 'Signal Transduction', 'Stimulus', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Training', 'Training Programs', 'United States', 'United States National Institutes of Health', 'Universities', 'World Health Organization', 'base', 'behavior measurement', 'career', 'career development', 'chronic depression', 'chronic pain', 'cognitive neuroscience', 'cost', 'design', 'disability burden', 'health training', 'imaging modality', 'improved', 'inattention', 'learning algorithm', 'machine learning algorithm', 'mental representation', 'mental state', 'mindfulness', 'mindfulness intervention', 'mindfulness meditation', 'mobile application', 'neural patterning', 'neurobiological mechanism', 'neuroimaging', 'novel', 'professor', 'relating to nervous system', 'skills', 'symptomatic improvement']",NCCIH,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K08,2019,195853,-0.0601780629808183
"Optimizing measurement of mindfulness meditation using brain pattern classification. PROJECT SUMMARY/ABSTRACT  	The candidate Dr. Helen Weng is seeking a K08 Mentored Clinical Scientist Research Career Development Award for her immediate career goal to gain mentorship and training to develop the EMBODY Task, a more objective and precise fMRI measure of meditation practice that uses multivariate brain pattern classification methods. Dr. Weng earned her doctorate in clinical psychology researching the neurobiological mechanisms of compassion meditation with Dr. Richard Davidson, an expert in contemplative neuroscience. She is currently a postdoctoral scholar at the Osher Center for Integrative Medicine (OCIM) at the University of California, San Francisco (UCSF) in a NCCIH T32-funded fellowship, where she is studying mindful body awareness using neuroscientific and psychophysiological methods. Dr. Weng is rigorously trained in functional magnetic resonance imaging (fMRI) to study contemplative neuroscience; however, she is primarily trained in using standard univariate analyses, which collapse across spatial and temporal information within fMRI data. She recognized the potential limitations of these methods to study the complex and fluctuating mental states that occur during meditation practice, and aims to develop novel fMRI tasks that use multivariate methods (which utilize spatial and temporal variability within fMRI data) to study brain states trained by meditation. Obtaining this K08 to develop the EMBODY Task would support Dr. Weng’s long-term career goal of becoming an independent investigator of the neurobiological mechanisms that underlie the therapeutic benefits of mindfulness-based interventions (MBIs). 	Dr. Weng’s career development plan includes training in the cognitive neuroscience of attention, fMRI pattern classification, and clinical trial methods to study meditation training. Dr. Weng has assembled a stellar mentoring team including top experts in cognitive neuroscience, mindfulness-based interventions, and pattern classification. Her primary sponsor and mentor, Dr. Adam Gazzaley (Director of the UCSF Neuroscience Imaging Center), is a world-class expert in the cognitive neuroscience of attention and neuroplasticity induced by app-based trainings. Her co-mentor, Dr. Rick Hecht (Research Director at OCIM) is a leading expert in clinical trials of MBIs. Her team also includes experts in fMRI pattern classification methodology (Dr. Bin Yu, Chancellor’s Professor, UC Berkeley; Dr. Melina Uncapher, UCSF) and cognitive neuroscience of internal attention (Dr. David Ziegler, UCSF). Dr. Weng’s training environment is at UCSF, a top NIH-funded research institution, and she will have access to state-of-the-art neuroimaging facilities at the Neuroscience Imaging Center and resources from OCIM, a flagship center for studying meditation. She will regularly meet and communicate with her mentorship team, and complete coursework, fMRI workshops, and training programs to fulfill her training goals. Dr. Weng’s K08 research proposal aims to develop the EMBODY Task, a novel fMRI task that uses brain pattern classification to decode the focus of attention during a period of breath meditation. This task will more accurately measure mental states during meditation by using precise information from individuals’ unique neural signatures. The task will distinguish neural patterns associated with attention (breath attention) and inattention (mind wandering) to the breath, and use these patterns to identify fluctuating brain states during ten minutes of breath meditation. This will produce objective brain-derived metrics of breath attention such as percentage time spent in breath attention, which can then be used to objectively assess outcomes of meditation training. She will pilot, develop, and validate this task within long-term practitioners, meditation novices, and a separate group of novices who go through mobile app-based meditation training. Dr. Weng will use this data to prepare a future R61/R33 proposal that will use the EMBODY Task to more rigorously test the mechanisms of action of how mindfulness-based interventions improve clinical outcomes by training meditation skills. PROJECT NARRATIVE Chronic pain is an enormous public health issue that affects 100 million Americans and costs more than $600 billion per year in treatments and lost productivity; and major depression is one of the most common mental disorders in the United States (6.7% prevalence in 2014) and carries the heaviest burden of disability among mental disorders (World Health Organization, 2010). Mindfulness-Based Interventions improve symptoms in chronic pain and depression; however, we do not currently understand what meditation skills learned from mindfulness-based interventions improve symptoms in chronic pain and depression, and this information is needed to improve treatment. This proposal will develop a more accurate and precise measure of meditation practice using brain imaging methods (functional MRI) that can identify attention states during meditation practice.",Optimizing measurement of mindfulness meditation using brain pattern classification.,9224446,K08AT009385,"['Affect', 'Algorithms', 'American', 'Attention', 'Awareness', 'Body measure procedure', 'Brain', 'Brain imaging', 'California', 'Categories', 'Cereals', 'Chronic', 'Classification', 'Clinical', 'Clinical Psychology', 'Clinical Trials', 'Cognitive', 'Compassion', 'Complex', 'Data', 'Development Plans', 'Educational workshop', 'Emotions', 'Environment', 'Esthesia', 'Exercise', 'Fellowship', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'Image', 'Individual', 'Institution', 'Insula of Reil', 'Integrative Medicine', 'Intervention', 'K-Series Research Career Programs', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Mathematics', 'Measurement', 'Measures', 'Medial', 'Meditation', 'Mental disorders', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Mind', 'Neuronal Plasticity', 'Neurosciences', 'Outcome', 'Patient Self-Report', 'Pattern', 'Performance', 'Persons', 'Pilot Projects', 'Population', 'Prefrontal Cortex', 'Prevalence', 'Procedures', 'Productivity', 'Psychophysiology', 'Public Health', 'Research', 'Research Personnel', 'Research Proposals', 'Resources', 'San Francisco', 'Scientist', 'Signal Transduction', 'Stimulus', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Training', 'Training Programs', 'United States', 'United States National Institutes of Health', 'Universities', 'World Health Organization', 'base', 'behavior measurement', 'career', 'career development', 'chronic depression', 'chronic pain', 'cognitive neuroscience', 'cost', 'design', 'disability burden', 'health training', 'imaging modality', 'improved', 'inattention', 'mental representation', 'mental state', 'mindfulness', 'mindfulness intervention', 'mindfulness meditation', 'mobile application', 'neural patterning', 'neurobiological mechanism', 'neuroimaging', 'novel', 'professor', 'relating to nervous system', 'skills', 'symptomatic improvement']",NCCIH,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K08,2017,195853,-0.0601780629808183
"CUTANEOUS PATTERN PERCEPTION The proposed research will investigate the skin's ability to perceive tactile patterns.  Using a vibrotactile array of stimulators that fit against the subjects' fingertips, complex patterns varying in duration, mode of generation, intensity, size, temporal proximity to masking stimuli, and location will be presented to subjects who are required to identify the patterns.  The role of pattern onset in generating vibrotactile contours and the limits of temporal integration of vibrotactile patterns will be investigated.  Measurements will be made of temporal masking functions with patterned stimuli, and a two-factor model of masking will be tested. Spatial factors such as the locus of stimulation and the size of patterns as well as the interaction between these factors and temporal and intensive variables will be explored.  The role of experience in tactile pattern recognition will be examined by testing individuals who have had extensive experience with the Optacon, a reading aid for the blind, with some of the pattern recognition tasks on which sighted subjects have been tested.  The project will be concerned with drawing parallels between tactile and visual pattern perception.  The project will also be concerned with the improvement of cutaneous communication systems and devices, particularly as these might be used by individuals with sensory handicaps.  n/a",CUTANEOUS PATTERN PERCEPTION,3394042,R01NS009783,"['artificial intelligence', ' blindness', ' cutaneous sensory nerve', ' form /pattern perception', ' human subject', ' mechanoreceptors', ' neural information processing', ' neurophysiology', ' perceptual maskings', ' psychometrics', ' psychophysics', ' sensory disorders', ' sensory thresholds', ' sequential perception', ' skin', ' somesthesis', ' space perception', ' stimulus /response', ' stimulus interval', ' substitute sight', ' touch', ' vibration perception']",NINDS,INDIANA UNIVERSITY BLOOMINGTON,R01,1985,157630,-0.055616699755679185
"AUTOMATED CORNEAL ENDOTHELIAL CELL MORPHOMETRY The SBIR is aimed at the development of an automated system for corneal endothelial cell counting and morphology analysis.  This system will be useful for both researcher and clinicians.  The incorporation of direct high-resolution acquisition of endothelial cell images, which allows analysis of cell morphology at the time of acquisition, will have a significant impact on clinical uses.  These automated techniques for rapid quantitative analysis of corneal endothelial cell morphology derived from specular microscope images would have a significant impact on evaluating the endothelium pre and post surgery, including intraocular lens implantation and phacoemulsification. Techniques for temporal image registration, based on cell pattern recognition, for patient follow-up will be included and will greatly expand the potential market.  In Phase I, a prototype system for automated endothelial cell counting and morphology analysis will be developed.  This system will be based on image enhancement and morphological techniques for automated endothelial cell discrimination, segmentation and measurement.  The potential for direct high-resolution acquisition of endothelial cell images will also be investigated.  n/a",AUTOMATED CORNEAL ENDOTHELIAL CELL MORPHOMETRY,2164681,R43EY010658,"['artificial intelligence', ' automated medical record system', ' computer assisted diagnosis', ' computer program /software', ' corneal endothelium', ' diagnosis design /evaluation', ' digital imaging', ' eye disorder diagnosis', ' image enhancement', ' method development', ' morphology', ' ophthalmoscopy']",NEI,"LOATS ASSOCIATES, INC.",R43,1994,81000,-0.031961701816989145
"KNOWLEDGE AND AUTOMATIC DIGITAL IMAGE RECOGNITION Long term goal: Automatic screening of computerized tomography (CT) images of the head by i) the creation of a 3D coordinate System from the image file header, ii) use of low level image processing techniques to detect potential anatomical structures and iii) use a knowledge base system (KBS) to classify the structures from a 3D model.  Specific aims from this pilot study: i) Evaluate available KBSs and ii) investigate how variation in KBS construction affects recognition accuracy.  Health relatedness: Automatic image recognition (MR) of CT images may improve diagnostic accuracy without significant cost increase. It would need a system which could classify patients as normal or abnormal which is the first major step in the diagnostic process. Abnormal images would be flagged prior to a radiologist's viewing. The radiologist not the computer would make the diagnosis. In essence the computer would be contributing to a double viewing method which has been shown with radiologists to improve diagnostic accuracy. This should decrease health costs and morbidity.  Only 5 AIR systems of the head have been reported and four of these explicitly incorporeity knowledge in some form. However, the influence of varying the KBS components on recognition success has not been studied.  Research design and methods: i) Potential KBSs will be evaluated for ability to a) interface to image data, b) represent data iconically, c) integration of semantic, spatial and grey level information, d) user interface and e) speed of operation. ii) 30 sets of adult axial CT images will be subjectively examined to identify image features to describe the maxillofacial region of the head. These image features will be detected by writing appropriate 'C' software. Their spatial relationships and distinguishing grey level properties will be stored in the KBS.  Only bone not soft tissue will be studied to simplify the pilot. General anatomical spatial relationships will be manually entered into the KBS model.  Testing different KBS structures: lO patient data sets (of up to 20 images per patient) will be repeatedly examined with variations in the KBS data and rules. The images will automatically be given english labels identifying anatomical points which are either true or false. Outcome: The alteration of KBS structure will be compared to rates of successful anatomical feature recognition.  n/a",KNOWLEDGE AND AUTOMATIC DIGITAL IMAGE RECOGNITION,2131816,R03DE010878,"['artificial intelligence', ' computed axial tomography', ' computer assisted diagnosis', ' diagnosis design /evaluation', ' digital imaging', ' facial bones', ' human data', ' information systems', ' mandible /maxilla', ' musculoskeletal disorder diagnosis', ' radiodiagnosis']",NIDCR,UNIVERSITY OF FLORIDA,R03,1994,46153,-0.011415422421172536
"Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular DESCRIPTION (provided by applicant): A career development plan is proposed for Dr. David Mayerich, a computer scientist who is committed to developing an interdisciplinary career in biomedical engineering, with a focus on the collection and analysis of large-scale data sets at sub-micrometer resolution. His graduate research was in the areas of computer visualization and optical imaging, where his work lead to the development of the prototype Knife-Edge Scanning Microscope (KESM). This is the first instrument capable of imaging three-dimensional macro-scale tissue volumes at sub-micrometer resolution while providing a data rate approaching the transfer speed of most modern computer systems.         Since receiving his Ph.D., Dr. Mayerich worked as a postdoctoral fellow at the Beckman Institute for Advanced Science and Technology at the University of Illinois at Urbana-Champaign, where he has worked with biologists and biomedical engineers to develop tools for the segmentation and classification of large data sets. This provided experience in addressing the needs and limitations of the computational tools available to the interdisciplinary community.        The goal of the mentored phase of this proposal is to provide Dr. Mayerich with the opportunity to work as a developer for the FARSIGHT Toolkit. The FARSIGHT Toolkit is an open-source segmentation toolkit that focuses on developing computer vision algorithms specifically tailored to deal with the unique structures found in microscopy data sets. This project is directed by Prof. Badrinath Roysam at the University of Houston, and was awarded first-place in the NIH-sponsored DIADEM Challenge in neuron segmentation. Dr. Mayerich will use his previous experience in biomedical segmentation, GPU-based computing, and efficient data structures to help make the FARSIGHT Toolkit scalable to the terabyte-scale data sets produced using next-generation high-throughput imaging techniques. Dr. Mayerich will receive mentoring in the algorithms and techniques used in the FARSIGHT Toolkit, as well as valuable experience working on a collaborative software development project.         The goal of the independent phase is to use recently developed imaging techniques, along with scalable segmentation algorithms, to construct complete microvascular models of mouse organs. Recent advances in KESM demonstrate that sub-micrometer images of 1cm3 tissue samples can be collected in less than 50 hours. These images have the resolution and quality necessary for (a) complete reconstruction of microvascular networks in whole organs, and (b) the geometric distribution of cell soma in relation to this network. Models describing cellular and microvascular relationships have implications in several diseases, including neurodegenerative disease and tumor growth, as well as clinical applications in tissue engineering and the quantitative analysis of angiogenic drugs and therapies. PROJECT NARRATIVE The goal of this work is to produce high-resolution microvascular models from mouse brain tissue, as well as create algorithms for querying, distributing, and building models from next-generation high-throughput microscopy data sets. These techniques will allow researchers to create large-scale blood flow simulations, simulate the extent of tissue damage due to stroke or aneurism, and explore the relationships between cells and microvessels on a tissue-wide scale. Clinical applications include the quantification of angiogenesis in tumors and tissue implants, and the quantification of neurovascular effects in neurodegenerative disease models.",Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular,9117645,R00LM011390,"['Active Learning', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Atlases', 'Award', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood Vessels', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Classification', 'Clinical Research', 'Collection', 'Communities', 'Complex', 'Computer Systems', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Doctor of Philosophy', 'Funding', 'Future', 'Goals', 'Hour', 'Illinois', 'Image', 'Imagery', 'Imaging Techniques', 'Implant', 'Institutes', 'Lead', 'Learning', 'Machine Learning', 'Memory', 'Mentors', 'Methods', 'Microscope', 'Microscopy', 'Modeling', 'Mus', 'Neurodegenerative Disorders', 'Neurons', 'Online Systems', 'Organ', 'Pharmacotherapy', 'Phase', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Speed', 'Stroke', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Engineering', 'Tissue Sample', 'Tissue Stains', 'Tissues', 'Training', 'Transgenic Organisms', 'Tumor Angiogenesis', 'Tumor Tissue', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'angiogenesis', 'base', 'brain tissue', 'career', 'career development', 'clinical application', 'computerized tools', 'design', 'experience', 'imaging Segmentation', 'improved', 'instrument', 'learning strategy', 'memory process', 'model building', 'mouse model', 'neuronal cell body', 'neurovascular', 'next generation', 'open source', 'optical imaging', 'programs', 'prototype', 'reconstruction', 'research study', 'simulation', 'software development', 'success', 'terabyte', 'tool', 'tumor growth']",NLM,UNIVERSITY OF HOUSTON,R00,2016,244245,-0.03996495131912587
"Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular DESCRIPTION (provided by applicant): A career development plan is proposed for Dr. David Mayerich, a computer scientist who is committed to developing an interdisciplinary career in biomedical engineering, with a focus on the collection and analysis of large-scale data sets at sub-micrometer resolution. His graduate research was in the areas of computer visualization and optical imaging, where his work lead to the development of the prototype Knife-Edge Scanning Microscope (KESM). This is the first instrument capable of imaging three-dimensional macro-scale tissue volumes at sub-micrometer resolution while providing a data rate approaching the transfer speed of most modern computer systems.         Since receiving his Ph.D., Dr. Mayerich worked as a postdoctoral fellow at the Beckman Institute for Advanced Science and Technology at the University of Illinois at Urbana-Champaign, where he has worked with biologists and biomedical engineers to develop tools for the segmentation and classification of large data sets. This provided experience in addressing the needs and limitations of the computational tools available to the interdisciplinary community.        The goal of the mentored phase of this proposal is to provide Dr. Mayerich with the opportunity to work as a developer for the FARSIGHT Toolkit. The FARSIGHT Toolkit is an open-source segmentation toolkit that focuses on developing computer vision algorithms specifically tailored to deal with the unique structures found in microscopy data sets. This project is directed by Prof. Badrinath Roysam at the University of Houston, and was awarded first-place in the NIH-sponsored DIADEM Challenge in neuron segmentation. Dr. Mayerich will use his previous experience in biomedical segmentation, GPU-based computing, and efficient data structures to help make the FARSIGHT Toolkit scalable to the terabyte-scale data sets produced using next-generation high-throughput imaging techniques. Dr. Mayerich will receive mentoring in the algorithms and techniques used in the FARSIGHT Toolkit, as well as valuable experience working on a collaborative software development project.         The goal of the independent phase is to use recently developed imaging techniques, along with scalable segmentation algorithms, to construct complete microvascular models of mouse organs. Recent advances in KESM demonstrate that sub-micrometer images of 1cm3 tissue samples can be collected in less than 50 hours. These images have the resolution and quality necessary for (a) complete reconstruction of microvascular networks in whole organs, and (b) the geometric distribution of cell soma in relation to this network. Models describing cellular and microvascular relationships have implications in several diseases, including neurodegenerative disease and tumor growth, as well as clinical applications in tissue engineering and the quantitative analysis of angiogenic drugs and therapies. PROJECT NARRATIVE The goal of this work is to produce high-resolution microvascular models from mouse brain tissue, as well as create algorithms for querying, distributing, and building models from next-generation high-throughput microscopy data sets. These techniques will allow researchers to create large-scale blood flow simulations, simulate the extent of tissue damage due to stroke or aneurism, and explore the relationships between cells and microvessels on a tissue-wide scale. Clinical applications include the quantification of angiogenesis in tumors and tissue implants, and the quantification of neurovascular effects in neurodegenerative disease models.",Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular,8920669,R00LM011390,"['Active Learning', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Atlases', 'Award', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood Vessels', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Classification', 'Clinical Research', 'Collection', 'Communities', 'Complex', 'Computer Systems', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Doctor of Philosophy', 'Funding', 'Future', 'Goals', 'Hour', 'Illinois', 'Image', 'Imagery', 'Imaging Techniques', 'Implant', 'Institutes', 'Lead', 'Learning', 'Machine Learning', 'Memory', 'Mentors', 'Methods', 'Microscope', 'Microscopy', 'Modeling', 'Mus', 'Neurodegenerative Disorders', 'Neurons', 'Online Systems', 'Organ', 'Pharmacotherapy', 'Phase', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Speed', 'Stroke', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Engineering', 'Tissue Sample', 'Tissue Stains', 'Tissues', 'Training', 'Transgenic Organisms', 'Tumor Angiogenesis', 'Tumor Tissue', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'angiogenesis', 'base', 'brain tissue', 'career', 'career development', 'clinical application', 'computerized tools', 'design', 'experience', 'imaging Segmentation', 'improved', 'instrument', 'memory process', 'model building', 'mouse model', 'neuronal cell body', 'next generation', 'open source', 'optical imaging', 'programs', 'prototype', 'reconstruction', 'research study', 'simulation', 'software development', 'success', 'tool', 'tumor growth']",NLM,UNIVERSITY OF HOUSTON,R00,2015,239130,-0.03996495131912587
"Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular DESCRIPTION (provided by applicant): A career development plan is proposed for Dr. David Mayerich, a computer scientist who is committed to developing an interdisciplinary career in biomedical engineering, with a focus on the collection and analysis of large-scale data sets at sub-micrometer resolution. His graduate research was in the areas of computer visualization and optical imaging, where his work lead to the development of the prototype Knife-Edge Scanning Microscope (KESM). This is the first instrument capable of imaging three-dimensional macro-scale tissue volumes at sub-micrometer resolution while providing a data rate approaching the transfer speed of most modern computer systems.         Since receiving his Ph.D., Dr. Mayerich worked as a postdoctoral fellow at the Beckman Institute for Advanced Science and Technology at the University of Illinois at Urbana-Champaign, where he has worked with biologists and biomedical engineers to develop tools for the segmentation and classification of large data sets. This provided experience in addressing the needs and limitations of the computational tools available to the interdisciplinary community.        The goal of the mentored phase of this proposal is to provide Dr. Mayerich with the opportunity to work as a developer for the FARSIGHT Toolkit. The FARSIGHT Toolkit is an open-source segmentation toolkit that focuses on developing computer vision algorithms specifically tailored to deal with the unique structures found in microscopy data sets. This project is directed by Prof. Badrinath Roysam at the University of Houston, and was awarded first-place in the NIH-sponsored DIADEM Challenge in neuron segmentation. Dr. Mayerich will use his previous experience in biomedical segmentation, GPU-based computing, and efficient data structures to help make the FARSIGHT Toolkit scalable to the terabyte-scale data sets produced using next-generation high-throughput imaging techniques. Dr. Mayerich will receive mentoring in the algorithms and techniques used in the FARSIGHT Toolkit, as well as valuable experience working on a collaborative software development project.         The goal of the independent phase is to use recently developed imaging techniques, along with scalable segmentation algorithms, to construct complete microvascular models of mouse organs. Recent advances in KESM demonstrate that sub-micrometer images of 1cm3 tissue samples can be collected in less than 50 hours. These images have the resolution and quality necessary for (a) complete reconstruction of microvascular networks in whole organs, and (b) the geometric distribution of cell soma in relation to this network. Models describing cellular and microvascular relationships have implications in several diseases, including neurodegenerative disease and tumor growth, as well as clinical applications in tissue engineering and the quantitative analysis of angiogenic drugs and therapies. PROJECT NARRATIVE The goal of this work is to produce high-resolution microvascular models from mouse brain tissue, as well as create algorithms for querying, distributing, and building models from next-generation high-throughput microscopy data sets. These techniques will allow researchers to create large-scale blood flow simulations, simulate the extent of tissue damage due to stroke or aneurism, and explore the relationships between cells and microvessels on a tissue-wide scale. Clinical applications include the quantification of angiogenesis in tumors and tissue implants, and the quantification of neurovascular effects in neurodegenerative disease models.",Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular,8916335,R00LM011390,"['Active Learning', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Atlases', 'Award', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood Vessels', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Classification', 'Clinical Research', 'Collection', 'Commit', 'Communities', 'Complex', 'Computer Systems', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Doctor of Philosophy', 'Funding', 'Future', 'Goals', 'Hour', 'Illinois', 'Image', 'Imagery', 'Imaging Techniques', 'Implant', 'Institutes', 'Lead', 'Learning', 'Machine Learning', 'Memory', 'Mentors', 'Methods', 'Microscope', 'Microscopy', 'Modeling', 'Mus', 'Neurodegenerative Disorders', 'Neurons', 'Online Systems', 'Organ', 'Pharmacotherapy', 'Phase', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Simulate', 'Speed', 'Stroke', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Engineering', 'Tissue Sample', 'Tissue Stains', 'Tissues', 'Training', 'Transgenic Organisms', 'Tumor Angiogenesis', 'Tumor Tissue', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'angiogenesis', 'base', 'brain tissue', 'career', 'career development', 'clinical application', 'computerized tools', 'design', 'experience', 'imaging Segmentation', 'improved', 'instrument', 'memory process', 'mouse model', 'neuronal cell body', 'next generation', 'open source', 'optical imaging', 'programs', 'prototype', 'reconstruction', 'research study', 'simulation', 'software development', 'success', 'tool', 'tumor growth']",NLM,UNIVERSITY OF HOUSTON,R00,2014,246867,-0.03996495131912587
"Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular     DESCRIPTION (provided by applicant): A career development plan is proposed for Dr. David Mayerich, a computer scientist who is committed to developing an interdisciplinary career in biomedical engineering, with a focus on the collection and analysis of large-scale data sets at sub-micrometer resolution. His graduate research was in the areas of computer visualization and optical imaging, where his work lead to the development of the prototype Knife-Edge Scanning Microscope (KESM). This is the first instrument capable of imaging three-dimensional macro-scale tissue volumes at sub-micrometer resolution while providing a data rate approaching the transfer speed of most modern computer systems.         Since receiving his Ph.D., Dr. Mayerich worked as a postdoctoral fellow at the Beckman Institute for Advanced Science and Technology at the University of Illinois at Urbana-Champaign, where he has worked with biologists and biomedical engineers to develop tools for the segmentation and classification of large data sets. This provided experience in addressing the needs and limitations of the computational tools available to the interdisciplinary community.        The goal of the mentored phase of this proposal is to provide Dr. Mayerich with the opportunity to work as a developer for the FARSIGHT Toolkit. The FARSIGHT Toolkit is an open-source segmentation toolkit that focuses on developing computer vision algorithms specifically tailored to deal with the unique structures found in microscopy data sets. This project is directed by Prof. Badrinath Roysam at the University of Houston, and was awarded first-place in the NIH-sponsored DIADEM Challenge in neuron segmentation. Dr. Mayerich will use his previous experience in biomedical segmentation, GPU-based computing, and efficient data structures to help make the FARSIGHT Toolkit scalable to the terabyte-scale data sets produced using next-generation high-throughput imaging techniques. Dr. Mayerich will receive mentoring in the algorithms and techniques used in the FARSIGHT Toolkit, as well as valuable experience working on a collaborative software development project.         The goal of the independent phase is to use recently developed imaging techniques, along with scalable segmentation algorithms, to construct complete microvascular models of mouse organs. Recent advances in KESM demonstrate that sub-micrometer images of 1cm3 tissue samples can be collected in less than 50 hours. These images have the resolution and quality necessary for (a) complete reconstruction of microvascular networks in whole organs, and (b) the geometric distribution of cell soma in relation to this network. Models describing cellular and microvascular relationships have implications in several diseases, including neurodegenerative disease and tumor growth, as well as clinical applications in tissue engineering and the quantitative analysis of angiogenic drugs and therapies.                  PROJECT NARRATIVE The goal of this work is to produce high-resolution microvascular models from mouse brain tissue, as well as create algorithms for querying, distributing, and building models from next-generation high-throughput microscopy data sets. These techniques will allow researchers to create large-scale blood flow simulations, simulate the extent of tissue damage due to stroke or aneurism, and explore the relationships between cells and microvessels on a tissue-wide scale. Clinical applications include the quantification of angiogenesis in tumors and tissue implants, and the quantification of neurovascular effects in neurodegenerative disease models.",Large-Scale Reconstruction of Microvascular Networks and the Surrounding Cellular,8508596,K99LM011390,"['Active Learning', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Atlases', 'Award', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood Vessels', 'Blood flow', 'Brain', 'Cell Nucleus', 'Cells', 'Classification', 'Clinical Research', 'Collection', 'Commit', 'Communities', 'Complex', 'Computer Systems', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Doctor of Philosophy', 'Funding', 'Future', 'Goals', 'Hour', 'Illinois', 'Image', 'Imagery', 'Imaging Techniques', 'Implant', 'Institutes', 'Lead', 'Learning', 'Machine Learning', 'Memory', 'Mentors', 'Methods', 'Microscope', 'Microscopy', 'Modeling', 'Mus', 'Neurodegenerative Disorders', 'Neurons', 'Online Systems', 'Organ', 'Pharmacotherapy', 'Phase', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Simulate', 'Speed', 'Stroke', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Engineering', 'Tissue Sample', 'Tissue Stains', 'Tissues', 'Training', 'Transgenic Organisms', 'Tumor Angiogenesis', 'Tumor Tissue', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'angiogenesis', 'base', 'brain tissue', 'career', 'career development', 'clinical application', 'computerized tools', 'design', 'experience', 'imaging Segmentation', 'improved', 'instrument', 'memory process', 'mouse model', 'neuronal cell body', 'next generation', 'open source', 'optical imaging', 'programs', 'prototype', 'reconstruction', 'research study', 'simulation', 'software development', 'success', 'tool', 'tumor growth']",NLM,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,K99,2013,87300,-0.03996495131912587
"MULTISPECTRAL FUNDUS IMAGING SYSTEM Recent advances in sensor technology for a broad class of applications           have resulted in the evolution of sophisticated hardware for high                resolution multispectral imaging.  Multispectral imaging and associated          cutting edge multispectral image and data fusion processing software             have led to the realization of techniques which add significantly to the         ability of identifying and characterizing the nature of scenes on the            earth from space, or the identification of other objects from their              multispectral and spatial signatures.                                                                                                                             A multispectral image fusion system is proposed which will present               additional diagnostic information to a clinical or research                      ophthalmologist.  Using state-of-the-art multispectral image capture             system, advanced multispectral image fusion techniques inspired by               the human vision system, and neural network feature classification               algorithms, a Multispectral Image fusion System for detection and                identification of ocular pathological features is described.                      n/a",MULTISPECTRAL FUNDUS IMAGING SYSTEM,2606924,R43EY011675,"['artificial intelligence', ' biomedical equipment development', ' charge coupled device camera', ' clinical biomedical equipment', ' computational neuroscience', ' diagnosis design /evaluation', ' eye disorder diagnosis', ' eye fundus photography', ' fundus oculi', ' hemoglobin', ' image processing']",NEI,KESTREL CORPORATION,R43,1997,99944,-0.07507158090662551
"Rational design of cytokine releasing angiogenic constructs    DESCRIPTION (provided by applicant): The development of organized vascular networks necessitates a tightly regulated interplay between variable cells, growth factors and soluble mediators. The applicant's long-term goal is to develop therapeutic angiogenic strategies based on the rational design of cytokine releasing constructs that promote vascular patterning and vessel stability. The objective of this proposal is to i) develop electrospun, three-dimensional constructs with patterned architecture, ii) demonstrate that the spatial and temporal delivery of two model angiogenic growth factors promotes the formation of an organized capillary network and iii) develop a computational model that can predict the biological effect of a growth factor releasing construct as a function of specified fabrication parameters. We hypothesize that guided therapeutic angiogenesis (i.e. patterned vascular networks) can be obtained by controlling the spatial and temporal presentation of soluble mediators at the site of ischemia. In AIM I, we will synthesize bFGF and G-CSF releasing electrospun constructs and determine their programmed delivery as a function of fabrication parameters. In AIM II, we will demonstrate the effect of spatial and temporal control of cytokine delivery in promoting directed angiogenesis in a three-dimensional in vitro angiogenesis model and we will develop a computational model/software that can predict the biological effect of different scaffold configurations. We will validate our model by assessing the angiogenic potential of our growth factor releasing constructs in a murine critical limb ischemic model.      PUBLIC HEALTH RELEVANCE: We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.           Project narrative We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.",Rational design of cytokine releasing angiogenic constructs,8112574,R21EB012136,"['Animal Model', 'Animals', 'Architecture', 'Biological', 'Blood Vessels', 'Blood capillaries', 'CSF3 gene', 'Cell Proliferation', 'Cell Transplantation', 'Clinical', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Electrodes', 'Fiber', 'Fibroblast Growth Factor 2', 'Gelatin', 'Goals', 'Growth', 'Growth Factor', 'In Vitro', 'Ischemia', 'Kinetics', 'Laboratories', 'Learning', 'Limb structure', 'Machine Learning', 'Mediator of activation protein', 'Modeling', 'Mus', 'Natural regeneration', 'Needles', 'Operative Surgical Procedures', 'Output', 'Pattern', 'Pharmacotherapy', 'Polymers', 'Process', 'Research', 'Series', 'Site', 'Specific qualifier value', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Effect', 'Tissue Engineering', 'angiogenesis', 'base', 'capillary', 'cell growth', 'cell motility', 'cytokine', 'design', 'electric field', 'high risk', 'interest', 'mathematical model', 'minimally invasive', 'multitask', 'nanofiber', 'novel', 'pre-clinical', 'programs', 'public health relevance', 'repaired', 'scaffold', 'spatiotemporal', 'success', 'therapeutic angiogenesis', 'tissue regeneration']",NIBIB,UNIVERSITY OF MIAMI CORAL GABLES,R21,2011,176524,0.005065049930177312
"Rational design of cytokine releasing angiogenic constructs    DESCRIPTION (provided by applicant): The development of organized vascular networks necessitates a tightly regulated interplay between variable cells, growth factors and soluble mediators. The applicant's long-term goal is to develop therapeutic angiogenic strategies based on the rational design of cytokine releasing constructs that promote vascular patterning and vessel stability. The objective of this proposal is to i) develop electrospun, three-dimensional constructs with patterned architecture, ii) demonstrate that the spatial and temporal delivery of two model angiogenic growth factors promotes the formation of an organized capillary network and iii) develop a computational model that can predict the biological effect of a growth factor releasing construct as a function of specified fabrication parameters. We hypothesize that guided therapeutic angiogenesis (i.e. patterned vascular networks) can be obtained by controlling the spatial and temporal presentation of soluble mediators at the site of ischemia. In AIM I, we will synthesize bFGF and G-CSF releasing electrospun constructs and determine their programmed delivery as a function of fabrication parameters. In AIM II, we will demonstrate the effect of spatial and temporal control of cytokine delivery in promoting directed angiogenesis in a three-dimensional in vitro angiogenesis model and we will develop a computational model/software that can predict the biological effect of different scaffold configurations. We will validate our model by assessing the angiogenic potential of our growth factor releasing constructs in a murine critical limb ischemic model.      PUBLIC HEALTH RELEVANCE: We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.           Project narrative We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.",Rational design of cytokine releasing angiogenic constructs,7961101,R21EB012136,"['Animal Model', 'Animals', 'Architecture', 'Biological', 'Blood Vessels', 'Blood capillaries', 'CSF3 gene', 'Cell Proliferation', 'Cell Transplantation', 'Clinical', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Electrodes', 'Fiber', 'Fibroblast Growth Factor 2', 'Gelatin', 'Goals', 'Growth', 'Growth Factor', 'In Vitro', 'Ischemia', 'Kinetics', 'Laboratories', 'Learning', 'Limb structure', 'Machine Learning', 'Mediator of activation protein', 'Modeling', 'Mus', 'Natural regeneration', 'Needles', 'Operative Surgical Procedures', 'Output', 'Pattern', 'Pharmacotherapy', 'Polymers', 'Process', 'Research', 'Series', 'Site', 'Specific qualifier value', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Effect', 'Tissue Engineering', 'angiogenesis', 'base', 'capillary', 'cell growth', 'cell motility', 'cytokine', 'design', 'electric field', 'high risk', 'interest', 'mathematical model', 'minimally invasive', 'multitask', 'nanofiber', 'novel', 'pre-clinical', 'programs', 'public health relevance', 'repaired', 'scaffold', 'success', 'therapeutic angiogenesis', 'tissue regeneration']",NIBIB,UNIVERSITY OF MIAMI CORAL GABLES,R21,2010,221192,0.005065049930177312
"InCell 6000 High Content Instrument for Cellular Systems Biology Program     DESCRIPTION: The General Electric Healthcare, InCell 6000 High Content Analysis (HCA) instrument is being requested to allow investigators from the University of Pittsburgh, the University of Pittsburgh Medical Center and Carnegie Mellon University to analyze complex cellular processes in both fixed and living cells, multicellular model systems and research organisms (Zebra Fish and C. elegans). This high instrument combines a high scan-rate confocal imaging device, an environmental chamber, 4-channel fluorescence, transmitted light and on-board fluid addition in a microplate-based system. The InCell 6000 permits automated, high throughput and high temporal and spatial resolution of subcelluar structures and biochemical reactions that are the basis of all cellular and tissue functions. The long-term objective is to extend our present capabilities in acquiring high temporal, spatial and spectral images from a small sample size to the acquisition of large data sets from populations of cells on a cell-by-cell basis, tissue models and experimental animals. This new capability will permit the application of computational and systems biology tools to understand the complexities of life processes based on statistical significance not possible before. The integration of the best biological experimental systems, fluorescence-based reagents and high throughput, automated imaging will enable large, combinatorial treatments of samples to allow the exploration of statistically relevant mechanisms of action in a variety of therapeutic areas. The NIH funded projects that will use this instrument include investigations on human adipocyte differentiation in cancer, the physiology of stem cell derived cardiomyocytes, the heterogeneity of drug responses in head and neck cancer models, the role of the immune response in cancer therapies, live, 3D breast cancer models with the analysis of pathways, cell migration in tumor models, modulation of Huntington's Disease progression in model systems, protein misfolding disease model of alpha 1- antitrypsin deficiency (ATD), and necrotizing enterocolitis (NEC) models in C. elgans. In addition, a kidney regeneration model in Zebra fish to identify small molecules that stimulate stem cell proliferation, as well as the application of a new generation of genetically encoded biosensors, standards for imaging and flow cytometry and the further application of active machine learning optimization of experimental strategies. The application of this platform to fundamental biomedical research and translational research programs will ultimately lead to better success in drug discovery and development, while helping to define the mechanisms of normal and disease processes.              n/a",InCell 6000 High Content Instrument for Cellular Systems Biology Program,8332956,S10OD012269,"['Animals', 'Area', 'Biochemical Reaction', 'Biological', 'Biological Models', 'Biomedical Research', 'Biosensor', 'Breast Cancer Model', 'Caenorhabditis elegans', 'Cancer Model', 'Cardiac Myocytes', 'Cell Migration Pathway', 'Cell Proliferation', 'Cell physiology', 'Cells', 'Complex', 'Data Set', 'Disease', 'Disease Progression', 'Disease model', 'Flow Cytometry', 'Fluorescence', 'Funding', 'Generations', 'Head and Neck Cancer', 'Healthcare', 'Heterogeneity', 'Human', 'Huntington Disease', 'Image', 'Image Cytometry', 'Imaging Device', 'Immune response', 'Investigation', 'Kidney', 'Lead', 'Life', 'Light', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Medical center', 'Modeling', 'Natural regeneration', 'Necrotizing Enterocolitis', 'Organism', 'Pharmaceutical Preparations', 'Physiology', 'Population', 'Process', 'Reagent', 'Research Personnel', 'Resolution', 'Role', 'Sample Size', 'Sampling', 'Scanning', 'Stem cells', 'Structure', 'System', 'Systems Biology', 'Therapeutic', 'Tissue Model', 'Tissues', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Zebrafish', 'adipocyte differentiation', 'alpha 1-Antitrypsin Deficiency', 'base', 'cancer therapy', 'combinatorial', 'drug development', 'drug discovery', 'instrument', 'programs', 'protein misfolding', 'response', 'small molecule', 'success', 'systems research', 'tool', 'tumor']",OD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,S10,2013,502020,-0.04467194522786127
"DEVELOPMENT OF SPATIAL SOFTWARER This SBIR project will develop software for identifying and correcting spatial patterns in data for a wide range of alcohol-related phenomenon including alcohol consumption, problematic outcomes, and treatment modalities. Identifying and correcting statistical relationships in spatially configured data sets would be invaluable to alcohol-related research, the overall health community, and even to most social scientists (and biomedical researchers). Ecological models or models with locational components that provide unbiased estimates and increased predictive performance enhance the researcher' ability to identify new patterns within alcohol-related phenomenon. While spatial analysis has been widely researched and is a proven statistical technique, commercially available software with reasonable diagnostics and commonly used regression techniques does not yet exist. This phase I project addresses this need and will pursue three objectives: (1) Research and increase the capabilities of the current software package, (2) Design interfaces easily useable (friendly) for alcohol researchers, and (3) Improve the speed and efficiency of the core code. The proposed software development will provide powerful diagnostic and corrective tool in the analysis of mapped data describing relationships between space and alcohol- related phenomenon. PROPOSED COMMERCIAL APPLICATIONS: The need for identifying and adjusting for spatial autocorrelation in alcohol- related data sets is huge (see page 21) and so there is a large market for the proposed statistical software. The proposed package is expected to provide an easy to use, speedy, and comprehensive tool relative to current packages.  n/a",DEVELOPMENT OF SPATIAL SOFTWARER,6073824,R43AA012373,"['alcoholism /alcohol abuse', ' artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' mathematics', ' statistics /biometry']",NIAAA,S-THREE DEVELOPMENT,R43,2000,129935,-0.10644444394151344
"KNOWWARE: AUDIO-SPATIAL GUIS FOR BLIND PEOPLE   DESCRIPTION: (Adapted from the Investigator's Abstract) Audio-Spatial GUIs for       blind people. The PI proposes to use an existing system that records and             recognizes hand gestures and movements (KnowWare), to enable blind users to use      GUIs. In particular the proposed system will provide access to the spatial           format of the Windows operating system, applications and internet browsers. It       will also permit blind programmers to design GUIs as well.                                                                                                                In the proposed system the user's hands rest upon the VIDEODESK which has a          known size grid and are viewed by a ceiling-mounted video camera. The image of       the hands is analyzed by specialized processors and the locations of the             fingers are reported to the computer. The virtual graphic interface is defined       on the desk's surface and the computer generates audio feedback to tell the          user what their finger are ""touching."" One hand will be used for pointing            within windows and the other for selecting from the menus. Voice input will          permit additional commands.                                                                                                                                               For phase I preliminary GUI interfaces will be developed and a tool for              creating graphic contents that will operate under windows will be implemented.       Blind subjects will be used to evaluate the functionality and usefulness of the      developed tools.                                                                     PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                                                                     n/a",KNOWWARE: AUDIO-SPATIAL GUIS FOR BLIND PEOPLE,6017026,R43EY012759,"['artificial intelligence', ' blindness', ' computer graphics /printing', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' fingers', ' human data', ' limb movement', ' sound']",NEI,ARTIFICIAL REALITY CORPORATION,R43,1999,100000,-0.030383448937739907
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,10016840,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data curation', 'data format', 'data resource', 'data warehouse', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'large scale data', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2020,330502,0.009103580540794586
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,9859232,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Dimensions', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'One-Step dentin bonding system', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data format', 'data resource', 'data warehouse', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2019,345016,0.009103580540794586
"Improving the Detection of Activation in High Resolution fMRI using Multivariate DESCRIPTION (provided by applicant): The overall goal of this project is to develop a local multivariate analysis software package for fMRI data analysis. It will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods. This project will lead to better brain activation maps and thus promote the discovery of currently unknown aspects of brain function. Mass-univariate analysis, such as the general linear model (GLM), is the prevailing fMRI data analysis method. However, it suffers from blurring of edges of activation and potential elimination of the detection of weak activated regions due to routinely applied fixed isotropic spatial Gaussian smoothing. Local multivariate methods such as canonical correlation analysis (CCA) and its variants have been shown to significantly increase the detection power of fMRI activations and improve activation maps. As an advantage, CCA uses adaptive spatial filtering kernels to accurately extract the signal better in a noisy environment. However, there are several drawbacks, particularly low spatial specificity, long computational time, and single-factor experimental design limitation. Furthermore, a parametric estimation method does not exist to determine the family-wise error rate, no extension to group analysis has been investigated, and no studies extending local CCA to nonlinear CCA for fMRI data using kernel methods have been systematically carried out. All these drawbacks prevent local CCA methods from being widely accepted in neuroscience research in fMRI. In this proposal, our goals are to eliminate these drawbacks using novel local multivariate analysis methods (based on CCA) and to develop a software tool to widen its broader application in the neuroscience research community. We expect this software tool to be particularly valuable for neuroscience research where detections of weak activations or spatially localized patterns of activations are desired. As high resolution imaging and computer power advance, we expect an increase in demand for this software tool, thus advancing new discoveries of brain function and more precise spatial localization of activations. As a particular application, we will focus on studying memory actions using a novel event-related recognition paradigm to investigate the effects of familiarity and recollection in subregions of the medial temporal lobes (MTL) for high resolution fMRI. This research will advance our understanding of hippocampal/MTL contributions to memory, which can substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer¿s disease, schizophrenia, and major depression. More generally, it will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods. PUBLIC HEALTH RELEVANCE: The proposed research is highly relevant to public health because advanced mathematical and statistical methods will allow better analysis of high-resolution fMRI data leading to better characterization of cortical function. For this project we choose to focus on studying memory activation in the medial temporal lobes using high-resolution imaging and multivariate analysis which could substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer's disease, schizophrenia, and major depression. Our proposed fMRI analysis methods also have great potential for significantly advancing our understanding of other neurological and psychiatric conditions.",Improving the Detection of Activation in High Resolution fMRI using Multivariate,8841351,R01EB014284,"['Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Brain', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Detection', 'Environment', 'Estimation Techniques', 'Event', 'Experimental Designs', 'Face', 'Familiarity', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Hemorrhage', 'Hippocampus (Brain)', 'Image Analysis', 'Individual', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Medial', 'Memory', 'Memory impairment', 'Methods', 'Modeling', 'Morphologic artifacts', 'Multivariate Analysis', 'Neighborhoods', 'Neurologic', 'Neurosciences Research', 'Occupations', 'Pattern', 'Problem Solving', 'Psychologist', 'Public Health', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Software Tools', 'Solutions', 'Specificity', 'Statistical Methods', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Variant', 'Weight', 'base', 'density', 'digital imaging', 'improved', 'interest', 'mathematical methods', 'memory recognition', 'mild cognitive impairment', 'novel', 'prevent', 'statistics', 'theories', 'tool', 'user friendly software']",NIBIB,CLEVELAND CLINIC LERNER COM-CWRU,R01,2015,275802,-0.07021704868305528
"Improving the Detection of Activation in High Resolution fMRI using Multivariate     DESCRIPTION (provided by applicant): The overall goal of this project is to develop a local multivariate analysis software package for fMRI data analysis. It will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods. This project will lead to better brain activation maps and thus promote the discovery of currently unknown aspects of brain function. Mass-univariate analysis, such as the general linear model (GLM), is the prevailing fMRI data analysis method. However, it suffers from blurring of edges of activation and potential elimination of the detection of weak activated regions due to routinely applied fixed isotropic spatial Gaussian smoothing. Local multivariate methods such as canonical correlation analysis (CCA) and its variants have been shown to significantly increase the detection power of fMRI activations and improve activation maps. As an advantage, CCA uses adaptive spatial filtering kernels to accurately extract the signal better in a noisy environment. However, there are several drawbacks, particularly low spatial specificity, long computational time, and single-factor experimental design limitation. Furthermore, a parametric estimation method does not exist to determine the family-wise error rate, no extension to group analysis has been investigated, and no studies extending local CCA to nonlinear CCA for fMRI data using kernel methods have been systematically carried out. All these drawbacks prevent local CCA methods from being widely accepted in neuroscience research in fMRI. In this proposal, our goals are to eliminate these drawbacks using novel local multivariate analysis methods (based on CCA) and to develop a software tool to widen its broader application in the neuroscience research community. We expect this software tool to be particularly valuable for neuroscience research where detections of weak activations or spatially localized patterns of activations are desired. As high resolution imaging and computer power advance, we expect an increase in demand for this software tool, thus advancing new discoveries of brain function and more precise spatial localization of activations. As a particular application, we will focus on studying memory actions using a novel event-related recognition paradigm to investigate the effects of familiarity and recollection in subregions of the medial temporal lobes (MTL) for high resolution fMRI. This research will advance our understanding of hippocampal/MTL contributions to memory, which can substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer¿s disease, schizophrenia, and major depression. More generally, it will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods.         PUBLIC HEALTH RELEVANCE: The proposed research is highly relevant to public health because advanced mathematical and statistical methods will allow better analysis of high-resolution fMRI data leading to better characterization of cortical function. For this project we choose to focus on studying memory activation in the medial temporal lobes using high-resolution imaging and multivariate analysis which could substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer's disease, schizophrenia, and major depression. Our proposed fMRI analysis methods also have great potential for significantly advancing our understanding of other neurological and psychiatric conditions.                ",Improving the Detection of Activation in High Resolution fMRI using Multivariate,8656325,R01EB014284,"['Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Brain', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Detection', 'Environment', 'Estimation Techniques', 'Event', 'Experimental Designs', 'Face', 'Familiarity', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hemorrhage', 'Hippocampus (Brain)', 'Image Analysis', 'Individual', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Medial', 'Memory', 'Memory impairment', 'Methods', 'Modeling', 'Morphologic artifacts', 'Multivariate Analysis', 'Neighborhoods', 'Neurologic', 'Neurosciences Research', 'Occupations', 'Pattern', 'Problem Solving', 'Psychologist', 'Public Health', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Software Tools', 'Solutions', 'Specificity', 'Statistical Methods', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Variant', 'Weight', 'base', 'density', 'digital imaging', 'improved', 'interest', 'mathematical methods', 'memory recognition', 'mild cognitive impairment', 'novel', 'prevent', 'public health relevance', 'statistics', 'theories', 'tool', 'user friendly software']",NIBIB,RYERSON  UNIVERSITY,R01,2014,65141,-0.07021704868305528
"Improving the Detection of Activation in High Resolution fMRI using Multivariate     DESCRIPTION (provided by applicant): The overall goal of this project is to develop a local multivariate analysis software package for fMRI data analysis. It will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods. This project will lead to better brain activation maps and thus promote the discovery of currently unknown aspects of brain function. Mass-univariate analysis, such as the general linear model (GLM), is the prevailing fMRI data analysis method. However, it suffers from blurring of edges of activation and potential elimination of the detection of weak activated regions due to routinely applied fixed isotropic spatial Gaussian smoothing. Local multivariate methods such as canonical correlation analysis (CCA) and its variants have been shown to significantly increase the detection power of fMRI activations and improve activation maps. As an advantage, CCA uses adaptive spatial filtering kernels to accurately extract the signal better in a noisy environment. However, there are several drawbacks, particularly low spatial specificity, long computational time, and single-factor experimental design limitation. Furthermore, a parametric estimation method does not exist to determine the family-wise error rate, no extension to group analysis has been investigated, and no studies extending local CCA to nonlinear CCA for fMRI data using kernel methods have been systematically carried out. All these drawbacks prevent local CCA methods from being widely accepted in neuroscience research in fMRI. In this proposal, our goals are to eliminate these drawbacks using novel local multivariate analysis methods (based on CCA) and to develop a software tool to widen its broader application in the neuroscience research community. We expect this software tool to be particularly valuable for neuroscience research where detections of weak activations or spatially localized patterns of activations are desired. As high resolution imaging and computer power advance, we expect an increase in demand for this software tool, thus advancing new discoveries of brain function and more precise spatial localization of activations. As a particular application, we will focus on studying memory actions using a novel event-related recognition paradigm to investigate the effects of familiarity and recollection in subregions of the medial temporal lobes (MTL) for high resolution fMRI. This research will advance our understanding of hippocampal/MTL contributions to memory, which can substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer¿s disease, schizophrenia, and major depression. More generally, it will provide psychologists and neuroscientists a more powerful tool to analyze their fMRI data using advanced multivariate methods.         PUBLIC HEALTH RELEVANCE: The proposed research is highly relevant to public health because advanced mathematical and statistical methods will allow better analysis of high-resolution fMRI data leading to better characterization of cortical function. For this project we choose to focus on studying memory activation in the medial temporal lobes using high-resolution imaging and multivariate analysis which could substantially advance our understanding of the memory deficits associated with a number of debilitating neurological and psychiatric conditions that show abnormalities in these regions, including mild cognitive impairment (MCI), Alzheimer's disease, schizophrenia, and major depression. Our proposed fMRI analysis methods also have great potential for significantly advancing our understanding of other neurological and psychiatric conditions.                ",Improving the Detection of Activation in High Resolution fMRI using Multivariate,8438968,R01EB014284,"['Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Brain', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Detection', 'Environment', 'Estimation Techniques', 'Event', 'Experimental Designs', 'Face', 'Familiarity', 'Family', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hemorrhage', 'Hippocampus (Brain)', 'Image Analysis', 'Individual', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Major Depressive Disorder', 'Maps', 'Medial', 'Memory', 'Memory impairment', 'Methods', 'Modeling', 'Morphologic artifacts', 'Multivariate Analysis', 'Neighborhoods', 'Neurologic', 'Neurosciences Research', 'Occupations', 'Pattern', 'Problem Solving', 'Psychologist', 'Public Health', 'Research', 'Resolution', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Software Tools', 'Solutions', 'Specificity', 'Statistical Methods', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Variant', 'Weight', 'base', 'density', 'digital imaging', 'improved', 'interest', 'memory recognition', 'mild cognitive impairment', 'novel', 'prevent', 'public health relevance', 'statistics', 'theories', 'tool', 'user friendly software']",NIBIB,RYERSON  UNIVERSITY,R01,2013,279190,-0.07021704868305528
"Shearography for Non-Invasive Dental Health Evaluation   DESCRIPTION: Optical holography has been applied to non-invasive clinical            diagnosis and monitoring of Dental and oral/facial pathology, but has been           sharply limited in its usefulness by its requirements of high laser coherence.       absolute stability of setup. and wet processing of holograms. Physical Optics        Corporation (POC) proposes to develop a shearographic micro-optic camera as a        novel means of non-invasive Dental evaluation and characterization based on          shearing speckle interferometry, miniature camera imaging, and proprietary           neural network image processing. The innovation in this concept is the use of        shearography to avoid the need for high stability, high temporal coherence, and      wet processing. Nearfield shearography will have high spatial resolution, and        the neural network will perform real-time data processing and display.                                                                                                    The unique high resolution. real-time operation. low cost. and miniaturization       will make this device attractive to a large commercial market in Dental and          clinical applications.                                                                                                                                                    In Phase 1. POC will develop a miniature shearographic micro-optic camera            (SMOC) with fiber light delivery. a micro-CCD imaging component. and neural          network. It will be capable of distinguishing among tooth enamel, cementum,           dentine, pulp, and soft tissue.                                                      PROPOSED COMMERCIAL APPLICATION:  This compact, low-cost, high resolution non-invasive shearography device will represent a  technological breakthrough not only for oral diagnostics but also for biomedical imaging in  general.  Because of its high resolution, real-time operation, and immunity to vibration, it will  also have wide applications beyond the medical field, particularly for industrial diagnostics.  High-strength aerospace composite material evaluation and testing as well as weld and pipe  defect inspection are areas where it will be particularly welcome.                                                                                     n/a",Shearography for Non-Invasive Dental Health Evaluation,6404393,R43DE014307,"['artificial intelligence', ' bioimaging /biomedical imaging', ' dental disorder diagnosis', ' diagnosis design /evaluation', ' fiber optics', ' image processing', ' interferometry', ' lasers', ' noninvasive diagnosis', ' oral health', ' video recording system']",NIDCR,PHYSICAL OPTICS CORPORATION,R43,2001,100000,0.00885077186475977
"INTERNAL BONDING IN PROTEINS    DESCRIPTION (provided by applicant): Our long-term objective is to ascertain how protein conformation plays a role in biological function and in various diseases. Our specific aims are to finish our development of our physics-based united-residue (UNRES) approach to the protein folding problem, i.e., to compute structure, folding pathways, and thermodynamic and dynamic properties. This involves replacing the last remaining knowledge-based term, corresponding to side chain-side chain interactions, by physics-based terms, extension of UNRES to simulate folding of disulfide-containing proteins, and to treat the lipid-membrane environment. At the all-atom level, we will treat the pH-dependent ionization of side chains (including solvation), and the use of 13C1 chemical shifts in protein-structure simulation. We will continue the development of our UNRES model of nucleic acids (NA-UNRES) and merge UNRES and NA-UNRES into a viable package, which will be provided to the community. We will also continue the developments of sampling techniques and parallelization of UNRES/MD to carry out simulations of very large single-chain and oligomeric proteins and their complexes, and develop tools, based on Principal Component Analysis (PCA) for the analysis of mesoscopic-dynamics trajectories. We will demonstrate how these aims can lead to valid predictions of structures and folding pathways of proteins, and protein-nucleic acid and protein-protein complexes. Our main focus will then involve the application of this methodology to a biological problem: the mechanism of action of the human HSP70 chaperone.      PUBLIC HEALTH RELEVANCE: As pointed out in the Project Summary, the long-term objective of this research is to ascertain how protein conformation plays a role in various diseases. Examples of such diseases in which conformation plays a role are sickle cell anemia (1) and amyloid diseases such as Alzheimer's (2) and mad cow disease.                 As pointed out in the Project Summary, the long-term objective of this research is to ascertain how protein conformation plays a role in various diseases. Examples of such diseases in which conformation plays a role are sickle cell anemia (1) and amyloid diseases such as Alzheimer's (2) and mad cow disease.",INTERNAL BONDING IN PROTEINS,8274782,R01GM014312,"['Alzheimer&apos', 's Disease', 'Amyloidosis', 'Biological', 'Biological Process', 'Bovine Spongiform Encephalopathy', 'Cereals', 'Chemicals', 'Communities', 'Complex', 'Computer Systems', 'Coupling', 'Databases', 'Dependence', 'Development', 'Disease', 'Disulfides', 'Environment', 'Free Energy', 'Goals', 'Grant', 'Heat-Shock Proteins 70', 'Homology Modeling', 'Human', 'Kinetics', 'Lead', 'Membrane', 'Membrane Lipids', 'Methodology', 'Methods', 'Modeling', 'Molecular Chaperones', 'Molecular Conformation', 'Nucleic Acids', 'Pathway interactions', 'Physics', 'Physiological', 'Play', 'Potential Energy', 'Principal Component Analysis', 'Procedures', 'Property', 'Protein Conformation', 'Proteins', 'Research', 'Role', 'Sampling', 'Sickle Cell Anemia', 'Side', 'Simulate', 'Solvents', 'Staging', 'Structure', 'Techniques', 'Temperature', 'Therapeutic Agents', 'Thermodynamics', 'Time', 'Work', 'base', 'design', 'improved', 'innovation', 'ionization', 'knowledge base', 'millisecond', 'molecular dynamics', 'parallel computer', 'protein complex', 'protein folding', 'protein structure', 'public health relevance', 'simulation', 'tool']",NIGMS,CORNELL UNIVERSITY,R01,2012,513448,-0.028723275236477903
"INTERNAL BONDING IN PROTEINS    DESCRIPTION (provided by applicant): Our long-term objective is to ascertain how protein conformation plays a role in biological function and in various diseases. Our specific aims are to finish our development of our physics-based united-residue (UNRES) approach to the protein folding problem, i.e., to compute structure, folding pathways, and thermodynamic and dynamic properties. This involves replacing the last remaining knowledge-based term, corresponding to side chain-side chain interactions, by physics-based terms, extension of UNRES to simulate folding of disulfide-containing proteins, and to treat the lipid-membrane environment. At the all-atom level, we will treat the pH-dependent ionization of side chains (including solvation), and the use of 13C1 chemical shifts in protein-structure simulation. We will continue the development of our UNRES model of nucleic acids (NA-UNRES) and merge UNRES and NA-UNRES into a viable package, which will be provided to the community. We will also continue the developments of sampling techniques and parallelization of UNRES/MD to carry out simulations of very large single-chain and oligomeric proteins and their complexes, and develop tools, based on Principal Component Analysis (PCA) for the analysis of mesoscopic-dynamics trajectories. We will demonstrate how these aims can lead to valid predictions of structures and folding pathways of proteins, and protein-nucleic acid and protein-protein complexes. Our main focus will then involve the application of this methodology to a biological problem: the mechanism of action of the human HSP70 chaperone.      PUBLIC HEALTH RELEVANCE: As pointed out in the Project Summary, the long-term objective of this research is to ascertain how protein conformation plays a role in various diseases. Examples of such diseases in which conformation plays a role are sickle cell anemia (1) and amyloid diseases such as Alzheimer's (2) and mad cow disease.                 As pointed out in the Project Summary, the long-term objective of this research is to ascertain how protein conformation plays a role in various diseases. Examples of such diseases in which conformation plays a role are sickle cell anemia (1) and amyloid diseases such as Alzheimer's (2) and mad cow disease.",INTERNAL BONDING IN PROTEINS,8080184,R01GM014312,"['Alzheimer&apos', 's Disease', 'Amyloidosis', 'Biological', 'Biological Process', 'Bovine Spongiform Encephalopathy', 'Cereals', 'Chemicals', 'Communities', 'Complex', 'Computer Systems', 'Coupling', 'Databases', 'Dependence', 'Development', 'Disease', 'Disulfides', 'Environment', 'Free Energy', 'Goals', 'Grant', 'Heat-Shock Proteins 70', 'Homology Modeling', 'Human', 'Kinetics', 'Lead', 'Membrane', 'Membrane Lipids', 'Methodology', 'Methods', 'Modeling', 'Molecular Chaperones', 'Molecular Conformation', 'Nucleic Acids', 'Pathway interactions', 'Physics', 'Physiological', 'Play', 'Potential Energy', 'Principal Component Analysis', 'Procedures', 'Property', 'Protein Conformation', 'Proteins', 'Research', 'Role', 'Sampling', 'Sickle Cell Anemia', 'Side', 'Simulate', 'Solvents', 'Staging', 'Structure', 'Techniques', 'Temperature', 'Therapeutic Agents', 'Thermodynamics', 'Time', 'Work', 'base', 'design', 'improved', 'innovation', 'ionization', 'knowledge base', 'millisecond', 'molecular dynamics', 'parallel computer', 'protein complex', 'protein folding', 'protein structure', 'public health relevance', 'simulation', 'tool']",NIGMS,CORNELL UNIVERSITY,R01,2011,513065,-0.028723275236477903
"INTERNAL BONDING IN PROTEINS    DESCRIPTION (provided by applicant): Our long-term objective is to ascertain how protein conformation plays a role in biological function and in various diseases. Our specific aims are to finish our development of our physics-based united-residue (UNRES) approach to the protein folding problem, i.e., to compute structure, folding pathways, and thermodynamic and dynamic properties. This involves replacing the last remaining knowledge-based term, corresponding to side chain-side chain interactions, by physics-based terms, extension of UNRES to simulate folding of disulfide-containing proteins, and to treat the lipid-membrane environment. At the all-atom level, we will treat the pH-dependent ionization of side chains (including solvation), and the use of 13C1 chemical shifts in protein-structure simulation. We will continue the development of our UNRES model of nucleic acids (NA-UNRES) and merge UNRES and NA-UNRES into a viable package, which will be provided to the community. We will also continue the developments of sampling techniques and parallelization of UNRES/MD to carry out simulations of very large single-chain and oligomeric proteins and their complexes, and develop tools, based on Principal Component Analysis (PCA) for the analysis of mesoscopic-dynamics trajectories. We will demonstrate how these aims can lead to valid predictions of structures and folding pathways of proteins, and protein-nucleic acid and protein-protein complexes. Our main focus will then involve the application of this methodology to a biological problem: the mechanism of action of the human HSP70 chaperone.      PUBLIC HEALTH RELEVANCE: As pointed out in the Project Summary, the long-term objective of this research is to ascertain how protein conformation plays a role in various diseases. Examples of such diseases in which conformation plays a role are sickle cell anemia (1) and amyloid diseases such as Alzheimer's (2) and mad cow disease.                 As pointed out in the Project Summary, the long-term objective of this research is to ascertain how protein conformation plays a role in various diseases. Examples of such diseases in which conformation plays a role are sickle cell anemia (1) and amyloid diseases such as Alzheimer's (2) and mad cow disease.",INTERNAL BONDING IN PROTEINS,7943357,R01GM014312,"['Alzheimer&apos', 's Disease', 'Amyloidosis', 'Biological', 'Biological Process', 'Bovine Spongiform Encephalopathy', 'Cereals', 'Chemicals', 'Communities', 'Complex', 'Computer Systems', 'Coupling', 'Databases', 'Dependence', 'Development', 'Disease', 'Disulfides', 'Environment', 'Free Energy', 'Goals', 'Grant', 'Heat-Shock Proteins 70', 'Homology Modeling', 'Human', 'Kinetics', 'Lead', 'Membrane', 'Membrane Lipids', 'Methodology', 'Methods', 'Modeling', 'Molecular Chaperones', 'Molecular Conformation', 'Nucleic Acids', 'Pathway interactions', 'Physics', 'Physiological', 'Play', 'Potential Energy', 'Principal Component Analysis', 'Procedures', 'Property', 'Protein Conformation', 'Proteins', 'Research', 'Role', 'Sampling', 'Sickle Cell Anemia', 'Side', 'Simulate', 'Solvents', 'Staging', 'Structure', 'Techniques', 'Temperature', 'Therapeutic Agents', 'Thermodynamics', 'Time', 'Work', 'base', 'design', 'improved', 'innovation', 'ionization', 'knowledge base', 'millisecond', 'molecular dynamics', 'parallel computer', 'protein complex', 'protein folding', 'protein structure', 'public health relevance', 'simulation', 'tool']",NIGMS,CORNELL UNIVERSITY,R01,2010,462869,-0.028723275236477903
"INTERNAL BONDING IN PROTEINS    DESCRIPTION (provided by applicant): Our long-term objective is to ascertain how protein conformation plays a role in biological function and in various diseases. Our specific aims are to finish our development of our physics-based united-residue (UNRES) approach to the protein folding problem, i.e., to compute structure, folding pathways, and thermodynamic and dynamic properties. This involves replacing the last remaining knowledge-based term, corresponding to side chain-side chain interactions, by physics-based terms, extension of UNRES to simulate folding of disulfide-containing proteins, and to treat the lipid-membrane environment. At the all-atom level, we will treat the pH-dependent ionization of side chains (including solvation), and the use of 13C1 chemical shifts in protein-structure simulation. We will continue the development of our UNRES model of nucleic acids (NA-UNRES) and merge UNRES and NA-UNRES into a viable package, which will be provided to the community. We will also continue the developments of sampling techniques and parallelization of UNRES/MD to carry out simulations of very large single-chain and oligomeric proteins and their complexes, and develop tools, based on Principal Component Analysis (PCA) for the analysis of mesoscopic-dynamics trajectories. We will demonstrate how these aims can lead to valid predictions of structures and folding pathways of proteins, and protein-nucleic acid and protein-protein complexes. Our main focus will then involve the application of this methodology to a biological problem: the mechanism of action of the human HSP70 chaperone.       PUBLIC HEALTH RELEVANCE: As pointed out in the Project Summary, the long-term objective of this research is to ascertain how protein conformation plays a role in various diseases. Examples of such diseases in which conformation plays a role are sickle cell anemia (1) and amyloid diseases such as Alzheimer's (2) and mad cow disease.               ",INTERNAL BONDING IN PROTEINS,8470167,R01GM014312,"['Alzheimer&apos', 's Disease', 'Amyloidosis', 'Biological', 'Biological Process', 'Bovine Spongiform Encephalopathy', 'Cereals', 'Chemicals', 'Communities', 'Complex', 'Computer Systems', 'Coupling', 'Databases', 'Dependence', 'Development', 'Disease', 'Disulfides', 'Environment', 'Free Energy', 'Goals', 'Grant', 'Heat-Shock Proteins 70', 'Homology Modeling', 'Human', 'Kinetics', 'Lead', 'Membrane', 'Membrane Lipids', 'Methodology', 'Methods', 'Modeling', 'Molecular Chaperones', 'Molecular Conformation', 'Nucleic Acids', 'Pathway interactions', 'Physics', 'Physiological', 'Play', 'Potential Energy', 'Principal Component Analysis', 'Procedures', 'Property', 'Protein Conformation', 'Proteins', 'Research', 'Role', 'Sampling', 'Sickle Cell Anemia', 'Side', 'Simulate', 'Solvents', 'Staging', 'Structure', 'Techniques', 'Temperature', 'Therapeutic Agents', 'Thermodynamics', 'Time', 'Work', 'base', 'design', 'improved', 'innovation', 'ionization', 'knowledge base', 'millisecond', 'molecular dynamics', 'parallel computer', 'protein complex', 'protein folding', 'protein structure', 'public health relevance', 'simulation', 'tool']",NIGMS,CORNELL UNIVERSITY,R01,2013,469561,-0.04095125218833297
"Integrating Multimodal Brain Imaging Data to Assess Subtle Cognitive Impairment    DESCRIPTION (provided by applicant): Functional neuroimaging studies of the human brain have become increasingly important in the understanding of normal and pathological processes of cognition. Sophisticated statistical analytic frameworks have been developed to locate signal change and define brain networks involved in various tasks. However, in subtle cognitive impairment-e.g., exposure-related illness, early stages of degenerative diseases, injury, secondary illness following adjuvant therapy for cancer-these methods tend to have low sensitivity for detecting small changes in brain states resulting from mild brain dysfunction. An understanding of disease mechanism or progression of subtle cognitive dysfunction requires a novel statistical analytic framework with improved sensitivity to measure small changes in brain states. We have developed an innovative methodology that we successfully applied in measures of regional cerebral blood flow experiments. These methods use well established spatial modeling procedures, new to the functional brain imaging field, to derive statistically optimal spatial summaries within effective resolution groups or ""kriging"", shown by preliminary studies to improve signal detection sensitivity and mitigate the multiple testing burden. Within this new spatial modeling framework, we propose to extend the kriging methodology to fMRI and EEG, modify existing techniques for characterizing brain networks of connectivity (e.g., kriging-based independent components analysis), and integrate the imaging modalities using a statistical classifier based on derived inputs of data driven effective resolution groups. Our primary goal is to develop this analysis framework to provide insight into the neurophysiological mechanisms of mild cognitive dysfunction. Achieving this goal may suggest treatments to alleviate symptoms, prevent progression, or at minimum, provide an informed clinical management of cognitively impaired patients.        Cognitive impairment is a major health concern, affecting people of all ages. Causes range from traumatic injury to toxin exposures, including chemotherapy for cancer treatment, to degenerative diseases. Mechanisms of damage or disease remain difficult to establish using current methods in functional brain imaging studies due to an inability to measure very small changes in brain states. We propose a new analytic framework using existing technology to improve the ability to measure subtle changes important in the understanding of disease pathology of impaired cognition and to greatly facilitate the integration of information from several imaging modalities with potential implications for clinical management and treatment.            ",Integrating Multimodal Brain Imaging Data to Assess Subtle Cognitive Impairment,8445205,R21EB014563,"['Adjuvant Therapy', 'Affect', 'Age', 'Biological Markers', 'Brain', 'Brain imaging', 'Brain region', 'Cerebrovascular Circulation', 'Classification', 'Clinical', 'Clinical Management', 'Clinical Treatment', 'Cognition', 'Data', 'Databases', 'Degenerative Disorder', 'Detection', 'Development', 'Diagnosis', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Functional Imaging', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Goals', 'Health', 'Human', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Injury', 'Ions', 'Lead', 'Linear Models', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Neurocognitive', 'Neurotoxins', 'Outcome', 'Output', 'Pathologic Processes', 'Pathology', 'Patients', 'Procedures', 'Process', 'Property', 'Reliance', 'Research Personnel', 'Resolution', 'Sampling', 'Secondary to', 'Semantic memory', 'Sensory Process', 'Short-Term Memory', 'Signal Transduction', 'Source', 'Spin Labels', 'Staging', 'Statistical Methods', 'Symptoms', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxin', 'Weight', 'Work', 'base', 'behavior measurement', 'behavior test', 'cancer therapy', 'case-based', 'chemobrain', 'chemotherapy', 'data reduction', 'executive function', 'image registration', 'imaging modality', 'improved', 'independent component analysis', 'innovation', 'insight', 'mild cognitive impairment', 'neurocognitive test', 'neuroimaging', 'neurophysiology', 'novel', 'prevent', 'research study', 'single photon emission computed tomography', 'tool', 'treatment strategy']",NIBIB,UNIVERSITY OF TEXAS DALLAS,R21,2013,184069,-0.029986644120905764
"Integrating Multimodal Brain Imaging Data to Assess Subtle Cognitive Impairment    DESCRIPTION (provided by applicant): Functional neuroimaging studies of the human brain have become increasingly important in the understanding of normal and pathological processes of cognition. Sophisticated statistical analytic frameworks have been developed to locate signal change and define brain networks involved in various tasks. However, in subtle cognitive impairment-e.g., exposure-related illness, early stages of degenerative diseases, injury, secondary illness following adjuvant therapy for cancer-these methods tend to have low sensitivity for detecting small changes in brain states resulting from mild brain dysfunction. An understanding of disease mechanism or progression of subtle cognitive dysfunction requires a novel statistical analytic framework with improved sensitivity to measure small changes in brain states. We have developed an innovative methodology that we successfully applied in measures of regional cerebral blood flow experiments. These methods use well established spatial modeling procedures, new to the functional brain imaging field, to derive statistically optimal spatial summaries within effective resolution groups or ""kriging"", shown by preliminary studies to improve signal detection sensitivity and mitigate the multiple testing burden. Within this new spatial modeling framework, we propose to extend the kriging methodology to fMRI and EEG, modify existing techniques for characterizing brain networks of connectivity (e.g., kriging-based independent components analysis), and integrate the imaging modalities using a statistical classifier based on derived inputs of data driven effective resolution groups. Our primary goal is to develop this analysis framework to provide insight into the neurophysiological mechanisms of mild cognitive dysfunction. Achieving this goal may suggest treatments to alleviate symptoms, prevent progression, or at minimum, provide an informed clinical management of cognitively impaired patients.      PUBLIC HEALTH RELEVANCE: Cognitive impairment is a major health concern, affecting people of all ages. Causes range from traumatic injury to toxin exposures, including chemotherapy for cancer treatment, to degenerative diseases. Mechanisms of damage or disease remain difficult to establish using current methods in functional brain imaging studies due to an inability to measure very small changes in brain states. We propose a new analytic framework using existing technology to improve the ability to measure subtle changes important in the understanding of disease pathology of impaired cognition and to greatly facilitate the integration of information from several imaging modalities with potential implications for clinical management and treatment.              Cognitive impairment is a major health concern, affecting people of all ages. Causes range from traumatic injury to toxin exposures, including chemotherapy for cancer treatment, to degenerative diseases. Mechanisms of damage or disease remain difficult to establish using current methods in functional brain imaging studies due to an inability to measure very small changes in brain states. We propose a new analytic framework using existing technology to improve the ability to measure subtle changes important in the understanding of disease pathology of impaired cognition and to greatly facilitate the integration of information from several imaging modalities with potential implications for clinical management and treatment.            ",Integrating Multimodal Brain Imaging Data to Assess Subtle Cognitive Impairment,8229843,R21EB014563,"['Adjuvant Therapy', 'Affect', 'Age', 'Biological Markers', 'Brain', 'Brain imaging', 'Brain region', 'Cerebrovascular Circulation', 'Classification', 'Clinical', 'Clinical Management', 'Clinical Treatment', 'Cognition', 'Data', 'Databases', 'Degenerative Disorder', 'Detection', 'Development', 'Diagnosis', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Functional Imaging', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Goals', 'Health', 'Human', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Injury', 'Ions', 'Lead', 'Linear Models', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Measures', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Neurocognitive', 'Neurotoxins', 'Outcome', 'Output', 'Pathologic Processes', 'Pathology', 'Patients', 'Procedures', 'Process', 'Property', 'Reliance', 'Research Personnel', 'Resolution', 'Sampling', 'Secondary to', 'Semantic memory', 'Sensory Process', 'Short-Term Memory', 'Signal Transduction', 'Source', 'Spin Labels', 'Staging', 'Statistical Methods', 'Symptoms', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxin', 'Weight', 'Work', 'base', 'behavior measurement', 'behavior test', 'cancer therapy', 'case-based', 'chemobrain', 'chemotherapy', 'data reduction', 'executive function', 'image registration', 'imaging modality', 'improved', 'independent component analysis', 'innovation', 'insight', 'mild neurocognitive impairment', 'neurocognitive test', 'neuroimaging', 'neurophysiology', 'novel', 'prevent', 'research study', 'single photon emission computed tomography', 'tool', 'treatment strategy']",NIBIB,UT SOUTHWESTERN MEDICAL CENTER,R21,2012,173138,-0.049290288581422
"Shifting auditory spatial attention: cognitive and neural mechanisms ﻿    DESCRIPTION (provided by applicant): Hearing can serve as an early warning system because it can panoramically monitor the environment for events happening at a distance or out of sight. These ecological considerations make the auditory system particularly useful for studying mechanisms for shifting spatial attention. The focus of this project is on the interplay between top-down and bottom-up spatial attention biases that govern shifting auditory attention to distractors during performance of a simple spatial attention task. The overall goal of this proposal is to use an interdisciplinary approach to better understand at the cognitive and neural levels of analysis how auditory attention is distributed over space. We will test the hypothesis that the spatial distribution of auditory attentional emerges from interactions among two basic factors. The first factor is a voluntary (top-down) attention bias that weakens with distance from the current focus of attention. Conversely, the second factor is an automatic (bottom-up) bias that is tuned to shift attention to unexpected events away from the current focus of attention. The first aim tests a computational model of top-down and bottom-up attention bias in shifting auditory spatial attention. Different aspects of the model will be quantitatively tested against findings from behavioral experiments, and observations will be used to further develop the model. The computational model will incorporate artificial intelligence methods to represent human cognitive processes. The second aim uses transcranial magnetic stimulation to test the role of key right hemisphere cortical areas in shifting of auditory spatial attention. We focus on neural mechanisms of bottom-up attentional bias. The third aim tests whether auditory attention gradients become less focused over time. This will determine how gradients relate to cognitive resources and neural blood flow measures that are known to decline with extended vigilance. This project will help advance knowledge in the field of auditory attention and cognitive hearing, and also addresses general issues in attention research such as top-down and bottom-up processes, vigilance, and their relations to neurobiological attention networks. Outcomes will have practical significance because shifting auditory attention is vital for avoiding accidents at ll ages, maintaining independent living at older ages, and is applicable to neurological and psychiatric disorders having attentional impairments (e.g. PTSD, attention deficit disorder, schizophrenia, stroke, dementia). PUBLIC HEALTH RELEVANCE: The project outcomes could have clinical in neurological and psychiatric disorders having attentional impairments (e.g. PTSD, attention deficit disorder, schizophrenia, stroke, dementia). The project will examine mechanisms of normal cognitive aging using computational modeling, which can inform the early detection, differential diagnosis, and treatment monitoring of age-related neurodegenerative disorders such as Alzheimer's disease. Knowledge from the transcranial magnetic and electrical DC stimulation experiments may have translational applications for rehabilitation following brain injuries.",Shifting auditory spatial attention: cognitive and neural mechanisms,9718188,R01DC014736,"['Accidents', 'Address', 'Affect', 'Age', 'Alzheimer&apos', 's Disease', 'Area', 'Artificial Intelligence', 'Attention', 'Attention Deficit Disorder', 'Auditory', 'Auditory system', 'Behavioral', 'Blood flow', 'Brain Injuries', 'Clinical', 'Cognitive', 'Cognitive aging', 'Computer Simulation', 'Dementia', 'Differential Diagnosis', 'Diffuse', 'Dimensions', 'Early Diagnosis', 'Electroencephalography', 'Environment', 'Event', 'Goals', 'Hearing', 'Human', 'Impairment', 'Independent Living', 'Individual Differences', 'Inferior', 'Knowledge', 'Learning', 'Left', 'Location', 'Loudness', 'Magnetic Resonance Imaging', 'Magnetism', 'Maps', 'Measures', 'Mediating', 'Mental disorders', 'Methods', 'Modeling', 'Monitor', 'Music', 'Neurobiology', 'Obstruction', 'Outcome', 'Parietal', 'Performance', 'Positioning Attribute', 'Post-Traumatic Stress Disorders', 'Probability', 'Process', 'Property', 'Reaction Time', 'Rehabilitation therapy', 'Reproduction', 'Research', 'Resources', 'Role', 'Schizophrenia', 'Short-Term Memory', 'Site', 'Spatial Distribution', 'Stimulus', 'Stroke', 'System', 'Testing', 'Time', 'Transcranial magnetic stimulation', 'Vision', 'Work', 'age related neurodegeneration', 'alpha-SNAP', 'attentional bias', 'attentional control', 'base', 'behavior measurement', 'cognitive process', 'cost shifting', 'experimental study', 'fighting', 'image guided', 'indexing', 'information processing', 'interdisciplinary approach', 'nervous system disorder', 'neural correlate', 'neural stimulation', 'neuroimaging', 'neuromechanism', 'normal aging', 'prefrontal lobe', 'public health relevance', 'relating to nervous system', 'response', 'sound', 'theories', 'vector', 'vigilance']",NIDCD,UNIVERSITY OF TEXAS SAN ANTONIO,R01,2019,263449,-0.02351350262921403
"Shifting auditory spatial attention: cognitive and neural mechanisms ﻿    DESCRIPTION (provided by applicant): Hearing can serve as an early warning system because it can panoramically monitor the environment for events happening at a distance or out of sight. These ecological considerations make the auditory system particularly useful for studying mechanisms for shifting spatial attention. The focus of this project is on the interplay between top-down and bottom-up spatial attention biases that govern shifting auditory attention to distractors during performance of a simple spatial attention task. The overall goal of this proposal is to use an interdisciplinary approach to better understand at the cognitive and neural levels of analysis how auditory attention is distributed over space. We will test the hypothesis that the spatial distribution of auditory attentional emerges from interactions among two basic factors. The first factor is a voluntary (top-down) attention bias that weakens with distance from the current focus of attention. Conversely, the second factor is an automatic (bottom-up) bias that is tuned to shift attention to unexpected events away from the current focus of attention. The first aim tests a computational model of top-down and bottom-up attention bias in shifting auditory spatial attention. Different aspects of the model will be quantitatively tested against findings from behavioral experiments, and observations will be used to further develop the model. The computational model will incorporate artificial intelligence methods to represent human cognitive processes. The second aim uses transcranial magnetic stimulation to test the role of key right hemisphere cortical areas in shifting of auditory spatial attention. We focus on neural mechanisms of bottom-up attentional bias. The third aim tests whether auditory attention gradients become less focused over time. This will determine how gradients relate to cognitive resources and neural blood flow measures that are known to decline with extended vigilance. This project will help advance knowledge in the field of auditory attention and cognitive hearing, and also addresses general issues in attention research such as top-down and bottom-up processes, vigilance, and their relations to neurobiological attention networks. Outcomes will have practical significance because shifting auditory attention is vital for avoiding accidents at ll ages, maintaining independent living at older ages, and is applicable to neurological and psychiatric disorders having attentional impairments (e.g. PTSD, attention deficit disorder, schizophrenia, stroke, dementia). PUBLIC HEALTH RELEVANCE: The project outcomes could have clinical in neurological and psychiatric disorders having attentional impairments (e.g. PTSD, attention deficit disorder, schizophrenia, stroke, dementia). The project will examine mechanisms of normal cognitive aging using computational modeling, which can inform the early detection, differential diagnosis, and treatment monitoring of age-related neurodegenerative disorders such as Alzheimer's disease. Knowledge from the transcranial magnetic and electrical DC stimulation experiments may have translational applications for rehabilitation following brain injuries.",Shifting auditory spatial attention: cognitive and neural mechanisms,9513308,R01DC014736,"['Accidents', 'Address', 'Affect', 'Age', 'Alzheimer&apos', 's Disease', 'Area', 'Artificial Intelligence', 'Attention', 'Attention Deficit Disorder', 'Auditory', 'Auditory system', 'Behavioral', 'Blood flow', 'Brain Injuries', 'Clinical', 'Cognitive', 'Cognitive aging', 'Computer Simulation', 'Dementia', 'Differential Diagnosis', 'Diffuse', 'Dimensions', 'Early Diagnosis', 'Electroencephalography', 'Environment', 'Event', 'Goals', 'Hearing', 'Human', 'Impairment', 'Independent Living', 'Individual Differences', 'Inferior', 'Knowledge', 'Learning', 'Left', 'Location', 'Loudness', 'Magnetic Resonance Imaging', 'Magnetism', 'Maps', 'Measures', 'Mediating', 'Mental disorders', 'Methods', 'Modeling', 'Monitor', 'Music', 'Neurobiology', 'Obstruction', 'Outcome', 'Parietal', 'Performance', 'Positioning Attribute', 'Post-Traumatic Stress Disorders', 'Probability', 'Process', 'Property', 'Reaction Time', 'Rehabilitation therapy', 'Reproduction', 'Research', 'Resources', 'Role', 'Schizophrenia', 'Short-Term Memory', 'Site', 'Spatial Distribution', 'Stimulus', 'Stroke', 'System', 'Testing', 'Time', 'Transcranial magnetic stimulation', 'Vision', 'Work', 'age related neurodegeneration', 'alpha-SNAP', 'attentional bias', 'attentional control', 'base', 'behavior measurement', 'cognitive process', 'cost shifting', 'experimental study', 'fighting', 'image guided', 'indexing', 'information processing', 'interdisciplinary approach', 'nervous system disorder', 'neural correlate', 'neural stimulation', 'neuroimaging', 'neuromechanism', 'normal aging', 'prefrontal lobe', 'public health relevance', 'relating to nervous system', 'response', 'sound', 'theories', 'vector', 'vigilance']",NIDCD,UNIVERSITY OF TEXAS SAN ANTONIO,R01,2018,262412,-0.02351350262921403
"Shifting auditory spatial attention: cognitive and neural mechanisms ﻿    DESCRIPTION (provided by applicant): Hearing can serve as an early warning system because it can panoramically monitor the environment for events happening at a distance or out of sight. These ecological considerations make the auditory system particularly useful for studying mechanisms for shifting spatial attention. The focus of this project is on the interplay between top-down and bottom-up spatial attention biases that govern shifting auditory attention to distractors during performance of a simple spatial attention task. The overall goal of this proposal is to use an interdisciplinary approach to better understand at the cognitive and neural levels of analysis how auditory attention is distributed over space. We will test the hypothesis that the spatial distribution of auditory attentional emerges from interactions among two basic factors. The first factor is a voluntary (top-down) attention bias that weakens with distance from the current focus of attention. Conversely, the second factor is an automatic (bottom-up) bias that is tuned to shift attention to unexpected events away from the current focus of attention. The first aim tests a computational model of top-down and bottom-up attention bias in shifting auditory spatial attention. Different aspects of the model will be quantitatively tested against findings from behavioral experiments, and observations will be used to further develop the model. The computational model will incorporate artificial intelligence methods to represent human cognitive processes. The second aim uses transcranial magnetic stimulation to test the role of key right hemisphere cortical areas in shifting of auditory spatial attention. We focus on neural mechanisms of bottom-up attentional bias. The third aim tests whether auditory attention gradients become less focused over time. This will determine how gradients relate to cognitive resources and neural blood flow measures that are known to decline with extended vigilance. This project will help advance knowledge in the field of auditory attention and cognitive hearing, and also addresses general issues in attention research such as top-down and bottom-up processes, vigilance, and their relations to neurobiological attention networks. Outcomes will have practical significance because shifting auditory attention is vital for avoiding accidents at ll ages, maintaining independent living at older ages, and is applicable to neurological and psychiatric disorders having attentional impairments (e.g. PTSD, attention deficit disorder, schizophrenia, stroke, dementia). PUBLIC HEALTH RELEVANCE: The project outcomes could have clinical in neurological and psychiatric disorders having attentional impairments (e.g. PTSD, attention deficit disorder, schizophrenia, stroke, dementia). The project will examine mechanisms of normal cognitive aging using computational modeling, which can inform the early detection, differential diagnosis, and treatment monitoring of age-related neurodegenerative disorders such as Alzheimer's disease. Knowledge from the transcranial magnetic and electrical DC stimulation experiments may have translational applications for rehabilitation following brain injuries.",Shifting auditory spatial attention: cognitive and neural mechanisms,9285756,R01DC014736,"['Accidents', 'Address', 'Affect', 'Age', 'Alzheimer&apos', 's Disease', 'Area', 'Artificial Intelligence', 'Attention', 'Attention Deficit Disorder', 'Auditory', 'Auditory system', 'Behavioral', 'Blood flow', 'Brain Injuries', 'Clinical', 'Cognitive', 'Cognitive aging', 'Computer Simulation', 'Dementia', 'Differential Diagnosis', 'Diffuse', 'Dimensions', 'Early Diagnosis', 'Electroencephalography', 'Environment', 'Event', 'Goals', 'Hearing', 'Human', 'Impairment', 'Independent Living', 'Individual Differences', 'Inferior', 'Knowledge', 'Learning', 'Left', 'Location', 'Loudness', 'Magnetic Resonance Imaging', 'Magnetism', 'Maps', 'Measures', 'Mediating', 'Mental disorders', 'Methods', 'Modeling', 'Monitor', 'Music', 'Neurobiology', 'Neurodegenerative Disorders', 'Obstruction', 'Outcome', 'Parietal', 'Performance', 'Positioning Attribute', 'Post-Traumatic Stress Disorders', 'Probability', 'Process', 'Property', 'Reaction Time', 'Rehabilitation therapy', 'Reproduction', 'Research', 'Resources', 'Role', 'Schizophrenia', 'Short-Term Memory', 'Site', 'Spatial Distribution', 'Stimulus', 'Stroke', 'System', 'Testing', 'Time', 'Transcranial magnetic stimulation', 'Vision', 'Work', 'age related', 'alpha-SNAP', 'attentional bias', 'attentional control', 'base', 'behavior measurement', 'cognitive process', 'cost shifting', 'experimental study', 'fighting', 'image guided', 'indexing', 'information processing', 'interdisciplinary approach', 'nervous system disorder', 'neural correlate', 'neural stimulation', 'neuroimaging', 'neuromechanism', 'normal aging', 'prefrontal lobe', 'public health relevance', 'relating to nervous system', 'response', 'sound', 'theories', 'vector', 'vigilance']",NIDCD,UNIVERSITY OF TEXAS SAN ANTONIO,R01,2017,237742,-0.02351350262921403
"Shifting auditory spatial attention: cognitive and neural mechanisms ﻿    DESCRIPTION (provided by applicant): Hearing can serve as an early warning system because it can panoramically monitor the environment for events happening at a distance or out of sight. These ecological considerations make the auditory system particularly useful for studying mechanisms for shifting spatial attention. The focus of this project is on the interplay between top-down and bottom-up spatial attention biases that govern shifting auditory attention to distractors during performance of a simple spatial attention task. The overall goal of this proposal is to use an interdisciplinary approach to better understand at the cognitive and neural levels of analysis how auditory attention is distributed over space. We will test the hypothesis that the spatial distribution of auditory attentional emerges from interactions among two basic factors. The first factor is a voluntary (top-down) attention bias that weakens with distance from the current focus of attention. Conversely, the second factor is an automatic (bottom-up) bias that is tuned to shift attention to unexpected events away from the current focus of attention. The first aim tests a computational model of top-down and bottom-up attention bias in shifting auditory spatial attention. Different aspects of the model will be quantitatively tested against findings from behavioral experiments, and observations will be used to further develop the model. The computational model will incorporate artificial intelligence methods to represent human cognitive processes. The second aim uses transcranial magnetic stimulation to test the role of key right hemisphere cortical areas in shifting of auditory spatial attention. We focus on neural mechanisms of bottom-up attentional bias. The third aim tests whether auditory attention gradients become less focused over time. This will determine how gradients relate to cognitive resources and neural blood flow measures that are known to decline with extended vigilance. This project will help advance knowledge in the field of auditory attention and cognitive hearing, and also addresses general issues in attention research such as top-down and bottom-up processes, vigilance, and their relations to neurobiological attention networks. Outcomes will have practical significance because shifting auditory attention is vital for avoiding accidents at ll ages, maintaining independent living at older ages, and is applicable to neurological and psychiatric disorders having attentional impairments (e.g. PTSD, attention deficit disorder, schizophrenia, stroke, dementia).         PUBLIC HEALTH RELEVANCE: The project outcomes could have clinical in neurological and psychiatric disorders having attentional impairments (e.g. PTSD, attention deficit disorder, schizophrenia, stroke, dementia). The project will examine mechanisms of normal cognitive aging using computational modeling, which can inform the early detection, differential diagnosis, and treatment monitoring of age-related neurodegenerative disorders such as Alzheimer's disease. Knowledge from the transcranial magnetic and electrical DC stimulation experiments may have translational applications for rehabilitation following brain injuries.                ",Shifting auditory spatial attention: cognitive and neural mechanisms,8946229,R01DC014736,"['Accidents', 'Address', 'Affect', 'Age', 'Alzheimer&apos', 's Disease', 'Area', 'Artificial Intelligence', 'Attention', 'Attention Deficit Disorder', 'Auditory', 'Auditory system', 'Behavioral', 'Blood flow', 'Brain Injuries', 'Clinical', 'Cognitive', 'Cognitive aging', 'Computer Simulation', 'Dementia', 'Differential Diagnosis', 'Diffuse', 'Early Diagnosis', 'Electroencephalography', 'Environment', 'Event', 'Goals', 'Hearing', 'Human', 'Impairment', 'Independent Living', 'Individual Differences', 'Inferior', 'Knowledge', 'Learning', 'Left', 'Location', 'Loudness', 'Magnetic Resonance Imaging', 'Magnetism', 'Maps', 'Measures', 'Mediating', 'Mental disorders', 'Methods', 'Modeling', 'Monitor', 'Music', 'Neurobiology', 'Neurodegenerative Disorders', 'Obstruction', 'Outcome', 'Parietal', 'Performance', 'Positioning Attribute', 'Post-Traumatic Stress Disorders', 'Probability', 'Process', 'Property', 'Reaction Time', 'Rehabilitation therapy', 'Relative (related person)', 'Reproduction', 'Research', 'Resources', 'Role', 'Schizophrenia', 'Short-Term Memory', 'Site', 'Spatial Distribution', 'Stimulus', 'Stroke', 'System', 'Testing', 'Time', 'Transcranial magnetic stimulation', 'Vision', 'Work', 'age related', 'attentional bias', 'base', 'behavior measurement', 'cognitive process', 'cost shifting', 'fighting', 'image guided', 'indexing', 'information processing', 'interdisciplinary approach', 'nervous system disorder', 'neural correlate', 'neural stimulation', 'neuroimaging', 'neuromechanism', 'normal aging', 'prefrontal lobe', 'public health relevance', 'relating to nervous system', 'research study', 'response', 'sound', 'theories', 'vector', 'vigilance']",NIDCD,TULANE UNIVERSITY OF LOUISIANA,R01,2015,283560,-0.02351350262921403
"Motion, Artifact Cancelling MIMO Method for Ambulatory Respiratory Rate Monitorin     DESCRIPTION (provided by applicant): Shortness of breath and difficulty in breathing are directly associated with deteriorating conditions in patients with heart failure (HF) and/or chroni obstructive pulmonary disease (COPD). Continuous monitoring of respiratory status in these patients can alert caregivers to administer early interventions to manage disease symptoms, thus preventing catastrophic events and improving quality of life. Unfortunately, continuous monitoring of respiratory rate and pattern is often overlooked or neglected in clinical practice due to the general difficulty in performing these measurements, especially in non-intubated ambulatory settings. Present measurement methods that capture the patient's airway are most accurate but difficult to administer and often intolerable for the patient, whereas methods that rely on capturing chest motion historically suffer from poor accuracy due to motion artifacts.  We propose a MIMO-based motion artifact cancelling multi-lead impedance measurement method for robust ambulatory respiratory rate monitoring. We have recently demonstrated that respiration and motion artifacts are theoretically separable in ambulatory subjects with multi-lead impedance measurements. The motion artifacts are sporadic and localized high energy non-stationary events compared to respiratory signals. Due to the limited dynamic range and quantization errors of the conventional data acquisition system, source separation algorithms struggle to extract reliable respiratory signal information. The proposed MIMO system approach overcomes these limitations by integrating statistical learning directly within analog to digital conversion process. As a result, this miniaturized hardware realization method enables continuous, real-time and power-efficient tracking of the respiratory signal corrupted by motion artifacts.  Preliminary study of the MIMO based algorithm has shown promising results for ambulatory respiratory rate monitoring. In this research we plan to develop the optimized hardware prototype of the MIMO system to validate the performance under real ambulatory conditions. We believe that the MIMO based multi-lead imped- ance measurement method is likely to be accepted by both clinicians and patients due to routine use of electrodes. Going forward, in our future work we envision integrating the proposed research with ECG monitoring sharing same set of electrodes to provide continuous and accurate cardiorespiratory information for general ward as well as for home health/wellness monitoring of ambulatory patients.         PUBLIC HEALTH RELEVANCE: Continuous monitoring of respiratory status in COPD and HF patients can alert caregivers to administer early interventions to manage disease symptoms, thus preventing catastrophic events and improving quality of life. In ambulatory patients, accuracy remains an issue for non-intubated methods relying on chest motion due to physical motion artifacts. We propose to develop and validate MIMO based motion-artifact cancelling multi-lead impedance measurement technique for robust ambulatory respiratory rate monitoring.            ","Motion, Artifact Cancelling MIMO Method for Ambulatory Respiratory Rate Monitorin",8702170,R21EB015608,"['Algorithms', 'Ambulatory Monitoring', 'Breathing', 'Caregivers', 'Chest', 'Chronic Obstructive Airway Disease', 'Clinical', 'Data', 'Data Set', 'Disease', 'Early Intervention', 'Electrocardiogram', 'Electrodes', 'Evaluation', 'Event', 'Future', 'General Ward', 'Goals', 'Government', 'Health', 'Heart failure', 'Home environment', 'Impedance Plethysmography', 'Joints', 'Lead', 'Lung diseases', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Michigan', 'Monitor', 'Morphologic artifacts', 'Motion', 'Outcome', 'Output', 'Patients', 'Pattern', 'Performance', 'Persons', 'Process', 'Quality of life', 'Research', 'Respiration', 'Shortness of Breath', 'Signal Transduction', 'Source', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'Universities', 'Work', 'analog', 'base', 'clinical practice', 'data acquisition', 'design', 'digital', 'electric impedance', 'experience', 'human subject', 'improved', 'instrument', 'miniaturize', 'neglect', 'novel', 'prevent', 'programs', 'prototype', 'public health relevance', 'respiratory', 'sedentary', 'signal processing']",NIBIB,GENERAL ELECTRIC GLOBAL RESEARCH CTR,R21,2014,230240,-0.02647505561898947
"Motion, Artifact Cancelling MIMO Method for Ambulatory Respiratory Rate Monitorin     DESCRIPTION (provided by applicant): Shortness of breath and difficulty in breathing are directly associated with deteriorating conditions in patients with heart failure (HF) and/or chroni obstructive pulmonary disease (COPD). Continuous monitoring of respiratory status in these patients can alert caregivers to administer early interventions to manage disease symptoms, thus preventing catastrophic events and improving quality of life. Unfortunately, continuous monitoring of respiratory rate and pattern is often overlooked or neglected in clinical practice due to the general difficulty in performing these measurements, especially in non-intubated ambulatory settings. Present measurement methods that capture the patient's airway are most accurate but difficult to administer and often intolerable for the patient, whereas methods that rely on capturing chest motion historically suffer from poor accuracy due to motion artifacts.  We propose a MIMO-based motion artifact cancelling multi-lead impedance measurement method for robust ambulatory respiratory rate monitoring. We have recently demonstrated that respiration and motion artifacts are theoretically separable in ambulatory subjects with multi-lead impedance measurements. The motion artifacts are sporadic and localized high energy non-stationary events compared to respiratory signals. Due to the limited dynamic range and quantization errors of the conventional data acquisition system, source separation algorithms struggle to extract reliable respiratory signal information. The proposed MIMO system approach overcomes these limitations by integrating statistical learning directly within analog to digital conversion process. As a result, this miniaturized hardware realization method enables continuous, real-time and power-efficient tracking of the respiratory signal corrupted by motion artifacts.  Preliminary study of the MIMO based algorithm has shown promising results for ambulatory respiratory rate monitoring. In this research we plan to develop the optimized hardware prototype of the MIMO system to validate the performance under real ambulatory conditions. We believe that the MIMO based multi-lead imped- ance measurement method is likely to be accepted by both clinicians and patients due to routine use of electrodes. Going forward, in our future work we envision integrating the proposed research with ECG monitoring sharing same set of electrodes to provide continuous and accurate cardiorespiratory information for general ward as well as for home health/wellness monitoring of ambulatory patients.         PUBLIC HEALTH RELEVANCE: Continuous monitoring of respiratory status in COPD and HF patients can alert caregivers to administer early interventions to manage disease symptoms, thus preventing catastrophic events and improving quality of life. In ambulatory patients, accuracy remains an issue for non-intubated methods relying on chest motion due to physical motion artifacts. We propose to develop and validate MIMO based motion-artifact cancelling multi-lead impedance measurement technique for robust ambulatory respiratory rate monitoring.            ","Motion, Artifact Cancelling MIMO Method for Ambulatory Respiratory Rate Monitorin",8582895,R21EB015608,"['Algorithms', 'Ambulatory Monitoring', 'Breathing', 'Caregivers', 'Chest', 'Chronic Obstructive Airway Disease', 'Clinical', 'Data', 'Data Set', 'Disease', 'Early Intervention', 'Electrocardiogram', 'Electrodes', 'Evaluation', 'Event', 'Future', 'General Ward', 'Goals', 'Government', 'Health', 'Heart failure', 'Home environment', 'Impedance Plethysmography', 'Joints', 'Lead', 'Lung diseases', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Michigan', 'Monitor', 'Morphologic artifacts', 'Motion', 'Outcome', 'Output', 'Patients', 'Pattern', 'Performance', 'Persons', 'Process', 'Quality of life', 'Research', 'Respiration', 'Shortness of Breath', 'Signal Transduction', 'Source', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'Universities', 'Work', 'analog', 'base', 'clinical practice', 'computerized data processing', 'data acquisition', 'design', 'digital', 'electric impedance', 'experience', 'human subject', 'improved', 'instrument', 'miniaturize', 'neglect', 'novel', 'prevent', 'programs', 'prototype', 'public health relevance', 'respiratory', 'sedentary']",NIBIB,GENERAL ELECTRIC GLOBAL RESEARCH CTR,R21,2013,302200,-0.02647505561898947
"In Vivo Mapping of Uncharted Functional Units of Tongue Motion During Speech Project Summary Tongue cancer and the treatments used to cure the disease such as glossectomy can have debilitating effects on speech and swallowing, yet not much is known about the relationship between tongue movements in speech and reconstruction choices after cancer resection. The tongue’s vital role in speaking and swallowing is executed by a uniquely complex array of muscles which deform locally into functional units. Tongue movements are governed by synergistic activation of these functional units using orthogonally oriented muscle fibers not a rigid skeletal structure. Functional units, which are regions of 3D coherent motion, are the structures that link low-level muscle activity to high-level surface tongue geometry. This link, the organization of functional units, can provide considerable insight into normal motor control and adapted tongue motion, as in patients who have had surgery for tongue cancer. However, there is poor understanding of the details of how muscle morphology contributes to function due to tongue muscle interdigitation and complex local activation. A key to understanding the relationship between the structure and function of the tongue is to identify functional units of motion in localized tongue regions and map them to muscle anatomy. A large step in this direction can be gained by developing a tool to cluster 3D coherent motion patterns from tagged MRI, and register them to muscle anatomy from high-resolution MRI. Speech motor control is a critical area of importance where platform tools for machine learning techniques are lacking. The goal of this project is to create a framework to relate the functional units to tongue muscle anatomy. This framework will enhance tongue motion analysis in medical research and clinical applications, where MRI is frequently used. To address this goal, we have already developed technologies to analyze high-resolution and tagged-MRI. The first is a detailed 3D muscle atlas based on 3D high-resolution MRI, which depicts the locations of both extrinsic and intrinsic muscles, and will be registered to each speaker to identify and track their muscles in the 3D tagged data sets. The second is software to derive 3D displacement and strain fields from tagged MR images. These displacements, strains, and derived quantities will be used to extract spatio-temporal feature vectors that define coherent regions, which will be verified with simulation. The vectors will then be input into a novel computational method using a NMF and clustering, the goal of which is to establish the functional units of speech. In addition, biomechanical simulations will be used to co-validate our findings. The concept of functional units is at the core of the proposed work, as it can provide insights into tongue muscle coordination in patients after glossectomy surgery. The research plan comprises two specific aims: (1) develop a computational method to establish functional units from MRI and (2) determine the relationship between alterations in anatomy and alterations in functional unit motion after tongue cancer treatment. Linking the functional units to the anatomy will allow the design of more focused treatments and therapies for tongue motion-related disorders. Project Narrative Functions of the tongue—speaking, eating, and breathing—are executed by using a complex muscular array to create global motions (synergies) by deforming local functional units which are intermediate structures that link muscle activity to surface tongue geometry. This project will develop new computational tools that will enable researchers and clinicians to determine the functional units of specific motions and relate them to the muscle anatomy. Such a breakthrough will dramatically improve our knowledge of the mechanisms of muscle coordination, thereby permitting improvement of diagnosis and treatment of tongue motion-related disorders.",In Vivo Mapping of Uncharted Functional Units of Tongue Motion During Speech,9636557,R21DC016047,"['3-Dimensional', 'Address', 'Anatomy', 'Area', 'Atlases', 'Behavior', 'Bone structure', 'Breathing', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Coupling', 'Data', 'Data Analyses', 'Data Set', 'Deglutition', 'Deglutition Disorders', 'Diagnosis', 'Disease', 'Eating', 'Elements', 'Excision', 'Geometry', 'Gestures', 'Glossectomy', 'Goals', 'Impairment', 'Individual', 'Knowledge', 'Learning', 'Link', 'Location', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measures', 'Mechanics', 'Medical Research', 'Methods', 'Modeling', 'Morphology', 'Motion', 'Movement', 'Muscle', 'Muscle Fibers', 'Operative Surgical Procedures', 'Outcome', 'Patients', 'Pattern', 'Positioning Attribute', 'Procedures', 'Production', 'Quality of life', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Shapes', 'Specific qualifier value', 'Speech', 'Speech Disorders', 'Speech Therapy', 'Stretching', 'Structure', 'Surface', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissue Extracts', 'Tissues', 'Tongue', 'Treatment-Related Cancer', 'Unit of Measure', 'Variant', 'Work', 'base', 'biomechanical model', 'cancer therapy', 'clinical application', 'computational basis', 'computerized tools', 'design', 'digital', 'functional independence', 'improved', 'in vivo', 'insight', 'malignant tongue neoplasm', 'motor control', 'multidimensional data', 'muscular structure', 'novel', 'outcome forecast', 'predict clinical outcome', 'reconstruction', 'rehabilitation strategy', 'simulation', 'spatiotemporal', 'synergism', 'theories', 'tool', 'vector']",NIDCD,MASSACHUSETTS GENERAL HOSPITAL,R21,2019,210725,-0.11347303372260838
"In Vivo Mapping of Uncharted Functional Units of Tongue Motion During Speech Project Summary Tongue cancer and the treatments used to cure the disease such as glossectomy can have debilitating effects on speech and swallowing, yet not much is known about the relationship between tongue movements in speech and reconstruction choices after cancer resection. The tongue’s vital role in speaking and swallowing is executed by a uniquely complex array of muscles which deform locally into functional units. Tongue movements are governed by synergistic activation of these functional units using orthogonally oriented muscle fibers not a rigid skeletal structure. Functional units, which are regions of 3D coherent motion, are the structures that link low-level muscle activity to high-level surface tongue geometry. This link, the organization of functional units, can provide considerable insight into normal motor control and adapted tongue motion, as in patients who have had surgery for tongue cancer. However, there is poor understanding of the details of how muscle morphology contributes to function due to tongue muscle interdigitation and complex local activation. A key to understanding the relationship between the structure and function of the tongue is to identify functional units of motion in localized tongue regions and map them to muscle anatomy. A large step in this direction can be gained by developing a tool to cluster 3D coherent motion patterns from tagged MRI, and register them to muscle anatomy from high-resolution MRI. Speech motor control is a critical area of importance where platform tools for machine learning techniques are lacking. The goal of this project is to create a framework to relate the functional units to tongue muscle anatomy. This framework will enhance tongue motion analysis in medical research and clinical applications, where MRI is frequently used. To address this goal, we have already developed technologies to analyze high-resolution and tagged-MRI. The first is a detailed 3D muscle atlas based on 3D high-resolution MRI, which depicts the locations of both extrinsic and intrinsic muscles, and will be registered to each speaker to identify and track their muscles in the 3D tagged data sets. The second is software to derive 3D displacement and strain fields from tagged MR images. These displacements, strains, and derived quantities will be used to extract spatio-temporal feature vectors that define coherent regions, which will be verified with simulation. The vectors will then be input into a novel computational method using a NMF and clustering, the goal of which is to establish the functional units of speech. In addition, biomechanical simulations will be used to co-validate our findings. The concept of functional units is at the core of the proposed work, as it can provide insights into tongue muscle coordination in patients after glossectomy surgery. The research plan comprises two specific aims: (1) develop a computational method to establish functional units from MRI and (2) determine the relationship between alterations in anatomy and alterations in functional unit motion after tongue cancer treatment. Linking the functional units to the anatomy will allow the design of more focused treatments and therapies for tongue motion-related disorders. Project Narrative Functions of the tongue—speaking, eating, and breathing—are executed by using a complex muscular array to create global motions (synergies) by deforming local functional units which are intermediate structures that link muscle activity to surface tongue geometry. This project will develop new computational tools that will enable researchers and clinicians to determine the functional units of specific motions and relate them to the muscle anatomy. Such a breakthrough will dramatically improve our knowledge of the mechanisms of muscle coordination, thereby permitting improvement of diagnosis and treatment of tongue motion-related disorders.",In Vivo Mapping of Uncharted Functional Units of Tongue Motion During Speech,9452335,R21DC016047,"['Address', 'Anatomy', 'Area', 'Atlases', 'Behavior', 'Bone structure', 'Breathing', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Coupling', 'Data', 'Data Analyses', 'Data Set', 'Deglutition', 'Deglutition Disorders', 'Diagnosis', 'Disease', 'Eating', 'Elements', 'Excision', 'Geometry', 'Gestures', 'Glossectomy', 'Goals', 'Impairment', 'Individual', 'Knowledge', 'Learning', 'Link', 'Location', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measures', 'Mechanics', 'Medical Research', 'Methods', 'Modeling', 'Morphology', 'Motion', 'Movement', 'Muscle', 'Muscle Fibers', 'Operative Surgical Procedures', 'Outcome', 'Patients', 'Pattern', 'Positioning Attribute', 'Procedures', 'Production', 'Quality of life', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Shapes', 'Specific qualifier value', 'Speech', 'Speech Disorders', 'Speech Therapy', 'Stretching', 'Structure', 'Surface', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissue Extracts', 'Tissues', 'Tongue', 'Treatment-Related Cancer', 'Unit of Measure', 'Variant', 'Work', 'base', 'biomechanical model', 'cancer therapy', 'clinical application', 'computerized tools', 'design', 'digital', 'high dimensionality', 'improved', 'in vivo', 'insight', 'malignant tongue neoplasm', 'motor control', 'muscular structure', 'novel', 'outcome forecast', 'predict clinical outcome', 'reconstruction', 'rehabilitation strategy', 'simulation', 'spatiotemporal', 'synergism', 'theories', 'tool', 'vector']",NIDCD,MASSACHUSETTS GENERAL HOSPITAL,R21,2018,249441,-0.11347303372260838
"Optimizing MRI for Radiation Therapy Treatment Planning Optimal integration of MRI in Radiation Oncology is hindered by the lack of methods that harvest the significant prior knowledge available to sample the anatomy, biological status, and physiologic motions of individual patients. While some generic image acquisition methods take advantage of non-specific low rank structure of human MR signals to achieve some modest acceleration, the wealth of specific prior knowledge, from both the population of similar patients as well as the specific patient, has yet to be effectively tapped to guide optimal treatment planning, positioning, and monitoring. We hypothesize that biological, morphological, and motion models of the patient can be accurately derived from a limited number of samples aided by prior knowledge. These advances will allow us to reduce scan times dramatically (to less than 10% of conventional scanning) for morphological imaging, support efficient biological imaging for high order diffusion modeling and create hierarchical motion-frozen image volumes of abdominal patients that simultaneously provide breathing, GI contraction, and potentially cardiac motion models with probability density functions that can be used to estimate the impact of intrafraction motion on treatments and eventually select local navigators for real-time monitoring of specific regions that are most sensitive to motion-related impacts on delivered doses to targets or organs at risk. We will investigate this hypothesis by developing a prior knowledge-based compressed sensing method to reconstruct densely sampled DW attenuation curves from sparsely sampled ones; performing principal component analysis of previously scanned FLAIR, contrast-enhanced T1-weighted and Diffusion-Weighted image volumes to support sparse sampling in k-space for anatomic imaging and in b-values for diffusion imaging; investigating potential gains in acceleration of imaging by combining a patient-specific prior with population-derived principal components of structure and diffusion; modeling breathing and peristaltic motion. Finally, we will develop and implement scanning sequences based on the modeled methods for subsampling b-values and anatomy. By these methods, we expect to provide efficient anatomic and high order diffusion imaging, as well as introduce means to automatically extract hierarchical motion models of the patient for use in treatment planning and future support of treatment monitoring. Relevance to PAR 18-484 (for the NCI): This investigation seeks to improve both the efficiency as well as the efficacy of precision radiation therapy for patients with GBMs, other intracranial targets as well as intrahepatic tumors. As Radiation therapy is part of the standard armamentarium of care options for these patients, this research falls within the purview of the NCI. NARRATIVE Magnetic Resonance imaging is rapidly advancing as a primary tool for guiding Radiation Therapy. With routine availability of MR Simulators as well as significant expansion of MR-guided treatment systems, the Radiation Oncology community will benefit greatly from methods to improve the speed and discriminatory power of Magnetic Resonance to monitor the shape, motion, and biological structure of tumors and normal tissues efficiently and accurately. Taking advantage of two concepts, both the lack of significant complexity in descriptions of shape, biology and motion, as well as prior knowledge from both the individual patient as well as populations of similar patients, we will produce highly efficient maps of diffusion for tumor identification and monitoring, patient shape for very rapid scanning both for planning as well as guiding treatments precisely, and finally images that can freeze the patient at selectable combinations of breathing and gastrointestinal motion states, as well as model these motions for more accurate estimation of delivered radiation doses and selection of treatment monitoring methods.",Optimizing MRI for Radiation Therapy Treatment Planning,9979862,R01EB016079,"['Abdomen', 'Acceleration', 'Anatomy', 'Area', 'Base Sequence', 'Biological', 'Biological Monitoring', 'Biology', 'Breathing', 'Caring', 'Community Clinical Oncology Program', 'Complex', 'Data', 'Databases', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Dose', 'Freezing', 'Future', 'Glioblastoma', 'Harvest', 'Head', 'Human', 'Image', 'Intestines', 'Investigation', 'Knowledge', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Morphology', 'Motion', 'Movement', 'Normal tissue morphology', 'Organ', 'Patients', 'Pattern', 'Periodicity', 'Peristalsis', 'Physiological', 'Population', 'Positioning Attribute', 'Principal Component Analysis', 'Probability', 'Radiation Dose Unit', 'Radiation Oncology', 'Radiation therapy', 'Research', 'Risk', 'Sampling', 'Scanning', 'Selection for Treatments', 'Shapes', 'Signal Transduction', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Tumor Tissue', 'anatomic imaging', 'attenuation', 'base', 'cancer imaging', 'contrast enhanced', 'cost', 'density', 'falls', 'gastrointestinal', 'heart motion', 'improved', 'individual patient', 'intrahepatic cancer', 'knowledge base', 'optimal treatments', 'patient population', 'prospective', 'real time monitoring', 'reconstruction', 'response', 'shape analysis', 'simulation', 'theories', 'tool', 'treatment planning', 'tumor']",NIBIB,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,344890,-0.011723521989450937
"Optimizing MRI for Radiation Therapy Treatment Planning Optimal integration of MRI in Radiation Oncology is hindered by the lack of methods that harvest the significant prior knowledge available to sample the anatomy, biological status, and physiologic motions of individual patients. While some generic image acquisition methods take advantage of non-specific low rank structure of human MR signals to achieve some modest acceleration, the wealth of specific prior knowledge, from both the population of similar patients as well as the specific patient, has yet to be effectively tapped to guide optimal treatment planning, positioning, and monitoring. We hypothesize that biological, morphological, and motion models of the patient can be accurately derived from a limited number of samples aided by prior knowledge. These advances will allow us to reduce scan times dramatically (to less than 10% of conventional scanning) for morphological imaging, support efficient biological imaging for high order diffusion modeling and create hierarchical motion-frozen image volumes of abdominal patients that simultaneously provide breathing, GI contraction, and potentially cardiac motion models with probability density functions that can be used to estimate the impact of intrafraction motion on treatments and eventually select local navigators for real-time monitoring of specific regions that are most sensitive to motion-related impacts on delivered doses to targets or organs at risk. We will investigate this hypothesis by developing a prior knowledge-based compressed sensing method to reconstruct densely sampled DW attenuation curves from sparsely sampled ones; performing principal component analysis of previously scanned FLAIR, contrast-enhanced T1-weighted and Diffusion-Weighted image volumes to support sparse sampling in k-space for anatomic imaging and in b-values for diffusion imaging; investigating potential gains in acceleration of imaging by combining a patient-specific prior with population-derived principal components of structure and diffusion; modeling breathing and peristaltic motion. Finally, we will develop and implement scanning sequences based on the modeled methods for subsampling b-values and anatomy. By these methods, we expect to provide efficient anatomic and high order diffusion imaging, as well as introduce means to automatically extract hierarchical motion models of the patient for use in treatment planning and future support of treatment monitoring. Relevance to PAR 18-484 (for the NCI): This investigation seeks to improve both the efficiency as well as the efficacy of precision radiation therapy for patients with GBMs, other intracranial targets as well as intrahepatic tumors. As Radiation therapy is part of the standard armamentarium of care options for these patients, this research falls within the purview of the NCI. NARRATIVE Magnetic Resonance imaging is rapidly advancing as a primary tool for guiding Radiation Therapy. With routine availability of MR Simulators as well as significant expansion of MR-guided treatment systems, the Radiation Oncology community will benefit greatly from methods to improve the speed and discriminatory power of Magnetic Resonance to monitor the shape, motion, and biological structure of tumors and normal tissues efficiently and accurately. Taking advantage of two concepts, both the lack of significant complexity in descriptions of shape, biology and motion, as well as prior knowledge from both the individual patient as well as populations of similar patients, we will produce highly efficient maps of diffusion for tumor identification and monitoring, patient shape for very rapid scanning both for planning as well as guiding treatments precisely, and finally images that can freeze the patient at selectable combinations of breathing and gastrointestinal motion states, as well as model these motions for more accurate estimation of delivered radiation doses and selection of treatment monitoring methods.",Optimizing MRI for Radiation Therapy Treatment Planning,9782948,R01EB016079,"['Abdomen', 'Acceleration', 'Anatomy', 'Area', 'Base Sequence', 'Biological', 'Biological Monitoring', 'Biology', 'Breathing', 'Caring', 'Community Clinical Oncology Program', 'Complex', 'Data', 'Databases', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Dose', 'Freezing', 'Future', 'Glioblastoma', 'Harvest', 'Head', 'Human', 'Image', 'Intestines', 'Investigation', 'Knowledge', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Morphology', 'Motion', 'Movement', 'Normal tissue morphology', 'Organ', 'Patients', 'Pattern', 'Periodicity', 'Peristalsis', 'Physiological', 'Population', 'Positioning Attribute', 'Principal Component Analysis', 'Probability', 'Radiation Dose Unit', 'Radiation Oncology', 'Radiation therapy', 'Research', 'Risk', 'Sampling', 'Scanning', 'Selection for Treatments', 'Shapes', 'Signal Transduction', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Tumor Tissue', 'anatomic imaging', 'attenuation', 'base', 'cancer imaging', 'contrast enhanced', 'cost', 'density', 'falls', 'gastrointestinal', 'heart motion', 'improved', 'individual patient', 'intrahepatic cancer', 'knowledge base', 'optimal treatments', 'patient population', 'prospective', 'real time monitoring', 'reconstruction', 'response', 'shape analysis', 'simulation', 'theories', 'tool', 'treatment planning', 'tumor']",NIBIB,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2019,344890,-0.011723521989450937
"Optimizing MRI for Radiation Therapy Treatment Planning Optimal integration of MRI in Radiation Oncology is hindered by the lack of methods that harvest the significant prior knowledge available to sample the anatomy, biological status, and physiologic motions of individual patients. While some generic image acquisition methods take advantage of non-specific low rank structure of human MR signals to achieve some modest acceleration, the wealth of specific prior knowledge, from both the population of similar patients as well as the specific patient, has yet to be effectively tapped to guide optimal treatment planning, positioning, and monitoring. We hypothesize that biological, morphological, and motion models of the patient can be accurately derived from a limited number of samples aided by prior knowledge. These advances will allow us to reduce scan times dramatically (to less than 10% of conventional scanning) for morphological imaging, support efficient biological imaging for high order diffusion modeling and create hierarchical motion-frozen image volumes of abdominal patients that simultaneously provide breathing, GI contraction, and potentially cardiac motion models with probability density functions that can be used to estimate the impact of intrafraction motion on treatments and eventually select local navigators for real-time monitoring of specific regions that are most sensitive to motion-related impacts on delivered doses to targets or organs at risk. We will investigate this hypothesis by developing a prior knowledge-based compressed sensing method to reconstruct densely sampled DW attenuation curves from sparsely sampled ones; performing principal component analysis of previously scanned FLAIR, contrast-enhanced T1-weighted and Diffusion-Weighted image volumes to support sparse sampling in k-space for anatomic imaging and in b-values for diffusion imaging; investigating potential gains in acceleration of imaging by combining a patient-specific prior with population-derived principal components of structure and diffusion; modeling breathing and peristaltic motion. Finally, we will develop and implement scanning sequences based on the modeled methods for subsampling b-values and anatomy. By these methods, we expect to provide efficient anatomic and high order diffusion imaging, as well as introduce means to automatically extract hierarchical motion models of the patient for use in treatment planning and future support of treatment monitoring. Relevance to PAR 18-484 (for the NCI): This investigation seeks to improve both the efficiency as well as the efficacy of precision radiation therapy for patients with GBMs, other intracranial targets as well as intrahepatic tumors. As Radiation therapy is part of the standard armamentarium of care options for these patients, this research falls within the purview of the NCI. NARRATIVE Magnetic Resonance imaging is rapidly advancing as a primary tool for guiding Radiation Therapy. With routine availability of MR Simulators as well as significant expansion of MR-guided treatment systems, the Radiation Oncology community will benefit greatly from methods to improve the speed and discriminatory power of Magnetic Resonance to monitor the shape, motion, and biological structure of tumors and normal tissues efficiently and accurately. Taking advantage of two concepts, both the lack of significant complexity in descriptions of shape, biology and motion, as well as prior knowledge from both the individual patient as well as populations of similar patients, we will produce highly efficient maps of diffusion for tumor identification and monitoring, patient shape for very rapid scanning both for planning as well as guiding treatments precisely, and finally images that can freeze the patient at selectable combinations of breathing and gastrointestinal motion states, as well as model these motions for more accurate estimation of delivered radiation doses and selection of treatment monitoring methods.",Optimizing MRI for Radiation Therapy Treatment Planning,9659414,R01EB016079,"['Abdomen', 'Acceleration', 'Anatomy', 'Area', 'Base Sequence', 'Biological', 'Biological Monitoring', 'Biology', 'Breathing', 'Caring', 'Community Clinical Oncology Program', 'Complex', 'Data', 'Databases', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Dose', 'Freezing', 'Future', 'Generic Drugs', 'Glioblastoma', 'Harvest', 'Head', 'Human', 'Image', 'Intestines', 'Investigation', 'Knowledge', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Morphology', 'Motion', 'Movement', 'Normal tissue morphology', 'Organ', 'Patients', 'Pattern', 'Periodicity', 'Peristalsis', 'Physiological', 'Population', 'Positioning Attribute', 'Principal Component Analysis', 'Probability', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Research', 'Risk', 'Sampling', 'Scanning', 'Selection for Treatments', 'Shapes', 'Signal Transduction', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Tumor Tissue', 'anatomic imaging', 'attenuation', 'base', 'cancer imaging', 'contrast enhanced', 'cost', 'density', 'falls', 'gastrointestinal', 'heart motion', 'improved', 'individual patient', 'intrahepatic cancer', 'knowledge base', 'optimal treatments', 'patient population', 'prospective', 'real time monitoring', 'reconstruction', 'response', 'shape analysis', 'simulation', 'theories', 'tool', 'treatment planning', 'tumor']",NIBIB,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2018,353890,-0.011723521989450937
"Array-Compressed Parallel Transmission for High Resolution Neuroimaging at 7T Project Summary The goal of this project is to develop a framework for high-performance parallel transmission (pTx) that is trans- ferable to a wide range of MRI scanners, and apply it to push the spatial encoding limits of echo planar imaging (EPI) at 7 Tesla. EPI is by far the most widely used pulse sequence for rapid functional, diffusion, and perfusion imaging, and has been the focus of considerable development in recent years to increase its speed and spatial resolution. Now there is a strong desire to push EPI's spatial resolution down to the micro scale. For functional MRI (fMRI), this would enable imaging of ﬁne structures (layers, columns, and nuclei) of cortical and subcortical architecture while better resolving the hemodynamic response. For diffusion MRI (dMRI), micro scale EPI would improve surface and laminar analysis of ﬁbers in the cortex, as well as brain parcelation using fractional anisotropy differences between gray matter regions, while broadly reducing partial volume effects. It would further enable EPI to be broadly applied to accelerate anatomic scans that are geometrically matched to fMRI and dMRI scans. However, increasing the resolution of single-shot EPI requires longer readouts which extend echo times and re- duce functional contrast in fMRI and signal-to-noise in dMRI at 7 Tesla, while increasing geometric distortions and blurring. Segmented or multishot EPI is a classic method to increase spatial resolution without increasing readout durations, but is underutilized, primarily due to its high sensitivity to motion and dynamic phase changes between shots which cause large image artifacts.  We propose to develop a new multishot EPI technique called shuttered EPI, which addresses the lim- itations of conventional multishot EPI by imaging a set of spatially disjoint shutters in each shot. The shutters are produced by a multidimensional excitation pulse and are spatially shifted between shots to cover an entire slice. However, with thin slices the length of the excitation pulses are impractical (20-100 ms). Many-coil pTx (> 8 coils) can shorten the length of these pulses to feasible durations, but current 7 Tesla scanners have only 8 transmit channels due to cost, footprint, cabling, and other constraints. In the ﬁrst project period we pioneered a technique called array-compressed pTx (acpTx) which overcomes this limitation. Using acpTx, 8 transmit chan- nels can control an arbitrarily large number of coils, where the channels and coils are connected via an array compression network that is optimized with RF pulses for speciﬁc excitations. In this project, we will develop and apply acpTx methods and hardware (a many-coil head transmit array and an 8 channel-to-many coil array com- pression network) to achieve feasible RF pulse durations when exciting the shutter patterns required for shuttered EPI. These developments will be implemented on two major 7T scanner platforms and evaluated in submillimeter (600 micron) fMRI and dMRI acquisitions. Overall, the project encompasses the synergistic design of RF pulses, hardware, acquisitions and reconstructions to achieve a major advance in spatial encoding. Project Narrative Diffusion and functional magnetic resonance imaging (MRI) at 7 Tesla ﬁeld strength using echo planar imaging (EPI) has the potential to deliver clear images of brain structure and function at the level of layers, columns, and nuclei. However, when existing EPI scans are pushed to the spatial resolutions required to resolve these structures, they become highly sensitive to off resonance-induced geometric distortions, relaxation-induced blur- ring, physiological noise and motion. To address this problem, in this project we will develop many-coil array- compressed parallel transmission and apply it to enable shuttered multishot EPI scans that are robust to these effects.",Array-Compressed Parallel Transmission for High Resolution Neuroimaging at 7T,9850978,R01EB016695,"['3-Dimensional', 'Address', 'Algorithms', 'Anatomy', 'Anisotropy', 'Architecture', 'Biological', 'Brain', 'Brain imaging', 'Cell Nucleus', 'Data', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Echo-Planar Imaging', 'Elements', 'Fiber', 'Functional Magnetic Resonance Imaging', 'Funding', 'Goals', 'Head', 'Homebound Persons', 'Image', 'Image Enhancement', 'Imaging Techniques', 'Length', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measurement', 'Methods', 'Morphologic artifacts', 'Motion', 'Noise', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physiologic pulse', 'Physiological', 'Problem Formulations', 'Relaxation', 'Research Project Grants', 'Resolution', 'Scanning', 'Shapes', 'Signal Transduction', 'Slice', 'Speed', 'Structure', 'Surface', 'System', 'Techniques', 'Thinness', 'Time', 'base', 'blood oxygen level dependent', 'cost', 'design', 'gray matter', 'hemodynamics', 'image reconstruction', 'improved', 'magnetic field', 'motion sensitivity', 'neuroimaging', 'perfusion imaging', 'phase change', 'reconstruction', 'response', 'simulation', 'spectroscopic imaging', 'time use', 'transmission process', 'virtual']",NIBIB,VANDERBILT UNIVERSITY,R01,2020,569563,0.042842344443703245
"Array-Compressed Parallel Transmission for High Resolution Neuroimaging at 7T Project Summary The goal of this project is to develop a framework for high-performance parallel transmission (pTx) that is trans- ferable to a wide range of MRI scanners, and apply it to push the spatial encoding limits of echo planar imaging (EPI) at 7 Tesla. EPI is by far the most widely used pulse sequence for rapid functional, diffusion, and perfusion imaging, and has been the focus of considerable development in recent years to increase its speed and spatial resolution. Now there is a strong desire to push EPI's spatial resolution down to the micro scale. For functional MRI (fMRI), this would enable imaging of ﬁne structures (layers, columns, and nuclei) of cortical and subcortical architecture while better resolving the hemodynamic response. For diffusion MRI (dMRI), micro scale EPI would improve surface and laminar analysis of ﬁbers in the cortex, as well as brain parcelation using fractional anisotropy differences between gray matter regions, while broadly reducing partial volume effects. It would further enable EPI to be broadly applied to accelerate anatomic scans that are geometrically matched to fMRI and dMRI scans. However, increasing the resolution of single-shot EPI requires longer readouts which extend echo times and re- duce functional contrast in fMRI and signal-to-noise in dMRI at 7 Tesla, while increasing geometric distortions and blurring. Segmented or multishot EPI is a classic method to increase spatial resolution without increasing readout durations, but is underutilized, primarily due to its high sensitivity to motion and dynamic phase changes between shots which cause large image artifacts.  We propose to develop a new multishot EPI technique called shuttered EPI, which addresses the lim- itations of conventional multishot EPI by imaging a set of spatially disjoint shutters in each shot. The shutters are produced by a multidimensional excitation pulse and are spatially shifted between shots to cover an entire slice. However, with thin slices the length of the excitation pulses are impractical (20-100 ms). Many-coil pTx (> 8 coils) can shorten the length of these pulses to feasible durations, but current 7 Tesla scanners have only 8 transmit channels due to cost, footprint, cabling, and other constraints. In the ﬁrst project period we pioneered a technique called array-compressed pTx (acpTx) which overcomes this limitation. Using acpTx, 8 transmit chan- nels can control an arbitrarily large number of coils, where the channels and coils are connected via an array compression network that is optimized with RF pulses for speciﬁc excitations. In this project, we will develop and apply acpTx methods and hardware (a many-coil head transmit array and an 8 channel-to-many coil array com- pression network) to achieve feasible RF pulse durations when exciting the shutter patterns required for shuttered EPI. These developments will be implemented on two major 7T scanner platforms and evaluated in submillimeter (600 micron) fMRI and dMRI acquisitions. Overall, the project encompasses the synergistic design of RF pulses, hardware, acquisitions and reconstructions to achieve a major advance in spatial encoding. Project Narrative Diffusion and functional magnetic resonance imaging (MRI) at 7 Tesla ﬁeld strength using echo planar imaging (EPI) has the potential to deliver clear images of brain structure and function at the level of layers, columns, and nuclei. However, when existing EPI scans are pushed to the spatial resolutions required to resolve these structures, they become highly sensitive to off resonance-induced geometric distortions, relaxation-induced blur- ring, physiological noise and motion. To address this problem, in this project we will develop many-coil array- compressed parallel transmission and apply it to enable shuttered multishot EPI scans that are robust to these effects.",Array-Compressed Parallel Transmission for High Resolution Neuroimaging at 7T,9669022,R01EB016695,"['Address', 'Algorithms', 'Anatomy', 'Anisotropy', 'Architecture', 'Biological', 'Brain', 'Brain imaging', 'Cell Nucleus', 'Data', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Echo-Planar Imaging', 'Elements', 'Fiber', 'Functional Magnetic Resonance Imaging', 'Funding', 'Goals', 'Head', 'Homebound Persons', 'Image', 'Image Enhancement', 'Imaging Techniques', 'Length', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measurement', 'Methods', 'Morphologic artifacts', 'Motion', 'Noise', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physiologic pulse', 'Physiological', 'Problem Formulations', 'Relaxation', 'Research Project Grants', 'Resolution', 'Scanning', 'Shapes', 'Signal Transduction', 'Slice', 'Speed', 'Structure', 'Surface', 'System', 'Techniques', 'Thinness', 'Time', 'base', 'blood oxygen level dependent', 'cost', 'design', 'gray matter', 'hemodynamics', 'image reconstruction', 'improved', 'magnetic field', 'motion sensitivity', 'neuroimaging', 'perfusion imaging', 'phase change', 'reconstruction', 'response', 'simulation', 'spectroscopic imaging', 'time use', 'transmission process', 'virtual']",NIBIB,VANDERBILT UNIVERSITY,R01,2019,569563,0.042842344443703245
"Array-Compressed Parallel Transmission for High Resolution Neuroimaging at 7T Project Summary The goal of this project is to develop a framework for high-performance parallel transmission (pTx) that is trans- ferable to a wide range of MRI scanners, and apply it to push the spatial encoding limits of echo planar imaging (EPI) at 7 Tesla. EPI is by far the most widely used pulse sequence for rapid functional, diffusion, and perfusion imaging, and has been the focus of considerable development in recent years to increase its speed and spatial resolution. Now there is a strong desire to push EPI's spatial resolution down to the micro scale. For functional MRI (fMRI), this would enable imaging of ﬁne structures (layers, columns, and nuclei) of cortical and subcortical architecture while better resolving the hemodynamic response. For diffusion MRI (dMRI), micro scale EPI would improve surface and laminar analysis of ﬁbers in the cortex, as well as brain parcelation using fractional anisotropy differences between gray matter regions, while broadly reducing partial volume effects. It would further enable EPI to be broadly applied to accelerate anatomic scans that are geometrically matched to fMRI and dMRI scans. However, increasing the resolution of single-shot EPI requires longer readouts which extend echo times and re- duce functional contrast in fMRI and signal-to-noise in dMRI at 7 Tesla, while increasing geometric distortions and blurring. Segmented or multishot EPI is a classic method to increase spatial resolution without increasing readout durations, but is underutilized, primarily due to its high sensitivity to motion and dynamic phase changes between shots which cause large image artifacts.  We propose to develop a new multishot EPI technique called shuttered EPI, which addresses the lim- itations of conventional multishot EPI by imaging a set of spatially disjoint shutters in each shot. The shutters are produced by a multidimensional excitation pulse and are spatially shifted between shots to cover an entire slice. However, with thin slices the length of the excitation pulses are impractical (20-100 ms). Many-coil pTx (> 8 coils) can shorten the length of these pulses to feasible durations, but current 7 Tesla scanners have only 8 transmit channels due to cost, footprint, cabling, and other constraints. In the ﬁrst project period we pioneered a technique called array-compressed pTx (acpTx) which overcomes this limitation. Using acpTx, 8 transmit chan- nels can control an arbitrarily large number of coils, where the channels and coils are connected via an array compression network that is optimized with RF pulses for speciﬁc excitations. In this project, we will develop and apply acpTx methods and hardware (a many-coil head transmit array and an 8 channel-to-many coil array com- pression network) to achieve feasible RF pulse durations when exciting the shutter patterns required for shuttered EPI. These developments will be implemented on two major 7T scanner platforms and evaluated in submillimeter (600 micron) fMRI and dMRI acquisitions. Overall, the project encompasses the synergistic design of RF pulses, hardware, acquisitions and reconstructions to achieve a major advance in spatial encoding. Project Narrative Diffusion and functional magnetic resonance imaging (MRI) at 7 Tesla ﬁeld strength using echo planar imaging (EPI) has the potential to deliver clear images of brain structure and function at the level of layers, columns, and nuclei. However, when existing EPI scans are pushed to the spatial resolutions required to resolve these structures, they become highly sensitive to off resonance-induced geometric distortions, relaxation-induced blur- ring, physiological noise and motion. To address this problem, in this project we will develop many-coil array- compressed parallel transmission and apply it to enable shuttered multishot EPI scans that are robust to these effects.",Array-Compressed Parallel Transmission for High Resolution Neuroimaging at 7T,9524416,R01EB016695,"['Address', 'Algorithms', 'Anatomy', 'Anisotropy', 'Architecture', 'Biological', 'Brain', 'Brain imaging', 'Cell Nucleus', 'Data', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Echo-Planar Imaging', 'Elements', 'Fiber', 'Functional Magnetic Resonance Imaging', 'Funding', 'Goals', 'Head', 'Homebound Persons', 'Image', 'Image Enhancement', 'Imaging Techniques', 'Length', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measurement', 'Methods', 'Morphologic artifacts', 'Motion', 'Noise', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physiologic pulse', 'Physiological', 'Problem Formulations', 'Relaxation', 'Research Project Grants', 'Resolution', 'Scanning', 'Shapes', 'Signal Transduction', 'Slice', 'Speed', 'Structure', 'Surface', 'System', 'Techniques', 'Thinness', 'Time', 'base', 'blood oxygen level dependent', 'cost', 'design', 'gray matter', 'hemodynamics', 'image reconstruction', 'improved', 'magnetic field', 'motion sensitivity', 'neuroimaging', 'perfusion imaging', 'phase change', 'reconstruction', 'response', 'simulation', 'spectroscopic imaging', 'time use', 'transmission process', 'virtual']",NIBIB,VANDERBILT UNIVERSITY,R01,2018,598063,0.042842344443703245
"Computational and Statistical Framework to Model Tissue Shape and Mechanics PROJECT SUMMARY  The morphologic and mechanical characteristics of a tissue are fundamental to understanding the development, homeostasis, and pathology of the human body. During the previous period of funding, we developed statistical shape modeling (SSM) methods and applied these to the study of structural hip disease. We also developed the initial framework to integrate SSM with finite element (FE) analysis to enable the study of shape and mechanics together. If incorporated into clinical practice, SSM and FE analysis could identify features of the anatomy likely responsible for injury, remodeling, or repair. Geometry needed for SSM and FE models is typically generated by segmentation of volumetric imaging data. This step can be painstakingly slow, error prone, and cost prohibitive, which hampers clinical application of these computational techniques. We have created a deep machine learning algorithm ‘DeepSSM’ that uses a convolutional neural network to establish the correspondence model directly from unsegmented images. In Aim 1 we will apply DepSSM to improve clinical understanding of structural hip disease by characterizing differences in anatomy between symptomatic and asymptomatic individuals; these morphometric comparisons will identify anatomic features most telling of disease, thereby guiding improvements in diagnosis. Computational advancements have simplified the process to generate patient-specific FE models, enabling clinically focused research. However, there is no framework to collectively visualize, compare, and interpret (i.e., post-process) results from multiple FE models. Currently, inter-subject comparisons require oversimplifications such as averaging results over subjectively defined regions. In Aim 2 we will develop new post-processing methods to collectively visualize, interpret and statistically analyze FE results across multiple subjects and study groups. We will map FE results to synthetic anatomies representing statistically meaningful distributions using the correspondence model. Statistical parametric mapping will be applied to preserve anatomic detail through statistical testing. We will use our published FE models of hip joint mechanics as the test system. Finally, volumetric images provide a wealth of information that is delivered to physicians in a familiar format. Yet, tools are not available to interpret model data with clinical findings from volumetric images. In Aim 3, we will develop methods that evaluate relationships between shape, mechanics, and clinical findings gleaned from imaging through integrated statistical tests and semi-automatic medical image annotation tools that utilize standard ontologies. Quantitative CT and MRI images of the hip, which estimate bone density and cartilage ultrastructure, respectively, will be evaluated as test datasets. To impart broad impact, we will disseminate our methods to the community as open source software that will call core functionality provided by existing, open source software that has a large user base (FEBio, ShapeWorks). PROJECT NARRATIVE The proposed technology will provide the methodologies necessary to increase the clinical acceptance and applicability of computer models. These models measure three-dimensional tissue shape and estimate tissue mechanics, providing information that cannot be measured conventionally. We will implement these methods into software that can be used by the public free-of-charge.",Computational and Statistical Framework to Model Tissue Shape and Mechanics,9972694,R01EB016701,"['3-Dimensional', 'Adoption', 'Algorithms', 'Anatomy', 'Architecture', 'Bone Density', 'Cardiology', 'Cartilage', 'Characteristics', 'Charge', 'Clinical', 'Communities', 'Computational Technique', 'Computer Models', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Deformity', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Finite Element Analysis', 'Foundations', 'Funding', 'Geometry', 'Glean', 'Grooming', 'Hip Joint', 'Hip region structure', 'Homeostasis', 'Human Pathology', 'Human body', 'Image', 'Individual', 'Injury', 'Intuition', 'Libraries', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Mechanics', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morphology', 'Neurology', 'Ontology', 'Orthopedics', 'Pathology', 'Patient imaging', 'Patients', 'Performance', 'Physicians', 'Procedures', 'Process', 'Publishing', 'Quantitative Evaluations', 'Research', 'Resources', 'Scheme', 'Shapes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Tissue Model', 'Tissues', 'Training', 'Validation', 'X-Ray Computed Tomography', 'annotation  system', 'base', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data modeling', 'disease diagnosis', 'improved', 'in vivo', 'machine learning algorithm', 'novel', 'open source', 'predictive modeling', 'preservation', 'relating to nervous system', 'repaired', 'shape analysis', 'simulation', 'three-dimensional modeling', 'tool']",NIBIB,UNIVERSITY OF UTAH,R01,2020,563658,-0.03386119045939458
"Multiresolution Autofocusing for Automated Microscopy    DESCRIPTION (provided by applicant): This project will further develop and refine an innovative digital auto-focus technology for automated microscopy. Auto-focusing is essential to automated microscope imaging. Currently available techniques rely on various algorithms of focus computation at a single image resolution and suffer from inherent performance limitations, which affect their success and utilization in clinical and research applications. Auto-focusing for fluorescence microscopy, for example, represents a serious challenge to existing methods for desired accuracy, reliability and speed since in this case the images have very low signal-to-noise ratio and narrow depth-of-fields while specimen exposure to fluorescent excitation must be minimized to avoid photo-bleaching and formation of undesirable substances such as free radicals and singlet oxygen. We propose a novel multi-resolution image analysis approach to microscope auto-focusing, based on the recently developed mathematical theory of wavelet transform. The new approach overcomes a number of inherent limitations of currently available techniques, and holds the promise to make the measurement of the microscope focus function and the detection of best-focus imaging position considerably more accurate, reliable, and fast. This innovative technology will significantly increase the ability and efficacy of automated microscope instruments for a wide range of clinical and research applications where a large number of specimens need to be imaged and quantitatively analyzed on a routine basis. During the Phase 1 project we investigated the feasibility of the proposed technology for fluorescence microscopy. We developed software to implement the algorithms for multi-resolution focus function measurements and for in-focus imaging position search. We evaluated the new approach in software simulation on a variety of sample image stacks of cytogenetic FISH specimens, and compared it with all current best-performing methods for microscope auto-focusing using the criteria of (1) accuracy, (2) range, (3) robustness, and (4) speed. The Phase 1 results suggest that, by using a proper wavelet-based auto-focus function, the new multi-resolution method significantly outperforms all competing methods in each of the aforementioned performance categories, and clearly exceeds the Phase 1 feasibility criteria. In the Phase 2 project, we will further develop, refine, integrate, and validate the new technology in real-time operation environment. We plan to build a prototype system with multi-resolution auto-focusing capabilities for both fluorescence and bright-field microscope imaging. We will evaluate the system extensively for a variety of applications including genetics, pathology, and cytology. We will beta test the new system and technology in routine clinical laboratory environment and optimize the technology as end user input and feedbacks are gathered. Once fully developed and qualified, this new technology will be patented and incorporated into future IRIS automated imaging cytometry instruments. It will also be made commercially available to Applied Imaging Corporation and other manufacturers of automated microscope instruments through licensing agreements and partnerships.           n/a",Multiresolution Autofocusing for Automated Microscopy,6951446,R44RR016817,"['artificial intelligence', 'bioimaging /biomedical imaging', 'biomedical automation', 'clinical research', 'computer program /software', 'computer system design /evaluation', 'digital imaging', 'flow cytometry', 'fluorescence microscopy', 'fluorescent in situ hybridization', 'human data', 'image enhancement', 'image processing', 'mathematical model']",NCRR,"ADVANCED DIGITAL IMAGING RESEARCH, LLC",R44,2005,318350,-0.009740331097368667
"Multiresolution Autofocusing for Automated Microscopy    DESCRIPTION (provided by applicant): This project will further develop and refine an innovative digital auto-focus technology for automated microscopy. Auto-focusing is essential to automated microscope imaging. Currently available techniques rely on various algorithms of focus computation at a single image resolution and suffer from inherent performance limitations, which affect their success and utilization in clinical and research applications. Auto-focusing for fluorescence microscopy, for example, represents a serious challenge to existing methods for desired accuracy, reliability and speed since in this case the images have very low signal-to-noise ratio and narrow depth-of-fields while specimen exposure to fluorescent excitation must be minimized to avoid photo-bleaching and formation of undesirable substances such as free radicals and singlet oxygen. We propose a novel multi-resolution image analysis approach to microscope auto-focusing, based on the recently developed mathematical theory of wavelet transform. The new approach overcomes a number of inherent limitations of currently available techniques, and holds the promise to make the measurement of the microscope focus function and the detection of best-focus imaging position considerably more accurate, reliable, and fast. This innovative technology will significantly increase the ability and efficacy of automated microscope instruments for a wide range of clinical and research applications where a large number of specimens need to be imaged and quantitatively analyzed on a routine basis. During the Phase 1 project we investigated the feasibility of the proposed technology for fluorescence microscopy. We developed software to implement the algorithms for multi-resolution focus function measurements and for in-focus imaging position search. We evaluated the new approach in software simulation on a variety of sample image stacks of cytogenetic FISH specimens, and compared it with all current best-performing methods for microscope auto-focusing using the criteria of (1) accuracy, (2) range, (3) robustness, and (4) speed. The Phase 1 results suggest that, by using a proper wavelet-based auto-focus function, the new multi-resolution method significantly outperforms all competing methods in each of the aforementioned performance categories, and clearly exceeds the Phase 1 feasibility criteria. In the Phase 2 project, we will further develop, refine, integrate, and validate the new technology in real-time operation environment. We plan to build a prototype system with multi-resolution auto-focusing capabilities for both fluorescence and bright-field microscope imaging. We will evaluate the system extensively for a variety of applications including genetics, pathology, and cytology. We will beta test the new system and technology in routine clinical laboratory environment and optimize the technology as end user input and feedbacks are gathered. Once fully developed and qualified, this new technology will be patented and incorporated into future IRIS automated imaging cytometry instruments. It will also be made commercially available to Applied Imaging Corporation and other manufacturers of automated microscope instruments through licensing agreements and partnerships.           n/a",Multiresolution Autofocusing for Automated Microscopy,6833120,R44RR016817,"['artificial intelligence', 'bioimaging /biomedical imaging', 'biomedical automation', 'clinical research', 'computer program /software', 'computer system design /evaluation', 'digital imaging', 'flow cytometry', 'fluorescence microscopy', 'fluorescent in situ hybridization', 'human data', 'image enhancement', 'image processing', 'mathematical model']",NCRR,"ADVANCED DIGITAL IMAGING RESEARCH, LLC",R44,2004,427818,-0.009740331097368667
"Center for Advanced Imaging Innovation and Research (CAI2R) Overall Project Summary  The Center for Advanced Imaging Innovation and Research (CAI2R) pursues a mission of bringing people together to create new ways of seeing. The work of our Center has been focused on creating new paradigms for the acquisition, reconstruction, and interpretation of biomedical images, and on implementing new collaboration models in order to translate these developments rapidly into clinical practice.  The world of biomedical imaging is changing, and CAI2R has been at the forefront of that change. Tasks that were once the sole domain of meticulously-engineered imaging hardware are now beginning to be accomplished in software, increasingly informed by diverse arrays of inexpensive auxiliary sensors. Information once pursued through the laborious acquisition of carefully separated image datasets is now being derived from newly integrated, and richly quantitative, data streams. In keeping with these themes, our Center will be organized around the following four Technology Research and Development (TR&D) projects going forward: 1. Reimagining the Future of Scanning: Intelligent image acquisition, reconstruction, and analysis. 2. Unshackling the Scanners of the Future: Flexible, self-correcting, multisensor machines. 3. Enriching the Data Stream: MRI and PET in concert. 4. Revealing Microstructure: Biophysical modeling and validation for discovery and clinical care.  In each of these projects, we aim to push medical imaging technology to the next level, both in hardware and in software. Having made great strides in developing rapid, continuous imaging data streams, we will next aim to add key new information to those streams, both from physics-driven microstructural modeling and from data- driven machine learning. Having focused on the development of robust tools for image acquisition and reconstruction, we will extend the pipeline to image interpretation, using the results of human- or machine- derived evaluations of image content as feedback for the further improvement of acquisition strategies and sensor designs. We will also aim to close the loop between diagnostic sensing and therapeutic intervention, exploring new ways to guide therapy with continuously-acquired information about tissue bioeffects.  Our Center has an explicit translational focus, which is reflected in the day-to-day operation of TR&D projects as well as in the topics of Collaborative Projects (CPs) and Service Projects (SPs), which are focused on three general areas of high public health impact: cancer, musculoskeletal disease, and neurologic disease.  In keeping with this translational emphasis, CAI2R is also be driven by an embedded collaboration model in which basic scientists, clinicians, and industry developers sit down together regularly at the scanners for interactive technology development and assessment. With early involvement of clinical stakeholders and industry partners, we aim to make CAI2R technologies widely available, for the advancement of biomedical knowledge and for the benefit of patients and the physicians who care for them. Overall Project Narrative  The Center for Advanced Imaging Innovation and Research (CAI2R) develops novel imaging techniques and technologies for the improved diagnosis and management of cancer, musculoskeletal disease, neurological disease and other disorders with a profound impact on human health. By exploiting connections between imaging modalities such as MRI and PET, we aim to advance the fundamental capabilities of each, so as to expand biomedical knowledge and improve the care of patients.",Center for Advanced Imaging Innovation and Research (CAI2R),9996604,P41EB017183,"['Adopted', 'Area', 'Artificial Intelligence', 'Biology', 'Caring', 'Clinical', 'Collaborations', 'Color', 'Computer software', 'Country', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Doctor of Philosophy', 'Engineering', 'Feedback', 'Funding', 'Future', 'Goals', 'Health', 'Human', 'Image', 'Image Analysis', 'Imagination', 'Imaging Device', 'Imaging technology', 'Industrial Product', 'Industry', 'Institution', 'Intelligence', 'Knowledge', 'Legal patent', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Mission', 'Modeling', 'Modernization', 'Musculoskeletal Diseases', 'Patient Care', 'Patients', 'Performance', 'Philosophy', 'Physicians', 'Physics', 'Positron-Emission Tomography', 'Process', 'Productivity', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Scanning', 'Scientist', 'Services', 'Software Tools', 'Stream', 'Technology', 'Technology Assessment', 'Testing', 'Therapeutic Intervention', 'Time', 'Tissues', 'Training', 'Translating', 'Ursidae Family', 'Validation', 'Visit', 'Work', 'bioimaging', 'biophysical model', 'cancer imaging', 'clinical care', 'clinical practice', 'data acquisition', 'data streams', 'design', 'flexibility', 'image reconstruction', 'imaging approach', 'imaging modality', 'imaging scientist', 'improved', 'industry partner', 'innovation', 'interest', 'medical schools', 'multidisciplinary', 'musculoskeletal imaging', 'nervous system disorder', 'neuroimaging', 'novel imaging technique', 'open source', 'operation', 'radio frequency', 'reconstruction', 'sensor', 'technology development', 'technology research and development', 'tool', 'web site']",NIBIB,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,P41,2020,1198860,-0.010886864523903316
"Center for Advanced Imaging Innovation and Research (CAI2R) Overall Project Summary  The Center for Advanced Imaging Innovation and Research (CAI2R) pursues a mission of bringing people together to create new ways of seeing. The work of our Center has been focused on creating new paradigms for the acquisition, reconstruction, and interpretation of biomedical images, and on implementing new collaboration models in order to translate these developments rapidly into clinical practice.  The world of biomedical imaging is changing, and CAI2R has been at the forefront of that change. Tasks that were once the sole domain of meticulously-engineered imaging hardware are now beginning to be accomplished in software, increasingly informed by diverse arrays of inexpensive auxiliary sensors. Information once pursued through the laborious acquisition of carefully separated image datasets is now being derived from newly integrated, and richly quantitative, data streams. In keeping with these themes, our Center will be organized around the following four Technology Research and Development (TR&D) projects going forward: 1. Reimagining the Future of Scanning: Intelligent image acquisition, reconstruction, and analysis. 2. Unshackling the Scanners of the Future: Flexible, self-correcting, multisensor machines. 3. Enriching the Data Stream: MRI and PET in concert. 4. Revealing Microstructure: Biophysical modeling and validation for discovery and clinical care.  In each of these projects, we aim to push medical imaging technology to the next level, both in hardware and in software. Having made great strides in developing rapid, continuous imaging data streams, we will next aim to add key new information to those streams, both from physics-driven microstructural modeling and from data- driven machine learning. Having focused on the development of robust tools for image acquisition and reconstruction, we will extend the pipeline to image interpretation, using the results of human- or machine- derived evaluations of image content as feedback for the further improvement of acquisition strategies and sensor designs. We will also aim to close the loop between diagnostic sensing and therapeutic intervention, exploring new ways to guide therapy with continuously-acquired information about tissue bioeffects.  Our Center has an explicit translational focus, which is reflected in the day-to-day operation of TR&D projects as well as in the topics of Collaborative Projects (CPs) and Service Projects (SPs), which are focused on three general areas of high public health impact: cancer, musculoskeletal disease, and neurologic disease.  In keeping with this translational emphasis, CAI2R is also be driven by an embedded collaboration model in which basic scientists, clinicians, and industry developers sit down together regularly at the scanners for interactive technology development and assessment. With early involvement of clinical stakeholders and industry partners, we aim to make CAI2R technologies widely available, for the advancement of biomedical knowledge and for the benefit of patients and the physicians who care for them. Overall Project Narrative  The Center for Advanced Imaging Innovation and Research (CAI2R) develops novel imaging techniques and technologies for the improved diagnosis and management of cancer, musculoskeletal disease, neurological disease and other disorders with a profound impact on human health. By exploiting connections between imaging modalities such as MRI and PET, we aim to advance the fundamental capabilities of each, so as to expand biomedical knowledge and improve the care of patients.",Center for Advanced Imaging Innovation and Research (CAI2R),9804438,P41EB017183,"['Adopted', 'Area', 'Artificial Intelligence', 'Biology', 'Caring', 'Clinical', 'Collaborations', 'Color', 'Computer software', 'Country', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Doctor of Philosophy', 'Engineering', 'Feedback', 'Funding', 'Future', 'Goals', 'Health', 'Human', 'Image', 'Image Analysis', 'Imagination', 'Imaging Device', 'Imaging technology', 'Industrial Product', 'Industry', 'Institution', 'Intelligence', 'Knowledge', 'Legal patent', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Mission', 'Modeling', 'Modernization', 'Musculoskeletal Diseases', 'Patient Care', 'Patients', 'Performance', 'Philosophy', 'Physicians', 'Physics', 'Positron-Emission Tomography', 'Process', 'Productivity', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Scanning', 'Scientist', 'Services', 'Software Tools', 'Stream', 'Technology', 'Technology Assessment', 'Testing', 'Therapeutic Intervention', 'Time', 'Tissues', 'Training', 'Translating', 'Ursidae Family', 'Validation', 'Visit', 'Work', 'bioimaging', 'biophysical model', 'cancer imaging', 'clinical care', 'clinical practice', 'data acquisition', 'design', 'flexibility', 'image reconstruction', 'imaging approach', 'imaging modality', 'imaging scientist', 'improved', 'industry partner', 'innovation', 'interest', 'medical schools', 'multidisciplinary', 'musculoskeletal imaging', 'nervous system disorder', 'neuroimaging', 'novel imaging technique', 'off-patent', 'open source', 'operation', 'radio frequency', 'reconstruction', 'sensor', 'technology development', 'technology research and development', 'tool', 'web site']",NIBIB,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,P41,2019,1715698,-0.010886864523903316
"Mechanisms for invariance in auditory cortex: Investigations with marmoset electrophysiology Project Summary Listening in noise is a core problem in everyday hearing. Sound sources of interest routinely occur amid irrelevant distractors, as when you talk with someone in a bustling coffee shop. This background “noise” distorts the pattern of spikes in the auditory nerve, often to a profound degree. Thus, to recognize sources of interest, the auditory system must somehow separate or suppress the effects of the background. Typical human hearing is remarkably noise-robust, but listeners with age-related hearing loss or other forms of impaired hearing struggle in noisy environments – and are not much helped by contemporary hearing aids. Previous work on the neural basis of noise robustness has typically employed simple, synthetic noise sources, which lack the structure present in real-world sounds, and this work has focused on subcortical regions or on primary auditory cortex. Reasoning that real-world conditions might necessitate more complicated solutions, in the applicant's doctoral work, he considered everyday sources of noise, and leveraged the large-scale coverage afforded by fMRI to examine noise robustness throughout human auditory cortex. Real-world “background noise” was operationalized as a natural sound with statistical properties that are stable over time (i.e., are stationary), conveying little new information about the world (e.g., swamp insects, an air conditioner, rain on pavement). The applicant measured fMRI responses in human listeners to a broad set of natural sounds presented in quiet, as well as embedded in the real-world background noises. Primary auditory cortical responses were substantially altered by the background, but non-primary responses were substantially more robust. This effect was not seen for simple synthetic backgrounds as had been used in previous work, suggesting that becoming robust to real-world background noises require different mechanisms. The applicant's thesis work demonstrates where noise invariance arises, but understanding how will require data with finer spatial and temporal resolution, and thus the proposed postdoctoral research will consist of training in single-unit electrophysiology using marmosets. Aim 1A builds on previous work examining single- unit noise robustness in artificial conditions, extending such work to real-world noise. Aim 1B leverages texture models to probe what aspects of real-world backgrounds disrupt the encoding of foregrounds. Aim 2A deploys linear reconstruction techniques to probe population representations. Aim 2B involves optimizing deep neural networks for noise invariance tasks, and using them as an encoding model to predict single-unit responses. Furthermore, such networks will be deployed as nonlinear decoding algorithms, reconstructing stimuli from neuronal populations. Throughout all aims, the work will characterize neuronal responses in non-primary areas, and in particular in parabelt, which is understudied in primates. The proposed work may enable improvements in hearing aid algorithms or neural prosthetics. Lastly, this training will lay the groundwork for the applicant's long-term goal of developing a marmoset model for hearing loss. Project Narrative Typical human hearing is remarkably robust to the presence of background noise, but listeners with age- related hearing loss or other forms of impaired hearing struggle in noisy environments – and often do not benefit from contemporary hearing aids in these conditions. In my doctoral work, using fMRI in humans I showed that real-world background noise engages different mechanisms than the simpler synthetic noise typically employed in previous work. In my postdoctoral work I will be trained in marmoset electrophysiology to zoom in to the level of neural circuits to probe the mechanisms that generate auditory cortex's robustness to background noise.",Mechanisms for invariance in auditory cortex: Investigations with marmoset electrophysiology,10104430,F32DC017628,"['Acoustic Nerve', 'Address', 'Air', 'Algorithms', 'Area', 'Attention', 'Auditory', 'Auditory Prosthesis', 'Auditory area', 'Auditory system', 'Basic Science', 'Brain', 'Callithrix', 'Clinical', 'Code', 'Coffee', 'Computer Models', 'Data', 'Data Analyses', 'Development', 'Electrophysiology (science)', 'Environment', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grain', 'Hearing', 'Hearing Aids', 'Human', 'Impaired cognition', 'Individual', 'Insecta', 'Investigation', 'Measures', 'Microelectrodes', 'Modeling', 'Neurons', 'Noise', 'Pattern', 'Performance', 'Physiological', 'Population', 'Presbycusis', 'Primates', 'Process', 'Property', 'Quality of life', 'Rain', 'Research', 'Silicon', 'Source', 'Stimulus', 'Structure', 'Techniques', 'Texture', 'Time', 'Training', 'Work', 'algorithm training', 'base', 'deep learning', 'deep neural network', 'experience', 'experimental study', 'hearing impairment', 'improved', 'interest', 'neural circuit', 'neural prosthesis', 'neuromechanism', 'programs', 'reconstruction', 'relating to nervous system', 'response', 'signal processing', 'social', 'sound', 'temporal measurement']",NIDCD,COLUMBIA UNIVERSITY HEALTH SCIENCES,F32,2020,64926,-0.0378008804444664
"NORMALIZED BODY SURFACE POTENTIAL MAPS The research has been divided into the following four integrated components:  (1) Instrumentation - Completion of the development of ""State-of-the Art"" Instrumentation to rapidly obtain, on-line and stand-alone at all age groups, electrocardiographic low noise body surface potential color coded maps (BSPM) utilizing 180 passive electrodes with active cables.  Insuring optimal safety in obtaining BSPM simultaneous with indwelling cardiac electrode catheters.  (2) Clinical Studies - four major subprojects.  a) Correlations of simultaneously obtained BSPM with endocardial electrophysiological studies (EPS), with analysis of spread of activation from known positioned stimulated activation during catheterization; b) Correlations of simultaneously obtained BSPM with spread of activation from known positioned epicardial pacemakers; c) Continued development of a normal quantified data base for P, QRS, ST-T, as well as development of maximal knowledge of BSPM's on a large number of children with varying pathology.  Correlations with Frank VCG plus M Mode and 2D Echo; d) Correlation of quantified BSPM's in adults with remote myocardial infarction undergoing cardiac catheterization involving coronary angiography and ventriculography, plus 12 lead simultaneous standard ECG, Frank VCG, and Positron Emission Tomography (PET).  (3) Image Measurements - Investigation of optimal imaging and display of BSPM data including determination of the effects of artifacts and system bandwidth upon the measurements of map features.  Develop a systematic approach to automatic measurements of the map features which encode clinical information.  (4) Biophysical Studies - a) Inverse Electrocardiography Methods for reconstructing epicardial potentials from body surface potential distributions utilizing the multipole expansion approach will be developed and tested.  The reconstructed epicardial maps will aid in the interpretations of BSPM's; b) Forward Problem - An interactive computer model of the electrocardiographic forward problem for realistic heart and torso geometry is being developed.  The model will contain the specialized conduction tissue and will generate isochrones, epicardial potentials, and body surface potential maps (BSPM's).  This model will be used to simulate BSPM's in normal and abnormal conditions and will serve as an additional tool for the interpretation of the maps.  n/a",NORMALIZED BODY SURFACE POTENTIAL MAPS,3335500,R01HL017931,"['Wolff Parkinson White syndrome', ' aortic valve stenosis', ' artificial intelligence', ' biological models', ' biophysics', ' child (0-11)', ' clinical biomedical equipment', ' computer assisted diagnosis', ' computer simulation', ' congenital heart disorder', ' diagnosis quality /standard', ' echocardiography', ' electrical potential', ' electrocardiography', ' electrodes', ' electronic pacemaker', ' electrophysiology', ' epicardial mapping', ' heart catheterization', ' heart disorder diagnosis', ' hemodynamics', ' human subject', ' image processing', ' mathematical model', ' myocardial infarction', ' noninvasive diagnosis', ' ventricular hypertrophy']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,1987,208742,-0.05562880738172249
"NORMALIZED BODY SURFACE POTENTIAL MAPS The research has been divided into the following four integrated components:  (1) Instrumentation - Completion of the development of ""State-of-the Art"" Instrumentation to rapidly obtain, on-line and stand-alone at all age groups, electrocardiographic low noise body surface potential color coded maps (BSPM) utilizing 180 passive electrodes with active cables.  Insuring optimal safety in obtaining BSPM simultaneous with indwelling cardiac electrode catheters.  (2) Clinical Studies - four major subprojects.  a) Correlations of simultaneously obtained BSPM with endocardial electrophysiological studies (EPS), with analysis of spread of activation from known positioned stimulated activation during catheterization; b) Correlations of simultaneously obtained BSPM with spread of activation from known positioned epicardial pacemakers; c) Continued development of a normal quantified data base for P, QRS, ST-T, as well as development of maximal knowledge of BSPM's on a large number of children with varying pathology.  Correlations with Frank VCG plus M Mode and 2D Echo; d) Correlation of quantified BSPM's in adults with remote myocardial infarction undergoing cardiac catheterization involving coronary angiography and ventriculography, plus 12 lead simultaneous standard ECG, Frank VCG, and Positron Emission Tomography (PET).  (3) Image Measurements - Investigation of optimal imaging and display of BSPM data including determination of the effects of artifacts and system bandwidth upon the measurements of map features.  Develop a systematic approach to automatic measurements of the map features which encode clinical information.  (4) Biophysical Studies - a) Inverse Electrocardiography Methods for reconstructing epicardial potentials from body surface potential distributions utilizing the multipole expansion approach will be developed and tested.  The reconstructed epicardial maps will aid in the interpretations of BSPM's; b) Forward Problem - An interactive computer model of the electrocardiographic forward problem for realistic heart and torso geometry is being developed.  The model will contain the specialized conduction tissue and will generate isochrones, epicardial potentials, and body surface potential maps (BSPM's).  This model will be used to simulate BSPM's in normal and abnormal conditions and will serve as an additional tool for the interpretation of the maps.  n/a",NORMALIZED BODY SURFACE POTENTIAL MAPS,3335499,R01HL017931,"['Wolff Parkinson White syndrome', ' aortic valve stenosis', ' artificial intelligence', ' biological models', ' biophysics', ' child (0-11)', ' clinical biomedical equipment', ' computer assisted diagnosis', ' computer simulation', ' congenital heart disorder', ' diagnosis quality /standard', ' echocardiography', ' electrical potential', ' electrocardiography', ' electrodes', ' electronic pacemaker', ' electrophysiology', ' epicardial mapping', ' heart catheterization', ' heart disorder diagnosis', ' hemodynamics', ' human subject', ' image processing', ' mathematical model', ' myocardial infarction', ' noninvasive diagnosis', ' ventricular hypertrophy']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,1986,203786,-0.05562880738172249
"NORMALIZED BODY SURFACE POTENTIAL MAPS The research has been divided into the following four integrated components:  (1) Instrumentation - Completion of the development of ""State-of-the Art"" Instrumentation to rapidly obtain, on-line and stand-alone at all age groups, electrocardiographic low noise body surface potential color coded maps (BSPM) utilizing 180 passive electrodes with active cables.  Insuring optimal safety in obtaining BSPM simultaneous with indwelling cardiac electrode catheters.  (2) Clinical Studies - four major subprojects.  a) Correlations of simultaneously obtained BSPM with endocardial electrophysiological studies (EPS), with analysis of spread of activation from known positioned stimulated activation during catheterization; b) Correlations of simultaneously obtained BSPM with spread of activation from known positioned epicardial pacemakers; c) Continued development of a normal quantified data base for P, QRS, ST-T, as well as development of maximal knowledge of BSPM's on a large number of children with varying pathology.  Correlations with Frank VCG plus M Mode and 2D Echo; d) Correlation of quantified BSPM's in adults with remote myocardial infarction undergoing cardiac catheterization involving coronary angiography and ventriculography, plus 12 lead simultaneous standard ECG, Frank VCG, and Positron Emission Tomography (PET).  (3) Image Measurements - Investigation of optimal imaging and display of BSPM data including determination of the effects of artifacts and system bandwidth upon the measurements of map features.  Develop a systematic approach to automatic measurements of the map features which encode clinical information.  (4) Biophysical Studies - a) Inverse Electrocardiography Methods for reconstructing epicardial potentials from body surface potential distributions utilizing the multipole expansion approach will be developed and tested.  The reconstructed epicardial maps will aid in the interpretations of BSPM's; b) Forward Problem - An interactive computer model of the electrocardiographic forward problem for realistic heart and torso geometry is being developed.  The model will contain the specialized conduction tissue and will generate isochrones, epicardial potentials, and body surface potential maps (BSPM's).  This model will be used to simulate BSPM's in normal and abnormal conditions and will serve as an additional tool for the interpretation of the maps.  n/a",NORMALIZED BODY SURFACE POTENTIAL MAPS,3335494,R01HL017931,"['Wolff Parkinson White syndrome', ' aortic valve stenosis', ' artificial intelligence', ' biological models', ' biophysics', ' child (0-11)', ' clinical biomedical equipment', ' computer assisted diagnosis', ' computer simulation', ' congenital heart disorder', ' diagnosis quality /standard', ' echocardiography', ' electrical potential', ' electrocardiography', ' electrodes', ' electronic pacemaker', ' electrophysiology', ' epicardial mapping', ' heart catheterization', ' heart disorder diagnosis', ' hemodynamics', ' human subject', ' image processing', ' mathematical model', ' myocardial infarction', ' noninvasive diagnosis', ' ventricular hypertrophy']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,1985,193552,-0.05562880738172249
"Artificial Neural Network modelling for studying posture A major challenge with the application of artificial neural network (ANN) modeling in examining the motor-sensory relation in postural control is that the neural synaptic weights that relate the inputs to the outputs of the ANN are chaotic in nature. In an earlier study through theoretical analysis, numerical simulations and experimental tests, the PI and her colleague have found that these weights are interdependent. The product of these weights is a statistically stable variable and can be used to quantify the input-output relation of the ANN, called Q value. The objective of this proposed research is to extend the above work to the area of human postural control. Specifically, we will explore whether or not a Q value concept in an ANN can be used to quantify the motor-sensory relation in a classical postural control task - maintaining upright balance when the supporting base is suddenly rotated in a toes up direction. We will construct an ANN model that includes two outputs and seven inputs. The two outputs are the EMG signals from ankle dorsiflexor and plantartlexor in response to the onset of the supporting base rotation. The seven inputs are average eye-target distance (distance from eye center to a visual target), head acceleration (both linear and angular), ankle joint rotation, ankle joint rotation speed, and ground reaction forces (both normal and shear) under feet. These inputs represent the mechanical stimulation to the visual, vestibular, and somatosensory systems, respectively. These inputs and outputs variables will be measured directly from two groups of elderly subjects: peripherally neuropathic and normal, non-peripherally neuropathic. We will then determine the weights in the ANN model by a backward-propagation training routine, and the corresponding Q values relating each output to each of the inputs. We will statistically compare the Q values among the multiple sensory inputs within each group. We hypothesize that under this experimental condition, the Q values relating postural muscle activities to the somatosensory inputs would be: (1) significantly higher than the Q values relating to other sensory inputs (such as visual and vestibular inputs) in normal, non-neuropathic subjects; and (2) significantly lower than the Q values relating to other sensory inputs in neuropathic subjects. It is hoped that this study will contribute to our understanding of how sensory information is used to control postural muscle activities, and how a modification in the motor-sensory relation can result in increased postural stability or falls.  n/a",Artificial Neural Network modelling for studying posture,6400894,R03AG019872,"['artificial intelligence', ' balance', ' mathematical model', ' model design /development', ' posture', ' psychomotor function', ' somesthetic sensory cortex', ' vestibular pathway', ' visual pathways']",NIA,UNIVERSITY OF VERMONT & ST AGRIC COLLEGE,R03,2001,75500,-0.06584762849762206
"Conflict Resolution    DESCRIPTION (provided by applicant): The purpose of this Small Business Innovation Research Phase I proposal is to develop Conflict Resolution for Recovery and Relapse Prevention, a 12-session, multimedia psychoeducational curriculum for adult substance abusers, facilitated by counselors and other treatment professionals in diverse treatment settings. The product's overarching goal is to help reduce relapse and sustain recovery by improving client conflict resolution knowledge, attitudes, and skills. When both Phase I and Phase II are completed, Conflict Resolution will be a bundled product consisting of the following interrelated components: 1) A facilitator's guide, which will feature talking points, exercises, and role plays, as well as tips for interacting with groups and individual/family/couple clients around substance abuse conflict resolution issues, visual aids, and evaluation forms; 2) A participant workbook that reiterates key concepts, provides visuals that reinforce content, and includes homework assignments and personal exercise sheets, and 3) A DVD with modules for both counselors and clients, including didactic and interactive elements. The goals of the conflict resolution package are to: 1) Serve as a research-based, empirically tested, psychoeducational curriculum that is effective and appropriate for use with diverse populations of adult substance abusers; 2) Provide treatment and training materials for professionals that are easy to use and integrate into existing community residential and outpatient substance abuse treatment facilities; 3) Provide an effective, cost-efficient, feasible model for improving client conflict resolution capacities; and 4) Offer an innovative program, based on concepts adapted from effective use in other disciplines/environments and making use of today's technology, to enhance relapse-prevention options. To accomplish these goals, the specific aims for the Phase I period are to: 1) Conduct a focus group with drug abuse treatment professionals; 2) Research and develop the content for the facilitator's guide and participant workbook; 3) Conduct a review of the developed content with an expert Advisory Panel; 4) Develop an outline for an accompanying DVD, to be produced in Phase II; and 5) Conduct a feasibility study of the materials with professional stakeholders. The project has the potential to impact public health by helping drug abuse clients build healthy relationships through conflict resolution principles. These healthy relationships will serve to sustain recovery and prevent relapse.          n/a",Conflict Resolution,7270276,R43DA020219,"['AODD relapse', 'Accounting', 'Active Learning', 'Address', 'Adherence', 'Adolescent', 'Adopted', 'Adult', 'Affect', 'Aftercare', 'Agreement', 'Alcohols', 'Anger', 'Area', 'Assertiveness', 'Attention', 'Attitude', 'Behavior', 'Behavioral', 'Behavioral Research', 'Brain', 'Brain Chemistry', 'Case Study', 'Chemicals', 'Child', 'Client', 'Climacteric', 'Clinical', 'Clinical Nurse Specialists', 'Clip', 'Cognitive', 'Cognitive Therapy', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Computers', 'Conflict (Psychology)', 'Contracts', 'Counseling', 'Country', 'Couples', 'Cues', 'Cyprus', 'Daily', 'Dependency', 'Depth', 'Development', 'Discipline', 'Disease', 'Disputes', 'Drug abuse', 'Drug usage', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Educational workshop', 'Effectiveness', 'Elements', 'Emotional', 'Emotions', 'Empathy', 'Employment', 'Ensure', 'Environment', 'Equilibrium', 'Evaluation', 'Exercise', 'Family', 'Family member', 'Family psychotherapy', 'Feasibility Studies', 'Feeling', 'Feeling hopeless', 'Film', 'Focus Groups', 'Frustration', 'Goals', 'Group Processes', 'Group Therapy', 'HIV', 'Healed', 'Health Personnel', 'Hearing', 'Hour', 'Housing', 'Human', 'Individual', 'Inpatients', 'Instinct', 'Institutes', 'Instruction', 'Intelligence', 'International', 'Interpersonal Relations', 'Intervention', 'Knowledge', 'Language', 'Latino', 'Lead', 'Learning', 'Left', 'Legal system', 'Length', 'Letters', 'Life', 'Life Style', 'Limbic System', 'Manuals', 'Marketing', 'Maryland', 'Mediation', 'Mental Health', 'Methods', 'Modeling', 'Motivation', 'Multimedia', 'Needs Assessment', 'Neurologic', 'Neurological Models', 'Nicotine', 'Numbers', 'Online Systems', 'Outcome', 'Outpatients', 'Participant', 'Patients', 'Pattern', 'Personal Satisfaction', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Piper', 'Play', 'Population Heterogeneity', 'Prevention approach', 'Prevention program', 'Procedures', 'Process', 'Professional counselor', 'Program Evaluation', 'Psychologist', 'Public Health', 'Publishing', 'Purpose', 'Qualifying', 'Range', 'Rate', 'Reaction', 'Recovery', 'Rehabilitation therapy', 'Relapse', 'Relative (related person)', 'Research', 'Research Proposals', 'Residential Treatment', 'Resolution', 'Resources', 'Risk', 'Role', 'Role playing therapy', 'Running', 'Schedule', 'School Teachers', 'Series', 'Services', 'Shame', 'Slide', 'Small Business Innovation Research Grant', 'Social Workers', 'Societies', 'Specialist', 'Staff Development', 'Staging', 'Stream', 'Stress', 'Substance Abuse, Other', 'Substance abuse problem', 'Suggestion', 'Surveys', 'System', 'Techniques', 'Technology', 'Teenagers', 'Testing', 'Therapeutic', 'Thinking', 'Time', 'Time Study', 'Tobacco', 'Today', 'Touch sensation', 'Trainers Training', 'Training', 'Training Programs', 'Translating', 'Turtles', 'Uncertainty', 'United States', 'Universities', 'Update', 'Variant', 'Violence', 'Visual Aid', 'Week', 'Work', 'addiction', 'adolescent smoking', 'adolescent substance abuse', 'adolescent substance use', 'alcohol rehabilitation', 'base', 'behavior change', 'brain behavior', 'brain research', 'cognitive change', 'commercial application', 'communication behavior', 'concept', 'coping', 'cost', 'day', 'design', 'disorder later incidence prevention', 'drug addict', 'emotional abuse', 'experience', 'family structure', 'healing', 'improved', 'innovation', 'instrument', 'interest', 'lectures', 'member', 'movie', 'prevent', 'problem drinker', 'programs', 'psychoeducational', 'response', 'size', 'skills', 'smoking cessation', 'social', 'substance abuser', 'technological innovation', 'text searching', 'theories', 'tool', 'treatment center', 'treatment program', 'usability', 'violence prevention']",NIDA,"DANYA INTERNATIONAL, INC.",R43,2007,99998,-0.03410746859441478
"Kernel-based Nonlinear Learning for Fast Magnetic Resonance Imaging with Sub-Nyquist Sampling ﻿    DESCRIPTION (provided by applicant): The quest for fast image acquisition speed has always been a perennial topic in the MRI community. To reduce the acquisition time for maximal spatial and temporal resolution, modern MRI protocols usually perform reduced acquisitions below the Nyquist rate. The reduced data is then used to reconstruct the image through advanced reconstruction techniques that leverage some prior information about the MRI system (e.g., parallel imaging) and/or MR signal (e.g., compressed sensing). Since such prior information is patient and system specific, recent techniques obtain the prior information using training data obtained through an empirical calibration procedure. All existing methods assume the prior models are linear. Since the intrinsic nonlinear relationship in the training data cannot be characterized in such simple models, the reconstruction is degraded by the inaccuracy of the prior information. Nonlinear learning from the training data have proven to be more powerful in machine learning because it is more general and includes the linear model as a special case. However, it is usually more challenging to learn the nonlinear models and even more challenging to incorporate the model in reconstruction due to the increased degree of freedom. We recently have introduced a novel concept of ""kernel"" in MR reconstruction to address the above challenges timely. Our preliminary results on parallel imaging and sparsity-constrained reconstruction demonstrate that the kernel-based algorithms improve the reconstruction quality over the original algorithms with linear prior models. Built upon our strong preliminary results, the objective of this application is to develop an innovative kernel-based framework for MR image reconstruction from undersampled data. This framework does not require explicit knowledge of nonlinear mapping (as in preliminary work) such that a broader family of nonlinear functions can be explored for different clinical applications. The proposed work is expected to advance the field of MR image reconstruction vertically. Specifically, the successful completion of the proposed project will result in a general framework leading to many new algorithms (including two developed in this project) for reconstruction from reduced acquisition. Therefore, virtually all of current clinical MRI could benefit from the improved resolution, image quality, and/or reduced acquisition times that the new framework will facilitate or the novel applications i may enable. PUBLIC HEALTH RELEVANCE: The proposed research is to develop a general framework and two specific new techniques to improve the spatial resolution and/or reduce the scan time in magnetic resonance imaging and evaluate the performance of the techniques for 3D parallel imaging and quantitative imaging in brain. The development of such novel fast imaging techniques may greatly enhance diagnosis of neurological disease. Therefore the project will potentially benefit numerous subjects and the healthcare system.",Kernel-based Nonlinear Learning for Fast Magnetic Resonance Imaging with Sub-Nyquist Sampling,9119020,R21EB020861,"['Address', 'Algorithms', 'Brain', 'Calibration', 'Clinical', 'Communities', 'Data', 'Development', 'Diagnosis', 'Dictionary', 'Family', 'Freedom', 'Health', 'Healthcare Systems', 'Image', 'Imaging Techniques', 'Industry', 'Knowledge', 'Learning', 'Letters', 'Linear Models', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Methods', 'Modeling', 'Non-linear Models', 'Patients', 'Performance', 'Phase', 'Physics', 'Polynomial Models', 'Principal Component Analysis', 'Procedures', 'Protocols documentation', 'Qualifying', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Signal Transduction', 'Software Tools', 'Speed', 'System', 'Techniques', 'Time', 'Training', 'Weight', 'Work', 'base', 'clinical application', 'image reconstruction', 'improved', 'innovation', 'nervous system disorder', 'neuroimaging', 'novel', 'quantitative imaging', 'reconstruction', 'temporal measurement']",NIBIB,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R21,2016,222652,0.021116507345612958
"Kernel-based Nonlinear Learning for Fast Magnetic Resonance Imaging with Sub-Nyquist Sampling ﻿    DESCRIPTION (provided by applicant): The quest for fast image acquisition speed has always been a perennial topic in the MRI community. To reduce the acquisition time for maximal spatial and temporal resolution, modern MRI protocols usually perform reduced acquisitions below the Nyquist rate. The reduced data is then used to reconstruct the image through advanced reconstruction techniques that leverage some prior information about the MRI system (e.g., parallel imaging) and/or MR signal (e.g., compressed sensing). Since such prior information is patient and system specific, recent techniques obtain the prior information using training data obtained through an empirical calibration procedure. All existing methods assume the prior models are linear. Since the intrinsic nonlinear relationship in the training data cannot be characterized in such simple models, the reconstruction is degraded by the inaccuracy of the prior information. Nonlinear learning from the training data have proven to be more powerful in machine learning because it is more general and includes the linear model as a special case. However, it is usually more challenging to learn the nonlinear models and even more challenging to incorporate the model in reconstruction due to the increased degree of freedom. We recently have introduced a novel concept of ""kernel"" in MR reconstruction to address the above challenges timely. Our preliminary results on parallel imaging and sparsity-constrained reconstruction demonstrate that the kernel-based algorithms improve the reconstruction quality over the original algorithms with linear prior models. Built upon our strong preliminary results, the objective of this application is to develop an innovative kernel-based framework for MR image reconstruction from undersampled data. This framework does not require explicit knowledge of nonlinear mapping (as in preliminary work) such that a broader family of nonlinear functions can be explored for different clinical applications. The proposed work is expected to advance the field of MR image reconstruction vertically. Specifically, the successful completion of the proposed project will result in a general framework leading to many new algorithms (including two developed in this project) for reconstruction from reduced acquisition. Therefore, virtually all of current clinical MRI could benefit from the improved resolution, image quality, and/or reduced acquisition times that the new framework will facilitate or the novel applications i may enable.         PUBLIC HEALTH RELEVANCE: The proposed research is to develop a general framework and two specific new techniques to improve the spatial resolution and/or reduce the scan time in magnetic resonance imaging and evaluate the performance of the techniques for 3D parallel imaging and quantitative imaging in brain. The development of such novel fast imaging techniques may greatly enhance diagnosis of neurological disease. Therefore the project will potentially benefit numerous subjects and the healthcare system.            ",Kernel-based Nonlinear Learning for Fast Magnetic Resonance Imaging with Sub-Nyquist Sampling,8953102,R21EB020861,"['Address', 'Algorithms', 'Brain', 'Calibration', 'Clinical', 'Communities', 'Data', 'Development', 'Diagnosis', 'Dictionary', 'Family', 'Freedom', 'Healthcare Systems', 'Image', 'Imaging Techniques', 'Industry', 'Knowledge', 'Learning', 'Letters', 'Linear Models', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Methods', 'Modeling', 'Non-linear Models', 'Patients', 'Performance', 'Phase', 'Physics', 'Principal Component Analysis', 'Procedures', 'Protocols documentation', 'Qualifying', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Signal Transduction', 'Software Tools', 'Speed', 'System', 'Techniques', 'Time', 'Training', 'Weight', 'Work', 'base', 'clinical application', 'image reconstruction', 'improved', 'innovation', 'nervous system disorder', 'neuroimaging', 'novel', 'public health relevance', 'quantitative imaging', 'reconstruction', 'temporal measurement']",NIBIB,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R21,2015,180330,0.021116507345612958
"COMPUTER SIMULATION STUDIES OF THE MENSTRUAL CYCLE In-vivo studies of the menstrual cycle present enormous challenges, as they deal with a complex entity which is the result of multiple interactions between a large number of variables, not all of which can be controlled at one time in an in-vivo setting. Adjunct or alternative approaches to the in-vivo study of the human reproductive cycle have been suggested, among which is the computer simulation approach.  This research proposal represents an attempt to evaluate the validity of this approach. The investigators propose to resume their studies with a computer model of the cyclic morphological and endocrine changes which characterize the menstrual cycle.  The proposal contains three practical aims.  The first aim is to refine the original menstrual cycle computer simulation model. This model was developed more than a decade ago; new data and concepts must now be evaluated, tested and, where required, integrated into the model.  The second aim is to develop a sophisticated user interface.  The model will be adapted to PC desktop type computer hardware and will incorporate principles of what are called ""expert system"" programs.  This will allow physiologists to test and use the computer model in a highly sophisticated, yet easily controllable fashion without the need for an extensive computer background.  The third aim is to test the utility of this computer modeling approach in an experimental setting.  Selected experimentalists will provide us with an experimental setting.  Selected experimentalists will provide us with an objective analysis of the model through their ability to log simulation results.  Ultimately, a refined and validated model of the menstrual cycle can be used to assist in both experimental design and result interpretation and thereby enhance knowledge gained from in-vivo experimentation.  n/a",COMPUTER SIMULATION STUDIES OF THE MENSTRUAL CYCLE,3321661,R01HD022209,"['computer simulation', ' computer system design /evaluation', ' estradiol', ' gonadotropins', ' hormone regulation /control mechanism', ' hypothalamic pituitary axis', ' hypothalamus', ' mathematical model', ' menstrual cycle', ' model design /development', ' neuroendocrine system', ' neuropeptides', ' ovulation']",NICHD,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,1989,121927,0.00011553877157819388
"COMPUTER SIMULATION STUDIES OF THE MENSTRUAL CYCLE In-vivo studies of the menstrual cycle present enormous challenges, as they deal with a complex entity which is the result of multiple interactions between a large number of variables, not all of which can be controlled at one time in an in-vivo setting. Adjunct or alternative approaches to the in-vivo study of the human reproductive cycle have been suggested, among which is the computer simulation approach.  This research proposal represents an attempt to evaluate the validity of this approach. The investigators propose to resume their studies with a computer model of the cyclic morphological and endocrine changes which characterize the menstrual cycle.  The proposal contains three practical aims.  The first aim is to refine the original menstrual cycle computer simulation model. This model was developed more than a decade ago; new data and concepts must now be evaluated, tested and, where required, integrated into the model.  The second aim is to develop a sophisticated user interface.  The model will be adapted to PC desktop type computer hardware and will incorporate principles of what are called ""expert system"" programs.  This will allow physiologists to test and use the computer model in a highly sophisticated, yet easily controllable fashion without the need for an extensive computer background.  The third aim is to test the utility of this computer modeling approach in an experimental setting.  Selected experimentalists will provide us with an experimental setting.  Selected experimentalists will provide us with an objective analysis of the model through their ability to log simulation results.  Ultimately, a refined and validated model of the menstrual cycle can be used to assist in both experimental design and result interpretation and thereby enhance knowledge gained from in-vivo experimentation.  n/a",COMPUTER SIMULATION STUDIES OF THE MENSTRUAL CYCLE,3321660,R01HD022209,"['computer simulation', ' computer system design /evaluation', ' estradiol', ' gonadotropins', ' hormone regulation /control mechanism', ' hypothalamic pituitary axis', ' hypothalamus', ' mathematical model', ' menstrual cycle', ' model design /development', ' neuroendocrine system', ' neuropeptides', ' ovulation']",NICHD,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,1988,121068,0.00011553877157819388
"COMPUTER SIMULATION STUDIES OF THE MENSTRUAL CYCLE In-vivo studies of the menstrual cycle present enormous challenges, as they deal with a complex entity which is the result of multiple interactions between a large number of variables, not all of which can be controlled at one time in an in-vivo setting. Adjunct or alternative approaches to the in-vivo study of the human reproductive cycle have been suggested, among which is the computer simulation approach.  This research proposal represents an attempt to evaluate the validity of this approach. The investigators propose to resume their studies with a computer model of the cyclic morphological and endocrine changes which characterize the menstrual cycle.  The proposal contains three practical aims.  The first aim is to refine the original menstrual cycle computer simulation model. This model was developed more than a decade ago; new data and concepts must now be evaluated, tested and, where required, integrated into the model.  The second aim is to develop a sophisticated user interface.  The model will be adapted to PC desktop type computer hardware and will incorporate principles of what are called ""expert system"" programs.  This will allow physiologists to test and use the computer model in a highly sophisticated, yet easily controllable fashion without the need for an extensive computer background.  The third aim is to test the utility of this computer modeling approach in an experimental setting.  Selected experimentalists will provide us with an experimental setting.  Selected experimentalists will provide us with an objective analysis of the model through their ability to log simulation results.  Ultimately, a refined and validated model of the menstrual cycle can be used to assist in both experimental design and result interpretation and thereby enhance knowledge gained from in-vivo experimentation.  n/a",COMPUTER SIMULATION STUDIES OF THE MENSTRUAL CYCLE,3321657,R01HD022209,"['computer simulation', ' computer system design /evaluation', ' estradiol', ' gonadotropins', ' hormone regulation /control mechanism', ' hypothalamic pituitary axis', ' hypothalamus', ' mathematical model', ' menstrual cycle', ' model design /development', ' neuroendocrine system', ' ovulation']",NICHD,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,1987,87102,0.00011553877157819388
"TWO DIMENSIONAL PHOTON COUNTING POSTITION ENCODER A two-dimensional Photon Counting Position Encoder is proposed for Positron Emission Tomography (PET) medical cameras.  The new detector uses three new concepts:  1) a method of controlling the spatial distribution of photons from Bismuth Germanate Scintillators, 2) the use of all digital photon counting for processing the photomultiplier signal, and 3) the use of pattern recognition to encode position.  The new detector is expected to reduce the cost of this portion of the PET scanner by approximately a factor of eight.  The cost reduction anticipated for a typical four ring camera will be in the range of $600,000 to $1,000,000 which is 20-40 percent of the cost of the PET scanner.  In addition to the potential cost savings of this new approach, the photon division technique will allow one to obtain 2-3 mm position resolution in two dimensions.  The state-of-the art commercial scanners.  n/a",TWO DIMENSIONAL PHOTON COUNTING POSTITION ENCODER,3503987,R43NS022306,"['artificial intelligence', ' clinical biomedical equipment', ' image enhancement', ' photomultiplier', ' scintillation counter']",NINDS,"CTI, INC.",R43,1985,50000,0.01463606147843282
"PROCESSOR CONTROLLED HEARING AID The long term objective of the proposed program is the development, design and manufacture of a signal processor-controlled hearing aid to maintain the overall sound pressure level (SPL) and the spectrum at the eardrum within optimum limits for the hearing impaired individual for allexpected input spectra and input levels.  An additional feature will be an increase in signal-to-noise ratio of the aided signal.  The signal processing will be based on the actual eardrum SPL, determined from an acoustic feedback signal obtained by a microphone or probe in the ear canal.  For all expected input levels, the spectrum of the eardrum sound pressure is controlled by the processor, so that the peak SPL in any 1/3-octave interval does not exceed the individual's loudness discomfort level (LDL) while still maintaining the desired spectrum and SPL at its most effective value.  The processor compares the inputs and outputs for each filter and the gain in each channel is then adjusted (on a real time basis) to obtain the desired output for the band.  Determination of averaging times for both the input and output signals are significant and will constitute a major element in the study.  Phase I objective is to breadboard this system (without) the signal-to-noise ratio improvement) and test it on a manikin, equipped with a ANSI S3.25-1979 simulator.  n/a",PROCESSOR CONTROLLED HEARING AID,3504003,R43NS022735,"['artificial intelligence', ' auditory feedback', ' clinical biomedical equipment', ' hearing aids', ' loudness', ' noise', ' sound']",NINDS,SAM GILMAN ASSOCIATES,R43,1985,50000,-0.03651386503542294
"3D Image Analysis Approach to Determine Severity and Cause of Optic Nerve Edema DESCRIPTION (provided by applicant): Currently, the clinical assessment of optic nerve swelling is limited by the subjective ophthalmoscopic evaluation by experts in order to diagnose and differentiate the cause of the optic disc edema. The long-term goal of our research effort is to develop automated 3D image-analysis approaches for the identification of an optimal set of 3D parameters to quantify the severity of optic nerve edema over time and to help differentiate the underlying cause. The overall objective in this application is to develop strategies, using spectral-domain optical coherence tomography (SD-OCT), to rapidly and accurately determine the severity of optic nerve swelling in patients diagnosed with papilledema and to ascertain morphological features that differentiate papilledema from other disorders causing optic nerve edema. The central hypothesis is that information about volumetric and shape parameters obtainable from 3D image analysis techniques will improve the ability to accurately assess the severity and cause of optic disc edema over the existing subjective ophthalmoscopic assessment of optic nerve swelling using the Fris�n scale or current 2D OCT parameters. The rationale for the proposed research is that having such 3D parameters will dramatically improve the way optic disc swelling is assessed. The following specific aims will be pursued: 1. Develop and evaluate the methodology for computing novel volumetric and shape parameters of a swollen optic nerve head from SD-OCT. This will be completed by refining and evaluating our novel 3D graph-based segmentation algorithms in SD-OCT volumes of patients with optic disc swelling. 2. Identify SD-OCT parameters that optimally correlate with clinical measurements of severity in patients with papilledema and develop a continuous severity scale. This will be accomplished by using machine-learning approaches to relate SD-OCT parameters to expert-defined Fris�n scale grades (a fundus-based measure of severity). It is anticipated that volumetric 3D parameters will more closely correlate with clinical measures than 2D parameters and will provide a continuous severity scale. 3. Identify SD-OCT parameters that differentiate papilledema from other causes of optic disc swelling (or apparent optic disc swelling, as in pseudopapilledema) and develop a corresponding predictive classifier. Our working hypothesis is that 3D shape parameters, especially those near Bruch's membrane opening, will contribute the most in the automatic differentiation process. The approach is innovative because the 3D image-analysis methodology developed by the applicants enables novel determination of 3D volumetric and shape parameters and represents a significant improvement over the status quo of using qualitative image information and 2D OCT image information for assessing optic disc swelling. The proposed research is significant because it will help to establish a much-needed alternative and more objective method by which to assess the severity and cause of optic disc swelling. PUBLIC HEALTH RELEVANCE: The proposed research is relevant to public health because the identification of novel spectral-domain optical coherence tomography parameters for assessing the severity and cause of optic nerve edema is expected to enable more efficient and effective diagnosis and management strategies of patients with such swelling. Thus, the proposed research is relevant to the part of NIH's mission that pertains to developing fundamental knowledge to reduce the burdens of disability and is particularly relevant to the part of NEI's mission regarding understanding blinding eye diseases and preserving sight.",3D Image Analysis Approach to Determine Severity and Cause of Optic Nerve Edema,9264531,R01EY023279,"['Acute', 'Algorithmic Analysis', 'Algorithms', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Categories', 'Clinical', 'Clinical assessments', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Diagnosis', 'Diagnostic Procedure', 'Dimensions', 'Disease', 'Early Diagnosis', 'Edema', 'Evaluation', 'Eye diseases', 'Fundus', 'Goals', 'Graph', 'Image', 'Image Analysis', 'Imaging Techniques', 'Intracranial Hypertension', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modality', 'Monitor', 'Morphology', 'Nerve Fibers', 'Ophthalmoscopes', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Papilledema', 'Patients', 'Process', 'Public Health', 'Research', 'Resolution', 'Severities', 'Shapes', 'Structure', 'Swelling', 'Techniques', 'Testing', 'Thick', 'Three-Dimensional Image', 'Time', 'United States National Institutes of Health', 'Vision', 'Work', 'base', 'cost', 'digital', 'disability burden', 'expectation', 'improved', 'innovation', 'instrument', 'novel', 'optic nerve disorder', 'optical imaging', 'public health relevance']",NEI,UNIVERSITY OF IOWA,R01,2017,339750,0.007292076636627512
"3D Image Analysis Approach to Determine Severity and Cause of Optic Nerve Edema DESCRIPTION (provided by applicant): Currently, the clinical assessment of optic nerve swelling is limited by the subjective ophthalmoscopic evaluation by experts in order to diagnose and differentiate the cause of the optic disc edema. The long-term goal of our research effort is to develop automated 3D image-analysis approaches for the identification of an optimal set of 3D parameters to quantify the severity of optic nerve edema over time and to help differentiate the underlying cause. The overall objective in this application is to develop strategies, using spectral-domain optical coherence tomography (SD-OCT), to rapidly and accurately determine the severity of optic nerve swelling in patients diagnosed with papilledema and to ascertain morphological features that differentiate papilledema from other disorders causing optic nerve edema. The central hypothesis is that information about volumetric and shape parameters obtainable from 3D image analysis techniques will improve the ability to accurately assess the severity and cause of optic disc edema over the existing subjective ophthalmoscopic assessment of optic nerve swelling using the Fris¿n scale or current 2D OCT parameters. The rationale for the proposed research is that having such 3D parameters will dramatically improve the way optic disc swelling is assessed. The following specific aims will be pursued: 1. Develop and evaluate the methodology for computing novel volumetric and shape parameters of a swollen optic nerve head from SD-OCT. This will be completed by refining and evaluating our novel 3D graph-based segmentation algorithms in SD-OCT volumes of patients with optic disc swelling. 2. Identify SD-OCT parameters that optimally correlate with clinical measurements of severity in patients with papilledema and develop a continuous severity scale. This will be accomplished by using machine-learning approaches to relate SD-OCT parameters to expert-defined Fris¿n scale grades (a fundus-based measure of severity). It is anticipated that volumetric 3D parameters will more closely correlate with clinical measures than 2D parameters and will provide a continuous severity scale. 3. Identify SD-OCT parameters that differentiate papilledema from other causes of optic disc swelling (or apparent optic disc swelling, as in pseudopapilledema) and develop a corresponding predictive classifier. Our working hypothesis is that 3D shape parameters, especially those near Bruch's membrane opening, will contribute the most in the automatic differentiation process. The approach is innovative because the 3D image-analysis methodology developed by the applicants enables novel determination of 3D volumetric and shape parameters and represents a significant improvement over the status quo of using qualitative image information and 2D OCT image information for assessing optic disc swelling. The proposed research is significant because it will help to establish a much-needed alternative and more objective method by which to assess the severity and cause of optic disc swelling. PUBLIC HEALTH RELEVANCE: The proposed research is relevant to public health because the identification of novel spectral-domain optical coherence tomography parameters for assessing the severity and cause of optic nerve edema is expected to enable more efficient and effective diagnosis and management strategies of patients with such swelling. Thus, the proposed research is relevant to the part of NIH's mission that pertains to developing fundamental knowledge to reduce the burdens of disability and is particularly relevant to the part of NEI's mission regarding understanding blinding eye diseases and preserving sight.",3D Image Analysis Approach to Determine Severity and Cause of Optic Nerve Edema,9050682,R01EY023279,"['Acute', 'Algorithms', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Clinical', 'Clinical assessments', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Diagnosis', 'Diagnostic Procedure', 'Dimensions', 'Disease', 'Early Diagnosis', 'Edema', 'Evaluation', 'Eye diseases', 'Fundus', 'Goals', 'Graph', 'Health', 'Image', 'Imaging Techniques', 'Intracranial Hypertension', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modality', 'Monitor', 'Nerve Fibers', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Papilledema', 'Patients', 'Process', 'Public Health', 'Research', 'Resolution', 'Severities', 'Shapes', 'Staging', 'Structure', 'Swelling', 'Techniques', 'Testing', 'Thick', 'Three-Dimensional Image', 'Time', 'Vision', 'Work', 'base', 'cost', 'digital', 'disability burden', 'expectation', 'improved', 'innovation', 'instrument', 'novel', 'optic nerve disorder']",NEI,UNIVERSITY OF IOWA,R01,2016,339750,0.007292076636627512
"3D Image Analysis Approach to Determine Severity and Cause of Optic Nerve Edema DESCRIPTION (provided by applicant): Currently, the clinical assessment of optic nerve swelling is limited by the subjective ophthalmoscopic evaluation by experts in order to diagnose and differentiate the cause of the optic disc edema. The long-term goal of our research effort is to develop automated 3D image-analysis approaches for the identification of an optimal set of 3D parameters to quantify the severity of optic nerve edema over time and to help differentiate the underlying cause. The overall objective in this application is to develop strategies, using spectral-domain optical coherence tomography (SD-OCT), to rapidly and accurately determine the severity of optic nerve swelling in patients diagnosed with papilledema and to ascertain morphological features that differentiate papilledema from other disorders causing optic nerve edema. The central hypothesis is that information about volumetric and shape parameters obtainable from 3D image analysis techniques will improve the ability to accurately assess the severity and cause of optic disc edema over the existing subjective ophthalmoscopic assessment of optic nerve swelling using the Fris¿n scale or current 2D OCT parameters. The rationale for the proposed research is that having such 3D parameters will dramatically improve the way optic disc swelling is assessed. The following specific aims will be pursued: 1. Develop and evaluate the methodology for computing novel volumetric and shape parameters of a swollen optic nerve head from SD-OCT. This will be completed by refining and evaluating our novel 3D graph-based segmentation algorithms in SD-OCT volumes of patients with optic disc swelling. 2. Identify SD-OCT parameters that optimally correlate with clinical measurements of severity in patients with papilledema and develop a continuous severity scale. This will be accomplished by using machine-learning approaches to relate SD-OCT parameters to expert-defined Fris¿n scale grades (a fundus-based measure of severity). It is anticipated that volumetric 3D parameters will more closely correlate with clinical measures than 2D parameters and will provide a continuous severity scale. 3. Identify SD-OCT parameters that differentiate papilledema from other causes of optic disc swelling (or apparent optic disc swelling, as in pseudopapilledema) and develop a corresponding predictive classifier. Our working hypothesis is that 3D shape parameters, especially those near Bruch's membrane opening, will contribute the most in the automatic differentiation process. The approach is innovative because the 3D image-analysis methodology developed by the applicants enables novel determination of 3D volumetric and shape parameters and represents a significant improvement over the status quo of using qualitative image information and 2D OCT image information for assessing optic disc swelling. The proposed research is significant because it will help to establish a much-needed alternative and more objective method by which to assess the severity and cause of optic disc swelling. PUBLIC HEALTH RELEVANCE: The proposed research is relevant to public health because the identification of novel spectral-domain optical coherence tomography parameters for assessing the severity and cause of optic nerve edema is expected to enable more efficient and effective diagnosis and management strategies of patients with such swelling. Thus, the proposed research is relevant to the part of NIH's mission that pertains to developing fundamental knowledge to reduce the burdens of disability and is particularly relevant to the part of NEI's mission regarding understanding blinding eye diseases and preserving sight.",3D Image Analysis Approach to Determine Severity and Cause of Optic Nerve Edema,8842639,R01EY023279,"['Acute', 'Algorithms', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Clinical', 'Clinical assessments', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Diagnosis', 'Diagnostic Procedure', 'Dimensions', 'Disease', 'Early Diagnosis', 'Edema', 'Evaluation', 'Eye diseases', 'Fundus', 'Goals', 'Graph', 'Health', 'Image', 'Imaging Techniques', 'Intracranial Hypertension', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modality', 'Monitor', 'Nerve Fibers', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Papilledema', 'Patients', 'Process', 'Public Health', 'Relative (related person)', 'Research', 'Resolution', 'Severities', 'Shapes', 'Staging', 'Structure', 'Swelling', 'Techniques', 'Testing', 'Thick', 'Three-Dimensional Image', 'Time', 'Vision', 'Work', 'base', 'cost', 'digital', 'disability burden', 'expectation', 'improved', 'innovation', 'instrument', 'novel', 'optic nerve disorder']",NEI,UNIVERSITY OF IOWA,R01,2015,332955,0.007292076636627512
"3D Image Analysis Approach to Determine Severity and Cause of Optic Nerve Edema     DESCRIPTION (provided by applicant): Currently, the clinical assessment of optic nerve swelling is limited by the subjective ophthalmoscopic evaluation by experts in order to diagnose and differentiate the cause of the optic disc edema. The long-term goal of our research effort is to develop automated 3D image-analysis approaches for the identification of an optimal set of 3D parameters to quantify the severity of optic nerve edema over time and to help differentiate the underlying cause. The overall objective in this application is to develop strategies, using spectral-domain optical coherence tomography (SD-OCT), to rapidly and accurately determine the severity of optic nerve swelling in patients diagnosed with papilledema and to ascertain morphological features that differentiate papilledema from other disorders causing optic nerve edema. The central hypothesis is that information about volumetric and shape parameters obtainable from 3D image analysis techniques will improve the ability to accurately assess the severity and cause of optic disc edema over the existing subjective ophthalmoscopic assessment of optic nerve swelling using the Fris¿n scale or current 2D OCT parameters. The rationale for the proposed research is that having such 3D parameters will dramatically improve the way optic disc swelling is assessed. The following specific aims will be pursued: 1. Develop and evaluate the methodology for computing novel volumetric and shape parameters of a swollen optic nerve head from SD-OCT. This will be completed by refining and evaluating our novel 3D graph-based segmentation algorithms in SD-OCT volumes of patients with optic disc swelling. 2. Identify SD-OCT parameters that optimally correlate with clinical measurements of severity in patients with papilledema and develop a continuous severity scale. This will be accomplished by using machine-learning approaches to relate SD-OCT parameters to expert-defined Fris¿n scale grades (a fundus-based measure of severity). It is anticipated that volumetric 3D parameters will more closely correlate with clinical measures than 2D parameters and will provide a continuous severity scale. 3. Identify SD-OCT parameters that differentiate papilledema from other causes of optic disc swelling (or apparent optic disc swelling, as in pseudopapilledema) and develop a corresponding predictive classifier. Our working hypothesis is that 3D shape parameters, especially those near Bruch's membrane opening, will contribute the most in the automatic differentiation process. The approach is innovative because the 3D image-analysis methodology developed by the applicants enables novel determination of 3D volumetric and shape parameters and represents a significant improvement over the status quo of using qualitative image information and 2D OCT image information for assessing optic disc swelling. The proposed research is significant because it will help to establish a much-needed alternative and more objective method by which to assess the severity and cause of optic disc swelling.         PUBLIC HEALTH RELEVANCE: The proposed research is relevant to public health because the identification of novel spectral-domain optical coherence tomography parameters for assessing the severity and cause of optic nerve edema is expected to enable more efficient and effective diagnosis and management strategies of patients with such swelling. Thus, the proposed research is relevant to the part of NIH's mission that pertains to developing fundamental knowledge to reduce the burdens of disability and is particularly relevant to the part of NEI's mission regarding understanding blinding eye diseases and preserving sight.                ",3D Image Analysis Approach to Determine Severity and Cause of Optic Nerve Edema,8652462,R01EY023279,"['Acute', 'Algorithms', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Clinical', 'Clinical assessments', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Diagnosis', 'Diagnostic Procedure', 'Dimensions', 'Disease', 'Early Diagnosis', 'Edema', 'Evaluation', 'Eye diseases', 'Fundus', 'Goals', 'Graph', 'Image', 'Imaging Techniques', 'Intracranial Hypertension', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modality', 'Monitor', 'Nerve Fibers', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Papilledema', 'Patients', 'Process', 'Public Health', 'Relative (related person)', 'Research', 'Resolution', 'Severities', 'Shapes', 'Staging', 'Structure', 'Swelling', 'Techniques', 'Testing', 'Thick', 'Three-Dimensional Image', 'Time', 'Vision', 'Work', 'base', 'cost', 'digital', 'disability burden', 'expectation', 'improved', 'innovation', 'instrument', 'novel', 'optic nerve disorder', 'public health relevance']",NEI,UNIVERSITY OF IOWA,R01,2014,332955,0.007292076636627512
"3D Image Analysis Approach to Determine Severity and Cause of Optic Nerve Edema     DESCRIPTION (provided by applicant): Currently, the clinical assessment of optic nerve swelling is limited by the subjective ophthalmoscopic evaluation by experts in order to diagnose and differentiate the cause of the optic disc edema. The long-term goal of our research effort is to develop automated 3D image-analysis approaches for the identification of an optimal set of 3D parameters to quantify the severity of optic nerve edema over time and to help differentiate the underlying cause. The overall objective in this application is to develop strategies, using spectral-domain optical coherence tomography (SD-OCT), to rapidly and accurately determine the severity of optic nerve swelling in patients diagnosed with papilledema and to ascertain morphological features that differentiate papilledema from other disorders causing optic nerve edema. The central hypothesis is that information about volumetric and shape parameters obtainable from 3D image analysis techniques will improve the ability to accurately assess the severity and cause of optic disc edema over the existing subjective ophthalmoscopic assessment of optic nerve swelling using the Fris¿n scale or current 2D OCT parameters. The rationale for the proposed research is that having such 3D parameters will dramatically improve the way optic disc swelling is assessed. The following specific aims will be pursued: 1. Develop and evaluate the methodology for computing novel volumetric and shape parameters of a swollen optic nerve head from SD-OCT. This will be completed by refining and evaluating our novel 3D graph-based segmentation algorithms in SD-OCT volumes of patients with optic disc swelling. 2. Identify SD-OCT parameters that optimally correlate with clinical measurements of severity in patients with papilledema and develop a continuous severity scale. This will be accomplished by using machine-learning approaches to relate SD-OCT parameters to expert-defined Fris¿n scale grades (a fundus-based measure of severity). It is anticipated that volumetric 3D parameters will more closely correlate with clinical measures than 2D parameters and will provide a continuous severity scale. 3. Identify SD-OCT parameters that differentiate papilledema from other causes of optic disc swelling (or apparent optic disc swelling, as in pseudopapilledema) and develop a corresponding predictive classifier. Our working hypothesis is that 3D shape parameters, especially those near Bruch's membrane opening, will contribute the most in the automatic differentiation process. The approach is innovative because the 3D image-analysis methodology developed by the applicants enables novel determination of 3D volumetric and shape parameters and represents a significant improvement over the status quo of using qualitative image information and 2D OCT image information for assessing optic disc swelling. The proposed research is significant because it will help to establish a much-needed alternative and more objective method by which to assess the severity and cause of optic disc swelling.         PUBLIC HEALTH RELEVANCE: The proposed research is relevant to public health because the identification of novel spectral-domain optical coherence tomography parameters for assessing the severity and cause of optic nerve edema is expected to enable more efficient and effective diagnosis and management strategies of patients with such swelling. Thus, the proposed research is relevant to the part of NIH's mission that pertains to developing fundamental knowledge to reduce the burdens of disability and is particularly relevant to the part of NEI's mission regarding understanding blinding eye diseases and preserving sight.                ",3D Image Analysis Approach to Determine Severity and Cause of Optic Nerve Edema,8477880,R01EY023279,"['Acute', 'Algorithms', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Clinical', 'Clinical assessments', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Diagnosis', 'Diagnostic Procedure', 'Dimensions', 'Disease', 'Early Diagnosis', 'Edema', 'Evaluation', 'Eye diseases', 'Fundus', 'Goals', 'Graph', 'Image', 'Imaging Techniques', 'Intracranial Hypertension', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modality', 'Monitor', 'Nerve Fibers', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Papilledema', 'Patients', 'Process', 'Public Health', 'Relative (related person)', 'Research', 'Resolution', 'Severities', 'Shapes', 'Staging', 'Structure', 'Swelling', 'Techniques', 'Testing', 'Thick', 'Three-Dimensional Image', 'Time', 'Vision', 'Work', 'base', 'cost', 'digital', 'disability burden', 'expectation', 'improved', 'innovation', 'instrument', 'novel', 'optic nerve disorder', 'public health relevance']",NEI,UNIVERSITY OF IOWA,R01,2013,339750,0.007292076636627512
"SVM based group data analysis for drug abuse disorder ASL perfusion study    DESCRIPTION (provided by applicant): Arterial spin labeling (ASL) perfusion MRI provides a potentially extremely useful tool for drug abuse/addiction studies due to the noninvasive cerebral blood flow (CBF) measurement and immunity to low frequency MR signal drift effects that degrade functional MRI based on BOLD contrast over time. However, data processing through conventional univariate general linear model (GLM) based methods has proved extremely challenging for ASL data due to its intrinsic low SNR, the increased motion and agitation typically present during drug craving states, and the ""patchy flow"" defects commonly found in the brains of patients with certain types of chronic substance abuse (e.g., cocaine). Assuming a linear brain response to the functional stimuli and ignoring the abundant spatial brain activity coupling, the standard GLM is initially sub-optimal for fMRI data analysis. Modeling each voxel's time series with a canonical hemodynamic response function (HRF), it may be further ill-posed to drug abuse studies since the actual shape of HRF may differ significantly in the addicted patient's brain from the canonical one and may differ significantly from patient to patient, from voxel to voxel. A more powerful data analysis method is fundamentally demanded for drug abuse/addiction ASL perfusion studies. The goal of this project is to develop a multivariate and brain response modeling-free fMRI data processing method and to use it for revealing the drug craving related brain activation patterns within existing drug abuse/addiction ASL perfusion fMRI data in our center. A machine-learning algorithm, the support vector machine (SVM), will be used to extract a spatial discriminance map between different experimental conditions for each subject, and a statistic framework will be provided to give a population inference about the extracted discriminance (Aim 1). We hypothesize that this machine-learning based data processing will increase the detection sensitivity of ASL perfusion fMRI as compared to the conventional GLM approach because of the data driven nature and multivariate processing of SVM. To verify this hypothesis and to validate the sensitivity, specificity and reliability of the proposed methods, we propose to acquire 40 normal controls' null-hypothesis ASL perfusion fMRI data and sensory-motor task data in Aim 2. The last but the most important aim (Aim 3) is to apply the proposed method to analyze the existing drug abuse/addiction ASL perfusion fMRI data in our center. In addition to the basic science implications for ASL perfusion fMRI and BOLD fMRI, we believe this line of work will provide critical information, e.g., more precise diagnostic measurement, and prediction of treatment or medication response for the current cocaine and nicotine addiction/relapse vulnerability studies. The output of these more powerful ASL data analysis methods will also pave the way for general application of ASL perfusion fMRI methods across multiple brain disorders, and for sustained states within the normal brain.          n/a",SVM based group data analysis for drug abuse disorder ASL perfusion study,7385333,R03DA023496,"['Agitation', 'Algorithms', 'Arterial Disorder', 'Basic Science', 'Brain', 'Brain Diseases', 'Cerebrovascular Circulation', 'Chronic', 'Cigarette', 'Classification', 'Clinical', 'Cocaine', 'Cocaine Dependence', 'Cognitive', 'Condition', 'Coupling', 'Cues', 'Data', 'Data Analyses', 'Defect', 'Detection', 'Development', 'Diagnostic', 'Disease', 'Drug Addiction', 'Drug abuse', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Immune', 'Immunity', 'Individual', 'Linear Models', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Motion', 'Motor', 'Nature', 'Neurobiology', 'Nicotine', 'Nicotine Dependence', 'Output', 'Patients', 'Pattern', 'Performance', 'Perfusion', 'Pharmaceutical Preparations', 'Population', 'Population Statistics', 'Positioning Attribute', 'Process', 'Relapse', 'Research', 'Research Personnel', 'Sensitivity and Specificity', 'Sensory', 'Series', 'Shapes', 'Signal Transduction', 'Speed', 'Spin Labels', 'Standards of Weights and Measures', 'Stimulus', 'Substance abuse problem', 'Testing', 'Time', 'Validation', 'Work', 'addiction', 'base', 'blood flow measurement', 'computerized data processing', 'craving', 'drug craving', 'experience', 'hemodynamics', 'multidisciplinary', 'neuroimaging', 'nicotine craving', 'response', 'size', 'statistics', 'tool']",NIDA,UNIVERSITY OF PENNSYLVANIA,R03,2007,236250,0.02228088453356027
"Human and Machine Learning for Customized Control of Assistive Robots PROJECT SUMMARY This application will result in a technological platform that re-empowers persons with severe paralysis, by allowing them to independently control a wide spectrum of robotic actions. Severe paralysis is devastating, and chronic—and reliance on caregivers is persistent. Assistive machines such as wheelchairs and robotic arms offer a groundbreaking path to independence: where control over their environment and interactions is returned to the person.  However, to operate complex machines like robotic arms and hands typically poses a difﬁcult learning challenge and requires complex control signals—and the commercial control interfaces accessible to persons with severe paralysis (e.g. sip-and-puff, switch-based head arrays) are not adequate. As a result, assistive robotic arms remain largely inaccessible to those with severe paralysis—arguably the population who would beneﬁt from them most.  The purpose of the proposed study is to provide people with tetraplegia with the means to control robotic arms with their available body mobility, while concurrently promoting the exercise of available body motions and the maintenance of physical health. Control interfaces that generate a unique map from a user's body motions to control signals for a machine offer a customized interaction, however these interfaces have only been used to issue low-dimensional (2-D) control signals whereas more complex machines require higher-dimensional (e.g. 6-D) signals. We propose an approach that leverages robotics autonomy and machine learning in order to aid the end-user in learning how to issue effective higher-dimensional control signals through body motions. Speciﬁcally, initially the human issues a lower- dimensional control signal and robotics autonomy is used to bridge the gap by taking over whatever is not covered by the human's control signal. Help from the robotics autonomy is then progressively scaled back, automatically, to cover fewer and fewer control dimensions as the user becomes more skilled.  The ﬁrst piece to our approach deals with how to extract control signals from the human, using the body-machine interface. The development and optimization of decoding procedures for controlling a robotic arm using residual body motions will be addressed under Speciﬁc Aim 1. The second piece to our approach deals with how to interpret control signals from a human within a paradigm that shares control between the human and robotics autonomy. To identify which shared-control formulations most effectively utilize the human's control signals will be the topic of Speciﬁc Aim 2. The ﬁnal piece to our approach deals with how to adapt the shared-control paradigm so that more control is transferred to the human over time. This adaptation is necessary for the human's learning process, since the goal in the end is for the human to be able to fully control the robotic arm him/herself, and will be assessed under Speciﬁc Aim 3.  At the completion of this project, tetraplegic end-users will be able to operate a robotic arm using their residual body motions, through an interface that both promotes the use of residual body motions (and thus also recovery of motor skill) and adapts with the human as their abilities change over time. By leveraging adaptive robotics autonomy, our application moreover provides a safe mechanism to facilitate learning how to operate the robotic platform. Frequently the more severe a person's motor impairment, the less able they are to operate the very assistive machines, like powered wheelchairs and robotic arms, which might enhance their quality of life. The purpose of the proposed study is to provide people with tetraplegia with the means, through non-invasive technologies, to control robotic arms with their available body mobility, while concurrently promoting the exercise of available body motions and the maintenance of physical health. We propose and study an approach that leverages robot machine learning and autonomy in order to facilitate human motor learning of how to operate a robotic arm using a customized interface, in order to make powered assisted manipulation more accessible to people with severe paralysis. 1",Human and Machine Learning for Customized Control of Assistive Robots,9773039,R01EB024058,"['3-Dimensional', 'Activities of Daily Living', 'Address', 'Affect', 'Algorithms', 'Back', 'Caregivers', 'Cerebral Palsy', 'Chronic', 'Complex', 'Computers', 'Custom', 'Data', 'Development', 'Devices', 'Dimensions', 'Eating', 'Effectiveness', 'Environment', 'Exercise', 'Formulation', 'Fostering', 'Freedom', 'Future', 'Goals', 'Hand', 'Head', 'Health Benefit', 'Human', 'Learning', 'Left', 'Limb structure', 'Machine Learning', 'Maintenance', 'Maps', 'Mental Depression', 'Mental Health', 'Motion', 'Motivation', 'Motor', 'Motor Skills', 'Movement', 'Multiple Sclerosis', 'Neurologic', 'Paralysed', 'Patients', 'Performance', 'Persons', 'Population', 'Posture', 'Powered wheelchair', 'Procedures', 'Process', 'Quadriplegia', 'Quality of life', 'Rehabilitation therapy', 'Residual state', 'Robot', 'Robotics', 'Self-Help Devices', 'Shoulder', 'Side', 'Signal Transduction', 'Specific qualifier value', 'Spinal cord injured survivor', 'Stroke', 'System', 'Technology', 'Testing', 'Time', 'Wheelchairs', 'Work', 'arm', 'assistive robot', 'base', 'body-machine interface', 'brain machine interface', 'empowerment', 'feeding', 'high dimensionality', 'improved', 'machine learning algorithm', 'motor impairment', 'motor learning', 'motor recovery', 'n-dimensional', 'novel strategies', 'physical conditioning', 'psychologic', 'robot control', 'sensor', 'two-dimensional']",NIBIB,REHABILITATION INSTITUTE OF CHICAGO D/B/A SHIRLEY RYAN ABILITYLAB,R01,2019,329494,-0.02789300641514541
"Human and Machine Learning for Customized Control of Assistive Robots PROJECT SUMMARY This application will result in a technological platform that re-empowers persons with severe paralysis, by allowing them to independently control a wide spectrum of robotic actions. Severe paralysis is devastating, and chronic—and reliance on caregivers is persistent. Assistive machines such as wheelchairs and robotic arms offer a groundbreaking path to independence: where control over their environment and interactions is returned to the person.  However, to operate complex machines like robotic arms and hands typically poses a difﬁcult learning challenge and requires complex control signals—and the commercial control interfaces accessible to persons with severe paralysis (e.g. sip-and-puff, switch-based head arrays) are not adequate. As a result, assistive robotic arms remain largely inaccessible to those with severe paralysis—arguably the population who would beneﬁt from them most.  The purpose of the proposed study is to provide people with tetraplegia with the means to control robotic arms with their available body mobility, while concurrently promoting the exercise of available body motions and the maintenance of physical health. Control interfaces that generate a unique map from a user's body motions to control signals for a machine offer a customized interaction, however these interfaces have only been used to issue low-dimensional (2-D) control signals whereas more complex machines require higher-dimensional (e.g. 6-D) signals. We propose an approach that leverages robotics autonomy and machine learning in order to aid the end-user in learning how to issue effective higher-dimensional control signals through body motions. Speciﬁcally, initially the human issues a lower- dimensional control signal and robotics autonomy is used to bridge the gap by taking over whatever is not covered by the human's control signal. Help from the robotics autonomy is then progressively scaled back, automatically, to cover fewer and fewer control dimensions as the user becomes more skilled.  The ﬁrst piece to our approach deals with how to extract control signals from the human, using the body-machine interface. The development and optimization of decoding procedures for controlling a robotic arm using residual body motions will be addressed under Speciﬁc Aim 1. The second piece to our approach deals with how to interpret control signals from a human within a paradigm that shares control between the human and robotics autonomy. To identify which shared-control formulations most effectively utilize the human's control signals will be the topic of Speciﬁc Aim 2. The ﬁnal piece to our approach deals with how to adapt the shared-control paradigm so that more control is transferred to the human over time. This adaptation is necessary for the human's learning process, since the goal in the end is for the human to be able to fully control the robotic arm him/herself, and will be assessed under Speciﬁc Aim 3.  At the completion of this project, tetraplegic end-users will be able to operate a robotic arm using their residual body motions, through an interface that both promotes the use of residual body motions (and thus also recovery of motor skill) and adapts with the human as their abilities change over time. By leveraging adaptive robotics autonomy, our application moreover provides a safe mechanism to facilitate learning how to operate the robotic platform. Frequently the more severe a person's motor impairment, the less able they are to operate the very assistive machines, like powered wheelchairs and robotic arms, which might enhance their quality of life. The purpose of the proposed study is to provide people with tetraplegia with the means, through non-invasive technologies, to control robotic arms with their available body mobility, while concurrently promoting the exercise of available body motions and the maintenance of physical health. We propose and study an approach that leverages robot machine learning and autonomy in order to facilitate human motor learning of how to operate a robotic arm using a customized interface, in order to make powered assisted manipulation more accessible to people with severe paralysis. 1",Human and Machine Learning for Customized Control of Assistive Robots,9448794,R01EB024058,"['3-Dimensional', 'Activities of Daily Living', 'Address', 'Affect', 'Algorithms', 'Back', 'Caregivers', 'Cerebral Palsy', 'Chronic', 'Complex', 'Computers', 'Custom', 'Data', 'Development', 'Devices', 'Dimensions', 'Eating', 'Effectiveness', 'Environment', 'Exercise', 'Formulation', 'Fostering', 'Freedom', 'Future', 'Goals', 'Hand', 'Head', 'Health Benefit', 'Human', 'Learning', 'Left', 'Limb structure', 'Machine Learning', 'Maintenance', 'Maps', 'Mental Depression', 'Mental Health', 'Motion', 'Motivation', 'Motor', 'Motor Skills', 'Movement', 'Multiple Sclerosis', 'Neurologic', 'Paralysed', 'Patients', 'Performance', 'Persons', 'Population', 'Posture', 'Powered wheelchair', 'Procedures', 'Process', 'Quadriplegia', 'Quality of life', 'Rehabilitation therapy', 'Residual state', 'Robot', 'Robotics', 'Self-Help Devices', 'Shoulder', 'Side', 'Signal Transduction', 'Specific qualifier value', 'Spinal cord injured survivor', 'Stroke', 'System', 'Technology', 'Testing', 'Time', 'Wheelchairs', 'Work', 'arm', 'base', 'body-machine interface', 'brain machine interface', 'empowerment', 'feeding', 'high dimensionality', 'improved', 'motor impairment', 'motor learning', 'motor recovery', 'n-dimensional', 'novel strategies', 'physical conditioning', 'psychologic', 'robot control', 'sensor', 'two-dimensional']",NIBIB,REHABILITATION INSTITUTE OF CHICAGO D/B/A SHIRLEY RYAN ABILITYLAB,R01,2018,331851,-0.02789300641514541
"Fast and Robust Low-Dose X-Ray CT Image Reconstruction Abstract:  The use of CT scans has recently increased, for example, in virtual colonoscopy, CT cardiac screening, screening of the lung in smokers, whole-body CT in asymptomatic patients, and CT imaging of children. Shortening of the scanning time to around 1 second, eliminating the strict need for the subject to remain still or be sedated, is one of the main reasons for the large increase in the pediatric population. CT scans of children have been estimated to produce non-negligible increases in the probability of lifetime cancer mortality, leading to calls for the use of reduced current settings for CT scans of children. For these reasons, the CT industry has put in a lot of effort to develop low-dose CT. One active area of research is methods to reduce the radiation counts by applying adaptive collimation to block unnecessary x-ray photons. Another active area of research is the development of more robust image reconstruction algorithms that are less sensitive to noise for low-count data.  This grant proposal is focused on the second approach — development of fast and robust reconstruction algorithms. It is known that some iterative image reconstruction algorithms outperform the analytical filtered backprojection (FBP) algorithm in terms of producing less-noisy images with the same data set. One disadvantage of these iterative algorithms is their long computation time, making them impractical in a real- world CT reconstruction tasks. For this reason, the FBP algorithm is still the main work horse for CT applications.  The main goal of the proposed research is to develop fast and robust iterative-algorithms so that their computation time is at the same order of an analytic FBP algorithm, using experimental low-dose phantom, cadaver data, and low-dose cancer screen chest CT patient data to perform comparison studies. We will answer the question: When the fast and robust algorithms are used, how much can the CT dose be reduced while retaining the image quality of a standard-dose CT produced by the conventional FBP?  This R15 project provides Weber State University (WSU) computer engineering and computer science students with hands-on opportunities and experiences of performing real-world research in the field of healthcare. It will stimulate the interests of students so that they consider a career in biomedical and bioengineering field/industry. Narrative:  CT dose is currently a major concern for the general public. Low-dose CT is under development. The proposed research will focus on developing fast, robust and practical image reconstruction methods that are able to produce a standard CT image using a lower CT dose.",Fast and Robust Low-Dose X-Ray CT Image Reconstruction,9440734,R15EB024283,"['Algorithms', 'Applications Grants', 'Area', 'Biomedical Engineering', 'Cadaver', 'Cancer Patient', 'Cardiac', 'Categories', 'Characteristics', 'Child', 'Childhood', 'Clinical', 'Collimator', 'Computed Tomographic Colonography', 'Computer Hardware', 'Computer Simulation', 'Computers', 'Contracts', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic', 'Dimensions', 'Disadvantaged', 'Disease', 'Dose', 'Engineering', 'Environment', 'Equilibrium', 'Equus caballus', 'Evaluation', 'General Population', 'Goals', 'Healthcare', 'Human', 'Image', 'Industry', 'Inferior', 'Internships', 'Investigation', 'Learning', 'Lesion', 'Liver Cirrhosis', 'Lung', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Morphologic artifacts', 'Noise', 'Patients', 'Performance', 'Photons', 'Population', 'Preparation', 'Probability', 'Radiation', 'Radiation exposure', 'Research', 'Resolution', 'Roentgen Rays', 'Running', 'Scanning', 'Screening for cancer', 'Smoker', 'Structure', 'Students', 'Supervision', 'Time', 'Training', 'Universities', 'Utah', 'Work', 'X-Ray Computed Tomography', 'accurate diagnosis', 'base', 'career', 'chest computed tomography', 'clinical application', 'computer science', 'data acquisition', 'design', 'digital imaging', 'experience', 'high risk', 'image reconstruction', 'improved', 'interest', 'low-dose spiral CT', 'lung cancer screening', 'mortality', 'parallel computer', 'professor', 'radiologist', 'reconstruction', 'screening', 'tomography', 'university student']",NIBIB,WEBER STATE UNIVERSITY,R15,2018,88321,-0.04442271109361379
"Novel Algorithms for Reducing Radiation Dose of CT Perfusion Project Summary/Abstract X-ray computed tomography (CT) has been increasingly used in medical diagnosis, currently reaching more than 100 million CT scans every year in the US. The increasing use of CT has sparked concern over the effects of radiation dose on patients. It is estimated that every 2000 CT scans will cause one future cancer, i.e., 50,000 cases of future cancers from 100 million CT scans every year. CT brain perfusion (CTP) is a widely used imaging technique for the evaluation of hemodynamic changes in stroke and cerebrovascular disorders. However, CTP involves high radiation dose for patients as the CTP scan is repeated on the order of 40 times at the same anatomical location, in order to capture the full passage of the contrast bolus. Several techniques have been applied for radiation dose reduction in CTP scans, including reduction of tube current and tube voltage, as well as the use of noise reduction techniques such as iterative reconstruction (IR). However, the resultant radiation dose of existing CTP scans is still significantly higher than that of a standard head CT scan. The application of IR techniques in CTP is very limited due to the high complexity and computational burden for processing multiple CTP images that impairs clinical workflow. During the Phase 1 STTR project, we introduced a novel low dose CTP imaging method based on the k-space weighted image contrast (KWIC) reconstruction algorithm. We performed thorough evaluation in both a CTP phantom and clinical CTP datasets, and demonstrated that the KWIC algorithm is able to reduce the radiation dose of existing CTP techniques by 75% without affecting the image quality and accuracy of quantification (i.e., Milestone of Phase 1 STTR). However, the original KWIC algorithm requires rapid-switching pulsed X-ray at pre-specified rotation angles – a hardware capability yet to be implemented by commercial CT vendors. In order to address this limitation, we recently introduced a variant of the KWIC algorithm termed k-space weighted image average (KWIA) that preserves high spatial and temporal resolutions as well as image quality of low dose CTP data (~75% dose reduction) to be comparable to those of standard CTP scans. Most importantly, KWIA does not require modification of existing CT hardware and is computationally simple and fast, therefore has a low barrier for market penetration. The purpose of the Phase 2 STTR project is to further optimize and validate the KWIA algorithm for reducing radiation dose of CTP scans by ~75% while preserving the image quality and quantification accuracy in CTP phantom, clinical CTP data and animal studies. We will further develop innovative deep-learning (DL) based algorithms to address potential motion and other artifacts in KWIA, and commercialize the developed algorithms by collaborating with CT vendors. Relevance to Public Health More than 100 million CT scans are performed every year in the US, estimated to cause 50,000 cases of future cancers. This project will develop, evaluate and commercialize novel CT imaging technologies that reduce the radiation dose of existing CT perfusion techniques by ~75% without compromising imaging speed or quality.",Novel Algorithms for Reducing Radiation Dose of CT Perfusion,10006737,R44EB024438,"['Address', 'Adoption', 'Affect', 'Algorithms', 'American Heart Association', 'Anatomy', 'Angiography', 'Animals', 'Bolus Infusion', 'Brain', 'Brain Neoplasms', 'Cerebrovascular Disorders', 'Clinical', 'Collaborations', 'Data', 'Data Set', 'Decision Making', 'Development', 'Diagnosis', 'Dose', 'Evaluation', 'Future', 'Goals', 'Guidelines', 'Head', 'Heart', 'Image', 'Imaging Techniques', 'Imaging technology', 'Impairment', 'Infarction', 'Location', 'Low Dose Radiation', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modification', 'Monitor', 'Morphologic artifacts', 'Motion', 'Noise', 'Organ', 'Patients', 'Pattern', 'Penetration', 'Perfusion', 'Phase', 'Physiologic pulse', 'Public Health', 'Radiation Dose Unit', 'Reperfusion Therapy', 'Roentgen Rays', 'Rotation', 'Scanning', 'Signal Transduction', 'Small Business Technology Transfer Research', 'Specific qualifier value', 'Speed', 'Stroke', 'Techniques', 'Technology', 'Time', 'Traumatic Brain Injury', 'Tube', 'Variant', 'Vendor', 'X-Ray Computed Tomography', 'acute stroke', 'base', 'brain tissue', 'contrast imaging', 'deep learning', 'denoising', 'hemodynamics', 'imaging modality', 'innovation', 'low dose computed tomography', 'novel', 'perfusion imaging', 'preservation', 'radiation effect', 'reconstruction', 'temporal measurement', 'voltage']",NIBIB,"HURA IMAGING, INC",R44,2020,820709,-0.008016182843781097
"Hamamatsu Nanozoomer S60 Digital Whole Slide Scanner Project Summary/Abstract This application is for a shared instrumentation grant from the Light Microscope Imaging Facility at Case Western Reserve University (CWRU) School of Medicine (SOM) to acquire a fully automated, high-capacity, high- resolution Hamamatsu Nanozoomer S60 slide scanner that can accommodate both brightfield and fluorescence imaging in single-slide and double-slide formats. We also request MicroDimensions 3D reconstruction and alignment software packages for manipulating and analyzing the whole slide images produced with the scanner. We need to replace a scanner that is not functioning properly. Our well-established shared core facility supports NIH-funded investigators by giving them access to state-of-the-art microscopy technologies that enhance collaborative, multidisciplinary research. Acquisition of this instrument will have a high impact on the biomedical research at CWRU and expand the scope of our NIH-funded projects. Several projects have been identified that will utilize the scanner and its associated analyses programs. These include: the genetic mechanisms underlying skin fibrosis and cranial bone development (Atit); the mechanisms behind the lifelong functions of transcription factors in axonal growth and architecture (Deneris); deep-learning for histologic image predictors of various diseases (Madabhushi); the development of diagnostic probes to discriminate between glioma subtypes for screening and survival therapies (Brady-Kalnay); the role of progesterone receptors in the control of parturition and the development of therapies to prevent preterm birth (Messiano); the mechanisms by which breast cancer stem cells overcome metastatic latency leading to disease recurrence and the biomarkers that could potentially identify those tumors likely to undergo this process (Schiemann); and the significance of cholesterol-related proteins in brain and retinal function (Pikuleva). Many additional projects of minor users and others at CWRU are anticipated. All of the proposed projects are in need of a high-capacity automated scanner acquiring whole slide images so that analyses can be applied to tissues that cover hundreds of fields of view, rather than the single regions of interest that can be acquired on a standard microscope. Narrative This proposal seeks to acquire a new, state-of-the-art high-speed, high content digital slide scanner for high resolution cellular and biomarker identfcation across large tissue areas. The dual mode (fluorescence and brightfield) allows flexibility in marker visualization while the software allows the automated alignment of whole slide images for multi-stain analysis and 3d reconstruction of serial section for in-depth analysis of specimens. This equipment is essential for our NIH disease-related studies providing a profound positive impact on a wide range of public health areas.",Hamamatsu Nanozoomer S60 Digital Whole Slide Scanner,9489976,S10OD024981,"['Architecture', 'Biological Markers', 'Biomedical Research', 'Birth', 'Bone Development', 'Brain', 'Cephalic', 'Cholesterol', 'Computer software', 'Core Facility', 'Development', 'Diagnostic', 'Disease', 'Enhancement Technology', 'Funding', 'Genetic', 'Glioma', 'Grant', 'Interdisciplinary Study', 'Light Microscope', 'Microscope', 'Microscopy', 'Minor', 'Premature Birth', 'Process', 'Progesterone Receptors', 'Proteins', 'Recurrence', 'Research Personnel', 'Resolution', 'Retinal', 'Role', 'Slide', 'Tissues', 'United States National Institutes of Health', 'Universities', 'axon growth', 'cancer stem cell', 'deep learning', 'digital', 'equipment acquisition', 'fluorescence imaging', 'histological image', 'imaging facilities', 'instrumentation', 'interest', 'malignant breast neoplasm', 'medical schools', 'prevent', 'programs', 'reconstruction', 'screening', 'skin fibrosis', 'therapy development', 'transcription factor', 'tumor', 'whole slide imaging']",OD,CASE WESTERN RESERVE UNIVERSITY,S10,2018,303390,-0.09529806680188739
"STRUCTURAL DETERMINATION OF INTERPHASE CHROMOSOMES The newly developed instrumentation and software now allows three-dimensional reconstruction, in a generalized fashion, of subcellular structures, at both the light and high voltage electron microscopic levels.  Eighteen nuclei have been reconstructed and interpreted in a quantitative fashion.  Six nuclei have their cytogenetic loci aligned with the three-dimensional structures showing that many chromosome features are three-dimensionally determined.  It is proposed to determine, three-dimensionally, many adjacent polytene nuclei in defined developmental stages and tissues so that it will be possible to see in a quantitative fashion the structural relationships. Many computational approaches are proposed to process the images, to refine the structures, to position the bands, hence genes three-dimensionally, and to determine the fine structure of nuclei.  These studies will also utilize monoclonal antibodies to localize active gene loci and their close associations one with another and the nuclear envelope, and to relate structure to function.  Diploid nuclei will, in addition, be three-dimensionally reconstructed at highest possible light microscope resolution in intact Drosophila embryos. Questions related to differences in the 3D patterns of the interphase chromosomes as a function of development, and the cell cycle will be studied.  The laboratory will continue to reconstruct mitotic chromosomes and interband/bands from polytene chromosomes from Drosophila in a resolution-overlaping fashion made possible by the three-dimensional light microscopy and much higher resolution of the high voltage electron microscope utilizing systematic high tilt about 70 degrees for three-dimensional structure.  New computational approaches to image processing in three dimensions are being continued.  New image collection hardware for the light microscopy and high voltage electron microscopy are proposed.  n/a",STRUCTURAL DETERMINATION OF INTERPHASE CHROMOSOMES,3272768,R01GM025101,"['Drosophilidae', ' animal population genetics', ' artificial intelligence', ' cell differentiation', ' chromosome disorders', ' cytogenetics', ' electron microscopy', ' endonuclease', ' eukaryote', ' fluorescence microscopy', ' fluorescent dye /probe', ' freeze etching', ' gene expression', ' genetic manipulation', ' genetic mapping', ' genetic regulation', ' genetic transcription', ' image enhancement', ' immature animal', ' immunofluorescence technique', ' larva', ' monoclonal antibody', ' nonmammalian vertebrate embryology', ' nucleic acid sequence', ' radionuclide double label', ' salivary glands', ' scanning electron microscopy', ' tissue /cell culture']",NIGMS,UNIVERSITY OF CALIFORNIA SAN FRANCISCO,R01,1988,98271,-0.006079232181945182
"STRUCTURAL DETERMINATION OF INTERPHASE CHROMOSOMES The newly developed instrumentation and software now allows three-dimensional reconstruction, in a generalized fashion, of subcellular structures, at both the light and high voltage electron microscopic levels.  Eighteen nuclei have been reconstructed and interpreted in a quantitative fashion.  Six nuclei have their cytogenetic loci aligned with the three-dimensional structures showing that many chromosome features are three-dimensionally determined.  It is proposed to determine, three-dimensionally, many adjacent polytene nuclei in defined developmental stages and tissues so that it will be possible to see in a quantitative fashion the structural relationships. Many computational approaches are proposed to process the images, to refine the structures, to position the bands, hence genes three-dimensionally, and to determine the fine structure of nuclei.  These studies will also utilize monoclonal antibodies to localize active gene loci and their close associations one with another and the nuclear envelope, and to relate structure to function.  Diploid nuclei will, in addition, be three-dimensionally reconstructed at highest possible light microscope resolution in intact Drosophila embryos. Questions related to differences in the 3D patterns of the interphase chromosomes as a function of development, and the cell cycle will be studied.  The laboratory will continue to reconstruct mitotic chromosomes and interband/bands from polytene chromosomes from Drosophila in a resolution-overlaping fashion made possible by the three-dimensional light microscopy and much higher resolution of the high voltage electron microscope utilizing systematic high tilt about 70 degrees for three-dimensional structure.  New computational approaches to image processing in three dimensions are being continued.  New image collection hardware for the light microscopy and high voltage electron microscopy are proposed.  n/a",STRUCTURAL DETERMINATION OF INTERPHASE CHROMOSOMES,3272767,R01GM025101,"['animal population genetics', ' artificial intelligence', ' cell differentiation', ' chromosome disorders', ' cytogenetics', ' electron microscopy', ' endonuclease', ' eukaryote', ' fluorescence microscopy', ' fluorescent dye /probe', ' freeze etching', ' gene expression', ' genetic manipulation', ' genetic mapping', ' genetic regulation', ' genetic transcription', ' image enhancement', ' immature animal', ' immunofluorescence technique', ' larva', ' monoclonal antibody', ' nonmammalian vertebrate embryology', ' nucleic acid sequence', ' radionuclide double label', ' salivary glands', ' scanning electron microscopy', ' tissue /cell culture']",NIGMS,UNIVERSITY OF CALIFORNIA SAN FRANCISCO,R01,1987,110774,-0.006079232181945182
"STRUCTURAL DETERMINATION OF INTERPHASE CHROMOSOMES The newly developed instrumentation and software now allows three-dimensional reconstruction, in a generalized fashion, of subcellular structures, at both the light and high voltage electron microscopic levels.  Eighteen nuclei have been reconstructed and interpreted in a quantitative fashion.  Six nuclei have their cytogenetic loci aligned with the three-dimensional structures showing that many chromosome features are three-dimensionally determined.  It is proposed to determine, three-dimensionally, many adjacent polytene nuclei in defined developmental stages and tissues so that it will be possible to see in a quantitative fashion the structural relationships. Many computational approaches are proposed to process the images, to refine the structures, to position the bands, hence genes three-dimensionally, and to determine the fine structure of nuclei.  These studies will also utilize monoclonal antibodies to localize active gene loci and their close associations one with another and the nuclear envelope, and to relate structure to function.  Diploid nuclei will, in addition, be three-dimensionally reconstructed at highest possible light microscope resolution in intact Drosophila embryos. Questions related to differences in the 3D patterns of the interphase chromosomes as a function of development, and the cell cycle will be studied.  The laboratory will continue to reconstruct mitotic chromosomes and interband/bands from polytene chromosomes from Drosophila in a resolution-overlaping fashion made possible by the three-dimensional light microscopy and much higher resolution of the high voltage electron microscope utilizing systematic high tilt about 70 degrees for three-dimensional structure.  New computational approaches to image processing in three dimensions are being continued.  New image collection hardware for the light microscopy and high voltage electron microscopy are proposed.  n/a",STRUCTURAL DETERMINATION OF INTERPHASE CHROMOSOMES,3272766,R01GM025101,"['animal population genetics', ' artificial intelligence', ' cell differentiation', ' chromosome disorders', ' cytogenetics', ' electron microscopy', ' endonuclease', ' eukaryote', ' fluorescence microscopy', ' fluorescent dye /probe', ' freeze etching', ' gene expression', ' genetic manipulation', ' genetic mapping', ' genetic regulation', ' genetic transcription', ' image enhancement', ' immature animal', ' immunofluorescence technique', ' larva', ' monoclonal antibody', ' nonmammalian vertebrate embryology', ' nucleic acid sequence', ' radionuclide double label', ' salivary glands', ' scanning electron microscopy', ' tissue /cell culture']",NIGMS,UNIVERSITY OF CALIFORNIA SAN FRANCISCO,R01,1986,103879,-0.006079232181945182
"STRUCTURAL DETERMINATION OF INTERPHASE CHROMOSOMES The newly developed instrumentation and software now allows three-dimensional reconstruction, in a generalized fashion, of subcellular structures, at both the light and high voltage electron microscopic levels.  Eighteen nuclei have been reconstructed and interpreted in a quantitative fashion.  Six nuclei have their cytogenetic loci aligned with the three-dimensional structures showing that many chromosome features are three-dimensionally determined.  It is proposed to determine, three-dimensionally, many adjacent polytene nuclei in defined developmental stages and tissues so that it will be possible to see in a quantitative fashion the structural relationships. Many computational approaches are proposed to process the images, to refine the structures, to position the bands, hence genes three-dimensionally, and to determine the fine structure of nuclei.  These studies will also utilize monoclonal antibodies to localize active gene loci and their close associations one with another and the nuclear envelope, and to relate structure to function.  Diploid nuclei will, in addition, be three-dimensionally reconstructed at highest possible light microscope resolution in intact Drosophila embryos. Questions related to differences in the 3D patterns of the interphase chromosomes as a function of development, and the cell cycle will be studied.  The laboratory will continue to reconstruct mitotic chromosomes and interband/bands from polytene chromosomes from Drosophila in a resolution-overlaping fashion made possible by the three-dimensional light microscopy and much higher resolution of the high voltage electron microscope utilizing systematic high tilt about 70 degrees for three-dimensional structure.  New computational approaches to image processing in three dimensions are being continued.  New image collection hardware for the light microscopy and high voltage electron microscopy are proposed.  n/a",STRUCTURAL DETERMINATION OF INTERPHASE CHROMOSOMES,3272765,R01GM025101,"['animal population genetics', ' artificial intelligence', ' cell differentiation', ' chromosome disorders', ' cytogenetics', ' electron microscopy', ' endonuclease', ' eukaryote', ' fluorescence microscopy', ' fluorescent dye /probe', ' freeze etching', ' gene expression', ' genetic manipulation', ' genetic mapping', ' genetic regulation', ' genetic transcription', ' image enhancement', ' immature animal', ' immunofluorescence technique', ' larva', ' monoclonal antibody', ' nonmammalian vertebrate embryology', ' nucleic acid sequence', ' radionuclide double label', ' salivary glands', ' scanning electron microscopy', ' tissue /cell culture']",NIGMS,UNIVERSITY OF CALIFORNIA SAN FRANCISCO,R01,1985,124739,-0.006079232181945182
"RESEARCHING THE SOCIAL DYNAMICS OF A LOCAL METHAMPHETAMINE MARKET    DESCRIPTION (provided by applicant): This project investigates a significant feature of the socioenvironmental context of illicit drug use. Directly or indirectly, all users engage the illicit drug economy. Understanding the market roles of producers, consumers, and distributors is critical for prevention, treatment and law enforcement. However, drug markets are dynamic; they continuously adapt to internal and external forces. No theory (or methodology) exists to describe such dynamics. This five-year study develops and systematically field-tests agent-based modeling (ABM) methods to represent consumer, dealer, and health behaviors associated with the methamphetamine market in Cuyahoga and Summit Counties, OH. This approach will provide researchers with an entirely new perspective for observing outcomes of interactive behaviors, as well as the opportunity to perform experiments not possible in the real world. To program simulations, we will characterize roles, motives, behaviors, and interactions of market participants utilizing ethnographic methods (specifically, decision-tree modeling techniques). To validate and extend these data, we will collect quantitative measures of participants' daily behaviors, drug consumption, costs, and interactions using Ecological Momentary Assessment (EMA). We will implement these techniques through a three-wave panel study (N=204); data from the panel study will characterize the sample, inform simulation parameters, and help validate simulation output. Our simulations will subsequently incorporate both descriptive ethnographic data and quantitative longitudinal data so that the behaviors of ""agents"" (simulated people interacting within the virtual market) will accurately reflect the realities of the real-world drug market. The simulation-based computer laboratory produced by this project will offer policymakers and researchers a tool for: (1) dynamically representing illicit drug market operations, (2) experimenting with agent behaviors and market parameters and conditions, and (3) evaluating ""what if"" policy scenarios intended to influence outcomes. Applying Complexity theory by systematically combining the rich detail of ethnography with ABM's power to aggregate socially complex behaviors, this project will have great practical utility. The specific aims of this project are to: (1) Conduct ethnographic research on the methamphetamine market in Cuyahoga and Summit Counties, OH. (2) Enrich these data using Ecological Momentary Assessment (EMA), collecting self-report data on daily drug consumption, production, sales, decisions, strategies etc. (3) Inform simulation parameters using a panel survey of 204 active methamphetamine users. (4) Construct a computer lab of ABM simulations reproducing how the local methamphetamine market operates integrating both social (i.e., health) and economic behaviors. (5) Experiment with the ABM simulations to: understand how the market operates and functions; create and test policy-based intervention scenarios (e.g. enforcement, treatment, and outreach) intended to impact outcomes; and model risk behaviors (e.g., needle sharing, trading drugs for sex) influencing the spread of HIV. PUBLIC HEALTH RELEVANCE: Drug users risk behaviors are influenced by the markets in which they acquire drugs. However, these markets are dynamic and reactive which make understanding and modeling this interaction challenging. The objective of this research is to field test a transportable, theoretically informed, and inter-disciplinary research methodology using agent-based simulation to evaluate market dynamics and their impact on methamphetamine distribution and HIV-related risk behaviors.           PROJECT NARRATIVE Drug users risk behaviors are influenced by the markets in which they acquire drugs. However, these markets are dynamic and reactive which make understanding and modeling this interaction challenging. The objective of this research is to field test a transportable, theoretically informed, and inter-disciplinary research methodology using agent-based simulation to evaluate market dynamics and their impact on methamphetamine distribution and HIV - related risk behaviors.",RESEARCHING THE SOCIAL DYNAMICS OF A LOCAL METHAMPHETAMINE MARKET,8517061,R01DA025163,"['Behavior', 'Complex', 'Computer Simulation', 'Computers', 'Consumption', 'County', 'Data', 'Decision Trees', 'Drug usage', 'Drug user', 'Ethnography', 'HIV', 'Health', 'Health behavior', 'Illicit Drugs', 'Individual', 'Intervention', 'Laboratories', 'Law Enforcement', 'Marketing', 'Measures', 'Methamphetamine', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Needle Sharing', 'Outcome', 'Output', 'Participant', 'Patient Self-Report', 'Pharmaceutical Preparations', 'Play', 'Policies', 'Prevention', 'Price', 'Production', 'Public Health', 'Research', 'Research Methodology', 'Research Personnel', 'Risk Behaviors', 'Role', 'Sales', 'Sampling', 'Simulate', 'Structure', 'Surveys', 'Techniques', 'Testing', 'base', 'behavior influence', 'comparative', 'cost', 'data modeling', 'drug market', 'economic behavior', 'ethnographic method', 'health economics', 'model development', 'models and simulation', 'novel', 'operation', 'outreach', 'programs', 'research study', 'sex', 'simulation', 'social', 'theories', 'tool', 'virtual']",NIDA,CASE WESTERN RESERVE UNIVERSITY,R01,2013,325668,-0.04875243724894738
"RESEARCHING THE SOCIAL DYNAMICS OF A LOCAL METHAMPHETAMINE MARKET    DESCRIPTION (provided by applicant): This project investigates a significant feature of the socioenvironmental context of illicit drug use. Directly or indirectly, all users engage the illicit drug economy. Understanding the market roles of producers, consumers, and distributors is critical for prevention, treatment and law enforcement. However, drug markets are dynamic; they continuously adapt to internal and external forces. No theory (or methodology) exists to describe such dynamics. This five-year study develops and systematically field-tests agent-based modeling (ABM) methods to represent consumer, dealer, and health behaviors associated with the methamphetamine market in Cuyahoga and Summit Counties, OH. This approach will provide researchers with an entirely new perspective for observing outcomes of interactive behaviors, as well as the opportunity to perform experiments not possible in the real world. To program simulations, we will characterize roles, motives, behaviors, and interactions of market participants utilizing ethnographic methods (specifically, decision-tree modeling techniques). To validate and extend these data, we will collect quantitative measures of participants' daily behaviors, drug consumption, costs, and interactions using Ecological Momentary Assessment (EMA). We will implement these techniques through a three-wave panel study (N=204); data from the panel study will characterize the sample, inform simulation parameters, and help validate simulation output. Our simulations will subsequently incorporate both descriptive ethnographic data and quantitative longitudinal data so that the behaviors of ""agents"" (simulated people interacting within the virtual market) will accurately reflect the realities of the real-world drug market. The simulation-based computer laboratory produced by this project will offer policymakers and researchers a tool for: (1) dynamically representing illicit drug market operations, (2) experimenting with agent behaviors and market parameters and conditions, and (3) evaluating ""what if"" policy scenarios intended to influence outcomes. Applying Complexity theory by systematically combining the rich detail of ethnography with ABM's power to aggregate socially complex behaviors, this project will have great practical utility. The specific aims of this project are to: (1) Conduct ethnographic research on the methamphetamine market in Cuyahoga and Summit Counties, OH. (2) Enrich these data using Ecological Momentary Assessment (EMA), collecting self-report data on daily drug consumption, production, sales, decisions, strategies etc. (3) Inform simulation parameters using a panel survey of 204 active methamphetamine users. (4) Construct a computer lab of ABM simulations reproducing how the local methamphetamine market operates integrating both social (i.e., health) and economic behaviors. (5) Experiment with the ABM simulations to: understand how the market operates and functions; create and test policy-based intervention scenarios (e.g. enforcement, treatment, and outreach) intended to impact outcomes; and model risk behaviors (e.g., needle sharing, trading drugs for sex) influencing the spread of HIV. PUBLIC HEALTH RELEVANCE: Drug users risk behaviors are influenced by the markets in which they acquire drugs. However, these markets are dynamic and reactive which make understanding and modeling this interaction challenging. The objective of this research is to field test a transportable, theoretically informed, and inter-disciplinary research methodology using agent-based simulation to evaluate market dynamics and their impact on methamphetamine distribution and HIV-related risk behaviors.           PROJECT NARRATIVE Drug users risk behaviors are influenced by the markets in which they acquire drugs. However, these markets are dynamic and reactive which make understanding and modeling this interaction challenging. The objective of this research is to field test a transportable, theoretically informed, and inter-disciplinary research methodology using agent-based simulation to evaluate market dynamics and their impact on methamphetamine distribution and HIV - related risk behaviors.",RESEARCHING THE SOCIAL DYNAMICS OF A LOCAL METHAMPHETAMINE MARKET,8301700,R01DA025163,"['Behavior', 'Complex', 'Computer Simulation', 'Computers', 'Consumption', 'County', 'Data', 'Decision Trees', 'Drug usage', 'Drug user', 'Ethnography', 'HIV', 'Health', 'Health behavior', 'Illicit Drugs', 'Individual', 'Intervention', 'Laboratories', 'Law Enforcement', 'Marketing', 'Measures', 'Methamphetamine', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Needle Sharing', 'Outcome', 'Output', 'Participant', 'Patient Self-Report', 'Pharmaceutical Preparations', 'Play', 'Policies', 'Prevention', 'Price', 'Production', 'Public Health', 'Research', 'Research Methodology', 'Research Personnel', 'Risk Behaviors', 'Role', 'Sales', 'Sampling', 'Simulate', 'Structure', 'Surveys', 'Techniques', 'Testing', 'base', 'behavior influence', 'comparative', 'cost', 'data modeling', 'drug market', 'economic behavior', 'ethnographic method', 'health economics', 'model development', 'models and simulation', 'novel', 'operation', 'outreach', 'programs', 'research study', 'sex', 'simulation', 'social', 'theories', 'tool', 'virtual']",NIDA,CASE WESTERN RESERVE UNIVERSITY,R01,2012,306138,-0.04875243724894738
"RESEARCHING THE SOCIAL DYNAMICS OF A LOCAL METHAMPHETAMINE MARKET    DESCRIPTION (provided by applicant): This project investigates a significant feature of the socioenvironmental context of illicit drug use. Directly or indirectly, all users engage the illicit drug economy. Understanding the market roles of producers, consumers, and distributors is critical for prevention, treatment and law enforcement. However, drug markets are dynamic; they continuously adapt to internal and external forces. No theory (or methodology) exists to describe such dynamics. This five-year study develops and systematically field-tests agent-based modeling (ABM) methods to represent consumer, dealer, and health behaviors associated with the methamphetamine market in Cuyahoga and Summit Counties, OH. This approach will provide researchers with an entirely new perspective for observing outcomes of interactive behaviors, as well as the opportunity to perform experiments not possible in the real world. To program simulations, we will characterize roles, motives, behaviors, and interactions of market participants utilizing ethnographic methods (specifically, decision-tree modeling techniques). To validate and extend these data, we will collect quantitative measures of participants' daily behaviors, drug consumption, costs, and interactions using Ecological Momentary Assessment (EMA). We will implement these techniques through a three-wave panel study (N=204); data from the panel study will characterize the sample, inform simulation parameters, and help validate simulation output. Our simulations will subsequently incorporate both descriptive ethnographic data and quantitative longitudinal data so that the behaviors of ""agents"" (simulated people interacting within the virtual market) will accurately reflect the realities of the real-world drug market. The simulation-based computer laboratory produced by this project will offer policymakers and researchers a tool for: (1) dynamically representing illicit drug market operations, (2) experimenting with agent behaviors and market parameters and conditions, and (3) evaluating ""what if"" policy scenarios intended to influence outcomes. Applying Complexity theory by systematically combining the rich detail of ethnography with ABM's power to aggregate socially complex behaviors, this project will have great practical utility. The specific aims of this project are to: (1) Conduct ethnographic research on the methamphetamine market in Cuyahoga and Summit Counties, OH. (2) Enrich these data using Ecological Momentary Assessment (EMA), collecting self-report data on daily drug consumption, production, sales, decisions, strategies etc. (3) Inform simulation parameters using a panel survey of 204 active methamphetamine users. (4) Construct a computer lab of ABM simulations reproducing how the local methamphetamine market operates integrating both social (i.e., health) and economic behaviors. (5) Experiment with the ABM simulations to: understand how the market operates and functions; create and test policy-based intervention scenarios (e.g. enforcement, treatment, and outreach) intended to impact outcomes; and model risk behaviors (e.g., needle sharing, trading drugs for sex) influencing the spread of HIV. PUBLIC HEALTH RELEVANCE: Drug users risk behaviors are influenced by the markets in which they acquire drugs. However, these markets are dynamic and reactive which make understanding and modeling this interaction challenging. The objective of this research is to field test a transportable, theoretically informed, and inter-disciplinary research methodology using agent-based simulation to evaluate market dynamics and their impact on methamphetamine distribution and HIV-related risk behaviors.           PROJECT NARRATIVE Drug users risk behaviors are influenced by the markets in which they acquire drugs. However, these markets are dynamic and reactive which make understanding and modeling this interaction challenging. The objective of this research is to field test a transportable, theoretically informed, and inter-disciplinary research methodology using agent-based simulation to evaluate market dynamics and their impact on methamphetamine distribution and HIV - related risk behaviors.",RESEARCHING THE SOCIAL DYNAMICS OF A LOCAL METHAMPHETAMINE MARKET,7737095,R01DA025163,"['Area', 'Behavior', 'Behavior assessment', 'Classification', 'Communities', 'Complex', 'Computer Simulation', 'Computers', 'Consumption', 'County', 'Data', 'Decision Making', 'Decision Trees', 'Diagnostic', 'Drug usage', 'Drug user', 'Economics', 'Electronics', 'Equation', 'Ethnography', 'Exhibits', 'Geographic Information Systems', 'HIV', 'Health behavior', 'Illicit Drugs', 'Individual', 'Injecting drug user', 'Intelligence', 'Intervention', 'Interview', 'Intramural Research Program', 'Laboratories', 'Law Enforcement', 'Linear Models', 'Marketing', 'Measures', 'Mental Health', 'Methamphetamine', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'National Institute of Drug Abuse', 'Needle Sharing', 'Ohio', 'Operative Surgical Procedures', 'Outcome', 'Output', 'Participant', 'Patient Self-Report', 'Personal Digital Assistant', 'Pharmaceutical Preparations', 'Play', 'Policies', 'Prevention', 'Price', 'Process Assessment', 'Production', 'Public Health', 'Research', 'Research Methodology', 'Research Personnel', 'Respondent', 'Risk Behaviors', 'Role', 'Sales', 'Sampling', 'Schedule', 'Simulate', 'Structure', 'Substance abuse problem', 'Surveys', 'System', 'Techniques', 'Testing', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'United States Substance Abuse and Mental Health Services Administration', 'acronyms', 'addiction', 'base', 'behavior influence', 'comparative', 'cost', 'data modeling', 'diaries', 'drug market', 'economic behavior', 'ethnographic method', 'health administration', 'health economics', 'men who have sex with men', 'model development', 'models and simulation', 'novel', 'outreach', 'programs', 'public health relevance', 'research study', 'sex', 'simulation', 'social', 'theories', 'tool', 'trafficking', 'transmission process', 'virtual']",NIDA,CASE WESTERN RESERVE UNIVERSITY,R01,2009,334040,-0.04875243724894738
"RESEARCHING THE SOCIAL DYNAMICS OF A LOCAL METHAMPHETAMINE MARKET    DESCRIPTION (provided by applicant): This project investigates a significant feature of the socioenvironmental context of illicit drug use. Directly or indirectly, all users engage the illicit drug economy. Understanding the market roles of producers, consumers, and distributors is critical for prevention, treatment and law enforcement. However, drug markets are dynamic; they continuously adapt to internal and external forces. No theory (or methodology) exists to describe such dynamics. This five-year study develops and systematically field-tests agent-based modeling (ABM) methods to represent consumer, dealer, and health behaviors associated with the methamphetamine market in Cuyahoga and Summit Counties, OH. This approach will provide researchers with an entirely new perspective for observing outcomes of interactive behaviors, as well as the opportunity to perform experiments not possible in the real world. To program simulations, we will characterize roles, motives, behaviors, and interactions of market participants utilizing ethnographic methods (specifically, decision-tree modeling techniques). To validate and extend these data, we will collect quantitative measures of participants' daily behaviors, drug consumption, costs, and interactions using Ecological Momentary Assessment (EMA). We will implement these techniques through a three-wave panel study (N=204); data from the panel study will characterize the sample, inform simulation parameters, and help validate simulation output. Our simulations will subsequently incorporate both descriptive ethnographic data and quantitative longitudinal data so that the behaviors of ""agents"" (simulated people interacting within the virtual market) will accurately reflect the realities of the real-world drug market. The simulation-based computer laboratory produced by this project will offer policymakers and researchers a tool for: (1) dynamically representing illicit drug market operations, (2) experimenting with agent behaviors and market parameters and conditions, and (3) evaluating ""what if"" policy scenarios intended to influence outcomes. Applying Complexity theory by systematically combining the rich detail of ethnography with ABM's power to aggregate socially complex behaviors, this project will have great practical utility. The specific aims of this project are to: (1) Conduct ethnographic research on the methamphetamine market in Cuyahoga and Summit Counties, OH. (2) Enrich these data using Ecological Momentary Assessment (EMA), collecting self-report data on daily drug consumption, production, sales, decisions, strategies etc. (3) Inform simulation parameters using a panel survey of 204 active methamphetamine users. (4) Construct a computer lab of ABM simulations reproducing how the local methamphetamine market operates integrating both social (i.e., health) and economic behaviors. (5) Experiment with the ABM simulations to: understand how the market operates and functions; create and test policy-based intervention scenarios (e.g. enforcement, treatment, and outreach) intended to impact outcomes; and model risk behaviors (e.g., needle sharing, trading drugs for sex) influencing the spread of HIV. PUBLIC HEALTH RELEVANCE: Drug users risk behaviors are influenced by the markets in which they acquire drugs. However, these markets are dynamic and reactive which make understanding and modeling this interaction challenging. The objective of this research is to field test a transportable, theoretically informed, and inter-disciplinary research methodology using agent-based simulation to evaluate market dynamics and their impact on methamphetamine distribution and HIV-related risk behaviors.           PROJECT NARRATIVE Drug users risk behaviors are influenced by the markets in which they acquire drugs. However, these markets are dynamic and reactive which make understanding and modeling this interaction challenging. The objective of this research is to field test a transportable, theoretically informed, and inter-disciplinary research methodology using agent-based simulation to evaluate market dynamics and their impact on methamphetamine distribution and HIV - related risk behaviors.",RESEARCHING THE SOCIAL DYNAMICS OF A LOCAL METHAMPHETAMINE MARKET,8103181,R01DA025163,"['Behavior', 'Complex', 'Computer Simulation', 'Computers', 'Consumption', 'County', 'Data', 'Decision Trees', 'Drug usage', 'Drug user', 'Ethnography', 'HIV', 'Health', 'Health behavior', 'Illicit Drugs', 'Individual', 'Intervention', 'Laboratories', 'Law Enforcement', 'Marketing', 'Measures', 'Methamphetamine', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Needle Sharing', 'Outcome', 'Output', 'Participant', 'Patient Self-Report', 'Pharmaceutical Preparations', 'Play', 'Policies', 'Prevention', 'Price', 'Production', 'Public Health', 'Research', 'Research Methodology', 'Research Personnel', 'Risk Behaviors', 'Role', 'Sales', 'Sampling', 'Simulate', 'Structure', 'Surveys', 'Techniques', 'Testing', 'base', 'behavior influence', 'comparative', 'cost', 'data modeling', 'drug market', 'economic behavior', 'ethnographic method', 'health economics', 'model development', 'models and simulation', 'novel', 'operation', 'outreach', 'programs', 'research study', 'sex', 'simulation', 'social', 'theories', 'tool', 'virtual']",NIDA,CASE WESTERN RESERVE UNIVERSITY,R01,2011,301062,-0.04875243724894738
"RESEARCHING THE SOCIAL DYNAMICS OF A LOCAL METHAMPHETAMINE MARKET    DESCRIPTION (provided by applicant): This project investigates a significant feature of the socioenvironmental context of illicit drug use. Directly or indirectly, all users engage the illicit drug economy. Understanding the market roles of producers, consumers, and distributors is critical for prevention, treatment and law enforcement. However, drug markets are dynamic; they continuously adapt to internal and external forces. No theory (or methodology) exists to describe such dynamics. This five-year study develops and systematically field-tests agent-based modeling (ABM) methods to represent consumer, dealer, and health behaviors associated with the methamphetamine market in Cuyahoga and Summit Counties, OH. This approach will provide researchers with an entirely new perspective for observing outcomes of interactive behaviors, as well as the opportunity to perform experiments not possible in the real world. To program simulations, we will characterize roles, motives, behaviors, and interactions of market participants utilizing ethnographic methods (specifically, decision-tree modeling techniques). To validate and extend these data, we will collect quantitative measures of participants' daily behaviors, drug consumption, costs, and interactions using Ecological Momentary Assessment (EMA). We will implement these techniques through a three-wave panel study (N=204); data from the panel study will characterize the sample, inform simulation parameters, and help validate simulation output. Our simulations will subsequently incorporate both descriptive ethnographic data and quantitative longitudinal data so that the behaviors of ""agents"" (simulated people interacting within the virtual market) will accurately reflect the realities of the real-world drug market. The simulation-based computer laboratory produced by this project will offer policymakers and researchers a tool for: (1) dynamically representing illicit drug market operations, (2) experimenting with agent behaviors and market parameters and conditions, and (3) evaluating ""what if"" policy scenarios intended to influence outcomes. Applying Complexity theory by systematically combining the rich detail of ethnography with ABM's power to aggregate socially complex behaviors, this project will have great practical utility. The specific aims of this project are to: (1) Conduct ethnographic research on the methamphetamine market in Cuyahoga and Summit Counties, OH. (2) Enrich these data using Ecological Momentary Assessment (EMA), collecting self-report data on daily drug consumption, production, sales, decisions, strategies etc. (3) Inform simulation parameters using a panel survey of 204 active methamphetamine users. (4) Construct a computer lab of ABM simulations reproducing how the local methamphetamine market operates integrating both social (i.e., health) and economic behaviors. (5) Experiment with the ABM simulations to: understand how the market operates and functions; create and test policy-based intervention scenarios (e.g. enforcement, treatment, and outreach) intended to impact outcomes; and model risk behaviors (e.g., needle sharing, trading drugs for sex) influencing the spread of HIV. PUBLIC HEALTH RELEVANCE: Drug users risk behaviors are influenced by the markets in which they acquire drugs. However, these markets are dynamic and reactive which make understanding and modeling this interaction challenging. The objective of this research is to field test a transportable, theoretically informed, and inter-disciplinary research methodology using agent-based simulation to evaluate market dynamics and their impact on methamphetamine distribution and HIV-related risk behaviors.           PROJECT NARRATIVE Drug users risk behaviors are influenced by the markets in which they acquire drugs. However, these markets are dynamic and reactive which make understanding and modeling this interaction challenging. The objective of this research is to field test a transportable, theoretically informed, and inter-disciplinary research methodology using agent-based simulation to evaluate market dynamics and their impact on methamphetamine distribution and HIV - related risk behaviors.",RESEARCHING THE SOCIAL DYNAMICS OF A LOCAL METHAMPHETAMINE MARKET,7929460,R01DA025163,"['Behavior', 'Complex', 'Computer Simulation', 'Computers', 'Consumption', 'County', 'Data', 'Decision Trees', 'Drug usage', 'Drug user', 'Ethnography', 'HIV', 'Health behavior', 'Illicit Drugs', 'Individual', 'Intervention', 'Laboratories', 'Law Enforcement', 'Marketing', 'Measures', 'Methamphetamine', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Needle Sharing', 'Outcome', 'Output', 'Participant', 'Patient Self-Report', 'Pharmaceutical Preparations', 'Play', 'Policies', 'Prevention', 'Price', 'Production', 'Public Health', 'Research', 'Research Methodology', 'Research Personnel', 'Risk Behaviors', 'Role', 'Sales', 'Sampling', 'Simulate', 'Structure', 'Surveys', 'Techniques', 'Testing', 'base', 'behavior influence', 'comparative', 'cost', 'data modeling', 'drug market', 'economic behavior', 'ethnographic method', 'health economics', 'model development', 'models and simulation', 'novel', 'operation', 'outreach', 'programs', 'public health relevance', 'research study', 'sex', 'simulation', 'social', 'theories', 'tool', 'virtual']",NIDA,CASE WESTERN RESERVE UNIVERSITY,R01,2010,317876,-0.04875243724894738
"Quantitative Low-Dose PET Imaging Project summary Quantitative PET has become increasingly important in clinical management and research, in particular for predicting and assessing response to therapy for cancer patients. Current PET protocols involve injection of PET tracers that typically result in ~6-7 mSv radiation dose to patients. For patients who require multiple repeated PET scans to monitor the response to therapy, and for patients who need PET scans with two or more tracers (e.g., FDG + FLT) to optimally predict response to therapy, it is critical to reduce the radiation dose from the PET tracer injection, while still maintaining the quantitative accuracy and image quality for cancer management. When reducing injection dose, the PET images will have higher noise due to fewer detected counts, which will subsequently introduce errors in quantitative measurements. For moving organs and tumors such as those in the lung and abdomen, respiratory motion can substantially degrade quantitative accuracy, so motion correction is required. Conventional motion correction uses a gating strategy that rebins the PET data, resulting in substantially higher noise in each gate. More advanced methods incorporate motion vector estimation in the image domain for post-registration or motion compensated image reconstruction using all detected events without increasing noise. The motion vectors need to be derived from gated PET, which are even noisier when using a reduced tracer injection in low-dose studies, imposing substantial challenges for accurate and reliable voxel-by-voxel motion vector estimation. In dynamic PET studies with clinical cardiac tracers and other novel oncology and neurology tracers, quantification is even more challenging for low-dose PET as each dynamic frame only contains a small fraction of detected events so the high image noise will affect the determination of image-derived input functions and can lead to bias and high noise in parametric images. In this project, to reduce image noise and maintain quantitative accuracy in PET, we propose to develop, optimize, and evaluate multiple innovative imaging methods for low-dose PET data to achieve comparable quantitative accuracy as full-dose PET. While the imaging developments are generally applicable to all PET tracers in oncology, neurology, and cardiology, since cancer is the primary clinical application of PET, we will focus our investigation and optimization in this project on three lung cancer imaging tracers at different clinical adoption stages as examples: 1) 18F-FDG as a routine clinical tracer, 2) 18F-FMISO for hypoxia studies as a tracer for human research, and 3) 18F-PD-L1 that specifically binds to human PD-L1 in tumors and other organs as a recent first-in-human tracer. For each tracer, we will investigate 1) static PET, 2) gated and respiratory motion corrected PET, and 3) dynamic PET. Project narrative Patient radiation dose is a major concern in PET imaging. This project will be an important step to develop and standardize low-dose PET imaging methods and provide guidance for clinical practice, research studies, and both single and multiple site clinical trials for PET dose reduction. This project will lead to the development of imaging approaches to achieve the lowest level of PET radiation dose for applications not only in evaluation and prediction of response to cancer therapy, but for other applications in oncology, neurology, and cardiology.",Quantitative Low-Dose PET Imaging,10137354,R01EB025468,"['3-Dimensional', 'Abdomen', 'Adoption', 'Affect', 'Anatomy', 'Binding', 'Breathing', 'Cancer Patient', 'Cardiac', 'Cardiology', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Data', 'Data Set', 'Development', 'Dose', 'Evaluation', 'Event', 'Goals', 'Gold', 'Human', 'Hypoxia', 'Image', 'Injections', 'Investigation', 'Lead', 'Lung', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Measurement', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Neurology', 'Noise', 'Oncology', 'Organ', 'Patients', 'Positron-Emission Tomography', 'Protocols documentation', 'Radiation Dose Unit', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Standardization', 'Texture', 'Tracer', 'Training', 'Translating', 'Translations', 'Vendor', 'attenuation', 'base', 'cancer imaging', 'cancer therapy', 'clinical application', 'clinical practice', 'clinical research site', 'deep learning', 'first-in-human', 'fluorodeoxyglucose', 'image reconstruction', 'imaging approach', 'imaging modality', 'innovation', 'kinetic model', 'novel', 'parametric imaging', 'predicting response', 'programmed cell death ligand 1', 'reconstruction', 'research study', 'respiratory', 'response', 'tumor', 'vector']",NIBIB,YALE UNIVERSITY,R01,2020,149331,-0.033905896191706836
"Quantitative Low-Dose PET Imaging Project summary Quantitative PET has become increasingly important in clinical management and research, in particular for predicting and assessing response to therapy for cancer patients. Current PET protocols involve injection of PET tracers that typically result in ~6-7 mSv radiation dose to patients. For patients who require multiple repeated PET scans to monitor the response to therapy, and for patients who need PET scans with two or more tracers (e.g., FDG + FLT) to optimally predict response to therapy, it is critical to reduce the radiation dose from the PET tracer injection, while still maintaining the quantitative accuracy and image quality for cancer management. When reducing injection dose, the PET images will have higher noise due to fewer detected counts, which will subsequently introduce errors in quantitative measurements. For moving organs and tumors such as those in the lung and abdomen, respiratory motion can substantially degrade quantitative accuracy, so motion correction is required. Conventional motion correction uses a gating strategy that rebins the PET data, resulting in substantially higher noise in each gate. More advanced methods incorporate motion vector estimation in the image domain for post-registration or motion compensated image reconstruction using all detected events without increasing noise. The motion vectors need to be derived from gated PET, which are even noisier when using a reduced tracer injection in low-dose studies, imposing substantial challenges for accurate and reliable voxel-by-voxel motion vector estimation. In dynamic PET studies with clinical cardiac tracers and other novel oncology and neurology tracers, quantification is even more challenging for low-dose PET as each dynamic frame only contains a small fraction of detected events so the high image noise will affect the determination of image-derived input functions and can lead to bias and high noise in parametric images. In this project, to reduce image noise and maintain quantitative accuracy in PET, we propose to develop, optimize, and evaluate multiple innovative imaging methods for low-dose PET data to achieve comparable quantitative accuracy as full-dose PET. While the imaging developments are generally applicable to all PET tracers in oncology, neurology, and cardiology, since cancer is the primary clinical application of PET, we will focus our investigation and optimization in this project on three lung cancer imaging tracers at different clinical adoption stages as examples: 1) 18F-FDG as a routine clinical tracer, 2) 18F-FMISO for hypoxia studies as a tracer for human research, and 3) 18F-PD-L1 that specifically binds to human PD-L1 in tumors and other organs as a recent first-in-human tracer. For each tracer, we will investigate 1) static PET, 2) gated and respiratory motion corrected PET, and 3) dynamic PET. Project narrative Patient radiation dose is a major concern in PET imaging. This project will be an important step to develop and standardize low-dose PET imaging methods and provide guidance for clinical practice, research studies, and both single and multiple site clinical trials for PET dose reduction. This project will lead to the development of imaging approaches to achieve the lowest level of PET radiation dose for applications not only in evaluation and prediction of response to cancer therapy, but for other applications in oncology, neurology, and cardiology.",Quantitative Low-Dose PET Imaging,9879968,R01EB025468,"['3-Dimensional', 'Abdomen', 'Adoption', 'Affect', 'Anatomy', 'Binding', 'Breathing', 'Cancer Patient', 'Cardiac', 'Cardiology', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Data', 'Data Set', 'Development', 'Dose', 'Evaluation', 'Event', 'Goals', 'Gold', 'Human', 'Hypoxia', 'Image', 'Injections', 'Investigation', 'Kinetics', 'Lead', 'Lung', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Measurement', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Neurology', 'Noise', 'Organ', 'PDCD1LG1 gene', 'Patients', 'Positron-Emission Tomography', 'Protocols documentation', 'Radiation Dose Unit', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Standardization', 'Texture', 'Tracer', 'Training', 'Translating', 'Translations', 'Vendor', 'attenuation', 'base', 'cancer imaging', 'cancer therapy', 'clinical application', 'clinical practice', 'clinical research site', 'deep learning', 'first-in-human', 'fluorodeoxyglucose', 'image reconstruction', 'imaging approach', 'imaging modality', 'innovation', 'novel', 'oncology', 'parametric imaging', 'predicting response', 'reconstruction', 'research study', 'respiratory', 'response', 'tumor', 'vector']",NIBIB,YALE UNIVERSITY,R01,2019,167376,-0.033905896191706836
"Quantitative Low-Dose PET Imaging Project summary Quantitative PET has become increasingly important in clinical management and research, in particular for predicting and assessing response to therapy for cancer patients. Current PET protocols involve injection of PET tracers that typically result in ~6-7 mSv radiation dose to patients. For patients who require multiple repeated PET scans to monitor the response to therapy, and for patients who need PET scans with two or more tracers (e.g., FDG + FLT) to optimally predict response to therapy, it is critical to reduce the radiation dose from the PET tracer injection, while still maintaining the quantitative accuracy and image quality for cancer management. When reducing injection dose, the PET images will have higher noise due to fewer detected counts, which will subsequently introduce errors in quantitative measurements. For moving organs and tumors such as those in the lung and abdomen, respiratory motion can substantially degrade quantitative accuracy, so motion correction is required. Conventional motion correction uses a gating strategy that rebins the PET data, resulting in substantially higher noise in each gate. More advanced methods incorporate motion vector estimation in the image domain for post-registration or motion compensated image reconstruction using all detected events without increasing noise. The motion vectors need to be derived from gated PET, which are even noisier when using a reduced tracer injection in low-dose studies, imposing substantial challenges for accurate and reliable voxel-by-voxel motion vector estimation. In dynamic PET studies with clinical cardiac tracers and other novel oncology and neurology tracers, quantification is even more challenging for low-dose PET as each dynamic frame only contains a small fraction of detected events so the high image noise will affect the determination of image-derived input functions and can lead to bias and high noise in parametric images. In this project, to reduce image noise and maintain quantitative accuracy in PET, we propose to develop, optimize, and evaluate multiple innovative imaging methods for low-dose PET data to achieve comparable quantitative accuracy as full-dose PET. While the imaging developments are generally applicable to all PET tracers in oncology, neurology, and cardiology, since cancer is the primary clinical application of PET, we will focus our investigation and optimization in this project on three lung cancer imaging tracers at different clinical adoption stages as examples: 1) 18F-FDG as a routine clinical tracer, 2) 18F-FMISO for hypoxia studies as a tracer for human research, and 3) 18F-PD-L1 that specifically binds to human PD-L1 in tumors and other organs as a recent first-in-human tracer. For each tracer, we will investigate 1) static PET, 2) gated and respiratory motion corrected PET, and 3) dynamic PET. Project narrative Patient radiation dose is a major concern in PET imaging. This project will be an important step to develop and standardize low-dose PET imaging methods and provide guidance for clinical practice, research studies, and both single and multiple site clinical trials for PET dose reduction. This project will lead to the development of imaging approaches to achieve the lowest level of PET radiation dose for applications not only in evaluation and prediction of response to cancer therapy, but for other applications in oncology, neurology, and cardiology.",Quantitative Low-Dose PET Imaging,9924591,R01EB025468,"['3-Dimensional', 'Abdomen', 'Adoption', 'Affect', 'Anatomy', 'Binding', 'Breathing', 'Cancer Patient', 'Cardiac', 'Cardiology', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Data', 'Data Set', 'Development', 'Dose', 'Evaluation', 'Event', 'Goals', 'Gold', 'Human', 'Hypoxia', 'Image', 'Injections', 'Investigation', 'Lead', 'Lung', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Measurement', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Neurology', 'Noise', 'Oncology', 'Organ', 'Patients', 'Positron-Emission Tomography', 'Protocols documentation', 'Radiation Dose Unit', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Standardization', 'Texture', 'Tracer', 'Training', 'Translating', 'Translations', 'Vendor', 'attenuation', 'base', 'cancer imaging', 'cancer therapy', 'clinical application', 'clinical practice', 'clinical research site', 'deep learning', 'first-in-human', 'fluorodeoxyglucose', 'image reconstruction', 'imaging approach', 'imaging modality', 'innovation', 'kinetic model', 'novel', 'parametric imaging', 'predicting response', 'programmed cell death ligand 1', 'reconstruction', 'research study', 'respiratory', 'response', 'tumor', 'vector']",NIBIB,YALE UNIVERSITY,R01,2020,679852,-0.033905896191706836
"Quantitative Low-Dose PET Imaging Project summary Quantitative PET has become increasingly important in clinical management and research, in particular for predicting and assessing response to therapy for cancer patients. Current PET protocols involve injection of PET tracers that typically result in ~6-7 mSv radiation dose to patients. For patients who require multiple repeated PET scans to monitor the response to therapy, and for patients who need PET scans with two or more tracers (e.g., FDG + FLT) to optimally predict response to therapy, it is critical to reduce the radiation dose from the PET tracer injection, while still maintaining the quantitative accuracy and image quality for cancer management. When reducing injection dose, the PET images will have higher noise due to fewer detected counts, which will subsequently introduce errors in quantitative measurements. For moving organs and tumors such as those in the lung and abdomen, respiratory motion can substantially degrade quantitative accuracy, so motion correction is required. Conventional motion correction uses a gating strategy that rebins the PET data, resulting in substantially higher noise in each gate. More advanced methods incorporate motion vector estimation in the image domain for post-registration or motion compensated image reconstruction using all detected events without increasing noise. The motion vectors need to be derived from gated PET, which are even noisier when using a reduced tracer injection in low-dose studies, imposing substantial challenges for accurate and reliable voxel-by-voxel motion vector estimation. In dynamic PET studies with clinical cardiac tracers and other novel oncology and neurology tracers, quantification is even more challenging for low-dose PET as each dynamic frame only contains a small fraction of detected events so the high image noise will affect the determination of image-derived input functions and can lead to bias and high noise in parametric images. In this project, to reduce image noise and maintain quantitative accuracy in PET, we propose to develop, optimize, and evaluate multiple innovative imaging methods for low-dose PET data to achieve comparable quantitative accuracy as full-dose PET. While the imaging developments are generally applicable to all PET tracers in oncology, neurology, and cardiology, since cancer is the primary clinical application of PET, we will focus our investigation and optimization in this project on three lung cancer imaging tracers at different clinical adoption stages as examples: 1) 18F-FDG as a routine clinical tracer, 2) 18F-FMISO for hypoxia studies as a tracer for human research, and 3) 18F-PD-L1 that specifically binds to human PD-L1 in tumors and other organs as a recent first-in-human tracer. For each tracer, we will investigate 1) static PET, 2) gated and respiratory motion corrected PET, and 3) dynamic PET. Project narrative Patient radiation dose is a major concern in PET imaging. This project will be an important step to develop and standardize low-dose PET imaging methods and provide guidance for clinical practice, research studies, and both single and multiple site clinical trials for PET dose reduction. This project will lead to the development of imaging approaches to achieve the lowest level of PET radiation dose for applications not only in evaluation and prediction of response to cancer therapy, but for other applications in oncology, neurology, and cardiology.",Quantitative Low-Dose PET Imaging,9750285,R01EB025468,"['3-Dimensional', 'Abdomen', 'Adoption', 'Affect', 'Anatomy', 'Binding', 'Breathing', 'Cancer Patient', 'Cardiac', 'Cardiology', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Data', 'Data Set', 'Development', 'Dose', 'Evaluation', 'Event', 'Goals', 'Gold', 'Human', 'Hypoxia', 'Image', 'Injections', 'Investigation', 'Kinetics', 'Lead', 'Lung', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Measurement', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Neurology', 'Noise', 'Organ', 'PDCD1LG1 gene', 'Patients', 'Positron-Emission Tomography', 'Protocols documentation', 'Radiation Dose Unit', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Standardization', 'Texture', 'Tracer', 'Training', 'Translating', 'Translations', 'Vendor', 'attenuation', 'base', 'cancer imaging', 'cancer therapy', 'clinical application', 'clinical practice', 'clinical research site', 'deep learning', 'first-in-human', 'fluorodeoxyglucose', 'image reconstruction', 'imaging approach', 'imaging modality', 'innovation', 'novel', 'oncology', 'parametric imaging', 'predicting response', 'reconstruction', 'research study', 'respiratory', 'response', 'tumor', 'vector']",NIBIB,YALE UNIVERSITY,R01,2019,679852,-0.033905896191706836
"Quantitative Low-Dose PET Imaging Project summary Quantitative PET has become increasingly important in clinical management and research, in particular for predicting and assessing response to therapy for cancer patients. Current PET protocols involve injection of PET tracers that typically result in ~6-7 mSv radiation dose to patients. For patients who require multiple repeated PET scans to monitor the response to therapy, and for patients who need PET scans with two or more tracers (e.g., FDG + FLT) to optimally predict response to therapy, it is critical to reduce the radiation dose from the PET tracer injection, while still maintaining the quantitative accuracy and image quality for cancer management. When reducing injection dose, the PET images will have higher noise due to fewer detected counts, which will subsequently introduce errors in quantitative measurements. For moving organs and tumors such as those in the lung and abdomen, respiratory motion can substantially degrade quantitative accuracy, so motion correction is required. Conventional motion correction uses a gating strategy that rebins the PET data, resulting in substantially higher noise in each gate. More advanced methods incorporate motion vector estimation in the image domain for post-registration or motion compensated image reconstruction using all detected events without increasing noise. The motion vectors need to be derived from gated PET, which are even noisier when using a reduced tracer injection in low-dose studies, imposing substantial challenges for accurate and reliable voxel-by-voxel motion vector estimation. In dynamic PET studies with clinical cardiac tracers and other novel oncology and neurology tracers, quantification is even more challenging for low-dose PET as each dynamic frame only contains a small fraction of detected events so the high image noise will affect the determination of image-derived input functions and can lead to bias and high noise in parametric images. In this project, to reduce image noise and maintain quantitative accuracy in PET, we propose to develop, optimize, and evaluate multiple innovative imaging methods for low-dose PET data to achieve comparable quantitative accuracy as full-dose PET. While the imaging developments are generally applicable to all PET tracers in oncology, neurology, and cardiology, since cancer is the primary clinical application of PET, we will focus our investigation and optimization in this project on three lung cancer imaging tracers at different clinical adoption stages as examples: 1) 18F-FDG as a routine clinical tracer, 2) 18F-FMISO for hypoxia studies as a tracer for human research, and 3) 18F-PD-L1 that specifically binds to human PD-L1 in tumors and other organs as a recent first-in-human tracer. For each tracer, we will investigate 1) static PET, 2) gated and respiratory motion corrected PET, and 3) dynamic PET. Project narrative Patient radiation dose is a major concern in PET imaging. This project will be an important step to develop and standardize low-dose PET imaging methods and provide guidance for clinical practice, research studies, and both single and multiple site clinical trials for PET dose reduction. This project will lead to the development of imaging approaches to achieve the lowest level of PET radiation dose for applications not only in evaluation and prediction of response to cancer therapy, but for other applications in oncology, neurology, and cardiology.",Quantitative Low-Dose PET Imaging,9595780,R01EB025468,"['Abdomen', 'Adoption', 'Affect', 'Anatomy', 'Binding', 'Breathing', 'Cancer Patient', 'Cardiac', 'Cardiology', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Data', 'Data Set', 'Development', 'Dose', 'Evaluation', 'Event', 'Goals', 'Gold', 'Human', 'Hypoxia', 'Image', 'Injections', 'Investigation', 'Kinetics', 'Lead', 'Lung', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Measurement', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Neurology', 'Noise', 'Organ', 'PDCD1LG1 gene', 'Patients', 'Positron-Emission Tomography', 'Protocols documentation', 'Radiation', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Standardization', 'Texture', 'Tracer', 'Training', 'Translating', 'Translations', 'Vendor', 'attenuation', 'base', 'cancer imaging', 'cancer therapy', 'clinical application', 'clinical practice', 'clinical research site', 'deep learning', 'first-in-human', 'fluorodeoxyglucose', 'image reconstruction', 'imaging approach', 'imaging modality', 'innovation', 'novel', 'oncology', 'parametric imaging', 'predicting response', 'reconstruction', 'research study', 'respiratory', 'response', 'tumor', 'vector']",NIBIB,YALE UNIVERSITY,R01,2018,670762,-0.033905896191706836
"Quantitative Low-Dose PET Imaging Project summary Quantitative PET has become increasingly important in clinical management and research, in particular for predicting and assessing response to therapy for cancer patients. Current PET protocols involve injection of PET tracers that typically result in ~6-7 mSv radiation dose to patients. For patients who require multiple repeated PET scans to monitor the response to therapy, and for patients who need PET scans with two or more tracers (e.g., FDG + FLT) to optimally predict response to therapy, it is critical to reduce the radiation dose from the PET tracer injection, while still maintaining the quantitative accuracy and image quality for cancer management. When reducing injection dose, the PET images will have higher noise due to fewer detected counts, which will subsequently introduce errors in quantitative measurements. For moving organs and tumors such as those in the lung and abdomen, respiratory motion can substantially degrade quantitative accuracy, so motion correction is required. Conventional motion correction uses a gating strategy that rebins the PET data, resulting in substantially higher noise in each gate. More advanced methods incorporate motion vector estimation in the image domain for post-registration or motion compensated image reconstruction using all detected events without increasing noise. The motion vectors need to be derived from gated PET, which are even noisier when using a reduced tracer injection in low-dose studies, imposing substantial challenges for accurate and reliable voxel-by-voxel motion vector estimation. In dynamic PET studies with clinical cardiac tracers and other novel oncology and neurology tracers, quantification is even more challenging for low-dose PET as each dynamic frame only contains a small fraction of detected events so the high image noise will affect the determination of image-derived input functions and can lead to bias and high noise in parametric images. In this project, to reduce image noise and maintain quantitative accuracy in PET, we propose to develop, optimize, and evaluate multiple innovative imaging methods for low-dose PET data to achieve comparable quantitative accuracy as full-dose PET. While the imaging developments are generally applicable to all PET tracers in oncology, neurology, and cardiology, since cancer is the primary clinical application of PET, we will focus our investigation and optimization in this project on three lung cancer imaging tracers at different clinical adoption stages as examples: 1) 18F-FDG as a routine clinical tracer, 2) 18F-FMISO for hypoxia studies as a tracer for human research, and 3) 18F-PD-L1 that specifically binds to human PD-L1 in tumors and other organs as a recent first-in-human tracer. For each tracer, we will investigate 1) static PET, 2) gated and respiratory motion corrected PET, and 3) dynamic PET. Project narrative Patient radiation dose is a major concern in PET imaging. This project will be an important step to develop and standardize low-dose PET imaging methods and provide guidance for clinical practice, research studies, and both single and multiple site clinical trials for PET dose reduction. This project will lead to the development of imaging approaches to achieve the lowest level of PET radiation dose for applications not only in evaluation and prediction of response to cancer therapy, but for other applications in oncology, neurology, and cardiology.",Quantitative Low-Dose PET Imaging,9749865,R01EB025468,"['Abdomen', 'Adoption', 'Affect', 'Anatomy', 'Binding', 'Breathing', 'Cancer Patient', 'Cardiac', 'Cardiology', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Data', 'Data Set', 'Development', 'Dose', 'Evaluation', 'Event', 'Goals', 'Gold', 'Human', 'Hypoxia', 'Image', 'Injections', 'Investigation', 'Kinetics', 'Lead', 'Lung', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Measurement', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Neurology', 'Noise', 'Organ', 'PDCD1LG1 gene', 'Patients', 'Positron-Emission Tomography', 'Protocols documentation', 'Radiation', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Standardization', 'Texture', 'Tracer', 'Training', 'Translating', 'Translations', 'Vendor', 'attenuation', 'base', 'cancer imaging', 'cancer therapy', 'clinical application', 'clinical practice', 'clinical research site', 'deep learning', 'first-in-human', 'fluorodeoxyglucose', 'image reconstruction', 'imaging approach', 'imaging modality', 'innovation', 'novel', 'oncology', 'parametric imaging', 'predicting response', 'reconstruction', 'research study', 'respiratory', 'response', 'tumor', 'vector']",NIBIB,YALE UNIVERSITY,R01,2018,141623,-0.033905896191706836
"Individualized spatial topology in functional neuroimaging Project Summary. Neuroimaging is poised to take a substantial leap forward in understanding the neurophysiological underpinnings of human behavior, due to a combination of improved analytic techniques and the quality of imaging data. These advances are allowing researchers to develop population-level multivariate models of the functional brain representations underlying behavior, performance, clinical status and prognosis, and other outcomes. Population-based models can identify patterns of brain activity, or `signatures', that can predict behavior and decode mental states in new individuals, producing generalizable knowledge and highly reproducible maps. These signatures can capture behavior with large effect sizes, and can be used and tested across research groups. However, the potential of such signatures is limited by neuroanatomical constraints, in particular individual variation in functional brain anatomy. To circumvent this problem, current models are either applied only to individual participants, severely limiting generalizability, or force participants' data into anatomical reference spaces (atlases) that do not respect individual functional topology and boundaries. Here we seek to overcome this shortcoming by developing new topological models for inter-subject alignment, which register participants' functional brain maps to one another. This will increase effective spatial resolution, and more importantly allow us to explicitly analyze the spatial topology of functional maps make inferences on differences in activation location and shape across persons and psychological states. We will test and validate the methods using a purpose-designed experiment (n = 120) that includes two types of naturalistic narrative experiences (movies and audio stories) and tasks from three functional domains (pain, emotion, and cognition). The tasks are designed with several constraints in mind, including: (1) systematic coverage of cognitive, emotional, and sensory tasks matched in stimulus properties (e.g., stimulus duration); and (2) multiple levels of task demand within each task, to permit parametric modeling and prediction of demand levels. We will compare our new methods to existing methods based on out-of-sample effect sizes in predicting behavior and test-retest reliability. We will make the analytic methods, software, and dataset available to other researchers, along with a library of functional reference spaces for multiple psychological states. Project Narrative. We develop new methods for enhancing the development of models that can predict behavior, clinical status, and other outcomes using neuroimaging data. Successful development will help improve the translational impact of neuroimaging. It will also contribute to developing multidisciplinary neuroscience, by promoting the development of neural signatures for specific mental processes in humans with increased precision and specificity.",Individualized spatial topology in functional neuroimaging,9908089,R01EB026549,"['Affective', 'Anatomy', 'Atlases', 'Behavior', 'Brain', 'Charon', 'Clinical', 'Cognition', 'Cognitive', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Emotional', 'Emotions', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Human', 'Image', 'Individual', 'Individual Differences', 'Knowledge', 'Libraries', 'Location', 'Machine Learning', 'Maps', 'Mental Processes', 'Methods', 'Mind', 'Modeling', 'Neurosciences', 'Outcome', 'Pain', 'Participant', 'Pattern', 'Pattern Recognition', 'Performance', 'Persons', 'Population', 'Process', 'Property', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Sensory', 'Shapes', 'Specificity', 'Stimulus', 'Sum', 'System', 'Techniques', 'Testing', 'Training', 'Variant', 'Wages', 'Work', 'affective neuroscience', 'analytical method', 'base', 'behavior prediction', 'behavior test', 'cognitive neuroscience', 'design', 'experience', 'experimental study', 'flexibility', 'improved', 'individual variation', 'mental state', 'model development', 'movie', 'multidisciplinary', 'neurodevelopment', 'neuroimaging', 'neurophysiology', 'outcome forecast', 'population based', 'predictive modeling', 'psychologic', 'statistics', 'translational impact']",NIBIB,JOHNS HOPKINS UNIVERSITY,R01,2020,667005,-0.014672532828536984
"Individualized spatial topology in functional neuroimaging Project Summary. Neuroimaging is poised to take a substantial leap forward in understanding the neurophysiological underpinnings of human behavior, due to a combination of improved analytic techniques and the quality of imaging data. These advances are allowing researchers to develop population-level multivariate models of the functional brain representations underlying behavior, performance, clinical status and prognosis, and other outcomes. Population-based models can identify patterns of brain activity, or `signatures', that can predict behavior and decode mental states in new individuals, producing generalizable knowledge and highly reproducible maps. These signatures can capture behavior with large effect sizes, and can be used and tested across research groups. However, the potential of such signatures is limited by neuroanatomical constraints, in particular individual variation in functional brain anatomy. To circumvent this problem, current models are either applied only to individual participants, severely limiting generalizability, or force participants' data into anatomical reference spaces (atlases) that do not respect individual functional topology and boundaries. Here we seek to overcome this shortcoming by developing new topological models for inter-subject alignment, which register participants' functional brain maps to one another. This will increase effective spatial resolution, and more importantly allow us to explicitly analyze the spatial topology of functional maps make inferences on differences in activation location and shape across persons and psychological states. We will test and validate the methods using a purpose-designed experiment (n = 120) that includes two types of naturalistic narrative experiences (movies and audio stories) and tasks from three functional domains (pain, emotion, and cognition). The tasks are designed with several constraints in mind, including: (1) systematic coverage of cognitive, emotional, and sensory tasks matched in stimulus properties (e.g., stimulus duration); and (2) multiple levels of task demand within each task, to permit parametric modeling and prediction of demand levels. We will compare our new methods to existing methods based on out-of-sample effect sizes in predicting behavior and test-retest reliability. We will make the analytic methods, software, and dataset available to other researchers, along with a library of functional reference spaces for multiple psychological states. Project Narrative. We develop new methods for enhancing the development of models that can predict behavior, clinical status, and other outcomes using neuroimaging data. Successful development will help improve the translational impact of neuroimaging. It will also contribute to developing multidisciplinary neuroscience, by promoting the development of neural signatures for specific mental processes in humans with increased precision and specificity.",Individualized spatial topology in functional neuroimaging,9747296,R01EB026549,"['Affective', 'Anatomy', 'Atlases', 'Behavior', 'Brain', 'Charon', 'Clinical', 'Cognition', 'Cognitive', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Emotional', 'Emotions', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Human', 'Image', 'Individual', 'Individual Differences', 'Knowledge', 'Libraries', 'Location', 'Machine Learning', 'Maps', 'Mental Processes', 'Methods', 'Mind', 'Modeling', 'Neurosciences', 'Outcome', 'Pain', 'Participant', 'Pattern', 'Pattern Recognition', 'Performance', 'Persons', 'Population', 'Process', 'Property', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Sensory', 'Shapes', 'Specificity', 'Stimulus', 'Sum', 'System', 'Techniques', 'Testing', 'Training', 'Variant', 'Wages', 'Work', 'affective neuroscience', 'analytical method', 'base', 'behavior prediction', 'behavior test', 'cognitive neuroscience', 'design', 'experience', 'experimental study', 'flexibility', 'improved', 'individual variation', 'mental state', 'model development', 'movie', 'multidisciplinary', 'neurodevelopment', 'neuroimaging', 'neurophysiology', 'outcome forecast', 'population based', 'predictive modeling', 'psychologic', 'statistics', 'translational impact']",NIBIB,JOHNS HOPKINS UNIVERSITY,R01,2019,692050,-0.014672532828536984
"Individualized spatial topology in functional neuroimaging Project Summary. Neuroimaging is poised to take a substantial leap forward in understanding the neurophysiological underpinnings of human behavior, due to a combination of improved analytic techniques and the quality of imaging data. These advances are allowing researchers to develop population-level multivariate models of the functional brain representations underlying behavior, performance, clinical status and prognosis, and other outcomes. Population-based models can identify patterns of brain activity, or `signatures', that can predict behavior and decode mental states in new individuals, producing generalizable knowledge and highly reproducible maps. These signatures can capture behavior with large effect sizes, and can be used and tested across research groups. However, the potential of such signatures is limited by neuroanatomical constraints, in particular individual variation in functional brain anatomy. To circumvent this problem, current models are either applied only to individual participants, severely limiting generalizability, or force participants' data into anatomical reference spaces (atlases) that do not respect individual functional topology and boundaries. Here we seek to overcome this shortcoming by developing new topological models for inter-subject alignment, which register participants' functional brain maps to one another. This will increase effective spatial resolution, and more importantly allow us to explicitly analyze the spatial topology of functional maps make inferences on differences in activation location and shape across persons and psychological states. We will test and validate the methods using a purpose-designed experiment (n = 120) that includes two types of naturalistic narrative experiences (movies and audio stories) and tasks from three functional domains (pain, emotion, and cognition). The tasks are designed with several constraints in mind, including: (1) systematic coverage of cognitive, emotional, and sensory tasks matched in stimulus properties (e.g., stimulus duration); and (2) multiple levels of task demand within each task, to permit parametric modeling and prediction of demand levels. We will compare our new methods to existing methods based on out-of-sample effect sizes in predicting behavior and test-retest reliability. We will make the analytic methods, software, and dataset available to other researchers, along with a library of functional reference spaces for multiple psychological states. Project Narrative. We develop new methods for enhancing the development of models that can predict behavior, clinical status, and other outcomes using neuroimaging data. Successful development will help improve the translational impact of neuroimaging. It will also contribute to developing multidisciplinary neuroscience, by promoting the development of neural signatures for specific mental processes in humans with increased precision and specificity.",Individualized spatial topology in functional neuroimaging,9577381,R01EB026549,"['Affective', 'Anatomy', 'Atlases', 'Behavior', 'Brain', 'Charon', 'Clinical', 'Cognition', 'Cognitive', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Emotional', 'Emotions', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Human', 'Image', 'Individual', 'Individual Differences', 'Knowledge', 'Libraries', 'Location', 'Machine Learning', 'Maps', 'Mental Processes', 'Methods', 'Mind', 'Modeling', 'Neurosciences', 'Outcome', 'Pain', 'Participant', 'Pattern', 'Pattern Recognition', 'Performance', 'Persons', 'Population', 'Process', 'Property', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Sensory', 'Shapes', 'Specificity', 'Stimulus', 'Sum', 'System', 'Techniques', 'Testing', 'Training', 'Variant', 'Wages', 'Work', 'affective neuroscience', 'analytical method', 'base', 'behavior prediction', 'behavior test', 'cognitive neuroscience', 'design', 'experience', 'experimental study', 'flexibility', 'improved', 'individual variation', 'mental state', 'model development', 'movie', 'multidisciplinary', 'neurodevelopment', 'neuroimaging', 'neurophysiology', 'outcome forecast', 'population based', 'predictive modeling', 'psychologic', 'statistics', 'translational impact']",NIBIB,JOHNS HOPKINS UNIVERSITY,R01,2018,710896,-0.014672532828536984
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE MC-IU effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing, exploration, and download of different types of tissue and individual cell data. The CCF will use different visual interfaces in order to exploit human and machine intelligence to improve data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, biology, and biomedical data standards. The goal is to develop a highly accurate and extensible multidimensional spatial basemap of the human body with associated data overlays. This basemap will be designed for online exploration as an atlas of tissue maps composed of diverse cell types, developed in close collaboration with the HIVE MC-NYGC team. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across multiple levels (whole body, organ, tissue, cells). MC-IU will work in close collaboration with the HIVE Infrastructure and Engagement Component (IEC) and tools components (TCs) to connect and integrate further computational, analytical, visualization, and biometric resources driven by spatial context. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an extensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spatial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high-resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets",10148333,OT2OD026671,"['Anatomy', 'Artificial Intelligence', 'Atlases', 'Biology', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Collaborations', 'Communication', 'Data', 'Data Set', 'Ecosystem', 'Goals', 'Human', 'Human BioMolecular Atlas Program', 'Human body', 'Image', 'Individual', 'Infrastructure', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Organ', 'Resolution', 'Resources', 'Tissues', 'Visual', 'Visualization', 'Work', 'base', 'cell type', 'data exploration', 'data standards', 'design', 'human imaging', 'improved', 'interoperability', 'tool', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2020,1000000,-0.030892271596810524
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE MC-IU effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing, exploration, and download of different types of tissue and individual cell data. The CCF will use different visual interfaces in order to exploit human and machine intelligence to improve data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, biology, and biomedical data standards. The goal is to develop a highly accurate and extensible multidimensional spatial basemap of the human body with associated data overlays. This basemap will be designed for online exploration as an atlas of tissue maps composed of diverse cell types, developed in close collaboration with the HIVE MC-NYGC team. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across multiple levels (whole body, organ, tissue, cells). MC-IU will work in close collaboration with the HIVE Infrastructure and Engagement Component (IEC) and tools components (TCs) to connect and integrate further computational, analytical, visualization, and biometric resources driven by spatial context. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets",9919259,OT2OD026671,"['Anatomy', 'Artificial Intelligence', 'Atlases', 'Biology', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Collaborations', 'Communication', 'Data', 'Data Set', 'Ecosystem', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Infrastructure', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Organ', 'Resolution', 'Resources', 'Tissues', 'Visual', 'Work', 'base', 'cell type', 'design', 'human imaging', 'improved', 'interoperability', 'tool', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2019,600000,-0.03328627246420072
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9751297,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Consumption', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2019,77750,0.006563950869906974
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9583854,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2018,74515,0.006563950869906974
"Mental, measurement, and model complexity in neuroscience PROJECT SUMMARY Neuroscience is producing increasingly complex data sets, including measures and manipulations of sub- cellular, cellular, and multi-cellular mechanisms operating over multiple timescales and in the context of different behaviors and task conditions. These data sets pose several fundamental challenges. First, for a given data set, what are the relevant spatial, temporal, and computational scales in which the underlying information-processing dynamics are best understood? Second, what are the best ways to design and select models to account for these dynamics, given the inevitably limited, noisy, and uneven spatial and temporal sampling used to collect the data? Third, what can increasingly complex data sets, collected under increasingly complex conditions, tells us about how the brain itself processes complex information? The goal of this project is to develop and disseminate new, theoretically grounded methods to help researchers to overcome these challenges. Our primary hypothesis is that resolving, modeling, and interpreting relevant information- processing dynamics from complex data sets depends critically on approaches that are built upon understanding the notion of complexity itself. A key insight driving this proposal is that definitions of complexity that come from different fields, and often with different interpretations, in fact have a common mathematical foundation. This common foundation implies that different approaches, from direct analyses of empirical data to model fitting, can extract statistical features related to computational complexity that can be compared directly to each other and interpreted in the context of ideal-observer benchmarks. Starting with this idea, we will pursue three specific aims: 1) establish a common theoretical foundation for analyzing both data and model complexity; 2) develop practical, complexity-based tools for data analysis and model selection; and 3) establish the usefulness of complexity-based metrics for understanding how the brain processes complex information. Together, these Aims provide new theoretical and practical tools for understanding how the brain integrates information across large temporal and spatial scales, using formal, universal definitions of complexity to facilitate the analysis and interpretation of complex neural and behavioral data sets. PROJECT NARRATIVE The proposed work will establish new, theoretically grounded computational tools to help neuroscience researchers design and analyze studies of brain function. These tools, which will be made widely available to the neuroscience research community, will help support a broad range of studies of the brain, enhance scientific discovery, and promote rigor and reproducibility.","Mental, measurement, and model complexity in neuroscience",10002220,R01EB026945,"['Address', 'Algorithms', 'Automobile Driving', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Benchmarking', 'Brain', 'Characteristics', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Decision Making', 'Dimensions', 'Foundations', 'Goals', 'Guidelines', 'Human', 'Individual', 'Information Theory', 'Length', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Neurosciences', 'Neurosciences Research', 'Noise', 'Pattern', 'Physics', 'Process', 'Psyche structure', 'Reproducibility', 'Research Personnel', 'Rodent', 'Sampling', 'Series', 'Structure', 'System', 'Techniques', 'Time', 'Work', 'base', 'complex data ', 'computer science', 'computerized tools', 'data modeling', 'data streams', 'data tools', 'design', 'information processing', 'insight', 'nonhuman primate', 'relating to nervous system', 'statistics', 'theories', 'tool']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2020,20957,0.0055366405849924605
"Mental, measurement, and model complexity in neuroscience PROJECT SUMMARY Neuroscience is producing increasingly complex data sets, including measures and manipulations of sub- cellular, cellular, and multi-cellular mechanisms operating over multiple timescales and in the context of different behaviors and task conditions. These data sets pose several fundamental challenges. First, for a given data set, what are the relevant spatial, temporal, and computational scales in which the underlying information-processing dynamics are best understood? Second, what are the best ways to design and select models to account for these dynamics, given the inevitably limited, noisy, and uneven spatial and temporal sampling used to collect the data? Third, what can increasingly complex data sets, collected under increasingly complex conditions, tells us about how the brain itself processes complex information? The goal of this project is to develop and disseminate new, theoretically grounded methods to help researchers to overcome these challenges. Our primary hypothesis is that resolving, modeling, and interpreting relevant information- processing dynamics from complex data sets depends critically on approaches that are built upon understanding the notion of complexity itself. A key insight driving this proposal is that definitions of complexity that come from different fields, and often with different interpretations, in fact have a common mathematical foundation. This common foundation implies that different approaches, from direct analyses of empirical data to model fitting, can extract statistical features related to computational complexity that can be compared directly to each other and interpreted in the context of ideal-observer benchmarks. Starting with this idea, we will pursue three specific aims: 1) establish a common theoretical foundation for analyzing both data and model complexity; 2) develop practical, complexity-based tools for data analysis and model selection; and 3) establish the usefulness of complexity-based metrics for understanding how the brain processes complex information. Together, these Aims provide new theoretical and practical tools for understanding how the brain integrates information across large temporal and spatial scales, using formal, universal definitions of complexity to facilitate the analysis and interpretation of complex neural and behavioral data sets. PROJECT NARRATIVE The proposed work will establish new, theoretically grounded computational tools to help neuroscience researchers design and analyze studies of brain function. These tools, which will be made widely available to the neuroscience research community, will help support a broad range of studies of the brain, enhance scientific discovery, and promote rigor and reproducibility.","Mental, measurement, and model complexity in neuroscience",9789280,R01EB026945,"['Address', 'Algorithms', 'Automobile Driving', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Benchmarking', 'Brain', 'Characteristics', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Decision Making', 'Dimensions', 'Foundations', 'Goals', 'Guidelines', 'Human', 'Individual', 'Information Theory', 'Length', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Neurosciences', 'Neurosciences Research', 'Noise', 'Pattern', 'Physics', 'Process', 'Psyche structure', 'Reproducibility', 'Research Personnel', 'Rodent', 'Sampling', 'Series', 'Stream', 'Structure', 'System', 'Techniques', 'Time', 'Work', 'base', 'computer science', 'computerized tools', 'data modeling', 'design', 'information processing', 'insight', 'nonhuman primate', 'relating to nervous system', 'statistics', 'theories', 'tool']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2019,21171,0.0055366405849924605
"Mental, measurement, and model complexity in neuroscience PROJECT SUMMARY Neuroscience is producing increasingly complex data sets, including measures and manipulations of sub- cellular, cellular, and multi-cellular mechanisms operating over multiple timescales and in the context of different behaviors and task conditions. These data sets pose several fundamental challenges. First, for a given data set, what are the relevant spatial, temporal, and computational scales in which the underlying information-processing dynamics are best understood? Second, what are the best ways to design and select models to account for these dynamics, given the inevitably limited, noisy, and uneven spatial and temporal sampling used to collect the data? Third, what can increasingly complex data sets, collected under increasingly complex conditions, tells us about how the brain itself processes complex information? The goal of this project is to develop and disseminate new, theoretically grounded methods to help researchers to overcome these challenges. Our primary hypothesis is that resolving, modeling, and interpreting relevant information- processing dynamics from complex data sets depends critically on approaches that are built upon understanding the notion of complexity itself. A key insight driving this proposal is that definitions of complexity that come from different fields, and often with different interpretations, in fact have a common mathematical foundation. This common foundation implies that different approaches, from direct analyses of empirical data to model fitting, can extract statistical features related to computational complexity that can be compared directly to each other and interpreted in the context of ideal-observer benchmarks. Starting with this idea, we will pursue three specific aims: 1) establish a common theoretical foundation for analyzing both data and model complexity; 2) develop practical, complexity-based tools for data analysis and model selection; and 3) establish the usefulness of complexity-based metrics for understanding how the brain processes complex information. Together, these Aims provide new theoretical and practical tools for understanding how the brain integrates information across large temporal and spatial scales, using formal, universal definitions of complexity to facilitate the analysis and interpretation of complex neural and behavioral data sets. PROJECT NARRATIVE The proposed work will establish new, theoretically grounded computational tools to help neuroscience researchers design and analyze studies of brain function. These tools, which will be made widely available to the neuroscience research community, will help support a broad range of studies of the brain, enhance scientific discovery, and promote rigor and reproducibility.","Mental, measurement, and model complexity in neuroscience",9615324,R01EB026945,"['Address', 'Algorithms', 'Automobile Driving', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Benchmarking', 'Brain', 'Characteristics', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Decision Making', 'Dimensions', 'Foundations', 'Goals', 'Guidelines', 'Human', 'Individual', 'Information Theory', 'Length', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Neurosciences', 'Neurosciences Research', 'Noise', 'Pattern', 'Physics', 'Process', 'Psyche structure', 'Reproducibility', 'Research Personnel', 'Rodent', 'Sampling', 'Series', 'Stream', 'System', 'Techniques', 'Time', 'Work', 'base', 'computer science', 'computerized tools', 'data modeling', 'design', 'information processing', 'insight', 'nonhuman primate', 'relating to nervous system', 'statistics', 'theories', 'tool']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2018,342000,0.0055366405849924605
"A TOF, DOI, MRI compatible PET detector to support sub-millimeter neuroPET imaging Abstract The overall goal of this research is to develop next generation positron emission tomography (PET) detector technology to support non-invasive, quantitative brain imaging at spatial and temporal resolutions currently not achievable with human neuro-PET systems. The developed PET detector technology will also be compatible with operation in an MRI system. The proposed research is targeted to the NIH Brain Research through Advancing Innovative Neurotechnologies (BRAIN) initiative. Human brain imaging with PET/MRI will be an essential tool in neuroscience studies to ""Develop innovative technologies to understand the human brain and treat its disorders; create and support integrated brain research networks."" The key advancement that we introduce is a PET detector with <100 psec time-of-flight (TOF) PET coincidence resolution, <2 mm continuous depth of interaction (DOI) positioning and intrinsic detector spatial resolution to support <1 mm PET image resolution throughout the system imaging field of view (FOV). Detector modules have been designed that individually achieve these performance metrics; however, no detector module has been designed that supports all of them. To make disruptive advancements in neuro-imaging using PET, one must push the image spatial resolution (i.e., currently 2-3 mm image resolution) as well as coincidence timing resolution and also be MRI compatible. The impact of this project is that we will advance the state of the art in all of these critical performance areas. We will achieve these goals by first understanding the role that different PET detector performance parameters have on task-based figures of merit for neuroPET imaging. We will investigate how both TOF and image resolution impact figure of merit performance for estimation, detection and characterization imaging tasks. Monte Carlo simulation will be used along with both object-based (i.e., mathematical) and anthropomorphic digital phantoms. Next we will optimize SiPM device selection, electronics and detector geometry for <100 psec TOF coincidence timing, 1 mm intrinsic spatial resolution and <2 mm DOI positioning resolution. We will build and characterize a prototype PET detector module utilizing a novel dual- sided slat crystal detector design. To advance coincidence timing performance we will investigate the use of machine learning to estimate the arrival time of detected events. Finally, we will optimize the detector design for MRI compatibility. We will fully test and characterize performance of our prototype detector on the benchtop and in a MRI scanner while running clinical MRI pulse sequences. At the end of this developmental project we will be in position to build a state of the art, MRI compatible, TOF, DOI PET imaging system. Narrative The overall goal of this work is to develop detector imaging technology that will enable new research into the development, function and aging of the human brain. This technology will allow functional imaging of the brain at higher spatial resolution and higher coincidence timing resolution than previously achieved and also facilitate simultaneous multi-modality imaging (i.e., PET/MRI) at these new levels of performance.","A TOF, DOI, MRI compatible PET detector to support sub-millimeter neuroPET imaging",9791188,R01EB026964,"['Aging', 'Algorithms', 'Area', 'Benchmarking', 'Brain', 'Brain imaging', 'Clinical', 'Crystallization', 'Data', 'Detection', 'Development', 'Devices', 'Dimensions', 'Disease', 'Electronics', 'Elements', 'Evaluation', 'Event', 'Functional Imaging', 'Geometry', 'Goals', 'Guidelines', 'Human', 'Image', 'Imaging technology', 'Individual', 'Investigation', 'Laboratory Research', 'Lead', 'Length', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Monte Carlo Method', 'Multimodal Imaging', 'Neurosciences', 'Optics', 'Outcome', 'Performance', 'Physiologic pulse', 'Positioning Attribute', 'Positron-Emission Tomography', 'Property', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Running', 'Side', 'Silicon', 'Surface', 'System', 'Technology', 'Testing', 'Time', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Washington', 'Work', 'base', 'brain research', 'design', 'detector', 'digital', 'imaging detector', 'imaging system', 'improved', 'innovative neurotechnologies', 'innovative technologies', 'learning strategy', 'neuroimaging', 'next generation', 'novel', 'operation', 'prototype', 'temporal measurement', 'tool']",NIBIB,UNIVERSITY OF WASHINGTON,R01,2019,22992,0.043241658581166485
"A TOF, DOI, MRI compatible PET detector to support sub-millimeter neuroPET imaging Abstract The overall goal of this research is to develop next generation positron emission tomography (PET) detector technology to support non-invasive, quantitative brain imaging at spatial and temporal resolutions currently not achievable with human neuro-PET systems. The developed PET detector technology will also be compatible with operation in an MRI system. The proposed research is targeted to the NIH Brain Research through Advancing Innovative Neurotechnologies (BRAIN) initiative. Human brain imaging with PET/MRI will be an essential tool in neuroscience studies to ""Develop innovative technologies to understand the human brain and treat its disorders; create and support integrated brain research networks."" The key advancement that we introduce is a PET detector with <100 psec time-of-flight (TOF) PET coincidence resolution, <2 mm continuous depth of interaction (DOI) positioning and intrinsic detector spatial resolution to support <1 mm PET image resolution throughout the system imaging field of view (FOV). Detector modules have been designed that individually achieve these performance metrics; however, no detector module has been designed that supports all of them. To make disruptive advancements in neuro-imaging using PET, one must push the image spatial resolution (i.e., currently 2-3 mm image resolution) as well as coincidence timing resolution and also be MRI compatible. The impact of this project is that we will advance the state of the art in all of these critical performance areas. We will achieve these goals by first understanding the role that different PET detector performance parameters have on task-based figures of merit for neuroPET imaging. We will investigate how both TOF and image resolution impact figure of merit performance for estimation, detection and characterization imaging tasks. Monte Carlo simulation will be used along with both object-based (i.e., mathematical) and anthropomorphic digital phantoms. Next we will optimize SiPM device selection, electronics and detector geometry for <100 psec TOF coincidence timing, 1 mm intrinsic spatial resolution and <2 mm DOI positioning resolution. We will build and characterize a prototype PET detector module utilizing a novel dual- sided slat crystal detector design. To advance coincidence timing performance we will investigate the use of machine learning to estimate the arrival time of detected events. Finally, we will optimize the detector design for MRI compatibility. We will fully test and characterize performance of our prototype detector on the benchtop and in a MRI scanner while running clinical MRI pulse sequences. At the end of this developmental project we will be in position to build a state of the art, MRI compatible, TOF, DOI PET imaging system. Narrative The overall goal of this work is to develop detector imaging technology that will enable new research into the development, function and aging of the human brain. This technology will allow functional imaging of the brain at higher spatial resolution and higher coincidence timing resolution than previously achieved and also facilitate simultaneous multi-modality imaging (i.e., PET/MRI) at these new levels of performance.","A TOF, DOI, MRI compatible PET detector to support sub-millimeter neuroPET imaging",9616697,R01EB026964,"['Aging', 'Algorithms', 'Area', 'Benchmarking', 'Brain', 'Brain imaging', 'Clinical', 'Crystallization', 'Data', 'Detection', 'Development', 'Devices', 'Dimensions', 'Disease', 'Electronics', 'Elements', 'Evaluation', 'Event', 'Functional Imaging', 'Geometry', 'Goals', 'Guidelines', 'Human', 'Image', 'Imaging technology', 'Individual', 'Investigation', 'Laboratory Research', 'Lead', 'Length', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Monte Carlo Method', 'Multimodal Imaging', 'Neurosciences', 'Optics', 'Outcome', 'Performance', 'Physiologic pulse', 'Positioning Attribute', 'Positron-Emission Tomography', 'Property', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Running', 'Side', 'Silicon', 'Surface', 'System', 'Technology', 'Testing', 'Time', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Washington', 'Work', 'base', 'brain research', 'design', 'detector', 'digital', 'imaging detector', 'imaging system', 'improved', 'innovative neurotechnologies', 'innovative technologies', 'learning strategy', 'neuroimaging', 'next generation', 'novel', 'operation', 'prototype', 'temporal measurement', 'tool']",NIBIB,UNIVERSITY OF WASHINGTON,R01,2018,29442,0.043241658581166485
"Random Matrix Theory-Based Noise Removal in MRI PROJECT SUMMARY MRI is a widely-used imaging modality which offers unique soft-tissue contrast and provides a wealth of anatomical and functional information. However, MRI is inherently slow and signal-to-noise ratio (SNR)-limited, resulting in variable diagnostic image quality and limiting statistical power for research studies. Particularly clinically relevant SNR-starved applications are diffusion MRI (dMRI) and functional (fMRI) for surgical planning (e.g., in functional neurosurgery and in brain tumors). dMRI suffers from long scan times, low resolution and subject motion; BOLD fMRI response signal changes are only about 3% using 3T MRI. State-of-the-art denoising methods, based on image models or smoothing, result in partial-volume effects and loss of fine anatomical detail. We have identified an untapped reserve for significant noise reduction in clinically feasible MRI protocols resulting in SNR increase and Rician MRI noise floor decrease by factors of up to 5-fold, using a model-free noise reduction (denoising) and image reconstruction technique, based on random matrix theory. It does not rely on user-specific input, and outperforms state-of-the-art denoising methods. Our method allows us to identify and remove a pure thermal noise contribution in the principal component analysis (PCA) representation of an MRI data matrix. Remarkably, while noise enters randomly in each voxel's signal, its contribution to the principal components becomes deterministic, when signals from large number of voxels and inequivalent acquisitions (e.g., q-space, time-domain, coils) are combined, which allows us to identify and remove pure- noise components. The key to our MP-PCA method is acquisition redundancy, such that the bulk of the PCA spectrum is dominated by the noise, whose contribution can then be identified and removed. While we initially exploited redundancy in the dMRI q-space, our preliminary findings show it is also present in multi-coil arrays, and in the temporal domain of fMRI. The main goals of this study are: To develop and optimize the MP-PCA denoising framework at the level of multi-coil image reconstruction and to evaluate its accuracy and precision in dMRI (Aim 1); to evaluate its clinical utility for increasing dMRI resolution in functional neurosurgery, based on the ground-truth derived from MR-guided ultrasound intra-operative feedback (Aim 2); and to evaluate its clinical utility for decreasing fMRI scan time in preoperative planning of brain tumor resections (Aim 3). Fundamentally, this project will establish an objective framework to quantify the information content of different MRI modalities, by separating between the signal and the noise. Its applications to dMRI and fMRI, together with using multi-coil redundancy, will lead to maximal possible SNR, thereby reducing scan time, and improving resolution, precision, sensitivity and diagnostic utility of clinically relevant MRI protocols. PROJECT NARRATIVE MRI, while offering unique soft-tissue contrast, anatomical and functional information, remains inherently slow and signal-to-noise ratio (SNR)-starved, which limits its spatial resolution, lengthens scans, and makes them prone to motion artifacts, thus reducing diagnostic quality. We have identified an untapped reserve for significant noise reduction in clinically feasible MRI acquisitions resulting in SNR increase by up to 5-fold, using a model-free post-processing noise reduction technique, based on random matrix theory. The main goals of this study are to evaluate the robustness and utility of our noise-reduction and reconstruction framework in terms of increasing resolution and shortening scan time, as well as to translate it into functional neurosurgery and preoperative planning for brain tumors.",Random Matrix Theory-Based Noise Removal in MRI,10018721,R01EB027075,"['Address', 'Algorithms', 'Anatomy', 'Body Surface', 'Body Temperature', 'Brain', 'Brain Mapping', 'Brain Neoplasms', 'Cell Nucleus', 'Clinical', 'Data', 'Deep Brain Stimulation', 'Deterioration', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Diffusion Magnetic Resonance Imaging', 'Essential Tremor', 'Excision', 'Feedback', 'Floor', 'Focused Ultrasound', 'Functional Magnetic Resonance Imaging', 'Geometry', 'Goals', 'Image', 'Joints', 'Language', 'Length', 'Lesion', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Medial', 'Methods', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Motion', 'Noise', 'Nuclear', 'Operative Surgical Procedures', 'Outcome', 'Parkinson Disease', 'Patients', 'Plant Roots', 'Principal Component Analysis', 'Protocols documentation', 'Pyramidal Tracts', 'RF coil', 'Residual state', 'Resolution', 'Retrospective Studies', 'Sampling', 'Scanning', 'Signal Transduction', 'Spectrum Analysis', 'Techniques', 'Testing', 'Thalamic Nuclei', 'Therapeutic', 'Time', 'Translating', 'Ultrasonography', 'Variant', 'base', 'brain tumor resection', 'clinically relevant', 'denoising', 'functional MRI scan', 'healthy volunteer', 'image reconstruction', 'imaging modality', 'improved', 'nervous system disorder', 'neurosurgery', 'reconstruction', 'research study', 'response', 'soft tissue', 'theories']",NIBIB,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2020,712432,-0.010066488983410905
"Random Matrix Theory-Based Noise Removal in MRI PROJECT SUMMARY MRI is a widely-used imaging modality which offers unique soft-tissue contrast and provides a wealth of anatomical and functional information. However, MRI is inherently slow and signal-to-noise ratio (SNR)-limited, resulting in variable diagnostic image quality and limiting statistical power for research studies. Particularly clinically relevant SNR-starved applications are diffusion MRI (dMRI) and functional (fMRI) for surgical planning (e.g., in functional neurosurgery and in brain tumors). dMRI suffers from long scan times, low resolution and subject motion; BOLD fMRI response signal changes are only about 3% using 3T MRI. State-of-the-art denoising methods, based on image models or smoothing, result in partial-volume effects and loss of fine anatomical detail. We have identified an untapped reserve for significant noise reduction in clinically feasible MRI protocols resulting in SNR increase and Rician MRI noise floor decrease by factors of up to 5-fold, using a model-free noise reduction (denoising) and image reconstruction technique, based on random matrix theory. It does not rely on user-specific input, and outperforms state-of-the-art denoising methods. Our method allows us to identify and remove a pure thermal noise contribution in the principal component analysis (PCA) representation of an MRI data matrix. Remarkably, while noise enters randomly in each voxel's signal, its contribution to the principal components becomes deterministic, when signals from large number of voxels and inequivalent acquisitions (e.g., q-space, time-domain, coils) are combined, which allows us to identify and remove pure- noise components. The key to our MP-PCA method is acquisition redundancy, such that the bulk of the PCA spectrum is dominated by the noise, whose contribution can then be identified and removed. While we initially exploited redundancy in the dMRI q-space, our preliminary findings show it is also present in multi-coil arrays, and in the temporal domain of fMRI. The main goals of this study are: To develop and optimize the MP-PCA denoising framework at the level of multi-coil image reconstruction and to evaluate its accuracy and precision in dMRI (Aim 1); to evaluate its clinical utility for increasing dMRI resolution in functional neurosurgery, based on the ground-truth derived from MR-guided ultrasound intra-operative feedback (Aim 2); and to evaluate its clinical utility for decreasing fMRI scan time in preoperative planning of brain tumor resections (Aim 3). Fundamentally, this project will establish an objective framework to quantify the information content of different MRI modalities, by separating between the signal and the noise. Its applications to dMRI and fMRI, together with using multi-coil redundancy, will lead to maximal possible SNR, thereby reducing scan time, and improving resolution, precision, sensitivity and diagnostic utility of clinically relevant MRI protocols. PROJECT NARRATIVE MRI, while offering unique soft-tissue contrast, anatomical and functional information, remains inherently slow and signal-to-noise ratio (SNR)-starved, which limits its spatial resolution, lengthens scans, and makes them prone to motion artifacts, thus reducing diagnostic quality. We have identified an untapped reserve for significant noise reduction in clinically feasible MRI acquisitions resulting in SNR increase by up to 5-fold, using a model-free post-processing noise reduction technique, based on random matrix theory. The main goals of this study are to evaluate the robustness and utility of our noise-reduction and reconstruction framework in terms of increasing resolution and shortening scan time, as well as to translate it into functional neurosurgery and preoperative planning for brain tumors.",Random Matrix Theory-Based Noise Removal in MRI,9819423,R01EB027075,"['Address', 'Algorithms', 'Anatomy', 'Body Surface', 'Body Temperature', 'Brain', 'Brain Mapping', 'Brain Neoplasms', 'Cell Nucleus', 'Clinical', 'Data', 'Deep Brain Stimulation', 'Deterioration', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Diffusion Magnetic Resonance Imaging', 'Essential Tremor', 'Excision', 'Feedback', 'Floor', 'Focused Ultrasound', 'Functional Magnetic Resonance Imaging', 'Geometry', 'Goals', 'Image', 'Joints', 'Language', 'Length', 'Lesion', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Medial', 'Methods', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Motion', 'Noise', 'Nuclear', 'Operative Surgical Procedures', 'Outcome', 'Parkinson Disease', 'Patients', 'Plant Roots', 'Principal Component Analysis', 'Protocols documentation', 'Pyramidal Tracts', 'RF coil', 'Residual state', 'Resolution', 'Retrospective Studies', 'Sampling', 'Scanning', 'Signal Transduction', 'Spectrum Analysis', 'Techniques', 'Testing', 'Thalamic Nuclei', 'Therapeutic', 'Time', 'Translating', 'Ultrasonography', 'Variant', 'base', 'brain tumor resection', 'clinically relevant', 'denoising', 'functional MRI scan', 'healthy volunteer', 'image reconstruction', 'imaging modality', 'improved', 'nervous system disorder', 'neurosurgery', 'reconstruction', 'research study', 'response', 'soft tissue', 'theories']",NIBIB,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2019,684421,-0.010066488983410905
"Development of Multi-Compartment MR-Fingerprinting for Subvoxel Estimation of Quantitative Tissue Biomarkers Project Summary Magnetic resonance fingerprinting (MRF) has been proposed as a technique to quantify tissue parameters, such as T1 and T2 relaxation times, which are biomarkers for various pathologies. One assumption of MRF is that the signal in each voxel is generated by exactly one set of tissue parameters. Due to MRI resolution at the range of millimeter in combination with the cellular structure of biological tissue, each voxel consists of multiple tissue compartments. An over-simplified single compartment model results in apparent relaxation times that are influenced by the relaxation times and the fractional proton densities of all contributing compartments. This can lead to a misinterpretation of signal changes. For example, in diseases that causes demyelination in white matter (Multiple Sclerosis, Dementia), a reduction of the myelin water fraction would result in a misleading change of the apparent relaxation time of the voxel. We propose a multi-compartment MRF method that allows to identify multiple tissue contributions within a voxel, including the fractional proton density (PD) of different compartments. Our machine learning based approach automatically identifies the number of compartments within each voxel that can be identified with the available SNR in that voxel. We will correct for partial-volume effects at the borders of two types of tissues, as well as analyze tissue microstructure. For the second case our learned model will also include chemical exchange between compartments. After an initial validation phase using numerical simulations, we will first perform MRF scans of dedicated 3D printed phantoms with multiple compartments. Our quality criterion is successful estimation of all simulated tissue compartments for all voxels with a relative error of less than 5% to the ground truth. We will then perform in-vivo MRF measurements of healthy volunteers (n=5). We will generate synthetic FLAIR and MP-RAGE contrasts from parameter maps estimated with conventional and the proposed multi-compartment MRF technique. We will compare them with currently used clinical contrasts acquired using established pulse sequences and validate the performance of our approach by measuring the cortical thickness. Further, we will validate the performance for microstructure composition in white matter. Our hypothesis is that it will be possible separate the compartments for myelin, intra- and extra-cellular water and compare the results to ex-vivo data found in literature. In summary, the methods developed in this R21 proposal will provide a novel technique to accurately and reproducibly identify biomarkers beyond the resolution of a voxel. It will allow to identify changes in tissue composition and fractional proton density at the microstructure level. Project Narrative The overarching goal of this proposal is to establish a MR imaging technique for quantifying biomarkers at the sub-voxel level. We will develop a machine learning based MR fingerprinting image reconstruction that enables the identification and characterization of multiple compartments within a voxel in terms of fractional proton density and relaxation times. We will generate accurate synthetic fluid suppressed images such as the FLAIR and MP- RAGE with accurate partial volume behavior and analyze the myelin water fraction and the corresponding relaxation times in white matter.",Development of Multi-Compartment MR-Fingerprinting for Subvoxel Estimation of Quantitative Tissue Biomarkers,10016296,R21EB027241,"['3D Print', 'Address', 'Behavior', 'Biological', 'Biological Markers', 'Cellular Structures', 'Cerebrospinal Fluid', 'Chemicals', 'Clinical', 'Complex', 'Computer-Assisted Diagnosis', 'Data', 'Data Set', 'Dementia', 'Demyelinations', 'Development', 'Diagnosis', 'Dictionary', 'Dimensions', 'Disease', 'Elements', 'Fingerprint', 'Goals', 'Image', 'Imaging Techniques', 'Lead', 'Learning', 'Liquid substance', 'Literature', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Multicenter Studies', 'Multiple Sclerosis', 'Myelin', 'Neurodegenerative Disorders', 'Noise', 'Pathology', 'Performance', 'Phase', 'Physics', 'Physiologic pulse', 'Procedures', 'Protons', 'Relaxation', 'Reproducibility', 'Resolution', 'Scanning', 'Signal Transduction', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Tissues', 'Training', 'Validation', 'Variant', 'Water', 'base', 'clinical translation', 'conditioning', 'deep neural network', 'density', 'gray matter', 'healthy volunteer', 'image reconstruction', 'imaging system', 'improved', 'in vivo', 'magnetic resonance imaging biomarker', 'millimeter', 'neuroimaging', 'noninvasive diagnosis', 'novel', 'physical process', 'reconstruction', 'simulation', 'soft tissue', 'tissue biomarkers', 'white matter']",NIBIB,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R21,2020,252175,-0.015754727476764274
"Development of Multi-Compartment MR-Fingerprinting for Subvoxel Estimation of Quantitative Tissue Biomarkers Project Summary Magnetic resonance fingerprinting (MRF) has been proposed as a technique to quantify tissue parameters, such as T1 and T2 relaxation times, which are biomarkers for various pathologies. One assumption of MRF is that the signal in each voxel is generated by exactly one set of tissue parameters. Due to MRI resolution at the range of millimeter in combination with the cellular structure of biological tissue, each voxel consists of multiple tissue compartments. An over-simplified single compartment model results in apparent relaxation times that are influenced by the relaxation times and the fractional proton densities of all contributing compartments. This can lead to a misinterpretation of signal changes. For example, in diseases that causes demyelination in white matter (Multiple Sclerosis, Dementia), a reduction of the myelin water fraction would result in a misleading change of the apparent relaxation time of the voxel. We propose a multi-compartment MRF method that allows to identify multiple tissue contributions within a voxel, including the fractional proton density (PD) of different compartments. Our machine learning based approach automatically identifies the number of compartments within each voxel that can be identified with the available SNR in that voxel. We will correct for partial-volume effects at the borders of two types of tissues, as well as analyze tissue microstructure. For the second case our learned model will also include chemical exchange between compartments. After an initial validation phase using numerical simulations, we will first perform MRF scans of dedicated 3D printed phantoms with multiple compartments. Our quality criterion is successful estimation of all simulated tissue compartments for all voxels with a relative error of less than 5% to the ground truth. We will then perform in-vivo MRF measurements of healthy volunteers (n=5). We will generate synthetic FLAIR and MP-RAGE contrasts from parameter maps estimated with conventional and the proposed multi-compartment MRF technique. We will compare them with currently used clinical contrasts acquired using established pulse sequences and validate the performance of our approach by measuring the cortical thickness. Further, we will validate the performance for microstructure composition in white matter. Our hypothesis is that it will be possible separate the compartments for myelin, intra- and extra-cellular water and compare the results to ex-vivo data found in literature. In summary, the methods developed in this R21 proposal will provide a novel technique to accurately and reproducibly identify biomarkers beyond the resolution of a voxel. It will allow to identify changes in tissue composition and fractional proton density at the microstructure level. Project Narrative The overarching goal of this proposal is to establish a MR imaging technique for quantifying biomarkers at the sub-voxel level. We will develop a machine learning based MR fingerprinting image reconstruction that enables the identification and characterization of multiple compartments within a voxel in terms of fractional proton density and relaxation times. We will generate accurate synthetic fluid suppressed images such as the FLAIR and MP- RAGE with accurate partial volume behavior and analyze the myelin water fraction and the corresponding relaxation times in white matter.",Development of Multi-Compartment MR-Fingerprinting for Subvoxel Estimation of Quantitative Tissue Biomarkers,9825278,R21EB027241,"['3D Print', 'Address', 'Behavior', 'Biological', 'Biological Markers', 'Cellular Structures', 'Cerebrospinal Fluid', 'Chemicals', 'Clinical', 'Complex', 'Computer-Assisted Diagnosis', 'Data', 'Data Set', 'Dementia', 'Demyelinations', 'Development', 'Diagnosis', 'Dictionary', 'Dimensions', 'Disease', 'Elements', 'Fingerprint', 'Goals', 'Image', 'Imaging Techniques', 'Lead', 'Learning', 'Liquid substance', 'Literature', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Multicenter Studies', 'Multiple Sclerosis', 'Myelin', 'Neurodegenerative Disorders', 'Noise', 'Pathology', 'Performance', 'Phase', 'Physics', 'Physiologic pulse', 'Procedures', 'Protons', 'Relaxation', 'Reproducibility', 'Resolution', 'Scanning', 'Signal Transduction', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Tissues', 'Training', 'Validation', 'Variant', 'Water', 'base', 'clinical translation', 'conditioning', 'deep neural network', 'density', 'gray matter', 'healthy volunteer', 'image reconstruction', 'imaging system', 'improved', 'in vivo', 'magnetic resonance imaging biomarker', 'millimeter', 'neuroimaging', 'noninvasive diagnosis', 'novel', 'physical process', 'reconstruction', 'simulation', 'soft tissue', 'tissue biomarkers', 'white matter']",NIBIB,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R21,2019,209851,-0.015754727476764274
"Understanding the impact of environmental disruption in biological timing systems through signal processing. Project Summary/Abstract.  Life on Earth evolved to take time cues from the Sun. Consequently, most or all cells in the mammalian body use genetic feedback loops to time their daily (circadian) rhythms. When a person or any mammal sees light, that winds an orchestrating circadian brain clock in the hypothalamic suprachiasmatic nucleus (SCN). The SCN in turn helps keep the myriad other tissue and endocrine rhythms in synchrony, enabling health. The modern environment is highly disruptive to this internal synchrony. Light at night from cell phones or urban light pollution, and social impositions like school start times or rotating work shifts all act as “temporal pollution,” causing loss of internal synchrony. The more severe the desynchrony, the higher the risk for a broad range of diseases, including obesity, cancer, infertility, depression and ultimately cognitive decline. Without knowing how these systems normally maintain synchrony or which systems are normally synchronized, it is hard to understand what happens in desynchrony to degrade health. This problem is complicated by the fact that some biological systems have ultradian (every few hours) and infradian (every few days) cycles in addition to circadian cycles. The hypothalamo-pituitary-adrenal axis (HPA) generates ultradian rhythms through negative feedback, but also shows a strong circadian cycle; the hypothalamo-pituitary-gonadal axis (HPG) shows the same negative feedback ultradian activity, circadian rhythmicity, and also infradian rhythms of ovulation and spermatogenesis. These two axes are regulated by the SCN. Recent work indicates that there is cross-talk between these axes, and that their hormonal outputs - corticosterone, and estradiol (in females) and testosterone (in males), respectively – work to synchronize extra-SCN tissues and behavioral rhythms of feeding and drinking (FaD). Finally, the SCN, HPA, and HPG axes all affect core body temperature (CBT), so that high temporal resolution recordings of CBT contain information about the cycling and synchrony of these systems across time scales.  There are three aims to this proposal, using rats as a model system: 1) Test at high temporal resolution the effects of changes to the HPA axis, HPG axis, and SCN on CBT. 2) Use these relationships to build a model that can back-predict the state of the HPA axis, HPG axis, and SCN from a high temporal resolution CBT record of a given individual. 3) Expose rats to environmental temporal disruption in the form of a 6 h “jetlag” phase advance of the light cycle, and use the model to predict the response across these systems at 1-minute temporal resolution. This work will employ within-animal comparisons before and after surgical and pharmacological manipulations of rats whose FaD, activity, and CBT are captured continuously at 1-minute resolution. These data will be analyzed using signal-processing and machine learning to define patterns and relationships. The resulting model will allow minimally-invasive exploration of environmental disruption across physiological systems in real time. The model will be used to quantify synchrony as it is disrupted and re-emerges, identifying markers for risk or resilience, and generating hypotheses for future work into preventive strategies and treatments. Project Narrative. Artificial lights and social obligation cause people living with modern infrastructure to suffer a loss of synchrony across their organs, which evolved to track the stable day and year light cycles. We know about internal synchrony mostly by the diseases that arise from its loss – everything from cancer to obesity, depression, and infertility. This work will develop a system for tracking the cycles of many body-systems at the same time with minute-to-minute accuracy, allowing rapid detection of desynchrony, and a potential way to study how synchrony works normally, and why its disruption by the modern environment causes disease.",Understanding the impact of environmental disruption in biological timing systems through signal processing.,9386306,K99ES027509,"['Address', 'Adrenal Glands', 'Affect', 'Animals', 'Automobile Driving', 'Back', 'Behavioral', 'Biological', 'Biological Models', 'Body Temperature', 'Body Temperature Changes', 'Brain', 'Cells', 'Cellular Phone', 'Chronic', 'Circadian Rhythms', 'Corticosterone', 'Coupling', 'Cues', 'Darkness', 'Data', 'Development', 'Diabetes Mellitus', 'Disease', 'Dose', 'Dysmenorrhea', 'Dyspepsia', 'Endocrine', 'Endocrine system', 'Environment', 'Environmental Impact', 'Estradiol', 'Feedback', 'Female', 'Frequencies', 'Future', 'Genetic', 'Glucocorticoids', 'Health', 'Hormonal', 'Hormonal Change', 'Hour', 'Human', 'Hypothalamic structure', 'Impaired cognition', 'Impairment', 'Implant', 'Individual', 'Infertility', 'Inflammation', 'Investigation', 'Jet Lag Syndrome', 'Life', 'Light', 'Lighting', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Measures', 'Mental Depression', 'Modeling', 'Modernization', 'Monitor', 'Myocardial Infarction', 'Obesity', 'Operative Surgical Procedures', 'Organ', 'Orphan', 'Output', 'Ovulation', 'Pattern', 'Periodicity', 'Persons', 'Pharmacology', 'Phase', 'Physiological', 'Physiology', 'Pituitary-Adrenal System', 'Planet Earth', 'Pollution', 'Prevention strategy', 'Preventive treatment', 'Rattus', 'Records', 'Regulation', 'Research Infrastructure', 'Resolution', 'Risk', 'Risk Marker', 'Sampling', 'Schools', 'Shapes', 'Signal Transduction', 'Social Obligations', 'Spermatogenesis', 'Stress', 'Stroke', 'Structure', 'System', 'Testing', 'Testosterone', 'The Sun', 'Time', 'Tissues', 'Wireless Technology', 'Work', 'base', 'biological systems', 'body system', 'comparative', 'drinking', 'feeding', 'high risk', 'male', 'mathematical model', 'minimally invasive', 'pituitary gonadal axis', 'predicting response', 'predictive modeling', 'rapid detection', 'reconstruction', 'resilience', 'response', 'shift work', 'signal processing', 'social', 'suprachiasmatic nucleus', 'targeted treatment', 'temporal measurement', 'time use']",NIEHS,UNIVERSITY OF CALIFORNIA BERKELEY,K99,2017,98712,-0.039069993794191966
"Understanding the impact of environmental disruption in biological timing systems through signal processing. Project Summary/Abstract.  Life on Earth evolved to take time cues from the Sun. Consequently, most or all cells in the mammalian body use genetic feedback loops to time their daily (circadian) rhythms. When a person or any mammal sees light, that winds an orchestrating circadian brain clock in the hypothalamic suprachiasmatic nucleus (SCN). The SCN in turn helps keep the myriad other tissue and endocrine rhythms in synchrony, enabling health. The modern environment is highly disruptive to this internal synchrony. Light at night from cell phones or urban light pollution, and social impositions like school start times or rotating work shifts all act as “temporal pollution,” causing loss of internal synchrony. The more severe the desynchrony, the higher the risk for a broad range of diseases, including obesity, cancer, infertility, depression and ultimately cognitive decline. Without knowing how these systems normally maintain synchrony or which systems are normally synchronized, it is hard to understand what happens in desynchrony to degrade health. This problem is complicated by the fact that some biological systems have ultradian (every few hours) and infradian (every few days) cycles in addition to circadian cycles. The hypothalamo-pituitary-adrenal axis (HPA) generates ultradian rhythms through negative feedback, but also shows a strong circadian cycle; the hypothalamo-pituitary-gonadal axis (HPG) shows the same negative feedback ultradian activity, circadian rhythmicity, and also infradian rhythms of ovulation and spermatogenesis. These two axes are regulated by the SCN. Recent work indicates that there is cross-talk between these axes, and that their hormonal outputs - corticosterone, and estradiol (in females) and testosterone (in males), respectively – work to synchronize extra-SCN tissues and behavioral rhythms of feeding and drinking (FaD). Finally, the SCN, HPA, and HPG axes all affect core body temperature (CBT), so that high temporal resolution recordings of CBT contain information about the cycling and synchrony of these systems across time scales.  There are three aims to this proposal, using rats as a model system: 1) Test at high temporal resolution the effects of changes to the HPA axis, HPG axis, and SCN on CBT. 2) Use these relationships to build a model that can back-predict the state of the HPA axis, HPG axis, and SCN from a high temporal resolution CBT record of a given individual. 3) Expose rats to environmental temporal disruption in the form of a 6 h “jetlag” phase advance of the light cycle, and use the model to predict the response across these systems at 1-minute temporal resolution. This work will employ within-animal comparisons before and after surgical and pharmacological manipulations of rats whose FaD, activity, and CBT are captured continuously at 1-minute resolution. These data will be analyzed using signal-processing and machine learning to define patterns and relationships. The resulting model will allow minimally-invasive exploration of environmental disruption across physiological systems in real time. The model will be used to quantify synchrony as it is disrupted and re-emerges, identifying markers for risk or resilience, and generating hypotheses for future work into preventive strategies and treatments. Project Narrative. Artificial lights and social obligation cause people living with modern infrastructure to suffer a loss of synchrony across their organs, which evolved to track the stable day and year light cycles. We know about internal synchrony mostly by the diseases that arise from its loss – everything from cancer to obesity, depression, and infertility. This work will develop a system for tracking the cycles of many body-systems at the same time with minute-to-minute accuracy, allowing rapid detection of desynchrony, and a potential way to study how synchrony works normally, and why its disruption by the modern environment causes disease.",Understanding the impact of environmental disruption in biological timing systems through signal processing.,9567170,K99ES027509,"['Address', 'Adrenal Glands', 'Affect', 'Animals', 'Automobile Driving', 'Back', 'Behavioral', 'Biological', 'Biological Models', 'Body Temperature', 'Body Temperature Changes', 'Brain', 'Cells', 'Cellular Phone', 'Chronic', 'Circadian Rhythms', 'Corticosterone', 'Coupling', 'Cues', 'Data', 'Development', 'Diabetes Mellitus', 'Disease', 'Dose', 'Dysmenorrhea', 'Dyspepsia', 'Endocrine', 'Endocrine system', 'Environment', 'Environmental Impact', 'Estradiol', 'Feedback', 'Female', 'Frequencies', 'Future', 'Genetic', 'Glucocorticoids', 'Health', 'Hormonal', 'Hormonal Change', 'Hour', 'Human', 'Hypothalamic structure', 'Impaired cognition', 'Impairment', 'Implant', 'Individual', 'Infertility', 'Inflammation', 'Investigation', 'Jet Lag Syndrome', 'Life', 'Light', 'Lighting', 'Machine Learning', 'Malignant Neoplasms', 'Mammals', 'Measures', 'Mental Depression', 'Modeling', 'Modernization', 'Monitor', 'Myocardial Infarction', 'Obesity', 'Operative Surgical Procedures', 'Organ', 'Orphan', 'Output', 'Ovulation', 'Pattern', 'Periodicity', 'Persons', 'Pharmacology', 'Phase', 'Physiological', 'Physiology', 'Pituitary-Adrenal System', 'Planet Earth', 'Pollution', 'Prevention strategy', 'Preventive treatment', 'Rattus', 'Records', 'Regulation', 'Research Infrastructure', 'Resolution', 'Risk', 'Risk Marker', 'Sampling', 'Schools', 'Shapes', 'Signal Transduction', 'Social Obligations', 'Spermatogenesis', 'Stress', 'Stroke', 'Structure', 'System', 'Testing', 'Testosterone', 'The Sun', 'Time', 'Tissues', 'Wireless Technology', 'Work', 'base', 'biological systems', 'body system', 'comparative', 'drinking', 'feeding', 'high risk', 'male', 'mathematical model', 'minimally invasive', 'pituitary gonadal axis', 'predicting response', 'predictive modeling', 'rapid detection', 'reconstruction', 'resilience', 'response', 'shift work', 'signal processing', 'social', 'suprachiasmatic nucleus', 'targeted treatment', 'temporal measurement', 'time use']",NIEHS,UNIVERSITY OF CALIFORNIA BERKELEY,K99,2018,104094,-0.039069993794191966
"Automatic Thoracic Organ Segmentation Tool for Radiation Treatment Planning of Cancers in Thoracic Region ABSTRACT As early detection and better treatment have increased cancer patient survival rates, the importance of protecting normal organs during radiation treatment is drawing more attention. Cancers in the thoracic region, which include lung, esophageal, thymus, mesothelioma and breast cancers, are among the most pervasive and deadly cancers. The protection of normal thoracic organs including lungs, heart, esophagus and spinal cord is critical in reducing long term toxicity in such cancers. To avoid excessively high radiation doses to such organs-at-risk (OARs), they are required to be correctly segmented from simulation computed tomography (CT) scans during radiation treatment planning to get an accurate dosage distribution. Despite tremendous effort into the development of semi- or fully-automatic segmentation solutions, current automated segmentation software, mostly using the atlas-based methods, has not yet reached the level of accuracy and robustness required for clinical usage. Therefore, in current practice, significant manual efforts are still required in the OAR segmentation process. Manual contouring suffers from inter- and intra-observer variability as well as institutional variability where different sites adopt distinct contouring atlases and labeling criteria and thus leads to inaccuracy and variability in OAR segmentation. When OARs are very close to the treatment target, segmentation errors as small as a few millimeters can have a statistically significant impact on dosimetry distribution and outcome. In addition, it is also costly and time consuming as it can take 1-2 hours of a clinicians’ time to segment major thoracic organs due to the large number of axial slices required. The associated human efforts would significantly increase if adaptive radiation therapy (ART) is used as OARs from two or more simulation CT scans need to be segmented to adjust treatment plans. In recent years, the rapid development of deep learning methods has revolutionized many computer-vision areas and the adoption of deep learning in medical applications has shown great success. Based on a deep-learning-based algorithm we developed that achieved better-than-human performance and ranked 1st in 2017 American Association of Physicist in Medicine Thoracic Auto-segmentation Challenge, a thoracic OAR auto-segmentation product will be developed in this project with the two aims: 1) improve and validate the deep-learning-based automatic thoracic organ segmentation algorithm on a larger clinical data set, and 2) incorporate this algorithm into a preliminary product that fits into the clinical workflow. With this product, the segmentation accuracy can be improved, leading to more robust treatment plans in protecting normal organs and improved long term patient outcome. Furthermore, the time and cost of radiation treatment planning can be greatly reduced, contributing to a more affordable cancer treatment and reduced healthcare burden. NARRATIVE As early detection and better treatment have increased cancer patient survival rates, the importance of protecting normal organs during radiation treatment is drawing more attention. To avoid excessively high radiation doses to such organs-at-risk (OARs), they are required to be correctly segmented from simulation computed tomography (CT) scans. A deep-learning-based thoracic OAR auto-segmentation product developed in this project can improve the segmentation accuracy and reduce the time and cost of radiation treatment planning as compared with the current manual process, leading to improved long term patient outcome and reduced cancer treatment cost.",Automatic Thoracic Organ Segmentation Tool for Radiation Treatment Planning of Cancers in Thoracic Region,9776272,R43EB027523,"['3-Dimensional', 'Adopted', 'Adoption', 'Algorithms', 'American', 'Anatomy', 'Area', 'Atlases', 'Attention', 'Cancer Center', 'Cancer Patient', 'Chest', 'Client', 'Clinical', 'Clinical Data', 'Computer Vision Systems', 'Computer software', 'Consumption', 'Data Set', 'Development', 'Digital Imaging and Communications in Medicine', 'Early Diagnosis', 'Environment', 'Esophageal', 'Esophagus', 'Goals', 'Healthcare', 'Heart', 'Hour', 'Human', 'Image', 'Intraobserver Variability', 'Kentucky', 'Label', 'Lung', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Medicine', 'Mesothelioma', 'Methods', 'Modeling', 'Organ', 'Outcome', 'Pathologic', 'Patient-Focused Outcomes', 'Performance', 'Phase', 'Privatization', 'Process', 'Protocols documentation', 'Radiation Dose Unit', 'Radiation therapy', 'Risk', 'Scanning', 'Site', 'Slice', 'Spinal Cord', 'Structure', 'Survival Rate', 'Testing', 'Thymus Gland', 'Time', 'Toxic effect', 'Training', 'Treatment Cost', 'Universities', 'Validation', 'X-Ray Computed Tomography', 'base', 'cancer radiation therapy', 'cancer therapy', 'clinically relevant', 'convolutional neural network', 'cost', 'deep learning', 'dosage', 'dosimetry', 'improved', 'learning strategy', 'malignant breast neoplasm', 'millimeter', 'novel', 'prototype', 'satisfaction', 'simulation', 'success', 'tool', 'treatment planning']",NIBIB,"CARINA MEDICAL, LLC",R43,2019,299288,-0.027431768183661106
"Generation of parametric images for FDG PET using dual-time-point scans Project Summary/Abstract Positron emission tomography combined with computed tomography (PET/CT) using the radiolabeled tracer 2- deoxy-2-(18F)fluoro-D-glucose (FDG) has become a standard imaging tool for cancer patient management. The semi-quantitative parameter standardized uptake value (SUV) is routinely used in clinical for tumor uptake quantification, which is computed on the static PET image acquired at a certain time (typically 60 min) post tracer injection for a short interval (typically 5-15 min). However, the quantification accuracy of SUV from a single PET scan suffers from the variabilities of tracer plasma clearance and acquisition start time. The dual- time-point FDG PET imaging has been intensively investigated and used in both clinical and research studies, typically one scan at 60 min and the other at 120 min, showing the potential to enhance the diagnostic accuracy of FDG PET by differentiating malignancy from inflammation and normal tissue. However, the current clinical dual-time-point FDG PET studies use the relative SUV change between two scans as the quantification index, which cannot eliminate the variations in tracer plasma clearance. Meanwhile, the dual-time-point protocol has not been optimized and standardized currently, leading to conflicting results. The fully-quantitative parameter, tracer net uptake rate constant Ki, is the most accurate parameter to quantify FDG PET, which is calculated using dynamic imaging with compartmental modeling. Ki is independent on the plasma clearance or acquisition start time. However, the long and complex acquisition protocol (typically at least 60 min), which requires dynamic scanning and sequential arterial blood sampling (or image-derived blood activity) used as input function from the time of injection, limits its application in clinical practice. Meanwhile, generation of the parametric Ki image, which can provide additional heterogeneity information for FDG PET, is challenging clinically using voxel-by-voxel compartmental modeling due to the computational cost and being sensitive to noise using non-linear least squares. The graphical Patlak plot, can be used for simplified Ki calculation and Ki image generation by voxel-by-voxel fitting. However, it still needs dynamic scanning starting from 15-30 min after injection and input function from the time of injection. The aims of this proposal are 1) to optimize the dual-time-point protocol for accurate Ki quantification using Patlak plot without the need for individual patient's input function, and 2) to generate high-quality low-noise dual-time-point Ki images using novel techniques based on deep learning. Upon the success of this project, our proposed approach can obtain reliable tumor Ki quantification and parametric Ki image ""for free"" without adding any additional complexity on the existing dual- time-point protocol currently used in clinical practice, with great potential of improving diagnosis and therapy assessment in oncology. We expect the translation of this approach to clinical investigation to be fast, as this is a post-processing approach and is based on data already acquired using clinically used protocol without imposing additional burden to technologists. Project Narrative For FDG PET imaging, we propose to develop a novel and simple approach of quantifying tumor Ki and generating parametric Ki image ""for free"" without adding any additional complexity on the existing dual-time- point protocol currently used in clinical practice, with great potential of improving diagnosis and therapy assessment in oncology.",Generation of parametric images for FDG PET using dual-time-point scans,9896329,R03EB027864,"['Blood', 'Blood specimen', 'Cancer Patient', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Conflict (Psychology)', 'Data', 'Diagnosis', 'Generations', 'Glucose', 'Heterogeneity', 'Image', 'Imaging Device', 'Inflammation', 'Injections', 'Label', 'Least-Squares Analysis', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Noise', 'Normal tissue morphology', 'Oncology', 'Patients', 'Plasma', 'Positron-Emission Tomography', 'Protocols documentation', 'Radiation exposure', 'Radiolabeled', 'Scanning', 'Standardization', 'Techniques', 'Time', 'Tracer', 'Training', 'Translations', 'Variant', 'X-Ray Computed Tomography', 'attenuation', 'base', 'clinical investigation', 'clinical practice', 'cohort', 'convolutional neural network', 'cost', 'deep learning', 'diagnostic accuracy', 'image reconstruction', 'improved', 'indexing', 'individual patient', 'innovation', 'novel', 'parametric imaging', 'population based', 'research study', 'simulation', 'success', 'tumor', 'uptake']",NIBIB,YALE UNIVERSITY,R03,2020,80375,-0.07881070246863216
"Tools for modeling state-dependent sensory encoding by neural populations across spatial and temporal scales Project Summary Throughout life, humans and other animals learn statistical regularities in the natural acoustic environment. They adapt their hearing to emphasize the features of sound that are important for making behavioral decisions. Normal-hearing humans are able to perceive important sounds in crowded noisy scenes and to understand the speech of individuals the first time they meet. However, patients with peripheral hearing loss or central processing disorders often have problems hearing in these challenging settings, even when sound is amplified above perceptual threshold. A better understanding of the function of the healthy and impaired auditory system will support new treatments for these deficits. This project will develop computational tools to study central auditory processing. A software library will support fitting and evaluating a large number of encoding models to describe the functional relationship between a time-varying natural auditory stimulus and the corresponding neural response. Many such models have been proposed, but relatively few direct comparisons have been made between them. This project will enable their comparison, allowing identification of the key features that contribute positively to their performance. The system will have a modular design so that useful elements from different models can be combined into comprehensive models with even greater explanatory power. The software will be open source and will support data from multiple recording modalities, including small-scale single unit electrophysiological and calcium imaging data, as well as large-scale local field and magnetoencephalography data. In addition to building on existing hypotheses about neural coding, the system will support machine learning methods for fitting artificial neural network models using the same datasets. These large, data-driven models have proven valuable for wide ranging signal processing problems, but their value and relation to existing models for neural sensory processing remain to be explored. Sensory processing involves coherent activity of large neural populations. To study coding at the population level, the system will support models that characterize the simultaneous activity of multiple neural signals and identifies latent subspaces of population activity related to sound encoding. Sensory coding is also influenced by behavioral context, reflecting changes in behavioral demands and the more general environment. The system will incorporate behavioral state variables into models, where encoding properties can be modulated by changes in behavioral context. Project Narrative Patients with hearing impairments and central neurological disorders have difficulty processing and making accurate judgments about auditory stimuli, particularly in challenging noisy environments. We seek to understand how the neural populations in the healthy brain represent complex natural sounds. These experiments will provide novel insight into how the brain solves problems that can be used to develop new devices and treatments for these debilitating conditions.",Tools for modeling state-dependent sensory encoding by neural populations across spatial and temporal scales,9774688,R01EB028155,"['Acoustics', 'Algorithms', 'Animals', 'Architecture', 'Area', 'Arousal', 'Attention', 'Auditory', 'Auditory Perceptual Disorders', 'Auditory system', 'Behavioral', 'Biological Models', 'Brain', 'Calcium', 'Calcium Signaling', 'Code', 'Communities', 'Complex', 'Computer software', 'Crowding', 'Data', 'Data Set', 'Development', 'Devices', 'Dimensions', 'Electrophysiology (science)', 'Elements', 'Environment', 'Foundations', 'Goals', 'Hearing', 'Hearing problem', 'Human', 'Image', 'Imagery', 'Impairment', 'Individual', 'Judgment', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Magnetoencephalography', 'Measurement', 'Metadata', 'Methods', 'Modality', 'Modeling', 'Motion', 'Neural Network Simulation', 'Neurons', 'Non-linear Models', 'Patients', 'Performance', 'Peripheral', 'Population', 'Problem Solving', 'Property', 'Publications', 'Publishing', 'Pythons', 'Reproducibility', 'Sensory', 'Sensory Process', 'Signal Transduction', 'Speech', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Visual system structure', 'artificial neural network', 'auditory processing', 'auditory stimulus', 'base', 'behavior influence', 'computerized tools', 'data format', 'deep neural network', 'design', 'experimental study', 'graphical user interface', 'hearing impairment', 'insight', 'learning strategy', 'nervous system disorder', 'neural model', 'neurophysiology', 'neurotransmission', 'normal hearing', 'novel', 'online repository', 'open source', 'receptive field', 'reconstruction', 'relating to nervous system', 'response', 'sensory stimulus', 'sensory system', 'signal processing', 'sound', 'tool']",NIBIB,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2019,794000,-0.04884264324524504
"Generalizing Deep Learning Reconstruction for Free-Breathing and Quantitative MRI Project Summary/Abstract The goal of this project is to increase the precision and resolution of quantitative magnetic resonance imaging (MRI). Quantitative information such as tissue relaxation parameters (e.g., T1 and T2) measure tissue function and indicate disease-related changes in the heart, liver, brain, and other organs. For instance, T1 changes can provide evidence of diffuse fibrosis in the myocardium that can signal heart disease. Quantitative maps also are reproducible, directly comparable longitudinally and across subjects, and less affected by the properties of the scanner used, when compared versus common weighted (non-quantitative) clinical imaging. But, quantitative imaging involves more complicated and time-consuming pulse sequences. To accomplish this goal, this project will develop new machine learning algorithms for high-quality parameter mapping from free-breathing data. The first aim of this project will increase parameter map resolution achievable from highly accelerated, noisy data. The proposed method will integrate existing deep cascade network-based image reconstructions with convolutional network-based blocks for super-resolution and parameter map estimation. Preliminary studies suggest these new blocks improve sharpness and mitigate artifacts in the reconstructed parameter maps. The next aim will improve the training precision of such artificial neural networks to account for the significant per-voxel nonlinear fit variability in quantitative MRI. The proposed method will reweight the loss function used for calibrating these networks by the goodness-of-fit (coefficient of determination) of the reference maps obtained from fully sampled training data. Preliminary results demonstrate that quality-aware reweighting significantly improves reconstructed image quality when working with noisy training data. Experiments will evaluate the precision of both of these innovations against existing deep-learning-based reconstructions on T1 maps obtained from pre- and post-contrast cardiac images of volunteer patients. The final aim will address motion during the acquisition by estimating and tracking nonrigid motion in the data consistency stages of the deep cascade artificial neural network architecture. Two methods are proposed: deformable motion estimation already demonstrated on compressive model-based image reconstructions, and a new “re-blurring” convolutional neural network that automatically introduces artifacts into a “clean” image to match the motion-corrupted data. Both of these methods enforce consistency between motion-affected data and a motion-free image during the reconstruction. Both methods will be validated on both cardiac and abdominal images for motion artifacts and reconstruction quality against breath-held parameter mapping acquisitions. Project Narrative Quantitative magnetic resonance imaging noninvasively measures physical properties of tissue connected to cardiovascular disease and many other conditions. Novel machine learning methods for processing data to produce higher quality maps will facilitate earlier and more accurate treatment of these diseases. This project will facilitate rapid quantitative imaging with freely breathing subjects.",Generalizing Deep Learning Reconstruction for Free-Breathing and Quantitative MRI,10007241,R56EB028254,"['Abdomen', 'Address', 'Adoption', 'Affect', 'Algorithms', 'Awareness', 'Balance training', 'Brain', 'Breathing', 'Cardiac', 'Cardiovascular Diseases', 'Clinical', 'Consumption', 'Data', 'Development', 'Diagnostic', 'Diffuse', 'Disease', 'Fibrosis', 'Financial compensation', 'Goals', 'Heart', 'Heart Diseases', 'Heart failure', 'Image', 'Infiltration', 'Literature', 'Liver', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Mainstreaming', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Morphologic artifacts', 'Motion', 'Myocardium', 'Network-based', 'Noise', 'Non-linear Models', 'Organ', 'Patients', 'Physiologic pulse', 'Process', 'Property', 'Protocols documentation', 'Relaxation', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Series', 'Signal Transduction', 'Techniques', 'Time', 'Tissues', 'Training', 'Weight', 'artificial neural network', 'base', 'clinical imaging', 'clinically relevant', 'computerized data processing', 'contrast imaging', 'convolutional neural network', 'coronary fibrosis', 'data space', 'deep learning', 'design', 'experimental study', 'heart imaging', 'image reconstruction', 'improved', 'innovation', 'learning strategy', 'loss of function', 'machine learning algorithm', 'motion sensitivity', 'multitask', 'neural network architecture', 'non-invasive imaging', 'novel', 'physical property', 'quantitative imaging', 'reconstruction', 'volunteer']",NIBIB,UNIVERSITY OF VIRGINIA,R56,2019,347527,-0.04295213799312524
"Multiband ASL for Neurodevelopment Study Project Summary/Abstract The developmental period between childhood to adolescence and young adulthood is marked by a mix of potential and vulnerability. A number of potentially life-long behavioral and emotional problems emerge during this critical period, including alcohol and illicit drug use, risky behaviors, and the first signs of emotional disorders. It is important to understand detailed patterns of typical development, so alterations can be identified and rectified as early as possible. As an entirely noninvasive and quantitative imaging method, arterial spin labeled (ASL) perfusion MRI is increasingly being recognized as an important biomarker for functional brain development in both healthy populations and neurodevelopmental disorders. However, there remain significant challenges for making ASL an impactful tool in studying neurodevelopment, including: 1) a coarse spatial resolution of ~4x4x4mm3, 2) susceptibility to head motion with segmented 3D acquisitions, and 3) potential confounding effects of age dependent variations in arterial transit time using a single post-labeling delay (PLD) scan. Simultaneous multi-slice (SMS) or multiband (MB) is a new accelerated imaging technology that simultaneously excites multiple slices and recovers each slice with parallel imaging techniques. Preliminary studies combining MB with ASL showed that MB can reduce T1 relaxation of the label, improve spatial coverage and/or resolution compared to those of standard 2D ASL. MB imaging may also overcome the limitation of 3D ASL acquisitions in terms of head motion and spatial blurring. However, the signal-to-noise ratio (SNR) of existing MB ASL is inferior to that of 3D ASL. This project builds on two recent innovations from our lab: 1) a constrained slice-dependent (CSD) background suppression (BS) technique that improves the SNR of 2D MB pCASL to be comparable to that of 3D pCASL; and 2) a single-shot 3D GRASE pCASL method with 2D CAIPIRINHA accelerations that improves the imaging speed of 3D pCASL. The goal of this R01 project is to develop and evaluate cutting-edge MB pCASL protocols that are able to offer a high spatial resolution of isotropic 2mm or higher, resistance to head motion and multi-delay capability for accurate perfusion quantification in pediatric populations. A convolutional neural network (CNN) based denoising algorithm for multi-delay MB pCASL will be further developed. The developed suite of MB pCASL protocol and post- processing algorithms will be evaluated in 40 typically developing children and adolescents. The successful completion of this R01 project will lead to a robust multi-delay MB pCASL protocol that is highly valuable as potential biomarker for both neurodevelopment research and pediatric clinical care. To maximize the scientific and clinical impact, we will continue disseminating the pulse sequence and associated post-processing software as we have been doing in the past decade. Relevance to Public Health A number of potentially life-long behavioral and emotional problems emerge during the developmental period between childhood to adolescence and young adulthood, including alcohol and illicit drug use, risky behaviors, and emotional disorders. This project will develop a robust noninvasive imaging technique using magnetic resonance imaging (MRI) that offers high spatial resolution, resistance to head motion and accurate quantification of cerebral blood flow in children and adolescents. The developed technology is highly valuable as potential biomarker for both neurodevelopment research and pediatric clinical care.",Multiband ASL for Neurodevelopment Study,9972888,R01EB028297,"['3-Dimensional', 'Acceleration', 'Adolescence', 'Adolescent', 'Affect', 'Age', 'Alcohol or Other Drugs use', 'Alcohols', 'Algorithms', 'Attention deficit hyperactivity disorder', 'Behavior Disorders', 'Behavioral', 'Biological Markers', 'Brain', 'Cerebrovascular Circulation', 'Child', 'Childhood', 'Clinical', 'Computer software', 'Data', 'Databases', 'Development', 'Emotional', 'Emotional disorder', 'Goals', 'Head', 'Image', 'Imaging Techniques', 'Imaging technology', 'Inferior', 'Joint repair', 'Label', 'Life', 'Magnetic Resonance Imaging', 'Methods', 'Motion', 'Network-based', 'Neurodevelopmental Disorder', 'Noise', 'Pattern', 'Performance', 'Perfusion', 'Perfusion Weighted MRI', 'Phase', 'Physiologic pulse', 'Population', 'Predisposition', 'Principal Component Analysis', 'Protocols documentation', 'Public Health', 'Publishing', 'Relaxation', 'Reporting', 'Research', 'Resistance', 'Resolution', 'Risk Behaviors', 'Sampling', 'Scanning', 'Scheme', 'Signal Transduction', 'Slice', 'Speed', 'Spin Labels', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'Variant', 'age effect', 'age related', 'autism spectrum disorder', 'base', 'child depression', 'clinical care', 'convolutional neural network', 'critical period', 'denoising', 'developmental disease', 'illicit drug use', 'imaging modality', 'improved', 'innovation', 'neurodevelopment', 'non-invasive imaging', 'novel', 'perfusion imaging', 'potential biomarker', 'quantitative imaging', 'socioeconomics', 'time use', 'tool', 'young adult']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,445500,-0.06414502408094544
"Multiband ASL for Neurodevelopment Study Project Summary/Abstract The developmental period between childhood to adolescence and young adulthood is marked by a mix of potential and vulnerability. A number of potentially life-long behavioral and emotional problems emerge during this critical period, including alcohol and illicit drug use, risky behaviors, and the first signs of emotional disorders. It is important to understand detailed patterns of typical development, so alterations can be identified and rectified as early as possible. As an entirely noninvasive and quantitative imaging method, arterial spin labeled (ASL) perfusion MRI is increasingly being recognized as an important biomarker for functional brain development in both healthy populations and neurodevelopmental disorders. However, there remain significant challenges for making ASL an impactful tool in studying neurodevelopment, including: 1) a coarse spatial resolution of ~4x4x4mm3, 2) susceptibility to head motion with segmented 3D acquisitions, and 3) potential confounding effects of age dependent variations in arterial transit time using a single post-labeling delay (PLD) scan. Simultaneous multi-slice (SMS) or multiband (MB) is a new accelerated imaging technology that simultaneously excites multiple slices and recovers each slice with parallel imaging techniques. Preliminary studies combining MB with ASL showed that MB can reduce T1 relaxation of the label, improve spatial coverage and/or resolution compared to those of standard 2D ASL. MB imaging may also overcome the limitation of 3D ASL acquisitions in terms of head motion and spatial blurring. However, the signal-to-noise ratio (SNR) of existing MB ASL is inferior to that of 3D ASL. This project builds on two recent innovations from our lab: 1) a constrained slice-dependent (CSD) background suppression (BS) technique that improves the SNR of 2D MB pCASL to be comparable to that of 3D pCASL; and 2) a single-shot 3D GRASE pCASL method with 2D CAIPIRINHA accelerations that improves the imaging speed of 3D pCASL. The goal of this R01 project is to develop and evaluate cutting-edge MB pCASL protocols that are able to offer a high spatial resolution of isotropic 2mm or higher, resistance to head motion and multi-delay capability for accurate perfusion quantification in pediatric populations. A convolutional neural network (CNN) based denoising algorithm for multi-delay MB pCASL will be further developed. The developed suite of MB pCASL protocol and post- processing algorithms will be evaluated in 40 typically developing children and adolescents. The successful completion of this R01 project will lead to a robust multi-delay MB pCASL protocol that is highly valuable as potential biomarker for both neurodevelopment research and pediatric clinical care. To maximize the scientific and clinical impact, we will continue disseminating the pulse sequence and associated post-processing software as we have been doing in the past decade. Relevance to Public Health A number of potentially life-long behavioral and emotional problems emerge during the developmental period between childhood to adolescence and young adulthood, including alcohol and illicit drug use, risky behaviors, and emotional disorders. This project will develop a robust noninvasive imaging technique using magnetic resonance imaging (MRI) that offers high spatial resolution, resistance to head motion and accurate quantification of cerebral blood flow in children and adolescents. The developed technology is highly valuable as potential biomarker for both neurodevelopment research and pediatric clinical care.",Multiband ASL for Neurodevelopment Study,9800619,R01EB028297,"['3-Dimensional', 'Acceleration', 'Adolescence', 'Adolescent', 'Affect', 'Age', 'Alcohol or Other Drugs use', 'Alcohols', 'Algorithms', 'Attention deficit hyperactivity disorder', 'Behavior Disorders', 'Behavioral', 'Biological Markers', 'Brain', 'Cerebrovascular Circulation', 'Child', 'Childhood', 'Clinical', 'Computer software', 'Data', 'Databases', 'Development', 'Emotional', 'Emotional disorder', 'Goals', 'Head', 'Image', 'Imaging Techniques', 'Imaging technology', 'Inferior', 'Joint repair', 'Label', 'Life', 'Magnetic Resonance Imaging', 'Methods', 'Motion', 'Network-based', 'Neurodevelopmental Disorder', 'Noise', 'Pattern', 'Performance', 'Perfusion', 'Perfusion Weighted MRI', 'Phase', 'Physiologic pulse', 'Population', 'Predisposition', 'Principal Component Analysis', 'Protocols documentation', 'Public Health', 'Publishing', 'Relaxation', 'Reporting', 'Research', 'Resistance', 'Resolution', 'Risk Behaviors', 'Sampling', 'Scanning', 'Scheme', 'Signal Transduction', 'Slice', 'Speed', 'Spin Labels', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'Variant', 'age effect', 'age related', 'autism spectrum disorder', 'base', 'child depression', 'clinical care', 'convolutional neural network', 'critical period', 'denoising', 'developmental disease', 'illicit drug use', 'imaging modality', 'improved', 'innovation', 'neurodevelopment', 'non-invasive imaging', 'novel', 'perfusion imaging', 'potential biomarker', 'quantitative imaging', 'socioeconomics', 'time use', 'tool', 'young adult']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,445500,-0.06414502408094544
"High performance PET Detector Module for Human Brain Imaging Project Summary / Abstract This proposal is in response to PA 18-484. In the BRAIN 2025 Report, PET (positron emission tomography) is identified as “the best means to translate studies of neurotransmitters, receptors, and neuromodulators to humans.” However dynamic assessment of receptor occupancy and metabolism is hindered by the spatial resolution and sensitivity of even the most modern of clinically available PET scanners. To address this challenge, we propose a next generation PET detector with a highly innovative design: a detector module with a layered scintillator structure and a side readout configuration. The crystal slabs in the module are stacked along the depth direction and are optically separated by reflective films. The scintillation light created in each layer is measured by photodetectors located on the four sides of the crystal. Compared with traditional PET detectors, which contain pixelated crystal arrays, the new design has the following advantages: (1) The layered structure provides depth of interaction (DOI) information such that a smaller diameter detector ring can be used without increasing parallax error, increasing sensitivity while lowering costs. (2) The four-sided readout method improves the energy resolution of the system with increased scintillation light collection efficiency by reducing light loss due to total internal reflection. (3) Sub millimeter spatial resolution is achievable without using very small pitch crystal arrays, since the interaction location in each crystal layer is determined via machine learning- based decoding of the light distribution collected on the four crystal sides. Therefore the production cost of the crystals is reduced. (4) Since the interaction location and energy resolution for each layer are determined independently, the system sensitivity can be increased by stacking more layers in the module without affecting the spatial and energy resolution of the system. (5) For side readout setup, a larger ratio of cross-sectional area to length requires fewer photodetectors to cover all four sides of the module; this reduces photodetector cost. The first four points above have been demonstrated in preliminary studies using a small, prototype module. We propose to build a large scale detector module with this new design to verify the fifth advantage, and to study the effect of detector size on the first four. The outcome of this proposal will be two DOI enabled detector modules with excellent spatial resolution (~1 mm) and energy resolution (~10%), as well as good timing resolution (~400 ps), and DOI resolution (~ 3 mm) and high system sensitivity. A full characterization study for the two modules and imaging studies for both the Derenzo and Hoffman brain phantoms will address the current limitations of human brain PET scanners, and will serve as the foundation for a new dynamic PET scanner for neuroimaging. Project Narrative The goal of this project is to develop a novel design for detector modules used in brain-dedicated PET system with good spatial resolution, energy resolution, timing resolution, and DOI resolution, as well as high sensitivity. If successful, the study will pave the way for building a novel PET system for greatly improved dynamic PET imaging of the human brain.",High performance PET Detector Module for Human Brain Imaging,10017238,R01EB028337,"['Address', 'Affect', 'Alzheimer&apos', 's Disease', 'Area', 'Brain', 'Brain Neoplasms', 'Brain imaging', 'Caliber', 'Calibration', 'Clinical', 'Collection', 'Crystallization', 'Development', 'Diagnosis', 'Evaluation', 'Event', 'Film', 'Foundations', 'Gamma Rays', 'Geometry', 'Goals', 'Human', 'Image', 'Imaging Phantoms', 'Individual', 'Length', 'Light', 'Location', 'Machine Learning', 'Measures', 'Metabolism', 'Methods', 'Modernization', 'Neuromodulator', 'Neurotransmitter Receptor', 'Optics', 'Outcome', 'Parkinson Disease', 'Pattern', 'Performance', 'Portugal', 'Positioning Attribute', 'Positron-Emission Tomography', 'Process', 'Production', 'Reporting', 'Resolution', 'Rod', 'Side', 'Signal Transduction', 'Silicon', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Thinness', 'Translating', 'Variant', 'base', 'brain size', 'cost', 'design', 'detector', 'human imaging', 'imaging study', 'improved', 'innovation', 'machine learning method', 'meetings', 'millimeter', 'nervous system disorder', 'neuroimaging', 'next generation', 'novel', 'photomultiplier', 'prototype', 'receptor', 'response']",NIBIB,"CANON MEDICAL RESEARCH USA, INC.",R01,2020,243919,0.03573078871906557
"High performance PET Detector Module for Human Brain Imaging Project Summary / Abstract This proposal is in response to PA 18-484. In the BRAIN 2025 Report, PET (positron emission tomography) is identified as “the best means to translate studies of neurotransmitters, receptors, and neuromodulators to humans.” However dynamic assessment of receptor occupancy and metabolism is hindered by the spatial resolution and sensitivity of even the most modern of clinically available PET scanners. To address this challenge, we propose a next generation PET detector with a highly innovative design: a detector module with a layered scintillator structure and a side readout configuration. The crystal slabs in the module are stacked along the depth direction and are optically separated by reflective films. The scintillation light created in each layer is measured by photodetectors located on the four sides of the crystal. Compared with traditional PET detectors, which contain pixelated crystal arrays, the new design has the following advantages: (1) The layered structure provides depth of interaction (DOI) information such that a smaller diameter detector ring can be used without increasing parallax error, increasing sensitivity while lowering costs. (2) The four-sided readout method improves the energy resolution of the system with increased scintillation light collection efficiency by reducing light loss due to total internal reflection. (3) Sub millimeter spatial resolution is achievable without using very small pitch crystal arrays, since the interaction location in each crystal layer is determined via machine learning- based decoding of the light distribution collected on the four crystal sides. Therefore the production cost of the crystals is reduced. (4) Since the interaction location and energy resolution for each layer are determined independently, the system sensitivity can be increased by stacking more layers in the module without affecting the spatial and energy resolution of the system. (5) For side readout setup, a larger ratio of cross-sectional area to length requires fewer photodetectors to cover all four sides of the module; this reduces photodetector cost. The first four points above have been demonstrated in preliminary studies using a small, prototype module. We propose to build a large scale detector module with this new design to verify the fifth advantage, and to study the effect of detector size on the first four. The outcome of this proposal will be two DOI enabled detector modules with excellent spatial resolution (~1 mm) and energy resolution (~10%), as well as good timing resolution (~400 ps), and DOI resolution (~ 3 mm) and high system sensitivity. A full characterization study for the two modules and imaging studies for both the Derenzo and Hoffman brain phantoms will address the current limitations of human brain PET scanners, and will serve as the foundation for a new dynamic PET scanner for neuroimaging. Project Narrative The goal of this project is to develop a novel design for detector modules used in brain-dedicated PET system with good spatial resolution, energy resolution, timing resolution, and DOI resolution, as well as high sensitivity. If successful, the study will pave the way for building a novel PET system for greatly improved dynamic PET imaging of the human brain.",High performance PET Detector Module for Human Brain Imaging,9802309,R01EB028337,"['Address', 'Affect', 'Alzheimer&apos', 's Disease', 'Area', 'Brain', 'Brain Neoplasms', 'Brain imaging', 'Caliber', 'Calibration', 'Clinical', 'Collection', 'Crystallization', 'Development', 'Diagnosis', 'Evaluation', 'Event', 'Film', 'Foundations', 'Gamma Rays', 'Geometry', 'Goals', 'Human', 'Image', 'Imaging Phantoms', 'Individual', 'Length', 'Light', 'Location', 'Machine Learning', 'Measures', 'Metabolism', 'Methods', 'Modernization', 'Neuromodulator', 'Neurotransmitter Receptor', 'Optics', 'Outcome', 'Parkinson Disease', 'Pattern', 'Performance', 'Portugal', 'Positioning Attribute', 'Positron-Emission Tomography', 'Process', 'Production', 'Reporting', 'Resolution', 'Side', 'Signal Transduction', 'Silicon', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Thinness', 'Translating', 'Variant', 'base', 'brain size', 'cost', 'design', 'detector', 'human imaging', 'imaging study', 'improved', 'innovation', 'learning strategy', 'meetings', 'millimeter', 'nervous system disorder', 'neuroimaging', 'next generation', 'novel', 'photomultiplier', 'prototype', 'receptor', 'response', 'retinal rods']",NIBIB,"CANON MEDICAL RESEARCH USA, INC.",R01,2019,293568,0.03573078871906557
"RECONSTRUCTION AND DISPLAY IN TOMOGRAPHIC RADIOLOGY We propose to cover various modalities in radiology -- namely, computerized tomography (CT), positron emission tomography (PET), magnetic resonance imaging (MRI) and ultrasound (US) -- all of which are tomographic in the sense that they produce images of slices through the human body.  ""Reconstruction"" in the title of the proposal is to be interpreted as the procedure to produce such images from the measured raw data in CT, PET, and MRI.  ""Display"" in the title is to be interpreted as the procedure to put together in three-dimensional (3D) or (including time) 4D -- space the information that exists in the individual slices.  In ""reconstruction"" we plan to concentrate on iterative finite series- expansion methods, preprocessing of incomplete or distorted data, applications to PET and MRI, and methods for objective  evaluation of  reconstruction  algorithms.  In ""display"" we plan to concentrate on segmentation (identification of structures of interest, such as specific organs, in the 3D space), the use of color for displaying medical images, computerized 3D radiation therapy treatment planning, and 3D display, quantitation and analysis of the normal, ischemic and infarcted left ventricle, of normal and disturbed flow, and of complex congenital heart disease. In all caSes computer software will be fully developed, implemented, tested, and evaluated, on the kind of computing equipment that is commonly available to radiology departments.  Supplemental funding is requested for the last seven months of the currently active grant for the projects which involve applications to cardiology.  Further supplemental funding is requested for seventeen months beyond the end of the currently active grant for all the projects listed above.  n/a",RECONSTRUCTION AND DISPLAY IN TOMOGRAPHIC RADIOLOGY,3339794,R01HL028438,"['angiocardiography', ' artificial intelligence', ' cardiography', ' child (0-11)', ' cineangiocardiography', ' clinical biomedical equipment', ' computed axial tomography', ' computer assisted diagnosis', ' computer graphics /printing', ' computer simulation', ' computers', ' congenital heart disorder', ' dogs', ' echocardiography', ' human subject', ' image enhancement', ' image processing', ' magnetic resonance imaging', ' mathematical model', ' mathematics', ' model design /development', ' myocardial ischemia /hypoxia', ' phantom model', ' positron emission tomography', ' radiodiagnosis', ' tomography', ' ultrasonography']",NHLBI,UNIVERSITY OF PENNSYLVANIA,R01,1989,611380,1.9003505975375687e-05
"RECONSTRUCTION AND DISPLAY IN TOMOGRAPHIC RADIOLOGY We propose to cover various modalities in radiology -- namely, computerized tomography (CT), positron emission tomography (PET), magnetic resonance imaging (MRI) and ultrasound (US) -- all of which are tomographic in the sense that they produce images of slices through the human body.  ""Reconstruction"" in the title of the proposal is to be interpreted as the procedure to produce such images from the measured raw data in CT, PET, and MRI.  ""Display"" in the title is to be interpreted as the procedure to put together in three-dimensional (3D) or (including time) 4D -- space the information that exists in the individual slices.  In ""reconstruction"" we plan to concentrate on iterative finite series- expansion methods, preprocessing of incomplete or distorted data, applications to PET and MRI, and methods for objective  evaluation of  reconstruction  algorithms.  In ""display"" we plan to concentrate on segmentation (identification of structures of interest, such as specific organs, in the 3D space), the use of color for displaying medical images, computerized 3D radiation therapy treatment planning, and 3D display, quantitation and analysis of the normal, ischemic and infarcted left ventricle, of normal and disturbed flow, and of complex congenital heart disease. In all caSes computer software will be fully developed, implemented, tested, and evaluated, on the kind of computing equipment that is commonly available to radiology departments.  Supplemental funding is requested for the last seven months of the currently active grant for the projects which involve applications to cardiology.  Further supplemental funding is requested for seventeen months beyond the end of the currently active grant for all the projects listed above.  n/a",RECONSTRUCTION AND DISPLAY IN TOMOGRAPHIC RADIOLOGY,3339790,R01HL028438,"['angiocardiography', ' artificial intelligence', ' cardiography', ' child (0-11)', ' cineangiocardiography', ' clinical biomedical equipment', ' computed axial tomography', ' computer assisted diagnosis', ' computer graphics /printing', ' computer simulation', ' computers', ' congenital heart disorder', ' dogs', ' echocardiography', ' human subject', ' image enhancement', ' image processing', ' magnetic resonance imaging', ' mathematical model', ' mathematics', ' model design /development', ' myocardial ischemia /hypoxia', ' phantom model', ' positron emission tomography', ' radiodiagnosis', ' tomography', ' ultrasonography']",NHLBI,UNIVERSITY OF PENNSYLVANIA,R01,1988,246149,1.9003505975375687e-05
"RECONSTRUCTION AND DISPLAY IN TOMOGRAPHIC RADIOLOGY This is a renewal application for a grant currently entitled ""Computer Technology for Transaxial Tomography"".  The change in title to ""Reconstruction and Display in Tomographic Radiology"" reflects a shift in the nature of the proposed work.  We propose to cover various modalities in radiology--namely, computerized tomography (CT), positron emission tomography (PET), magnetic resonance imaging (MRI) and ultrasound (US)--all of which are tomographic in the sense that they produce images of slices through the human body.  ""Reconstruction"" in the title of the proposal is to be interpreted as the procedure to produce such images from the measured raw data in CT, PET, and MRI.  ""Display"" in the title is to be interpreted as the procedure to put together in three-dimensional (3D)--or (including time) 4D--space the information that exists in the individual slices.  In ""reconstruction"" we plan to concentrate on finite series expansion methods, methods derived from a semi-continuous formulation, preprocessing of incomplete or distorted data, with applications to CT, PET, and MRI.  In ""display"" we plan to concentrate on segmentation (identification of structures of interest, such as specific organs and tumors, in the 3D space) and on rapid display, analysis, and manipulation of the identified structures, with applications to measurement of volumetric changes in soft-tissue masses, surgical planning, radiation therapy treatment planning, and characterization of the normal, ischemic and scarred myocardium.  In all cases computer software will be fully developed, implemented, tested, and evaluated, on the kind of computing equipment that is commonly available in radiology departments.  n/a",RECONSTRUCTION AND DISPLAY IN TOMOGRAPHIC RADIOLOGY,3339793,R01HL028438,"['artificial intelligence', ' clinical biomedical equipment', ' computed axial tomography', ' computer assisted diagnosis', ' computer graphics /printing', ' computer human interaction', ' echocardiography', ' image enhancement', ' image processing', ' mathematical model', ' myocardial ischemia /hypoxia', ' neoplasm /cancer radiodiagnosis', ' online computer', ' oral facial restoration', ' phantom model', ' positron emission tomography', ' radiation therapy', ' radiodiagnosis', ' surgery', ' tomography', ' ultrasonography']",NHLBI,UNIVERSITY OF PENNSYLVANIA,R01,1987,684659,-0.026424497888447963
"RECONSTRUCTION AND DISPLAY IN TOMOGRAPHIC RADIOLOGY This is a renewal application for a grant currently entitled ""Computer Technology for Transaxial Tomography"".  The change in title to ""Reconstruction and Display in Tomographic Radiology"" reflects a shift in the nature of the proposed work.  We propose to cover various modalities in radiology--namely, computerized tomography (CT), positron emission tomography (PET), magnetic resonance imaging (MRI) and ultrasound (US)--all of which are tomographic in the sense that they produce images of slices through the human body.  ""Reconstruction"" in the title of the proposal is to be interpreted as the procedure to produce such images from the measured raw data in CT, PET, and MRI.  ""Display"" in the title is to be interpreted as the procedure to put together in three-dimensional (3D)--or (including time) 4D--space the information that exists in the individual slices.  In ""reconstruction"" we plan to concentrate on finite series expansion methods, methods derived from a semi-continuous formulation, preprocessing of incomplete or distorted data, with applications to CT, PET, and MRI.  In ""display"" we plan to concentrate on segmentation (identification of structures of interest, such as specific organs and tumors, in the 3D space) and on rapid display, analysis, and manipulation of the identified structures, with applications to measurement of volumetric changes in soft-tissue masses, surgical planning, radiation therapy treatment planning, and characterization of the normal, ischemic and scarred myocardium.  In all cases computer software will be fully developed, implemented, tested, and evaluated, on the kind of computing equipment that is commonly available in radiology departments.  n/a",RECONSTRUCTION AND DISPLAY IN TOMOGRAPHIC RADIOLOGY,3339792,R01HL028438,"['artificial intelligence', ' clinical biomedical equipment', ' computed axial tomography', ' computer assisted diagnosis', ' computer graphics /printing', ' computer human interaction', ' echocardiography', ' image enhancement', ' image processing', ' mathematical model', ' myocardial ischemia /hypoxia', ' neoplasm /cancer radiodiagnosis', ' online computer', ' oral facial restoration', ' phantom model', ' positron emission tomography', ' radiation therapy', ' radiodiagnosis', ' surgery', ' tomography', ' ultrasonography']",NHLBI,UNIVERSITY OF PENNSYLVANIA,R01,1986,614137,-0.026424497888447963
"RECONSTRUCTION AND DISPLAY IN TOMOGRAPHIC RADIOLOGY This is a renewal application for a grant currently entitled ""Computer Technology for Transaxial Tomography"".  The change in title to ""Reconstruction and Display in Tomographic Radiology"" reflects a shift in the nature of the proposed work.  We propose to cover various modalities in radiology--namely, computerized tomography (CT), positron emission tomography (PET), magnetic resonance imaging (MRI) and ultrasound (US)--all of which are tomographic in the sense that they produce images of slices through the human body.  ""Reconstruction"" in the title of the proposal is to be interpreted as the procedure to produce such images from the measured raw data in CT, PET, and MRI.  ""Display"" in the title is to be interpreted as the procedure to put together in three-dimensional (3D)--or (including time) 4D--space the information that exists in the individual slices.  In ""reconstruction"" we plan to concentrate on finite series expansion methods, methods derived from a semi-continuous formulation, preprocessing of incomplete or distorted data, with applications to CT, PET, and MRI.  In ""display"" we plan to concentrate on segmentation (identification of structures of interest, such as specific organs and tumors, in the 3D space) and on rapid display, analysis, and manipulation of the identified structures, with applications to measurement of volumetric changes in soft-tissue masses, surgical planning, radiation therapy treatment planning, and characterization of the normal, ischemic and scarred myocardium.  In all cases computer software will be fully developed, implemented, tested, and evaluated, on the kind of computing equipment that is commonly available in radiology departments.  n/a",RECONSTRUCTION AND DISPLAY IN TOMOGRAPHIC RADIOLOGY,3339789,R01HL028438,"['artificial intelligence', ' clinical biomedical equipment', ' computed axial tomography', ' computer assisted diagnosis', ' computer graphics /printing', ' computer human interaction', ' echocardiography', ' image enhancement', ' image processing', ' mathematical model', ' myocardial ischemia /hypoxia', ' neoplasm /cancer radiodiagnosis', ' online computer', ' oral facial restoration', ' phantom model', ' radiation therapy', ' radiodiagnosis', ' surgery', ' tomography', ' ultrasonography']",NHLBI,UNIVERSITY OF PENNSYLVANIA,R01,1985,659781,-0.026424497888447963
"Comprehensive characterization of coronary atherosclerotic disease using photon-counting-detector dual-source CT and its impact on patient management PROJECT SUMMARY/ABSTRACT Coronary artery disease (CAD) remains the main cause of morbidity and mortality in the United States. Cardiac CT provides fast non-invasive assessment of CAD with a high sensitivity and negative predictive value – provided that the lumen can be visualized. However, heavily calcified or stented coronary segments are non- assessable, precluding non-invasive diagnosis of flow-limiting coronary plaques in an estimated 2 million U.S. adults. In addition, the spatial resolution of state-of-the-art CT systems is insufficient for robust visualization of features associated with high risk plaques. Further, while CT can quantitatively evaluate the impact of obstructive CAD on myocardial function using dynamic perfusion imaging, this requires relatively high patient radiation doses, which has limited widespread adoption. Considering the high personal and societal cost of CAD, robust, accurate, non-invasive imaging of calcified and stented coronary arteries, high-risk plaque features, and myocardial perfusion defects in a single, low-radiation-dose exam is critically needed. Built by Siemens Healthcare, a first-of-its-kind, whole-body, photon-counting-detector (PCD) CT system was installed in 2014 at the Mayo Clinic. With support from NIH award EB016966, we showed that the increased iodine contrast-to-noise ratio, decreased electronic noise, spectral imaging capabilities, and improved spatial resolution of PCD-CT relative to commercial CT enabled us to accurately measure increased vasa vasorum density in injured swine carotid arterial walls, demonstrating the exceptional potential of PCD-CT in vascular imaging. Because this system lacks cardiac imaging capabilities, our objective is to develop and validate a PCD dual-source (DS) CT system and novel imaging algorithms to accurately assess CAD in humans, especially in patients with heavily calcified, stented, or high-risk plaques, and to identify patients with myocardial perfusion defects. Our premise is that the established benefits of PCD-CT, used with a DS geometry and advanced noise reduction and material decomposition algorithms, can meet these objectives. Our proposal is significant in many ways: the technology developments will benefit all of CT imaging; robust, accurate, non-invasive imaging of calcified and stented coronary arteries, high-risk plaque features, and myocardial perfusion defects in a single, low-radiation-dose exam will obviate the need for additional imaging, reducing the overall time and cost to comprehensively evaluate CAD and its clinical significance. To extend the demonstrated benefits of PCDs to cardiac CT will require numerous physics, engineering, and algorithm innovations, including novel noise reduction and material decomposition algorithms using energy, spatial and temporal domain redundancies, as well as deep learning. These advances will culminate in a large clinical study to demonstrate not merely that the images are “better,” as is so often done, but that PCD-DSCT provides clinically-significant improvements in the diagnosis and management of patients with suspected CAD. PROJECT NARRATIVE This project will develop a new type of cardiac computed tomography (CT) scanner that is able to comprehensively assess coronary artery disease in humans. This technology, known as photon-counting- detector dual-source CT, is capable of exceptional spatial and temporal resolution, multi-energy spectral imaging and reduced radiation doses, allowing it to image the coronary artery and myocardium with unparalleled quality. This will enable comprehensive assessment of coronary artery anatomy and myocardial function from a single imaging exam, reducing time to diagnosis and cost, while also improving patient diagnosis and management.",Comprehensive characterization of coronary atherosclerotic disease using photon-counting-detector dual-source CT and its impact on patient management,9972330,R01EB028590,"['Address', 'Adoption', 'Adult', 'Algorithms', 'Anatomy', 'Award', 'Blood Vessels', 'Cardiac', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Computed Tomography Scanners', 'Contrast Media', 'Coronary', 'Coronary Arteriosclerosis', 'Coronary artery', 'Defect', 'Diagnosis', 'Diagnostic', 'Dose', 'Engineering', 'Equipment', 'Family suidae', 'Geometry', 'Goals', 'Healthcare', 'Heart failure', 'Human', 'Image', 'Individual', 'Iodine', 'Lesion', 'Low Dose Radiation', 'Magnetic Resonance Imaging', 'Measures', 'Modeling', 'Morbidity - disease rate', 'Myocardial', 'Myocardial Infarction', 'Myocardial perfusion', 'Myocardium', 'Noise', 'Patients', 'Perfusion', 'Physics', 'Physiological', 'Predictive Value', 'Radiation', 'Radiation Dose Unit', 'Reproducibility', 'Resolution', 'Signal Transduction', 'Societies', 'Source', 'Specimen', 'Stents', 'Sudden Death', 'System', 'Techniques', 'Technology', 'Time', 'Translating', 'United States', 'United States National Institutes of Health', 'Visualization', 'Work', 'X-Ray Computed Tomography', 'algorithm development', 'calcification', 'clinically significant', 'coronary plaque', 'cost', 'deep learning', 'density', 'design', 'detector', 'heart imaging', 'heart motion', 'high risk', 'human subject', 'imaging capabilities', 'improved', 'industry partner', 'injured', 'innovation', 'mortality', 'non-invasive imaging', 'noninvasive diagnosis', 'novel', 'perfusion imaging', 'photon-counting detector', 'routine practice', 'single photon emission computed tomography', 'societal costs', 'spectral energy', 'spectrograph', 'technology development', 'temporal measurement', 'vasa vasorum']",NIBIB,MAYO CLINIC ROCHESTER,R01,2020,628053,-0.05560385171181127
"S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450 This application is requesting funds to purchase the Aperio™ GT-450 digital pathology slide scanner from Leica Biosystems. The requested instrumentation will be located in the Pathology and Biobanking Core of the Lester and Sue Smith Breast Center at Baylor College of Medicine (BCM). The predominant use of the Aperio scanner will be research-based whole slide imaging (WSI) and analysis of patient specimens, patient-derived xenograft (PDX) cancer models, and pre-clinical investigations on various animal- and cell-line model systems. All user projects have large sample cohorts that require high throughput, high-resolution scanning and image analysis. High capacity and improved scanning with dynamic focusing makes the GT-450 microscope scanner well-suited and the most cost-effective for use in the proposed projects. An underlying theme in the studies selected for Aperio scanner-supported services integrates novel biomarker and molecular pathway discovery with spatial morphological characterization, a necessary process to investigate heterogeneity in disease states. This instrument leverages high-throughput scanning capability with open-source, fully customizable machine-learning analytics to meet the evolving needs of investigators at Baylor College of Medicine, in particular faculty groups studying mechanisms of cancer cell dynamics and the development of new therapeutic targets. Expansion of systems biology and precision medicine research is an essential component of the college’s strategic roadmap. The Aperio GT-450 is critically needed as we modernize our laboratory offerings and capabilities; the acquisition of this digital scanner will strengthen existing research programs underway and establish new, collaborative research opportunities and directions within Baylor College of Medicine and surrounding institutions. To address the growing demand for integrating quantitative spatial assessment of biomarkers with molecular pathway discovery, we request funding for the Leica Aperio GT-450 digital microscope scanner. This instrument will facilitate research that seeks to better understand molecular mechanisms of tumorigenesis in the context of its spatial environment and will be critical for the development of clinically correlative biomarkers for next generation precision medicine research initiatives at Baylor College of Medicine.",S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450,9940426,S10OD028671,"['Animals', 'Biological Models', 'Breast', 'Cancer Model', 'Cell Line', 'Development', 'Disease', 'Faculty', 'Funding', 'Grant', 'Heterogeneity', 'Image Analysis', 'Institution', 'Laboratories', 'Machine Learning', 'Medicine', 'Microscope', 'Modernization', 'Molecular', 'Morphology', 'Pathology', 'Pathway interactions', 'Patients', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Slide', 'Specimen', 'Systems Biology', 'Xenograft procedure', 'base', 'biobank', 'cancer cell', 'clinical investigation', 'cohort', 'college', 'cost effective', 'digital', 'digital pathology', 'improved', 'instrument', 'instrumentation', 'new therapeutic target', 'novel marker', 'open source', 'pre-clinical', 'precision medicine', 'programs', 'whole slide imaging']",OD,BAYLOR COLLEGE OF MEDICINE,S10,2020,477043,-0.022834603747840417
"Dissemination of a Software Platform for Efficient CT Radiation Dose Optimization and Diagnostic Performance Assessment PROJECT SUMMARY/ABSTRACT With the introduction of many novel techniques to minimize radiation dose in CT, there is still a large variation in terms of radiation dose levels prescribed in CT exams and therefore a large variation of diagnostic performance. Some patients may receive higher dose than necessary. Some may be under-dosed and mis- diagnosed as a result of insufficient image quality. In order to determine the appropriate amount of radiation dose reduction in each exam, accurate quantification of diagnostic performance is needed so that the dose reduction can be achieved without sacrificing important diagnostic information. However, currently there is a lack of efficient and quantitative tools for objective assessment of diagnostic performance, particularly for many of the novel dose reduction methods involving non-linear processing of the data such as iterative reconstruction and deep-learning-based noise reduction methods. The specific goal of this application is to disseminate a highly automated solution, CT Protocol optimization (CTPro) software, to a wide CT community. This quantitative tool provides an efficient implementation of diagnostic performance assessment and CT radiation dose optimization. This tool is based on channelized Hotelling observer (CHO), which itself was developed decades ago to mimic human observer visual responses in signal detection tasks. However, the use of CHO in clinical CT is quite limited because of a lack of rigorous validation and efficient and robust implementation in practice. We were the first to demonstrate its correlation with human observer performance in low-contrast detection, classification and localization tasks in clinical CT. The main objective of the current proposal is to optimize this tool for simplicity and robustness, and disseminate it to CT researchers and clinical users, which will be accomplished through 3 specific aims: Aim 1: Optimize CTPro for simplicity, robustness, and generalizability. Aim 2: Develop an open-source web-based platform for software dissemination. Aim 3: Build use cases and disseminate CTPro. The proposed work is significant because the software tool will allow any CT users and researchers to perform CT radiation dose optimization and diagnostic performance evaluation in an efficient, quantitative, and objective manner. This work is innovative in that the automated tool will use quantitative measures of diagnostic performance to systematically guide the complex task of CT dose optimization, moving beyond traditional metrics that are inappropriate for many novel dose reduction techniques. The software tool, once widely employed, will facilitate a paradigm shift in how dose optimization and the evaluation of dose reduction techniques are performed, and will allow a more rapid and consistent adoption of dose reduction technology into clinical practice, which will benefit millions of CT patients. PROJECT NARRATIVE There has been a lack of quantitative tools for efficient and objective assessment of diagnostic performance in CT, which is the reason why inappropriate radiation dose is frequently used in CT exams, resulting in unnecessarily high radiation exposure to patients or lose of important diagnostic information. The purpose of this project is to disseminate a highly automated solution to a wide CT community for efficient CT radiation dose optimization. If successful, appropriate amount of radiation can be prescribed for millions of CT patients at any facility, while maintaining the level of diagnostic information required for high quality patient care.",Dissemination of a Software Platform for Efficient CT Radiation Dose Optimization and Diagnostic Performance Assessment,10019348,U24EB028936,"['Address', 'Adoption', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Dose', 'Educational workshop', 'Electronic Mail', 'Encapsulated', 'Ensure', 'Evaluation', 'Exposure to', 'Funding', 'Goals', 'Human', 'Image', 'Imaging Phantoms', 'Knowledge', 'Laboratories', 'Lesion', 'Libraries', 'Manufacturer Name', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Newsletter', 'Noise', 'Online Systems', 'Patient Care', 'Patients', 'Performance', 'Play', 'Privatization', 'Protocols documentation', 'Publications', 'Radiation', 'Radiation Dose Unit', 'Radiation exposure', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Scanning', 'Signal Transduction', 'Software Tools', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'United States National Institutes of Health', 'Validation', 'Variant', 'Visual', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical practice', 'clinical research site', 'computerized data processing', 'deep learning', 'improved', 'innovation', 'interest', 'novel', 'novel diagnostics', 'open source', 'reconstruction', 'response', 'symposium', 'tool', 'validation studies', 'web site']",NIBIB,MAYO CLINIC ROCHESTER,U24,2020,314388,-0.011501521887057997
"Dissemination of a Software Platform for Efficient CT Radiation Dose Optimization and Diagnostic Performance Assessment PROJECT SUMMARY/ABSTRACT With the introduction of many novel techniques to minimize radiation dose in CT, there is still a large variation in terms of radiation dose levels prescribed in CT exams and therefore a large variation of diagnostic performance. Some patients may receive higher dose than necessary. Some may be under-dosed and mis- diagnosed as a result of insufficient image quality. In order to determine the appropriate amount of radiation dose reduction in each exam, accurate quantification of diagnostic performance is needed so that the dose reduction can be achieved without sacrificing important diagnostic information. However, currently there is a lack of efficient and quantitative tools for objective assessment of diagnostic performance, particularly for many of the novel dose reduction methods involving non-linear processing of the data such as iterative reconstruction and deep-learning-based noise reduction methods. The specific goal of this application is to disseminate a highly automated solution, CT Protocol optimization (CTPro) software, to a wide CT community. This quantitative tool provides an efficient implementation of diagnostic performance assessment and CT radiation dose optimization. This tool is based on channelized Hotelling observer (CHO), which itself was developed decades ago to mimic human observer visual responses in signal detection tasks. However, the use of CHO in clinical CT is quite limited because of a lack of rigorous validation and efficient and robust implementation in practice. We were the first to demonstrate its correlation with human observer performance in low-contrast detection, classification and localization tasks in clinical CT. The main objective of the current proposal is to optimize this tool for simplicity and robustness, and disseminate it to CT researchers and clinical users, which will be accomplished through 3 specific aims: Aim 1: Optimize CTPro for simplicity, robustness, and generalizability. Aim 2: Develop an open-source web-based platform for software dissemination. Aim 3: Build use cases and disseminate CTPro. The proposed work is significant because the software tool will allow any CT users and researchers to perform CT radiation dose optimization and diagnostic performance evaluation in an efficient, quantitative, and objective manner. This work is innovative in that the automated tool will use quantitative measures of diagnostic performance to systematically guide the complex task of CT dose optimization, moving beyond traditional metrics that are inappropriate for many novel dose reduction techniques. The software tool, once widely employed, will facilitate a paradigm shift in how dose optimization and the evaluation of dose reduction techniques are performed, and will allow a more rapid and consistent adoption of dose reduction technology into clinical practice, which will benefit millions of CT patients. PROJECT NARRATIVE There has been a lack of quantitative tools for efficient and objective assessment of diagnostic performance in CT, which is the reason why inappropriate radiation dose is frequently used in CT exams, resulting in unnecessarily high radiation exposure to patients or lose of important diagnostic information. The purpose of this project is to disseminate a highly automated solution to a wide CT community for efficient CT radiation dose optimization. If successful, appropriate amount of radiation can be prescribed for millions of CT patients at any facility, while maintaining the level of diagnostic information required for high quality patient care.",Dissemination of a Software Platform for Efficient CT Radiation Dose Optimization and Diagnostic Performance Assessment,9881453,U24EB028936,"['Address', 'Adoption', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Dose', 'Educational workshop', 'Electronic Mail', 'Encapsulated', 'Ensure', 'Evaluation', 'Exposure to', 'Funding', 'Goals', 'Human', 'Image', 'Imaging Phantoms', 'Knowledge', 'Laboratories', 'Lesion', 'Libraries', 'Manufacturer Name', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Newsletter', 'Noise', 'Online Systems', 'Patient Care', 'Patients', 'Performance', 'Play', 'Privatization', 'Protocols documentation', 'Publications', 'Radiation', 'Radiation Dose Unit', 'Radiation exposure', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Scanning', 'Signal Transduction', 'Software Tools', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'United States National Institutes of Health', 'Validation', 'Variant', 'Visual', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical practice', 'clinical research site', 'computerized data processing', 'deep learning', 'improved', 'innovation', 'interest', 'novel', 'novel diagnostics', 'open source', 'reconstruction', 'response', 'symposium', 'tool', 'validation studies', 'web site']",NIBIB,MAYO CLINIC ROCHESTER,U24,2019,397500,-0.011501521887057997
"Data-driven Head Motion Correction in PET Imaging Using Deep Learning Project Summary Positron-emission tomography (PET) is an imaging modality that allows clinicians and researchers to study the physiological or pathological processes of the human body, and in particular the brain via the use of specific tracers. For brain PET imaging, patient head movement during scanning presents a challenge for accurate PET image reconstruction and subsequent quantitative analysis. Problems due to head motion are exacerbated by the long duration of the scans, with scan times commonly over one hour. Furthermore, some PET studies specifically involve subjects that either have trouble staying still due to psychological variations, e.g. patients with neurodegenerative disorders such as Alzheimer's disease and Parkinson's disease, or psychological variations, e.g. subjects with anxiety disorders, or are required to participate in tasks that involve movement, e.g. smoking cigarettes while scanning. In brain scans, the average head motion can vary from 7 mm in clinical scans to triple this amount for longer research scans. Quantitatively, a 5 mm head motion can produce biases of up to ~35% in regional intensities and ∼15% in volume of distribution estimates, which could much larger than the difference observed in regional intensities or binding potential that distinguish different demographic groups being studied. The ability to track and correct head motion, therefore, would be of high utility in both clinical and research PET studies. In the past, many motion correction methods have been proposed. However, except for hardware-based approaches, there has been no method that can track frequent head motion on-the-fly during the PET acquisition. Hardware-based approaches are not readily available for clinical translation or used by other research facilities due to highly-customized software/hardware setup. To address this challenge, we propose to develop a data-driven methodology using deep learning to track and estimate rigid head motion using PET raw data, and incorporate both tracer type and time as conditional variables into this deep neural network design in order to handle diverse PET tracer types and their dynamic behavior. Overall, these solutions will provide for a data-driven motion estimation methodology to improve the quality of PET imaging. Specifically, we will start with the development and testing of our methodology for rigid head motion estimation using single-tracer PET raw data. Then we will perform evaluation of our multi-tracer motion estimation methodology applied to real PET data with a diverse range of tracers. Finally, in the exploratory phase, we will integrate time-of-flight information into deep learning-based motion prediction. The significance of this proposal is that it will allow for improved quality of PET imaging in real time and potentially allow for its use in clinical PET systems that do not have special motion tracking hardware. This work will serve as a first step towards developing data-driven motion estimation algorithms for full body PET imaging. The innovation lies in the development of what is a data-driven solution to the problem of real time motion estimation. Project Narrative Positron-emission tomography (PET) imaging of the brain is a highly useful tool for biomedical research and clinical practice. Head motion during scanning degrades PET image quality and introduces image artifacts. We propose to develop new data-driven methods, based on PET raw data, to estimate head motion using deep learning, which can be used for real time motion estimation in PET imaging.",Data-driven Head Motion Correction in PET Imaging Using Deep Learning,9877261,R21EB028954,"['Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anxiety Disorders', 'Behavior', 'Binding', 'Biomedical Research', 'Brain', 'Brain imaging', 'Brain scan', 'Cigarette', 'Clinical', 'Clinical Research', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Development', 'Devices', 'Effect Modifiers (Epidemiology)', 'Evaluation', 'Event', 'Funding', 'Gold', 'Head', 'Head Movements', 'Hour', 'Human', 'Human body', 'Image', 'Individual', 'Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Morphologic artifacts', 'Motion', 'Movement', 'Neurodegenerative Disorders', 'Parkinson Disease', 'Pathologic Processes', 'Patients', 'Performance', 'Phase', 'Physiological Processes', 'Positron-Emission Tomography', 'Research', 'Research Personnel', 'Scanning', 'Smoking', 'Synapses', 'System', 'Testing', 'Time', 'Tracer', 'Training', 'Variant', 'Work', 'base', 'clinical practice', 'clinical translation', 'deep learning', 'deep neural network', 'density', 'design', 'effectiveness evaluation', 'fluorodeoxyglucose', 'image reconstruction', 'imaging modality', 'improved', 'innovation', 'interest', 'neural network', 'novel', 'psychologic', 'reconstruction', 'research facility', 'simulation', 'statistics', 'tool']",NIBIB,YALE UNIVERSITY,R21,2020,243150,-0.0132624493335798
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,10019388,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2020,381282,-0.02144430571390646
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,9882822,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2019,390806,-0.02144430571390646
"ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis Project Summary The morphology (or shape) of anatomical structures forms the common language among clinicians, where ab- normalities in anatomical shapes are often tied to deleterious function. While these observations are often quali- tative, ﬁnding subtle, quantitative shape effects requires the application of mathematics, statistics, and computing to parse the anatomy into a numerical representation that will facilitate testing of biologically relevant hypotheses. Particle-based shape modeling (PSM) and its associated suite of software tools, ShapeWorks, enable learning population-level shape representation via automatic dense placement of homologous landmarks on image seg- mentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. Despite its obvious utility for the research enterprise and highly permissive open-source license, ShapeWorks does not have a viable commercialization path due to the inherent trade-off between development and maintenance costs, and a specialized scientiﬁc and clinical market. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread ap- plicability to medicine and biology is hindered by several barriers that most existing shape modeling packages face. The most important roadblocks are (1) the complexity and steep learning curve of existing shape modeling pipelines and their increased computational and computer memory requirements; (2) the considerable expertise, time, and effort required to segment anatomies of interest for statistical analyses; and (3) the lack of interoperable implementations that can be readily incorporated into biomedical research laboratories. In this project, we pro- pose ShapeWorksStudio, a software suite that leverages ShapeWorks for the automated population-/patient-level modeling of anatomical shapes, and Seg3D – a widely used open-source tool to visualize and process volumet- ric images – for ﬂexible manual/semiautomatic segmentation and interactive manual correction of segmented anatomy. In Aim 1, we will integrate ShapeWorks and Seg3D in a framework that supports big data cohorts to enable users to transparently proceed from image data to shape models in a straightforward manner. In Aim 2, we will endow Seg3D with a machine learning approach that provides automated segmentations within a statisti- cal framework that combines image data with population-speciﬁc shape priors provided by ShapeWorks. In Aim 3, we will support interoperability with existing open-source software packages and toolkits, and provide bindings to commonly used programming languages in the biomedical research community. To promote reproducibility, we will develop and disseminate standard workﬂows and domain-speciﬁc test cases. This project combines an interdisciplinary research and development team with decades of experience in statistical analysis and image understanding, and application scientists to conﬁrm that the proposed developments have a real impact on the biomedical and clinical research communities. Our long-term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. ShapeWorks has been effective in a range of applications, including psychology, biological phenotyping, cardiology, and orthopedics. If funded, this application will ensure the viability of ShapeWorks in the face of the ever-increasing complexity of shape datasets and support its availability to biomedical researchers in the future, as well as provide opportunities for use in a wide spectrum of new biological and clinical applications, including anatomy reconstruction from sparse/low- dimensional imaging data, large-scale clinical trials, surgical planning, optimal designs of medical implants, and reconstructive surgery.","ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis",9882865,U24EB029011,"['Address', 'Adoption', 'Anatomic Models', 'Anatomy', 'Applied Research', 'Area', 'Big Data', 'Binding', 'Biological', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Cardiology', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communities', 'Complex', 'Complex Analysis', 'Computer software', 'Computers', 'Consensus', 'Data', 'Data Set', 'Development', 'Dimensions', 'Electronic Mail', 'Ensure', 'Exhibits', 'Face', 'Funding', 'Future', 'Goals', 'Image', 'Interdisciplinary Study', 'Laboratory Research', 'Language', 'Learning', 'Licensing', 'Machine Learning', 'Maintenance', 'Manuals', 'Mathematics', 'Measures', 'Medical', 'Medicine', 'Memory', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Morphology', 'Normalcy', 'Operative Surgical Procedures', 'Orthopedics', 'Phenotype', 'Population', 'Process', 'Programming Languages', 'Psychology', 'Reconstructive Surgical Procedures', 'Reproducibility', 'Research', 'Research Personnel', 'Scientist', 'Shapes', 'Software Engineering', 'Software Tools', 'Statistical Data Interpretation', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Work', 'base', 'clinical application', 'clinical care', 'clinical investigation', 'cohort', 'commercialization', 'computerized tools', 'cost', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'interest', 'interoperability', 'medical implant', 'open source', 'outreach', 'particle', 'patient population', 'reconstruction', 'research and development', 'shape analysis', 'software development', 'statistics', 'tool', 'usability', 'user-friendly']",NIBIB,UNIVERSITY OF UTAH,U24,2019,340827,-0.01775798251842648
"ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis Project Summary The morphology (or shape) of anatomical structures forms the common language among clinicians, where ab- normalities in anatomical shapes are often tied to deleterious function. While these observations are often quali- tative, ﬁnding subtle, quantitative shape effects requires the application of mathematics, statistics, and computing to parse the anatomy into a numerical representation that will facilitate testing of biologically relevant hypotheses. Particle-based shape modeling (PSM) and its associated suite of software tools, ShapeWorks, enable learning population-level shape representation via automatic dense placement of homologous landmarks on image seg- mentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. Despite its obvious utility for the research enterprise and highly permissive open-source license, ShapeWorks does not have a viable commercialization path due to the inherent trade-off between development and maintenance costs, and a specialized scientiﬁc and clinical market. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread ap- plicability to medicine and biology is hindered by several barriers that most existing shape modeling packages face. The most important roadblocks are (1) the complexity and steep learning curve of existing shape modeling pipelines and their increased computational and computer memory requirements; (2) the considerable expertise, time, and effort required to segment anatomies of interest for statistical analyses; and (3) the lack of interoperable implementations that can be readily incorporated into biomedical research laboratories. In this project, we pro- pose ShapeWorksStudio, a software suite that leverages ShapeWorks for the automated population-/patient-level modeling of anatomical shapes, and Seg3D – a widely used open-source tool to visualize and process volumet- ric images – for ﬂexible manual/semiautomatic segmentation and interactive manual correction of segmented anatomy. In Aim 1, we will integrate ShapeWorks and Seg3D in a framework that supports big data cohorts to enable users to transparently proceed from image data to shape models in a straightforward manner. In Aim 2, we will endow Seg3D with a machine learning approach that provides automated segmentations within a statisti- cal framework that combines image data with population-speciﬁc shape priors provided by ShapeWorks. In Aim 3, we will support interoperability with existing open-source software packages and toolkits, and provide bindings to commonly used programming languages in the biomedical research community. To promote reproducibility, we will develop and disseminate standard workﬂows and domain-speciﬁc test cases. This project combines an interdisciplinary research and development team with decades of experience in statistical analysis and image understanding, and application scientists to conﬁrm that the proposed developments have a real impact on the biomedical and clinical research communities. Our long-term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. ShapeWorks has been effective in a range of applications, including psychology, biological phenotyping, cardiology, and orthopedics. If funded, this application will ensure the viability of ShapeWorks in the face of the ever-increasing complexity of shape datasets and support its availability to biomedical researchers in the future, as well as provide opportunities for use in a wide spectrum of new biological and clinical applications, including anatomy reconstruction from sparse/low- dimensional imaging data, large-scale clinical trials, surgical planning, optimal designs of medical implants, and reconstructive surgery.","ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis",10023935,U24EB029011,"['Address', 'Adoption', 'Anatomic Models', 'Anatomy', 'Applied Research', 'Area', 'Big Data', 'Binding', 'Biological', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Cardiology', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communities', 'Complex', 'Complex Analysis', 'Computer software', 'Computers', 'Consensus', 'Data', 'Data Set', 'Development', 'Dimensions', 'Electronic Mail', 'Ensure', 'Exhibits', 'Face', 'Funding', 'Future', 'Goals', 'Image', 'Interdisciplinary Study', 'Laboratory Research', 'Language', 'Learning', 'Licensing', 'Machine Learning', 'Maintenance', 'Manuals', 'Mathematics', 'Measures', 'Medical', 'Medicine', 'Memory', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Morphology', 'Normalcy', 'Operative Surgical Procedures', 'Orthopedics', 'Phenotype', 'Population', 'Process', 'Programming Languages', 'Psychology', 'Reconstructive Surgical Procedures', 'Reproducibility', 'Research', 'Research Personnel', 'Scientist', 'Shapes', 'Software Engineering', 'Software Tools', 'Statistical Data Interpretation', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Work', 'automated segmentation', 'base', 'clinical application', 'clinical care', 'clinical investigation', 'cohort', 'commercialization', 'computerized tools', 'cost', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'interest', 'interoperability', 'medical implant', 'open source', 'outreach', 'particle', 'patient population', 'reconstruction', 'research and development', 'shape analysis', 'software development', 'statistics', 'tool', 'usability', 'user-friendly']",NIBIB,UNIVERSITY OF UTAH,U24,2020,256578,-0.01775798251842648
"Multi-Task MR Simulation for Abdominal Radiation Treatment Planning ABSTRACT The accuracy of radiation treatment planning (RTP) heavily influences the effectiveness of external beam radiotherapy (EBRT). Individualized RTP begins with a “simulation”, in which the patient in a treatment position is commonly scanned using computed tomography (CT) to define the treatment target and organs at risk (OARs). When soft-tissue contrast is inadequate to support accurate target and OAR delineation in CT based RTP, conservatively large treatment margins are used to avoid a geometric miss. The crude treatment prevents delivering sufficient radiation dose to the tumor without exceeding the tolerance of surrounding normal tissues. Magnetic resonance (MR) can be used as a simulation platform complementary to CT for improved soft-tissue conspicuity. Yet, such a complicated, costly and tedious multi-modal RTP workflow along with unavoidable systematic MR-CT co-registration errors has limited its applications in EBRT, especially at the abdominal site whereby anatomies are highly mobile. Over the past few years, there is a keen interest in the integration of MR alone into RTP and even the therapy workflow (i.e. MR-guided radiotherapy, MRgRT). The abdomen poses critical challenges to MR simulation. Current MR imaging sequences are suboptimal to produce motion-free images and resolve respiratory motion. MR data processing for abdominal RTP is underdeveloped. Contouring of OARs typically relies on manual, tedious procedures that are time-consuming and variation-prone. In this proposal, we will substantially improve the MR acquisition and multi-organ auto-segmentation, so the potential of MR as a simulation modality can be fully unleashed for abdominal EBRT. Three specific aims will be completed. In Aim 1, we will develop a standalone multi-task MR (MT-MR) sequence dedicated to abdominal MR simulation. In Aim 2, we will optimize multi-organ auto-segmentation based on MT-MR images. In Aim 3, we will assess the performance of MT-MR in the context of pancreatic cancer stereotactic body radiotherapy planning. Successful completion of the project will dramatically improve treatment precision and clinical outcomes, thus further promoting the adoption of radiotherapy in the management of abdominal cancers. Moreover, the developed techniques will open the door to future studies aiming at optimizations in many aspects of radiotherapy. PROJECT NARRATIVE Imaging is essential for precise radiation treatment planning. MR based planning is challenging in the abdomen whereby anatomies are highly mobile. We will substantially improve the MR acquisition and multi-organ auto- segmentation, so the potential of MR as an imaging-based planning modality can be fully unleashed for abdominal radiation treatment.",Multi-Task MR Simulation for Abdominal Radiation Treatment Planning,10053211,R01EB029088,"['3-Dimensional', '4D MRI', 'Abdomen', 'Adoption', 'Agreement', 'Algorithms', 'Anatomy', 'Breathing', 'Clinical', 'Consumption', 'Data', 'Data Collection', 'Development', 'Dose', 'Effectiveness', 'Fatty acid glycerol esters', 'Future', 'Geometry', 'Goals', 'Image', 'Institution', 'Interobserver Variability', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant neoplasm of abdomen', 'Malignant neoplasm of pancreas', 'Manuals', 'Methods', 'Modality', 'Motion', 'Normal tissue morphology', 'Organ', 'Outcome', 'Patients', 'Performance', 'Phase', 'Planning Techniques', 'Positioning Attribute', 'Precision therapeutics', 'Procedures', 'Protons', 'Radiation Dose Unit', 'Radiation therapy', 'Research', 'Risk', 'Scanning', 'Site', 'Solid', 'Study Subject', 'Techniques', 'Testing', 'Three-Dimensional Image', 'Time', 'Tissues', 'Translations', 'Variant', 'Water', 'Weight', 'X-Ray Computed Tomography', 'automated segmentation', 'base', 'computerized data processing', 'contrast imaging', 'cost', 'deep learning', 'density', 'experience', 'image reconstruction', 'improved', 'interest', 'multimodality', 'multitask', 'novel', 'prevent', 'reconstruction', 'respiratory', 'segmentation algorithm', 'simulation', 'soft tissue', 'success', 'treatment planning', 'tumor']",NIBIB,CEDARS-SINAI MEDICAL CENTER,R01,2020,556712,0.009558308003240423
"CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience The field of network neuroscience has developed powerful analysis tools for studying brain networks and holds promise for deepening our understanding of the role played by brain networks in health, disease, development, and cognition. Despite widespread interest, barriers exist that prevent these tools from having broader impact. These include (1) unstandardized practices for sharing and documenting software, (2) long delays from when a method is first introduced to when it becomes publicly available, and (3) gaps in theoretic knowledge and understanding leading to incorrect, delays due to mistakes, and errors in reported results. These barriers ultimately slow the rate of neuroscientific discovery and stall progress in applied domains. To overcome these challenges, we will use open science methods and cloud-computing, to increase the availability of network neuroscience tools. We will use the platform ""brainlife.io"" for sharing these tools, which will be packaged into self-contained, standardized, reproducible Apps, shared with and modified by a community of users, and integrated into existing brainlife.io analysis pipelines. Apps will also be accompanied by links to primary sources, in-depth tutorials, and documentation, and worked-through examples, highlighting their correct usage and offering solutions for mitigating possible pitfalls. In standardizing and packaging network neuroscience tools as Apps, this proposed research will engage a new generation of neuroscientists, providing them powerful new and leading to new discoveries. Second, the proposed research will contribute growing suite of modeling analysis that can be modified to suit specialized purposes. Finally, the Brainlife.io platform will serve as part of the infrastructure supporting neuroscience research. Altogether, these advances will lead to new opportunities in network neuroscience research and further stimulate its growth while increasing synergies with other domains in neuroscience. Structural and functional networks support cognitive processes. Miswiring networks lead to maladaptive behavior and neuropsychicatric disorders. Network neuroscience is a young field that provides a quantitative framework for modeling brain networks. This project will make network neuroscientific tools available to new users via open science and cloud-computing. New applications of these tools this will lead deeper insight into the role of networks in health as well as in clinical disorders.",CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience,9916138,R01EB029272,"['Address', 'Aging', 'Biophysics', 'Brain', 'Cloud Computing', 'Cognition', 'Communities', 'Complex Analysis', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Documentation', 'Ecosystem', 'Education', 'Elements', 'France', 'Funding', 'Generations', 'Graph', 'Growth', 'Instruction', 'Knowledge', 'Language', 'Lead', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Mathematics', 'Methods', 'Modeling', 'Neurosciences', 'Pathway Analysis', 'Play', 'Process', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Science', 'Sociology', 'Standardization', 'Structure', 'Study models', 'System', 'Techniques', 'Time', 'Training', 'analysis pipeline', 'brain computer interface', 'cloud based', 'cyber infrastructure', 'data sharing', 'experience', 'innovation', 'insight', 'learning strategy', 'network architecture', 'network models', 'neuroimaging', 'open data', 'prevent', 'relating to nervous system', 'statistics', 'tool']",NIBIB,INDIANA UNIVERSITY BLOOMINGTON,R01,2019,215134,-0.022047547651786917
"CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience The field of network neuroscience has developed powerful analysis tools for studying brain networks and holds promise for deepening our understanding of the role played by brain networks in health, disease, development, and cognition. Despite widespread interest, barriers exist that prevent these tools from having broader impact. These include (1) unstandardized practices for sharing and documenting software, (2) long delays from when a method is first introduced to when it becomes publicly available, and (3) gaps in theoretic knowledge and understanding leading to incorrect, delays due to mistakes, and errors in reported results. These barriers ultimately slow the rate of neuroscientific discovery and stall progress in applied domains. To overcome these challenges, we will use open science methods and cloud-computing, to increase the availability of network neuroscience tools. We will use the platform ""brainlife.io"" for sharing these tools, which will be packaged into self-contained, standardized, reproducible Apps, shared with and modified by a community of users, and integrated into existing brainlife.io analysis pipelines. Apps will also be accompanied by links to primary sources, in-depth tutorials, and documentation, and worked-through examples, highlighting their correct usage and offering solutions for mitigating possible pitfalls. In standardizing and packaging network neuroscience tools as Apps, this proposed research will engage a new generation of neuroscientists, providing them powerful new and leading to new discoveries. Second, the proposed research will contribute growing suite of modeling analysis that can be modified to suit specialized purposes. Finally, the Brainlife.io platform will serve as part of the infrastructure supporting neuroscience research. Altogether, these advances will lead to new opportunities in network neuroscience research and further stimulate its growth while increasing synergies with other domains in neuroscience. Structural and functional networks support cognitive processes. Miswiring networks lead to maladaptive behavior and neuropsychicatric disorders. Network neuroscience is a young field that provides a quantitative framework for modeling brain networks. This project will make network neuroscientific tools available to new users via open science and cloud-computing. New applications of these tools this will lead deeper insight into the role of networks in health as well as in clinical disorders.",CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience,10019389,R01EB029272,"['Address', 'Aging', 'Behavior', 'Biophysics', 'Brain', 'Clinical', 'Cloud Computing', 'Cognition', 'Communities', 'Complex Analysis', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Documentation', 'Ecosystem', 'Education', 'Elements', 'France', 'Funding', 'Generations', 'Graph', 'Growth', 'Health', 'Infrastructure', 'Instruction', 'Knowledge', 'Language', 'Lead', 'Libraries', 'Link', 'Literature', 'Mathematics', 'Methods', 'Modeling', 'Neurosciences', 'Neurosciences Research', 'Pathway Analysis', 'Play', 'Process', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Science', 'Sociology', 'Source', 'Standardization', 'Structure', 'Study models', 'System', 'Techniques', 'Time', 'Training', 'Work', 'analysis pipeline', 'brain computer interface', 'cloud based', 'cognitive process', 'cyber infrastructure', 'data sharing', 'experience', 'innovation', 'insight', 'interest', 'machine learning method', 'network architecture', 'network models', 'neuroimaging', 'open data', 'prevent', 'relating to nervous system', 'statistics', 'support network', 'synergism', 'tool']",NIBIB,INDIANA UNIVERSITY BLOOMINGTON,R01,2020,218594,-0.022047547651786917
"Reconstruction of robust, high SNR functional brain images using machine learning and oscillatory steady state MRI Project Summary/Abstract: Functional magnetic resonance imaging (fMRI) is an important tool both clinically and in scientific research, with broad applications ranging from cognitive neuroscience to presurgical planning. FMRI can generate whole brain neuronal activation maps, yet it is an inherently low signal to noise ratio (SNR) method due to the relatively small changes in activation signal relative to the baseline signal. The overarching goal of this project is to develop and validate robust, high SNR fMRI by using a novel acquisition method, oscillating steady state (OSS) fMRI, along with machine learning (ML) techniques, to reconstruct high-quality images of the OSS fMRI data. OSS fMRI can increase SNR by >2x compared to the current state-of-the-art in fMRI acquisition methods, and this boost is roughly equivalent to the SNR gain going from a 3T MRI scanner to a 7T MRI scanner. The SNR increase is a direct result of the steady state approach. However, with a more complex, oscillating acquisition, OSS fMRI can be more susceptible to common MRI artifacts than traditional methods if left uncorrected. Two of the most common sources of MRI artifacts are changes in the main magnetic field (B0) due to physiological noise (such as respiration) and patient motion. This project will develop robust, high SNR fMRI at 3T by incorporating the effects of (1) B0 changes and (2) patient motion into the OSS signal model and image reconstruction algorithms. A new image reconstruction that incorporates neural networks will correct for B0 fluctuations and remove B0 induced artifacts. We will train a neural network to generate B0 field maps using data from a conventional, physics-based two echo field mapping technique to implicitly incorporate prior information of the physics into the reconstruction, while also providing a fast, ML-based field mapping method. To correct for subject motion, we will develop a neural network that estimates rigid-body motion parameters from sequential image frames in the fMRI scan. These motion parameters will be used in an iterative image reconstruction to produce high-quality resting-state and task-based fMRI, even in the presence of subject motion. The technology developed in this proposal will result in improved functional MRI that has the potential to significantly advance both the study of the human brain and the treatment of neurological disorders. The University of Michigan is one of the top research universities in the US, and provides an ideal environment and infrastructure to complete the proposed research strategy. The Functional Magnetic Resonance Laboratory and the Electrical Engineering and Computer Science Department at UM have all the necessary hardware and computational resources needed for this project, including two state-of-the-art GE 3T MRI scanners and extensive GPU hardware for the machine learning components of the project. Furthermore, Drs. Jeffrey Fessler and Douglas Noll have proven expertise in fMRI image acquisition and reconstruction, as well as extensive mentorship experience, that will help guide this project and my training. Project Narrative Functional magnetic resonance imaging (fMRI) is used to measure brain function, and it has revolutionized our understanding of cognitive processes during the last 25 years and has also been used as a tool for presurgical mapping of language and other sensitive brain regions. More recently, fMRI is being used as a biomarker for the progression of neurologic and psychiatric diseases, for example, in Alzheimer’s disease, multiple sclerosis, and major depression. In this project we will improve fMRI techniques by developing new methods that are more robust to common sources of MRI image degradation, which will improve the sharpness of the fMRI images without increasing instrumentation costs.","Reconstruction of robust, high SNR functional brain images using machine learning and oscillatory steady state MRI",10067072,F32EB029289,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Brain', 'Brain imaging', 'Brain region', 'Clinical', 'Clinical Research', 'Complex', 'Data', 'Dictionary', 'Echo-Planar Imaging', 'Electrical Engineering', 'Elements', 'Environment', 'Functional Magnetic Resonance Imaging', 'Goals', 'Head', 'Head Movements', 'Human', 'Hybrids', 'Image', 'Imaging Phantoms', 'Imaging Techniques', 'Infrastructure', 'Laboratories', 'Language', 'Left', 'Length', 'Location', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Major Depressive Disorder', 'Maps', 'Measures', 'Mental disorders', 'Mentorship', 'Methods', 'Michigan', 'Modeling', 'Morphologic artifacts', 'Motion', 'Multiple Sclerosis', 'Neurologic Process', 'Neurons', 'Noise', 'Patients', 'Physics', 'Physiological', 'Research', 'Resolution', 'Respiration', 'Rest', 'Scanning', 'Signal Transduction', 'Site', 'Source', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Training', 'Universities', 'base', 'blood oxygen level dependent', 'cognitive neuroscience', 'cognitive process', 'computer science', 'computing resources', 'cost', 'experience', 'functional MRI scan', 'image reconstruction', 'imaging modality', 'improved', 'in vivo', 'instrumentation', 'magnetic field', 'nervous system disorder', 'neural network', 'novel', 'open source', 'progression marker', 'reconstruction', 'tool']",NIBIB,UNIVERSITY OF MICHIGAN AT ANN ARBOR,F32,2020,64926,0.01960758918592356
"Machine learning for fast motion compensated quantitative abdominal DCE-MRI Project Summary: Functional imaging with dynamic contrast-enhanced MRI (DCE-MRI) provides important physiological markers of permeability, perfusion and glomerular filtration rate (GFR), a measure of kidney function, without exposing patients to ionizing radiation. DCE-MR images are at the same time used for evaluation of anatomy. Functional markers from DCE-MRI, if computed accurately, would play a critical role in diagnosing and assessing the progression of a number of pediatric diseases including those compromising kidney function, liver diseases, tumors, and Crohn's disease. One of the most important applications of DCE-MRI is assessing kidney function (GFR) in hydronephrosis patients with obstruction. In the absence of GFR information, children who stand to benefit from immediate surgical reconstruction might be overlooked or delayed in receiving treatment, and those who might benefit from a more conservative approach (i.e., “watchful waiting”) might receive an unnecessary surgical intervention. While the current reference standard, nuclear renography (MAG3), yields some useful diagnostic information, it is slow, provides low resolution, does not offer anatomic detail, and delivers potentially harmful ionizing radiation. There is a clinical need for accurate computation of quantitative functional markers. Unfortunately, current methods of DCE-MRI in neonates and children are less than optimal, and therefore, DCE-MRI is underutilized in clinical practice. The technical challenges include insufficient temporal resolution to capture fast arterial input function (AIF) dynamics (which are required for accurate computation of quantitative markers), unavoidable respiratory motion and bulk motion (which reduce image quality and significantly lower the accuracy of parameter estimates), and a lack of robust, fast, automated post processing techniques for accurate computation of markers. Thus, there is an urgent, unmet need to develop a motion-compensated, high spatiotemporal resolution DCE-MRI method addressing these challenges. The primary objective of this exploratory, three-year study, is three-fold: first, to develop and evaluate a new bulk and respiratory motion-compensated, high spatiotemporal resolution DCE-MRI technique for accurate estimation of functional markers; second, to further improve the robustness and speed of DCE-MRI using a fast, deep learning (DL) technique with integrated temporal prior for the reconstruction of motion-compensated, higher quality, high temporal resolution images; and third, to develop an automatic quantitative analysis pipeline including segmentation and tracer kinetic model-fitting using DL techniques for fast, robust and accurate quantification of functional markers. The successful completion of these aims will provide new, clinically important abdominal imaging capabilities, with real-time, motion-compensated image reconstruction and reliable real-time parameter estimation from high temporal and spatial resolution DCE-MRI. This work will extend the usefulness of DCE-MRI to pediatric patients who are unable to remain still in the scanner, and eliminate the need for repeated scans and sedation in infants. ! ! Project Narrative: This project addresses the need to develop advanced methods of magnetic resonance imaging (MRI) to provide new, clinically important abdominal imaging capabilities, with real-time, motion-compensated image reconstruction and reliable real-time estimation of clinically important quantitative imaging markers from high temporal and spatial resolution DCE-MRI. These markers will be used to evaluate the extent of several disorders and would play a critical role in diagnosing and assessing the progression of a number of pediatric diseases including those compromising kidney function, liver diseases, tumors, and Crohn's disease. This work will extend the usefulness of DCE-MRI to pediatric patients who are unable to remain still in the scanner, and eliminate the need of repeated scans, sedation and anesthesia when imaging newborns with congenital abnormalities such as congenital hydronephrosis, which if left untreated, can result in permanent damage to the child's kidneys.",Machine learning for fast motion compensated quantitative abdominal DCE-MRI,9957672,R21EB029627,"['Abdomen', 'Address', 'Affect', 'Algorithms', 'Anatomy', 'Anesthesia procedures', 'Breathing', 'Child', 'Childhood', 'Clinical', 'Congenital Abnormality', 'Crohn&apos', 's disease', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline of Nuclear Medicine', 'Disease', 'Disease Marker', 'Evaluation', 'Failure', 'Financial compensation', 'Functional Imaging', 'Glomerular Filtration Rate', 'Hydronephrosis', 'Image', 'Imaging Techniques', 'Infant', 'Ionizing radiation', 'Kidney', 'Lead', 'Left', 'Liver diseases', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Motion', 'Newborn Infant', 'Nuclear', 'Obstruction', 'Operative Surgical Procedures', 'Organ', 'Patient observation', 'Patients', 'Performance', 'Perfusion', 'Permeability', 'Physiological', 'Play', 'Reference Standards', 'Renal function', 'Resolution', 'Role', 'Scanning', 'Sedation procedure', 'Series', 'Signal Transduction', 'Speed', 'Techniques', 'Time', 'Tracer', 'Ureteropelvic junction obstruction', 'Work', 'analysis pipeline', 'anxious', 'base', 'bulk motion', 'clinical decision-making', 'clinical practice', 'contrast enhanced', 'deep learning', 'image reconstruction', 'imaging biomarker', 'imaging capabilities', 'imaging modality', 'improved', 'kinetic model', 'neonate', 'neural network architecture', 'pediatric patients', 'quantitative imaging', 'radiologist', 'real-time images', 'reconstruction', 'recursive neural network', 'respiratory', 'spatiotemporal', 'temporal measurement', 'time use', 'tumor']",NIBIB,BOSTON CHILDREN'S HOSPITAL,R21,2020,708000,-0.10434610405017497
"NeuroExplorer: Ultra-high Performance Human Brain PET Imager for Highly-resolved In Vivo Imaging of Neurochemistry Research applications of brain Positron Emission Tomography (PET) have been in place for over 40 years. The combination of quantitative PET systems with novel radiotracers has led to a numerous imaging para- digms to understand normal brain physiology including neurotransmitter dynamics and receptor pharmacology at rest and during activation. Brain-dedicated PET systems offer important advantages over currently available PET systems in terms of sensitivity and resolution. However, the state-of-the-art for brain PET has not progressed beyond the 20-year-old HRRT. Therefore, there is a compelling need to build the next generation of brain PET systems for human studies. This proposal brings together a highly experienced collaborative team from Yale, UC Davis, and United Imaging Healthcare America (UIHA). to develop the next generation NeuroEXPLORER (NX) PET system with the following Aims. Specific Aim 1: Design and Build the NeuroEXPLORER: In 2 years, we will complete the design and build the NX system. The design includes high performance LYSO-SiPM blocks with small detectors, 4-mm depth-of-interaction, 250 ps time-of-flight resolution, and axial length of ~50 cm, paired with CT for attenuation correction. This design will produce a factor of 10 greater effective sensitivity than the HRRT and practical resolution of 1.5-2 mm in the human brain. The system will include built-in real-time state-of-art motion tracking cameras and will be tested using novel phantom experiments to assess the full-range of operation to validate the dramatic improvement in small- region precision and accuracy. Specific Aim 2: Algorithm Development for Fully-Quantitative Brain PET. We will develop the novel algorithms for this system. Using EXPLORER experience. we will implement reconstruction algorithms to produce dynamic images with uniform ultra-high resolution in space and time, Extending Yale’s HRRT motion correction experience, we will develop camera-based motion detection and correction algorithms to deliver ultra-high resolution human brain images. Using the carotid artery shape and geometry, we will develop methods to accurately measure blood activity to be compared to human arterial data with the goal to permit kinetic modeling without arterial sampling. We will develop noise reduction methods with machine learning to reduce dose for studying health brains and to eliminate the need for the CT scan for attenuation correction. Specific Aim 3: Human Paradigm Demonstration. With human subjects, we will evaluate specific imaging paradigms to demonstrate the effectiveness of the NX system: 1) demonstration of the dramatic sensitivity increase (with a direct comparison to the HRRT) and its impact on detection of pharmacologic effects, 2) leveraging high sensitivity to reliably measure uptake in small nuclei; and 3) opening new frontiers of imaging neurotransmitter dynamics, including dopamine and opioid release. The ultimate goal is a fully functioning and characterized system that dramatically expands the scope of brain PET protocols and applications. Human research applications of brain Positron Emission Tomography (PET) imaging have been in place for over 40 years and have led to a detailed understand of normal brain physiology including neurotransmitter dynamics and receptor pharmacology at rest and during activation. Brain-dedicated PET systems offer important advantages over currently available PET systems, but the state-of-the-art for brain PET imaging systems has not progressed beyond the 20-year-old HRRT. The proposed next generation NeuroEXPLORER (NX) PET system will have a factor of 10 greater sensitivity and will dramatically expand the scope of human brain PET protocols and applications.",NeuroExplorer: Ultra-high Performance Human Brain PET Imager for Highly-resolved In Vivo Imaging of Neurochemistry,10005604,U01EB029811,"['20 year old', 'Adolescent', 'Algorithms', 'Americas', 'Area', 'Blood', 'Brain', 'Brain imaging', 'Brain scan', 'Carotid Arteries', 'Cell Nucleus', 'Child', 'Clinical', 'Collaborations', 'Data', 'Detection', 'Development', 'Disease', 'Dopamine', 'Dose', 'Effectiveness', 'Enzymes', 'Evaluation', 'Functional disorder', 'Future', 'Geometry', 'Goals', 'Healthcare', 'Hippocampus (Brain)', 'Human', 'Image', 'Inferior', 'Length', 'Machine Learning', 'Manufacturer Name', 'Measures', 'Metabolism', 'Methods', 'Midbrain structure', 'Modeling', 'Motion', 'Nerve Degeneration', 'Neurotransmitters', 'Noise', 'Opioid', 'Performance', 'Pharmacology', 'Physics', 'Physiology', 'Positron-Emission Tomography', 'Proteins', 'Protocols documentation', 'Radioactive', 'Research', 'Resolution', 'Rest', 'Sampling', 'Sensory Receptors', 'Shapes', 'Structure', 'Substantia nigra structure', 'Synapses', 'System', 'Technology', 'Testing', 'Thalamic structure', 'Time', 'Tracer', 'X-Ray Computed Tomography', 'algorithm development', 'attenuation', 'base', 'body system', 'brain health', 'data quality', 'density', 'design', 'detector', 'experience', 'experimental study', 'frontier', 'human subject', 'imager', 'imaging system', 'improved', 'in vivo imaging', 'kinetic model', 'locus ceruleus structure', 'neurochemistry', 'neurotransmitter release', 'next generation', 'novel', 'operation', 'pre-clinical', 'radiotracer', 'raphe nuclei', 'receptor', 'reconstruction', 'solid state', 'statistics', 'ultra high resolution', 'uptake']",NIBIB,YALE UNIVERSITY,U01,2020,1700000,0.01861080251426611
"A Comparative Framework for Modeling the Low-Dimensional Geometry of Neural Population States Project Summary Advances in neural recording technology now provide access to neural activity at high temporal resolutions, from many brain areas, and during complex and naturalistic behavior. Interpreting these types of high-dimensional and unconstrained neural recordings is still a major challenge in neuroscience. The aim of this project is to develop innovative methods for distilling high-dimensional neural activity patterns into simpler low-dimensional formats that can be effectively compared across time, conditions, or even across species. Our team is uniquely positioned to not only develop these novel methods, but also apply them to characterize changes in neural systems across a wide range of clinically-relevant perturbations, including addiction, sensory manipulation, and disease. In this project, theory, methods, and models will be developed for: 1) learning low-dimensional latent space models that align many neural datasets onto a common reference frame for comparison, 2) comparing datasets and testing the impact of a variety of perturbations (e.g. monocular deprivation, addiction and withdrawal) on the shape or geometry of neural activity from its baseline state, and 3) investigating the role of specific cell types and microcircuits on shaping population activity over time, during sleep, and in response to certain classes of perturbations. This project will provide new tools and frameworks for comparing neural datasets, leading to robust measures of disease, signatures of addiction, and other network-level reflections of environment and behavior. Significance: As neural datasets continue to grow in size, new methods for analysis are becoming of utmost importance in driving scientific understanding of the brain. The methods developed in this proposal will identify new ways to learn network-level signatures that allow us to link and compare different neural activity patterns. A robust ability to compare activity across time and animals will have wide reaching impacts, and provide new tools to advance network-level understanding of disease. Innovation: This project will leverage state-of-the-art approaches in high-dimensional statistics and geometry, which are simultaneously advancing in the context of deep learning (DL) architectures, and to tackle challenges in neural coding. This project represents a truly innovative combination of tools in machine learning and computational neuroscience which will likely transfer knowledge in both directions: from machine learning to neuroscience and back. The unique application of advanced mathematical tools in geometry and optimization to population-level analysis of perturbations will be transformative, not only for neuroscience but also in the study of DL architectures. Project Narrative Interpreting patterns of activity across large networks of neurons is an incredibly challenging problem that remains a key objective of the BRAIN initiative. This project will develop transformative methods for distilling neural activity patterns into compact, informative representations that facilitate comparison across datasets from different brain areas and multiple time-scales, and at different states of neurological disease.",A Comparative Framework for Modeling the Low-Dimensional Geometry of Neural Population States,10007243,R01EB029852,"['Address', 'Animals', 'Architecture', 'Area', 'Automobile Driving', 'BRAIN initiative', 'Back', 'Behavior', 'Behavioral', 'Brain', 'Code', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Esthesia', 'Formulation', 'Foundations', 'Geometry', 'Health', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Neurons', 'Neurophysiology - biologic function', 'Neurosciences', 'Pattern', 'Perception', 'Population', 'Population Dynamics', 'Positioning Attribute', 'Problem Solving', 'Role', 'Rotation', 'Sampling', 'Sensory', 'Shapes', 'Sleep', 'Space Models', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Translations', 'Vision', 'Visual Cortex', 'Withdrawal', 'addiction', 'base', 'cell type', 'clinically relevant', 'comparative', 'computational neuroscience', 'deep learning', 'density', 'deprivation', 'high dimensionality', 'innovation', 'learning network', 'learning strategy', 'lens', 'monocular deprivation', 'nervous system disorder', 'neural circuit', 'neural model', 'novel', 'relating to nervous system', 'response', 'statistics', 'temporal measurement', 'theories', 'tool']",NIBIB,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2020,142747,-0.028867740270216984
"Crossing space and time: uncovering the nonlinear dynamics of multimodal and multiscale brain activity The brain is a complex dynamical system, with a hierarchy of spatial and temporal scales ranging from microns and milliseconds to centimeters and years. Activity at any given scale contributes to activity at the scales above it and can influence activity at smaller scales. Thus a true understanding of the brain requires the ability to understand how each level contributes to the system as a whole. Most brain research focuses on a single scale (single unit firing, activity in a circuit), which cannot account for the constraints imposed by activities at other scales. The goal of this proposal is to develop a framework for the integration of multiscalar, multimodal measurements of brain activity. One of the challenges in understanding how activity translates across scales is that features that are relevant at one scale (e.g., firing rate) do not have clear analogues at other scales. We address this issue by defining trajectories in “state space” at each scale, where the state space is defined by parameters and time scales appropriate to each type of data. The trajectory of brain activity through state space can uncover features like attractor dynamics and limit cycles that characterize the evolution of activity. Using machine learning along with new and existing multimodal measurements of brain activity (MRI, optical, and electrophysiological), we propose to establish methods that relate trajectories across scales while handling the mismatch in temporal sampling rates inherent in multi-scale data. Specific aims are 1. Create and test a tool for learning how trajectories at fast scales influence activity at slower scales. Different modalities have different inherent temporal resolutions in addition to different types of contrast. Current methods generally downsample the faster modality in some way, losing much information in the process. We will leverage variants on long short-term memory (LSTM) network architectures to learn the relationship between state space trajectories acquired simultaneously with population recording and optical imaging, and with optical imaging and fMRI. 2. Create and test a tool for learning how trajectories at slow scales influence activity at faster scales. Leveraging the same LSTM-based approach, we will learn how slower, larger scale activity affects activity at smaller scales, using whisker stimulation as a test case. We anticipate inclusion of the large scale activity (measured with fMRI or optical imaging) will improve prediction of the response at smaller scales (measured with optical imaging or population recording). Our work will allow us to begin to answer a wide range of questions about how the brain functions (e.g., what type of localized stimulation that will drive the brain to a desired global state? How does modulation of the global brain state affect local information processing?) and provide guidance for future experiments by identifying key features that influence activity across scales. By approaching the whole brain as a complex dynamical system, we will break free from the limitations of previous studies that focus on individual cells or circuits. We also expect our work to stimulate new theories that incorporate multiple scales of activity. The brain is a complex, dynamical system that exhibits structured activity at many different space and time scales. Most existing studies focus on a single scale (for example, spiking of individual neurons or activity in a particular circuit). However, interactions occur across scales and may account for much of the variability observed in the brain. This proposal will develop a framework for integrating data from different scales using machine learning tools. The results will improve our understanding of how the brain functions and serve as a foundation for modeling neuromodulatory treatments for brain disorders.",Crossing space and time: uncovering the nonlinear dynamics of multimodal and multiscale brain activity,10007011,R01EB029857,"['Address', 'Affect', 'Arousal', 'Automobile Driving', 'BRAIN initiative', 'Behavior', 'Brain', 'Brain Diseases', 'Cells', 'Characteristics', 'Complex', 'Data', 'Data Set', 'Development', 'Dimensions', 'Electrophysiology (science)', 'Evolution', 'Exhibits', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Individual', 'Knowledge', 'Lead', 'Learning', 'Location', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modality', 'Modeling', 'Neurons', 'Nonlinear Dynamics', 'Optics', 'Population', 'Process', 'Resolution', 'Sampling', 'Structure', 'System', 'Testing', 'Time', 'Translating', 'Variant', 'Vibrissae', 'Work', 'analog', 'base', 'brain research', 'dynamic system', 'experimental study', 'improved', 'information processing', 'insight', 'large datasets', 'long short term memory', 'long short term memory network', 'millisecond', 'multimodal data', 'multimodality', 'multiscale data', 'network architecture', 'neural network', 'neuroregulation', 'optical imaging', 'predicting response', 'response', 'somatosensory', 'temporal measurement', 'theories', 'tool', 'tool development']",NIBIB,EMORY UNIVERSITY,R01,2020,1124452,-0.02226751085571149
"Center for Mesoscale Mapping Overview of the Proposed Resource – Abstract The goal of the Center for Mesoscale Mapping is to drive the convergence of microscopic- and macroscopic- scale evaluation of brain structure and function for human translational neuroscience, by developing and applying tools to study the spatial distribution and temporal orchestration of mesoscopic events in the human brain. Our Collaborators will, through a dynamic “push-pull” relationship, provide unique problems which drive the development of these tools, and in return guide us in the design and optimization of our toolbox for practical use in a variety of normal and disease settings. While there is still no formal consensus on the definition of mesoscopic within the neuroscience community, we take as our guide the spatial and temporal scales at which local groups of neurons act in coherent fashion – in the cortex, this includes the spatial scale of columns and laminar structures (between ~0.1-1 mm), while in deeper structures includes the myriad of deep brain and brainstem nuclei. Preliminary data from our own center, and of course others throughout the world, now support the notion that we are on the threshold of being able to map, measure and perturb the human brain at these scales, and do so comprehensively across wide swaths of the human brain. Temporally too, recent advances suggest a convergence between temporal scales addressable with tools like fMRI, which can now investigate delta frequency coherent phenomena, and advanced electromagnetic tools to measure and perturb coherent electrophysiological activity at higher frequencies still. With this convergence in mind, the tools we proposed to develop within the TRDs of the CMM will provide our Collaborative and Service User community with the important “missing links” between the advances in human cognitive neuroscience at the “system level,” and the enormous strides in cellular level circuit functional characterization. Our Collaborators will bring their own unique challenges to help us define and further refine these tools, offering problems requiring distinct measures of human brain structural and functional properties in a variety of normal and disease settings. Our Service Users will utilize our tools to better understand human neural systems, and particularly human disease states from multiple sclerosis to Alzheimer’s, to depression and epilepsy. Finally, our Center will seek to disseminate these tools, through open-source software and hardware designs, industrial partnerships and “hands-on” teaching courses for hardware, and to train a new generation of human neuroscientists in the use of our advanced tools to explore the human brain at this next frontier. Overview of the Proposed Resource – Narrative The goal of the Center for Mesoscale Mapping is to drive the convergence of microscopic- and macroscopic- scale evaluation of brain structure and function for human translational neuroscience, by developing and applying tools to study the spatial distribution and temporal orchestration of mesoscopic events in the human brain. Our Collaborators will, through a dynamic “push-pull” relationship, provide unique problems which drive the development of these tools, and in return guide us in the design and optimization of our toolbox for practical use in a variety of normal and disease settings.",Center for Mesoscale Mapping,10038177,P41EB030006,"['3-Dimensional', 'Acceleration', 'Address', 'Alzheimer&apos', 's Disease', 'Anatomic Models', 'Basic Science', 'Biological', 'Brain', 'Brain Stem', 'Cell Nucleus', 'Cerebral Palsy', 'Communities', 'Computer Models', 'Computer software', 'Consensus', 'Data', 'Data Set', 'Development', 'Devices', 'Diffusion', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Electroencephalography', 'Electromagnetics', 'Electrophysiology (science)', 'Epilepsy', 'Evaluation', 'Event', 'Fiber', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Generations', 'Goals', 'Histologic', 'Human', 'Image', 'Investigation', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Mental Depression', 'Mental disorders', 'Microscopic', 'Mind', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Motion', 'Multiple Sclerosis', 'Neurons', 'Neurosciences', 'Online Systems', 'Performance', 'Peripheral Nerve Stimulation', 'Property', 'Publications', 'Research Personnel', 'Resolution', 'Resources', 'Respiration', 'Scanning', 'Services', 'Signal Transduction', 'Sleep Disorders', 'Slice', 'Space Models', 'Spatial Distribution', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Time', 'Training', 'Training Programs', 'Transcranial magnetic stimulation', 'Visualization', 'Work', 'base', 'cognitive neuroscience', 'data space', 'deep learning', 'design', 'electric field', 'frontier', 'human disease', 'human imaging', 'improved', 'in vivo', 'industry partner', 'instrumentation', 'machine learning algorithm', 'nervous system disorder', 'neuroimaging', 'novel', 'open source', 'post-doctoral training', 'pre-doctoral', 'reconstruction', 'relating to nervous system', 'response', 'spatiotemporal', 'tool', 'tool development', 'translational neuroscience', 'usability', 'white matter']",NIBIB,MASSACHUSETTS GENERAL HOSPITAL,P41,2020,1731501,-0.016970896484279296
"Multi-modal and Extreme PET/MRI Reconstruction Methods Project Summary / Abstract  Hybrid PET/MRI systems are very advantageous for a variety of clinical applications by combining the soft tissue contrast of MRI with the functional and metabolic information of PET. These systems have found success for oncology studies, particularly in head and abdomen/pelvis, as well as for epilepsy, neurological diseases, heart disease, and pediatrics for dose reduction. However, the PET resolution and SNR is typically worse than MRI, and suffers from the loss of feature and data due to motion as well. PET/MRI systems offer the potential to create more accurate, higher resolution PET reconstructions, including correction of artifacts, motion, and im- proved localization, by performing synergistic reconstructions that leverage the simultaneous data acquisition. In particular, this fellowship proposes to develop novel physics-constrained machine learning models for informa- tion sharing between PET and MRI for enhanced spatial localization, estimation of attenuation and activity, and motion. We propose to develop a deep maximum-likelihood estimation of attenuation and activity (MLAA) that can compensate for artifacts and improve PET reconstruction accuracy. We also propose a motion-enhanced joint PET/MRI reconstruction to capture arbitrary motions and reduce dose requirements for chest and abdomen studies. Together, these models aim to improve the PET spatio-temporal resolution, SNR, and quantiﬁcation for a broad range of clinical applications, and will be evaluated for cancer assessment in the pelvis, liver, and lung.  This fellowship will be performed in the Department of Radiology and Biomedical Imaging at UCSF under the guidance of Prof. Peder Larson, who leads a research program on advanced imaging methods development, and Dr. Thomas Hope, a radiologist and nuclear medicine physician who leads multiple PET/MRI projects. The Department is one of the leading centers in biomedical imaging research, and has been at the forefront on translating PET/MRI systems into clinical practice. The UCSF PET/MRI scanner has dedicated research time, which is also available on other MRI and PET/CT research systems, and extensive computational resources to support the proposed project. The applicant, Dr. Abhejit Rajagopal, has a background in computational imaging and machine learning, will be jointly mentored by this engineer/physician team. He will be trained to become a biomedical imaging scientist by participating in formal coursework on medical imaging systems, training on the PET/MRI system, grant writing, and performing clinical research, supporting his development into a creative, independent biomedical researcher. Project Narrative  Hybrid positron emission tomography (PET) and magnetic resonance imagery (MRI) imaging systems cur- rently aid in diagnosis and prognosis of numerous types of cancer and disease, but are not always precise enough to accurately measure and track a patient’s response to therapy, particularly in organs and tissue that are sub- ject to motion. There is an unrealized potential here to synergistically combine complimentary PET-MRI data to dramatically improve the spatial resolution and SNR of PET, as well as to create motion-resolved 4D (x,y,z,t) imagery by combining information across modalities and time-frames to combat severe undersampling. These methods will be evaluated in human studies of cancer to capture ﬁne structure and micro-features on moving organs (e.g. lung nodules, liver metastases), ultimately aiding in quantitative characterization of disease.",Multi-modal and Extreme PET/MRI Reconstruction Methods,10069132,F32EB030411,"['3-Dimensional', 'Abdomen', 'Address', 'Affect', 'Algorithms', 'Attention', 'Chest', 'Childhood', 'Clinical', 'Clinical Research', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Discipline of Nuclear Medicine', 'Disease', 'Dose', 'Engineering', 'Epilepsy', 'FOLH1 gene', 'Fellowship', 'Financial compensation', 'Goals', 'Grant', 'Head', 'Head and Neck Cancer', 'Heart Diseases', 'Human', 'Hybrids', 'Image', 'Imagery', 'Implant', 'Joint repair', 'Joints', 'Learning', 'Liver', 'Lung', 'Lung nodule', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Female Reproductive System Neoplasm', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Measures', 'Medical Imaging', 'Mentors', 'Metabolic', 'Metals', 'Metastatic Neoplasm to the Liver', 'Methods', 'Modality', 'Modeling', 'Morphologic artifacts', 'Motion', 'Neurodegenerative Disorders', 'Oncology', 'Organ', 'Output', 'Patients', 'Pediatrics', 'Pelvis', 'Performance', 'Physicians', 'Physics', 'Positron-Emission Tomography', 'Research', 'Research Personnel', 'Research Support', 'Resolution', 'Scanning', 'Signal Transduction', 'Standardization', 'Structure', 'System', 'Time', 'Tissues', 'Tracer', 'Training', 'Translating', 'Writing', 'attenuation', 'bioimaging', 'bone', 'cancer type', 'clinical application', 'clinical practice', 'combat', 'computing resources', 'data acquisition', 'deep learning', 'heart imaging', 'high resolution imaging', 'imaging modality', 'imaging scientist', 'imaging system', 'improved', 'information model', 'lung imaging', 'method development', 'multimodality', 'nervous system disorder', 'neuro-oncology', 'novel', 'outcome forecast', 'patient response', 'programs', 'radiological imaging', 'radiologist', 'reconstruction', 'respiratory', 'soft tissue', 'spatiotemporal', 'success', 'systems research', 'uptake']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",F32,2020,69426,-0.045282513268283656
"Associating retinal nerve fiber layer thickness with glucose metabolism and diabetic retinopathy Project Summary/Abstract Type 2 diabetes mellitus (T2DM), a metabolic disease that affects over 300 million people worldwide and that can be accompanied by serious health complications such as heart disease, kidney failure, stroke, and damage to the eyes, in particular diabetic retinopathy (DR), which is diagnosed in a third of people with diabetes and which is the leading cause of blindness within the age group between 20 and 64 years. T2DM is clinically diagnosed by parameters related to glucose metabolism obtained by blood tests. Due to its long pre- symptomatic phase, an estimate of 25% of diabetics in the US are undiagnosed. In this project, the relationship between spatial patterns of retinal nerve fiber layer (RNFL) thickness (RNFLT), measured by spectral-domain optical coherence tomography (OCT), and blood test levels as well as levels of DR severity is investigated in 9,261 participants of a population based study.  In a first step, OCT RNFLT measurements of the macular and the circumpapillary area around optic nerve head are segmented into spatial sectors, and representative spatial patterns of RNFLT are calculated by an unsupervised machine learning method. Afterwards, a multivariate linear model comparison is performed with the coefficients of the spatial RNFLT patterns as regressors and diagnostic blood test results as dependent variable. The optimal combination of the RNFLT patterns, determined by an established model selection criterion (Bayes Factor), is expected to reveal insight into the association between the specific retinal locations of RNFL thinning accompanying the change in parameters related glucose metabolism during the development and progression of T2DM. Furthermore, fundus images are graded by DR severity following a nine-step scale derived from the Early Treatment Diabetic Retinopathy Study from no DR to severe proliferative DR. The spatial RNFLT patterns and metabolic blood test scores are then compared with respect to modeling DR severity by linear regression. An optimal model of DR severity combining glucose metabolism parameters and RNFLT patterns is developed. Finally, in an analogous procedure, DR severity of the follow-up measurement, five years after baseline, is statistically predicted from RNFLT and metabolic blood parameters and from their change over time.  To summarize, the proposed research identifies spatial patterns of RNFLT associated with parameters of glucose metabolism and their development over DR severity. Once accomplished, the proposed project would provide the details to establish RNFLT as an alternative manifestation of T2DM that complements diagnostic blood tests and thereby, for instance, lay the foundations for the development of novel and more accurate T2DM progression monitoring or the prediction of the onset of DR. Project Narrative Parameters related to glucose metabolism obtained by blood tests are clinically used to diagnose diabetes, a metabolic disease that affects over 300 million people worldwide and that can be accompanied by serious health complications, such as diabetic retinopathy (DR), the leading cause of blindness within the age group between 20 and 64 years. Decreased levels of blood glucose tolerance have been associated with retinal nerve fiber layer (RNFL) thinning, but these results were based on comparisons between small populations of diagnosed diabetics and healthy controls, and RNFL was typically represented by coarse summary parameters which neglect retinal anatomy. This project contributes directly and immediately to public health by exploring the relationship between spatial patterns of RNFL thickness, present and future DR severity, and diagnostic blood test results in 9,261 participants of a population based study, with the final goal to establish and quantify RNFL thickness as an alternative manifestation of diabetes that complements diagnostic blood tests and lays the foundations for the development of novel and more accurate disease progression monitoring or the prediction of DR onset.",Associating retinal nerve fiber layer thickness with glucose metabolism and diabetic retinopathy,10002287,R21EY030631,"['Affect', 'Anatomy', 'Area', 'Bayesian Modeling', 'Blindness', 'Blood', 'Blood Glucose', 'Blood Tests', 'Clinical', 'Complement', 'Data', 'Development', 'Diabetes Mellitus', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Progression', 'Early treatment', 'Eye', 'Foundations', 'Future', 'Glycosylated hemoglobin A', 'Goals', 'Health', 'Heart Diseases', 'Kidney Failure', 'Linear Models', 'Linear Regressions', 'Location', 'Maps', 'Measurement', 'Measures', 'Metabolic', 'Metabolic Diseases', 'Modeling', 'Monitor', 'Non-Insulin-Dependent Diabetes Mellitus', 'OGTT', 'Optic Disk', 'Optical Coherence Tomography', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Population Study', 'Procedures', 'Public Health', 'Research', 'Retina', 'Scanning', 'Selection Criteria', 'Severities', 'Severity of illness', 'Stroke', 'Sum', 'Techniques', 'Test Result', 'Testing', 'Thick', 'Thinness', 'Time', 'Validation', 'age group', 'archetypal analysis', 'base', 'clinical Diagnosis', 'diabetic', 'fasting plasma glucose', 'follow-up', 'fundus imaging', 'glucose metabolism', 'glucose tolerance', 'insight', 'machine learning method', 'macula', 'neglect', 'novel', 'predictive modeling', 'proliferative diabetic retinopathy', 'public health relevance', 'retinal nerve fiber layer', 'unsupervised learning']",NEI,SCHEPENS EYE RESEARCH INSTITUTE,R21,2020,240285,-0.06840237009192426
"Associating retinal nerve fiber layer thickness with glucose metabolism and diabetic retinopathy Project Summary/Abstract Type 2 diabetes mellitus (T2DM), a metabolic disease that affects over 300 million people worldwide and that can be accompanied by serious health complications such as heart disease, kidney failure, stroke, and damage to the eyes, in particular diabetic retinopathy (DR), which is diagnosed in a third of people with diabetes and which is the leading cause of blindness within the age group between 20 and 64 years. T2DM is clinically diagnosed by parameters related to glucose metabolism obtained by blood tests. Due to its long pre- symptomatic phase, an estimate of 25% of diabetics in the US are undiagnosed. In this project, the relationship between spatial patterns of retinal nerve fiber layer (RNFL) thickness (RNFLT), measured by spectral-domain optical coherence tomography (OCT), and blood test levels as well as levels of DR severity is investigated in 9,261 participants of a population based study.  In a first step, OCT RNFLT measurements of the macular and the circumpapillary area around optic nerve head are segmented into spatial sectors, and representative spatial patterns of RNFLT are calculated by an unsupervised machine learning method. Afterwards, a multivariate linear model comparison is performed with the coefficients of the spatial RNFLT patterns as regressors and diagnostic blood test results as dependent variable. The optimal combination of the RNFLT patterns, determined by an established model selection criterion (Bayes Factor), is expected to reveal insight into the association between the specific retinal locations of RNFL thinning accompanying the change in parameters related glucose metabolism during the development and progression of T2DM. Furthermore, fundus images are graded by DR severity following a nine-step scale derived from the Early Treatment Diabetic Retinopathy Study from no DR to severe proliferative DR. The spatial RNFLT patterns and metabolic blood test scores are then compared with respect to modeling DR severity by linear regression. An optimal model of DR severity combining glucose metabolism parameters and RNFLT patterns is developed. Finally, in an analogous procedure, DR severity of the follow-up measurement, five years after baseline, is statistically predicted from RNFLT and metabolic blood parameters and from their change over time.  To summarize, the proposed research identifies spatial patterns of RNFLT associated with parameters of glucose metabolism and their development over DR severity. Once accomplished, the proposed project would provide the details to establish RNFLT as an alternative manifestation of T2DM that complements diagnostic blood tests and thereby, for instance, lay the foundations for the development of novel and more accurate T2DM progression monitoring or the prediction of the onset of DR. Project Narrative Parameters related to glucose metabolism obtained by blood tests are clinically used to diagnose diabetes, a metabolic disease that affects over 300 million people worldwide and that can be accompanied by serious health complications, such as diabetic retinopathy (DR), the leading cause of blindness within the age group between 20 and 64 years. Decreased levels of blood glucose tolerance have been associated with retinal nerve fiber layer (RNFL) thinning, but these results were based on comparisons between small populations of diagnosed diabetics and healthy controls, and RNFL was typically represented by coarse summary parameters which neglect retinal anatomy. This project contributes directly and immediately to public health by exploring the relationship between spatial patterns of RNFL thickness, present and future DR severity, and diagnostic blood test results in 9,261 participants of a population based study, with the final goal to establish and quantify RNFL thickness as an alternative manifestation of diabetes that complements diagnostic blood tests and lays the foundations for the development of novel and more accurate disease progression monitoring or the prediction of DR onset.",Associating retinal nerve fiber layer thickness with glucose metabolism and diabetic retinopathy,9809589,R21EY030631,"['Affect', 'Anatomy', 'Area', 'Bayesian Modeling', 'Blindness', 'Blood', 'Blood Glucose', 'Blood Tests', 'Clinical', 'Complement', 'Data', 'Development', 'Diabetes Mellitus', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Progression', 'Early treatment', 'Eye', 'Foundations', 'Future', 'Glycosylated hemoglobin A', 'Goals', 'Health', 'Heart Diseases', 'Kidney Failure', 'Linear Models', 'Linear Regressions', 'Location', 'Maps', 'Measurement', 'Measures', 'Metabolic', 'Metabolic Diseases', 'Modeling', 'Monitor', 'Non-Insulin-Dependent Diabetes Mellitus', 'OGTT', 'Optic Disk', 'Optical Coherence Tomography', 'Participant', 'Patients', 'Pattern', 'Phase', 'Population', 'Population Study', 'Procedures', 'Public Health', 'Research', 'Retina', 'Retinal', 'Scanning', 'Selection Criteria', 'Severities', 'Severity of illness', 'Stroke', 'Sum', 'Techniques', 'Test Result', 'Testing', 'Thick', 'Thinness', 'Time', 'Validation', 'age group', 'base', 'clinical Diagnosis', 'diabetic', 'fasting plasma glucose', 'follow-up', 'fundus imaging', 'glucose metabolism', 'glucose tolerance', 'insight', 'learning strategy', 'macula', 'neglect', 'novel', 'predictive modeling', 'proliferative diabetic retinopathy', 'public health relevance', 'retinal nerve fiber layer', 'unsupervised learning']",NEI,SCHEPENS EYE RESEARCH INSTITUTE,R21,2019,313837,-0.06840237009192426
"Expanding field-of-view with reduced tissue displacement in micro-endoscopic computational imaging PROJECT SUMMARY Optical imaging methods are well-established in neuroscience, but high-speed, high- resolution volumetric imaging of neural activity in deep tissue remains a challenge. A number of techniques address limited aspects of this goal, and most are applicable primarily to acute preparations. We propose to develop and test a novel approach to achieve three-dimensional “deep-tissue” imaging for high spatial and temporal resolution neural recording by combining aspects of embedded optical probes with computational imaging techniques. Rather than use a single micro-endoscopic probe, we propose to utilize an array of narrower probes, or optrodes, to reduce the volume of tissue displacement. Computational imaging through each probe can be performed to achieve a field of view (FOV) at a desired distance from the probe tip. Combining the fields of view from multiple probes arranged in an array then provides a composite image field that is much larger than achievable from a single micro-endoscope. In our approach, each ∼0.1 mm diameter probe of the array acts as an independent micro- endoscope. In order to achieve full-field imaging across the array, the individual fields must intersect, and the computational method must be scaled to accommodate, and stitch, multiple fields. In pursuit of these goals, we propose three Aims: Optimizing the FOV of a single micro-endoscope - The purpose of this Aim is to characterize the FOV for an individual probe at multiple depths, and optimize the FOV to about 0.3mm through control over the shape of the probe tip and light collection numerical aperture. Accelerating calibration and reconstruction - In this Aim, we will pursue efficient computational approaches for calibration based upon ray-tracing simulations and image reconstruction based on deep learning. Scaling the FOV with an endoscope array - The computational image reconstruction method will be scaled to accommodate small micro-endoscope arrays (e.g. 4 element) arranged in a hexagonal lattice with FOV of 0.6mm at a 1.5mm depth. NARRATIVE Imaging deep inside tissue, including the brain, is critical to understanding various biological processes. Doing so through a small probe is also of primary importance for minimizing tissue damage. In this proposal, we apply computational techniques to create fluorescent images using an array of microscopic glass needles to guide light in and out of a mouse brain. The simplicity and small footprint of our system have the potential for deep-brain imaging (depths > 1.5 mm) across a large (mm) field of view, which should enable a wide variety of biological and neuroscience studies in the future.",Expanding field-of-view with reduced tissue displacement in micro-endoscopic computational imaging,9829467,R21EY030717,"['3-Dimensional', 'Acute', 'Address', 'Biological', 'Biological Process', 'Brain', 'Brain imaging', 'Caliber', 'Calibration', 'Collection', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Confocal Microscopy', 'Coupled', 'Devices', 'Dimensions', 'Elements', 'Endoscopes', 'Fluorescence Microscopy', 'Fluorescent Probes', 'Future', 'Glass', 'Goals', 'Head', 'Holography', 'Image', 'Imaging Techniques', 'Individual', 'Light', 'Methods', 'Microscope', 'Microscopic', 'Microscopy', 'Miniaturization', 'Mus', 'Needles', 'Neurosciences', 'Optics', 'Penetration', 'Preparation', 'Resolution', 'Risk', 'Running', 'Scanning', 'Shapes', 'Source', 'Speed', 'System', 'Techniques', 'Testing', 'Time', 'Tissue imaging', 'Tissues', 'Work', 'absorption', 'adaptive optics', 'attenuation', 'base', 'cost', 'deep learning', 'fluorescence imaging', 'image reconstruction', 'imaging modality', 'improved', 'in vivo', 'interest', 'lens', 'microendoscope', 'multi-photon', 'neural circuit', 'novel strategies', 'optical fiber', 'optical imaging', 'reconstruction', 'relating to nervous system', 'retinal rods', 'simulation', 'temporal measurement']",NEI,UNIVERSITY OF UTAH,R21,2019,456800,-0.01568737467726465
"Development of Hardware and Software for Clinical MEG The ongoing objective of our research, since the establishment of the HFH MEG Lab, has been to develop hardware, software, and techniques to expand the utility of Magnetoencephalography (MEG), both as a clinical diagnostic tool, and as a modality for basic neuroscience studies. With the proliferation of MEGsys- tems both in the U.S. and worldwide, and the availability of CPT codes for some clinical MEG studies, the development of new and more effective clinical applications to enable users to more fully exploit these costly systems has become our highest priority. Accordingly we are developing new analytical tools and demon- strating that MEG data contains much more clinically useful information than can be found using only the usual dipole techniques. For example, the integration of our Multi Resolution FOCUSS with Principal Com- ponent Analysis (PCA)or Singular Value Decomposition (SVD) shows promise of enhanced computational efficiency and the ability to identify regions of high brain coherence indicative of epileptic tissue. During the proposed grant, these new techniques will be further developed and applied to neurologic and learning dis- orders. In Specific Aim One, interoperative source confirmation will be compared to results of our mapping techniques. DC-MEG will be used to monitor interictal cortical excitability in migraine patients and how this excitability responds to anti-migraine medications. Determination of focal secondary generalized epileptic activity may be used to predict successful surgical resections. In Specific Aim Two we propose to continue the development of new analytical tools and to make them available to the MEG community on the Internet as they become validated. In particular, we will add new functionality to our ""MEG Tools"" software suite by incorporating ICA, Frequency Analysis, and MEG co-registration with Diffusion Tensor Imaging (DTI). In Specific Aim Three, our new techniques will be applied to imaging differences in cortical activation between normal readers and individuals with learning disorders during reading tasks. These techniques will be used to map various cortical areas activated by auditory and visual lexical stimuli, and to differentiate the re- sponses of normal and reading disabled individuals. The exquisite temporal and spatial resolution of MEG, enhanced by these new imaging techniques, will provide clinicians and researchers new noninvasive meth- ods to investigate pathological and normal neurological functioning. n/a",Development of Hardware and Software for Clinical MEG,7361358,R01NS030914,"['Ache', 'Area', 'Auditory', 'Brain', 'Brain imaging', 'Brain region', 'Characteristics', 'Clinical', 'Communities', 'Complex', 'Computer information processing', 'Computer software', 'Current Procedural Terminology Codes', 'Data', 'Detection', 'Development', 'Developmental reading disorder', 'Diagnostic', 'Diffusion Magnetic Resonance Imaging', 'Disabled Persons', 'Disease', 'Doctor of Philosophy', 'Dyslexia', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Excision', 'Fiber', 'Frequencies', 'Grant', 'Hearing', 'Image', 'Imaging Techniques', 'Implant', 'Individual', 'Internet', 'Knowledge', 'Language', 'Learning', 'Learning Disorders', 'Link', 'Localized', 'Location', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Maps', 'Migraine', 'Modality', 'Modeling', 'Monitor', 'Nervous System Physiology', 'Neurologic', 'Neurons', 'Neurosciences', 'Noise', 'Operating Rooms', 'Operative Surgical Procedures', 'Patients', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Principal Component Analysis', 'Purpose', 'Reader', 'Reading', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Route', 'Seizures', 'Semantic memory', 'Series', 'Signal Transduction', 'Software Tools', 'Source', 'Stimulus', 'Surgeon', 'System', 'Techniques', 'Time', 'Tissues', 'Visual', 'Word Processing', 'Writing', 'analytical tool', 'base', 'clinical application', 'density', 'human prostaglandin D2 receptor', 'improved', 'lexical', 'magnetic field', 'programs', 'prophylactic', 'research clinical testing', 'response', 'technique development', 'tool']",NINDS,HENRY FORD HEALTH SYSTEM,R01,2008,565393,-0.03292549366711076
"Development of Hardware and Software for Clinical MEG    DESCRIPTION (provided by applicant): The ongoing objective of our research, since the establishment of the HFH MEG Lab, has been to develop hardware, software, and techniques to expand the utility of Magnetoencephalography (MEG), both as a clinical diagnostic tool, and as a modality for basic neuroscience studies. With the proliferation of MEG sys- tems both in the U.S. and worldwide, and the availability of CPT codes for some clinical MEG studies, the development of new and more effective clinical applications to enable users to more fully exploit these costly systems has become our highest priority. Accordingly we are developing new analytical tools and demon- strating that MEG data contains much more clinically useful information than can be found using only the usual dipole techniques. For example, the integration of our Multi Resolution FOCUSS with Principal Com- ponent Analysis (PCA) or Singular Value Decomposition (SVD) shows promise of enhanced computational efficiency and the ability to identify regions of high brain coherence indicative of epileptic tissue. During the proposed grant, these new techniques will be further developed and applied to neurologic and learning dis- orders. In Specific Aim One, interoperative source confirmation will be compared to results of our mapping techniques. DC-MEG will be used to monitor interictal cortical excitability in migraine patients and how this excitability responds to anti-migraine medications. Determination of focal secondary generalized epileptic activity may be used to predict successful surgical resections. In Specific Aim Two we propose to continue the development of new analytical tools and to make them available to the MEG community on the Internet as they become validated. In particular, we will add new functionality to our ""MEG Tools"" software suite by incorporating ICA, Frequency Analysis, and MEG co-registration with Diffusion Tensor Imaging (DTI). In Specific Aim Three, our new techniques will be applied to imaging differences in cortical activation between normal readers and individuals with learning disorders during reading tasks. These techniques will be used to map various cortical areas activated by auditory and visual lexical stimuli, and to differentiate the re- sponses of normal and reading disabled individuals. The exquisite temporal and spatial resolution of MEG, enhanced by these new imaging techniques, will provide clinicians and researchers new noninvasive meth- ods to investigate pathological and normal neurological functioning.           n/a",Development of Hardware and Software for Clinical MEG,7184328,R01NS030914,"['Ache', 'Area', 'Auditory', 'Brain', 'Brain imaging', 'Brain region', 'Characteristics', 'Clinical', 'Communities', 'Complex', 'Computer information processing', 'Computer software', 'Current Procedural Terminology Codes', 'Data', 'Detection', 'Development', 'Developmental reading disorder', 'Diagnostic', 'Diffusion Magnetic Resonance Imaging', 'Disabled Persons', 'Disease', 'Doctor of Philosophy', 'Dyslexia', 'Electroencephalography', 'Epilepsy', 'Epileptogenesis', 'Excision', 'Fiber', 'Frequencies', 'Grant', 'Hearing', 'Image', 'Imaging Techniques', 'Implant', 'Individual', 'Internet', 'Knowledge', 'Language', 'Learning', 'Learning Disorders', 'Link', 'Localized', 'Location', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Maps', 'Migraine', 'Modality', 'Modeling', 'Monitor', 'Nervous System Physiology', 'Neurologic', 'Neurons', 'Neurosciences', 'Noise', 'Operating Rooms', 'Operative Surgical Procedures', 'Patients', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Principal Component Analysis', 'Purpose', 'Reader', 'Reading', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Route', 'Seizures', 'Semantic memory', 'Series', 'Signal Transduction', 'Software Tools', 'Source', 'Stimulus', 'Surgeon', 'System', 'Techniques', 'Time', 'Tissues', 'Visual', 'Word Processing', 'Writing', 'analytical tool', 'base', 'clinical application', 'density', 'human prostaglandin D2 receptor', 'improved', 'lexical', 'magnetic field', 'programs', 'prophylactic', 'research clinical testing', 'response', 'technique development', 'tool']",NINDS,HENRY FORD HEALTH SYSTEM,R01,2007,567126,-0.03292549366711076
"Development of Hardware and Software for Clinical MEG    DESCRIPTION (provided by applicant): The ongoing objective of our research, since the establishment of the HFH MEG Lab, has been to develop hardware, software, and techniques to expand the utility of Magnetoencephalography (MEG), both as a clinical diagnostic tool, and as a modality for basic neuroscience studies. With the proliferation of MEG sys- tems both in the U.S. and worldwide, and the availability of CPT codes for some clinical MEG studies, the development of new and more effective clinical applications to enable users to more fully exploit these costly systems has become our highest priority. Accordingly we are developing new analytical tools and demon- strating that MEG data contains much more clinically useful information than can be found using only the usual dipole techniques. For example, the integration of our Multi Resolution FOCUSS with Principal Com- ponent Analysis (PCA) or Singular Value Decomposition (SVD) shows promise of enhanced computational efficiency and the ability to identify regions of high brain coherence indicative of epileptic tissue. During the proposed grant, these new techniques will be further developed and applied to neurologic and learning dis- orders. In Specific Aim One, interoperative source confirmation will be compared to results of our mapping techniques. DC-MEG will be used to monitor interictal cortical excitability in migraine patients and how this excitability responds to anti-migraine medications. Determination of focal secondary generalized epileptic activity may be used to predict successful surgical resections. In Specific Aim Two we propose to continue the development of new analytical tools and to make them available to the MEG community on the Internet as they become validated. In particular, we will add new functionality to our ""MEG Tools"" software suite by incorporating ICA, Frequency Analysis, and MEG co-registration with Diffusion Tensor Imaging (DTI). In Specific Aim Three, our new techniques will be applied to imaging differences in cortical activation between normal readers and individuals with learning disorders during reading tasks. These techniques will be used to map various cortical areas activated by auditory and visual lexical stimuli, and to differentiate the re- sponses of normal and reading disabled individuals. The exquisite temporal and spatial resolution of MEG, enhanced by these new imaging techniques, will provide clinicians and researchers new noninvasive meth- ods to investigate pathological and normal neurological functioning.           n/a",Development of Hardware and Software for Clinical MEG,7097791,R01NS030914,"['brain', 'clinical research', 'epilepsy', 'learning disorders', 'magnetic field', 'reading']",NINDS,HENRY FORD HEALTH SYSTEM,R01,2006,610630,-0.03292549366711076
"BIOPHYSICAL BASIS OF FUNCTIONAL BRAIN MRI This proposal aims to develop an improved understanding of the mechanisms        involved in functional MRI of the brain and to optimize imaging and data         analysis strategies for the detection of neuronal activity.  Functional MRI      relies on the ability to detect the changes in NMR signal that are produced      in discrete regions of cortex in response to specific activating stimuli,        and are believed to reflect changes in local blood flow, volume and              oxygenation.  Functional MRI promises to be a major addition to the methods      available for studying brain activation.  Despite the widespread claims for      the power and successes of the method, there remain several unanswered           questions regarding its optimal mode of use, the tissue and technical            factors that are important in determining the signal changes detected, and       the significance and interpretation of these signal changes.  The research       proposed would systematically address such issues.  The underlying               mechanism may include both susceptibility contrast effects, based on the         BOLD effect, as well as wash-in effects, and these will be separately            quantified.  The factors that affect each mechanism will be separately           identified and measured.  For the BOLD effect, extensive computer modeling       and measurements in phantoms and animals brains will be used to establish        the relative sensitivity to vascular structures of different sizes,              spacings and orientations, as well as other tissue properties such as the        rate of water diffusion.  The separate sensitivities to s-called static          field effects (T2*),  diffusive losses and other mechanisms will also be         established.  The performance of different pulse sequences will be compared      to devise optimal methods of scanning and detection at 1.5T.  Echo planar        imaging, conventional gradient echo and fast spin echo imaging as well as        more novel schemes will be compared in phantoms, animal brains and examples      of human activation.  Human and animal activations will be produced in vivo      using  visual and motor stimuli as well as by alteration of global blood         flow by acetazolamide and hypercarbia.  A critical feature of current            paradigms for detecting activation is the method of data analysis, which is      interrelated with the nature of the task and imaging method used.  We will       compare different methods of analyzing functional data sets, including           statistical parameter mapping, time-correlation analyses, and principal          component analysis.  The sensitivity of each to motion and other artifacts       will be established by in in vivo comparisons and by computer simulations.       From these studies, we anticipate being able to improve strategies for the       use and interpretation of functional MRI in human studies of function and        cognition.                                                                        n/a",BIOPHYSICAL BASIS OF FUNCTIONAL BRAIN MRI,2714543,R01NS033332,"['acetazolamide', ' biophysics', ' blood flow measurement', ' blood vessels', ' blood volume', ' brain electrical activity', ' capillary', ' computer data analysis', ' computer simulation', ' human subject', ' hypercapnia', ' laboratory rat', ' magnetic resonance imaging', ' method development', ' motor neurons', ' nuclear magnetic resonance spectroscopy', ' phantom model', ' respiratory oxygenation', ' statistics /biometry', ' visual stimulus', ' water flow']",NINDS,YALE UNIVERSITY,R01,1998,360071,-0.007487390746172806
"BIOPHYSICAL BASIS OF FUNCTIONAL BRAIN MRI This proposal aims to develop an improved understanding of the mechanisms        involved in functional MRI of the brain and to optimize imaging and data         analysis strategies for the detection of neuronal activity.  Functional MRI      relies on the ability to detect the changes in NMR signal that are produced      in discrete regions of cortex in response to specific activating stimuli,        and are believed to reflect changes in local blood flow, volume and              oxygenation.  Functional MRI promises to be a major addition to the methods      available for studying brain activation.  Despite the widespread claims for      the power and successes of the method, there remain several unanswered           questions regarding its optimal mode of use, the tissue and technical            factors that are important in determining the signal changes detected, and       the significance and interpretation of these signal changes.  The research       proposed would systematically address such issues.  The underlying               mechanism may include both susceptibility contrast effects, based on the         BOLD effect, as well as wash-in effects, and these will be separately            quantified.  The factors that affect each mechanism will be separately           identified and measured.  For the BOLD effect, extensive computer modeling       and measurements in phantoms and animals brains will be used to establish        the relative sensitivity to vascular structures of different sizes,              spacings and orientations, as well as other tissue properties such as the        rate of water diffusion.  The separate sensitivities to s-called static          field effects (T2*),  diffusive losses and other mechanisms will also be         established.  The performance of different pulse sequences will be compared      to devise optimal methods of scanning and detection at 1.5T.  Echo planar        imaging, conventional gradient echo and fast spin echo imaging as well as        more novel schemes will be compared in phantoms, animal brains and examples      of human activation.  Human and animal activations will be produced in vivo      using  visual and motor stimuli as well as by alteration of global blood         flow by acetazolamide and hypercarbia.  A critical feature of current            paradigms for detecting activation is the method of data analysis, which is      interrelated with the nature of the task and imaging method used.  We will       compare different methods of analyzing functional data sets, including           statistical parameter mapping, time-correlation analyses, and principal          component analysis.  The sensitivity of each to motion and other artifacts       will be established by in in vivo comparisons and by computer simulations.       From these studies, we anticipate being able to improve strategies for the       use and interpretation of functional MRI in human studies of function and        cognition.                                                                        n/a",BIOPHYSICAL BASIS OF FUNCTIONAL BRAIN MRI,2431248,R01NS033332,"['acetazolamide', ' biophysics', ' blood flow measurement', ' blood vessels', ' blood volume', ' brain electrical activity', ' capillary', ' computer data analysis', ' computer simulation', ' human subject', ' hypercapnia', ' laboratory rat', ' magnetic resonance imaging', ' method development', ' motor neurons', ' nuclear magnetic resonance spectroscopy', ' phantom model', ' respiratory oxygenation', ' statistics /biometry', ' visual stimulus', ' water flow']",NINDS,YALE UNIVERSITY,R01,1997,346221,-0.007487390746172806
"BIOPHYSICAL BASIS OF FUNCTIONAL BRAIN MRI This proposal aims to develop an improved understanding of the mechanisms  involved in functional MRI of the brain and to optimize imaging and data  analysis strategies for the detection of neuronal activity.  Functional MRI  relies on the ability to detect the changes in NMR signal that are produced  in discrete regions of cortex in response to specific activating stimuli,  and are believed to reflect changes in local blood flow, volume and  oxygenation.  Functional MRI promises to be a major addition to the methods  available for studying brain activation.  Despite the widespread claims for  the power and successes of the method, there remain several unanswered  questions regarding its optimal mode of use, the tissue and technical  factors that are important in determining the signal changes detected, and  the significance and interpretation of these signal changes.  The research  proposed would systematically address such issues.  The underlying  mechanism may include both susceptibility contrast effects, based on the  BOLD effect, as well as wash-in effects, and these will be separately  quantified.  The factors that affect each mechanism will be separately  identified and measured.  For the BOLD effect, extensive computer modeling  and measurements in phantoms and animals brains will be used to establish  the relative sensitivity to vascular structures of different sizes,  spacings and orientations, as well as other tissue properties such as the  rate of water diffusion.  The separate sensitivities to s-called static  field effects (T2*),  diffusive losses and other mechanisms will also be  established.  The performance of different pulse sequences will be compared  to devise optimal methods of scanning and detection at 1.5T.  Echo planar  imaging, conventional gradient echo and fast spin echo imaging as well as  more novel schemes will be compared in phantoms, animal brains and examples  of human activation.  Human and animal activations will be produced in vivo  using  visual and motor stimuli as well as by alteration of global blood  flow by acetazolamide and hypercarbia.  A critical feature of current  paradigms for detecting activation is the method of data analysis, which is  interrelated with the nature of the task and imaging method used.  We will  compare different methods of analyzing functional data sets, including  statistical parameter mapping, time-correlation analyses, and principal  component analysis.  The sensitivity of each to motion and other artifacts  will be established by in in vivo comparisons and by computer simulations.  From these studies, we anticipate being able to improve strategies for the  use and interpretation of functional MRI in human studies of function and  cognition.  n/a",BIOPHYSICAL BASIS OF FUNCTIONAL BRAIN MRI,2272080,R01NS033332,"['acetazolamide', ' biophysics', ' blood flow measurement', ' blood vessels', ' blood volume', ' brain electrical activity', ' capillary', ' computer data analysis', ' computer simulation', ' human subject', ' hypercapnia', ' laboratory rat', ' magnetic resonance imaging', ' method development', ' motor neurons', ' nuclear magnetic resonance spectroscopy', ' phantom model', ' respiratory oxygenation', ' statistics /biometry', ' visual stimulus', ' water flow']",NINDS,YALE UNIVERSITY,R01,1995,331160,-0.007487390746172806
"PULSE SIGNALS TO CELLS USING A UNIQUE PERIFUSION SYSTEM The long-term objective of this project is to develop a flexible, perifused cell micro-culture system.  The system will be capable of providing a dynamically-changing environment to the cells.  The specific pattern of nutrients, stimuli, inhibitors, etc. applied to the cells will be programmable by the user.  On-line sensors (e.g., special electrodes) will be able to quantify responses of the cells under test.  The system will include (1) a flow module which includes the cell perifusion chamber, (2) a pump module, to provide the patterned environment to the cells, (3) an incubator module, to provide a noise-free electrical environment for measurements, (4) a variety of electrodes and other on-line biosensors, (5) a software package to permit flexible user control and sophisticated analysis and display of acquired data.  The modules will have market appeal separately; together, they comprise the system.  Many significant applications include the study of biological systems under approximately in vivo conditions; the study of drug efficacy when delivery is in predefined patterns; the optimization of conditions for, and when scaled up, on-line feedback control of fermentation systems; the study of genetically-engineered cells; the study of environmental pollutants; medical diagnostic applications.  n/a",PULSE SIGNALS TO CELLS USING A UNIQUE PERIFUSION SYSTEM,3507781,R44GM034169,"['analytical method', ' artificial intelligence', ' biotechnology', ' diagnostic tests', ' drug delivery systems', ' electrodes', ' enzyme linked immunosorbent assay', ' online computer', ' perfusion', ' radioimmunoassay', ' tissue /cell culture', ' toxicant screening']",NIGMS,"KMS FUSION, INC.",R44,1987,166534,-0.016174501426901196
"PULSE SIGNALS TO CELLS USING A UNIQUE PERIFUSION SYSTEM The long-term objective of this project is to develop a flexible, perifused cell micro-culture system.  The system will be capable of providing a dynamically-changing environment to the cells.  The specific pattern of nutrients, stimuli, inhibitors, etc. applied to the cells will be programmable by the user.  On-line sensors (e.g., special electrodes) will be able to quantify responses of the cells under test.  The system will include (1) a flow module which includes the cell perifusion chamber, (2) a pump module, to provide the patterned environment to the cells, (3) an incubator module, to provide a noise-free electrical environment for measurements, (4) a variety of electrodes and other on-line biosensors, (5) a software package to permit flexible user control and sophisticated analysis and display of acquired data.  The modules will have market appeal separately; together, they comprise the system.  Many significant applications include the study of biological systems under approximately in vivo conditions; the study of drug efficacy when delivery is in predefined patterns; the optimization of conditions for, and when scaled up, on-line feedback control of fermentation systems; the study of genetically-engineered cells; the study of environmental pollutants; medical diagnostic applications.  n/a",PULSE SIGNALS TO CELLS USING A UNIQUE PERIFUSION SYSTEM,3507780,R44GM034169,"['analytical method', ' artificial intelligence', ' biotechnology', ' diagnostic tests', ' drug delivery systems', ' electrodes', ' enzyme linked immunosorbent assay', ' online computer', ' perfusion', ' radioimmunoassay', ' tissue /cell culture', ' toxicant screening']",NIGMS,"KMS FUSION, INC.",R44,1986,221383,-0.016174501426901196
"IMPROVEMENT OF DIGITAL ANGIOGRAPHY OF CORONARY ARTERIES Coronary arteriosclerosis is a leading cause of death in U.S., and it is to a large degree the presence or absence of severe coronary stenoses which determines the risk of death.  Because many patients who suffer from this disease are asymptomatic until their first attack, a safe, inexpensive diagnostic procedure for detecting coronary artery stenoses would be beneficial.  The diagnosis as currently carried out requires that a large catheter be inserted into the orifice of each artery in order to inject opaque dye to make the arteries visible with X rays.  The increased contrast sensitivity of digital subtraction angiography (DSA) permits a less invasive technique involving an injection into the aortic root, a smaller catheter, and reduced dye volumes, but it has been of only limited success largely because of image degradation caused by patient motion during image acquisition.  The aim of this project is to develop and test image processing algorithms for the improvement of digital angiography of coronary arteries.  The algorithms search for a geometrical transformation to produce an optimal registration of two images before subtraction.  They are based on a new application of the fundamental principles of fluid dynamics, a new class of transformation functions, a new method of machine calibration, and new application of search methods developed in the field of artifical intelligence for function optimization.  As a result these algorithms represent the first approach to image registration which is capable of handling true three dimensional warping.  The development and evaluation of the algorithms will be carried out on both a specially designed phantom and on data derived in a new way from patient studies. The patient studies will be acquired during angioplasty procedures in which an inflated balloon and catheter will serve as phantom arteries of known dimensions.  The availability of known phantoms located in the chest on a beating human heart will provide for the first time an opportunity for quantitative evaluation of the success of the image enhancement algorithms in a realistic environment.  Thus, the proposed research will employ new techniques of image registration with a new technique for image evaluation to the problem of reducing motion effects in digital subtraction angiography.  n/a",IMPROVEMENT OF DIGITAL ANGIOGRAPHY OF CORONARY ARTERIES,3347903,R01HL034703,"['angiography', ' arteriosclerosis', ' artery stenosis', ' artificial intelligence', ' clinical biomedical equipment', ' coronary artery', ' diagnosis procedure safety', ' heart disorder diagnosis', ' heart imaging /visualization /scanning', ' human subject', ' image processing']",NHLBI,VANDERBILT UNIVERSITY,R01,1988,221748,-0.017893359092287658
"IMPROVEMENT OF DIGITAL ANGIOGRAPHY OF CORONARY ARTERIES Coronary arteriosclerosis is a leading cause of death in U.S., and it is to a large degree the presence or absence of severe coronary stenoses which determines the risk of death.  Because many patients who suffer from this disease are asymptomatic until their first attack, a safe, inexpensive diagnostic procedure for detecting coronary artery stenoses would be beneficial.  The diagnosis as currently carried out requires that a large catheter be inserted into the orifice of each artery in order to inject opaque dye to make the arteries visible with X rays.  The increased contrast sensitivity of digital subtraction angiography (DSA) permits a less invasive technique involving an injection into the aortic root, a smaller catheter, and reduced dye volumes, but it has been of only limited success largely because of image degradation caused by patient motion during image acquisition.  The aim of this project is to develop and test image processing algorithms for the improvement of digital angiography of coronary arteries.  The algorithms search for a geometrical transformation to produce an optimal registration of two images before subtraction.  They are based on a new application of the fundamental principles of fluid dynamics, a new class of transformation functions, a new method of machine calibration, and new application of search methods developed in the field of artifical intelligence for function optimization.  As a result these algorithms represent the first approach to image registration which is capable of handling true three dimensional warping.  The development and evaluation of the algorithms will be carried out on both a specially designed phantom and on data derived in a new way from patient studies. The patient studies will be acquired during angioplasty procedures in which an inflated balloon and catheter will serve as phantom arteries of known dimensions.  The availability of known phantoms located in the chest on a beating human heart will provide for the first time an opportunity for quantitative evaluation of the success of the image enhancement algorithms in a realistic environment.  Thus, the proposed research will employ new techniques of image registration with a new technique for image evaluation to the problem of reducing motion effects in digital subtraction angiography.  n/a",IMPROVEMENT OF DIGITAL ANGIOGRAPHY OF CORONARY ARTERIES,3347902,R01HL034703,"['angiography', ' arteriosclerosis', ' artery stenosis', ' artificial intelligence', ' clinical biomedical equipment', ' coronary artery', ' diagnosis procedure safety', ' heart disorder diagnosis', ' heart imaging /visualization /scanning', ' human subject', ' image processing']",NHLBI,VANDERBILT UNIVERSITY,R01,1987,214228,-0.017893359092287658
"IMPROVEMENT OF DIGITAL ANGIOGRAPHY OF CORONARY ARTERIES Coronary arteriosclerosis is a leading cause of death in U.S., and it is to a large degree the presence or absence of severe coronary stenoses which determines the risk of death.  Because many patients who suffer from this disease are asymptomatic until their first attack, a safe, inexpensive diagnostic procedure for detecting coronary artery stenoses would be beneficial.  The diagnosis as currently carried out requires that a large catheter be inserted into the orifice of each artery in order to inject opaque dye to make the arteries visible with X rays.  The increased contrast sensitivity of digital subtraction angiography (DSA) permits a less invasive technique involving an injection into the aortic root, a smaller catheter, and reduced dye volumes, but it has been of only limited success largely because of image degradation caused by patient motion during image acquisition.  The aim of this project is to develop and test image processing algorithms for the improvement of digital angiography of coronary arteries.  The algorithms search for a geometrical transformation to produce an optimal registration of two images before subtraction.  They are based on a new application of the fundamental principles of fluid dynamics, a new class of transformation functions, a new method of machine calibration, and new application of search methods developed in the field of artifical intelligence for function optimization.  As a result these algorithms represent the first approach to image registration which is capable of handling true three dimensional warping.  The development and evaluation of the algorithms will be carried out on both a specially designed phantom and on data derived in a new way from patient studies. The patient studies will be acquired during angioplasty procedures in which an inflated balloon and catheter will serve as phantom arteries of known dimensions.  The availability of known phantoms located in the chest on a beating human heart will provide for the first time an opportunity for quantitative evaluation of the success of the image enhancement algorithms in a realistic environment.  Thus, the proposed research will employ new techniques of image registration with a new technique for image evaluation to the problem of reducing motion effects in digital subtraction angiography.  n/a",IMPROVEMENT OF DIGITAL ANGIOGRAPHY OF CORONARY ARTERIES,3347900,R01HL034703,"['angiography', ' arteriosclerosis', ' artery stenosis', ' artificial intelligence', ' clinical biomedical equipment', ' coronary artery', ' diagnosis procedure safety', ' heart disorder diagnosis', ' heart imaging /visualization /scanning', ' human subject', ' image processing']",NHLBI,VANDERBILT UNIVERSITY,R01,1986,191615,-0.017893359092287658
"QUANTITATIVE REGIONAL CURVATURE ANALYSIS Artificial intelligence and machine vision systems often use shape analysis as a fundamental approach to characterization of complex objects and motion.  Such concepts have been used to advantage in industry but have not been extensively exploited for cardiac diagnosis.  Aims:  (1) To develop a method of quantitative regional function analysis based on analysis of dynamics of ventricular shape that will replace traditional methods used to assess wall motion so that assumptions regarding coordinate, reference and indexing systems, geometry of the ventricle and uniformity of its contraction pattern can be abandoned.  (2) To quantify the regional shape of the normal right and left ventricles and establish the patterns of shape abnormality seen in the setting of coronary artery disease.  (3) To establish the relationship between regional shape and regional function.  (4) To develop a clinically useable, automated method for ventricular feature extraction and determine its usefulness in predicting the presence and location of coronary disease by analysis of ventricular images obtained during an ischemic stress.  (5) To assess the prognostic significance of regional shape analysis.  (6) To automate the requisite edge detection by using integrative methods of artificial intelligence.  The hypothesis is that the normal ventricle has a characteristic shape and rate of shape change throughout the phases of the cardiac cycle and that abnormal ventricles have abnormal shapes and shape changes by virtue of abnormalities of function.  Therefore, quantitation of abnormalities of shape parameters will provide an indirect measure of ventricular dysfunction.  This will be determined by frame-by-frame curvature analysis of cineventriculograms in patients with normal and stenotic coronary arteries.  Animal studies will compare results of regional thickening and thinning determined by implanted sonomicrometers with shape changes assessed from simultaneous contrast ventriculograms and ventricular outlines determined from implanted subendocardial lead pellets.  The success of this approach will allow abandonment of numerous controversial assumptions mandated by traditional approaches and will solve a long-standing problem in cardiology.  The concepts to be explored will provide new and unique information of potential prognastic importance and it will advance the neglected field of computer vision in the study of non-rigid bodies in motion.  n/a",QUANTITATIVE REGIONAL CURVATURE ANALYSIS,3352105,R01HL036813,"['artery stenosis', ' artificial intelligence', ' cineangiocardiography', ' clinical biomedical equipment', ' computer assisted diagnosis', ' computer assisted medical decision making', ' computer simulation', ' dogs', ' heart ventricle', ' human subject', ' ischemia', ' mathematical model', ' prognosis']",NHLBI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,1990,115629,-0.03660314939570933
"QUANTITATIVE REGIONAL CURVATURE ANALYSIS Artificial intelligence and machine vision systems often use shape analysis as a fundamental approach to characterization of complex objects and motion.  Such concepts have been used to advantage in industry but have not been extensively exploited for cardiac diagnosis.  Aims:  (1) To develop a method of quantitative regional function analysis based on analysis of dynamics of ventricular shape that will replace traditional methods used to assess wall motion so that assumptions regarding coordinate, reference and indexing systems, geometry of the ventricle and uniformity of its contraction pattern can be abandoned.  (2) To quantify the regional shape of the normal right and left ventricles and establish the patterns of shape abnormality seen in the setting of coronary artery disease.  (3) To establish the relationship between regional shape and regional function.  (4) To develop a clinically useable, automated method for ventricular feature extraction and determine its usefulness in predicting the presence and location of coronary disease by analysis of ventricular images obtained during an ischemic stress.  (5) To assess the prognostic significance of regional shape analysis.  (6) To automate the requisite edge detection by using integrative methods of artificial intelligence.  The hypothesis is that the normal ventricle has a characteristic shape and rate of shape change throughout the phases of the cardiac cycle and that abnormal ventricles have abnormal shapes and shape changes by virtue of abnormalities of function.  Therefore, quantitation of abnormalities of shape parameters will provide an indirect measure of ventricular dysfunction.  This will be determined by frame-by-frame curvature analysis of cineventriculograms in patients with normal and stenotic coronary arteries.  Animal studies will compare results of regional thickening and thinning determined by implanted sonomicrometers with shape changes assessed from simultaneous contrast ventriculograms and ventricular outlines determined from implanted subendocardial lead pellets.  The success of this approach will allow abandonment of numerous controversial assumptions mandated by traditional approaches and will solve a long-standing problem in cardiology.  The concepts to be explored will provide new and unique information of potential prognastic importance and it will advance the neglected field of computer vision in the study of non-rigid bodies in motion.  n/a",QUANTITATIVE REGIONAL CURVATURE ANALYSIS,3352104,R01HL036813,"['artery stenosis', ' artificial intelligence', ' cineangiocardiography', ' clinical biomedical equipment', ' computer assisted diagnosis', ' computer assisted medical decision making', ' computer simulation', ' dogs', ' heart ventricle', ' human subject', ' ischemia', ' mathematical model', ' prognosis']",NHLBI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,1989,119204,-0.03660314939570933
"QUANTITATIVE REGIONAL CURVATURE ANALYSIS Artificial intelligence and machine vision systems often use shape analysis as a fundamental approach to characterization of complex objects and motion.  Such concepts have been used to advantage in industry but have not been extensively exploited for cardiac diagnosis.  Aims:  (1) To develop a method of quantitative regional function analysis based on analysis of dynamics of ventricular shape that will replace traditional methods used to assess wall motion so that assumptions regarding coordinate, reference and indexing systems, geometry of the ventricle and uniformity of its contraction pattern can be abandoned.  (2) To quantify the regional shape of the normal right and left ventricles and establish the patterns of shape abnormality seen in the setting of coronary artery disease.  (3) To establish the relationship between regional shape and regional function.  (4) To develop a clinically useable, automated method for ventricular feature extraction and determine its usefulness in predicting the presence and location of coronary disease by analysis of ventricular images obtained during an ischemic stress.  (5) To assess the prognostic significance of regional shape analysis.  (6) To automate the requisite edge detection by using integrative methods of artificial intelligence.  The hypothesis is that the normal ventricle has a characteristic shape and rate of shape change throughout the phases of the cardiac cycle and that abnormal ventricles have abnormal shapes and shape changes by virtue of abnormalities of function.  Therefore, quantitation of abnormalities of shape parameters will provide an indirect measure of ventricular dysfunction.  This will be determined by frame-by-frame curvature analysis of cineventriculograms in patients with normal and stenotic coronary arteries.  Animal studies will compare results of regional thickening and thinning determined by implanted sonomicrometers with shape changes assessed from simultaneous contrast ventriculograms and ventricular outlines determined from implanted subendocardial lead pellets.  The success of this approach will allow abandonment of numerous controversial assumptions mandated by traditional approaches and will solve a long-standing problem in cardiology.  The concepts to be explored will provide new and unique information of potential prognastic importance and it will advance the neglected field of computer vision in the study of non-rigid bodies in motion.  n/a",QUANTITATIVE REGIONAL CURVATURE ANALYSIS,3352103,R01HL036813,"['artery stenosis', ' artificial intelligence', ' cineangiocardiography', ' clinical biomedical equipment', ' computer assisted diagnosis', ' computer assisted medical decision making', ' computer simulation', ' dogs', ' heart ventricle', ' human subject', ' ischemia', ' mathematical model', ' prognosis']",NHLBI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,1988,147759,-0.03660314939570933
"A ROTATING SLIT SCANNING UNIT FOR DIGITAL RADIOGRAPHY The purpose of the research we are proposing is the development of a rotational slit scanning system for digital radiographic applications.  The x-ray receptors will consist of linear photodiode arrays coated with an x-ray phosphor.  The system should be compact and of low cost compared to commercially available linear scanning units.  The spatial resolution imaging capabilities of such a device should exceed one 1p/mm, while offering shorter scan times than conventional linear scanning units.  The benefits expected from such a system would include:  1. significant reduction in the detection of compton scattered x-rays (which will degrade image contrast and increase dose)  2. scan rates sufficient to minimize many patient motion problems  3. an increase in the dynamic range available over that of film  4. adequate spatial resolution for many radiologic applications  Since the same coated arrays may be employed in a linear scan format, direct comparison of image quality between the two scanning formats for the same receptor technology will be possible.  n/a",A ROTATING SLIT SCANNING UNIT FOR DIGITAL RADIOGRAPHY,3174442,R01CA036859,"['artificial intelligence', ' clinical biomedical equipment', ' diagnosis quality /standard', ' image processing', ' online computer']",NCI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,1985,24091,0.022785329247508885
"COMPUTER SIMULATION THEORY OF GLOBULAR PROTEIN DYNA The prediction of the three dimensional structure of a globular protein          from its amino acid sequence along with the mechanism by which protein           folding occurs are among the most important unsolved problems of                 contemporary molecular biology.  The overall objectives of this proposal         are the continued development and refinement of algorithms which not only        can predict protein tertiary structure using only sequence information as        input but also may provide insights into the folding pathway.  To achieve        these goals, this proposal focuses on the lattice based aspects of a             hierarchical approach to protein folding.  High resolution lattice models        of proteins, comprised of an alpha-carbon plus reduced off lattice, side         chain description, will provide the overall folding pathways and folded          conformations.  The resulting folded lattice structures are estimated for        the alpha-carbons to have a 2-4 angstroms rms deviation from the native          state.  Turning to the folding pathways, the predicted molten globule            states and their free energy landscape will be characterized in detail.          The factors responsible for side chain fixation on passage from the molten       globule to the native state will be explored, with particular attention          focused on the interplay of protein sequence and side chain packing.             Specifically this proposal will address the following.  (1).  A new high         coordination lattice model of proteins will be refined, different side           chain realizations will be examined and the dynamic Monte Carlo algorithms       parallelized.  (2).  Better empirical free energy functions will be              developed.  These include better methods for predicting the propensities         for secondary structure and generalization of the hydrogen bond scheme to        include backbone-side chain hydrogen bonds.  To help eliminate misfolded         structures, additional very robust knowledge based rules, such as the            connections in supersecondary structural elements do not cross, will be          included in the interaction scheme.  Sequence specific tertiary                  interactions including a local burial turn, pair interactions and                generalized cooperative multibody side chain contact templates will be           self consistently derived in the presence of predicted secondary structure       propensities.  Then, a recently developed neural network which can               recognize whether 7 by 7 subfragments of sidechain contact maps are              protein like or not will be extended to include sequence specific                preferences for subsequences to adopt specific patterns.  This information       will be obtained from a neural network trained on both homologous and non        homologous subsequences that adopt these patterns.  Thus, it should be           general and not simply applicable to homologous sequence fragments.  (3).        The folding of representative motifs of globular proteins will be                undertaken.  Included are the helical proteins such as cytochrome c, whose       predicted folding pathway will be compared to experiment, myohemerythrin,        myoglobin and complement factor, 1c5a.  The mixed motif proteins include         ubiquitin, flavodoxin and PRA isomerase, and the beta-proteins include the       16th complement control protein of factor H, 1hcc, alpha-amylase,                plastocyanin and retinol binding protein.  (4).  To validate the                 methodology, additional blind predictions of proteins whose structures are       unknown will be undertaken.  Likely candidates include rusticyanin and           erythropoietin.                                                                   n/a",COMPUTER SIMULATION THEORY OF GLOBULAR PROTEIN DYNA,2022122,R01GM037408,"['amylases', ' artificial intelligence', ' chemical models', ' computer simulation', ' cytochrome c', ' globular protein', ' hemerythrin', ' hydrogen bond', ' mathematics', ' model design /development', ' molecular dynamics', ' molecular energy level', ' myoglobin', ' plastocyanin', ' protein folding', ' retinoid binding proteins']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,1996,182701,-0.047290706564762086
"COMPUTER SIMULATION THEORY OF GLOBULAR PROTEIN DYNA The prediction of the three dimensional structure of a globular protein  from its amino acid sequence along with the mechanism by which protein  folding occurs are among the most important unsolved problems of  contemporary molecular biology.  The overall objectives of this proposal  are the continued development and refinement of algorithms which not only  can predict protein tertiary structure using only sequence information as  input but also may provide insights into the folding pathway.  To achieve  these goals, this proposal focuses on the lattice based aspects of a  hierarchical approach to protein folding.  High resolution lattice models  of proteins, comprised of an alpha-carbon plus reduced off lattice, side  chain description, will provide the overall folding pathways and folded  conformations.  The resulting folded lattice structures are estimated for  the alpha-carbons to have a 2-4 angstroms rms deviation from the native  state.  Turning to the folding pathways, the predicted molten globule  states and their free energy landscape will be characterized in detail.  The factors responsible for side chain fixation on passage from the molten  globule to the native state will be explored, with particular attention  focused on the interplay of protein sequence and side chain packing.  Specifically this proposal will address the following.  (1).  A new high  coordination lattice model of proteins will be refined, different side  chain realizations will be examined and the dynamic Monte Carlo algorithms  parallelized.  (2).  Better empirical free energy functions will be  developed.  These include better methods for predicting the propensities  for secondary structure and generalization of the hydrogen bond scheme to  include backbone-side chain hydrogen bonds.  To help eliminate misfolded  structures, additional very robust knowledge based rules, such as the  connections in supersecondary structural elements do not cross, will be  included in the interaction scheme.  Sequence specific tertiary  interactions including a local burial turn, pair interactions and  generalized cooperative multibody side chain contact templates will be  self consistently derived in the presence of predicted secondary structure  propensities.  Then, a recently developed neural network which can  recognize whether 7 by 7 subfragments of sidechain contact maps are  protein like or not will be extended to include sequence specific  preferences for subsequences to adopt specific patterns.  This information  will be obtained from a neural network trained on both homologous and non  homologous subsequences that adopt these patterns.  Thus, it should be  general and not simply applicable to homologous sequence fragments.  (3).  The folding of representative motifs of globular proteins will be  undertaken.  Included are the helical proteins such as cytochrome c, whose  predicted folding pathway will be compared to experiment, myohemerythrin,  myoglobin and complement factor, 1c5a.  The mixed motif proteins include  ubiquitin, flavodoxin and PRA isomerase, and the beta-proteins include the  16th complement control protein of factor H, 1hcc, alpha-amylase,  plastocyanin and retinol binding protein.  (4).  To validate the  methodology, additional blind predictions of proteins whose structures are  unknown will be undertaken.  Likely candidates include rusticyanin and  erythropoietin.  n/a",COMPUTER SIMULATION THEORY OF GLOBULAR PROTEIN DYNA,2178769,R01GM037408,"['amylases', ' artificial intelligence', ' chemical models', ' computer simulation', ' cytochrome c', ' globular protein', ' hemerythrin', ' hydrogen bond', ' mathematics', ' model design /development', ' molecular dynamics', ' molecular energy level', ' myoglobin', ' protein folding', ' retinoid binding proteins']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,1995,175781,-0.047290706564762086
"COMPUTER SIMULATION THEORY OF GLOBULAR PROTEIN DYNA The prediction of the three dimensional structure of a globular protein from its amino acid sequence along with the mechanism by which protein folding occurs are among the most important unsolved problems of contemporary molecular biology.  The overall objectives of this proposal are the continued development and refinement of algorithms which not only can predict protein tertiary structure using only sequence information as input but also may provide insights into the folding pathway.  To achieve these goals, this proposal focuses on the lattice based aspects of a hierarchical approach to protein folding.  High resolution lattice models of proteins, comprised of an alpha-carbon plus reduced off lattice, side chain description, will provide the overall folding pathways and folded conformations.  The resulting folded lattice structures are estimated for the alpha-carbons to have a 2-4 angstroms rms deviation from the native state.  Turning to the folding pathways, the predicted molten globule states and their free energy landscape will be characterized in detail. The factors responsible for side chain fixation on passage from the molten globule to the native state will be explored, with particular attention focused on the interplay of protein sequence and side chain packing. Specifically this proposal will address the following.  (1).  A new high coordination lattice model of proteins will be refined, different side chain realizations will be examined and the dynamic Monte Carlo algorithms parallelized.  (2).  Better empirical free energy functions will be developed.  These include better methods for predicting the propensities for secondary structure and generalization of the hydrogen bond scheme to include backbone-side chain hydrogen bonds.  To help eliminate misfolded structures, additional very robust knowledge based rules, such as the connections in supersecondary structural elements do not cross, will be included in the interaction scheme.  Sequence specific tertiary interactions including a local burial turn, pair interactions and generalized cooperative multibody side chain contact templates will be self consistently derived in the presence of predicted secondary structure propensities.  Then, a recently developed neural network which can recognize whether 7 by 7 subfragments of sidechain contact maps are protein like or not will be extended to include sequence specific preferences for subsequences to adopt specific patterns.  This information will be obtained from a neural network trained on both homologous and non homologous subsequences that adopt these patterns.  Thus, it should be general and not simply applicable to homologous sequence fragments.  (3). The folding of representative motifs of globular proteins will be undertaken.  Included are the helical proteins such as cytochrome c, whose predicted folding pathway will be compared to experiment, myohemerythrin, myoglobin and complement factor, 1c5a.  The mixed motif proteins include ubiquitin, flavodoxin and PRA isomerase, and the beta-proteins include the 16th complement control protein of factor H, 1hcc, alpha-amylase, plastocyanin and retinol binding protein.  (4).  To validate the methodology, additional blind predictions of proteins whose structures are unknown will be undertaken.  Likely candidates include rusticyanin and erythropoietin.  n/a",COMPUTER SIMULATION THEORY OF GLOBULAR PROTEIN DYNA,2178767,R01GM037408,"['amylases', ' artificial intelligence', ' chemical models', ' computer simulation', ' cytochrome c', ' globular protein', ' hemerythrin', ' hydrogen bond', ' mathematics', ' model design /development', ' molecular dynamics', ' molecular energy level', ' myoglobin', ' protein folding', ' retinoid binding proteins']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,1994,176981,-0.047290706564762086
"Statistical Learning in Language Acquisition DESCRIPTION (provided by applicant): Acquiring a language is among the most daunting feats uniformly accomplished by our species. Explorations of the language learning process present the opportunity to address central issues pertaining to human cognition and its development. The proposed research represents one approach to this topic: the study of the architecture of the learning processes underlying language acquisition, using laboratory learning experiments performed with adult, child, and infant participants. The goal of this research program is to substantially increase our knowledge of the statistical learning mechanisms that detect linguistic units by tracking the patterns of sounds, words, and other units in the input. Recent results suggest that statistical learning processes play an important role in the acquisition of language. However, little is currently known about the types of statistical regularities computed by learners and the constraints on learning that support successful knowledge acquisition. The proposed experiments will ask: (1) How does the structure of the input constrain statistical learning? Studies will consider perceptual, linguistic, and informational constraints on the choice of cues as input to learning. (2) How does prior experience influence statistical learning? Manipulations of prior experience, inside and outside the lab, will be used to assess effects on subsequent learning. (3) Can statistical learning account for basic phenomena in language acquisition? Interactions between statistical learning and other types of processes, such as rule learning and the presence of other cues in the input, will be investigated. All of these issues will be addressed using previously developed laboratory learning paradigms that permit careful manipulation of input and detailed assessment of what participants are able to learn. Studies using linguistic materials will be contrasted with studies using nonlinguistic materials to further explore the locus and domain-specificity of the learning mechanisms under consideration. The answers to these questions will inform an emerging theoretical framework, constrained statistical learning, intended to elucidate the study of language acquisition and other pressing issues in human learning and development. n/a",Statistical Learning in Language Acquisition,7531059,R01HD037466,"['Accounting', 'Acoustics', 'Address', 'Adult', 'Affect', 'Architecture', 'Categories', 'Child', 'Cognition', 'Complex', 'Cues', 'Development', 'Goals', 'Human', 'Indium', 'Infant', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Language Development', 'Learning', 'Linguistics', 'Machine Learning', 'Methodology', 'Nature', 'Operative Surgical Procedures', 'Output', 'Participant', 'Pattern', 'Play', 'Probability', 'Process', 'Research', 'Role', 'Solutions', 'Specificity', 'Speech', 'Structure', 'System', 'analog', 'base', 'design', 'experience', 'programs', 'research study', 'sound', 'theories']",NICHD,UNIVERSITY OF WISCONSIN-MADISON,R01,2009,269938,-0.06319454632647326
"Statistical Learning in Language Acquisition DESCRIPTION (provided by applicant): Acquiring a language is among the most daunting feats uniformly accomplished by our species. Explorations of the language learning process present the opportunity to address central issues pertaining to human cognition and its development. The proposed research represents one approach to this topic: the study of the architecture of the learning processes underlying language acquisition, using laboratory learning experiments performed with adult, child, and infant participants. The goal of this research program is to substantially increase our knowledge of the statistical learning mechanisms that detect linguistic units by tracking the patterns of sounds, words, and other units in the input. Recent results suggest that statistical learning processes play an important role in the acquisition of language. However, little is currently known about the types of statistical regularities computed by learners and the constraints on learning that support successful knowledge acquisition. The proposed experiments will ask: (1) How does the structure of the input constrain statistical learning? Studies will consider perceptual, linguistic, and informational constraints on the choice of cues as input to learning. (2) How does prior experience influence statistical learning? Manipulations of prior experience, inside and outside the lab, will be used to assess effects on subsequent learning. (3) Can statistical learning account for basic phenomena in language acquisition? Interactions between statistical learning and other types of processes, such as rule learning and the presence of other cues in the input, will be investigated. All of these issues will be addressed using previously developed laboratory learning paradigms that permit careful manipulation of input and detailed assessment of what participants are able to learn. Studies using linguistic materials will be contrasted with studies using nonlinguistic materials to further explore the locus and domain-specificity of the learning mechanisms under consideration. The answers to these questions will inform an emerging theoretical framework, constrained statistical learning, intended to elucidate the study of language acquisition and other pressing issues in human learning and development. n/a",Statistical Learning in Language Acquisition,7336359,R01HD037466,"['Accounting', 'Acoustics', 'Address', 'Adult', 'Affect', 'Architecture', 'Categories', 'Child', 'Cognition', 'Complex', 'Cues', 'Development', 'Goals', 'Human', 'Indium', 'Infant', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Language Development', 'Learning', 'Linguistics', 'Machine Learning', 'Methodology', 'Nature', 'Operative Surgical Procedures', 'Output', 'Participant', 'Pattern', 'Play', 'Probability', 'Process', 'Research', 'Role', 'Solutions', 'Specificity', 'Speech', 'Structure', 'System', 'analog', 'base', 'design', 'experience', 'programs', 'research study', 'sound', 'theories']",NICHD,UNIVERSITY OF WISCONSIN-MADISON,R01,2008,270051,-0.06319454632647326
"Statistical Learning in Language Acquisition DESCRIPTION (provided by applicant): Acquiring a language is among the most daunting feats uniformly accomplished by our species. Explorations of the language learning process present the opportunity to address central issues pertaining to human cognition and its development. The proposed research represents one approach to this topic: the study of the architecture of the learning processes underlying language acquisition, using laboratory learning experiments performed with adult, child, and infant participants. The goal of this research program is to substantially increase our knowledge of the statistical learning mechanisms that detect linguistic units by tracking the patterns of sounds, words, and other units in the input. Recent results suggest that statistical learning processes play an important role in the acquisition of language. However, little is currently known about the types of statistical regularities computed by learners and the constraints on learning that support successful knowledge acquisition. The proposed experiments will ask: (1) How does the structure of the input constrain statistical learning? Studies will consider perceptual, linguistic, and informational constraints on the choice of cues as input to learning. (2) How does prior experience influence statistical learning? Manipulations of prior experience, inside and outside the lab, will be used to assess effects on subsequent learning. (3) Can statistical learning account for basic phenomena in language acquisition? Interactions between statistical learning and other types of processes, such as rule learning and the presence of other cues in the input, will be investigated. All of these issues will be addressed using previously developed laboratory learning paradigms that permit careful manipulation of input and detailed assessment of what participants are able to learn. Studies using linguistic materials will be contrasted with studies using nonlinguistic materials to further explore the locus and domain-specificity of the learning mechanisms under consideration. The answers to these questions will inform an emerging theoretical framework, constrained statistical learning, intended to elucidate the study of language acquisition and other pressing issues in human learning and development. n/a",Statistical Learning in Language Acquisition,7932637,R01HD037466,"['Accounting', 'Acoustics', 'Address', 'Adult', 'Affect', 'Architecture', 'Categories', 'Child', 'Cognition', 'Complex', 'Cues', 'Development', 'Goals', 'Human', 'Indium', 'Infant', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Language Development', 'Learning', 'Linguistics', 'Machine Learning', 'Methodology', 'Nature', 'Operative Surgical Procedures', 'Output', 'Participant', 'Pattern', 'Play', 'Probability', 'Process', 'Research', 'Role', 'Solutions', 'Specificity', 'Speech', 'Structure', 'System', 'analog', 'base', 'design', 'experience', 'programs', 'research study', 'sound', 'theories']",NICHD,UNIVERSITY OF WISCONSIN-MADISON,R01,2009,64856,-0.06319454632647326
"Statistical Learning in Language Acquisition DESCRIPTION (provided by applicant): Acquiring a language is among the most daunting feats uniformly accomplished by our species. Explorations of the language learning process present the opportunity to address central issues pertaining to human cognition and its development. The proposed research represents one approach to this topic: the study of the architecture of the learning processes underlying language acquisition, using laboratory learning experiments performed with adult, child, and infant participants. The goal of this research program is to substantially increase our knowledge of the statistical learning mechanisms that detect linguistic units by tracking the patterns of sounds, words, and other units in the input. Recent results suggest that statistical learning processes play an important role in the acquisition of language. However, little is currently known about the types of statistical regularities computed by learners and the constraints on learning that support successful knowledge acquisition. The proposed experiments will ask: (1) How does the structure of the input constrain statistical learning? Studies will consider perceptual, linguistic, and informational constraints on the choice of cues as input to learning. (2) How does prior experience influence statistical learning? Manipulations of prior experience, inside and outside the lab, will be used to assess effects on subsequent learning. (3) Can statistical learning account for basic phenomena in language acquisition? Interactions between statistical learning and other types of processes, such as rule learning and the presence of other cues in the input, will be investigated. All of these issues will be addressed using previously developed laboratory learning paradigms that permit careful manipulation of input and detailed assessment of what participants are able to learn. Studies using linguistic materials will be contrasted with studies using nonlinguistic materials to further explore the locus and domain-specificity of the learning mechanisms under consideration. The answers to these questions will inform an emerging theoretical framework, constrained statistical learning, intended to elucidate the study of language acquisition and other pressing issues in human learning and development. n/a",Statistical Learning in Language Acquisition,7144995,R01HD037466,"['Accounting', 'Acoustics', 'Address', 'Adult', 'Affect', 'Architecture', 'Categories', 'Child', 'Cognition', 'Complex', 'Cues', 'Development', 'Goals', 'Human', 'Indium', 'Infant', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Language Development', 'Learning', 'Linguistics', 'Machine Learning', 'Methodology', 'Nature', 'Operative Surgical Procedures', 'Output', 'Participant', 'Pattern', 'Play', 'Probability', 'Process', 'Research', 'Role', 'Solutions', 'Specificity', 'Speech', 'Structure', 'System', 'analog', 'base', 'design', 'experience', 'programs', 'research study', 'sound', 'theories']",NICHD,UNIVERSITY OF WISCONSIN-MADISON,R01,2007,275672,-0.06319454632647326
"THERAPY FOR THALASSEMIA The overall objective of this work is to extend receptor based drug design to include the use of modern state-of-the-art simulation of the structure, energetics, and dynamics of protein-protein and protein-ligand interactions.  In particular, we propose to use these techniques to model an alpha-hemoglobin tetramer and to use information obtained from these studies for designing ligands that stabilize such oxygen carrier alpha-tetramers.  The resulting ligands could ultimately be used in the treatment of Beta Thalassemia.  n/a",THERAPY FOR THALASSEMIA,3501110,R43HL038563,"['artificial intelligence', ' blood disorder chemotherapy', ' chemical binding', ' chemical condensation', ' chemical models', ' chemical structure', ' computer simulation', ' diphosphoglycerate', ' drug design /synthesis /production', ' heme', ' hemoprotein structure', ' ligands', ' molecular film', ' oxygen transport', ' thalassemia']",NHLBI,"BIOSYM TECHNOLOGIES, INC.",R43,1987,50000,-0.013096554118867667
"HIGH-RESOLUTION MULTIPARAMETER CELL ANALYSIS AND SORTING Our proposed multivariate statistical analysis and sorting methods are based on what we perceive to be some of the more fundamental problems in multiparameter flow cytometry and cell sorting.  The ability of experimenters in both the basic research and clinical laboratories to acquire complex multiparameter flow cytometric data has far out-stripped most currently available data analysis methods which deal with data at the histogram or bivariate display level.  Listmode reprocessing and use of color to ""map"" the data through various histogram or bivariate displays is useful but inadequate for at least three reasons.  First, the selection of regions is usually done by arbitrarily chosen rectilinear or bit-map boundaries.  Second, even if all possible bivariate displays are viewed one does not see all of the data since these projections are at fixed orientations to the actual multidimensional data.  Expecting to see all of the multidimensional data with bivariate displays is analogous to trying to see 2 dimensional data with the separate histograms.  Third, non-flow cytometric (FCM) data is usually not included in the overall analysis (and never directly in the sorting process) of flow cytometry data in terms of statistical analyses of the combined FCM and non-FCM data.  To address these problems we first attempt to view this complex data in as high a dimensionality as possible through use of projection pursuit analyses (e.g. principal components, Friedman-Tukey) and our own ""local multivariate dimensionality"" analyses and the use of our unique autostereo scopic display and 3D mouse.  Second, we attempt to address the question of proper selection of analysis and sort boundaries based on rigorous multivariate statistical techniques rather than arbitrarily constructed rectilinear or bit-map boundaries.  Third, we attempt to directly combine other non-flow cytometric information about the cells of interest (e.g. patient information, physicians' diagnoses) with flow cytometric variables using recursive partitioning techniques.  In addition, we attempt to address the problem of high-resolution cell sorting beyond rectilinear and bit-map sorting by using the preceding statistical methods to allow real-time sorting of cells based on sophisticated statistics and expert systems criteria using our special ""flexible"" sorting system.  Lastly, we propose implementing the preceding techniques in hardware, software and optics that will allow for direct sorting and subsequent analyses of cells to confirm the utility of these multivariate techniques for a variety of important biological and clinical applications.  n/a",HIGH-RESOLUTION MULTIPARAMETER CELL ANALYSIS AND SORTING,2179438,R01GM038645,"['artificial intelligence', ' bioengineering /biomedical engineering', ' cell population study', ' computer assisted diagnosis', ' computer data analysis', ' computer graphics /printing', ' computer program /software', ' flow cytometry', ' gel electrophoresis', ' image processing', ' method development', ' personal computers', ' polymerase chain reaction', ' single cell analysis', ' statistics /biometry', ' time resolved data']",NIGMS,UNIVERSITY OF ROCHESTER,R01,1994,252477,-0.002944992593295906
"HIGH-RESOLUTION MULTIPARAMETER CELL ANALYSIS AND SORTING Our proposed multivariate statistical analysis and sorting methods are based on what we perceive to be some of the more fundamental problems in multiparameter flow cytometry and cell sorting.  The ability of experimenters in both the basic research and clinical laboratories to acquire complex multiparameter flow cytometric data has far out-stripped most currently available data analysis methods which deal with data at the histogram or bivariate display level.  Listmode reprocessing and use of color to ""map"" the data through various histogram or bivariate displays is useful but inadequate for at least three reasons.  First, the selection of regions is usually done by arbitrarily chosen rectilinear or bit-map boundaries.  Second, even if all possible bivariate displays are viewed one does not see all of the data since these projections are at fixed orientations to the actual multidimensional data.  Expecting to see all of the multidimensional data with bivariate displays is analogous to trying to see 2 dimensional data with the separate histograms.  Third, non-flow cytometric (FCM) data is usually not included in the overall analysis (and never directly in the sorting process) of flow cytometry data in terms of statistical analyses of the combined FCM and non-FCM data.  To address these problems we first attempt to view this complex data in as high a dimensionality as possible through use of projection pursuit analyses (e.g. principal components, Friedman-Tukey) and our own ""local multivariate dimensionality"" analyses and the use of our unique autostereo scopic display and 3D mouse.  Second, we attempt to address the question of proper selection of analysis and sort boundaries based on rigorous multivariate statistical techniques rather than arbitrarily constructed rectilinear or bit-map boundaries.  Third, we attempt to directly combine other non-flow cytometric information about the cells of interest (e.g. patient information, physicians' diagnoses) with flow cytometric variables using recursive partitioning techniques.  In addition, we attempt to address the problem of high-resolution cell sorting beyond rectilinear and bit-map sorting by using the preceding statistical methods to allow real-time sorting of cells based on sophisticated statistics and expert systems criteria using our special ""flexible"" sorting system.  Lastly, we propose implementing the preceding techniques in hardware, software and optics that will allow for direct sorting and subsequent analyses of cells to confirm the utility of these multivariate techniques for a variety of important biological and clinical applications.  n/a",HIGH-RESOLUTION MULTIPARAMETER CELL ANALYSIS AND SORTING,3295228,R01GM038645,"['artificial intelligence', ' bioengineering /biomedical engineering', ' cell population study', ' computer assisted diagnosis', ' computer data analysis', ' computer graphics /printing', ' computer program /software', ' flow cytometry', ' gel electrophoresis', ' image processing', ' method development', ' personal computers', ' polymerase chain reaction', ' single cell analysis', ' statistics /biometry', ' time resolved data']",NIGMS,UNIVERSITY OF ROCHESTER,R01,1993,243540,-0.002944992593295906
"HIGH-RESOLUTION MULTIPARAMETER CELL ANALYSIS AND SORTING Our proposed multivariate statistical analysis and sorting methods are based on what we perceive to be some of the more fundamental problems in multiparameter flow cytometry and cell sorting.  The ability of experimenters in both the basic research and clinical laboratories to acquire complex multiparameter flow cytometric data has far out-stripped most currently available data analysis methods which deal with data at the histogram or bivariate display level.  Listmode reprocessing and use of color to ""map"" the data through various histogram or bivariate displays is useful but inadequate for at least three reasons.  First, the selection of regions is usually done by arbitrarily chosen rectilinear or bit-map boundaries.  Second, even if all possible bivariate displays are viewed one does not see all of the data since these projections are at fixed orientations to the actual multidimensional data.  Expecting to see all of the multidimensional data with bivariate displays is analogous to trying to see 2 dimensional data with the separate histograms.  Third, non-flow cytometric (FCM) data is usually not included in the overall analysis (and never directly in the sorting process) of flow cytometry data in terms of statistical analyses of the combined FCM and non-FCM data.  To address these problems we first attempt to view this complex data in as high a dimensionality as possible through use of projection pursuit analyses (e.g. principal components, Friedman-Tukey) and our own ""local multivariate dimensionality"" analyses and the use of our unique autostereo scopic display and 3D mouse.  Second, we attempt to address the question of proper selection of analysis and sort boundaries based on rigorous multivariate statistical techniques rather than arbitrarily constructed rectilinear or bit-map boundaries.  Third, we attempt to directly combine other non-flow cytometric information about the cells of interest (e.g. patient information, physicians' diagnoses) with flow cytometric variables using recursive partitioning techniques.  In addition, we attempt to address the problem of high-resolution cell sorting beyond rectilinear and bit-map sorting by using the preceding statistical methods to allow real-time sorting of cells based on sophisticated statistics and expert systems criteria using our special ""flexible"" sorting system.  Lastly, we propose implementing the preceding techniques in hardware, software and optics that will allow for direct sorting and subsequent analyses of cells to confirm the utility of these multivariate techniques for a variety of important biological and clinical applications.  n/a",HIGH-RESOLUTION MULTIPARAMETER CELL ANALYSIS AND SORTING,3295225,R01GM038645,"['artificial intelligence', ' bioengineering /biomedical engineering', ' cell population study', ' computer assisted diagnosis', ' computer data analysis', ' computer graphics /printing', ' computer program /software', ' flow cytometry', ' gel electrophoresis', ' image processing', ' method development', ' personal computers', ' polymerase chain reaction', ' single cell analysis', ' statistics /biometry', ' time resolved data']",NIGMS,UNIVERSITY OF ROCHESTER,R01,1992,243662,-0.002944992593295906
"ATTENTIONAL DEFICIT CHILDREN: A DIMENSIONAL ANALYSIS This study will map certain dimensional aspects of personality (impulsivity, anxiety-proneness, and sensation seeking) and laboratory measures of nervous system sensitivity in 200 heterogeneous, clinically and school referred attention deficit disordered (ADD) subjects (structured interview diagnosis) and 50 controls.  Behavioral and physiological measures will be obtained during augmentation-reduction and signal detection procedures.  The primary goal is to construct a decision tree that will enhance the DSM-III diagnosis of ADD subjects in the sense of making it more predictive of clinical outcome at follow-up.  Past research of ours and others indicates that an augmentation-reduction procedure, in which the individual responds to tones of increasing intensity, differentiates hyperactive ADD children from controls and is predictive of therapeutic drug dose.  We will attempt to replicate and extend these results using signal detection techniques which would appear to be superior to augmenting procedures for purposes of studying nervous system sensitivity and attentional processes.  Russian investigators refer to sensitivity as strength.  This is assessed by them as a subject's ability to detect stimuli low in intensity and/or to withstand strong stimuli.  We extend this definition to include four hypothesized relatively independent dimensions:  sensitivity to tones low and high in intensity and sensitivity to reward and punishment.  These four laboratory measures will be correlated with dimensional aspects of personality, including impulsivity, aggressivity, anxiety-proneness, and sensation seeking.  Subjects will be grouped post hoc in several ways:  (a) on the basis of DSM-III diagnosis; (b) on the basis of DSM-III and personality attributes; (c) on the basis of DSM-III diagnoses and sensitivity indices; and (d) on the basis of DSM-III, personality, and sensitivity measures.  We hypothesize that (b), (c) and (d) will be more predictive of clinical outcome than (a).  We also expect (b) to correlate more closely with the laboratory measures of sensitivity than (a), but expect (b) and (c) to be independent predictors of outcome and to correlate about equally with outcome.  And we expect (d) to be the best predictor of outcome, particularly for children receiving methylphenidate.  n/a",ATTENTIONAL DEFICIT CHILDREN: A DIMENSIONAL ANALYSIS,3377187,R01MH039189,"['attention deficit disorder', ' auditory stimulus', ' behavior test', ' central nervous system', ' diagnosis design /evaluation', ' electrical measurement', ' electroencephalography', ' human subject', ' learning disorders', ' mental disorder chemotherapy', ' mental disorder diagnosis', ' methylphenidate', ' middle childhood (6-11)', ' personality', ' personality tests', ' psychophysiology', ' reinforcer', ' stimulus /response', ' visual stimulus']",NIMH,UNIVERSITY OF ARKANSAS MED SCIS LTL ROCK,R01,1989,125616,-0.06064910423841129
"ATTENTIONAL DEFICIT CHILDREN:  A DIMENSIONAL ANALYSIS This study will map certain dimensional aspects of personality (impulsivity, anxiety-proneness, and sensation seeking) and laboratory measures of nervous system sensitivity in 200 heterogeneous, clinically and school referred attention deficit disordered (ADD) subjects (structured interview diagnosis) and 50 controls.  Behavioral and physiological measures will be obtained during augmentation-reduction and signal detection procedures.  The primary goal is to construct a decision tree that will enhance the DSM-III diagnosis of ADD subjects in the sense of making it more predictive of clinical outcome at follow-up.  Past research of ours and others indicates that an augmentation-reduction procedure, in which the individual responds to tones of increasing intensity, differentiates hyperactive ADD children from controls and is predictive of therapeutic drug dose.  We will attempt to replicate and extend these results using signal detection techniques which would appear to be superior to augmenting procedures for purposes of studying nervous system sensitivity and attentional processes.  Russian investigators refer to sensitivity as strength.  This is assessed by them as a subject's ability to detect stimuli low in intensity and/or to withstand strong stimuli.  We extend this definition to include four hypothesized relatively independent dimensions:  sensitivity to tones low and high in intensity and sensitivity to reward and punishment.  These four laboratory measures will be correlated with dimensional aspects of personality, including impulsivity, aggressivity, anxiety-proneness, and sensation seeking.  Subjects will be grouped post hoc in several ways:  (a) on the basis of DSM-III diagnosis; (b) on the basis of DSM-III and personality attributes; (c) on the basis of DSM-III diagnoses and sensitivity indices; and (d) on the basis of DSM-III, personality, and sensitivity measures.  We hypothesize that (b), (c) and (d) will be more predictive of clinical outcome than (a).  We also expect (b) to correlate more closely with the laboratory measures of sensitivity than (a), but expect (b) and (c) to be independent predictors of outcome and to correlate about equally with outcome.  And we expect (d) to be the best predictor of outcome, particularly for children receiving methylphenidate.  n/a",ATTENTIONAL DEFICIT CHILDREN:  A DIMENSIONAL ANALYSIS,3377190,R01MH039189,"['aggression', ' anxiety', ' attention', ' attention deficit disorder', ' auditory stimulus', ' behavior test', ' child mental disorders', ' electroencephalography', ' human subject', ' impulsive behavior', ' learning disorders', ' mental disorder chemotherapy', ' mental disorder diagnosis', ' methylphenidate', ' middle childhood (6-11)', ' nervous system', ' personality', ' personality tests', ' psychophysiology', ' reinforcer', ' stimulus /response', ' visual stimulus']",NIMH,UNIVERSITY OF ARKANSAS MED SCIS LTL ROCK,R01,1987,95058,-0.06064910423841129
"ATTENTIONAL DEFICIT CHILDREN:  A DIMENSIONAL ANALYSIS This study will map certain dimensional aspects of personality (impulsivity, anxiety-proneness, and sensation seeking) and laboratory measures of nervous system sensitivity in 200 heterogeneous, clinically and school referred attention deficit disordered (ADD) subjects (structured interview diagnosis) and 50 controls.  Behavioral and physiological measures will be obtained during augmentation-reduction and signal detection procedures.  The primary goal is to construct a decision tree that will enhance the DSM-III diagnosis of ADD subjects in the sense of making it more predictive of clinical outcome at follow-up.  Past research of ours and others indicates that an augmentation-reduction procedure, in which the individual responds to tones of increasing intensity, differentiates hyperactive ADD children from controls and is predictive of therapeutic drug dose.  We will attempt to replicate and extend these results using signal detection techniques which would appear to be superior to augmenting procedures for purposes of studying nervous system sensitivity and attentional processes.  Russian investigators refer to sensitivity as strength.  This is assessed by them as a subject's ability to detect stimuli low in intensity and/or to withstand strong stimuli.  We extend this definition to include four hypothesized relatively independent dimensions:  sensitivity to tones low and high in intensity and sensitivity to reward and punishment.  These four laboratory measures will be correlated with dimensional aspects of personality, including impulsivity, aggressivity, anxiety-proneness, and sensation seeking.  Subjects will be grouped post hoc in several ways:  (a) on the basis of DSM-III diagnosis; (b) on the basis of DSM-III and personality attributes; (c) on the basis of DSM-III diagnoses and sensitivity indices; and (d) on the basis of DSM-III, personality, and sensitivity measures.  We hypothesize that (b), (c) and (d) will be more predictive of clinical outcome than (a).  We also expect (b) to correlate more closely with the laboratory measures of sensitivity than (a), but expect (b) and (c) to be independent predictors of outcome and to correlate about equally with outcome.  And we expect (d) to be the best predictor of outcome, particularly for children receiving methylphenidate.  n/a",ATTENTIONAL DEFICIT CHILDREN:  A DIMENSIONAL ANALYSIS,3377189,R01MH039189,"['aggression', ' anxiety', ' attention', ' attention deficit disorder', ' auditory stimulus', ' behavior test', ' child mental disorders', ' electroencephalography', ' human subject', ' impulsive behavior', ' learning disorders', ' mental disorder chemotherapy', ' mental disorder diagnosis', ' methylphenidate', ' middle childhood (6-11)', ' nervous system', ' personality', ' personality tests', ' psychophysiology', ' reinforcer', ' stimulus /response', ' visual stimulus']",NIMH,UNIVERSITY OF ARKANSAS MED SCIS LTL ROCK,R01,1986,126780,-0.06064910423841129
"ATTENTIONAL DEFICIT CHILDREN:  A DIMENSIONAL ANALYSIS This study will map certain dimensional aspects of personality (impulsivity, anxiety-proneness, and sensation seeking) and laboratory measures of nervous system sensitivity in 200 heterogeneous, clinically and school referred attention deficit disordered (ADD) subjects (structured interview diagnosis) and 50 controls.  Behavioral and physiological measures will be obtained during augmentation-reduction and signal detection procedures.  The primary goal is to construct a decision tree that will enhance the DSM-III diagnosis of ADD subjects in the sense of making it more predictive of clinical outcome at follow-up.  Past research of ours and others indicates that an augmentation-reduction procedure, in which the individual responds to tones of increasing intensity, differentiates hyperactive ADD children from controls and is predictive of therapeutic drug dose.  We will attempt to replicate and extend these results using signal detection techniques which would appear to be superior to augmenting procedures for purposes of studying nervous system sensitivity and attentional processes.  Russian investigators refer to sensitivity as strength.  This is assessed by them as a subject's ability to detect stimuli low in intensity and/or to withstand strong stimuli.  We extend this definition to include four hypothesized relatively independent dimensions:  sensitivity to tones low and high in intensity and sensitivity to reward and punishment.  These four laboratory measures will be correlated with dimensional aspects of personality, including impulsivity, aggressivity, anxiety-proneness, and sensation seeking.  Subjects will be grouped post hoc in several ways:  (a) on the basis of DSM-III diagnosis; (b) on the basis of DSM-III and personality attributes; (c) on the basis of DSM-III diagnoses and sensitivity indices; and (d) on the basis of DSM-III, personality, and sensitivity measures.  We hypothesize that (b), (c) and (d) will be more predictive of clinical outcome than (a).  We also expect (b) to correlate more closely with the laboratory measures of sensitivity than (a), but expect (b) and (c) to be independent predictors of outcome and to correlate about equally with outcome.  And we expect (d) to be the best predictor of outcome, particularly for children receiving methylphenidate.  n/a",ATTENTIONAL DEFICIT CHILDREN:  A DIMENSIONAL ANALYSIS,3377188,R01MH039189,"['aggression', ' anxiety', ' attention', ' attention deficit disorder', ' auditory stimulus', ' behavior test', ' child mental disorders', ' electroencephalography', ' human subject', ' impulsive behavior', ' learning disorders', ' mental disorder chemotherapy', ' mental disorder diagnosis', ' methylphenidate', ' middle childhood (6-11)', ' nervous system', ' personality', ' personality tests', ' psychophysiology', ' reinforcer', ' stimulus /response', ' visual stimulus']",NIMH,UNIVERSITY OF ARKANSAS MED SCIS LTL ROCK,R01,1985,128972,-0.06064910423841129
"OPERATION, SUPPORT AND STRATEGIC ENHANCEMENT OF THE NEUROSCIENCE INFORMATION FRAMEWORK ﻿    DESCRIPTION: The Neuroscience Information Framework (NIF; http://neuinfo.org) is currently managed, maintained, and hosted by researchers in the Center for Research in Biological Systems (CRBS) at the University of California, San Diego. Our group is the principal developer of the NIF system and has overseen its growth since 2008 from a modest catalog of 300 resources developed during the first phase of NIF, to the largest source of neuroscience resources on the web. As defined here, resources include data, databases, software/web-based tools, materials, literature, networks, terminologies, or information that would accelerate the pace of neuroscience research and discovery. NIF was instantiated because many of these valuable tools and services were largely unknown to the scientific community they were meant to serve. With the launch of major brain initiatives in the US and Europe, the amount of neuroscience data and tools will continue to increase.  NIF can be viewed as a cost effective ""PubMed"" and ""PubMed Central"" for digital assets, e.g., databases, software tools, alternative media, to make them collectively searchable and present a unified discovery environment for biomedical researchers. The NIF is heavily used, as measured by the number of visitors per month (more than 40,000) to the NIF web resources and the large number of repeat users (~35%) that visit the NIF discovery portal on a regular basis. NIF's services, standards and products are also heavily used as our web services are now regularly receiving more than 15 million hits per month. NIF has developed a novel sustainability plan that provides for continued enhancement and population of the resource. NIF has developed a reputation as a trusted community partner, allowing us to gain cooperation with large segments of the neuroscience community, as well as publishers, non-profit and government organizations. Through these networks, we've been able to launch major initiatives, help launch new collaborative efforts in basic and clinical neuroscience, and implement standards to help transform the way that we cite and track research resources. The work to be performed during the award period will be directed towards operating and maintaining the current NIF system while providing necessary strategic enhancements and providing broad outreach and dissemination efforts to encourage utilization of the NIF. A core new area of development will be centered around providing analytic tools to explore and identify lacunas of knowledge in Neuroscience. The initial work will focus on developing analytic heatmaps for activation foci from the neuroimaging literature. PUBLIC HEALTH RELEVANCE: Modern biomedical science involves the accrual of increasingly larger data sets in many forms. The NIH Institutes that are a part of the Blueprint fo Neuroscience Research have invested heavily in the production of such data sets and research resources. This award will ensure that the Neuroscience Information Framework will continue to serve as the cost effective ""PubMed"" and ""PubMed Central"" for these assets (e.g., databases, software tools), making them collectively searchable and presenting them in a unified discovery environment for biomedical researchers and students.","OPERATION, SUPPORT AND STRATEGIC ENHANCEMENT OF THE NEUROSCIENCE INFORMATION FRAMEWORK",9039028,U24DA039832,"['Accounting', 'Area', 'Award', 'Awareness', 'Brain', 'Brain region', 'California', 'Cataloging', 'Catalogs', 'Characteristics', 'Clinical', 'Communities', 'Computer software', 'Contracts', 'Data', 'Data Science', 'Data Set', 'Databases', 'Development', 'Education and Outreach', 'Ensure', 'Environment', 'Europe', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Government', 'Growth', 'Health', 'Human', 'Imagery', 'Informatics', 'Ingestion', 'Institutes', 'Internet', 'Knowledge', 'Life', 'Literature', 'Maintenance', 'Maps', 'Measures', 'Mission', 'Neurosciences', 'Neurosciences Research', 'Online Systems', 'Ontology', 'Paper', 'Participant', 'Phase', 'Pilot Projects', 'Population', 'Production', 'Provider', 'PubMed', 'Public Domains', 'Publishing', 'Questioning individuals', 'Registries', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Semantics', 'Series', 'Services', 'Slice', 'Software Tools', 'Source', 'Specific qualifier value', 'Students', 'System', 'Terminology', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'Universities', 'Update', 'Vertebral column', 'Visit', 'Work', 'base', 'biological systems', 'cognitive task', 'cost effective', 'crowdsourcing', 'data portal', 'digital', 'improved', 'indexing', 'information framework', 'innovation', 'meetings', 'neuroimaging', 'novel', 'operation', 'outreach', 'prospective', 'social media', 'text searching', 'tool', 'web interface', 'web services', 'web site', 'webinar']",NIDA,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2016,877912,-0.049751720108207234
"OPERATION, SUPPORT AND STRATEGIC ENHANCEMENT OF THE NEUROSCIENCE INFORMATION FRAMEWORK ﻿    DESCRIPTION: The Neuroscience Information Framework (NIF; http://neuinfo.org) is currently managed, maintained, and hosted by researchers in the Center for Research in Biological Systems (CRBS) at the University of California, San Diego. Our group is the principal developer of the NIF system and has overseen its growth since 2008 from a modest catalog of 300 resources developed during the first phase of NIF, to the largest source of neuroscience resources on the web. As defined here, resources include data, databases, software/web-based tools, materials, literature, networks, terminologies, or information that would accelerate the pace of neuroscience research and discovery. NIF was instantiated because many of these valuable tools and services were largely unknown to the scientific community they were meant to serve. With the launch of major brain initiatives in the US and Europe, the amount of neuroscience data and tools will continue to increase.  NIF can be viewed as a cost effective ""PubMed"" and ""PubMed Central"" for digital assets, e.g., databases, software tools, alternative media, to make them collectively searchable and present a unified discovery environment for biomedical researchers. The NIF is heavily used, as measured by the number of visitors per month (more than 40,000) to the NIF web resources and the large number of repeat users (~35%) that visit the NIF discovery portal on a regular basis. NIF's services, standards and products are also heavily used as our web services are now regularly receiving more than 15 million hits per month. NIF has developed a novel sustainability plan that provides for continued enhancement and population of the resource. NIF has developed a reputation as a trusted community partner, allowing us to gain cooperation with large segments of the neuroscience community, as well as publishers, non-profit and government organizations. Through these networks, we've been able to launch major initiatives, help launch new collaborative efforts in basic and clinical neuroscience, and implement standards to help transform the way that we cite and track research resources. The work to be performed during the award period will be directed towards operating and maintaining the current NIF system while providing necessary strategic enhancements and providing broad outreach and dissemination efforts to encourage utilization of the NIF. A core new area of development will be centered around providing analytic tools to explore and identify lacunas of knowledge in Neuroscience. The initial work will focus on developing analytic heatmaps for activation foci from the neuroimaging literature.         PUBLIC HEALTH RELEVANCE: Modern biomedical science involves the accrual of increasingly larger data sets in many forms. The NIH Institutes that are a part of the Blueprint fo Neuroscience Research have invested heavily in the production of such data sets and research resources. This award will ensure that the Neuroscience Information Framework will continue to serve as the cost effective ""PubMed"" and ""PubMed Central"" for these assets (e.g., databases, software tools), making them collectively searchable and presenting them in a unified discovery environment for biomedical researchers and students.            ","OPERATION, SUPPORT AND STRATEGIC ENHANCEMENT OF THE NEUROSCIENCE INFORMATION FRAMEWORK",8936291,U24DA039832,"['Accounting', 'Area', 'Award', 'Awareness', 'Brain', 'Brain region', 'California', 'Cataloging', 'Catalogs', 'Characteristics', 'Clinical', 'Communities', 'Computer software', 'Contracts', 'Crowding', 'Data', 'Data Set', 'Databases', 'Development', 'Education and Outreach', 'Ensure', 'Environment', 'Europe', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Government', 'Growth', 'Human', 'Imagery', 'Informatics', 'Ingestion', 'Institutes', 'Internet', 'Knowledge', 'Life', 'Literature', 'Maintenance', 'Maps', 'Measures', 'Mission', 'Neurosciences', 'Neurosciences Research', 'Online Systems', 'Ontology', 'Paper', 'Participant', 'Phase', 'Pilot Projects', 'Population', 'Production', 'Provider', 'PubMed', 'Public Domains', 'Publishing', 'Questioning individuals', 'Registries', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Semantics', 'Series', 'Services', 'Slice', 'Software Tools', 'Source', 'Specific qualifier value', 'Students', 'System', 'Terminology', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'Universities', 'Update', 'Vertebral column', 'Visit', 'Work', 'base', 'biological systems', 'cognitive task', 'cost effective', 'data portal', 'digital', 'improved', 'indexing', 'information framework', 'innovation', 'meetings', 'neuroimaging', 'novel', 'operation', 'outreach', 'prospective', 'public health relevance', 'social', 'text searching', 'tool', 'web interface', 'web services', 'web site']",NIDA,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2015,913759,-0.049751720108207234
"OPERATION, SUPPORT AND STRATEGIC ENHANCEMENT OF THE NEUROSCIENCE INFORMATION FRAMEWORK ﻿    DESCRIPTION: The Neuroscience Information Framework (NIF; http://neuinfo.org) is currently managed, maintained, and hosted by researchers in the Center for Research in Biological Systems (CRBS) at the University of California, San Diego. Our group is the principal developer of the NIF system and has overseen its growth since 2008 from a modest catalog of 300 resources developed during the first phase of NIF, to the largest source of neuroscience resources on the web. As defined here, resources include data, databases, software/web-based tools, materials, literature, networks, terminologies, or information that would accelerate the pace of neuroscience research and discovery. NIF was instantiated because many of these valuable tools and services were largely unknown to the scientific community they were meant to serve. With the launch of major brain initiatives in the US and Europe, the amount of neuroscience data and tools will continue to increase.  NIF can be viewed as a cost effective ""PubMed"" and ""PubMed Central"" for digital assets, e.g., databases, software tools, alternative media, to make them collectively searchable and present a unified discovery environment for biomedical researchers. The NIF is heavily used, as measured by the number of visitors per month (more than 40,000) to the NIF web resources and the large number of repeat users (~35%) that visit the NIF discovery portal on a regular basis. NIF's services, standards and products are also heavily used as our web services are now regularly receiving more than 15 million hits per month. NIF has developed a novel sustainability plan that provides for continued enhancement and population of the resource. NIF has developed a reputation as a trusted community partner, allowing us to gain cooperation with large segments of the neuroscience community, as well as publishers, non-profit and government organizations. Through these networks, we've been able to launch major initiatives, help launch new collaborative efforts in basic and clinical neuroscience, and implement standards to help transform the way that we cite and track research resources. The work to be performed during the award period will be directed towards operating and maintaining the current NIF system while providing necessary strategic enhancements and providing broad outreach and dissemination efforts to encourage utilization of the NIF. A core new area of development will be centered around providing analytic tools to explore and identify lacunas of knowledge in Neuroscience. The initial work will focus on developing analytic heatmaps for activation foci from the neuroimaging literature. PUBLIC HEALTH RELEVANCE: Modern biomedical science involves the accrual of increasingly larger data sets in many forms. The NIH Institutes that are a part of the Blueprint fo Neuroscience Research have invested heavily in the production of such data sets and research resources. This award will ensure that the Neuroscience Information Framework will continue to serve as the cost effective ""PubMed"" and ""PubMed Central"" for these assets (e.g., databases, software tools), making them collectively searchable and presenting them in a unified discovery environment for biomedical researchers and students.","OPERATION, SUPPORT AND STRATEGIC ENHANCEMENT OF THE NEUROSCIENCE INFORMATION FRAMEWORK",9097153,U24DA039832,"['Accounting', 'Area', 'Award', 'Awareness', 'Brain', 'Brain region', 'California', 'Cataloging', 'Catalogs', 'Characteristics', 'Clinical', 'Communities', 'Computer software', 'Contracts', 'Crowding', 'Data', 'Data Set', 'Databases', 'Development', 'Education and Outreach', 'Ensure', 'Environment', 'Europe', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Government', 'Growth', 'Health', 'Human', 'Imagery', 'Informatics', 'Ingestion', 'Institutes', 'Internet', 'Knowledge', 'Life', 'Literature', 'Maintenance', 'Maps', 'Measures', 'Mission', 'Neurosciences', 'Neurosciences Research', 'Online Systems', 'Ontology', 'Paper', 'Participant', 'Phase', 'Pilot Projects', 'Population', 'Production', 'Provider', 'PubMed', 'Public Domains', 'Publishing', 'Questioning individuals', 'Registries', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Semantics', 'Series', 'Services', 'Slice', 'Software Tools', 'Source', 'Specific qualifier value', 'Students', 'System', 'Terminology', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'Universities', 'Update', 'Vertebral column', 'Visit', 'Work', 'base', 'biological systems', 'cognitive task', 'cost effective', 'data portal', 'digital', 'improved', 'indexing', 'information framework', 'innovation', 'meetings', 'neuroimaging', 'novel', 'operation', 'outreach', 'prospective', 'social', 'text searching', 'tool', 'web interface', 'web services', 'web site']",NIDA,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2015,230189,-0.049751720108207234
"iFACS: Imaging Florescence Activated Cell Sorter to sort cells based on images iFACS: Imaging Florescence Activated Cell Sorter to sort cells based on images NanoCellect Biomedical, Inc. RESEARCH & RELATED Other Project Information 7. PROJECT SUMMARY Advances in reagents (e.g. CRiSPR) and analytical tools (e.g. flow cytometers) have improved the ability to alter and characterize cellular phenotypes. Ultimately, many key applications in biomedicine require efficient and accurate isolation of cell populations according to features contained in high content images. Unfortunately, microscopic laser microdissection systems have a throughput that is too slow to be practical in many applications; while the existing flow cytometers that can sort cells (fluorescence-activated cell sorters or FACS) provide only size, internal complexity, and fluorescence intensity information and lack the rich data of imaging. Another critical limitation is that the existing flow cytometers that can image, cannot sort cells. NanoCellect has made a highly affordable FACS to increase access to this important high-throughput tool for cell analysis. Here we propose to enhance our existing low-cost FACS with the ability to image cells and sort them based on image features. This will allow users to pursue new strategies in drug screening and mechanism of action research; as well as work with suspension cell lines, such as those that dominate the recent advances in immuno-oncology. In Phase I research, we have demonstrated the world's first imaging flow cytometer with cell sorting capabilities (iFACS) in a unique design of space-time coding with an optical spatial filter. The approach adds negligible cost to the system for the desirable features of cell imaging and sorting. To fully realize the enormous potential of the design and to meet the demands for most applications, in Phase II we will develop high-throughput image-based cell sorting with innovative image-guided gating schemes supported by machine learning and interactive user/machine interface. Essentially, image-based flow cytometry gating uses similar cell isolation criteria as the techniques of laser capture microdissection or cell aspiration to isolate cells of interest, with 10,000X throughput improvements to 1000+ cells per second. We envision such unique capabilities will become common, default features for tomorrow's users as the tool becomes as intuitive and ubiquitous as fluorescent microscopy. The proposed iFACS will be transformative and benefit numerous biomedical applications, such as isolation of cells based on organelle translocation, cell cycle analyses, detection and counting of phagocytosed particles, and protein co-localization, to name just a few. iFACS: Imaging Florescence Activated Cell Sorter to sort cells based on images NanoCellect Biomedical, Inc. RESEARCH & RELATED Other Project Information 8. PROJECT NARRATIVE The advances proposed here will allow NanoCellect to achieve its mission: to advance image-based fluorescence-activated cell sorting technology that can be placed in an affordable bench-top device. This will be achieved by integrating cell-imaging and cell-sorting, and relevant software into an easy-to-use package that extends the features of the existing WOLF® Cell Sorter.",iFACS: Imaging Florescence Activated Cell Sorter to sort cells based on images,9572551,R44DA042636,"['Action Research', 'Adopted', 'Algorithmic Software', 'Back', 'Benchmarking', 'Cell Cycle', 'Cell Line', 'Cell Separation', 'Cell Size', 'Cells', 'Cellular biology', 'Chromatin', 'Code', 'Computer software', 'Data', 'Detection', 'Devices', 'Dexamethasone', 'Drug Screening', 'Flow Cytometry', 'Fluorescence', 'Fluorescence-Activated Cell Sorting', 'Frequencies', 'Glucocorticoid Receptor', 'Histones', 'Image', 'Image Analysis', 'Immunooncology', 'Individual', 'Intuition', 'Lasers', 'Liquid substance', 'Machine Learning', 'Measures', 'Membrane', 'Methodology', 'Microdissection', 'Microfluidics', 'Microscopic', 'Microscopy', 'Mission', 'Mitosis', 'Morphology', 'Motion', 'Names', 'Nuclear', 'Optics', 'Organelles', 'Patients', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Phorbol Esters', 'Photons', 'Physiological', 'Population', 'Proteins', 'Reagent', 'Reporter', 'Research', 'Sampling', 'Scheme', 'Sorting - Cell Movement', 'Structure', 'Suspensions', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tube', 'Validation', 'Work', 'analytical tool', 'base', 'cellular imaging', 'commercialization', 'cost', 'design', 'detector', 'drug mechanism', 'experience', 'experimental study', 'fluorescence activated cell sorter device', 'gene therapy', 'high throughput analysis', 'image guided', 'image processing', 'imaging capabilities', 'improved', 'innovation', 'interest', 'laser capture microdissection', 'optical imaging', 'particle', 'photomultiplier', 'prevent', 'protein kinase C gamma', 'prototype', 'response', 'screening', 'sensor', 'tool']",NIDA,"NANOCELLECT BIOMEDICAL, INC.",R44,2018,794381,0.0012970042904554124
"iFACS: Imaging Florescence Activated Cell Sorter to sort cells based on images iFACS: Imaging Florescence Activated Cell Sorter to sort cells based on images NanoCellect Biomedical, Inc. RESEARCH & RELATED Other Project Information 7. PROJECT SUMMARY Advances in reagents (e.g. CRiSPR) and analytical tools (e.g. flow cytometers) have improved the ability to alter and characterize cellular phenotypes. Ultimately, many key applications in biomedicine require efficient and accurate isolation of cell populations according to features contained in high content images. Unfortunately, microscopic laser microdissection systems have a throughput that is too slow to be practical in many applications; while the existing flow cytometers that can sort cells (fluorescence-activated cell sorters or FACS) provide only size, internal complexity, and fluorescence intensity information and lack the rich data of imaging. Another critical limitation is that the existing flow cytometers that can image, cannot sort cells. NanoCellect has made a highly affordable FACS to increase access to this important high-throughput tool for cell analysis. Here we propose to enhance our existing low-cost FACS with the ability to image cells and sort them based on image features. This will allow users to pursue new strategies in drug screening and mechanism of action research; as well as work with suspension cell lines, such as those that dominate the recent advances in immuno-oncology. In Phase I research, we have demonstrated the world's first imaging flow cytometer with cell sorting capabilities (iFACS) in a unique design of space-time coding with an optical spatial filter. The approach adds negligible cost to the system for the desirable features of cell imaging and sorting. To fully realize the enormous potential of the design and to meet the demands for most applications, in Phase II we will develop high-throughput image-based cell sorting with innovative image-guided gating schemes supported by machine learning and interactive user/machine interface. Essentially, image-based flow cytometry gating uses similar cell isolation criteria as the techniques of laser capture microdissection or cell aspiration to isolate cells of interest, with 10,000X throughput improvements to 1000+ cells per second. We envision such unique capabilities will become common, default features for tomorrow's users as the tool becomes as intuitive and ubiquitous as fluorescent microscopy. The proposed iFACS will be transformative and benefit numerous biomedical applications, such as isolation of cells based on organelle translocation, cell cycle analyses, detection and counting of phagocytosed particles, and protein co-localization, to name just a few. iFACS: Imaging Florescence Activated Cell Sorter to sort cells based on images NanoCellect Biomedical, Inc. RESEARCH & RELATED Other Project Information 8. PROJECT NARRATIVE The advances proposed here will allow NanoCellect to achieve its mission: to advance image-based fluorescence-activated cell sorting technology that can be placed in an affordable bench-top device. This will be achieved by integrating cell-imaging and cell-sorting, and relevant software into an easy-to-use package that extends the features of the existing WOLF® Cell Sorter.",iFACS: Imaging Florescence Activated Cell Sorter to sort cells based on images,9409931,R44DA042636,"['Action Research', 'Adopted', 'Algorithmic Software', 'Back', 'Benchmarking', 'Cell Cycle', 'Cell Line', 'Cell Separation', 'Cell Size', 'Cells', 'Cellular biology', 'Chromatin', 'Code', 'Computer software', 'Data', 'Detection', 'Devices', 'Dexamethasone', 'Flow Cytometry', 'Fluorescence', 'Fluorescence-Activated Cell Sorting', 'Frequencies', 'Glucocorticoid Receptor', 'Histones', 'Image', 'Image Analysis', 'Immunooncology', 'Individual', 'Intuition', 'Lasers', 'Liquid substance', 'Machine Learning', 'Measures', 'Membrane', 'Methodology', 'Microdissection', 'Microfluidics', 'Microscopic', 'Microscopy', 'Mission', 'Mitosis', 'Morphology', 'Motion', 'Names', 'Nuclear', 'Optics', 'Organelles', 'Patients', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Phorbol Esters', 'Photons', 'Physiological', 'Population', 'Preclinical Drug Evaluation', 'Proteins', 'Reagent', 'Reporter', 'Research', 'Sampling', 'Scheme', 'Sorting - Cell Movement', 'Structure', 'Suspensions', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tube', 'Validation', 'Work', 'analytical tool', 'base', 'cellular imaging', 'commercialization', 'cost', 'design', 'detector', 'drug mechanism', 'experience', 'experimental study', 'fluorescence activated cell sorter device', 'gene therapy', 'high throughput analysis', 'image guided', 'image processing', 'imaging capabilities', 'improved', 'innovation', 'interest', 'laser capture microdissection', 'optical imaging', 'particle', 'photomultiplier', 'prevent', 'protein kinase C gamma', 'prototype', 'response', 'screening', 'sensor', 'tool']",NIDA,"NANOCELLECT BIOMEDICAL, INC.",R44,2017,705531,0.0012970042904554124
"ULTRASONIC ASSESSMENT OF BONE USING NEURAL NETWORKS The long-term objective of this research is to establish ultrasound as a safe, effective, and non-invasive method for assessing fracture risk, an important component in clinical management of osteoporosis. Osteoporosis afflicts over 20 million people in the U.S., responsible for more than 275,000 hip fractures annually.  Currently, the primary means for assessment relies on densitometric techniques.  These methods subject the patient to ionizing radiation, are relatively expensive, and do not always provide good estimates of bone strength.  Ultrasound offers several potential advantages.  It is non-ionizing and relatively inexpensive.  Moreover, since ultrasound is a mechanical wave and interacts with bone in a fundamentally different manner than electromagnetic radiation, it may be able to provide more accurate estimates of bone strength compared with current densitometric methods. The goal of this research is to significantly improve the effectiveness of current ultrasonic bone assessment techniques by demonstrating the feasibility of a powerful new technology, i.e., neural networks, which can be used in conjunction with ultrasonic measurements, to accurately determine bone density and strength.  The specific aims are to measure ultrasonic velocity and attenuation in bovine trabecular bone and use neural networks to determine its bone mineral density and ultimate strength.  n/a",ULTRASONIC ASSESSMENT OF BONE USING NEURAL NETWORKS,2082640,R43AR043045,"['animal tissue', ' artificial intelligence', ' bone density', ' bone fracture', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' metabolism disorder diagnosis', ' musculoskeletal disorder diagnosis', ' osteoporosis', ' photon absorptiometry', ' ultrasound']",NIAMS,"CYBERLOGIC, INC.",R43,1994,75000,-0.00689871804766488
"CONTEXT INFORMATION IN AUTOMATED CERVICAL SMEAR ANALYSIS The long-term objective of this research is to develop and test automated cell image analysis for the improved diagnosis and prognosis of cervical cancer. our previous research has shown that contextual analysis (low resolution analysis of scenes) and marker analysis (morphometric changes in ""normal"" looking cells) adds significant information to high resolution individual cell analysis of breast, prostate, and cervical neoplasia.  The previous studies were performed in several steps, which was inefficient and only permitted us to collect small data sets.  The primary objective of the proposed research is to develop an Integrated Analysis System (IAS) that will employ more sophisticated methods to combine the single cell, contextual, and marker analyses.  The first aim of this research is to develop a system to permit simultaneous contextual, marker, and individual cell analyses of monolayer preparations of exfoliated cervical cells.  This will include the use of adaptive methodologies that incorporate a priori information to guide the analysis process.  The second aim is to test the combined methodolology to determine whether it provides an improvement over any single analysis method. The third aim is to apply these technique to investigate the feasibility of discriminating persistent/progressive from regressive and negative cases.  This research should have significant impact on cost-containment and availability of cervical cytology for pre-screening of cervical cancer Automated cytology can partly offset the growing shortage of qualified cytotechnologists.  It can also provide more objective criteria for diagnosis and prognosis of cervical neoplasia at all levels of experience and training.  The same methodology can eventually be applied to expert systems for diagnosis and teaching.  Moreover, advances made in this research should be readily transferable to other types of human cancer. Thus, we expect that this research will have far-reaching effects on practical as well as scientific aspects of cancer prevention and control.  n/a",CONTEXT INFORMATION IN AUTOMATED CERVICAL SMEAR ANALYSIS,3185093,R01CA043133,"['DVD /CD ROM', ' artificial intelligence', ' biomarker', ' biomedical automation', ' cancer risk', ' cervical /vaginal smear', ' cervix neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' cytodiagnosis', ' cytology', ' data collection', ' diagnosis design /evaluation', ' digital imaging', ' female', ' human subject', ' image processing', ' mass screening', ' morphology', ' neoplasm /cancer classification /staging', ' neoplasm /cancer diagnosis', ' neoplastic transformation', ' prognosis', ' single cell analysis']",NCI,TUFTS MEDICAL CENTER,R01,1993,175145,-0.07801818629970143
"CONTEXT INFORMATION IN AUTOMATED CERVICAL SMEAR ANALYSIS The long-term objective of this research is to develop and test automated cell image analysis for the improved diagnosis and prognosis of cervical cancer. our previous research has shown that contextual analysis (low resolution analysis of scenes) and marker analysis (morphometric changes in ""normal"" looking cells) adds significant information to high resolution individual cell analysis of breast, prostate, and cervical neoplasia.  The previous studies were performed in several steps, which was inefficient and only permitted us to collect small data sets.  The primary objective of the proposed research is to develop an Integrated Analysis System (IAS) that will employ more sophisticated methods to combine the single cell, contextual, and marker analyses.  The first aim of this research is to develop a system to permit simultaneous contextual, marker, and individual cell analyses of monolayer preparations of exfoliated cervical cells.  This will include the use of adaptive methodologies that incorporate a priori information to guide the analysis process.  The second aim is to test the combined methodolology to determine whether it provides an improvement over any single analysis method. The third aim is to apply these technique to investigate the feasibility of discriminating persistent/progressive from regressive and negative cases.  This research should have significant impact on cost-containment and availability of cervical cytology for pre-screening of cervical cancer Automated cytology can partly offset the growing shortage of qualified cytotechnologists.  It can also provide more objective criteria for diagnosis and prognosis of cervical neoplasia at all levels of experience and training.  The same methodology can eventually be applied to expert systems for diagnosis and teaching.  Moreover, advances made in this research should be readily transferable to other types of human cancer. Thus, we expect that this research will have far-reaching effects on practical as well as scientific aspects of cancer prevention and control.  n/a",CONTEXT INFORMATION IN AUTOMATED CERVICAL SMEAR ANALYSIS,3185090,R01CA043133,"['DVD /CD ROM', ' artificial intelligence', ' biomarker', ' biomedical automation', ' cancer risk', ' cervical /vaginal smear', ' cervix neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' cytodiagnosis', ' cytology', ' data collection', ' diagnosis design /evaluation', ' digital imaging', ' female', ' human subject', ' image processing', ' mass screening', ' morphology', ' neoplasm /cancer classification /staging', ' neoplasm /cancer diagnosis', ' neoplastic transformation', ' prognosis', ' single cell analysis']",NCI,TUFTS MEDICAL CENTER,R01,1992,182944,-0.07801818629970143
"ADAPTIVE ULTRASONIC IMAGING DESCRIPTION (Adapted from Applicant's Abstract): The applicants proposed         to  develop adaptive imaging algorithms and instrumentation to compensate        for  tissue-induced ultrasonic image degradation.  Theoretical and               simulation  studies are proposed to optimize the accuracy, stability, and        speed of  adaptive algorithms and to explore the impact of transducer            design on adaptive  imaging.  In addition, the applicants proposed to            collect high-quality tissue  echo data and through-transmission data to          investigate the nature of tissue- induced image degradation.  The                adaptive imaging techniques would be implemented in real-time on an              advanced engineering prototype scanner.  Synthetic receive aperture (SRA)        techniques, combined with adaptive imaging,  would be used to address            1000 and 2000 element two dimensional (2-D) arrays  and to form very high        resolution images.  Specialized analog multiplexers and  other hardware          would be constructed for this system.  Clinical trials would  evaluate           the performance of the adaptive/SRA system in imaging breast lesions  and        breast microcalcifications, and in renal and adrenal gland imaging               studies.  The applicants hypothesized that the proposed techniques and           system  would markedly improve ultrasonic image quality in a wide variety        of clinical  applications.                                                        n/a",ADAPTIVE ULTRASONIC IMAGING,2837619,R01CA043334,"['adrenal disorder', ' artificial intelligence', ' bioimaging /biomedical imaging', ' calcification', ' computer simulation', ' computer system design /evaluation', ' endocrine disorder diagnosis', ' human subject', ' image processing', ' kidney disorder diagnosis', ' mammary disorder', ' nephrolithiasis', ' reproductive system disorder diagnosis', ' ultrasonography']",NCI,DUKE UNIVERSITY,R01,1999,235325,-0.06894978771582888
"ADAPTIVE ULTRASONIC IMAGING DESCRIPTION (Adapted from Applicant's Abstract): The applicants proposed         to  develop adaptive imaging algorithms and instrumentation to compensate        for  tissue-induced ultrasonic image degradation.  Theoretical and               simulation  studies are proposed to optimize the accuracy, stability, and        speed of  adaptive algorithms and to explore the impact of transducer            design on adaptive  imaging.  In addition, the applicants proposed to            collect high-quality tissue  echo data and through-transmission data to          investigate the nature of tissue- induced image degradation.  The                adaptive imaging techniques would be implemented in real-time on an              advanced engineering prototype scanner.  Synthetic receive aperture (SRA)        techniques, combined with adaptive imaging,  would be used to address            1000 and 2000 element two dimensional (2-D) arrays  and to form very high        resolution images.  Specialized analog multiplexers and  other hardware          would be constructed for this system.  Clinical trials would  evaluate           the performance of the adaptive/SRA system in imaging breast lesions  and        breast microcalcifications, and in renal and adrenal gland imaging               studies.  The applicants hypothesized that the proposed techniques and           system  would markedly improve ultrasonic image quality in a wide variety        of clinical  applications.                                                        n/a",ADAPTIVE ULTRASONIC IMAGING,2608036,R01CA043334,"['adrenal disorder', ' artificial intelligence', ' calcification', ' computer simulation', ' computer system design /evaluation', ' endocrine disorder diagnosis', ' human subject', ' image processing', ' kidney disorder diagnosis', ' mammary disorder', ' nephrolithiasis', ' reproductive system disorder diagnosis', ' ultrasonography']",NCI,DUKE UNIVERSITY,R01,1998,239717,-0.06894978771582888
"ADAPTIVE ULTRASONIC IMAGING DESCRIPTION (Adapted from Applicant's Abstract): The applicants proposed         to  develop adaptive imaging algorithms and instrumentation to compensate        for  tissue-induced ultrasonic image degradation.  Theoretical and               simulation  studies are proposed to optimize the accuracy, stability, and        speed of  adaptive algorithms and to explore the impact of transducer            design on adaptive  imaging.  In addition, the applicants proposed to            collect high-quality tissue  echo data and through-transmission data to          investigate the nature of tissue- induced image degradation.  The                adaptive imaging techniques would be implemented in real-time on an              advanced engineering prototype scanner.  Synthetic receive aperture (SRA)        techniques, combined with adaptive imaging,  would be used to address            1000 and 2000 element two dimensional (2-D) arrays  and to form very high        resolution images.  Specialized analog multiplexers and  other hardware          would be constructed for this system.  Clinical trials would  evaluate           the performance of the adaptive/SRA system in imaging breast lesions  and        breast microcalcifications, and in renal and adrenal gland imaging               studies.  The applicants hypothesized that the proposed techniques and           system  would markedly improve ultrasonic image quality in a wide variety        of clinical  applications.                                                        n/a",ADAPTIVE ULTRASONIC IMAGING,2007600,R01CA043334,"['adrenal disorder', ' artificial intelligence', ' calcification', ' computer simulation', ' computer system design /evaluation', ' endocrine disorder diagnosis', ' human subject', ' image processing', ' kidney disorder diagnosis', ' mammary disorder', ' nephrolithiasis', ' reproductive system disorder diagnosis', ' ultrasonography']",NCI,DUKE UNIVERSITY,R01,1997,225308,-0.06894978771582888
"ADAPTIVE ULTRASONIC IMAGING DESCRIPTION (Adapted from Applicant's Abstract): The applicants proposed  to  develop adaptive imaging algorithms and instrumentation to compensate  for  tissue-induced ultrasonic image degradation.  Theoretical and  simulation  studies are proposed to optimize the accuracy, stability, and  speed of  adaptive algorithms and to explore the impact of transducer  design on adaptive  imaging.  In addition, the applicants proposed to  collect high-quality tissue  echo data and through-transmission data to  investigate the nature of tissue- induced image degradation.  The  adaptive imaging techniques would be implemented in real-time on an  advanced engineering prototype scanner.  Synthetic receive aperture (SRA)  techniques, combined with adaptive imaging,  would be used to address  1000 and 2000 element two dimensional (2-D) arrays  and to form very high  resolution images.  Specialized analog multiplexers and  other hardware  would be constructed for this system.  Clinical trials would  evaluate  the performance of the adaptive/SRA system in imaging breast lesions  and  breast microcalcifications, and in renal and adrenal gland imaging  studies.  The applicants hypothesized that the proposed techniques and  system  would markedly improve ultrasonic image quality in a wide variety  of clinical  applications.  n/a",ADAPTIVE ULTRASONIC IMAGING,2091175,R01CA043334,"['adrenal disorder', ' artificial intelligence', ' calcification', ' computer simulation', ' computer system design /evaluation', ' endocrine disorder diagnosis', ' human subject', ' image processing', ' kidney disorder diagnosis', ' mammary disorder', ' nephrolithiasis', ' reproductive system disorder diagnosis', ' ultrasonography']",NCI,DUKE UNIVERSITY,R01,1995,206746,-0.06894978771582888
"SLIT-SCAN LIGHT SCATTER The major goal of this grant is the understanding of slit-scan light scatter signals and the information they provide.  This will be accomplished through the following tasks:  (1) The completion of the addition of low angle forward and large angle light scatter sensors to two multidimensional slit-scan flow systems.  (2) The reassembly of a Correlation System to provide two orthogonal fluorescence images of cells in flow together with fluorescence and light scatter slit-scan contours.  (3) The collection and correlation of slit-scan light scatter and fluorescence contours. Because the two contours are perfectly correlated, this will permit identification in the slit-scan light scatter contour of the exact point at which the cytoplasm and nuclear boundaries of the cell intersect the slit of laser illumination.  Specific cell types will be analyzed to aid in the understanding of the information available from the slit-scan light scatter contours.  Of particular interest are nuclear and cell boundaries together with information on intracellular structure.  (4) The investigation of detector geometry to optimize morphological boundary, clump, internal structure, and orientation information from slit-scan light scatter contours on cells in flow.  (5) The collection and correlation of slit-scan light scatter contours with slit-scan fluorescence contours and two orthogonal fluorescence images of cells in flow. This data will aid in the study and understanding of the effects of cell orientation in flow on slit-scan light scatter signals and the use of slit-scan light scatter signals to provide information on cell orientation in flow, internal structure, multinucleation, and clumping of cells, and the identification of low resolution morphological features.  (6) Increasing the resolution on one system to two microns to permit the study and understanding of slit-scan light scatter information on other cell types of interest. (7) Parallel theoretical studies and guidance to aid in hardware design, study direction, and the understanding of slit-scan light scatter information.  This study is new and unique and will provide information relevant to cancer detection utilizing multidimensional slit-scan instrumentation, as well as be useful in conventional flow cytometry.  n/a",SLIT-SCAN LIGHT SCATTER,3188317,R01CA045246,"['DVD /CD ROM', ' acridines', ' artificial intelligence', ' biomedical equipment development', ' cell components', ' cell type', ' clinical biomedical equipment', ' computer system design /evaluation', ' computers', ' cytodiagnosis', ' flow cytometry', ' fluorescence', ' image enhancement', ' image processing', ' lasers', ' light scattering', ' model design /development', ' morphology', ' neoplasm /cancer diagnosis', ' orientation', ' physical model', ' single cell analysis', ' stainings']",NCI,UNIVERSITY OF ROCHESTER,R01,1990,191998,0.02606545941393904
"SLIT-SCAN LIGHT SCATTER The major goal of this grant is the understanding of slit-scan light scatter signals and the information they provide.  This will be accomplished through the following tasks:  (1) The completion of the addition of low angle forward and large angle light scatter sensors to two multidimensional slit-scan flow systems.  (2) The reassembly of a Correlation System to provide two orthogonal fluorescence images of cells in flow together with fluorescence and light scatter slit-scan contours.  (3) The collection and correlation of slit-scan light scatter and fluorescence contours. Because the two contours are perfectly correlated, this will permit identification in the slit-scan light scatter contour of the exact point at which the cytoplasm and nuclear boundaries of the cell intersect the slit of laser illumination.  Specific cell types will be analyzed to aid in the understanding of the information available from the slit-scan light scatter contours.  Of particular interest are nuclear and cell boundaries together with information on intracellular structure.  (4) The investigation of detector geometry to optimize morphological boundary, clump, internal structure, and orientation information from slit-scan light scatter contours on cells in flow.  (5) The collection and correlation of slit-scan light scatter contours with slit-scan fluorescence contours and two orthogonal fluorescence images of cells in flow. This data will aid in the study and understanding of the effects of cell orientation in flow on slit-scan light scatter signals and the use of slit-scan light scatter signals to provide information on cell orientation in flow, internal structure, multinucleation, and clumping of cells, and the identification of low resolution morphological features.  (6) Increasing the resolution on one system to two microns to permit the study and understanding of slit-scan light scatter information on other cell types of interest. (7) Parallel theoretical studies and guidance to aid in hardware design, study direction, and the understanding of slit-scan light scatter information.  This study is new and unique and will provide information relevant to cancer detection utilizing multidimensional slit-scan instrumentation, as well as be useful in conventional flow cytometry.  n/a",SLIT-SCAN LIGHT SCATTER,3188316,R01CA045246,"['DVD /CD ROM', ' acridines', ' artificial intelligence', ' biomedical equipment development', ' cell components', ' cell type', ' clinical biomedical equipment', ' computer system design /evaluation', ' computers', ' cytodiagnosis', ' flow cytometry', ' fluorescence', ' image enhancement', ' image processing', ' lasers', ' light scattering', ' model design /development', ' morphology', ' neoplasm /cancer diagnosis', ' orientation', ' physical model', ' single cell analysis', ' stainings']",NCI,UNIVERSITY OF ROCHESTER,R01,1989,194868,0.02606545941393904
"SLIT-SCAN LIGHT SCATTER The major goal of this grant is the understanding of slit-scan light scatter signals and the information they provide.  This will be accomplished through the following tasks:  (1) The completion of the addition of low angle forward and large angle light scatter sensors to two multidimensional slit-scan flow systems.  (2) The reassembly of a Correlation System to provide two orthogonal fluorescence images of cells in flow together with fluorescence and light scatter slit-scan contours.  (3) The collection and correlation of slit-scan light scatter and fluorescence contours. Because the two contours are perfectly correlated, this will permit identification in the slit-scan light scatter contour of the exact point at which the cytoplasm and nuclear boundaries of the cell intersect the slit of laser illumination.  Specific cell types will be analyzed to aid in the understanding of the information available from the slit-scan light scatter contours.  Of particular interest are nuclear and cell boundaries together with information on intracellular structure.  (4) The investigation of detector geometry to optimize morphological boundary, clump, internal structure, and orientation information from slit-scan light scatter contours on cells in flow.  (5) The collection and correlation of slit-scan light scatter contours with slit-scan fluorescence contours and two orthogonal fluorescence images of cells in flow. This data will aid in the study and understanding of the effects of cell orientation in flow on slit-scan light scatter signals and the use of slit-scan light scatter signals to provide information on cell orientation in flow, internal structure, multinucleation, and clumping of cells, and the identification of low resolution morphological features.  (6) Increasing the resolution on one system to two microns to permit the study and understanding of slit-scan light scatter information on other cell types of interest. (7) Parallel theoretical studies and guidance to aid in hardware design, study direction, and the understanding of slit-scan light scatter information.  This study is new and unique and will provide information relevant to cancer detection utilizing multidimensional slit-scan instrumentation, as well as be useful in conventional flow cytometry.  n/a",SLIT-SCAN LIGHT SCATTER,3188315,R01CA045246,"['DVD /CD ROM', ' acridines', ' artificial intelligence', ' biomedical equipment development', ' cell components', ' cell type', ' clinical biomedical equipment', ' computer system design /evaluation', ' computers', ' cytodiagnosis', ' flow cytometry', ' fluorescence', ' image enhancement', ' image processing', ' lasers', ' light scattering', ' model design /development', ' morphology', ' neoplasm /cancer diagnosis', ' orientation', ' physical model', ' single cell analysis', ' stainings']",NCI,UNIVERSITY OF ROCHESTER,R01,1987,296414,0.02606545941393904
"CCD DETECTOR DEVELOPMENT FOR DIGITAL X-RAY MAMMOGRAPHY A high resolution scanned projection radiography system is proposed for digital mammorgraphic applications.  By using a wide dynamic range CCD detector some of the current limitations encountered in conventional film-screen mammography can be avoided.  One-dimensional fan-beam geometry provides very good scattered x-ray reduction.  A time-delay-and-integrate (TDI) mode of CCD imaging with a 2048 x 96 element CCD array provides the opportunity to image without the excessive power loading of x-ray tubes which would be required with a 2048 x 1 array, for example.  By utilizing TDI charge integration while the detector is scanned, each projection through stationary anatomy is averaged over 96 exposures, thereby providing a significant signal to quantum mottle improvement of approximately 10:1 as compared to a 2048 x 1 detector at the equivalent tube loading. Developmental tasks to be performed include construction of a TDI CCD x-ray detector in which the light output of an x-ray intensifying screen is coupled by means of a lens to the CCD focal plane, measurement of the x-ray sensitivity, noise characteristics and homogeneity of the x-ray detector construction of a moving sample x-ray imaging system based on the TDI detector, and determination of the optimum x-ray spectrum for digital mammorgraphic imaging tasks in order to evaluate its clinical potential.  Additionally, image processing tasks to aid in subtle lesion or microcalcivication detection will be surveyed in order that further work toward an artifical intelligence based expert system for breast imaging may result in continuing development phases.  n/a",CCD DETECTOR DEVELOPMENT FOR DIGITAL X-RAY MAMMOGRAPHY,3491801,R43CA045856,"['biomedical equipment resource', ' clinical biomedical equipment', ' mammography']",NCI,FISCHER IMAGING CORPORATION,R43,1987,50000,-0.0498050770943747
"PROGNOSIS AND VENTRICULAR CURVATURE IN TIMI TRIAL The principle objective of the proposed research is to determine whether the diagnosis of regional dysfunction indicative of myocardial infarction, and the assessment of prognosis can be more accurately performed from analysis of regional left ventricular curvature than from analysis of left ventricular wall motion.  Data accumulated during the NIH sponsored trial of Thrombolysis In Myocardial Infarction (TIMI) will be analyzed.  An advanced computational technique will be used to correlate curvature measurements with diagnosis and with prognosis: the artificial neural network, which applies the experience gained from processing patient data to form associations between the input data (ventricular curvature) and the desired output (infarct diagnosis, prognosis).  The accuracy with which the neural network assisted evaluation of regional left ventricular curvature can distinguish infarct (TIMI) patients from patients with normal coronary arteries will be compared with the diagnostic accuracy of traditional wall motion analysis and of neural network assisted analysis of wall motion.  In addition, the ability of the neural network to predict survival from analysis of function will be compared with the accuracy of predicting survival using Cox regression analysis.  The results of the study will help to evaluate patients with angina and nondiagnostic electrocardiograms, by improving the accuracy with which regional dysfunction can be identified.  The results of the study may also help identify patients at high risk who may benefit from more aggressive therapy.  It is envisioned that artificial neural network assisted analysis of regional ventricular curvature will ultimately be applied to two-dimensional echocardiography, to which curvature analysis is theoretically more suitable than wall motion, thus enabling noninvasive diagnosis and prognostication.  n/a",PROGNOSIS AND VENTRICULAR CURVATURE IN TIMI TRIAL,3426782,R03HL046144,"['angiography', ' artificial intelligence', ' brain ventriculography', ' computational neuroscience', ' computer assisted diagnosis', ' diagnosis design /evaluation', ' fibrinolysis', ' heart dimension /size', ' heart function', ' heart motion', ' heart ventricle', ' myocardial infarct sizing', ' myocardial infarction', ' noninvasive diagnosis', ' prognosis']",NHLBI,UNIVERSITY OF WASHINGTON,R03,1992,75493,-0.011100210924202113
"PROGNOSIS AND VENTRICULAR CURVATURE IN TIMI TRIAL The principle objective of the proposed research is to determine whether the diagnosis of regional dysfunction indicative of myocardial infarction, and the assessment of prognosis can be more accurately performed from analysis of regional left ventricular curvature than from analysis of left ventricular wall motion.  Data accumulated during the NIH sponsored trial of Thrombolysis In Myocardial Infarction (TIMI) will be analyzed.  An advanced computational technique will be used to correlate curvature measurements with diagnosis and with prognosis: the artificial neural network, which applies the experience gained from processing patient data to form associations between the input data (ventricular curvature) and the desired output (infarct diagnosis, prognosis).  The accuracy with which the neural network assisted evaluation of regional left ventricular curvature can distinguish infarct (TIMI) patients from patients with normal coronary arteries will be compared with the diagnostic accuracy of traditional wall motion analysis and of neural network assisted analysis of wall motion.  In addition, the ability of the neural network to predict survival from analysis of function will be compared with the accuracy of predicting survival using Cox regression analysis.  The results of the study will help to evaluate patients with angina and nondiagnostic electrocardiograms, by improving the accuracy with which regional dysfunction can be identified.  The results of the study may also help identify patients at high risk who may benefit from more aggressive therapy.  It is envisioned that artificial neural network assisted analysis of regional ventricular curvature will ultimately be applied to two-dimensional echocardiography, to which curvature analysis is theoretically more suitable than wall motion, thus enabling noninvasive diagnosis and prognostication.  n/a",PROGNOSIS AND VENTRICULAR CURVATURE IN TIMI TRIAL,3426781,R03HL046144,"['angiography', ' artificial intelligence', ' brain ventriculography', ' computational neuroscience', ' computer assisted diagnosis', ' diagnosis design /evaluation', ' fibrinolysis', ' heart dimension /size', ' heart function', ' heart motion', ' heart ventricle', ' myocardial infarct sizing', ' myocardial infarction', ' noninvasive diagnosis', ' prognosis']",NHLBI,UNIVERSITY OF WASHINGTON,R03,1991,75493,-0.011100210924202113
"CONFORMAL RADIATION THERAPY TREATMENT PLAN OPTIMIZATION The aim of this project is to study computer methods for designing conformal radiation therapy treatments for cancer.  Automated delivery of radiation therapy permits the use of new conformal treatment strategies which uniquely meet the needs of individual patients but are too complex and sophisticated for manual delivery.  Computer control removes limitations on numbers of beams, beam directions, and beam shapes, so that conventional methods of treatment planning using experience-guided trial- and-error search are totally inadequate.  Efficient generation of clinically optimal computer-controlled treatment plans will require mathematical optimization techniques guided by an expert system advisor.  Clinically realistic treatment objectives must include normal tissue complication probabilities and normal tissue dose-volume constraint limitations, both of which are non-linear non-analytic functions. Simulated annealing and downhill simplex and general mathematical methods for constrained optimization of such functions. With proper customizing, both can be applied to the creation of computer-controlled treatment plans. Different implementation methods, parameter values, and clinical objective/constraint functions will be studied for both methods using a variety of patient test cases.  A variety of different computer-controlled delivery strategies are possible depending on the capabilities of the treatment machine.  Dynamic rotation, conformal static beam, and non-coplanar beam techniques, with and without wedges, are practical with an automated multileaf collimator.  Segmented conformal therapy and ""field-in-field"" techniques are deliverable using either an automated multileaf collimator or four independently-movable collimator jaws.  The relative advantages of these different delivery techniques will be compared using mathematically optimized plans.  n/a",CONFORMAL RADIATION THERAPY TREATMENT PLAN OPTIMIZATION,3189988,R01CA046634,"['computer assisted patient care', ' computer program /software', ' ionizing radiation', ' mathematical model', ' neoplasm /cancer radiation therapy', ' radiation dosage']",NCI,UNIVERSITY OF TEXAS MEDICAL BR GALVESTON,R01,1993,196198,0.010639627828654682
"CONFORMAL RADIATION THERAPY TREATMENT PLAN OPTIMIZATION The aim of this project is to study computer methods for designing conformal radiation therapy treatments for cancer.  Automated delivery of radiation therapy permits the use of new conformal treatment strategies which uniquely meet the needs of individual patients but are too complex and sophisticated for manual delivery.  Computer control removes limitations on numbers of beams, beam directions, and beam shapes, so that conventional methods of treatment planning using experience-guided trial- and-error search are totally inadequate.  Efficient generation of clinically optimal computer-controlled treatment plans will require mathematical optimization techniques guided by an expert system advisor.  Clinically realistic treatment objectives must include normal tissue complication probabilities and normal tissue dose-volume constraint limitations, both of which are non-linear non-analytic functions. Simulated annealing and downhill simplex and general mathematical methods for constrained optimization of such functions. With proper customizing, both can be applied to the creation of computer-controlled treatment plans. Different implementation methods, parameter values, and clinical objective/constraint functions will be studied for both methods using a variety of patient test cases.  A variety of different computer-controlled delivery strategies are possible depending on the capabilities of the treatment machine.  Dynamic rotation, conformal static beam, and non-coplanar beam techniques, with and without wedges, are practical with an automated multileaf collimator.  Segmented conformal therapy and ""field-in-field"" techniques are deliverable using either an automated multileaf collimator or four independently-movable collimator jaws.  The relative advantages of these different delivery techniques will be compared using mathematically optimized plans.  n/a",CONFORMAL RADIATION THERAPY TREATMENT PLAN OPTIMIZATION,3189985,R01CA046634,"['computer assisted patient care', ' ionizing radiation', ' mathematical model', ' neoplasm /cancer radiation therapy', ' radiation dosage']",NCI,UNIVERSITY OF TEXAS MEDICAL BR GALVESTON,R01,1992,208058,0.010639627828654682
"Linking Language Comprehension to Production Patterns    DESCRIPTION (provided by applicant): This project tests specific hypotheses about dependencies between language comprehension and production, which are typically studied independently. The PI's production-distribution-comprehension (PDC) account holds that utterance planning choices during language production yield distributional patterns in the language, in which certain syntactic structures co-vary with particular word choices, messages, and discourse environments. Comprehenders, through statistical learning during prior comprehension experiences, become highly sensitive to these patterns, and this sensitivity guides comprehension processes. Thus, many aspects of comprehension can ultimately be traced to task demands related to language production. Specific aims of the project include: (1) Link syntactic structure choice in production to mechanisms of sentence planning. (2) Compare the PDC account of comprehension to alternative views of relative clause interpretation. (3) Test the causal relations between production constraints, distributional patterns in the language, and comprehension performance. (4) Relate adult sentence comprehension to statistical learning. (5) Test the current limits of constraint-based models of language comprehension. The PDC approach offers a significant alternative to other views and also may inform language acquisition research by illuminating the role of distributional patterns in child language acquisition. The work also can inform language therapies for brain injured patients in several ways. First, sources of production difficulty are precisely investigated, as are accommodations that unimpaired speakers make in the face of this difficulty. Second, the project investigates the relationship between prior experience with a syntactic construction and comprehension difficulty, which can have implications for the amount and nature of practice that should be provided to patients to improve their comprehension of certain sentence types.         n/a",Linking Language Comprehension to Production Patterns,7426941,R01HD047425,"['Accounting', 'Address', 'Adult', 'Affect', 'Back', 'Brain', 'Brain Injuries', 'Characteristics', 'Child Language', 'Collaborations', 'Comprehension', 'Data', 'Dependence', 'Dependency', 'Depth', 'Disease', 'Environment', 'Face', 'Facility Construction Funding Category', 'Foxes', 'Human Characteristics', 'Individual', 'Infant', 'Investigation', 'Knowledge', 'Language', 'Language Development', 'Language Therapy', 'Learning', 'Linguistics', 'Link', 'Longevity', 'Machine Learning', 'Maintenance', 'Measures', 'Methods', 'Modeling', 'Nature', 'Patients', 'Pattern', 'Performance', 'Play', 'Population', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Reading', 'Relative (related person)', 'Research', 'Research Personnel', 'Role', 'Short-Term Memory', 'Source', 'Structure', 'Testing', 'Time', 'Training', 'Trees', 'Work', 'base', 'experience', 'feeding', 'improved', 'infancy', 'injured', 'insight', 'neuropathology', 'novel', 'preference', 'research study', 'statistics', 'stem', 'syntax', 'theories']",NICHD,UNIVERSITY OF WISCONSIN-MADISON,R01,2008,164747,-0.04436150924857582
"Linking Language Comprehension to Production Patterns    DESCRIPTION (provided by applicant): This project tests specific hypotheses about dependencies between language comprehension and production, which are typically studied independently. The PI's production-distribution-comprehension (PDC) account holds that utterance planning choices during language production yield distributional patterns in the language, in which certain syntactic structures co-vary with particular word choices, messages, and discourse environments. Comprehenders, through statistical learning during prior comprehension experiences, become highly sensitive to these patterns, and this sensitivity guides comprehension processes. Thus, many aspects of comprehension can ultimately be traced to task demands related to language production. Specific aims of the project include: (1) Link syntactic structure choice in production to mechanisms of sentence planning. (2) Compare the PDC account of comprehension to alternative views of relative clause interpretation. (3) Test the causal relations between production constraints, distributional patterns in the language, and comprehension performance. (4) Relate adult sentence comprehension to statistical learning. (5) Test the current limits of constraint-based models of language comprehension. The PDC approach offers a significant alternative to other views and also may inform language acquisition research by illuminating the role of distributional patterns in child language acquisition. The work also can inform language therapies for brain injured patients in several ways. First, sources of production difficulty are precisely investigated, as are accommodations that unimpaired speakers make in the face of this difficulty. Second, the project investigates the relationship between prior experience with a syntactic construction and comprehension difficulty, which can have implications for the amount and nature of practice that should be provided to patients to improve their comprehension of certain sentence types.         n/a",Linking Language Comprehension to Production Patterns,7226029,R01HD047425,"['Accounting', 'Address', 'Adult', 'Affect', 'Back', 'Brain', 'Brain Injuries', 'Characteristics', 'Child Language', 'Collaborations', 'Comprehension', 'Data', 'Dependence', 'Dependency', 'Depth', 'Disease', 'Environment', 'Face', 'Facility Construction Funding Category', 'Foxes', 'Human Characteristics', 'Individual', 'Infant', 'Investigation', 'Knowledge', 'Language', 'Language Development', 'Language Therapy', 'Learning', 'Linguistics', 'Link', 'Longevity', 'Machine Learning', 'Maintenance', 'Measures', 'Methods', 'Modeling', 'Nature', 'Patients', 'Pattern', 'Performance', 'Play', 'Population', 'Process', 'Production', 'Property', 'Psycholinguistics', 'Reading', 'Relative (related person)', 'Research', 'Research Personnel', 'Role', 'Short-Term Memory', 'Source', 'Structure', 'Testing', 'Time', 'Training', 'Trees', 'Work', 'base', 'experience', 'feeding', 'improved', 'infancy', 'injured', 'insight', 'neuropathology', 'novel', 'preference', 'research study', 'statistics', 'stem', 'syntax', 'theories']",NICHD,UNIVERSITY OF WISCONSIN-MADISON,R01,2007,168217,-0.04436150924857582
"Constraints on Learning from Inconsistent Input All human learners with normal cognitive capacities who are exposed to language input in childhood  manage to acquire a language that looks very much like their input. However, at present we have little  idea how, exactly, learners accomplish this feat. We have little understanding of the nature of the learning  mechanisms that underlie and support language learning. This is the focus of the research in this  proposal. Three general questions guide the research.(1) What are the constraints on language learning  mechanisms? (2) Are the learning mechanisms involved in language acquisition specific to language or  are they of a more general nature? (3) Do these mechanisms change over time as learners age, and if  'so, how? The proposed research examines these questions by investigating the learning of probabilistic  and inconsistent patterns in the input using miniature artificial languages. This provides a rather unique  view of the constraints on learning mechanisms, examining the limits of what can and cannot be learned.  Previous work has shown that learners can acquire probabilistic patterns, however they sometimes  impose consistency on variation. Moreover, children are more likely to change such patterns than are  adults. The present research expands on the earlier results, asking about the nature of the interaction  between learners and input. Series 1 asks about the nature of the input that makes some inconstancy  learnable and some not. The studies examine the limits of veridical learning of inconsistent patterns in  languages, asking questions about the specificity of computations learners can perform, the  representations over which such computations can be performed, and the effect of prior domain-specific  knowledge. Series 2 asks about the nature of the learner, asking why learners regularize over  inconsistency at all.In particular, the studies examine whether working memory constraints, rather than  constraints stemming directly from the learning mechanisms themselves, are an important factor  influencing whether learners acquire the variation or instead impose regularity. Both series will be  conducted with adults and children to examine how learning changes over development. Although the  input in the proposed studies is somewhat atypical, the results from these studies will contribute to our  understanding of the learning mechanisms involved in language acquisition more generally, and  ultimately, this increases our understanding of both normal and disordered acquisition. n/a",Constraints on Learning from Inconsistent Input,7560411,R01HD048572,"['Address', 'Adult', 'Affect', 'Age', 'Animals', 'Child', 'Childhood', 'Cognitive', 'Complex', 'Data', 'Development', 'Disease', 'Elements', 'Goals', 'Human', 'Knowledge', 'Language', 'Language Development', 'Lead', 'Learning', 'Machine Learning', 'Modality', 'Nature', 'Pattern', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Property', 'Research', 'Research Design', 'Research Personnel', 'Series', 'Short-Term Memory', 'Source', 'Specificity', 'Time', 'Variant', 'Work', 'age related', 'design', 'information processing', 'programs', 'research study', 'stem']",NICHD,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2009,177315,-0.048726213412850786
"Constraints on Learning from Inconsistent Input All human learners with normal cognitive capacities who are exposed to language input in childhood  manage to acquire a language that looks very much like their input. However, at present we have little  idea how, exactly, learners accomplish this feat. We have little understanding of the nature of the learning  mechanisms that underlie and support language learning. This is the focus of the research in this  proposal. Three general questions guide the research.(1) What are the constraints on language learning  mechanisms? (2) Are the learning mechanisms involved in language acquisition specific to language or  are they of a more general nature? (3) Do these mechanisms change over time as learners age, and if  'so, how? The proposed research examines these questions by investigating the learning of probabilistic  and inconsistent patterns in the input using miniature artificial languages. This provides a rather unique  view of the constraints on learning mechanisms, examining the limits of what can and cannot be learned.  Previous work has shown that learners can acquire probabilistic patterns, however they sometimes  impose consistency on variation. Moreover, children are more likely to change such patterns than are  adults. The present research expands on the earlier results, asking about the nature of the interaction  between learners and input. Series 1 asks about the nature of the input that makes some inconstancy  learnable and some not. The studies examine the limits of veridical learning of inconsistent patterns in  languages, asking questions about the specificity of computations learners can perform, the  representations over which such computations can be performed, and the effect of prior domain-specific  knowledge. Series 2 asks about the nature of the learner, asking why learners regularize over  inconsistency at all.In particular, the studies examine whether working memory constraints, rather than  constraints stemming directly from the learning mechanisms themselves, are an important factor  influencing whether learners acquire the variation or instead impose regularity. Both series will be  conducted with adults and children to examine how learning changes over development. Although the  input in the proposed studies is somewhat atypical, the results from these studies will contribute to our  understanding of the learning mechanisms involved in language acquisition more generally, and  ultimately, this increases our understanding of both normal and disordered acquisition. n/a",Constraints on Learning from Inconsistent Input,7390362,R01HD048572,"['Address', 'Adult', 'Affect', 'Age', 'Animals', 'Child', 'Childhood', 'Cognitive', 'Complex', 'Computer information processing', 'Data', 'Development', 'Disease', 'Elements', 'Goals', 'Human', 'Knowledge', 'Language', 'Language Development', 'Lead', 'Learning', 'Machine Learning', 'Modality', 'Nature', 'Numbers', 'Pattern', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Property', 'Research', 'Research Design', 'Research Personnel', 'Series', 'Short-Term Memory', 'Source', 'Specificity', 'Time', 'Variant', 'Work', 'age related', 'design', 'programs', 'research study', 'stem']",NICHD,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2008,180295,-0.048726213412850786
"Constraints on Learning from Inconsistent Input    DESCRIPTION (provided by applicant): All human learners with normal cognitive capacities who are exposed to language input in childhood manage to acquire a language that looks very much like their input. However, at present we have little idea how, exactly, learners accomplish this feat. We have little understanding of the nature of the learning mechanisms that underlie and support language learning. This is the focus of the research in this proposal. Three general questions guide the research. (1) What are the constraints on language learning mechanisms? (2) Are the learning mechanisms involved in language acquisition specific to language or are they of a more general nature? (3) Do these mechanisms change over time as learners age, and if 'so, how? The proposed research examines these questions by investigating the learning of probabilistic and inconsistent patterns in the input using miniature artificial languages. This provides a rather unique view of the constraints on learning mechanisms, examining the limits of what can and cannot be learned. Previous work has shown that learners can acquire probabilistic patterns, however they sometimes impose consistency on variation. Moreover, children are more likely to change such patterns than are adults. The present research expands on the earlier results, asking about the nature of the interaction between learners and input. Series 1 asks about the nature of the input that makes some inconstancy learnable and some not. The studies examine the limits of veridical learning of inconsistent patterns in languages, asking questions about the specificity of computations learners can perform, the representations over which such computations can be performed, and the effect of prior domain-specific knowledge. Series 2 asks about the nature of the learner, asking why learners regularize over inconsistency at all. In particular, the studies examine whether working memory constraints, rather than constraints stemming directly from the learning mechanisms themselves, are an important factor influencing whether learners acquire the variation or instead impose regularity. Both series will be conducted with adults and children to examine how learning changes over development. Although the input in the proposed studies is somewhat atypical, the results from these studies will contribute to our understanding of the learning mechanisms involved in language acquisition more generally, and ultimately, this increases our understanding of both normal and disordered acquisition.         n/a",Constraints on Learning from Inconsistent Input,7212176,R01HD048572,"['Address', 'Adult', 'Affect', 'Age', 'Animals', 'Child', 'Childhood', 'Cognitive', 'Complex', 'Computer information processing', 'Data', 'Development', 'Disease', 'Elements', 'Goals', 'Human', 'Knowledge', 'Language', 'Language Development', 'Lead', 'Learning', 'Machine Learning', 'Modality', 'Nature', 'Numbers', 'Pattern', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Property', 'Research', 'Research Design', 'Research Personnel', 'Series', 'Short-Term Memory', 'Source', 'Specificity', 'Time', 'Variant', 'Work', 'age related', 'design', 'programs', 'research study', 'stem']",NICHD,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2007,186741,-0.048726213412850786
"Real-time fMRI neurofeedback of large-scale network dynamics in opioid use disorder PROJECT SUMMARY The misuse of opioids, opioid addiction and overdose are a serious national public health crisis—the opioid epidemic—that despite increased scientific, clinical and government attention, continues to grow. Methadone is a generally effective treatment for opioid use disorder, however relapse rates remain high, and risk of overdose is greatest during relapse. There is a need for improved mechanistic understanding of the factors that contribute to opioid relapse to improve our understanding of opioid use disorder and its treatment. Using connectome-based methods (i.e., functional connectivity) in functional magnetic resonance imaging (fMRI), we recently identified a large-scale brain network that predicted opioid relapse from both resting and task states. Connectome-based methods enable data-driven characterization of whole brain networks related to behavior that might be better suited to describe complex clinical phenomena (e.g., opioid relapse). Building on prior work indicating the utility of real-time fMRI neurofeedback to test brain activation patterns related to specific functions and individual abilities to regulate these functions, the proposed project will use connectome-based neurofeedback to target patterns of functional connectivity within our recently identified “opioid abstinence network”. This information is critical to improve understanding of mechanisms of opioid relapse. Individuals on methadone will be randomized to receive either active (n=12) or sham (n=12) connectome-based neurofeedback at 3 weekly scanning sessions including feedback and transfer runs. Additional baseline and follow-up scans will include resting state and reward and cognitive task runs. Craving, negative affect and opioid use will be measured weekly and at 1-mo follow-up. Based on our pilot data, connectome-based feedback will be targeted at the opioid abstinence network and we hypothesize that increased connectivity in this network will be associated with improved clinical outcomes. Aim 1 will test the hypothesis that active feedback is associated with reduced opioid use from baseline to follow-up scans (Aim 1a) and at 1-mo follow- up (Aim 1b). Aim 2 will test the hypothesis that active feedback is associated with increased opioid abstinence network connectivity in resting state (Aim 2a) and task (reward, cognitive) state (Aim 2b) versus sham feedback, as in our pilot work. Aim 3 will test the hypothesis that active feedback is associated with greater improvements in clinical features of opioid use disorder (craving, negative affect) than sham feedback (Aim 3a) and that increased opioid abstinence network connectivity will correlate with these improvements (Aim 3b). Overall, this project tests a potentially transformative hypothesis relating large-scale brain network dynamics to outcomes in opioid use disorder, and tests a highly innovative method for real-time fMRI neurofeedback from the opioid abstinence network to improve clinical features of opioid use disorder. This project will provide unprecedented insight into the functional neurobiology of opioid relapse and more generally has the potential to transform existing real-time fMRI paradigms in addictions. PROJECT NARRATIVE This project leverages advances in real-time fMRI neurofeedback and machine learning to test whether methadone-treated individuals with opioid use disorder can increase functional connectivity within an empirically-derived “opioid abstinence network” that has previously been identified to predict opioid relapse. This project will provide critical data and information to improve our understanding of opioid relapse and may transform existing real-time fMRI paradigms in addictions more generally. Findings should contribute to the scientific and clinical response to the growing opioid epidemic and be used to inform treatment development.",Real-time fMRI neurofeedback of large-scale network dynamics in opioid use disorder,10025590,R21DA049583,"['Abstinence', 'Alcohol or Other Drugs use', 'Attention', 'Behavior', 'Behavioral', 'Brain', 'Brain region', 'Characteristics', 'Clinical', 'Cognitive', 'Complex', 'Data', 'Data Analytics', 'Dose', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Government', 'Individual', 'Intervention', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Methadone', 'Methods', 'Modeling', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Outcome', 'Pathway interactions', 'Pattern', 'Population', 'Public Health', 'Randomized', 'Refractory', 'Relapse', 'Reporting', 'Research', 'Rest', 'Rewards', 'Running', 'Scanning', 'Signal Transduction', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'Work', 'addiction', 'base', 'brain behavior', 'cognitive process', 'cognitive task', 'connectome', 'craving', 'design', 'effective therapy', 'follow-up', 'high risk', 'imaging study', 'improved', 'improved outcome', 'innovation', 'insight', 'methadone treatment', 'negative affect', 'neural network', 'neural patterning', 'neurofeedback', 'novel', 'opioid epidemic', 'opioid misuse', 'opioid overdose', 'opioid use', 'opioid use disorder', 'overdose risk', 'prescription opioid', 'relating to nervous system', 'response', 'therapy development', 'tool']",NIDA,YALE UNIVERSITY,R21,2020,209375,-0.009838640240565525
"Real-time fMRI neurofeedback of large-scale network dynamics in opioid use disorder PROJECT SUMMARY The misuse of opioids, opioid addiction and overdose are a serious national public health crisis—the opioid epidemic—that despite increased scientific, clinical and government attention, continues to grow. Methadone is a generally effective treatment for opioid use disorder, however relapse rates remain high, and risk of overdose is greatest during relapse. There is a need for improved mechanistic understanding of the factors that contribute to opioid relapse to improve our understanding of opioid use disorder and its treatment. Using connectome-based methods (i.e., functional connectivity) in functional magnetic resonance imaging (fMRI), we recently identified a large-scale brain network that predicted opioid relapse from both resting and task states. Connectome-based methods enable data-driven characterization of whole brain networks related to behavior that might be better suited to describe complex clinical phenomena (e.g., opioid relapse). Building on prior work indicating the utility of real-time fMRI neurofeedback to test brain activation patterns related to specific functions and individual abilities to regulate these functions, the proposed project will use connectome-based neurofeedback to target patterns of functional connectivity within our recently identified “opioid abstinence network”. This information is critical to improve understanding of mechanisms of opioid relapse. Individuals on methadone will be randomized to receive either active (n=12) or sham (n=12) connectome-based neurofeedback at 3 weekly scanning sessions including feedback and transfer runs. Additional baseline and follow-up scans will include resting state and reward and cognitive task runs. Craving, negative affect and opioid use will be measured weekly and at 1-mo follow-up. Based on our pilot data, connectome-based feedback will be targeted at the opioid abstinence network and we hypothesize that increased connectivity in this network will be associated with improved clinical outcomes. Aim 1 will test the hypothesis that active feedback is associated with reduced opioid use from baseline to follow-up scans (Aim 1a) and at 1-mo follow- up (Aim 1b). Aim 2 will test the hypothesis that active feedback is associated with increased opioid abstinence network connectivity in resting state (Aim 2a) and task (reward, cognitive) state (Aim 2b) versus sham feedback, as in our pilot work. Aim 3 will test the hypothesis that active feedback is associated with greater improvements in clinical features of opioid use disorder (craving, negative affect) than sham feedback (Aim 3a) and that increased opioid abstinence network connectivity will correlate with these improvements (Aim 3b). Overall, this project tests a potentially transformative hypothesis relating large-scale brain network dynamics to outcomes in opioid use disorder, and tests a highly innovative method for real-time fMRI neurofeedback from the opioid abstinence network to improve clinical features of opioid use disorder. This project will provide unprecedented insight into the functional neurobiology of opioid relapse and more generally has the potential to transform existing real-time fMRI paradigms in addictions. PROJECT NARRATIVE This project leverages advances in real-time fMRI neurofeedback and machine learning to test whether methadone-treated individuals with opioid use disorder can increase functional connectivity within an empirically-derived “opioid abstinence network” that has previously been identified to predict opioid relapse. This project will provide critical data and information to improve our understanding of opioid relapse and may transform existing real-time fMRI paradigms in addictions more generally. Findings should contribute to the scientific and clinical response to the growing opioid epidemic and be used to inform treatment development.",Real-time fMRI neurofeedback of large-scale network dynamics in opioid use disorder,9841187,R21DA049583,"['Abstinence', 'Alcohol or Other Drugs use', 'Attention', 'Behavior', 'Behavioral', 'Brain', 'Brain region', 'Characteristics', 'Clinical', 'Cognitive', 'Complex', 'Data', 'Data Analytics', 'Dose', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Government', 'Individual', 'Intervention', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Methadone', 'Methods', 'Modeling', 'Neurobiology', 'Opiate Addiction', 'Opioid', 'Outcome', 'Pathway interactions', 'Pattern', 'Population', 'Public Health', 'Randomized', 'Refractory', 'Relapse', 'Reporting', 'Research', 'Rest', 'Rewards', 'Running', 'Scanning', 'Signal Transduction', 'Symptoms', 'Testing', 'Therapeutic', 'Time', 'Training', 'Work', 'addiction', 'base', 'brain behavior', 'cognitive process', 'cognitive task', 'connectome', 'craving', 'design', 'effective therapy', 'follow-up', 'high risk', 'imaging study', 'improved', 'improved outcome', 'innovation', 'insight', 'methadone treatment', 'negative affect', 'neural network', 'neural patterning', 'neurofeedback', 'novel', 'opioid epidemic', 'opioid misuse', 'opioid overdose', 'opioid use', 'opioid use disorder', 'overdose risk', 'prescription opioid', 'relating to nervous system', 'response', 'therapy development', 'tool']",NIDA,YALE UNIVERSITY,R21,2019,251250,-0.009838640240565525
"OPTIMIZATION OF MULTIDIMENSIONAL NMR DATA ANALYSIS The advent of efficient methods for isotopic enrichment of proteins with 13C and 15N, together with the development of three- and four-dimensional (3D and 4D) NMR methods, now makes it possible to study in detail the structure and dynamics of proteins in the 15-30 kD molecular weight range, as was demonstrated most recently for the proteins interleukin- 1beta, calmodulin complexed with a fragment of myosin light chain kinase and interferon-gamma.  However, the present procedures for data collection and analysis are extremely time consuming, and the spectral resolution obtainable frequently presents a limiting factor.  The goal of this proposal is the development of improved and new procedures for the analysis of multi-dimensional NMR spectra. Significant improvements over present methodology are expected to be possible considering that most of the methodology presently does not utilize the advances which have been made in the last decade in the area of signal processing.  Improved and new procedures will focus on but not limit to the following:  (1) Improvement of the linear prediction methods:  improving the stability and computation efficiency of the 1-D and 2-D linear prediction algorithms.  (2) Optimization of a priori information:  Use of a priori information in NMR data to improve the spectral resolution.  (3) Non-linear spectral enhancement:  Development of the 3-D maximum entropy method and improving the convergence rate of methods based on iterative deconvolution.  (4) Development of fast algorithms:  Development of fast routines that make possible the use of algorithms in multi-dimensional space.  (5) signal subspace approach: Separation of the signal and the noise subspace to obtain high NMR spectral resolution.  (6) Higher-order spectra estimation:  Use of high- order statistics to gain a better estimate of dense NMR spectra.  (7) Multi-resolution techniques:  New signal representation for NMR data to achieve optimal interpretation and processing.  (8) Data compression: Compression of multi-dimensional NMR data by removing redundant information for processing, networking, and archiving.  (9) Parallel implementation:  Development of parallel routines for high-performance parallel computing.  Emphasis will be on the development of code which is easily transportable and which will be made freely available to the NMR community.  The complexities of the task of developing algorithms and software for advanced processing of multi-dimensional NMR data are substantial and require effective integration of expertise in the areas of NMR and signal processing.  This proposal is designed to foster such integration.  n/a",OPTIMIZATION OF MULTIDIMENSIONAL NMR DATA ANALYSIS,2187241,R01GM049707,"['artificial intelligence', ' bioengineering /biomedical engineering', ' computer program /software', ' computer system design /evaluation', ' mathematical model', ' method development', ' model design /development', ' nuclear magnetic resonance spectroscopy', ' parallel processing', ' protein structure']",NIGMS,UNIVERSITY OF MARYLAND COLLEGE PK CAMPUS,R01,1994,99974,-0.014039989032342031
"OPTIMIZATION OF MULTIDIMENSIONAL NMR DATA ANALYSIS The advent of efficient methods for isotopic enrichment of proteins with 13C and 15N, together with the development of three- and four-dimensional (3D and 4D) NMR methods, now makes it possible to study in detail the structure and dynamics of proteins in the 15-30 kD molecular weight range, as was demonstrated most recently for the proteins interleukin- 1beta, calmodulin complexed with a fragment of myosin light chain kinase and interferon-gamma.  However, the present procedures for data collection and analysis are extremely time consuming, and the spectral resolution obtainable frequently presents a limiting factor.  The goal of this proposal is the development of improved and new procedures for the analysis of multi-dimensional NMR spectra. Significant improvements over present methodology are expected to be possible considering that most of the methodology presently does not utilize the advances which have been made in the last decade in the area of signal processing.  Improved and new procedures will focus on but not limit to the following:  (1) Improvement of the linear prediction methods:  improving the stability and computation efficiency of the 1-D and 2-D linear prediction algorithms.  (2) Optimization of a priori information:  Use of a priori information in NMR data to improve the spectral resolution.  (3) Non-linear spectral enhancement:  Development of the 3-D maximum entropy method and improving the convergence rate of methods based on iterative deconvolution.  (4) Development of fast algorithms:  Development of fast routines that make possible the use of algorithms in multi-dimensional space.  (5) signal subspace approach: Separation of the signal and the noise subspace to obtain high NMR spectral resolution.  (6) Higher-order spectra estimation:  Use of high- order statistics to gain a better estimate of dense NMR spectra.  (7) Multi-resolution techniques:  New signal representation for NMR data to achieve optimal interpretation and processing.  (8) Data compression: Compression of multi-dimensional NMR data by removing redundant information for processing, networking, and archiving.  (9) Parallel implementation:  Development of parallel routines for high-performance parallel computing.  Emphasis will be on the development of code which is easily transportable and which will be made freely available to the NMR community.  The complexities of the task of developing algorithms and software for advanced processing of multi-dimensional NMR data are substantial and require effective integration of expertise in the areas of NMR and signal processing.  This proposal is designed to foster such integration.  n/a",OPTIMIZATION OF MULTIDIMENSIONAL NMR DATA ANALYSIS,3308869,R01GM049707,"['artificial intelligence', ' bioengineering /biomedical engineering', ' computer program /software', ' computer system design /evaluation', ' mathematical model', ' method development', ' model design /development', ' nuclear magnetic resonance spectroscopy', ' parallel processing', ' protein structure']",NIGMS,UNIVERSITY OF MARYLAND COLLEGE PK CAMPUS,R01,1993,96129,-0.014039989032342031
"Temporally Adaptive fMRI DESCRIPTION (provided by applicant):    This research plan envisages a methodological advance in functional MRI (fMRI) to allow for adaptive stimulus presentation derived directly from the acquired image data. Adaptation of stimuli will be accomplished by modeling fMRI to classify brain states during the image reconstruction process, and subsequently modulating a visual display. This emphasis on image-based prediction constitutes a fundamental shift from the conventional approach of using temporal changes in images to detect spatial ""hot spots"". This research will generate significant insights and development of capabilities for adaptive fMRI experiments using prediction of brain states. This has several significant applications. Primary among these is the potential contribution to designing much more flexible experiments to enhance our basic understanding of brain function. Also relevant are biofeedback rehabilitation, therapeutic meditation, learning studies, sports therapy or other virtual reality-based training, and lie-detection. Moreover this approach will provide spatially resolved data that complements ongoing EEG-based brain computer interface (BCI) research.  The experimental plan incorporates a constructive progression that first develops offtine predictive algorithms to a range, of fMRI experminents, secondly treats the case of measurable human learning characterized by offline analysis, and ifinally utilizes these initial studies to characterize system comprising a real-time machine learning algorithm coupled with a responsive human volunteer.  Long-term goal: Initiate a research program that will enhance current spatial mapping studies by allowing for temporal classification of brain states based on image data and biofeedback capabilities for adaptive fMRI experiments.  Specific Aims:  1) Characterize the relationship between choice of fMRI task and choice of predictive technique to examine the importance of the particular predictive model, the connection between task difficulty and modeling accuracy, and the amount of training data required to build accurate predictive models.  2) Analyze fMRi data from a motor-learning task to study how behaviorally demonstrated learning by a subject corresponds with changes in the image data, and if this effect is directly observable using predictive models.  3) Develop capabilities to perform real-time feedback of stimulus based on interaction between a predictive algorithm and subject adaptation. n/a",Temporally Adaptive fMRI,7001247,R21NS050183,"['behavior prediction', 'behavior test', 'behavioral /social science research tag', 'biofeedback', 'bioimaging /biomedical imaging', 'brain mapping', 'choice', 'clinical research', 'functional magnetic resonance imaging', 'human subject', 'learning', 'mathematical model', 'model design /development', 'preference', 'psychological models', 'statistics /biometry', 'stimulus /response', 'time resolved data']",NINDS,EMORY UNIVERSITY,R21,2006,172749,-0.007305727855712391
"Temporally Adaptive fMRI DESCRIPTION (provided by applicant):    This research plan envisages a methodological advance in functional MRI (fMRI) to allow for adaptive stimulus presentation derived directly from the acquired image data. Adaptation of stimuli will be accomplished by modeling fMRI to classify brain states during the image reconstruction process, and subsequently modulating a visual display. This emphasis on image-based prediction constitutes a fundamental shift from the conventional approach of using temporal changes in images to detect spatial ""hot spots"". This research will generate significant insights and development of capabilities for adaptive fMRI experiments using prediction of brain states. This has several significant applications. Primary among these is the potential contribution to designing much more flexible experiments to enhance our basic understanding of brain function. Also relevant are biofeedback rehabilitation, therapeutic meditation, learning studies, sports therapy or other virtual reality-based training, and lie-detection. Moreover this approach will provide spatially resolved data that complements ongoing EEG-based brain computer interface (BCI) research.  The experimental plan incorporates a constructive progression that first develops offtine predictive algorithms to a range, of fMRI experminents, secondly treats the case of measurable human learning characterized by offline analysis, and ifinally utilizes these initial studies to characterize system comprising a real-time machine learning algorithm coupled with a responsive human volunteer.  Long-term goal: Initiate a research program that will enhance current spatial mapping studies by allowing for temporal classification of brain states based on image data and biofeedback capabilities for adaptive fMRI experiments.  Specific Aims:  1) Characterize the relationship between choice of fMRI task and choice of predictive technique to examine the importance of the particular predictive model, the connection between task difficulty and modeling accuracy, and the amount of training data required to build accurate predictive models.  2) Analyze fMRi data from a motor-learning task to study how behaviorally demonstrated learning by a subject corresponds with changes in the image data, and if this effect is directly observable using predictive models.  3) Develop capabilities to perform real-time feedback of stimulus based on interaction between a predictive algorithm and subject adaptation. n/a",Temporally Adaptive fMRI,6854105,R21NS050183,"['behavior prediction', 'behavior test', 'behavioral /social science research tag', 'biofeedback', 'bioimaging /biomedical imaging', 'brain mapping', 'choice', 'clinical research', 'functional magnetic resonance imaging', 'human subject', 'learning', 'mathematical model', 'model design /development', 'preference', 'psychological models', 'statistics /biometry', 'stimulus /response', 'time resolved data']",NINDS,EMORY UNIVERSITY,R21,2005,205820,-0.007305727855712391
"AMPLIFIERS--DENSE ELECTRODE ARRAY ELECTROENCEPHALOGRAPHY The proposed research would develop an inexpensive electro- encephalographic (EEG) amplifier system suitable for dense (128 or 256) electrode arrays. These arrays would provide the measurements necessary for new source localization algorithms constrained by MRI data that image the electrical activity of the cortex throughout the three dimensions of intracranial space.  Although the anatomical resolution of these images remains to be determined, they are generated for each millisecond sample of electrical recording, and thus, provide information with a temporal resolution not possible with metabolic and hemodynamic methods of neuroimaging. This temporal resolution may be necessary to research the cognitive activity of cortical networks.  The major advances in visualizing dynamic brain function promised by this new technology will have immediate applications in psychology, neurology, psychiatry, education, and human factors research.  Within 5 years, this research will provide the basis for widespread clinical applications of this technology to public health problems ranging from attention deficit disorder to Alzheimer's dementia.  n/a",AMPLIFIERS--DENSE ELECTRODE ARRAY ELECTROENCEPHALOGRAPHY,3503466,R43MH051069,"['artificial intelligence', ' biomedical equipment development', ' brain electrical activity', ' brain imaging /visualization /scanning', ' clinical biomedical equipment', ' computer program /software', ' electrodes', ' electroencephalography']",NIMH,"ELECTRICAL GEODESICS, INC.",R43,1993,50000,-0.053012188514938746
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6881434,R01MH051435,"['artificial intelligence', 'behavioral /social science research tag', 'bioimaging /biomedical imaging', 'computer program /software', 'computer system design /evaluation', 'digital imaging', 'emotions', 'face expression', 'human subject', 'image processing', 'interpersonal relations', 'statistics /biometry', 'videotape /videodisc', 'visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2005,358843,-0.07500841625889482
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6907787,R01MH051435,"['artificial intelligence', 'behavioral /social science research tag', 'bioimaging /biomedical imaging', 'computer program /software', 'computer system design /evaluation', 'digital imaging', 'emotions', 'face expression', 'human subject', 'image processing', 'interpersonal relations', 'statistics /biometry', 'videotape /videodisc', 'visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2004,42260,-0.07500841625889482
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6736314,R01MH051435,"['artificial intelligence', 'behavioral /social science research tag', 'bioimaging /biomedical imaging', 'computer program /software', 'computer system design /evaluation', 'digital imaging', 'emotions', 'face expression', 'human subject', 'image processing', 'interpersonal relations', 'statistics /biometry', 'videotape /videodisc', 'visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2004,305282,-0.07500841625889482
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6639029,R01MH051435,"['artificial intelligence', ' behavioral /social science research tag', ' bioimaging /biomedical imaging', ' computer program /software', ' computer system design /evaluation', ' digital imaging', ' emotions', ' face expression', ' human subject', ' image processing', ' interpersonal relations', ' statistics /biometry', ' videotape /videodisc', ' visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2003,297999,-0.07500841625889482
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6538699,R01MH051435,"['artificial intelligence', ' behavioral /social science research tag', ' bioimaging /biomedical imaging', ' computer program /software', ' computer system design /evaluation', ' digital imaging', ' emotions', ' face expression', ' human subject', ' image processing', ' interpersonal relations', ' statistics /biometry', ' videotape /videodisc', ' visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2002,290602,-0.07500841625889482
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6287970,R01MH051435,"['artificial intelligence', ' behavioral /social science research tag', ' bioimaging /biomedical imaging', ' computer program /software', ' computer system design /evaluation', ' digital imaging', ' emotions', ' face expression', ' human subject', ' image processing', ' interpersonal relations', ' statistics /biometry', ' videotape /videodisc', ' visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2001,334747,-0.07500841625889482
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7351765,R01NS051826,"['Affect', 'Age', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomic Models', 'Anatomy', 'Area', 'Back', 'Class', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Diffuse Pattern', 'Disease', 'Disease Progression', 'Evaluation', 'Fetal Growth Retardation', 'Goals', 'Gold', 'Human', 'Image', 'Imagery', 'Imaging Device', 'Intuition', 'Knowledge', 'Learning', 'Localized', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical Research', 'Methods', 'Modeling', 'Morphology', 'Neonatal', 'Neuroanatomy', 'Normal Range', 'Operative Surgical Procedures', 'Pathology', 'Patients', 'Population', 'Population Study', 'Process', 'Range', 'Research', 'Schizophrenia', 'Shapes', 'Standards of Weights and Measures', 'Statistical Distributions', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Variant', 'base', 'computer studies', 'desire', 'disease classification', 'feeding', 'imaging Segmentation', 'improved', 'neonate', 'nervous system disorder', 'neurosurgery', 'normal aging', 'novel', 'shape analysis', 'tool']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2008,281588,-0.03683891505892032
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7560409,R01NS051826,"['Affect', 'Age', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Area', 'Back', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Disease', 'Disease Progression', 'Evaluation', 'Fetal Growth Retardation', 'Goals', 'Gold', 'Human', 'Image', 'Imagery', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Modeling', 'Neonatal', 'Normal Range', 'Operative Surgical Procedures', 'Patients', 'Population', 'Population Study', 'Process', 'Research', 'Schizophrenia', 'Shapes', 'Statistical Distributions', 'Structure', 'System', 'Techniques', 'Testing', 'Training', 'Variant', 'base', 'computer studies', 'disease classification', 'feeding', 'improved', 'neonate', 'nervous system disorder', 'normal aging', 'novel', 'shape analysis', 'tool']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2009,280500,-0.03683891505892032
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7186695,R01NS051826,"['Accounting', 'Adult', 'Affect', 'Age', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomic Models', 'Anatomic structures', 'Anatomy', 'Area', 'Atlases', 'Back', 'Biomechanics', 'Boston', 'Brain', 'Caring', 'Class', 'Classification', 'Clinical assessments', 'Clutterings', 'Collaborations', 'Competence', 'Computer Simulation', 'Computer Vision Systems', 'Computing Methodologies', 'Corpus Callosum', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diffuse Pattern', 'Discipline of obstetrics', 'Disease', 'Disease Progression', 'Effectiveness', 'Effectiveness of Interventions', 'Electroencephalography', 'Elements', 'Ensure', 'Evaluation', 'Evolution', 'Fetal Growth Retardation', 'General Hospitals', 'Genetic Markers', 'Goals', 'Gold', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Hospitals', 'Human', 'Image', 'Imagery', 'Imaging Device', 'Incidence', 'Individual', 'Infant', 'Intervention', 'Intuition', 'Invasive', 'Knowledge', 'Label', 'Learning', 'Learning Disabilities', 'Link', 'Localized', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Massachusetts', 'Measurement', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Morphologic artifacts', 'Morphology', 'Motivation', 'Neonatal', 'Neuroanatomy', 'Neurosciences', 'Neurosurgeon', 'Noise', 'Normal Range', 'Operative Surgical Procedures', 'Outcome', 'Pathology', 'Patients', 'Pediatric Hospitals', 'Population', 'Population Characteristics', 'Population Study', 'Positioning Attribute', 'Premature Infant', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Property', 'Psyche structure', 'Range', 'Rate', 'Relative (related person)', 'Research', 'Research Personnel', 'Residual state', 'Resolution', 'Rest', 'Role', 'Scanning', 'Schizophrenia', 'Shapes', 'Site', 'Specificity', 'Staging', 'Standards of Weights and Measures', 'Statistical Distributions', 'Statistical Models', 'Statistical Study', 'Statistically Significant', 'Structure', 'Surface', 'Surgeon', 'Surgical Instruments', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Training', 'Tweens', 'Universities', 'Validation', 'Variant', 'Washington', 'Woman', 'base', 'cohort', 'computer studies', 'computerized tools', 'desire', 'deviant', 'disease classification', 'expectation', 'feeding', 'healthy aging', 'imaging Segmentation', 'improved', 'instrument', 'interest', 'mortality', 'neonate', 'nervous system disorder', 'neuroimaging', 'neurosurgery', 'normal aging', 'novel', 'programs', 'radiologist', 'reconstruction', 'relating to nervous system', 'research clinical testing', 'response', 'shape analysis', 'statistics', 'tool', 'tumor']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2007,282619,-0.03683891505892032
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,7015019,R01NS051826,"['Alzheimer&apos', 's disease', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain morphology', 'human data', 'image enhancement', 'image guided surgery /therapy', 'magnetic resonance imaging', 'mathematical model', 'prenatal growth disorder']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2006,289590,-0.03683891505892032
"Computational Modeling of Anatomical Shape Distributions    DESCRIPTION (provided by applicant): Segmentation of detailed, patient-specific models from medical imagery can provide invaluable assistance for surgical planning and navigation. Current segmentation methods often make errors when confronted with subtle intensity boundaries. Adding knowledge of expected shape of a structure, and the range of normal variations in shape, can greatly improve segmentation, by guiding it towards the most likely shape consistent with the image information. The resulting segmentations can be used to plan surgical procedures, and when registered to the patient, can provide navigational guidance around critical structures. Many neurological diseases, such as Alzheimer's, schizophrenia, and Fetal Growth Restriction, affect the shape of specific anatomical areas. To understand the development and progression of these diseases, as well as to develop methods for classifying instances into diseased or normal classes, 1 needs methods that capture differences in shape distributions between populations. Our goal is to develop and validate methods for learning from images concise representations of anatomical shape and its variability, Modeling shape distributions will improve segmentation algorithms by biasing the search towards more likely shapes. It will also enable quantitative analysis based on shape in population studies, where imaging is used to study differences in anatomy between populations, as well as changes within a population, for example with age. The proposed research builds on prior methods for segmentation and shape analysis, using tools from computer vision and machine learning applied to questions of shape representation, shape based segmentation and shape analysis for population studies. We plan to further develop the methods and to validate them with our collaborators in several different applications, including surgical planning, neonatal imaging and image-based studies of aging and Alzheimer's disease.            n/a",Computational Modeling of Anatomical Shape Distributions,6916728,R01NS051826,"['Alzheimer&apos', 's disease', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain morphology', 'human data', 'image enhancement', 'image guided surgery /therapy', 'magnetic resonance imaging', 'mathematical model', 'prenatal growth disorder']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2005,292728,-0.03683891505892032
"Identification and tracking of neural stem cells in vivo: a metabolomic approach    DESCRIPTION (provided by applicant):  The ability to identify human neural stem cells (NSC) by brain imaging may have profound implications for diagnostic, prognostic, and therapeutic purposes.  Currently, there are no clinical, high-resolution imaging techniques that enable investigations of the survival, migration, fate, and function of unlabeled NSC and their progeny.  The study of human NSC in vivo is hindered by the absence of well-defined markers that can distinguish them from other neural cell types.  The goal of this proposal is to define markers of NSC by characterizing metabolomic fingerprint of NSC both in vitro and in vivo.  Our objectives are to develop novel imaging and signal processing methodologies that would enable non-invasive investigations of NSC behavior in healthy and disease states, from early human development to older age.  We hypothesize that mammalian NSC have a specific metabolic marker that can be identified by spectroscopy.  Our specific aims are:  1) to characterize a metabolomic fingerprint of NSC in vitro using proton nuclear magnetic resonance (1H-NMR) spectroscopy and to compare it to the neuronal and glial 1H-NMR fingerprints; 2) to develop high resolution proton MR spectroscopy (1H-MRS) acquisition protocols that will allow for characterization of the NSC fate in vivo; 3) to develop signal processing algorithms that will provide accurate estimates of cell densities even from data with low signal-to-noise ratio and limited spectral, spatial, and temporal resolutions.  Our preliminary experiments have demonstrated that we are able to identify the NSC on the basis of their 1H-NMR metabolomic fingerprint.  In addition, we can detect both endogenous and exogenous NSC in the living rat brain, using 1H-MRS and 9.4T mMRI scanner.  We plan to further characterize the metabolomic signature of NSC and other neural cell types, and to further perform metabolomic profiling of the living rat brain.  The signal processing algorithms for identification, quantification, and tracking of NSC in vivo will be based on singular value decomposition methodology and will exploit prior knowledge gained from in vitro experiments.  This innovative research will not only demonstrate the feasibility of using 1H-MRS spectroscopy for metabolomic investigations in vivo, but will also lead to a breakthrough in the field of stem cell research.  Most importantly, this research is an essential prerequisite for future clinical investigations of NSC.  The ability to monitor the fundamental changes of the NSC in the human brain will instigate new studies of neurological disorders where NSC pathology might contribute to the etiology of the disease and will initiate developments of new treatments.  The proposed research is intrinsically multidisciplinary and involves collaborations of neuroscientists, physicists, engineers, chemists, and imaging scientists.  Each of them will provide unique yet complementary expertise and resources available at the Stony Brook University and Brookhaven National Laboratory.         n/a",Identification and tracking of neural stem cells in vivo: a metabolomic approach,7286826,R21NS053875,"['18 year old', 'Address', 'Affect', 'Age', 'Algorithms', 'Astrocytes', 'Biological', 'Brain', 'Brain imaging', 'Brain region', 'Cell Density', 'Cells', 'Cerebral Palsy', 'Cessation of life', 'Chemicals', 'Child', 'Choline', 'Chronic', 'Clinical', 'Clinical Trials', 'Collaborations', 'Complex', 'Condition', 'Contrast Media', 'Data', 'Detection', 'Development', 'Diagnostic', 'Disease', 'Emission-Computed Tomography', 'Engineering', 'Etiology', 'Face', 'Fingerprint', 'Functional disorder', 'Future', 'Goals', 'Human', 'Human Development', 'Image', 'Imaging Techniques', 'Imaging technology', 'In Vitro', 'Invasive', 'Investigation', 'Knowledge', 'Laboratories', 'Lead', 'Life', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Mental Retardation', 'Metabolic', 'Metabolic Marker', 'Methodology', 'Modality', 'Monitor', 'Mus', 'N-acetylaspartate', 'NMR Spectroscopy', 'Neurologic', 'Neurons', 'Noise', 'Nuclear Magnetic Resonance', 'Pathology', 'Perinatal', 'Photons', 'Population', 'Positron-Emission Tomography', 'Protocols documentation', 'Protons', 'Purpose', 'Rattus', 'Reagent', 'Research', 'Resolution', 'Resources', 'Scanning', 'Scientist', 'Signal Transduction', 'Specificity', 'Spectrum Analysis', 'Stem Cell Research', 'Techniques', 'Therapeutic', 'Time', 'Tissues', 'Transplantation', 'Universities', 'Ursidae Family', 'base', 'brain tissue', 'cell behavior', 'cell type', 'computerized data processing', 'data acquisition', 'density', 'dentate gyrus', 'in vivo', 'innovation', 'interest', 'iron oxide', 'metabolomics', 'migration', 'multidisciplinary', 'myoinositol', 'neonate', 'nerve stem cell', 'nervous system disorder', 'novel', 'novel strategies', 'prenatal', 'prognostic', 'research study', 'stem cell fate']",NINDS,STATE UNIVERSITY NEW YORK STONY BROOK,R21,2007,169319,-0.022201248638296745
"Identification and tracking of neural stem cells in vivo: a metabolomic approach    DESCRIPTION (provided by applicant):  The ability to identify human neural stem cells (NSC) by brain imaging may have profound implications for diagnostic, prognostic, and therapeutic purposes.  Currently, there are no clinical, high-resolution imaging techniques that enable investigations of the survival, migration, fate, and function of unlabeled NSC and their progeny.  The study of human NSC in vivo is hindered by the absence of well-defined markers that can distinguish them from other neural cell types.  The goal of this proposal is to define markers of NSC by characterizing metabolomic fingerprint of NSC both in vitro and in vivo.  Our objectives are to develop novel imaging and signal processing methodologies that would enable non-invasive investigations of NSC behavior in healthy and disease states, from early human development to older age.  We hypothesize that mammalian NSC have a specific metabolic marker that can be identified by spectroscopy.  Our specific aims are:  1) to characterize a metabolomic fingerprint of NSC in vitro using proton nuclear magnetic resonance (1H-NMR) spectroscopy and to compare it to the neuronal and glial 1H-NMR fingerprints; 2) to develop high resolution proton MR spectroscopy (1H-MRS) acquisition protocols that will allow for characterization of the NSC fate in vivo; 3) to develop signal processing algorithms that will provide accurate estimates of cell densities even from data with low signal-to-noise ratio and limited spectral, spatial, and temporal resolutions.  Our preliminary experiments have demonstrated that we are able to identify the NSC on the basis of their 1H-NMR metabolomic fingerprint.  In addition, we can detect both endogenous and exogenous NSC in the living rat brain, using 1H-MRS and 9.4T mMRI scanner.  We plan to further characterize the metabolomic signature of NSC and other neural cell types, and to further perform metabolomic profiling of the living rat brain.  The signal processing algorithms for identification, quantification, and tracking of NSC in vivo will be based on singular value decomposition methodology and will exploit prior knowledge gained from in vitro experiments.  This innovative research will not only demonstrate the feasibility of using 1H-MRS spectroscopy for metabolomic investigations in vivo, but will also lead to a breakthrough in the field of stem cell research.  Most importantly, this research is an essential prerequisite for future clinical investigations of NSC.  The ability to monitor the fundamental changes of the NSC in the human brain will instigate new studies of neurological disorders where NSC pathology might contribute to the etiology of the disease and will initiate developments of new treatments.  The proposed research is intrinsically multidisciplinary and involves collaborations of neuroscientists, physicists, engineers, chemists, and imaging scientists.  Each of them will provide unique yet complementary expertise and resources available at the Stony Brook University and Brookhaven National Laboratory.         n/a",Identification and tracking of neural stem cells in vivo: a metabolomic approach,7145549,R21NS053875,"['brain', 'cell type', 'human', 'hydrogen ions', 'metabolomics', 'stem cells', 'tissues']",NINDS,STATE UNIVERSITY NEW YORK STONY BROOK,R21,2006,209250,-0.022201248638296745
"FLOW CYTOMETRY DATA ANALYSIS BY EXPERT SYSTEMS The broad, long term objective of this proposal is to develop user-friendly, intelligent software that will enable biologists to analyze their multivariate now cytometry data more efficiently and more effectively.  Flow Cytometers can now collect eight or more measurements per cell and biochemists and cell biologists have devised markers to take advantage of this multiparameter capability.  The software to enable biologists to easily analyze this multivariate data is only now beginning to be developed.  This proposal addresses the needs of investigators doing basic cell biology work and clinicians needing rapid and thorough analyses of their multivariate flow cytometry data.  Specific Aim 1 is focused on the development of device independent software for interactive, exploratory analysis of multivariate flow cytometry data. The software will use a mouse and menu user interface so that the user will not have to read bulky manuals.  The program will include two and three dimensional dynamic color graphics.  It will incorporate a statistical expert system called GRAPH HELPER to assist the user in choosing the best set of displays to examine.  In Specific Aim 2, an expert system will be developed that incorporates the knowledge of a statistician and that of a biologist to assist in the automatic processing of many datasets of multivariate flow cytometry data.  The ANALYST expert system will contain separate knowledge bases for statistical expertise and biological expertise.  The statistician knowledge base will include rules and facts about hypothesis testing, cluster analysis, dimensionality reduction, probability modeling and two and three dimensional graphical display techniques.  The biologist knowledge base will at first include rules and facts governing human differentiation antigen clusters and the fluorescent monoclonal antibodies used to detect them.  It will later include knowledge about other specific biological systems.  Specific Aim 3 is focused on the investigation of several approaches to partitioning multivariate flow cytometry data. Parametric clustering schemes using the K-means algorithm and the Mahalanobis distance metric will be investigated along with several nonparametric approaches.  Various techniques will be examined for efficiently starting the clustering process and for determining the correct number of clusters.  Classification and Regression Tree (CART) algorithms will be investigated for partitioning multivariate flow cytometry data after an initial clustering.  Clustering methods will be developed to run on the LANL Connection Machine so that data from large numbers of cells can be included in a cluster analysis during an interactive session with the user.  n/a",FLOW CYTOMETRY DATA ANALYSIS BY EXPERT SYSTEMS,2095998,R01CA054518,"['CD antigens', ' artificial intelligence', ' classification', ' computer data analysis', ' computer graphics /printing', ' computer program /software', ' computer system design /evaluation', ' flow cytometry', ' lymphocyte', ' mathematical model', ' monoclonal antibody', ' personal computers', ' statistics /biometry']",NCI,UNIVERSITY OF CALIF-LOS ALAMOS NAT LAB,R01,1994,89138,-0.03295728359474943
"FLOW CYTOMETRY DATA ANALYSIS BY EXPERT SYSTEMS The broad, long term objective of this proposal is to develop user-friendly, intelligent software that will enable biologists to analyze their multivariate now cytometry data more efficiently and more effectively.  Flow Cytometers can now collect eight or more measurements per cell and biochemists and cell biologists have devised markers to take advantage of this multiparameter capability.  The software to enable biologists to easily analyze this multivariate data is only now beginning to be developed.  This proposal addresses the needs of investigators doing basic cell biology work and clinicians needing rapid and thorough analyses of their multivariate flow cytometry data.  Specific Aim 1 is focused on the development of device independent software for interactive, exploratory analysis of multivariate flow cytometry data. The software will use a mouse and menu user interface so that the user will not have to read bulky manuals.  The program will include two and three dimensional dynamic color graphics.  It will incorporate a statistical expert system called GRAPH HELPER to assist the user in choosing the best set of displays to examine.  In Specific Aim 2, an expert system will be developed that incorporates the knowledge of a statistician and that of a biologist to assist in the automatic processing of many datasets of multivariate flow cytometry data.  The ANALYST expert system will contain separate knowledge bases for statistical expertise and biological expertise.  The statistician knowledge base will include rules and facts about hypothesis testing, cluster analysis, dimensionality reduction, probability modeling and two and three dimensional graphical display techniques.  The biologist knowledge base will at first include rules and facts governing human differentiation antigen clusters and the fluorescent monoclonal antibodies used to detect them.  It will later include knowledge about other specific biological systems.  Specific Aim 3 is focused on the investigation of several approaches to partitioning multivariate flow cytometry data. Parametric clustering schemes using the K-means algorithm and the Mahalanobis distance metric will be investigated along with several nonparametric approaches.  Various techniques will be examined for efficiently starting the clustering process and for determining the correct number of clusters.  Classification and Regression Tree (CART) algorithms will be investigated for partitioning multivariate flow cytometry data after an initial clustering.  Clustering methods will be developed to run on the LANL Connection Machine so that data from large numbers of cells can be included in a cluster analysis during an interactive session with the user.  n/a",FLOW CYTOMETRY DATA ANALYSIS BY EXPERT SYSTEMS,2095997,R01CA054518,"['CD antigens', ' artificial intelligence', ' classification', ' computer data analysis', ' computer graphics /printing', ' computer program /software', ' computer system design /evaluation', ' flow cytometry', ' lymphocyte', ' mathematical model', ' monoclonal antibody', ' personal computers', ' statistics /biometry']",NCI,UNIVERSITY OF CALIF-LOS ALAMOS NAT LAB,R01,1993,345756,-0.03295728359474943
"FLOW CYTOMETRY DATA ANALYSIS BY EXPERT SYSTEMS The broad, long term objective of this proposal is to develop user-friendly, intelligent software that will enable biologists to analyze their multivariate now cytometry data more efficiently and more effectively.  Flow Cytometers can now collect eight or more measurements per cell and biochemists and cell biologists have devised markers to take advantage of this multiparameter capability.  The software to enable biologists to easily analyze this multivariate data is only now beginning to be developed.  This proposal addresses the needs of investigators doing basic cell biology work and clinicians needing rapid and thorough analyses of their multivariate flow cytometry data.  Specific Aim 1 is focused on the development of device independent software for interactive, exploratory analysis of multivariate flow cytometry data. The software will use a mouse and menu user interface so that the user will not have to read bulky manuals.  The program will include two and three dimensional dynamic color graphics.  It will incorporate a statistical expert system called GRAPH HELPER to assist the user in choosing the best set of displays to examine.  In Specific Aim 2, an expert system will be developed that incorporates the knowledge of a statistician and that of a biologist to assist in the automatic processing of many datasets of multivariate flow cytometry data.  The ANALYST expert system will contain separate knowledge bases for statistical expertise and biological expertise.  The statistician knowledge base will include rules and facts about hypothesis testing, cluster analysis, dimensionality reduction, probability modeling and two and three dimensional graphical display techniques.  The biologist knowledge base will at first include rules and facts governing human differentiation antigen clusters and the fluorescent monoclonal antibodies used to detect them.  It will later include knowledge about other specific biological systems.  Specific Aim 3 is focused on the investigation of several approaches to partitioning multivariate flow cytometry data. Parametric clustering schemes using the K-means algorithm and the Mahalanobis distance metric will be investigated along with several nonparametric approaches.  Various techniques will be examined for efficiently starting the clustering process and for determining the correct number of clusters.  Classification and Regression Tree (CART) algorithms will be investigated for partitioning multivariate flow cytometry data after an initial clustering.  Clustering methods will be developed to run on the LANL Connection Machine so that data from large numbers of cells can be included in a cluster analysis during an interactive session with the user.  n/a",FLOW CYTOMETRY DATA ANALYSIS BY EXPERT SYSTEMS,3199070,R01CA054518,"['CD antigens', ' artificial intelligence', ' classification', ' computer data analysis', ' computer graphics /printing', ' computer program /software', ' computer system design /evaluation', ' flow cytometry', ' lymphocyte', ' mathematical model', ' monoclonal antibody', ' personal computers', ' statistics /biometry']",NCI,UNIVERSITY OF CALIF-LOS ALAMOS NAT LAB,R01,1992,290178,-0.03295728359474943
"FLOW CYTOMETRY DATA ANALYSIS BY EXPERT SYSTEMS The broad, long term objective of this proposal is to develop user-friendly, intelligent software that will enable biologists to analyze their multivariate now cytometry data more efficiently and more effectively.  Flow Cytometers can now collect eight or more measurements per cell and biochemists and cell biologists have devised markers to take advantage of this multiparameter capability.  The software to enable biologists to easily analyze this multivariate data is only now beginning to be developed.  This proposal addresses the needs of investigators doing basic cell biology work and clinicians needing rapid and thorough analyses of their multivariate flow cytometry data.  Specific Aim 1 is focused on the development of device independent software for interactive, exploratory analysis of multivariate flow cytometry data. The software will use a mouse and menu user interface so that the user will not have to read bulky manuals.  The program will include two and three dimensional dynamic color graphics.  It will incorporate a statistical expert system called GRAPH HELPER to assist the user in choosing the best set of displays to examine.  In Specific Aim 2, an expert system will be developed that incorporates the knowledge of a statistician and that of a biologist to assist in the automatic processing of many datasets of multivariate flow cytometry data.  The ANALYST expert system will contain separate knowledge bases for statistical expertise and biological expertise.  The statistician knowledge base will include rules and facts about hypothesis testing, cluster analysis, dimensionality reduction, probability modeling and two and three dimensional graphical display techniques.  The biologist knowledge base will at first include rules and facts governing human differentiation antigen clusters and the fluorescent monoclonal antibodies used to detect them.  It will later include knowledge about other specific biological systems.  Specific Aim 3 is focused on the investigation of several approaches to partitioning multivariate flow cytometry data. Parametric clustering schemes using the K-means algorithm and the Mahalanobis distance metric will be investigated along with several nonparametric approaches.  Various techniques will be examined for efficiently starting the clustering process and for determining the correct number of clusters.  Classification and Regression Tree (CART) algorithms will be investigated for partitioning multivariate flow cytometry data after an initial clustering.  Clustering methods will be developed to run on the LANL Connection Machine so that data from large numbers of cells can be included in a cluster analysis during an interactive session with the user.  n/a",FLOW CYTOMETRY DATA ANALYSIS BY EXPERT SYSTEMS,3199068,R01CA054518,"['CD antigens', ' artificial intelligence', ' classification', ' computer data analysis', ' computer graphics /printing', ' computer program /software', ' computer system design /evaluation', ' flow cytometry', ' lymphocyte', ' mathematical model', ' monoclonal antibody', ' personal computers', ' statistics /biometry']",NCI,UNIVERSITY OF CALIF-LOS ALAMOS NAT LAB,R01,1991,284442,-0.03295728359474943
"Combining artificial neural network models and fMRI to study brain and language    DESCRIPTION (provided by applicant): The overall goal of this project is to supplement the applicant's existing training in functional magnetic resonance imaging (fMRI) with additional training in artificial neural network (ANN) modeling. In general, ANN models have been used to provide mechanistic accounts of behavior, while fMRI has been used to test for neural activation differences corresponding to differences in task conditions. To date there have been few efforts to combine the two approaches. Successfully doing so will enable an additional level of inference in which, on the one hand, a specific cognitive mechanism can be attributed to a particular brain activation pattern, and on the other hand, neural correlates can be ascribed to dynamic, quantitative models of behavior. Aim 1 is to develop methods for relating network activity and fMRI data. We hypothesize that successfully combining ANN models with fMRI will yield results similar to but not identical with results from more traditional task or stimulus based methods of fMRI analysis. The class of models considered here contains an input layer, an output layer, and at least one hidden layer, in addition to weighted connections between these layers. We will use an aggregate energy term to derive model-related activity associated with each stimulus presented to the model. When concatenated across stimuli, this model-derived signal will be treated as a prediction of the blood oxygen level dependent time course against which the signal in each brain image voxel will be tested. The expected advantage of the combined approach is that it should provide a clear functional interpretation of the fMRI results. Aim 2 is to apply these methods to the study of a few primary phenomena in reading aloud: word frequency effects, spelling-to-sound regularity, as well as sublexical orthographies and phonotactics. We hypothesize that network-derived regressors for these factors will yield more interpretive power than have standard regressors alone. The critical point is that interpretation of results from the combined approach is constrained by specific processing characteristics of the model from which the regressors were derived. Public Health Relevance: We aim to combine the complimentary strengths of fMRI and modeling to gain a deeper understanding of the brain, which will be invaluable in understanding and developing treatments for brain damage. This work could set the stage for a better understanding not only of language disorders, but, in principle, of any neurally-mediated disruption of behavior that can be modeled (e.g., amnesia, executive dysfunction, perceptual disorders, motor impairments).          n/a",Combining artificial neural network models and fMRI to study brain and language,7487572,F32HD056767,"['Accounting', 'Amnesia', 'Area', 'Behavior', 'Biological Neural Networks', 'Body of uterus', 'Brain', 'Brain Injuries', 'Brain imaging', 'Characteristics', 'Class', 'Cognitive', 'Condition', 'Data', 'Data Set', 'Depth', 'Disruption', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Goals', 'Hand', 'Image Analysis', 'Language', 'Language Disorders', 'Link', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Neural Network Simulation', 'Orthography', 'Output', 'Pattern', 'Perceptual Disorders', 'Positioning Attribute', 'Process', 'Public Health', 'Rate', 'Reading', 'Semantics', 'Signal Transduction', 'Staging', 'Standards of Weights and Measures', 'Stimulus', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Weight', 'Work', 'base', 'blood oxygen level dependent', 'brain behavior', 'motor impairment', 'novel', 'phonology', 'relating to nervous system', 'sound', 'spelling']",NICHD,MEDICAL COLLEGE OF WISCONSIN,F32,2008,46826,-0.001869678318191043
"Combining artificial neural network models and fMRI to study brain and language    DESCRIPTION (provided by applicant): The overall goal of this project is to supplement the applicant's existing training in functional magnetic resonance imaging (fMRI) with additional training in artificial neural network (ANN) modeling. In general, ANN models have been used to provide mechanistic accounts of behavior, while fMRI has been used to test for neural activation differences corresponding to differences in task conditions. To date there have been few efforts to combine the two approaches. Successfully doing so will enable an additional level of inference in which, on the one hand, a specific cognitive mechanism can be attributed to a particular brain activation pattern, and on the other hand, neural correlates can be ascribed to dynamic, quantitative models of behavior. Aim 1 is to develop methods for relating network activity and fMRI data. We hypothesize that successfully combining ANN models with fMRI will yield results similar to but not identical with results from more traditional task or stimulus based methods of fMRI analysis. The class of models considered here contains an input layer, an output layer, and at least one hidden layer, in addition to weighted connections between these layers. We will use an aggregate energy term to derive model-related activity associated with each stimulus presented to the model. When concatenated across stimuli, this model-derived signal will be treated as a prediction of the blood oxygen level dependent time course against which the signal in each brain image voxel will be tested. The expected advantage of the combined approach is that it should provide a clear functional interpretation of the fMRI results. Aim 2 is to apply these methods to the study of a few primary phenomena in reading aloud: word frequency effects, spelling-to-sound regularity, as well as sublexical orthographies and phonotactics. We hypothesize that network-derived regressors for these factors will yield more interpretive power than have standard regressors alone. The critical point is that interpretation of results from the combined approach is constrained by specific processing characteristics of the model from which the regressors were derived. Public Health Relevance: We aim to combine the complimentary strengths of fMRI and modeling to gain a deeper understanding of the brain, which will be invaluable in understanding and developing treatments for brain damage. This work could set the stage for a better understanding not only of language disorders, but, in principle, of any neurally-mediated disruption of behavior that can be modeled (e.g., amnesia, executive dysfunction, perceptual disorders, motor impairments).          n/a",Combining artificial neural network models and fMRI to study brain and language,7609013,F32HD056767,"['Accounting', 'Amnesia', 'Area', 'Behavior', 'Biological Neural Networks', 'Body of uterus', 'Brain', 'Brain Injuries', 'Brain imaging', 'Characteristics', 'Cognitive', 'Data', 'Data Set', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Goals', 'Hand', 'Image Analysis', 'Language', 'Language Disorders', 'Link', 'Maps', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Neural Network Simulation', 'Orthography', 'Output', 'Pattern', 'Perceptual Disorders', 'Positioning Attribute', 'Process', 'Reading', 'Semantics', 'Signal Transduction', 'Staging', 'Stimulus', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Weight', 'Work', 'base', 'blood oxygen level dependent', 'brain behavior', 'motor impairment', 'novel', 'phonology', 'public health relevance', 'relating to nervous system', 'sound', 'spelling']",NICHD,MEDICAL COLLEGE OF WISCONSIN,F32,2009,50054,-0.001869678318191043
"OPTIMIZING MAGNETIC FIELDS FOR MRI MAGNETS The intent of this proposal is to make it practical to Optimize Magnetic         Fields in Magnetic Resonance imaging (MRI) Magnets, both old and new, by         providing products and services for characterizing and correcting the            inhomogeneities of the magnetic field. The technology proposed will make         it possible to enhance the imaging performance of magnets, with particular       importance for newer MR applications, like Fast MR, Functional MR, and           Interventional MR techniques. Use of the technology to enhance the               performance of older magnets offers newer techniques and higher levels of        performance as well as extended life of the system for low cost. An              additional benefit is the prospect of independent assurance measurement of       the quality of the magnetic field.                                                                                                                                The developments proposed are based on automated, accurate, and precise          measurement and analysis of the magnetic field. The analysis can then be         used to correct the homogeneity of the magnetic field optimally by               automated control of the electrical currents driving correction coil             systems. The analysis can also be used to control new approaches to              ferromagnetic correction designs proposed here. These new ferromagnetic          correction techniques offer control well beyond that currently obtainable        either ferromagnetically or electrically.                                                                                                                         PROPOSED COMMERCIAL APPLICATION: Development of technology for automated         mapping and analysis of magnetic fields in MR magnets and use of this            technology to drive advanced designs for ferromagnetic shimming of such          magnets are proposed.                                                             n/a",OPTIMIZING MAGNETIC FIELDS FOR MRI MAGNETS,2414243,R44CA057050,"['artificial intelligence', ' biomedical equipment development', ' biosensor device', ' clinical biomedical equipment', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' image enhancement', ' magnetic field', ' magnetic resonance imaging']",NCI,"RESONANCE RESEARCH, INC.",R44,1997,194434,-0.05778634756778773
"Robust BCT for Clinical Use    DESCRIPTION (provided by applicant): Osteoporosis is a major public health threat for over 50% of the population over age 50. Despite its importance, osteoporosis is largely under-treated, with less than 20% of those recommended for testing being screened. With substantial reimbursement cuts being introduced by Medicare for bone densitometry by dual energy X-ray absorptiometry (DXA, the current clinical standard), with a sensitivity of DXA for fracture prediction of less than 50%, and with the rapidly increasing size of the aging population of the U.S., there is an urgent need for additional and more sensitive modalities than DXA for clinical assessment of fracture risk. Biomechanical Computed Tomography (BCT) has emerged as a powerful alternative to DXA. This CT-based technology creates a structural ""finite element"" model of a patient's bone from their CT scans, and subjects that model to virtual forces in order to provide an estimate of the strength of the bone. Well validated in cadaver studies and being a better predictor of bone strength than is bone mineral density by DXA, BCT has also been shown to be highly predictive of osteoporotic fractures in clinical research studies. However, robustness remains an issue - can the technique be used easily by non-experts in research and clinical environments? Addressing this issue, the overall goal of this research is to improve the robustness of our software, such that it can automatically analyze scans from a wide range of CT scanners and using a wide variety of CT acquisition protocols, including new low-dose protocols that limit radiation exposure to the patient. Such a robust BCT diagnostic tool could then be offered as a supplementary ""add-on"" analysis to many types of CT exams taken for other purposes such as CT colonography, pelvic, abdominal, and spine exams, thus reducing hospital costs, incurring no addition radiation to the patient, requiring no change in the CT acquisition protocols, and therefore greatly increasing the number of patients that could be screened at low cost. Specifically, we propose in this Phase-I project to combine expertise in computer vision, CT scanning, and biomechanics in order to develop an automated method of ""phantomless"" cross-calibration of CT scans for robust vertebral strength assessment. Focusing on the spine, our major tasks are to perform a series of clinical studies in which patients are scanned twice using a variety of CT acquisition protocols; develop a custom external-calibration phantom and use that to determine the effects of various CT acquisition parameters on scanning standardization; and use machine learning techniques to develop a ""statistical atlas"" of the spine for automation of all image processing. We will combine these efforts to develop a phantomless BCT method that accounts for differences in image quality due to variations in CT scanners and acquisition protocols, including low-dose protocols, and that does so in a highly automated fashion requiring minimal user expertise and input. Should this project be successful, future work will further refine the techniques, extend them to the hip and quantitative analysis of muscle and other soft tissues, and address robustness of longitudinal changes for clinical monitoring.  PUBLIC HEALTH RELEVANCE: With a mortality rate up to 30% one year after hip fracture, and an economic burden exceeding $17 billion annually, osteoporotic fracture is a debilitating condition whose impact on our aging society is growing. Early identification of those at risk for fracture can guide prevention and treatment, and BCT will provide a means for such detection with a sensitivity and specificity lacking in DXA based bone densitometry. The greater radiation exposure from CT, however, limits the market for such a diagnostic. The proposed project will result in a robust diagnostic test that significantly lowers radiation dose to the patient, and in some implementations, completely eliminates additional radiation by using CT scans already ordered for other medical purposes. Successful development of this product will broaden the pool of individuals who will benefit from a more accurate and sensitive fracture risk prediction, expand the market for O. N. Diagnostics' business, and result in an important advance in the preventative care and treatment of osteoporosis.           Statement of Relevance With a mortality rate up to 30% one year after hip fracture, and an economic burden exceeding $17 billion annually, osteoporotic fracture is a debilitating condition whose impact on our aging society is growing. Early identification of those at risk for fracture can guide prevention and treatment, and BCT will provide a means for such detection with a sensitivity and specificity lacking in DXA based bone densitometry. The greater radiation exposure from CT, however, limits the market for such a diagnostic. The proposed project will result in a robust diagnostic test that significantly lowers radiation dose to the patient, and in some implementations, completely eliminates additional radiation by using CT scans already ordered for other medical purposes. Successful development of this product will broaden the pool of individuals who will benefit from a more accurate and sensitive fracture risk prediction, expand the market for O. N. Diagnostics' business, and result in an important advance in the preventative care and treatment of osteoporosis.",Robust BCT for Clinical Use,7902171,R43AR057616,"['Abdomen', 'Accounting', 'Address', 'Adoption', 'Affect', 'Age', 'Aging', 'Algorithms', 'Angiography', 'Atlases', 'Automation', 'Biomechanics', 'Bone Density', 'Businesses', 'Cadaver', 'Calibration', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical assessments', 'Computed Tomographic Colonography', 'Computer Vision Systems', 'Computer software', 'Custom', 'Data', 'Densitometry', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Dose', 'Dual-Energy X-Ray Absorptiometry', 'Early identification', 'Economic Burden', 'Elderly', 'Elements', 'Environment', 'Exposure to', 'Fracture', 'Future', 'Goals', 'Growth', 'Guide prevention', 'Healthcare', 'Healthcare Systems', 'Hip Fractures', 'Hip region structure', 'Hospital Costs', 'Image', 'Individual', 'Intervertebral disc structure', 'Low Dose Radiation', 'Lung', 'Machine Learning', 'Marketing', 'Measures', 'Medical', 'Medicare', 'Methods', 'Minerals', 'Modality', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Muscle', 'Osteoporosis', 'Outcome', 'Patients', 'Pelvis', 'Performance', 'Phase', 'Population', 'Postmenopause', 'Protocols documentation', 'Public Health', 'Radiation', 'Research', 'Research Personnel', 'Risk', 'Scanning', 'Screening procedure', 'Second lumbar vertebra', 'Sensitivity and Specificity', 'Series', 'Societies', 'Standardization', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Tube', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'aging population', 'base', 'bone', 'bone strength', 'cohort', 'cost', 'cost effective', 'image processing', 'improved', 'meetings', 'mortality', 'novel', 'osteoporosis with pathological fracture', 'product development', 'public health relevance', 'reconstruction', 'research study', 'soft tissue', 'spine bone structure', 'tool', 'treatment effect', 'virtual', 'voltage']",NIAMS,"O. N. DIAGNOSTICS, LLC",R43,2010,350000,-0.007378395120471158
"Robust BCT for Clinical Use    DESCRIPTION (provided by applicant): Osteoporosis is a major public health threat for over 50% of the population over age 50. Despite its importance, osteoporosis is largely under-treated, with less than 20% of those recommended for testing being screened. With substantial reimbursement cuts being introduced by Medicare for bone densitometry by dual energy X-ray absorptiometry (DXA, the current clinical standard), with a sensitivity of DXA for fracture prediction of less than 50%, and with the rapidly increasing size of the aging population of the U.S., there is an urgent need for additional and more sensitive modalities than DXA for clinical assessment of fracture risk. Biomechanical Computed Tomography (BCT) has emerged as a powerful alternative to DXA. This CT-based technology creates a structural ""finite element"" model of a patient's bone from their CT scans, and subjects that model to virtual forces in order to provide an estimate of the strength of the bone. Well validated in cadaver studies and being a better predictor of bone strength than is bone mineral density by DXA, BCT has also been shown to be highly predictive of osteoporotic fractures in clinical research studies. However, robustness remains an issue - can the technique be used easily by non-experts in research and clinical environments? Addressing this issue, the overall goal of this research is to improve the robustness of our software, such that it can automatically analyze scans from a wide range of CT scanners and using a wide variety of CT acquisition protocols, including new low-dose protocols that limit radiation exposure to the patient. Such a robust BCT diagnostic tool could then be offered as a supplementary ""add-on"" analysis to many types of CT exams taken for other purposes such as CT colonography, pelvic, abdominal, and spine exams, thus reducing hospital costs, incurring no addition radiation to the patient, requiring no change in the CT acquisition protocols, and therefore greatly increasing the number of patients that could be screened at low cost. Specifically, we propose in this Phase-I project to combine expertise in computer vision, CT scanning, and biomechanics in order to develop an automated method of ""phantomless"" cross-calibration of CT scans for robust vertebral strength assessment. Focusing on the spine, our major tasks are to perform a series of clinical studies in which patients are scanned twice using a variety of CT acquisition protocols; develop a custom external-calibration phantom and use that to determine the effects of various CT acquisition parameters on scanning standardization; and use machine learning techniques to develop a ""statistical atlas"" of the spine for automation of all image processing. We will combine these efforts to develop a phantomless BCT method that accounts for differences in image quality due to variations in CT scanners and acquisition protocols, including low-dose protocols, and that does so in a highly automated fashion requiring minimal user expertise and input. Should this project be successful, future work will further refine the techniques, extend them to the hip and quantitative analysis of muscle and other soft tissues, and address robustness of longitudinal changes for clinical monitoring.  PUBLIC HEALTH RELEVANCE: With a mortality rate up to 30% one year after hip fracture, and an economic burden exceeding $17 billion annually, osteoporotic fracture is a debilitating condition whose impact on our aging society is growing. Early identification of those at risk for fracture can guide prevention and treatment, and BCT will provide a means for such detection with a sensitivity and specificity lacking in DXA based bone densitometry. The greater radiation exposure from CT, however, limits the market for such a diagnostic. The proposed project will result in a robust diagnostic test that significantly lowers radiation dose to the patient, and in some implementations, completely eliminates additional radiation by using CT scans already ordered for other medical purposes. Successful development of this product will broaden the pool of individuals who will benefit from a more accurate and sensitive fracture risk prediction, expand the market for O. N. Diagnostics' business, and result in an important advance in the preventative care and treatment of osteoporosis.           Statement of Relevance With a mortality rate up to 30% one year after hip fracture, and an economic burden exceeding $17 billion annually, osteoporotic fracture is a debilitating condition whose impact on our aging society is growing. Early identification of those at risk for fracture can guide prevention and treatment, and BCT will provide a means for such detection with a sensitivity and specificity lacking in DXA based bone densitometry. The greater radiation exposure from CT, however, limits the market for such a diagnostic. The proposed project will result in a robust diagnostic test that significantly lowers radiation dose to the patient, and in some implementations, completely eliminates additional radiation by using CT scans already ordered for other medical purposes. Successful development of this product will broaden the pool of individuals who will benefit from a more accurate and sensitive fracture risk prediction, expand the market for O. N. Diagnostics' business, and result in an important advance in the preventative care and treatment of osteoporosis.",Robust BCT for Clinical Use,7747873,R43AR057616,"['Abdomen', 'Accounting', 'Address', 'Adoption', 'Affect', 'Age', 'Aging', 'Algorithms', 'Angiography', 'Atlases', 'Automation', 'Biomechanics', 'Bone Density', 'Businesses', 'Cadaver', 'Calibration', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Clinical assessments', 'Computed Tomographic Colonography', 'Computer Vision Systems', 'Computer software', 'Custom', 'Data', 'Densitometry', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Dose', 'Dual-Energy X-Ray Absorptiometry', 'Early identification', 'Economic Burden', 'Elderly', 'Elements', 'Environment', 'Exposure to', 'Fracture', 'Future', 'Goals', 'Growth', 'Guide prevention', 'Healthcare', 'Healthcare Systems', 'Hip Fractures', 'Hip region structure', 'Hospital Costs', 'Image', 'Individual', 'Intervertebral disc structure', 'Low Dose Radiation', 'Lung', 'Machine Learning', 'Marketing', 'Measures', 'Medical', 'Medicare', 'Methods', 'Minerals', 'Modality', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Muscle', 'Osteoporosis', 'Outcome', 'Patients', 'Pelvis', 'Performance', 'Phase', 'Population', 'Postmenopause', 'Protocols documentation', 'Public Health', 'Radiation', 'Research', 'Research Personnel', 'Risk', 'Scanning', 'Screening procedure', 'Second lumbar vertebra', 'Sensitivity and Specificity', 'Series', 'Societies', 'Standardization', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Tube', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'aging population', 'base', 'bone', 'bone strength', 'cohort', 'cost', 'cost effective', 'image processing', 'improved', 'meetings', 'mortality', 'novel', 'osteoporosis with pathological fracture', 'product development', 'public health relevance', 'reconstruction', 'research study', 'soft tissue', 'spine bone structure', 'tool', 'treatment effect', 'virtual', 'voltage']",NIAMS,"O. N. DIAGNOSTICS, LLC",R43,2009,350000,-0.007378395120471158
"PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY Computer algorithms based on pattern recognition are being used in many areas of science and technology to assist the scientist in solving complex, time-consuming, and often tedious real-world problems.  The basic premise is to train a computer to efficiently identify a known pattern in an unknown dataset.  This needle-in-a-haystack approach is being used in the area of genomics, where there are already several examples of very powerful computational pattern recognition approaches available for searching new sequences for structural motifs, similarities to other proteins and DNA, and predicting secondary structure, based solely on the DNA or amino acid sequence.  We believe that macromolecular crystallography can also benefit from the application of pattern recognition to the often daunting task of fitting atoms into an electron density map.  The fact that electron density maps are three-dimensional images provides an additional challenge to this technology in that the procedures we are developing in order to find matching patterns must be rotation invariant.  To test the validity of our hypothesis we will complete the following aims: 1) we will develop a set of rotation invariant features that can characterize the patterns in regions of an electron density map, 2) we will determine the optimal size of feature regions and the size and type of structural database required to find similar regions of electron density capable of accurately determining structures, and 3) we will develop a methodology to synthesize matched regions to produce coherent local and global models of protein structure. If these goals can be met, we will investigate the feasibility of incorporating knowledge-based methods, neural networks, and other AI techniques to augment the interpretation of structures from electron density maps.  In addition, we will attempt to extend this methodology to produce initial structures for electron density maps that are either of poor quality and/or low resolution.  n/a",PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY,6182183,R21GM059398,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer simulation', ' computer system design /evaluation', ' electron crystallography', ' electron density', ' molecular biology information system', ' physical model', ' protein structure', ' structural biology']",NIGMS,TEXAS ENGINEERING EXPERIMENT STATION,R21,2000,101500,-0.046562444727332286
"PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY Computer algorithms based on pattern recognition are being used in many areas of science and technology to assist the scientist in solving complex, time-consuming, and often tedious real-world problems.  The basic premise is to train a computer to efficiently identify a known pattern in an unknown dataset.  This needle-in-a-haystack approach is being used in the area of genomics, where there are already several examples of very powerful computational pattern recognition approaches available for searching new sequences for structural motifs, similarities to other proteins and DNA, and predicting secondary structure, based solely on the DNA or amino acid sequence.  We believe that macromolecular crystallography can also benefit from the application of pattern recognition to the often daunting task of fitting atoms into an electron density map.  The fact that electron density maps are three-dimensional images provides an additional challenge to this technology in that the procedures we are developing in order to find matching patterns must be rotation invariant.  To test the validity of our hypothesis we will complete the following aims: 1) we will develop a set of rotation invariant features that can characterize the patterns in regions of an electron density map, 2) we will determine the optimal size of feature regions and the size and type of structural database required to find similar regions of electron density capable of accurately determining structures, and 3) we will develop a methodology to synthesize matched regions to produce coherent local and global models of protein structure. If these goals can be met, we will investigate the feasibility of incorporating knowledge-based methods, neural networks, and other AI techniques to augment the interpretation of structures from electron density maps.  In addition, we will attempt to extend this methodology to produce initial structures for electron density maps that are either of poor quality and/or low resolution.  n/a",PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY,2835580,R21GM059398,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer simulation', ' computer system design /evaluation', ' electron crystallography', ' electron density', ' molecular biology information system', ' physical model', ' protein structure', ' structural biology']",NIGMS,TEXAS ENGINEERING EXPERIMENT STATION,R21,1999,101500,-0.046562444727332286
"Action Potentials vs. Field Potentials as Inputs to a Brain-Machine Interface    DESCRIPTION (provided by applicant):      Over 600,000 Americans have severely impaired motor function from disorders including spinal cord injury, amyotrophic lateral sclerosis, pontine stroke, and cerebral palsy. A brain-machine interface (BMI) could enable locked-in or tetraplegic patients to communicate and interact with their environment. Two crucial decisions in designing a BMI are (1) what type of brain signals to use as inputs to a controller and (2) what methods to use to decode those signals. Most BMIs have used either noninvasive scalp EEG recordings or invasive intracortical recordings of single- or multi-neuron spikes as control inputs. A few have used subdural or intracortical local field potentials (LFPs). However, no group has yet systematically compared these signals in motor cortex for use in BMI applications. This proposal's first goal is to assess the relative performance of spikes and field potentials (both intracortical and epidural) as control inputs for a variety of movement-related outputs. Epidural field potentials (EFPs) are intermediate in invasiveness, signal quality, stability and spatial resolution compared with existing scalp, subdural, and intracortical recordings, and thus represent an unexplored middle ground. This proposal's second goal is to evaluate linear and nonlinear techniques-including several novel to BMI applications-for both decoding data and reducing the inherently large dimensionality of data from multiple neural signals. The primary hypotheses of the proposed project are (1) that spikes will perform better in decoding more complex movement-related outputs, but that field potentials may perform similarly on decoding simpler outputs, and (2) that nonlinear decoders and dimensionality-reduction techniques may provide improved accuracy over linear methods. The specific aims to address these hypotheses are 1) to evaluate single neuron spikes as inputs to decoders of movement- related outputs, 2) to develop a novel epidural multi-electrode recording technique in the macaque monkey, and 3) to evaluate field potential signals as inputs to decoders of movement-related outputs. Aims 1 and 3 will involve application of dimensionality-reduction algorithms (e.g., independent components analysis, Isomap) and decoding algorithms (system identification, neural networks, support vector machines) to both spikes and field potentials. Aim 2 will entail using a computer model and spatial spectral analysis to optimize the epidural electrode array design. This project will provide the first comparison of spikes, LFPs and EFPs as inputs for identical BMI output applications. The supervision of Drs. Lee Miller and W. Zev Rymer, with additional guidance from Drs. Simon Levine, Jonathan Wolpaw and Nicholas Hatsopoulos, will provide the principal investigator with expertise in recording and processing both spikes and field potentials for BMI applications using a variety of state-of-the- art techniques. A comprehensive career development plan including clinical and research mentoring, seminars, and courses, will foster the candidate's transition into an independent physician-scientist.           n/a",Action Potentials vs. Field Potentials as Inputs to a Brain-Machine Interface,7470575,K08NS060223,"['Action Potentials', 'Address', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Arts', 'Benchmarking', 'Biological Neural Networks', 'Brain', 'Cerebral Palsy', 'Cervical spinal cord injury', 'Clinical Research', 'Complex', 'Computer Simulation', 'Count', 'Craniotomy', 'Data', 'Development Plans', 'Discriminant Analysis', 'Disease', 'Disease regression', 'Dura Mater', 'Electrodes', 'Electroencephalography', 'Electromyography', 'Environment', 'Evaluation', 'Figs - dietary', 'Fostering', 'Frequencies', 'Genetic Programming', 'Goals', 'Hand', 'Implant', 'Implanted Electrodes', 'Invasive', 'Knowledge', 'Learning', 'Macaca', 'Macaca mulatta', 'Machine Learning', 'Measures', 'Mentors', 'Methods', 'Modeling', 'Monkeys', 'Motor', 'Motor Cortex', 'Movement', 'Muscle', 'Neurons', 'Operative Surgical Procedures', 'Output', 'Patients', 'Performance', 'Physicians', 'Pontine structure', 'Principal Investigator', 'Procedures', 'Process', 'Quadriplegia', 'Rattus', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Scalp structure', 'Scientist', 'Signal Transduction', 'Source', 'Spinal cord injury', 'Statistical Methods', 'Stroke', 'Sum', 'Supervision', 'System', 'Techniques', 'Time', 'Tissues', 'Training', 'Upper arm', 'Visual', 'brain machine interface', 'career', 'computerized data processing', 'cranium', 'design', 'disability', 'feeding', 'improved', 'in vivo', 'independent component analysis', 'motor control', 'novel', 'programs', 'prototype', 'relating to nervous system', 'research study', 'surgical technique development']",NINDS,NORTHWESTERN UNIVERSITY AT CHICAGO,K08,2008,167832,-0.03659902283458855
"Action Potentials vs. Field Potentials as Inputs to a Brain-Machine Interface    DESCRIPTION (provided by applicant):      Over 600,000 Americans have severely impaired motor function from disorders including spinal cord injury, amyotrophic lateral sclerosis, pontine stroke, and cerebral palsy. A brain-machine interface (BMI) could enable locked-in or tetraplegic patients to communicate and interact with their environment. Two crucial decisions in designing a BMI are (1) what type of brain signals to use as inputs to a controller and (2) what methods to use to decode those signals. Most BMIs have used either noninvasive scalp EEG recordings or invasive intracortical recordings of single- or multi-neuron spikes as control inputs. A few have used subdural or intracortical local field potentials (LFPs). However, no group has yet systematically compared these signals in motor cortex for use in BMI applications. This proposal's first goal is to assess the relative performance of spikes and field potentials (both intracortical and epidural) as control inputs for a variety of movement-related outputs. Epidural field potentials (EFPs) are intermediate in invasiveness, signal quality, stability and spatial resolution compared with existing scalp, subdural, and intracortical recordings, and thus represent an unexplored middle ground. This proposal's second goal is to evaluate linear and nonlinear techniques-including several novel to BMI applications-for both decoding data and reducing the inherently large dimensionality of data from multiple neural signals. The primary hypotheses of the proposed project are (1) that spikes will perform better in decoding more complex movement-related outputs, but that field potentials may perform similarly on decoding simpler outputs, and (2) that nonlinear decoders and dimensionality-reduction techniques may provide improved accuracy over linear methods. The specific aims to address these hypotheses are 1) to evaluate single neuron spikes as inputs to decoders of movement- related outputs, 2) to develop a novel epidural multi-electrode recording technique in the macaque monkey, and 3) to evaluate field potential signals as inputs to decoders of movement-related outputs. Aims 1 and 3 will involve application of dimensionality-reduction algorithms (e.g., independent components analysis, Isomap) and decoding algorithms (system identification, neural networks, support vector machines) to both spikes and field potentials. Aim 2 will entail using a computer model and spatial spectral analysis to optimize the epidural electrode array design. This project will provide the first comparison of spikes, LFPs and EFPs as inputs for identical BMI output applications. The supervision of Drs. Lee Miller and W. Zev Rymer, with additional guidance from Drs. Simon Levine, Jonathan Wolpaw and Nicholas Hatsopoulos, will provide the principal investigator with expertise in recording and processing both spikes and field potentials for BMI applications using a variety of state-of-the- art techniques. A comprehensive career development plan including clinical and research mentoring, seminars, and courses, will foster the candidate's transition into an independent physician-scientist.           n/a",Action Potentials vs. Field Potentials as Inputs to a Brain-Machine Interface,8091226,K08NS060223,"['Action Potentials', 'Address', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Benchmarking', 'Biological Neural Networks', 'Brain', 'Cerebral Palsy', 'Cervical spinal cord injury', 'Clinical Research', 'Complex', 'Computer Simulation', 'Craniotomy', 'Data', 'Development Plans', 'Discriminant Analysis', 'Disease', 'Dura Mater', 'Electrodes', 'Electroencephalography', 'Environment', 'Evaluation', 'Figs - dietary', 'Fostering', 'Frequencies', 'Genetic Programming', 'Goals', 'Hand', 'Implant', 'Implanted Electrodes', 'Knowledge', 'Learning', 'Macaca', 'Macaca mulatta', 'Machine Learning', 'Measures', 'Mentors', 'Methods', 'Modeling', 'Monkeys', 'Motor', 'Motor Cortex', 'Movement', 'Muscle', 'Neurons', 'Operative Surgical Procedures', 'Output', 'Patients', 'Performance', 'Physicians', 'Pontine structure', 'Principal Investigator', 'Procedures', 'Process', 'Quadriplegia', 'Rattus', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Scalp structure', 'Scientist', 'Signal Transduction', 'Source', 'Spinal cord injury', 'Statistical Methods', 'Stroke', 'Sum', 'Supervision', 'System', 'Techniques', 'Time', 'Tissues', 'Training', 'Visual', 'arm', 'brain machine interface', 'career development', 'computerized data processing', 'cranium', 'design', 'disability', 'feeding', 'flexibility', 'improved', 'in vivo', 'independent component analysis', 'motor control', 'novel', 'programs', 'prototype', 'relating to nervous system', 'research study', 'surgical technique development']",NINDS,NORTHWESTERN UNIVERSITY AT CHICAGO,K08,2011,167832,-0.03659902283458855
"Action Potentials vs. Field Potentials as Inputs to a Brain-Machine Interface    DESCRIPTION (provided by applicant):      Over 600,000 Americans have severely impaired motor function from disorders including spinal cord injury, amyotrophic lateral sclerosis, pontine stroke, and cerebral palsy. A brain-machine interface (BMI) could enable locked-in or tetraplegic patients to communicate and interact with their environment. Two crucial decisions in designing a BMI are (1) what type of brain signals to use as inputs to a controller and (2) what methods to use to decode those signals. Most BMIs have used either noninvasive scalp EEG recordings or invasive intracortical recordings of single- or multi-neuron spikes as control inputs. A few have used subdural or intracortical local field potentials (LFPs). However, no group has yet systematically compared these signals in motor cortex for use in BMI applications. This proposal's first goal is to assess the relative performance of spikes and field potentials (both intracortical and epidural) as control inputs for a variety of movement-related outputs. Epidural field potentials (EFPs) are intermediate in invasiveness, signal quality, stability and spatial resolution compared with existing scalp, subdural, and intracortical recordings, and thus represent an unexplored middle ground. This proposal's second goal is to evaluate linear and nonlinear techniques-including several novel to BMI applications-for both decoding data and reducing the inherently large dimensionality of data from multiple neural signals. The primary hypotheses of the proposed project are (1) that spikes will perform better in decoding more complex movement-related outputs, but that field potentials may perform similarly on decoding simpler outputs, and (2) that nonlinear decoders and dimensionality-reduction techniques may provide improved accuracy over linear methods. The specific aims to address these hypotheses are 1) to evaluate single neuron spikes as inputs to decoders of movement- related outputs, 2) to develop a novel epidural multi-electrode recording technique in the macaque monkey, and 3) to evaluate field potential signals as inputs to decoders of movement-related outputs. Aims 1 and 3 will involve application of dimensionality-reduction algorithms (e.g., independent components analysis, Isomap) and decoding algorithms (system identification, neural networks, support vector machines) to both spikes and field potentials. Aim 2 will entail using a computer model and spatial spectral analysis to optimize the epidural electrode array design. This project will provide the first comparison of spikes, LFPs and EFPs as inputs for identical BMI output applications. The supervision of Drs. Lee Miller and W. Zev Rymer, with additional guidance from Drs. Simon Levine, Jonathan Wolpaw and Nicholas Hatsopoulos, will provide the principal investigator with expertise in recording and processing both spikes and field potentials for BMI applications using a variety of state-of-the- art techniques. A comprehensive career development plan including clinical and research mentoring, seminars, and courses, will foster the candidate's transition into an independent physician-scientist.           n/a",Action Potentials vs. Field Potentials as Inputs to a Brain-Machine Interface,7876844,K08NS060223,"['Action Potentials', 'Address', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Arts', 'Benchmarking', 'Biological Neural Networks', 'Brain', 'Cerebral Palsy', 'Cervical spinal cord injury', 'Clinical Research', 'Complex', 'Computer Simulation', 'Craniotomy', 'Data', 'Development Plans', 'Discriminant Analysis', 'Disease', 'Dura Mater', 'Electrodes', 'Electroencephalography', 'Environment', 'Evaluation', 'Figs - dietary', 'Fostering', 'Frequencies', 'Genetic Programming', 'Goals', 'Hand', 'Implant', 'Implanted Electrodes', 'Knowledge', 'Learning', 'Macaca', 'Macaca mulatta', 'Machine Learning', 'Measures', 'Mentors', 'Methods', 'Modeling', 'Monkeys', 'Motor', 'Motor Cortex', 'Movement', 'Muscle', 'Neurons', 'Operative Surgical Procedures', 'Output', 'Patients', 'Performance', 'Physicians', 'Pontine structure', 'Principal Investigator', 'Procedures', 'Process', 'Quadriplegia', 'Rattus', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Scalp structure', 'Scientist', 'Signal Transduction', 'Source', 'Spinal cord injury', 'Statistical Methods', 'Stroke', 'Sum', 'Supervision', 'System', 'Techniques', 'Time', 'Tissues', 'Training', 'Visual', 'arm', 'brain machine interface', 'career development', 'computerized data processing', 'cranium', 'design', 'disability', 'feeding', 'flexibility', 'improved', 'in vivo', 'independent component analysis', 'motor control', 'novel', 'programs', 'prototype', 'relating to nervous system', 'research study', 'surgical technique development']",NINDS,NORTHWESTERN UNIVERSITY AT CHICAGO,K08,2010,167832,-0.03659902283458855
"Action Potentials vs. Field Potentials as Inputs to a Brain-Machine Interface    DESCRIPTION (provided by applicant):      Over 600,000 Americans have severely impaired motor function from disorders including spinal cord injury, amyotrophic lateral sclerosis, pontine stroke, and cerebral palsy. A brain-machine interface (BMI) could enable locked-in or tetraplegic patients to communicate and interact with their environment. Two crucial decisions in designing a BMI are (1) what type of brain signals to use as inputs to a controller and (2) what methods to use to decode those signals. Most BMIs have used either noninvasive scalp EEG recordings or invasive intracortical recordings of single- or multi-neuron spikes as control inputs. A few have used subdural or intracortical local field potentials (LFPs). However, no group has yet systematically compared these signals in motor cortex for use in BMI applications. This proposal's first goal is to assess the relative performance of spikes and field potentials (both intracortical and epidural) as control inputs for a variety of movement-related outputs. Epidural field potentials (EFPs) are intermediate in invasiveness, signal quality, stability and spatial resolution compared with existing scalp, subdural, and intracortical recordings, and thus represent an unexplored middle ground. This proposal's second goal is to evaluate linear and nonlinear techniques-including several novel to BMI applications-for both decoding data and reducing the inherently large dimensionality of data from multiple neural signals. The primary hypotheses of the proposed project are (1) that spikes will perform better in decoding more complex movement-related outputs, but that field potentials may perform similarly on decoding simpler outputs, and (2) that nonlinear decoders and dimensionality-reduction techniques may provide improved accuracy over linear methods. The specific aims to address these hypotheses are 1) to evaluate single neuron spikes as inputs to decoders of movement- related outputs, 2) to develop a novel epidural multi-electrode recording technique in the macaque monkey, and 3) to evaluate field potential signals as inputs to decoders of movement-related outputs. Aims 1 and 3 will involve application of dimensionality-reduction algorithms (e.g., independent components analysis, Isomap) and decoding algorithms (system identification, neural networks, support vector machines) to both spikes and field potentials. Aim 2 will entail using a computer model and spatial spectral analysis to optimize the epidural electrode array design. This project will provide the first comparison of spikes, LFPs and EFPs as inputs for identical BMI output applications. The supervision of Drs. Lee Miller and W. Zev Rymer, with additional guidance from Drs. Simon Levine, Jonathan Wolpaw and Nicholas Hatsopoulos, will provide the principal investigator with expertise in recording and processing both spikes and field potentials for BMI applications using a variety of state-of-the- art techniques. A comprehensive career development plan including clinical and research mentoring, seminars, and courses, will foster the candidate's transition into an independent physician-scientist.           n/a",Action Potentials vs. Field Potentials as Inputs to a Brain-Machine Interface,7643089,K08NS060223,"['Action Potentials', 'Address', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Arts', 'Benchmarking', 'Biological Neural Networks', 'Brain', 'Cerebral Palsy', 'Cervical spinal cord injury', 'Clinical Research', 'Complex', 'Computer Simulation', 'Craniotomy', 'Data', 'Development Plans', 'Discriminant Analysis', 'Disease', 'Dura Mater', 'Electrodes', 'Electroencephalography', 'Environment', 'Evaluation', 'Figs - dietary', 'Fostering', 'Frequencies', 'Genetic Programming', 'Goals', 'Hand', 'Implant', 'Implanted Electrodes', 'Knowledge', 'Learning', 'Macaca', 'Macaca mulatta', 'Machine Learning', 'Measures', 'Mentors', 'Methods', 'Modeling', 'Monkeys', 'Motor', 'Motor Cortex', 'Movement', 'Muscle', 'Neurons', 'Operative Surgical Procedures', 'Output', 'Patients', 'Performance', 'Physicians', 'Pontine structure', 'Principal Investigator', 'Procedures', 'Process', 'Quadriplegia', 'Rattus', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Scalp structure', 'Scientist', 'Signal Transduction', 'Source', 'Spinal cord injury', 'Statistical Methods', 'Stroke', 'Sum', 'Supervision', 'System', 'Techniques', 'Time', 'Tissues', 'Training', 'Upper arm', 'Visual', 'brain machine interface', 'career development', 'computerized data processing', 'cranium', 'design', 'disability', 'feeding', 'flexibility', 'improved', 'in vivo', 'independent component analysis', 'motor control', 'novel', 'programs', 'prototype', 'relating to nervous system', 'research study', 'surgical technique development']",NINDS,NORTHWESTERN UNIVERSITY AT CHICAGO,K08,2009,167832,-0.03659902283458855
"Action Potentials vs. Field Potentials as Inputs to a Brain-Machine Interface    DESCRIPTION (provided by applicant):      Over 600,000 Americans have severely impaired motor function from disorders including spinal cord injury, amyotrophic lateral sclerosis, pontine stroke, and cerebral palsy. A brain-machine interface (BMI) could enable locked-in or tetraplegic patients to communicate and interact with their environment. Two crucial decisions in designing a BMI are (1) what type of brain signals to use as inputs to a controller and (2) what methods to use to decode those signals. Most BMIs have used either noninvasive scalp EEG recordings or invasive intracortical recordings of single- or multi-neuron spikes as control inputs. A few have used subdural or intracortical local field potentials (LFPs). However, no group has yet systematically compared these signals in motor cortex for use in BMI applications. This proposal's first goal is to assess the relative performance of spikes and field potentials (both intracortical and epidural) as control inputs for a variety of movement-related outputs. Epidural field potentials (EFPs) are intermediate in invasiveness, signal quality, stability and spatial resolution compared with existing scalp, subdural, and intracortical recordings, and thus represent an unexplored middle ground. This proposal's second goal is to evaluate linear and nonlinear techniques-including several novel to BMI applications-for both decoding data and reducing the inherently large dimensionality of data from multiple neural signals. The primary hypotheses of the proposed project are (1) that spikes will perform better in decoding more complex movement-related outputs, but that field potentials may perform similarly on decoding simpler outputs, and (2) that nonlinear decoders and dimensionality-reduction techniques may provide improved accuracy over linear methods. The specific aims to address these hypotheses are 1) to evaluate single neuron spikes as inputs to decoders of movement- related outputs, 2) to develop a novel epidural multi-electrode recording technique in the macaque monkey, and 3) to evaluate field potential signals as inputs to decoders of movement-related outputs. Aims 1 and 3 will involve application of dimensionality-reduction algorithms (e.g., independent components analysis, Isomap) and decoding algorithms (system identification, neural networks, support vector machines) to both spikes and field potentials. Aim 2 will entail using a computer model and spatial spectral analysis to optimize the epidural electrode array design. This project will provide the first comparison of spikes, LFPs and EFPs as inputs for identical BMI output applications. The supervision of Drs. Lee Miller and W. Zev Rymer, with additional guidance from Drs. Simon Levine, Jonathan Wolpaw and Nicholas Hatsopoulos, will provide the principal investigator with expertise in recording and processing both spikes and field potentials for BMI applications using a variety of state-of-the- art techniques. A comprehensive career development plan including clinical and research mentoring, seminars, and courses, will foster the candidate's transition into an independent physician-scientist.           n/a",Action Potentials vs. Field Potentials as Inputs to a Brain-Machine Interface,7318680,K08NS060223,"['Action Potentials', 'Address', 'Algorithms', 'American', 'Amyotrophic Lateral Sclerosis', 'Arts', 'Benchmarking', 'Biological Neural Networks', 'Brain', 'Cerebral Palsy', 'Cervical spinal cord injury', 'Clinical Research', 'Complex', 'Computer Simulation', 'Count', 'Craniotomy', 'Data', 'Development Plans', 'Discriminant Analysis', 'Disease', 'Disease regression', 'Dura Mater', 'Electrodes', 'Electroencephalography', 'Electromyography', 'Environment', 'Evaluation', 'Figs - dietary', 'Fostering', 'Frequencies', 'Genetic Programming', 'Goals', 'Hand', 'Implant', 'Implanted Electrodes', 'Invasive', 'Knowledge', 'Learning', 'Macaca', 'Macaca mulatta', 'Machine Learning', 'Measures', 'Mentors', 'Methods', 'Modeling', 'Monkeys', 'Motor', 'Motor Cortex', 'Movement', 'Muscle', 'Neurons', 'Operative Surgical Procedures', 'Output', 'Patients', 'Performance', 'Physicians', 'Pontine structure', 'Principal Investigator', 'Procedures', 'Process', 'Quadriplegia', 'Rattus', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Scalp structure', 'Scientist', 'Signal Transduction', 'Source', 'Spinal cord injury', 'Statistical Methods', 'Stroke', 'Sum', 'Supervision', 'System', 'Techniques', 'Time', 'Tissues', 'Training', 'Upper arm', 'Visual', 'brain machine interface', 'career', 'computerized data processing', 'cranium', 'design', 'disability', 'feeding', 'improved', 'in vivo', 'independent component analysis', 'motor control', 'novel', 'programs', 'prototype', 'relating to nervous system', 'research study', 'surgical technique development']",NINDS,NORTHWESTERN UNIVERSITY AT CHICAGO,K08,2007,167832,-0.03659902283458855
"Bayesian Methods for Localizing Dynamic Brain Activity and Epileptogenic Zones    DESCRIPTION (provided by applicant): Magnetoencephalography (MEG) and related electroencephalography (EEG) use an array of sensors to non-invasively measure electromagnetic (EM) fields produced by synchronous current activity within the brain. While the temporal resolution is excellent relative to other functional imaging modalities, accurately localizing in 3D space the sources of brain activity involves solving a difficult, underdetermined inverse problem. Existing localization methods used clinically and for research purposes maintain significant shortcomings, including the inability to resolve complex source configurations, bias caused by source correlations, and sensitivity to sources of noise and interference. The latter can arise from eye blinks, heart beats, sensor imperfections, and industrial noise as well as from spontaneous background brain activity not associated with the brain sources of interest. Additionally, prototype algorithms ostensibly designed to deal with some of these issues are heuristic in nature and have not been rigorously evaluated or compared, making their ultimate utility difficult to assess for neuroelectromagnetic imaging practitioners. The proposed research plan addresses all of these concerns by developing a principled localization scheme that unifies and extends existing localization strategies using modern concepts from Bayesian statistics and machine learning. Based on the notion of automatic relevance determination (ARD), brain regions with probable (relevant) activity are located with high spatial resolution. Interference sources are effectively removed by integrating with a variation factor analysis model. To quantify the improvement afforded by the proposed methodology, source location estimates will be compared with standard algorithms using realistic simulations, near-ground-truth data obtained from invasive electrocorticographic (ECoG) recordings, and surgical data. The result will be implemented as a user-friendly localization toolbox and made freely available to the community by integrating with existing open-source functional brain imaging software. Non-invasive mapping of brain activity with high spatio-temporal resolution has important consequences for basic neuroscience studies of human cognition. It also has profound implications for the diagnosis, characterization and treatment of various neurological, neurooncological, mental health, developmental, and communication disorders. For example, localizations of brain sources are used to map cognitive function in epileptogenic areas and in neighboring brain regions. Such brain mapping procedures are then useful to guide neurosurgical planning, navigation, and resection and to minimize post-operative deficits.           n/a",Bayesian Methods for Localizing Dynamic Brain Activity and Epileptogenic Zones,7942859,F32NS061395,"['Academia', 'Address', 'Algorithms', 'Area', 'Automation', 'Bayesian Method', 'Benchmarking', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Engineering', 'Blinking', 'Brain', 'Brain Mapping', 'Brain imaging', 'Brain region', 'Clinical', 'Code', 'Cognition', 'Cognitive Science', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Developmental Communication Disorders', 'Diagnosis', 'Diffuse', 'Electroencephalography', 'Electromagnetic Fields', 'Epilepsy', 'Evaluation', 'Event', 'Excision', 'Experimental Designs', 'Exposure to', 'Factor Analysis', 'Failure', 'Frequencies', 'Functional Imaging', 'Heart', 'Human', 'Image', 'Individual', 'Interdisciplinary Study', 'Intractable Epilepsy', 'Language', 'Lead', 'Learning', 'Location', 'Machine Learning', 'Magnetoencephalography', 'Manuals', 'Maps', 'Measurement', 'Measures', 'Mental Health', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Morphologic artifacts', 'Motor', 'Nature', 'Neurologic', 'Neurosciences', 'Noise', 'Occupations', 'Operative Surgical Procedures', 'Partial Epilepsies', 'Patients', 'Pattern', 'Performance', 'Positioning Attribute', 'Postoperative Period', 'Procedures', 'Radiology Specialty', 'Relative (related person)', 'Research', 'Research Training', 'Resected', 'Resolution', 'Scalp structure', 'Scheme', 'Science', 'Series', 'Signal Transduction', 'Simulate', 'Sorting - Cell Movement', 'Source', 'Statistical Methods', 'Surface', 'Surrogate Markers', 'Techniques', 'Testing', 'Time', 'Tissues', 'Training', 'United States National Institutes of Health', 'Validation', 'Variant', 'Visual', 'Work', 'base', 'career', 'cognitive function', 'computerized data processing', 'cost', 'design', 'genetic pedigree', 'heuristics', 'human CYP2B6 protein', 'human subject', 'imaging modality', 'interest', 'neurophysiology', 'open source', 'operation', 'prototype', 'reconstruction', 'sensor', 'simulation', 'statistics', 'user friendly software', 'user-friendly', 'validation studies']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",F32,2011,19755,-0.02748811362214258
"Bayesian Methods for Localizing Dynamic Brain Activity and Epileptogenic Zones    DESCRIPTION (provided by applicant): Magnetoencephalography (MEG) and related electroencephalography (EEG) use an array of sensors to non-invasively measure electromagnetic (EM) fields produced by synchronous current activity within the brain. While the temporal resolution is excellent relative to other functional imaging modalities, accurately localizing in 3D space the sources of brain activity involves solving a difficult, underdetermined inverse problem. Existing localization methods used clinically and for research purposes maintain significant shortcomings, including the inability to resolve complex source configurations, bias caused by source correlations, and sensitivity to sources of noise and interference. The latter can arise from eye blinks, heart beats, sensor imperfections, and industrial noise as well as from spontaneous background brain activity not associated with the brain sources of interest. Additionally, prototype algorithms ostensibly designed to deal with some of these issues are heuristic in nature and have not been rigorously evaluated or compared, making their ultimate utility difficult to assess for neuroelectromagnetic imaging practitioners. The proposed research plan addresses all of these concerns by developing a principled localization scheme that unifies and extends existing localization strategies using modern concepts from Bayesian statistics and machine learning. Based on the notion of automatic relevance determination (ARD), brain regions with probable (relevant) activity are located with high spatial resolution. Interference sources are effectively removed by integrating with a variation factor analysis model. To quantify the improvement afforded by the proposed methodology, source location estimates will be compared with standard algorithms using realistic simulations, near-ground-truth data obtained from invasive electrocorticographic (ECoG) recordings, and surgical data. The result will be implemented as a user-friendly localization toolbox and made freely available to the community by integrating with existing open-source functional brain imaging software. Non-invasive mapping of brain activity with high spatio-temporal resolution has important consequences for basic neuroscience studies of human cognition. It also has profound implications for the diagnosis, characterization and treatment of various neurological, neurooncological, mental health, developmental, and communication disorders. For example, localizations of brain sources are used to map cognitive function in epileptogenic areas and in neighboring brain regions. Such brain mapping procedures are then useful to guide neurosurgical planning, navigation, and resection and to minimize post-operative deficits.           n/a",Bayesian Methods for Localizing Dynamic Brain Activity and Epileptogenic Zones,7751495,F32NS061395,"['Academia', 'Address', 'Algorithms', 'Area', 'Arts', 'Automation', 'Bayesian Method', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Engineering', 'Blinking', 'Brain', 'Brain Mapping', 'Brain imaging', 'Brain region', 'Clinical', 'Code', 'Cognition', 'Cognitive Science', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Developmental Communication Disorders', 'Diagnosis', 'Diffuse', 'Electroencephalography', 'Electromagnetic Fields', 'Electromagnetics', 'Epilepsy', 'Evaluation', 'Event', 'Excision', 'Experimental Designs', 'Exposure to', 'Factor Analysis', 'Failure', 'Frequencies', 'Functional Imaging', 'Heart', 'Human', 'Image', 'Individual', 'Interdisciplinary Study', 'Intractable Epilepsy', 'Language', 'Lead', 'Learning', 'Location', 'Machine Learning', 'Magnetoencephalography', 'Manuals', 'Maps', 'Measurement', 'Measures', 'Mental Health', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Morphologic artifacts', 'Motor', 'Nature', 'Neurologic', 'Neurosciences', 'Noise', 'Nutmeg - dietary', 'Occupations', 'Operative Surgical Procedures', 'Partial Epilepsies', 'Patients', 'Pattern', 'Performance', 'Positioning Attribute', 'Postoperative Period', 'Procedures', 'Radiology Specialty', 'Relative (related person)', 'Research', 'Research Training', 'Resected', 'Resolution', 'Scalp structure', 'Scheme', 'Science', 'Series', 'Signal Transduction', 'Simulate', 'Sorting - Cell Movement', 'Source', 'Statistical Methods', 'Surface', 'Surrogate Markers', 'Techniques', 'Testing', 'Time', 'Tissues', 'Training', 'United States National Institutes of Health', 'Validation', 'Variant', 'Visual', 'Work', 'base', 'career', 'cognitive function', 'computerized data processing', 'cost', 'design', 'genetic pedigree', 'heuristics', 'human CYP2B6 protein', 'human subject', 'imaging modality', 'interest', 'neurophysiology', 'open source', 'prototype', 'reconstruction', 'sensor', 'simulation', 'statistics', 'user friendly software', 'user-friendly', 'validation studies']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",F32,2009,46257,-0.02748811362214258
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6636516,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2003,374063,0.012517517556563563
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6520333,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2002,363261,0.012517517556563563
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6484619,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2001,337739,0.012517517556563563
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6193019,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN,R01,2000,478050,0.012517517556563563
"Neurophysiology of Human Cortical Epilepsy ABSTRACT Epilepsy remains a devastating and poorly understood illness whose therapies are inadequate for many patients and, in large degree, unchanged for decades. The experiments proposed in this project utilize novel microelectrode recording techniques in patients with epilepsy as well as quantitative features and large data sets to obtain information about the neuronal dynamics underlying epilepsy at an unprecedented level of resolution. The primary hypothesis of this project is that this high resolution, multi-scale information can be applied to separate seizures into different classes which differ in the mechanisms which underlie seizure initiation. More specifically, we will be examining the role and interplay of widespread networks, different cortical layers, infraslow activity and both excitatory and inhibitory single neuronal activity as seizures start. We expect to find substantial differences in these different features of neural action in different kinds of seizures. This knowledge will foster the development of a more complete understanding of seizures and how they can be better detected, predicted and ultimately controlled. PROJECT NARRATIVE The experiments proposed in this project utilize novel microelectrode recording techniques as well as automatic clustering approaches based on quantitative features and large data sets to obtain information about the neuronal dynamics underlying epilepsy at an unprecedented level of resolution. We hope to use these assessments to better understand multiscale neurophysiological processes and demonstrate that there are different types of seizures which have contrasting mechanisms underlying their initiation. This knowledge will foster the development of a more complete understanding of the seizure and how it can be better detected, predicted and treated.",Neurophysiology of Human Cortical Epilepsy,9546165,R01NS062092,"['Adopted', 'Adverse effects', 'Affect', 'Age of Onset', 'Amplifiers', 'Area', 'Behavior', 'Biological Neural Networks', 'Brain', 'Classification', 'Clinical', 'Data', 'Data Set', 'Databases', 'Development', 'Epilepsy', 'Etiology', 'Focal Seizure', 'Fostering', 'Foundations', 'Goals', 'High Frequency Oscillation', 'Human', 'Individual', 'Intervention', 'Investigation', 'Knowledge', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Microelectrodes', 'Mind', 'Neurons', 'Patients', 'Pharmaceutical Preparations', 'Physiologic pulse', 'Physiological', 'Physiology', 'Probability', 'Procedures', 'Process', 'Regression Analysis', 'Research', 'Resolution', 'Role', 'Seizures', 'Synapses', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Utah', 'Work', 'base', 'experimental study', 'neurophysiology', 'novel', 'novel therapeutic intervention', 'personalized approach', 'relating to nervous system', 'response', 'spatiotemporal', 'success', 'surgery outcome', 'voltage']",NINDS,MASSACHUSETTS GENERAL HOSPITAL,R01,2018,672554,-0.02415874954170298
"Neurophysiology of Human Cortical Epilepsy ABSTRACT Epilepsy remains a devastating and poorly understood illness whose therapies are inadequate for many patients and, in large degree, unchanged for decades. The experiments proposed in this project utilize novel microelectrode recording techniques in patients with epilepsy as well as quantitative features and large data sets to obtain information about the neuronal dynamics underlying epilepsy at an unprecedented level of resolution. The primary hypothesis of this project is that this high resolution, multi-scale information can be applied to separate seizures into different classes which differ in the mechanisms which underlie seizure initiation. More specifically, we will be examining the role and interplay of widespread networks, different cortical layers, infraslow activity and both excitatory and inhibitory single neuronal activity as seizures start. We expect to find substantial differences in these different features of neural action in different kinds of seizures. This knowledge will foster the development of a more complete understanding of seizures and how they can be better detected, predicted and ultimately controlled. PROJECT NARRATIVE The experiments proposed in this project utilize novel microelectrode recording techniques as well as automatic clustering approaches based on quantitative features and large data sets to obtain information about the neuronal dynamics underlying epilepsy at an unprecedented level of resolution. We hope to use these assessments to better understand multiscale neurophysiological processes and demonstrate that there are different types of seizures which have contrasting mechanisms underlying their initiation. This knowledge will foster the development of a more complete understanding of the seizure and how it can be better detected, predicted and treated.",Neurophysiology of Human Cortical Epilepsy,9928130,R01NS062092,"['Adopted', 'Affect', 'Age of Onset', 'Amplifiers', 'Area', 'Behavior', 'Brain', 'Classification', 'Clinical', 'Data', 'Databases', 'Development', 'Epilepsy', 'Etiology', 'Focal Seizure', 'Fostering', 'Foundations', 'Goals', 'High Frequency Oscillation', 'Human', 'Individual', 'Intervention', 'Investigation', 'Knowledge', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Microelectrodes', 'Mind', 'Neurons', 'Patients', 'Pharmaceutical Preparations', 'Physiologic pulse', 'Physiological', 'Physiology', 'Probability', 'Procedures', 'Process', 'Regression Analysis', 'Research', 'Resolution', 'Role', 'Seizures', 'Synapses', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Utah', 'Work', 'base', 'experimental study', 'large datasets', 'neural network', 'neurophysiology', 'novel', 'novel therapeutic intervention', 'personalized approach', 'relating to nervous system', 'response', 'side effect', 'spatiotemporal', 'success', 'surgery outcome', 'voltage']",NINDS,MASSACHUSETTS GENERAL HOSPITAL,R01,2020,615074,-0.02415874954170298
"Neurophysiology of Human Cortical Epilepsy ABSTRACT Epilepsy remains a devastating and poorly understood illness whose therapies are inadequate for many patients and, in large degree, unchanged for decades. The experiments proposed in this project utilize novel microelectrode recording techniques in patients with epilepsy as well as quantitative features and large data sets to obtain information about the neuronal dynamics underlying epilepsy at an unprecedented level of resolution. The primary hypothesis of this project is that this high resolution, multi-scale information can be applied to separate seizures into different classes which differ in the mechanisms which underlie seizure initiation. More specifically, we will be examining the role and interplay of widespread networks, different cortical layers, infraslow activity and both excitatory and inhibitory single neuronal activity as seizures start. We expect to find substantial differences in these different features of neural action in different kinds of seizures. This knowledge will foster the development of a more complete understanding of seizures and how they can be better detected, predicted and ultimately controlled. PROJECT NARRATIVE The experiments proposed in this project utilize novel microelectrode recording techniques as well as automatic clustering approaches based on quantitative features and large data sets to obtain information about the neuronal dynamics underlying epilepsy at an unprecedented level of resolution. We hope to use these assessments to better understand multiscale neurophysiological processes and demonstrate that there are different types of seizures which have contrasting mechanisms underlying their initiation. This knowledge will foster the development of a more complete understanding of the seizure and how it can be better detected, predicted and treated.",Neurophysiology of Human Cortical Epilepsy,9767289,R01NS062092,"['Adopted', 'Affect', 'Age of Onset', 'Amplifiers', 'Area', 'Behavior', 'Brain', 'Classification', 'Clinical', 'Data', 'Data Set', 'Databases', 'Development', 'Epilepsy', 'Etiology', 'Focal Seizure', 'Fostering', 'Foundations', 'Goals', 'High Frequency Oscillation', 'Human', 'Individual', 'Intervention', 'Investigation', 'Knowledge', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Microelectrodes', 'Mind', 'Neurons', 'Patients', 'Pharmaceutical Preparations', 'Physiologic pulse', 'Physiological', 'Physiology', 'Probability', 'Procedures', 'Process', 'Regression Analysis', 'Research', 'Resolution', 'Role', 'Seizures', 'Synapses', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Utah', 'Work', 'base', 'experimental study', 'neural network', 'neurophysiology', 'novel', 'novel therapeutic intervention', 'personalized approach', 'relating to nervous system', 'response', 'side effect', 'spatiotemporal', 'success', 'surgery outcome', 'voltage']",NINDS,MASSACHUSETTS GENERAL HOSPITAL,R01,2019,623939,-0.02415874954170298
"Neurophysiology of Human Cortical Epilepsy ABSTRACT Epilepsy remains a devastating and poorly understood illness whose therapies are inadequate for many patients and, in large degree, unchanged for decades. The experiments proposed in this project utilize novel microelectrode recording techniques in patients with epilepsy as well as quantitative features and large data sets to obtain information about the neuronal dynamics underlying epilepsy at an unprecedented level of resolution. The primary hypothesis of this project is that this high resolution, multi-scale information can be applied to separate seizures into different classes which differ in the mechanisms which underlie seizure initiation. More specifically, we will be examining the role and interplay of widespread networks, different cortical layers, infraslow activity and both excitatory and inhibitory single neuronal activity as seizures start. We expect to find substantial differences in these different features of neural action in different kinds of seizures. This knowledge will foster the development of a more complete understanding of seizures and how they can be better detected, predicted and ultimately controlled. PROJECT NARRATIVE The experiments proposed in this project utilize novel microelectrode recording techniques as well as automatic clustering approaches based on quantitative features and large data sets to obtain information about the neuronal dynamics underlying epilepsy at an unprecedented level of resolution. We hope to use these assessments to better understand multiscale neurophysiological processes and demonstrate that there are different types of seizures which have contrasting mechanisms underlying their initiation. This knowledge will foster the development of a more complete understanding of the seizure and how it can be better detected, predicted and treated.",Neurophysiology of Human Cortical Epilepsy,9513187,R56NS062092,"['Adopted', 'Adverse effects', 'Affect', 'Age of Onset', 'Amplifiers', 'Area', 'Behavior', 'Biological Neural Networks', 'Brain', 'Classification', 'Clinical', 'Data', 'Data Set', 'Databases', 'Development', 'Epilepsy', 'Etiology', 'Focal Seizure', 'Fostering', 'Foundations', 'Goals', 'High Frequency Oscillation', 'Human', 'Individual', 'Intervention', 'Investigation', 'Knowledge', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Microelectrodes', 'Mind', 'Neurons', 'Operative Surgical Procedures', 'Outcome', 'Patients', 'Physiologic pulse', 'Physiological', 'Physiology', 'Probability', 'Procedures', 'Process', 'Regression Analysis', 'Research', 'Resolution', 'Role', 'Seizures', 'Synapses', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Utah', 'Work', 'base', 'experimental study', 'neurophysiology', 'novel', 'relating to nervous system', 'response', 'spatiotemporal', 'success', 'voltage']",NINDS,MASSACHUSETTS GENERAL HOSPITAL,R56,2017,607712,-0.02415874954170298
"Referential Contrast Effects in Language Processing   DESCRIPTION (provided by applicant): A great deal of work in sentence                processing over the years has dealt with the question of whether contextual          information can guide language processing. There is by now considerable              evidence that suggests that some kinds of information from the discourse             context have immediate effects. However, the question of how these effects           occur has remained largely ignored. The studies in this proposal focus on the        discourse properties of modified definite noun phrases. Work in sentence             processing has shown that the resolution of ambiguities in which one of the          possible readings involves noun modification is affected by the availability of      a discourse model in which the modificational phrase serves to distinguish           between two possible referents. A central question is whether such discourse         effects found with modifiers reflect a general, conventionalized property of         modification, or whether they are more aptly characterized as a more subtle          system based on expectations regarding typical usages. A series of studies is        proposed to investigate the hypothesis that a typical default expression exists      for neutral (i.e., non-contrastive) contexts, and that the use of a more             informative expression signals a contrastive function in the discourse, with         immediate processing consequences. Data will come from elicited production           tasks, on-line comprehension experiments which monitor subjects' eye movements       to a visual array in response to spoken linguistic stimuli, traditional reading      time studies, and prosodic analyses in a read-aloud task. The current proposal       represents a significant departure from existing work in two salient ways:           First, it attempts to provide a detailed investigation into the nature of the        referential effects, using a methodology that is especially well-suited for          studying referential aspects of language. Second, whereas previous findings          have been couched almost exclusively in terms of the mechanisms of sentence          processing, the current proposal seeks to integrate experiments from on-line         language processing and language production. Results of this project may be          useful in developing models for language disorders, for the development of           pedagogical tools, and for progress in artificial intelligence.                                                                                                           n/a",Referential Contrast Effects in Language Processing,6615572,R01MH062566,"['clinical research', ' computer data analysis', ' data collection methodology /evaluation', ' eye movements', ' human subject', ' language', ' language development', ' neural information processing', ' phonology', ' reading', ' speech', ' syntax', ' visual stimulus', ' visual tracking']",NIMH,BROWN UNIVERSITY,R01,2003,114544,-0.05217267597636824
"Referential Contrast Effects in Language Processing   DESCRIPTION (provided by applicant): A great deal of work in sentence                processing over the years has dealt with the question of whether contextual          information can guide language processing. There is by now considerable              evidence that suggests that some kinds of information from the discourse             context have immediate effects. However, the question of how these effects           occur has remained largely ignored. The studies in this proposal focus on the        discourse properties of modified definite noun phrases. Work in sentence             processing has shown that the resolution of ambiguities in which one of the          possible readings involves noun modification is affected by the availability of      a discourse model in which the modificational phrase serves to distinguish           between two possible referents. A central question is whether such discourse         effects found with modifiers reflect a general, conventionalized property of         modification, or whether they are more aptly characterized as a more subtle          system based on expectations regarding typical usages. A series of studies is        proposed to investigate the hypothesis that a typical default expression exists      for neutral (i.e., non-contrastive) contexts, and that the use of a more             informative expression signals a contrastive function in the discourse, with         immediate processing consequences. Data will come from elicited production           tasks, on-line comprehension experiments which monitor subjects' eye movements       to a visual array in response to spoken linguistic stimuli, traditional reading      time studies, and prosodic analyses in a read-aloud task. The current proposal       represents a significant departure from existing work in two salient ways:           First, it attempts to provide a detailed investigation into the nature of the        referential effects, using a methodology that is especially well-suited for          studying referential aspects of language. Second, whereas previous findings          have been couched almost exclusively in terms of the mechanisms of sentence          processing, the current proposal seeks to integrate experiments from on-line         language processing and language production. Results of this project may be          useful in developing models for language disorders, for the development of           pedagogical tools, and for progress in artificial intelligence.                                                                                                           n/a",Referential Contrast Effects in Language Processing,6539200,R01MH062566,"['clinical research', ' computer data analysis', ' data collection methodology /evaluation', ' eye movements', ' human subject', ' language', ' language development', ' neural information processing', ' phonology', ' reading', ' speech', ' syntax', ' visual stimulus', ' visual tracking']",NIMH,BROWN UNIVERSITY,R01,2002,152849,-0.05217267597636824
"Referential Contrast Effects in Language Processing   DESCRIPTION (provided by applicant): A great deal of work in sentence                processing over the years has dealt with the question of whether contextual          information can guide language processing. There is by now considerable              evidence that suggests that some kinds of information from the discourse             context have immediate effects. However, the question of how these effects           occur has remained largely ignored. The studies in this proposal focus on the        discourse properties of modified definite noun phrases. Work in sentence             processing has shown that the resolution of ambiguities in which one of the          possible readings involves noun modification is affected by the availability of      a discourse model in which the modificational phrase serves to distinguish           between two possible referents. A central question is whether such discourse         effects found with modifiers reflect a general, conventionalized property of         modification, or whether they are more aptly characterized as a more subtle          system based on expectations regarding typical usages. A series of studies is        proposed to investigate the hypothesis that a typical default expression exists      for neutral (i.e., non-contrastive) contexts, and that the use of a more             informative expression signals a contrastive function in the discourse, with         immediate processing consequences. Data will come from elicited production           tasks, on-line comprehension experiments which monitor subjects' eye movements       to a visual array in response to spoken linguistic stimuli, traditional reading      time studies, and prosodic analyses in a read-aloud task. The current proposal       represents a significant departure from existing work in two salient ways:           First, it attempts to provide a detailed investigation into the nature of the        referential effects, using a methodology that is especially well-suited for          studying referential aspects of language. Second, whereas previous findings          have been couched almost exclusively in terms of the mechanisms of sentence          processing, the current proposal seeks to integrate experiments from on-line         language processing and language production. Results of this project may be          useful in developing models for language disorders, for the development of           pedagogical tools, and for progress in artificial intelligence.                                                                                                           n/a",Referential Contrast Effects in Language Processing,6399830,R01MH062566,"['clinical research', ' computer data analysis', ' data collection methodology /evaluation', ' eye movements', ' human subject', ' language', ' language development', ' neural information processing', ' phonology', ' reading', ' speech', ' syntax', ' visual stimulus', ' visual tracking']",NIMH,BROWN UNIVERSITY,R01,2001,145788,-0.05217267597636824
"Multi-Resolution Docking Methods for Electron Microscopy Summary In the past decade, we have witnessed a revolutionary progress in camera technology and the attainable resolution of macromolecular assemblies via cryogenic electron microscopy (cryo-EM) and in the development of computational algorithms that relate the resulting 3D maps to atomic resolution structures. Whereas single- particle cryo-EM today is capable of directly solving atomic structures of biomolecular assemblies in isolation, electron tomography (ET) in unstained frozen-hydrated samples is widely used to capture the 3D organization of supramolecular complexes in their native (organelle, cell, or tissue) environments. We have identified three inter-related research areas where our computational modeling experience (historically rooted in pre-revolution multi-scale approaches) offers the biggest value to today's post-revolution EM community: (1) medium resolution cryo-EM modeling, (2) the segmentation and denoising of cryo-ET data, and (3) the validation of atomic models and their corresponding maps. The first aim is an extension of promising new ideas in flexible fitting as well as secondary structure prediction for medium resolution maps, which have been our key research areas in the past. medium resolution (5-10Å) maps are still widely used in EM and can be of significant biological importance. This is particularly true in the case of cryo-ET maps, which are harder to read than single particle cryo-EM maps because they often exhibit considerable noise, anisotropic resolution, and anisotropic density variations due to the low dose requirements and the missing wedge in the Fourier space. In the case of tightly packed or crowded macromolecular structures, the fusion of nearby biomolecular densities prevents an automated segmentation of geometric shapes, requiring a labor-intensive manual tracing by human experts. We are currently developing novel computational approaches to provide a more objective strategy for missing wedge correction in homogeneous specimen areas of tomograms. Our hybrid approach combines deconvolution and denoising with template matching in a unified mathematical framework that allows modeling constraints to be imposed in a least-squares optimization process. Our approach can also be extended to the flexible refinement of atomic structures using our damped dynamics flexible fitting approach by tuning the internal point-spread functions to the missing wedge of the ET data. To support these aims, we will quantitatively measure the fitness of an atomic model in local density regions and characterize the fitness of maps with reliable reference structures. The collaborative efforts supported by this grant will include the refinement of cytoskeletal filaments, molecular motors, bacterial chemoreceptor arrays, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established Internet-based mechanisms used by the Situs and Sculptor packages and as plugins for the popular UCSF Chimera graphics program. Project Narrative This project will help biological electron microscopists bridge a broad range of resolution levels, from the atomic to the living organism. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,10120245,R01GM062968,"['3-Dimensional', 'Algorithms', 'Architecture', 'Area', 'Biological', 'Cells', 'Characteristics', 'Chemoreceptors', 'Chimera organism', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Computational algorithm', 'Computer Models', 'Computer software', 'Computing Methodologies', 'Crowding', 'Cryo-electron tomography', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Databases', 'Deposition', 'Detection', 'Development', 'Docking', 'Dose', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Elements', 'Environment', 'Equilibrium', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Human', 'Hybrids', 'Hydration status', 'Internet', 'Laboratories', 'Least-Squares Analysis', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Mathematics', 'Measures', 'Medical', 'Methods', 'Microscope', 'Modeling', 'Modernization', 'Molecular Motors', 'Molecular Structure', 'Morphologic artifacts', 'Nature', 'Noise', 'Organelles', 'Organism', 'Pattern', 'Plant Roots', 'Research', 'Resolution', 'Sampling', 'Shapes', 'Specimen', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Tomogram', 'Training', 'Validation', 'Variant', 'Visualization software', 'Work', 'algorithmic methodologies', 'automated segmentation', 'base', 'beta pleated sheet', 'computer code', 'cryogenics', 'data warehouse', 'deep learning', 'denoising', 'density', 'electron tomography', 'experience', 'feature detection', 'fitness', 'flexibility', 'fundamental research', 'heuristics', 'high standard', 'image reconstruction', 'improved', 'interest', 'learning network', 'macromolecular assembly', 'novel', 'particle', 'prevent', 'process optimization', 'programs', 'reconstruction', 'structured data', 'theories', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2020,313572,0.02729414536696851
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9517061,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Hydration status', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Medical', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Reproducibility', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'algorithmic methodologies', 'base', 'computer code', 'cryogenics', 'data warehouse', 'density', 'design', 'fiber cell', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2018,306284,0.01230892386940475
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9306122,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Drug Targeting', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Hydration status', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Medical', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Modernization', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Reproducibility', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'algorithmic methodologies', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fiber cell', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2017,306527,0.01230892386940475
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages. PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.",Multi-Resolution Docking Methods for Electron Microscopy,9099858,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Heating', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Statistical Data Interpretation', 'Stereocilium', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'microscopic imaging', 'new technology', 'next generation', 'programs', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2016,306754,0.01230892386940475
"Multi-Resolution Docking Methods for Electron Microscopy ﻿    DESCRIPTION (provided by applicant): In the past decade, significant progress was made in 3D imaging of macromolecular assemblies via electron microscopy and in the development of computational algorithms that relate the resulting volumetric maps to atomic-resolution structures. The overall goal of the proposed research is to further develop computational fitting and validation tools for electron microscopy (EM). We intend to establish new modeling, visualization, and simulation techniques that would serve as bridges between atomic structures and EM densities. The proposed multi-scale software will aid in the routine determination of large-scale structures of biomolecular assemblies and in the validation of structural models that will be deposited to public databases such as the Protein Data Bank (PDB) and the EM Data Bank (EMDB). Key questions to be addressed include the following: (i) How can one improve, validate, and disseminate well-established matching algorithms for intermediate-resolution (8-15 Å) cryo-electron microscopy? (ii) How can one accurately identify and segment geometric features of subcellular assemblies in low-resolution (4-5 nm) cryo-electron tomograms or in focused ion beam milling of resin-embedded specimen blocks? (iii) Given the recent increase in resolution achieved with direct detection cameras, how can one systematically characterize high-resolution (2-10 Å) density patterns and validate atomic models based on local signatures in the data? We will adapt a new modeling paradigm for these studies, namely simultaneous refinement of multiple subunits. This approach is based on a ""systems"" perspective because biological assemblies exhibit ""emergent behavior"" in the spatial domain, that is, the whole is more than the sum of its parts. The new paradigm, in combination with docking protocols, improves model accuracy and opens the door to new global fitting applications in the above three areas. In addition, we will use statistical analysis and machine learning of local signatures to complement the global strategies. The collaborative efforts supported by this grant will include refinement of cytoskeletal filaments, molecular motors, chromatin fibers, and hair cell stereocilia. The algorithmic and methodological developments will be distributed freely through the established internet-based mechanisms used by the Situs and Sculptor packages.         PUBLIC HEALTH RELEVANCE: This project helps biological electron microscopists bridge a broad range of resolution levels from atomic to living organism-level. Macromolecular assemblies are the basic functional units of biological cells; they furnish targets for drug design because deficiencies in macromolecular assembly architecture are frequently linked to health problems. The results of our fundamental research will be new computer codes for modeling macromolecular assemblies, the structures of which facilitate the prediction of medically relevant functions.            ",Multi-Resolution Docking Methods for Electron Microscopy,8964685,R01GM062968,"['Address', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'Biological', 'Cells', 'Characteristics', 'Chromatin Fiber', 'Code', 'Collaborations', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Computer-Assisted Image Analysis', 'Cryoelectron Microscopy', 'Cytoskeletal Filaments', 'Data', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Discipline', 'Docking', 'Drug Design', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Exhibits', 'Feedback', 'Filament', 'Freezing', 'Funding', 'Goals', 'Grant', 'Hair Cells', 'Health', 'Heating', 'Image', 'Imagery', 'Internet', 'Ions', 'Laboratories', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Membrane', 'Methods', 'Microtubules', 'Modeling', 'Molecular', 'Molecular Motors', 'Noise', 'Organism', 'Pattern', 'Pattern Recognition', 'Plant Resins', 'Proteins', 'Protocols documentation', 'Relative (related person)', 'Research', 'Resolution', 'Scanning Electron Microscopy', 'Series', 'Specimen', 'Stereocilium', 'Structural Models', 'Structure', 'Sum', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Tomogram', 'Training', 'Validation', 'Vesicle', 'base', 'computer code', 'cryogenics', 'density', 'design', 'fitness', 'fundamental research', 'high standard', 'image reconstruction', 'improved', 'in vivo', 'insight', 'macromolecular assembly', 'new technology', 'next generation', 'programs', 'public health relevance', 'reconstruction', 'relating to nervous system', 'simulation', 'statistics', 'tomography', 'tool']",NIGMS,OLD DOMINION UNIVERSITY,R01,2015,307928,0.01230892386940475
"Single cell activation dynamics as a predictor and regulator of aged MuSC dysfunction. Project Summary During tissue repair, many stem cell populations undergo a dynamic phenotypic change from a quiescent state to an activated state. In muscle stem cells (MuSCs), this dynamic activation process is essential for effective tissue regeneration. Despite the conserved nature of these activation processes, the dynamics of stem cell activation and their contribution to disease states remains largely unknown. We have generated single cell assays that allowed us to study state transitions of adult and aged MuSCs during activation. These results support a conceptual view of the aged stem cell phenotype as a combination of pathological steady-states and deficiencies in cell state dynamics. This provides us with the opportunity to identify factors that rejuvenate MuSC function during aging. In this project we will examine the role of physiological rejuvenation interventions on MuSC heterogeneity and activation state transitions. Understanding how rejuvenation interventions control MuSC activation response is critical for the effective treatment of the ever-expanding aged population. Project Narrative Satellite cells or Muscle stem cells (MuSCs) are heterogeneous at the molecular and function level. Aging leads to a loss of MuSC function through a delay in activation at the level of molecular and behavioral kinetics. We have developed an exciting and novel project that will identify rejuvenation interventions that activation state transitions at the single cell level.",Single cell activation dynamics as a predictor and regulator of aged MuSC dysfunction.,9891934,R21AG063416,"['Adult', 'Aging', 'Back', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Assay', 'Biological Assay', 'Biological Markers', 'Caloric Restriction', 'Cell Aging', 'Cell physiology', 'Cells', 'Cellular Assay', 'Complement', 'Data', 'Development', 'Disease', 'Exercise', 'Exploratory/Developmental Grant', 'Exposure to', 'Fasting', 'Functional disorder', 'Future', 'Genetic Transcription', 'Heterogeneity', 'Intermittent fasting', 'Intervention', 'Investigation', 'Kinetics', 'Lead', 'Machine Learning', 'Maps', 'Methods', 'Molecular', 'Muscle satellite cell', 'Muscular Atrophy', 'Nature', 'Parabiosis', 'Pathologic', 'Phenotype', 'Physiological', 'Plasma', 'Process', 'Proxy', 'RNA', 'Rejuvenation', 'Research', 'Role', 'Series', 'Skeletal Muscle', 'Time', 'Tissues', 'aged', 'aging population', 'base', 'biomarker panel', 'cell age', 'cell behavior', 'effective therapy', 'falls', 'genetic variant', 'improved', 'learning classifier', 'muscle aging', 'novel', 'regenerative', 'response', 'satellite cell', 'single-cell RNA sequencing', 'stem cell population', 'stem cells', 'tissue regeneration', 'tissue repair', 'tool', 'transcriptome', 'transcriptomics']",NIA,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R21,2020,156625,-0.008124814561438614
"Single cell activation dynamics as a predictor and regulator of aged MuSC dysfunction. Project Summary During tissue repair, many stem cell populations undergo a dynamic phenotypic change from a quiescent state to an activated state. In muscle stem cells (MuSCs), this dynamic activation process is essential for effective tissue regeneration. Despite the conserved nature of these activation processes, the dynamics of stem cell activation and their contribution to disease states remains largely unknown. We have generated single cell assays that allowed us to study state transitions of adult and aged MuSCs during activation. These results support a conceptual view of the aged stem cell phenotype as a combination of pathological steady-states and deficiencies in cell state dynamics. This provides us with the opportunity to identify factors that rejuvenate MuSC function during aging. In this project we will examine the role of physiological rejuvenation interventions on MuSC heterogeneity and activation state transitions. Understanding how rejuvenation interventions control MuSC activation response is critical for the effective treatment of the ever-expanding aged population. Project Narrative Satellite cells or Muscle stem cells (MuSCs) are heterogeneous at the molecular and function level. Aging leads to a loss of MuSC function through a delay in activation at the level of molecular and behavioral kinetics. We have developed an exciting and novel project that will identify rejuvenation interventions that activation state transitions at the single cell level.",Single cell activation dynamics as a predictor and regulator of aged MuSC dysfunction.,9756173,R21AG063416,"['Adult', 'Aging', 'Back', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Assay', 'Biological Assay', 'Biological Markers', 'Caloric Restriction', 'Cell Aging', 'Cell physiology', 'Cells', 'Cellular Assay', 'Complement', 'Data', 'Development', 'Disease', 'Exercise', 'Exploratory/Developmental Grant', 'Exposure to', 'Fasting', 'Functional disorder', 'Future', 'Genetic Transcription', 'Heterogeneity', 'Intermittent fasting', 'Intervention', 'Investigation', 'Kinetics', 'Lead', 'Machine Learning', 'Maps', 'Methods', 'Molecular', 'Muscle satellite cell', 'Muscular Atrophy', 'Nature', 'Parabiosis', 'Pathologic', 'Phenotype', 'Physiological', 'Plasma', 'Process', 'Proxy', 'RNA', 'Rejuvenation', 'Research', 'Role', 'Series', 'Skeletal Muscle', 'Stem cells', 'Time', 'Tissues', 'aged', 'aging population', 'base', 'biomarker panel', 'cell age', 'cell behavior', 'effective therapy', 'falls', 'genetic variant', 'improved', 'muscle aging', 'novel', 'regenerative', 'response', 'satellite cell', 'single-cell RNA sequencing', 'stem cell population', 'tissue regeneration', 'tissue repair', 'tool', 'transcriptome', 'transcriptomics']",NIA,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R21,2019,268627,-0.008124814561438614
COMPUTER AIDED CARDIAC MEASUREMENT The objective of the proposed research is to develop a method for providing fast and accurate measurements of volume and ejection fraction from 3D echo images. Our Computer Aided Measurement System will reconstruct the LV endocardium using a few user selected points on oriented echocardiographic images together with prior shape and size knowledge. The Specific Aims for Phase I are: 1. To improve the accuracy of quantitative echo while minimizing manual labor. 2. To improve the ease of use of the prototype system for application in a clinical setting. 3. To expand the catalog representing our knowledge base by acquiring additional large volume and abnormally shaped LV's in order to enhance fitting accuracy for atypical shapes. Previously described methods of analyzing echocardiograms in 3D require so much manual labor that this modality has been limited to research applications. The advantages of our proposed approach are that it makes the superior accuracy and reproducibility of 3D echo available for clinical practice. Furthermore this process will be applicable to other imaging modalities. PROPOSED COMMERCIAL APPLICATIONS: This research will lead to products which can be sold to echocardiography system manufacturers and end users. The products will provide accurate and convenient value measurements.  n/a,COMPUTER AIDED CARDIAC MEASUREMENT,6211201,R43HL065827,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' echocardiography', ' heart dimension /size', ' heart disorder diagnosis', ' human data', ' image processing', ' measurement']",NHLBI,"QUANTIGRAPHICS, INC.",R43,2000,100000,-0.05699867146467056
"New Kinetic Approaches to Investigate Protein Folding  DESCRIPTION (provided by applicant): Proteins must fold into specific three-dimensional structures to be functional, in a process dictated by their primary sequence. Current understanding of the mechanisms by which proteins fold is limited by the deceivingly simple picture arising from standard kinetic experiments. In these experiments, folding appears as a two or three-state process because the inter-conversions between the myriad of intermediate structures that determine the mechanism are too transient to be directly detected. In this proposal, a group of new experimental approaches to circumvent these limitation is presented. To facilitate extracting mechanistic information from kinetics observations, a catalog of small proteins with simple structural patterns; i.e., structural archetypes, will be produced. Their folding properties will be investigated by fast-kinetic methods such as the laser-induced temperature-jump technique. In an alternative approach, the existence of the theoretically predicted downhill scenario for folding will be explored experimentally. Identification of downhill folders is important because during downhill folding all intermediate structures are potentially detectable. Additionally, kinetic methods with improved structural and/or time resolution will be developed. A two-dimensional version of the phi-analysis will be implemented to investigate the population dynamics of transition-state ensembles for folding. Time-dependent information on transient intermediates will be obtained for the first time from equilibrium nuclear magnetic resonance hydrogen-exchange experiments by performing them in kinetic coupling mode. The application of these kinetic techniques to study the folding of structural archetypes and downhill folders will provide direct information about the structural rules governing protein folding.     n/a",New Kinetic Approaches to Investigate Protein Folding,7226235,R01GM066800,"['Behavior', 'Cataloging', 'Catalogs', 'Complex', 'Computer Simulation', 'Coupling', 'Dimensions', 'Elements', 'Equilibrium', 'Goals', 'Hydrogen', 'Kinetics', 'Lasers', 'Measures', 'Methods', 'Molecular Conformation', 'Monitor', 'Nature', 'Nuclear Magnetic Resonance', 'Pathway interactions', 'Pattern', 'Population', 'Population Dynamics', 'Positioning Attribute', 'Process', 'Property', 'Protein Engineering', 'Proteins', 'Range', 'Rate', 'Reporter', 'Research', 'Resolution', 'Series', 'Standards of Weights and Measures', 'Structure', 'Techniques', 'Testing', 'Thermodynamics', 'Time', 'Urea', 'base', 'design', 'improved', 'protein folding', 'protein structure', 'research study', 'residence', 'stop flow technique', 'temperature jump', 'text searching', 'theories', 'three dimensional structure', 'two-dimensional']",NIGMS,UNIVERSITY OF MARYLAND COLLEGE PK CAMPUS,R01,2007,241173,-0.012049650722111692
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,7107885,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2006,385718,-0.026622744833449195
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6949109,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2005,395000,-0.026622744833449195
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6797879,R01MH067204,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'functional magnetic resonance imaging', 'human subject', 'mathematics', 'phantom model', 'technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2004,395000,-0.026622744833449195
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6663283,R01MH067204,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain imaging /visualization /scanning', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' functional magnetic resonance imaging', ' human subject', ' mathematics', ' phantom model', ' technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2003,395000,-0.026622744833449195
"New Wavelet-based and Source Separation Methods for fMRI  DESCRIPTION (provided by applicant): Available methods of analysis for functional Magnetic Resonance Imaging offer a wealth of possibilities to researchers using this neuroimaging modality. However, these tools suffer from the inherent low signal to noise ratio of the data, and from the limitations of widely used model-based approaches. These problems have been addressed by the community and the literature now describes numerous methods that can remove part of the noise and extract brain activity pattern in a data-driven fashion. This project focuses on the design of optimized algorithms for the estimation and removal of the noise, on the understanding of the applicability of existing data-driven approaches, and on the development of new blind source separation methods for fMRI data. Particular attention will be given to quantification of the gains provided by the newly proposed methods by working on simulated datasets and specifically designed fMRI experiments. The first specific aim is to use a spatio-temporal four-dimensional multiresolution analysis to define an ""'ideal denoising"" scheme for a given study. It will make extensive use of the concept of best wavelet packet basis, which allows the most efficient representation of a signal. The concept wilt first be validated on fMRI rest datasets, and its efficiency will then be measured on simulated and actual data. The second specific aim focuses on blind source separation methods. An in depth study of Independent Component Analysis will be carried out to precisely define its field of applicability on fMRI data. By using sparsity together with time-frequency methods, we will develop new source separation algorithms and will demonstrate their robustness on both simulated and real data.   n/a",New Wavelet-based and Source Separation Methods for fMRI,6554738,R01MH067204,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain imaging /visualization /scanning', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' functional magnetic resonance imaging', ' human subject', ' mathematics', ' method development', ' phantom model', ' technology /technique development']",NIMH,PRINCETON UNIVERSITY,R01,2002,395000,-0.026622744833449195
"Optimization of Folding and Threading Proteins    DESCRIPTION (provided by applicant):  Determining protein structure and function from genomic sequences and protein classification remains one of the most significant challenges in modern computational biology.  Significant enhancement to the capacity of algorithms to predict protein shapes from sequences is proposed, focusing on major bottlenecks; e.g., the folding energy and the ability of making approximate matches.  Algorithms to determine protein shapes from sequences have two major components: The first component (sampling) generates a set of plausible protein shapes; at least one of the sampled shapes is expected to be similar to the correct fold.  The second component scores the different structures and decides on the best model.  The radius of convergence of the energy function must be sufficiently large so that approximate matches will be detected as well (in threading approximate matches may include deletions and insertion).  It is therefore clear that poor scoring functions (or energies), which are unable to identify the correct fold, are likely to diminish the capacity of the folding algorithm.  At present, it is easy to generate a set of decoy (wrong) structures that will confuse existing energy functions.  Mathematical programming and machine learning techniques (Support Vector Machines) will design enhanced folding and threading potentials.  The training by these methods is automated and will lead to monotonic improvement in recognition as a function of the data size.  To more effectively cover protein space, the goal is to learn 100 million data points in a single consistent potential with (at most) 10,000 parameters.  The automated large scale learning is crucial at times in which the information on sequences and structures grows rapidly.  A threading prediction server, based on the old and the new potentials, is and will be available, to the community at http://ser-loopp.tc.cornell.edu/Ioopp.html         n/a",Optimization of Folding and Threading Proteins,7173005,R01GM067823,"['Algorithms', 'Amino Acid Sequence', 'Biological', 'Biological Models', 'Canis familiaris', 'Classification', 'Code', 'Collaborations', 'Communities', 'Computational Biology', 'Data', 'Development', 'Family', 'Genome', 'Genomics', 'Goals', 'Lead', 'Learning', 'Machine Learning', 'Membrane', 'Membrane Proteins', 'Methods', 'Modeling', 'Modems', 'Noise', 'Numbers', 'Object Attachment', 'Peptide Sequence Determination', 'Procedures', 'Proteins', 'Protocols documentation', 'Relative (related person)', 'Research', 'Research Personnel', 'Sampling', 'Score', 'Sequence Alignment', 'Shapes', 'Signal Transduction', 'Simulate', 'Structure', 'System', 'Techniques', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'improved', 'insertion/deletion mutation', 'method development', 'programs', 'protein folding', 'protein structure', 'protein structure function', 'protein structure prediction', 'radius bone structure', 'size', 'tool']",NIGMS,CORNELL UNIVERSITY,R01,2007,102767,-0.0139670392659843
"Optimization of Folding and Threading Proteins    DESCRIPTION (provided by applicant):  Determining protein structure and function from genomic sequences and protein classification remains one of the most significant challenges in modern computational biology.  Significant enhancement to the capacity of algorithms to predict protein shapes from sequences is proposed, focusing on major bottlenecks; e.g., the folding energy and the ability of making approximate matches.  Algorithms to determine protein shapes from sequences have two major components: The first component (sampling) generates a set of plausible protein shapes; at least one of the sampled shapes is expected to be similar to the correct fold.  The second component scores the different structures and decides on the best model.  The radius of convergence of the energy function must be sufficiently large so that approximate matches will be detected as well (in threading approximate matches may include deletions and insertion).  It is therefore clear that poor scoring functions (or energies), which are unable to identify the correct fold, are likely to diminish the capacity of the folding algorithm.  At present, it is easy to generate a set of decoy (wrong) structures that will confuse existing energy functions.  Mathematical programming and machine learning techniques (Support Vector Machines) will design enhanced folding and threading potentials.  The training by these methods is automated and will lead to monotonic improvement in recognition as a function of the data size.  To more effectively cover protein space, the goal is to learn 100 million data points in a single consistent potential with (at most) 10,000 parameters.  The automated large scale learning is crucial at times in which the information on sequences and structures grows rapidly.  A threading prediction server, based on the old and the new potentials, is and will be available, to the community at http://ser-loopp.tc.cornell.edu/Ioopp.html         n/a",Optimization of Folding and Threading Proteins,7486680,R01GM067823,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Bioinformatics', 'Categories', 'Classification', 'Code', 'Collaborations', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Electrostatics', 'Employee Strikes', 'Evaluation', 'Facility Construction Funding Category', 'Genomics', 'Goals', 'Grant', 'Homology Modeling', 'Humulus', 'Hydrogen Bonding', 'Hydrophobicity', 'Lead', 'Learning', 'Machine Learning', 'Martes zibellina', 'Methods', 'Modeling', 'Molecular Conformation', 'Output', 'Pattern', 'Pediatric Hospitals', 'Peptide Sequence Determination', 'Performance', 'Physics', 'Point Mutation', 'Principal Investigator', 'Protein Dynamics', 'Proteins', 'Research', 'Resolution', 'Roentgen Rays', 'Sampling', 'Score', 'Sequence Analysis', 'Services', 'Shapes', 'Signal Transduction', 'Solvents', 'Structure', 'Surface', 'Techniques', 'Testing', 'Time', 'Torsion', 'Training', 'University Hospitals', 'base', 'design', 'desire', 'insertion/deletion mutation', 'molecular dynamics', 'novel', 'programs', 'protein structure', 'protein structure function', 'protein structure prediction', 'radius bone structure', 'research study', 'simulation', 'size', 'tool']",NIGMS,"UNIVERSITY OF TEXAS, AUSTIN",R01,2007,132856,-0.0139670392659843
"Optimization of Folding and Threading Proteins    DESCRIPTION (provided by applicant):  Determining protein structure and function from genomic sequences and protein classification remains one of the most significant challenges in modern computational biology.  Significant enhancement to the capacity of algorithms to predict protein shapes from sequences is proposed, focusing on major bottlenecks; e.g., the folding energy and the ability of making approximate matches.  Algorithms to determine protein shapes from sequences have two major components: The first component (sampling) generates a set of plausible protein shapes; at least one of the sampled shapes is expected to be similar to the correct fold.  The second component scores the different structures and decides on the best model.  The radius of convergence of the energy function must be sufficiently large so that approximate matches will be detected as well (in threading approximate matches may include deletions and insertion).  It is therefore clear that poor scoring functions (or energies), which are unable to identify the correct fold, are likely to diminish the capacity of the folding algorithm.  At present, it is easy to generate a set of decoy (wrong) structures that will confuse existing energy functions.  Mathematical programming and machine learning techniques (Support Vector Machines) will design enhanced folding and threading potentials.  The training by these methods is automated and will lead to monotonic improvement in recognition as a function of the data size.  To more effectively cover protein space, the goal is to learn 100 million data points in a single consistent potential with (at most) 10,000 parameters.  The automated large scale learning is crucial at times in which the information on sequences and structures grows rapidly.  A threading prediction server, based on the old and the new potentials, is and will be available, to the community at http://ser-loopp.tc.cornell.edu/Ioopp.html         n/a",Optimization of Folding and Threading Proteins,7011260,R01GM067823,"['bioinformatics', 'chemical models', 'computational biology', 'molecular shape', 'protein folding', 'protein sequence']",NIGMS,CORNELL UNIVERSITY ITHACA,R01,2006,254672,-0.0139670392659843
"Optimization of Folding and Threading Proteins    DESCRIPTION (provided by applicant):  Determining protein structure and function from genomic sequences and protein classification remains one of the most significant challenges in modern computational biology.  Significant enhancement to the capacity of algorithms to predict protein shapes from sequences is proposed, focusing on major bottlenecks; e.g., the folding energy and the ability of making approximate matches.  Algorithms to determine protein shapes from sequences have two major components: The first component (sampling) generates a set of plausible protein shapes; at least one of the sampled shapes is expected to be similar to the correct fold.  The second component scores the different structures and decides on the best model.  The radius of convergence of the energy function must be sufficiently large so that approximate matches will be detected as well (in threading approximate matches may include deletions and insertion).  It is therefore clear that poor scoring functions (or energies), which are unable to identify the correct fold, are likely to diminish the capacity of the folding algorithm.  At present, it is easy to generate a set of decoy (wrong) structures that will confuse existing energy functions.  Mathematical programming and machine learning techniques (Support Vector Machines) will design enhanced folding and threading potentials.  The training by these methods is automated and will lead to monotonic improvement in recognition as a function of the data size.  To more effectively cover protein space, the goal is to learn 100 million data points in a single consistent potential with (at most) 10,000 parameters.  The automated large scale learning is crucial at times in which the information on sequences and structures grows rapidly.  A threading prediction server, based on the old and the new potentials, is and will be available, to the community at http://ser-loopp.tc.cornell.edu/Ioopp.html         n/a",Optimization of Folding and Threading Proteins,6846573,R01GM067823,"['bioinformatics', 'chemical models', 'computational biology', 'molecular shape', 'protein folding', 'protein sequence']",NIGMS,CORNELL UNIVERSITY ITHACA,R01,2005,260361,-0.0139670392659843
"Optimization of Folding and Threading Proteins    DESCRIPTION (provided by applicant):  Determining protein structure and function from genomic sequences and protein classification remains one of the most significant challenges in modern computational biology.  Significant enhancement to the capacity of algorithms to predict protein shapes from sequences is proposed, focusing on major bottlenecks; e.g., the folding energy and the ability of making approximate matches.  Algorithms to determine protein shapes from sequences have two major components: The first component (sampling) generates a set of plausible protein shapes; at least one of the sampled shapes is expected to be similar to the correct fold.  The second component scores the different structures and decides on the best model.  The radius of convergence of the energy function must be sufficiently large so that approximate matches will be detected as well (in threading approximate matches may include deletions and insertion).  It is therefore clear that poor scoring functions (or energies), which are unable to identify the correct fold, are likely to diminish the capacity of the folding algorithm.  At present, it is easy to generate a set of decoy (wrong) structures that will confuse existing energy functions.  Mathematical programming and machine learning techniques (Support Vector Machines) will design enhanced folding and threading potentials.  The training by these methods is automated and will lead to monotonic improvement in recognition as a function of the data size.  To more effectively cover protein space, the goal is to learn 100 million data points in a single consistent potential with (at most) 10,000 parameters.  The automated large scale learning is crucial at times in which the information on sequences and structures grows rapidly.  A threading prediction server, based on the old and the new potentials, is and will be available, to the community at http://ser-loopp.tc.cornell.edu/Ioopp.html         n/a",Optimization of Folding and Threading Proteins,6727166,R01GM067823,"['bioinformatics', 'chemical models', 'computational biology', 'molecular shape', 'protein folding', 'protein sequence']",NIGMS,CORNELL UNIVERSITY ITHACA,R01,2004,261122,-0.0139670392659843
"Fracture Prediction by Machine Learning and 3D Finite Element Models from DXA ﻿    DESCRIPTION (provided by applicant): Osteoporosis is a disease characterized by loss of bone mass and structural deterioration leading to increased risk of fracture. Currently, osteoporosis is diagnosed by measurement of areal bone mineral density by dual- energy x-ray absorptiometry (DXA). However, the majority of fractures occur in both women and men who are not classified as osteoporotic by current DXA criteria (T-score = -2.5). As a 2-dimensional (2D) technology, DXA does not provide information about 3-dimensional (3D) bone structure, shape and geometry, which substantially contribute to bone strength and resistance to fracture. Finite element (FE) analysis of quantitative computed tomography (QCT) images can provide 3D structure and strength measurements but QCT is impractical for widespread clinical use because of high radiation exposure and expense. In contrast, DXA is widely available, inexpensive and has low radiation exposure. What is needed is a method by which DXA images can be used to generate 3D shape models that incorporate bone structure and geometry. However, fractures are complex events influenced by other factors including age, race, body mass index, risk of falls, and prior medical and fracture history. Even sophisticated measurements of bone density, structure, and strength may not be able to predict fractures accurately. Machine learning is an emerging field in which models are created by ""learning"" from previous data. These models can incorporate various factors and be used to classify or predict outcomes for new data. The overall hypothesis of this proposal is that advanced analyses of widely available DXA images that incorporate structural and strength information and statistical modeling using machine learning to incorporate additional risk factors will better identify patient at high risk of osteoporotic fracture. This hypothesis will be tested using QCT and DXA data from previous studies to generate 3D statistical shape models that describe variability in proximal femur morphology. By aligning 2D DXA images to the models, patient-specific 3D models will be reconstructed for quantitative analyses and combined with FE analysis to estimate bone strength. Machine learning models will be used to incorporate these novel measurements, demographics, and various risk factors for fracture to predict incident fractures in two very large, prospective studies. The ultimate goal of this proposal is to increase the diagnostic utility of DXA, a safe, non-invasive, and widely available technology, by applying novel image processing and statistical techniques to predict fractures more accurately. PUBLIC HEALTH RELEVANCE: Approximately 50% of women and 25% of men over age 50 are destined to suffer an osteoporotic fracture during their remaining lifetime. Unfortunately, the current standard for the diagnosis of osteoporosis, DXA, does not predict most fractures. This research will develop advanced analyses of DXA images and use machine learning to improve individualized fracture risk assessment.",Fracture Prediction by Machine Learning and 3D Finite Element Models from DXA,9068774,K99AR067883,"['3-Dimensional', 'Age', 'American', 'Body mass index', 'Bone Density', 'Cadaver', 'Characteristics', 'Clinical', 'Complex', 'Data', 'Data Set', 'Deterioration', 'Diagnosis', 'Diagnostic', 'Disease', 'Elements', 'Epidemiology', 'Event', 'Femur', 'Finite Element Analysis', 'Fracture', 'Future', 'Geometry', 'Goals', 'Gold', 'Health', 'Height', 'Hip Fractures', 'Image', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Mechanics', 'Medical', 'Methods', 'Modeling', 'Morphology', 'Osteopenia', 'Osteoporosis', 'Osteoporotic', 'Outcome', 'Patients', 'Peripheral', 'Postmenopause', 'Prospective Studies', 'Race', 'Radiation', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Resistance', 'Risk', 'Risk Assessment', 'Risk Factors', 'Scanning', 'Shapes', 'Specimen', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Weight', 'Woman', 'X-Ray Computed Tomography', 'base', 'bone', 'bone geometry', 'bone mass', 'bone strength', 'cohort', 'demographics', 'density', 'diagnosis standard', 'fall risk', 'high risk', 'image processing', 'improved', 'in vivo', 'information model', 'learning strategy', 'men', 'novel', 'osteoporosis with pathological fracture', 'prospective', 'three dimensional structure', 'tool', 'two-dimensional']",NIAMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,K99,2016,91800,-0.06733451065991342
"Fracture Prediction by Machine Learning and 3D Finite Element Models from DXA ﻿    DESCRIPTION (provided by applicant): Osteoporosis is a disease characterized by loss of bone mass and structural deterioration leading to increased risk of fracture. Currently, osteoporosis is diagnosed by measurement of areal bone mineral density by dual- energy x-ray absorptiometry (DXA). However, the majority of fractures occur in both women and men who are not classified as osteoporotic by current DXA criteria (T-score = -2.5). As a 2-dimensional (2D) technology, DXA does not provide information about 3-dimensional (3D) bone structure, shape and geometry, which substantially contribute to bone strength and resistance to fracture. Finite element (FE) analysis of quantitative computed tomography (QCT) images can provide 3D structure and strength measurements but QCT is impractical for widespread clinical use because of high radiation exposure and expense. In contrast, DXA is widely available, inexpensive and has low radiation exposure. What is needed is a method by which DXA images can be used to generate 3D shape models that incorporate bone structure and geometry. However, fractures are complex events influenced by other factors including age, race, body mass index, risk of falls, and prior medical and fracture history. Even sophisticated measurements of bone density, structure, and strength may not be able to predict fractures accurately. Machine learning is an emerging field in which models are created by ""learning"" from previous data. These models can incorporate various factors and be used to classify or predict outcomes for new data. The overall hypothesis of this proposal is that advanced analyses of widely available DXA images that incorporate structural and strength information and statistical modeling using machine learning to incorporate additional risk factors will better identify patient at high risk of osteoporotic fracture. This hypothesis will be tested using QCT and DXA data from previous studies to generate 3D statistical shape models that describe variability in proximal femur morphology. By aligning 2D DXA images to the models, patient-specific 3D models will be reconstructed for quantitative analyses and combined with FE analysis to estimate bone strength. Machine learning models will be used to incorporate these novel measurements, demographics, and various risk factors for fracture to predict incident fractures in two very large, prospective studies. The ultimate goal of this proposal is to increase the diagnostic utility of DXA, a safe, non-invasive, and widely available technology, by applying novel image processing and statistical techniques to predict fractures more accurately.         PUBLIC HEALTH RELEVANCE: Approximately 50% of women and 25% of men over age 50 are destined to suffer an osteoporotic fracture during their remaining lifetime. Unfortunately, the current standard for the diagnosis of osteoporosis, DXA, does not predict most fractures. This research will develop advanced analyses of DXA images and use machine learning to improve individualized fracture risk assessment.            ",Fracture Prediction by Machine Learning and 3D Finite Element Models from DXA,8869145,K99AR067883,"['3-Dimensional', 'Age', 'American', 'Body mass index', 'Bone Density', 'Cadaver', 'Characteristics', 'Clinical', 'Complex', 'Data', 'Data Set', 'Deterioration', 'Diagnosis', 'Diagnostic', 'Disease', 'Elements', 'Epidemiology', 'Event', 'Femur', 'Finite Element Analysis', 'Fracture', 'Future', 'Geometry', 'Goals', 'Gold', 'Height', 'Hip Fractures', 'Image', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Mechanics', 'Medical', 'Methods', 'Modeling', 'Morphology', 'Osteopenia', 'Osteoporosis', 'Outcome', 'Patients', 'Peripheral', 'Postmenopause', 'Prospective Studies', 'Race', 'Radiation', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Resistance', 'Risk', 'Risk Assessment', 'Risk Factors', 'Scanning', 'Shapes', 'Specimen', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Weight', 'Woman', 'X-Ray Computed Tomography', 'base', 'bone', 'bone geometry', 'bone mass', 'bone strength', 'cohort', 'demographics', 'density', 'diagnosis standard', 'fall risk', 'high risk', 'image processing', 'improved', 'in vivo', 'information model', 'men', 'novel', 'osteoporosis with pathological fracture', 'prospective', 'public health relevance', 'three dimensional structure', 'tool', 'two-dimensional']",NIAMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,K99,2015,91800,-0.06733451065991342
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9274155,R01AR068456,"['Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Dimensions', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'screening', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'tool', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2017,269514,0.017559615662953054
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9911975,R01AR068456,"['3-Dimensional', 'Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Screening procedure', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Visualization', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'osteoporosis with pathological fracture', 'patient response', 'population based', 'public health relevance', 'response', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2020,276227,0.017559615662953054
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9688116,R01AR068456,"['3-Dimensional', 'Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Dimensions', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Screening procedure', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2019,275934,0.017559615662953054
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9548457,R01AR068456,"['Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Bone structure', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Dimensions', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Screening procedure', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cortical bone', 'cost', 'density', 'experience', 'fracture risk', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'treatment response']",NIAMS,UNIVERSITY OF COLORADO DENVER,R01,2018,273031,0.017559615662953054
"Multi-Parametric Spatial Assessment of Bone with HR-pQCT ﻿    DESCRIPTION (provided by applicant):  Osteoporosis is a skeletal disorder characterized by compromised bone strength predisposing a person to an increased risk of fracture. In the U.S. today, 10 million individuals are estimated to already have the disease and almost 34 million more are estimated to have low bone density, placing them at increased risk for osteoporosis and broken bones. Currently, determination of fracture risk, aging effects, and therapeutic efficacy is primarily based on bone mineral density (BMD) measured by areal or volumetric X-ray-based imaging techniques. BMD can predict bone strength and fracture risk to some extent, however, studies have shown that BMD only explains about 70%-75% of the variance in strength, while the remaining variance has been attributed to the cumulative and synergistic effect of other factors such as bone structure, topology, geometry, tissue composition, microdamage, and biomechanical factors. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a noninvasive in-vivo imaging technique which depicts many of these features, including density, geometry, structure, topology, and mechanics of cortical and trabecular bone in the distal radius and distal tibia. To date HR-pQCT imagery has been analyzed using conventional quantitative approaches that average bone features over large regions of interest. The individual quantification of average bone features (uni-parametric) or their statistical combination (multi-parametric) disregard how these three-dimensional (3D) features synergistically contribute to bone strength. As a result the traditional methods fail to capture the spatial patterning of the effect being studied, which is key to understanding the underlying biology. Bone is a 3D organ experiencing constant adaptation through remodeling, and should therefore be analyzed with 3D techniques that reflect the complementary and interdependent nature of different bone features. Statistical parametric mapping (SPM) is a technique that enables 3D spatial comparisons of multi-parametric maps between groups of subjects. Instead of measuring summary properties for arbitrary or subjective volumes of interest, this data-driven process identifies regions significantly associated with a variable of interest through valid statistical tests, thus generating 3D statistical and P-value maps that facilitate the visualization and consequently the interpretation of comparisons between target populations. The ultimate goal of this proposal is to establish a framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. For this purpose, specialized SPM techniques have been developed for HR-pQCT. To evaluate the potential of SPM in clinical science, we propose to apply SPM to image data from three existing in-vivo HR-pQCT studies investigating: a) regional variations in bone structure related to gender and age; b) differences due to fracture of the forearm; and c) longitudinal effects of two osteoporosis treatments.         PUBLIC HEALTH RELEVANCE:  We propose a population-based framework to automatically identify relevant bone sub-regions and features in specific populations for the targeted quantitative assessment of the spatial distribution and prediction of bone strength using HR-pQCT. To demonstrate the potential of this framework in clinical science, we apply it to existing HR-pQCT studies to identify bone sub-regions and features significantly associated with age, gender, fracture status and response to osteoporosis treatment in post menopausal women; identify spatial associations between the central and distal skeleton with respect to treatment response; and improve fracture discrimination, and the prediction and understanding of the effects of osteoporosis treatment. This framework could improve the development of innovative, more active and safer drugs and therapies, and directly benefit patients suffering osteoporosis and other bone disorders since based on HR-pQCT maps of parameters estimating bone density and quality, a treatment offering the most clinical benefits to them could be prescribed.            ",Multi-Parametric Spatial Assessment of Bone with HR-pQCT,9106828,R01AR068456,"['Accounting', 'Affect', 'Age', 'Aging', 'Biology', 'Biomechanics', 'Bone Density', 'Bone Diseases', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Data', 'Development', 'Diagnosis', 'Discrimination', 'Disease', 'Distal', 'Elderly', 'Etiology', 'Exercise', 'Forearm Fracture', 'Fracture', 'Gender', 'Geometry', 'Goals', 'Hip region structure', 'Hormonal', 'Image', 'Imagery', 'Imaging Techniques', 'Incidence', 'Individual', 'Information Distribution', 'Machine Learning', 'Maps', 'Measures', 'Mechanics', 'Metabolic', 'Methods', 'Nature', 'Organ', 'Osteoporosis', 'Patients', 'Pattern', 'Peripheral', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Postmenopause', 'Process', 'Property', 'Public Health', 'Radial', 'Resolution', 'Risk', 'Roentgen Rays', 'Role', 'Skeleton', 'Spatial Distribution', 'Stimulus', 'Structure', 'Target Populations', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Treatment Efficacy', 'Variant', 'Vertebral column', 'Woman', 'Work', 'X-Ray Computed Tomography', 'age effect', 'base', 'bone', 'bone quality', 'bone strength', 'cost', 'density', 'experience', 'improved', 'in vivo', 'in vivo imaging', 'innovation', 'insight', 'interest', 'population based', 'public health relevance', 'response', 'screening', 'skeletal', 'skeletal disorder', 'spatial relationship', 'substantia spongiosa', 'tibia', 'tool', 'treatment response']",NIAMS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2016,31678,0.017559615662953054
"Deep-learning based spatial multiple testing for Alzheimer's neuroimaging data Project Summary The broad objective of this research is to develop a powerful deep-learning based multiple testing approach for high-dimensional spatial data that arise commonly in biomedical imaging studies, in particular, brain imaging studies. The motivating problem is to detect the cerebral metabolic abnormalities in Alzheimer’s disease (AD) from Fluorine-18 fluorodeoxyglucose positron emission tomography (FDG-PET) data. Existing multiple testing approaches in solving this problem often ignore or inadequately capture the spatial dependence among the test statistics obtained from brain voxels and thus lose substantial power for the detection. We will develop a novel spatial multiple testing method that utilizes the deep convolutional neural network (DCNN), a key deep- learning technique, to well capture the spatial dependence among test statistics and thus to achieve the optimal power in the sense of minimizing the false nondiscovery rate (FNR) while correctly controlling the false discovery rate (FDR) at a given level. The proposed DCNN-based FDR controlling method has enhanced power to discover new AD-related brain regions that are missed by conventional methods, thereby leading to novel clinical and pathological studies. The specific aims of this proposal include: 1. To develop an optimal spatial FDR controlling approach by connecting the unsupervised local-significance-index based multiple testing with the supervised DCNN-based image segmentation; 2. To evaluate the proposed spatial FDR controlling approach via extensive simulations under various three-dimensional spatial dependence structures, in comparison with multiple classical and state-of-the-art methods; 3. To apply proposed spatial FDR controlling approach to detect AD-related brain regions using the FDG-PET datasets from the Alzheimer’s Disease Neuroimaging Initiative and the Weill Cornell Brain Health Imaging Institute; 4. To develop a user- friendly and publicly available software package with versions in both Python and R to implement the proposed spatial FDR controlling approach. The proposed DCNN-based approach will also be widely applicable to large- scale multiple testing problems in other fields of biomedical research that involve spatial dependence. Project Narrative This project will exploit recent advances in deep learning to efficiently solve the large-scale spatial multiple testing problems that arise commonly in biomedical imaging studies. The proposed powerful deep-learning based spatial multiple testing approach will be particularly useful in brain imaging studies on neurodegenerative disorders such as Alzheimer’s disease and age-related cognitive impairment.",Deep-learning based spatial multiple testing for Alzheimer's neuroimaging data,10107565,R21AG070303,"['3-Dimensional', 'Affect', 'Age-associated memory impairment', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease patient', 'Architecture', 'Biomedical Research', 'Brain', 'Brain imaging', 'Brain region', 'Cerebrum', 'Clinical', 'Cognitive', 'Complex', 'Computer Vision Systems', 'Computer software', 'Data', 'Data Set', 'Dependence', 'Detection', 'Disease', 'Early Diagnosis', 'Family', 'Fluorine', 'Glucose', 'Goals', 'Health Sciences', 'Image', 'Institutes', 'Learning', 'Literature', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Monitor', 'Network-based', 'Neurodegenerative Disorders', 'Pathologic', 'Patients', 'Performance', 'Population Group', 'Positron-Emission Tomography', 'Problem Solving', 'Procedures', 'Pythons', 'Research', 'Research Personnel', 'Structural Models', 'Structure', 'Supervision', 'Techniques', 'Testing', 'Training', 'base', 'bioimaging', 'brain health', 'convolutional neural network', 'deep learning', 'fluorodeoxyglucose positron emission tomography', 'high dimensionality', 'imaging Segmentation', 'imaging study', 'indexing', 'metabolic rate', 'mild cognitive impairment', 'neuroimaging', 'novel', 'repository', 'simulation', 'statistics', 'success', 'theories', 'user friendly software', 'user-friendly']",NIA,NEW YORK UNIVERSITY,R21,2020,435875,-0.036422554295107885
"fMRI ABILITY TO MAP COLUMNAR STRUCTURES IN HUMANS    DESCRIPTION (provided by applicant):    Functional magnetic resonance imaging offers an unmatched combination of spatial and temporal resolution that continues to be pushed to new levels. Recently, the limits of BOLD fMRI have been challenged to the level of sub-millimeter structures. The reliability and reproducibility of these maps, as well as the optimization of the fMRI methods to acquire these maps has not yet been established, especially in humans. Differential mapping using gradient echoes, which may not be reliable in the general case because of the well known large vessel artifacts (more significantly at low fields), has been used in humans. We have recently demonstrated, made possible by funding from an R21 proposal, at sub-millimeter spatial resolutions, the advantages of high field Hahn spin echo (HSE) BOLD signals. Differences in specificity to neural activity between GE and HSE BOLD signals at high fields has not yet been addressed. Optimized BOLD signals to map brain function could allow for mapping of functional architecture without apriori knowledge of orthogonal conditions (i.e. single condition mapping). This would be an invaluable tool for many neuroscience applications where orthogonal conditions and columnar organizations in general are not known. This work will advance the foundation of our previous R21 proposal which investigated T2 weighted fMRI at high magnetic fields. The central hypothesis of this application is that HSE based BOLD fMRI signals at high fields (7 Tesla) can be used to attain a higher spatial specificity and therefore more reliable maps of cortical columns, in awake humans, than at lower fields (4 Tesla) and/or with gradient echoes. Finally, the existence of orientation columns in humans, which has never been shown, can be addressed using fMRI techniques developed in this work.         n/a",fMRI ABILITY TO MAP COLUMNAR STRUCTURES IN HUMANS,7385960,R01MH070800,"['Address', 'Architecture', 'Blood Vessels', 'Blood capillaries', 'Brain', 'Brain Mapping', 'Condition', 'Cortical Column', 'Data', 'Dependence', 'Felis catus', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Generations', 'Human', 'Image', 'Knowledge', 'Magnetic Resonance Imaging', 'Magnetism', 'Maps', 'Methods', 'Monitor', 'Morphologic artifacts', 'Neurons', 'Neurosciences', 'Numbers', 'Ocular dominance columns', 'Perfusion', 'Publications', 'Publishing', 'Reproducibility', 'Resolution', 'Signal Transduction', 'Specificity', 'Structure', 'Techniques', 'Testing', 'Veins', 'Visual Cortex', 'Weight', 'Width', 'Work', 'awake', 'base', 'blood oxygen level dependent', 'blood oxygenation level dependent response', 'capillary', 'hemodynamics', 'improved', 'magnetic field', 'millimeter', 'orientation columns', 'relating to nervous system', 'research study', 'response', 'tool']",NIMH,UNIVERSITY OF MINNESOTA,R01,2008,316811,-0.010683654894113841
"fMRI ABILITY TO MAP COLUMNAR STRUCTURES IN HUMANS    DESCRIPTION (provided by applicant):    Functional magnetic resonance imaging offers an unmatched combination of spatial and temporal resolution that continues to be pushed to new levels. Recently, the limits of BOLD fMRI have been challenged to the level of sub-millimeter structures. The reliability and reproducibility of these maps, as well as the optimization of the fMRI methods to acquire these maps has not yet been established, especially in humans. Differential mapping using gradient echoes, which may not be reliable in the general case because of the well known large vessel artifacts (more significantly at low fields), has been used in humans. We have recently demonstrated, made possible by funding from an R21 proposal, at sub-millimeter spatial resolutions, the advantages of high field Hahn spin echo (HSE) BOLD signals. Differences in specificity to neural activity between GE and HSE BOLD signals at high fields has not yet been addressed. Optimized BOLD signals to map brain function could allow for mapping of functional architecture without apriori knowledge of orthogonal conditions (i.e. single condition mapping). This would be an invaluable tool for many neuroscience applications where orthogonal conditions and columnar organizations in general are not known. This work will advance the foundation of our previous R21 proposal which investigated T2 weighted fMRI at high magnetic fields. The central hypothesis of this application is that HSE based BOLD fMRI signals at high fields (7 Tesla) can be used to attain a higher spatial specificity and therefore more reliable maps of cortical columns, in awake humans, than at lower fields (4 Tesla) and/or with gradient echoes. Finally, the existence of orientation columns in humans, which has never been shown, can be addressed using fMRI techniques developed in this work.         n/a",fMRI ABILITY TO MAP COLUMNAR STRUCTURES IN HUMANS,7189859,R01MH070800,"['Address', 'Architecture', 'Blood Vessels', 'Blood capillaries', 'Brain', 'Brain Mapping', 'Condition', 'Cortical Column', 'Data', 'Dependence', 'Felis catus', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Generations', 'Human', 'Image', 'Knowledge', 'Magnetic Resonance Imaging', 'Magnetism', 'Maps', 'Methods', 'Monitor', 'Morphologic artifacts', 'Neurons', 'Neurosciences', 'Numbers', 'Ocular dominance columns', 'Perfusion', 'Publications', 'Publishing', 'Reproducibility', 'Resolution', 'Signal Transduction', 'Specificity', 'Structure', 'Techniques', 'Testing', 'Veins', 'Visual Cortex', 'Weight', 'Width', 'Work', 'awake', 'base', 'blood oxygen level dependent', 'blood oxygenation level dependent response', 'capillary', 'hemodynamics', 'improved', 'magnetic field', 'millimeter', 'orientation columns', 'relating to nervous system', 'research study', 'response', 'tool']",NIMH,UNIVERSITY OF MINNESOTA,R01,2007,316811,-0.010683654894113841
"fMRI ABILITY TO MAP COLUMNAR STRUCTURES IN HUMANS    DESCRIPTION (provided by applicant):    Functional magnetic resonance imaging offers an unmatched combination of spatial and temporal resolution that continues to be pushed to new levels. Recently, the limits of BOLD fMRI have been challenged to the level of sub-millimeter structures. The reliability and reproducibility of these maps, as well as the optimization of the fMRI methods to acquire these maps has not yet been established, especially in humans. Differential mapping using gradient echoes, which may not be reliable in the general case because of the well known large vessel artifacts (more significantly at low fields), has been used in humans. We have recently demonstrated, made possible by funding from an R21 proposal, at sub-millimeter spatial resolutions, the advantages of high field Hahn spin echo (HSE) BOLD signals. Differences in specificity to neural activity between GE and HSE BOLD signals at high fields has not yet been addressed. Optimized BOLD signals to map brain function could allow for mapping of functional architecture without apriori knowledge of orthogonal conditions (i.e. single condition mapping). This would be an invaluable tool for many neuroscience applications where orthogonal conditions and columnar organizations in general are not known. This work will advance the foundation of our previous R21 proposal which investigated T2 weighted fMRI at high magnetic fields. The central hypothesis of this application is that HSE based BOLD fMRI signals at high fields (7 Tesla) can be used to attain a higher spatial specificity and therefore more reliable maps of cortical columns, in awake humans, than at lower fields (4 Tesla) and/or with gradient echoes. Finally, the existence of orientation columns in humans, which has never been shown, can be addressed using fMRI techniques developed in this work.         n/a",fMRI ABILITY TO MAP COLUMNAR STRUCTURES IN HUMANS,7035857,R01MH070800,"['bioimaging /biomedical imaging', 'brain mapping', 'cats', 'clinical research', 'electrophysiology', 'functional magnetic resonance imaging', 'hemodynamics', 'human subject', 'magnetic field', 'neurophysiology', 'technology /technique development', 'visual cortex', 'visual stimulus']",NIMH,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2006,326274,-0.010683654894113841
"fMRI ABILITY TO MAP COLUMNAR STRUCTURES IN HUMANS    DESCRIPTION (provided by applicant):    Functional magnetic resonance imaging offers an unmatched combination of spatial and temporal resolution that continues to be pushed to new levels. Recently, the limits of BOLD fMRI have been challenged to the level of sub-millimeter structures. The reliability and reproducibility of these maps, as well as the optimization of the fMRI methods to acquire these maps has not yet been established, especially in humans. Differential mapping using gradient echoes, which may not be reliable in the general case because of the well known large vessel artifacts (more significantly at low fields), has been used in humans. We have recently demonstrated, made possible by funding from an R21 proposal, at sub-millimeter spatial resolutions, the advantages of high field Hahn spin echo (HSE) BOLD signals. Differences in specificity to neural activity between GE and HSE BOLD signals at high fields has not yet been addressed. Optimized BOLD signals to map brain function could allow for mapping of functional architecture without apriori knowledge of orthogonal conditions (i.e. single condition mapping). This would be an invaluable tool for many neuroscience applications where orthogonal conditions and columnar organizations in general are not known. This work will advance the foundation of our previous R21 proposal which investigated T2 weighted fMRI at high magnetic fields. The central hypothesis of this application is that HSE based BOLD fMRI signals at high fields (7 Tesla) can be used to attain a higher spatial specificity and therefore more reliable maps of cortical columns, in awake humans, than at lower fields (4 Tesla) and/or with gradient echoes. Finally, the existence of orientation columns in humans, which has never been shown, can be addressed using fMRI techniques developed in this work.         n/a",fMRI ABILITY TO MAP COLUMNAR STRUCTURES IN HUMANS,6869607,R01MH070800,"['bioimaging /biomedical imaging', 'brain mapping', 'cats', 'clinical research', 'electrophysiology', 'functional magnetic resonance imaging', 'hemodynamics', 'human subject', 'magnetic field', 'neurophysiology', 'technology /technique development', 'visual cortex', 'visual stimulus']",NIMH,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2005,334125,-0.010683654894113841
"fMRI ABILITY TO MAP COLUMNAR STRUCTURES IN HUMANS    DESCRIPTION (provided by applicant):    Functional magnetic resonance imaging offers an unmatched combination of spatial and temporal resolution that continues to be pushed to new levels. Recently, the limits of BOLD fMRI have been challenged to the level of sub-millimeter structures. The reliability and reproducibility of these maps, as well as the optimization of the fMRI methods to acquire these maps has not yet been established, especially in humans. Differential mapping using gradient echoes, which may not be reliable in the general case because of the well known large vessel artifacts (more significantly at low fields), has been used in humans. We have recently demonstrated, made possible by funding from an R21 proposal, at sub-millimeter spatial resolutions, the advantages of high field Hahn spin echo (HSE) BOLD signals. Differences in specificity to neural activity between GE and HSE BOLD signals at high fields has not yet been addressed. Optimized BOLD signals to map brain function could allow for mapping of functional architecture without apriori knowledge of orthogonal conditions (i.e. single condition mapping). This would be an invaluable tool for many neuroscience applications where orthogonal conditions and columnar organizations in general are not known. This work will advance the foundation of our previous R21 proposal which investigated T2 weighted fMRI at high magnetic fields. The central hypothesis of this application is that HSE based BOLD fMRI signals at high fields (7 Tesla) can be used to attain a higher spatial specificity and therefore more reliable maps of cortical columns, in awake humans, than at lower fields (4 Tesla) and/or with gradient echoes. Finally, the existence of orientation columns in humans, which has never been shown, can be addressed using fMRI techniques developed in this work.         n/a",fMRI ABILITY TO MAP COLUMNAR STRUCTURES IN HUMANS,6770600,R01MH070800,"['bioimaging /biomedical imaging', 'brain mapping', 'cats', 'clinical research', 'electrophysiology', 'functional magnetic resonance imaging', 'hemodynamics', 'human subject', 'magnetic field', 'neurophysiology', 'technology /technique development', 'visual cortex', 'visual stimulus']",NIMH,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2004,317756,-0.010683654894113841
"NEW STATISTICAL METHODS FOR MEDICAL SIGNALS AND IMAGES DESCRIPTION:  (Adapted from the applicant's abstract):  Medical and              biological data often come in the form of digitized signals and images, for      example magnetic resonance images (MRI), ion channel electrical series, and      human gait paths.  As data acquisition becomes easier, sequences of such         images or signals are collected, often along with other covariate                measurements, resulting in data sets where the basic unit of measurement or      response is a high dimensional object.  This project proposes a battery of       statistical techniques for modeling and understanding such data, that            explicitly takes into account and indeed exploits the inherent, spatial, or      temporal correlation, and when appropriate, relates it to covariate              information.  By imposing spatial smoothness in the image or signal domain,      pixel-wise regression, and canonical correlation models can borrow strength      from neighboring pixels.  This not only improves the overall efficiency of       these techniques, but also allows identification of important regions rather     than individual pixels.  The project develops appropriate versions of            nonparametric regressions for such series of images, as well as data             descriptions such as clustering, principal component, and singular value         decomposition models.  In many cases, wavelets will be used to achieve           spatial smoothness.  In the case of ion channel data, the models are used to     isolate particular weak high frequency components from correlated noise.         Much of this work will be carried out in collaboration with radiologists,        physiologists, and other biomedical researchers working on cancer, heart         disease and stroke, brain mapping, and gait analysis.                             n/a",NEW STATISTICAL METHODS FOR MEDICAL SIGNALS AND IMAGES,2769887,R01CA072028,"['computer data analysis', ' diagnosis design /evaluation', ' digital imaging', ' mathematical model', ' statistics /biometry']",NCI,STANFORD UNIVERSITY,R01,1998,146191,-0.03209533638253041
"NEW STATISTICAL METHODS FOR MEDICAL SIGNALS AND IMAGES DESCRIPTION:  (Adapted from the applicant's abstract):  Medical and              biological data often come in the form of digitized signals and images, for      example magnetic resonance images (MRI), ion channel electrical series, and      human gait paths.  As data acquisition becomes easier, sequences of such         images or signals are collected, often along with other covariate                measurements, resulting in data sets where the basic unit of measurement or      response is a high dimensional object.  This project proposes a battery of       statistical techniques for modeling and understanding such data, that            explicitly takes into account and indeed exploits the inherent, spatial, or      temporal correlation, and when appropriate, relates it to covariate              information.  By imposing spatial smoothness in the image or signal domain,      pixel-wise regression, and canonical correlation models can borrow strength      from neighboring pixels.  This not only improves the overall efficiency of       these techniques, but also allows identification of important regions rather     than individual pixels.  The project develops appropriate versions of            nonparametric regressions for such series of images, as well as data             descriptions such as clustering, principal component, and singular value         decomposition models.  In many cases, wavelets will be used to achieve           spatial smoothness.  In the case of ion channel data, the models are used to     isolate particular weak high frequency components from correlated noise.         Much of this work will be carried out in collaboration with radiologists,        physiologists, and other biomedical researchers working on cancer, heart         disease and stroke, brain mapping, and gait analysis.                             n/a",NEW STATISTICAL METHODS FOR MEDICAL SIGNALS AND IMAGES,2517753,R01CA072028,"['computer data analysis', ' diagnosis design /evaluation', ' digital imaging', ' mathematical model', ' statistics /biometry']",NCI,STANFORD UNIVERSITY,R01,1997,143855,-0.03209533638253041
"Sensory based CNS diagnostics for the clinic    DESCRIPTION (provided by applicant): There is currently a significant gap that exists between fundamental neuroscience research and translation of the findings of that research into everyday practice. Experimental findings at the genetic, cellular, molecular and systems level often take a fairly long and frequently circuitous route to make an impact on a particular neurological disease or disorder. The goal of our work is to bridge the neuroscientific gap at the systems level of study by developing standardized sensory measures that can be not only utilized in clinical or clinical research settings, but can be directly correlated with the observations obtained directly from sensory cortex in non-human primates via high resolution imaging and extracellular recording. Successful development of an experimental model that iteratively evaluates the relationship of clinical measures and systemic CNS responses to specific mechanistic alterations will be quite significant. Such an evaluation of an individual's CNS status could be directly linked to systemic mechanistic deficiencies or alterations observed in animal experimentation.  Towards that goal, we have successfully designed and fabricated a tactile sensory diagnostic device. In parallel with that development, we designed a number of protocols - based on experimental neurophysiological findings from both our non human primate research and that of others - that could be rapidly and efficiently delivered (1-3 minutes) to a number of subject populations. The tactile diagnostic system that we have developed was conceptually designed to investigate differences in cortical information processing strategies between people with autism and people without. In this proposal we ask whether or not the strategy that we have devised for investigating a population with a neurodevelopmental disorder could be broadly applied to a number of neurological disorders. In other words, we consider the changes manifested by the neurodevelpmental disorder autism to be systemic, and if systemic cortical alterations occur in other neurological disorders, could they also be detected in the same manner?  Proof-of-concept studies in a number of clinical research areas demonstrated that these newly developed metrics were sensitive to systemic cortical alterations. One question that emerges from this data is that most of these neurological disorders result in some type of altered central sensitization, no matter what the cause - whether it be neurodevelopmental, neurodegenerative, pharmacological or trauma induced - in which there is a significant change in the balance between excitation and inhibition. This application proposes to determine if sensory perceptual metrics, similar to those that were used to successfully distinguish subjects with autism from healthy control populations (with 90% accuracy using SVM to assess the results of a 25 minute battery of 9 protocols), could be used to reliably distinguish - on an individual basis - subjects with neurological disorders that are not neurodevelopmental in nature. Towards this goal, we target subjects from one broad category of neurological disorders - chronic pain. More specifically, we will examine the differences and commonalities from observations of pain patients diagnosed with one of the following: fibromyalgia, vulvodynia, TMJD, IBS and migraine.        The overall goal of the proposed work is to investigate the utility of novel sensory-based methodologies that are currently being used in both basic and clinical research. Recently, utilizing state-of-the-art technology, we built a multi-site tactile stimulator that allows for investigation of central nervous system (CNS) health and advanced methods in sensory perceptual metrics. These metrics have been demonstrated to be sensitive to changes in centrally mediated mechanisms; and systemic alterations of cortical health (via neurodegenerational, neurodevelopmental, pharmacological or trauma induced changes) robustly change the measures. It is anticipated that clinicians will be able to utilize these measures to improve diagnostic performance and enable assessment of efficacy of treatment. The study itself will serve to validate the utility of a number of these measures in several types of pain, specifically fibromyalgia, TMJD, IBS, vulvodynia and migraine. The information from this study could aid in understanding centrally mediated mechanisms that undergo significant alterations with chronic pain.         ",Sensory based CNS diagnostics for the clinic,8293088,R21NS072811,"['Address', 'Age', 'Animal Experimentation', 'Area', 'Autistic Disorder', 'Basic Science', 'Behavior', 'Brain Concussion', 'Caregivers', 'Categories', 'Cerebrum', 'Clinic', 'Clinical', 'Clinical Research', 'Data', 'Databases', 'Development', 'Devices', 'Dextromethorphan', 'Diagnosis', 'Diagnostic', 'Disease', 'Equilibrium', 'Evaluation', 'Experimental Models', 'Fibromyalgia', 'GABA Agonists', 'Genetic', 'Goals', 'Health', 'Image', 'Individual', 'Investigation', 'Laboratory Animals', 'Lead', 'Letters', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Metric', 'Migraine', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'N-Methylaspartate', 'Nature', 'Nerve Degeneration', 'Neuraxis', 'Neurodevelopmental Disorder', 'Neurons', 'Neurosciences Research', 'Ophthalmic examination and evaluation', 'Pain', 'Patients', 'Performance', 'Physiological', 'Play', 'Population', 'Population Control', 'Primary Health Care', 'Process', 'Protocols documentation', 'Recruitment Activity', 'Research', 'Resolution', 'Role', 'Route', 'Sensory', 'Site', 'Stimulus', 'System', 'Tactile', 'Techniques', 'Technology', 'Temporomandibular Joint Disorders', 'Testing', 'Translations', 'Trauma', 'Treatment Efficacy', 'United States National Institutes of Health', 'Vulvodynia', 'Work', 'analytical tool', 'base', 'central sensitization', 'chronic pain', 'cohort', 'cost effective', 'data mining', 'demographics', 'design', 'extracellular', 'gamma-Aminobutyric Acid', 'improved', 'in vivo', 'information processing', 'nervous system disorder', 'neurophysiology', 'neurotransmission', 'nonhuman primate', 'novel', 'process optimization', 'protocol development', 'response', 'sensory cortex', 'white matter damage']",NINDS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2012,181885,-0.17416716573253369
"Spatial Models of Cell Regulatory Networks The overall goal of this project is to understand how the shape of cells in the context of tissue organization controls transmembrane signal transduction. In the previous term we had found that cells with different aspect ratios produce transient microdomains of activated receptors and differentially regulate cytoplasmic and nuclear signaling. Building on these observations in the coming term we plan to use a combination of constrained analytic representations and numerical simulations with super resolution imaging of biochemical reactions in individual cells assembled within engineered 3D chips to mimic tissue organization to obtain deep understanding of transmembrane signaling when the cell is part of the 3D tissue environment. Our central hypothesis is that within tissues, cells, to maintain their functional phenotype sense a multifaceted relationship between extra and intra cellular spaces and density of receptors at the cell surface, in the context of various cell shapes. This multidimensional relationship controls transmembrane signal transduction and cell health. To test our central hypothesis we propose to develop analytical models of role of cell shape in information storage and retrieval as cells function within tissues. We will address one key question: 1) how changes in intra and extracellular spaces and consequently their effect on concentrations of soluble components such as receptor agonists, and intracellular adapters and transducers separated by the plasma membrane and membrane bound receptors interact to regulate transmembrane signaling. Using human and rat vascular smooth muscle cells as model systems we will use the analytical representations to develop multicompartment ODE and PDE models and run numerical simulations to determine how relationships between intracellular and extracellular spaces and cell shape control transmembrane signal transduction. To test model predictions we will develop 3D chips ‐ microfabricated devices that enable the assembly of multilayered cells of different shapes and varying extracellular spaces to mimic organization of these cells within tissues. .We will then test model predictions of control of formation of activated receptor signaling complexes using super resolution fluorescence microscopy and single molecule fluorescence spectroscopy. From these studies we hope to elucidate fundamental principles of how cell shape and tissue organization information is processed at the transmembrane level. The shape of cells within tissues is an indicator of health and disease. However we understand little of the physico‐chemical principles of how cell shape relates to disease. In this project we will use mathematical modeling and experiments to understand cellular communication and its role in cell health in the tissue environment.",Spatial Models of Cell Regulatory Networks,9621391,R01GM072853,"['3-Dimensional', 'Address', 'Agonist', 'Area', 'Biochemical Reaction', 'Biological Models', 'Blood Vessels', 'Calcium Signaling', 'Cell Shape', 'Cell membrane', 'Cell model', 'Cell physiology', 'Cell surface', 'Cells', 'Communication', 'Complex', 'Differential Equation', 'Diffusion', 'Disease', 'Engineering', 'Environment', 'Extracellular Space', 'Fluorescence Microscopy', 'Fluorescence Spectroscopy', 'Goals', 'Health', 'Human', 'Image', 'Individual', 'Information Retrieval', 'Information Storage', 'Intracellular Space', 'Ligands', 'Membrane', 'Mitogen-Activated Protein Kinases', 'Modeling', 'Muscarinic Acetylcholine Receptor', 'Nuclear', 'Phenotype', 'Platelet-Derived Growth Factor Receptor', 'Process', 'Rattus', 'Receptor Activation', 'Receptor Signaling', 'Resolution', 'Role', 'Running', 'Shapes', 'Signal Transduction', 'Smooth Muscle Myocytes', 'Testing', 'Tissues', 'Transducers', 'Vascular Smooth Muscle', 'base', 'experimental study', 'information model', 'information organization', 'mathematical model', 'microdevice', 'predictive modeling', 'receptor', 'receptor binding', 'receptor density', 'role model', 'simulation', 'single molecule']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2019,603731,-0.0020281308456519364
"Spatial Models of Cell Regulatory Networks The overall goal of this project is to understand how the shape of cells in the context of tissue organization controls transmembrane signal transduction. In the previous term we had found that cells with different aspect ratios produce transient microdomains of activated receptors and differentially regulate cytoplasmic and nuclear signaling. Building on these observations in the coming term we plan to use a combination of constrained analytic representations and numerical simulations with super resolution imaging of biochemical reactions in individual cells assembled within engineered 3D chips to mimic tissue organization to obtain deep understanding of transmembrane signaling when the cell is part of the 3D tissue environment. Our central hypothesis is that within tissues, cells, to maintain their functional phenotype sense a multifaceted relationship between extra and intra cellular spaces and density of receptors at the cell surface, in the context of various cell shapes. This multidimensional relationship controls transmembrane signal transduction and cell health. To test our central hypothesis we propose to develop analytical models of role of cell shape in information storage and retrieval as cells function within tissues. We will address one key question: 1) how changes in intra and extracellular spaces and consequently their effect on concentrations of soluble components such as receptor agonists, and intracellular adapters and transducers separated by the plasma membrane and membrane bound receptors interact to regulate transmembrane signaling. Using human and rat vascular smooth muscle cells as model systems we will use the analytical representations to develop multicompartment ODE and PDE models and run numerical simulations to determine how relationships between intracellular and extracellular spaces and cell shape control transmembrane signal transduction. To test model predictions we will develop 3D chips ‐ microfabricated devices that enable the assembly of multilayered cells of different shapes and varying extracellular spaces to mimic organization of these cells within tissues. .We will then test model predictions of control of formation of activated receptor signaling complexes using super resolution fluorescence microscopy and single molecule fluorescence spectroscopy. From these studies we hope to elucidate fundamental principles of how cell shape and tissue organization information is processed at the transmembrane level. The shape of cells within tissues is an indicator of health and disease. However we understand little of the physico‐chemical principles of how cell shape relates to disease. In this project we will use mathematical modeling and experiments to understand cellular communication and its role in cell health in the tissue environment.",Spatial Models of Cell Regulatory Networks,9406869,R01GM072853,"['Address', 'Agonist', 'Area', 'Biochemical Reaction', 'Biological Models', 'Blood Vessels', 'Calcium Signaling', 'Cell Shape', 'Cell membrane', 'Cell model', 'Cell physiology', 'Cell surface', 'Cells', 'Communication', 'Complex', 'Differential Equation', 'Diffusion', 'Disease', 'Engineering', 'Environment', 'Extracellular Space', 'Fluorescence Microscopy', 'Fluorescence Spectroscopy', 'Goals', 'Health', 'Human', 'Image', 'Individual', 'Information Retrieval', 'Information Storage', 'Intracellular Space', 'Ligands', 'Membrane', 'Mitogen-Activated Protein Kinases', 'Modeling', 'Muscarinic Acetylcholine Receptor', 'Nuclear', 'Phenotype', 'Platelet-Derived Growth Factor Receptor', 'Process', 'Rattus', 'Receptor Activation', 'Receptor Signaling', 'Resolution', 'Role', 'Running', 'Shapes', 'Signal Transduction', 'Smooth Muscle Myocytes', 'Testing', 'Tissues', 'Transducers', 'Vascular Smooth Muscle', 'base', 'experimental study', 'information model', 'information organization', 'mathematical model', 'microdevice', 'predictive modeling', 'receptor', 'receptor binding', 'receptor density', 'role model', 'simulation', 'single molecule']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2018,607399,-0.0020281308456519364
"Spatial Models of Cell Regulatory Networks The overall goal of this project is to understand how the shape of cells in the context of tissue organization controls transmembrane signal transduction. In the previous term we had found that cells with different aspect ratios produce transient microdomains of activated receptors and differentially regulate cytoplasmic and nuclear signaling. Building on these observations in the coming term we plan to use a combination of constrained analytic representations and numerical simulations with super resolution imaging of biochemical reactions in individual cells assembled within engineered 3D chips to mimic tissue organization to obtain deep understanding of transmembrane signaling when the cell is part of the 3D tissue environment. Our central hypothesis is that within tissues, cells, to maintain their functional phenotype sense a multifaceted relationship between extra and intra cellular spaces and density of receptors at the cell surface, in the context of various cell shapes. This multidimensional relationship controls transmembrane signal transduction and cell health. To test our central hypothesis we propose to develop analytical models of role of cell shape in information storage and retrieval as cells function within tissues. We will address one key question: 1) how changes in intra and extracellular spaces and consequently their effect on concentrations of soluble components such as receptor agonists, and intracellular adapters and transducers separated by the plasma membrane and membrane bound receptors interact to regulate transmembrane signaling. Using human and rat vascular smooth muscle cells as model systems we will use the analytical representations to develop multicompartment ODE and PDE models and run numerical simulations to determine how relationships between intracellular and extracellular spaces and cell shape control transmembrane signal transduction. To test model predictions we will develop 3D chips ‐ microfabricated devices that enable the assembly of multilayered cells of different shapes and varying extracellular spaces to mimic organization of these cells within tissues. .We will then test model predictions of control of formation of activated receptor signaling complexes using super resolution fluorescence microscopy and single molecule fluorescence spectroscopy. From these studies we hope to elucidate fundamental principles of how cell shape and tissue organization information is processed at the transmembrane level. The shape of cells within tissues is an indicator of health and disease. However we understand little of the physico‐chemical principles of how cell shape relates to disease. In this project we will use mathematical modeling and experiments to understand cellular communication and its role in cell health in the tissue environment.",Spatial Models of Cell Regulatory Networks,9199087,R01GM072853,"['Address', 'Agonist', 'Area', 'Biochemical Reaction', 'Biological Models', 'Blood Vessels', 'Calcium Signaling', 'Cell Shape', 'Cell membrane', 'Cell model', 'Cell physiology', 'Cell surface', 'Cells', 'Communication', 'Complex', 'Devices', 'Differential Equation', 'Diffusion', 'Disease', 'Engineering', 'Environment', 'Extracellular Space', 'Fluorescence Microscopy', 'Fluorescence Spectroscopy', 'Goals', 'Health', 'Human', 'Image', 'Individual', 'Information Retrieval', 'Information Storage', 'Intracellular Space', 'Ligands', 'Membrane', 'Mitogen-Activated Protein Kinases', 'Modeling', 'Muscarinic Acetylcholine Receptor', 'Muscle Cells', 'Nuclear', 'Phenotype', 'Platelet-Derived Growth Factor Receptor', 'Process', 'Rattus', 'Receptor Activation', 'Receptor Signaling', 'Resolution', 'Role', 'Running', 'Shapes', 'Signal Transduction', 'Testing', 'Tissues', 'Transducers', 'Vascular Smooth Muscle', 'base', 'experimental study', 'information model', 'information organization', 'mathematical model', 'receptor', 'receptor binding', 'receptor density', 'role model', 'simulation', 'single molecule']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2017,610959,-0.0020281308456519364
"Spatial Models of Cell Regulatory Networks The overall goal of this project is to understand how the shape of cells in the context of tissue organization controls transmembrane signal transduction. In the previous term we had found that cells with different aspect ratios produce transient microdomains of activated receptors and differentially regulate cytoplasmic and nuclear signaling. Building on these observations in the coming term we plan to use a combination of constrained analytic representations and numerical simulations with super resolution imaging of biochemical reactions in individual cells assembled within engineered 3D chips to mimic tissue organization to obtain deep understanding of transmembrane signaling when the cell is part of the 3D tissue environment. Our central hypothesis is that within tissues, cells, to maintain their functional phenotype sense a multifaceted relationship between extra and intra cellular spaces and density of receptors at the cell surface, in the context of various cell shapes. This multidimensional relationship controls transmembrane signal transduction and cell health. To test our central hypothesis we propose to develop analytical models of role of cell shape in information storage and retrieval as cells function within tissues. We will address one key question: 1) how changes in intra and extracellular spaces and consequently their effect on concentrations of soluble components such as receptor agonists, and intracellular adapters and transducers separated by the plasma membrane and membrane bound receptors interact to regulate transmembrane signaling. Using human and rat vascular smooth muscle cells as model systems we will use the analytical representations to develop multicompartment ODE and PDE models and run numerical simulations to determine how relationships between intracellular and extracellular spaces and cell shape control transmembrane signal transduction. To test model predictions we will develop 3D chips ‐ microfabricated devices that enable the assembly of multilayered cells of different shapes and varying extracellular spaces to mimic organization of these cells within tissues. .We will then test model predictions of control of formation of activated receptor signaling complexes using super resolution fluorescence microscopy and single molecule fluorescence spectroscopy. From these studies we hope to elucidate fundamental principles of how cell shape and tissue organization information is processed at the transmembrane level. The shape of cells within tissues is an indicator of health and disease. However we understand little of the physico‐chemical principles of how cell shape relates to disease. In this project we will use mathematical modeling and experiments to understand cellular communication and its role in cell health in the tissue environment.",Spatial Models of Cell Regulatory Networks,9030524,R01GM072853,"['Address', 'Agonist', 'Area', 'Binding', 'Biochemical Reaction', 'Biological Models', 'Blood Vessels', 'Calcium Signaling', 'Cell Shape', 'Cell membrane', 'Cell model', 'Cell physiology', 'Cell surface', 'Cells', 'Chemicals', 'Communication', 'Complex', 'Devices', 'Differential Equation', 'Diffusion', 'Disease', 'Engineering', 'Environment', 'Extracellular Space', 'Fluorescence Microscopy', 'Fluorescence Spectroscopy', 'Goals', 'Health', 'Human', 'Image', 'Individual', 'Information Retrieval', 'Information Storage', 'Intracellular Space', 'Ligands', 'Membrane', 'Mitogen-Activated Protein Kinases', 'Modeling', 'Muscarinic Acetylcholine Receptor', 'Nuclear', 'Phenotype', 'Platelet-Derived Growth Factor Receptor', 'Process', 'Rattus', 'Receptor Activation', 'Receptor Signaling', 'Resolution', 'Role', 'Running', 'Shapes', 'Signal Transduction', 'Smooth Muscle Myocytes', 'Testing', 'Tissues', 'Transducers', 'Vascular Smooth Muscle', 'base', 'information organization', 'mathematical model', 'receptor', 'receptor binding', 'receptor density', 'research study', 'role model', 'simulation', 'single molecule']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2016,649165,-0.0020281308456519364
"Spatial Models of Cell Regulatory Network The overall goal of this project is to understand how the shape of cells in the context of tissue organization controls transmembrane signal transduction. In the previous term we had found that cells with different aspect ratios produce transient microdomains of activated receptors and differentially regulate cytoplasmic and nuclear signaling. Building on these observations in the coming term we plan to use a combination of constrained analytic representations and numerical simulations with super resolution imaging of biochemical reactions in individual cells assembled within engineered 3D chips to mimic tissue organization to obtain deep understanding of transmembrane signaling when the cell is part of the 3D tissue environment. Our central hypothesis is that within tissues, cells, to maintain their functional phenotype sense a multifaceted relationship between extra and intra cellular spaces and density of receptors at the cell surface, in the context of various cell shapes. This multidimensional relationship controls transmembrane signal transduction and cell health. To test our central hypothesis we propose to develop analytical models of role of cell shape in information storage and retrieval as cells function within tissues. We will address one key question: 1) how changes in intra and extracellular spaces and consequently their effect on concentrations of soluble components such as receptor agonists, and intracellular adapters and transducers separated by the plasma membrane and membrane bound receptors interact to regulate transmembrane signaling. Using human and rat vascular smooth muscle cells as model systems we will use the analytical representations to develop multicompartment ODE and PDE models and run numerical simulations to determine how relationships between intracellular and extracellular spaces and cell shape control transmembrane signal transduction. To test model predictions we will develop 3D chips ‐ microfabricated devices that enable the assembly of multilayered cells of different shapes and varying extracellular spaces to mimic organization of these cells within tissues. .We will then test model predictions of control of formation of activated receptor signaling complexes using super resolution fluorescence microscopy and single molecule fluorescence spectroscopy. From these studies we hope to elucidate fundamental principles of how cell shape and tissue organization information is processed at the transmembrane level. The shape of cells within tissues is an indicator of health and disease. However we understand little of the physico‐chemical principles of how cell shape relates to disease. In this project we will use mathematical modeling and experiments to understand cellular communication and its role in cell health in the tissue environment.",Spatial Models of Cell Regulatory Network,9276933,R01GM072853,"['Address', 'Agonist', 'Area', 'Binding', 'Biochemical Reaction', 'Biological Models', 'Blood Vessels', 'Calcium Signaling', 'Cell Shape', 'Cell membrane', 'Cell model', 'Cell physiology', 'Cell surface', 'Cells', 'Chemicals', 'Communication', 'Complex', 'Devices', 'Differential Equation', 'Diffusion', 'Disease', 'Engineering', 'Environment', 'Extracellular Space', 'Fluorescence Microscopy', 'Fluorescence Spectroscopy', 'Goals', 'Health', 'Human', 'Image', 'Individual', 'Information Retrieval', 'Information Storage', 'Intracellular Space', 'Ligands', 'Membrane', 'Mitogen-Activated Protein Kinases', 'Modeling', 'Muscarinic Acetylcholine Receptor', 'Nuclear', 'Phenotype', 'Platelet-Derived Growth Factor Receptor', 'Process', 'Rattus', 'Receptor Activation', 'Receptor Signaling', 'Resolution', 'Role', 'Running', 'Shapes', 'Signal Transduction', 'Smooth Muscle Myocytes', 'Testing', 'Tissues', 'Transducers', 'Vascular Smooth Muscle', 'base', 'information organization', 'mathematical model', 'receptor', 'receptor binding', 'receptor density', 'research study', 'role model', 'simulation', 'single molecule']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2016,58782,-0.0020281308456519364
"CRCNS: Information Flow in the Brain During Language and Meaning Comprehension DESCRIPTION (provided by applicant): Over the past ten years a good deal has been learned from fMRI studies about the spatial patterns of neural activation used by the human brain to represent meanings of words and concepts. Much less is understood about the time evolution of this neural activity, including the temporally interrelated sub-processes the brain employs during the hundreds of milliseconds it takes to comprehend a single word, or the more complex processes it uses to construct and encode meaning of entire sentences as the words arrive one by one. We propose research to study, and to build computational models of, the detailed spatial and temporal neural activity observed during the comprehension of single words, phrases, sentences, and stories. This proposed research will specifically target the following questions: ""What information is encoded by neural activity where and when, and by which subprocesses in the brain, during the time it takes to comprehend a single word in isolation?"" ""What is the flow of information encoded when a newly sensed word first activates sensory cortex, then later results in neural activation encoding the word meaning?"" ""How does the brain integrate a newly encountered word in the context of earlier words in the sentence or phrase, to compose the meaning representation of the multi-word phrase or sentence?"" and ""How do semantic expectations and demands, together with syntactic sentence structure alter the processing of words, compared to processing the same words in isolation, or as an unstructured set such as {kick, Joe, ball}?"" To study these questions we will (1) devise novel experimental protocols to probe the flow of information encoded in neural signals during word and sentence processing, (2) collect new brain image data using both fMRI to achieve spatial resolution of a few millimeters, and MEG to achieve temporal resolution of a few milliseconds, (3) develop and apply novel machine learning approaches to build computational models that integrate and that predict this combined experimental data. Our goal is to develop an increasingly accurate computational model of how the brain comprehends words, phrases and sentences - a model that makes testable predictions about the neural activity observed in response to novel language stimuli. Intellectual Merit: This collaborative research brings together advanced machine learning algorithms with novel experimental protocols for MEG and fMRI brain imaging to advance our understanding of two fundamental open questions about the human brain: how does the brain represent meaning, and what neuro-cognitive processes construct that meaning piece-by-piece from perceived language stimuli? Broader Impacts: If successful, this research will impact a broad range of communities, including (1) cognitive neuroscience and computational linguistics, providing improved understanding of language processing in the brain, (2) machine learning, by driving the development of new methods for time series and latent variable analysis, integrating multiple data sets, and incorporating diverse  background knowledge as priors, (3) clinical studies of brain pathologies, especially those related to language processing, and informing treatment strategies for developmental and acquired language disorders (4) education of graduates, undergraduates and the general public, through dissemination of technical articles, teaching materials, and news about our work in the public press. n/a",CRCNS: Information Flow in the Brain During Language and Meaning Comprehension,9107487,R01HD075328,"['Acquired Language Disorders', 'Algorithms', 'Automobile Driving', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Research', 'Communities', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Evolution', 'Functional Magnetic Resonance Imaging', 'General Population', 'Goals', 'Graduate Education', 'Human', 'Knowledge', 'Language', 'Learning', 'Machine Learning', 'Methods', 'Modeling', 'Process', 'Protocols documentation', 'Research', 'Resolution', 'Semantics', 'Series', 'Stimulus', 'Structure', 'Teaching Materials', 'Time', 'Word Processing', 'Work', 'cognitive neuroscience', 'cognitive process', 'expectation', 'improved', 'language processing', 'millimeter', 'millisecond', 'neural patterning', 'neurotransmission', 'news', 'novel', 'phrases', 'relating to nervous system', 'response', 'sensory cortex', 'syntax', 'temporal measurement', 'treatment strategy']",NICHD,CARNEGIE-MELLON UNIVERSITY,R01,2016,230351,0.008367453083184285
"CRCNS: Information Flow in the Brain During Language and Meaning Comprehension DESCRIPTION (provided by applicant): Over the past ten years a good deal has been learned from fMRI studies about the spatial patterns of neural activation used by the human brain to represent meanings of words and concepts. Much less is understood about the time evolution of this neural activity, including the temporally interrelated sub-processes the brain employs during the hundreds of milliseconds it takes to comprehend a single word, or the more complex processes it uses to construct and encode meaning of entire sentences as the words arrive one by one. We propose research to study, and to build computational models of, the detailed spatial and temporal neural activity observed during the comprehension of single words, phrases, sentences, and stories. This proposed research will specifically target the following questions: ""What information is encoded by neural activity where and when, and by which subprocesses in the brain, during the time it takes to comprehend a single word in isolation?"" ""What is the flow of information encoded when a newly sensed word first activates sensory cortex, then later results in neural activation encoding the word meaning?"" ""How does the brain integrate a newly encountered word in the context of earlier words in the sentence or phrase, to compose the meaning representation of the multi-word phrase or sentence?"" and ""How do semantic expectations and demands, together with syntactic sentence structure alter the processing of words, compared to processing the same words in isolation, or as an unstructured set such as {kick, Joe, ball}?"" To study these questions we will (1) devise novel experimental protocols to probe the flow of information encoded in neural signals during word and sentence processing, (2) collect new brain image data using both fMRI to achieve spatial resolution of a few millimeters, and MEG to achieve temporal resolution of a few milliseconds, (3) develop and apply novel machine learning approaches to build computational models that integrate and that predict this combined experimental data. Our goal is to develop an increasingly accurate computational model of how the brain comprehends words, phrases and sentences - a model that makes testable predictions about the neural activity observed in response to novel language stimuli. Intellectual Merit: This collaborative research brings together advanced machine learning algorithms with novel experimental protocols for MEG and fMRI brain imaging to advance our understanding of two fundamental open questions about the human brain: how does the brain represent meaning, and what neuro-cognitive processes construct that meaning piece-by-piece from perceived language stimuli? Broader Impacts: If successful, this research will impact a broad range of communities, including (1) cognitive neuroscience and computational linguistics, providing improved understanding of language processing in the brain, (2) machine learning, by driving the development of new methods for time series and latent variable analysis, integrating multiple data sets, and incorporating diverse  background knowledge as priors, (3) clinical studies of brain pathologies, especially those related to language processing, and informing treatment strategies for developmental and acquired language disorders (4) education of graduates, undergraduates and the general public, through dissemination of technical articles, teaching materials, and news about our work in the public press. n/a",CRCNS: Information Flow in the Brain During Language and Meaning Comprehension,8860217,R01HD075328,"['Acquired Language Disorders', 'Algorithms', 'Automobile Driving', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Research', 'Communities', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Evolution', 'Functional Magnetic Resonance Imaging', 'General Population', 'Goals', 'Graduate Education', 'Human', 'Knowledge', 'Language', 'Learning', 'Machine Learning', 'Methods', 'Modeling', 'Process', 'Protocols documentation', 'Research', 'Resolution', 'Semantics', 'Series', 'Stimulus', 'Structure', 'Teaching Materials', 'Time', 'Word Processing', 'Work', 'cognitive neuroscience', 'cognitive process', 'expectation', 'improved', 'language processing', 'millimeter', 'millisecond', 'neural patterning', 'neurotransmission', 'news', 'novel', 'phrases', 'relating to nervous system', 'response', 'sensory cortex', 'syntax', 'temporal measurement', 'treatment strategy']",NICHD,CARNEGIE-MELLON UNIVERSITY,R01,2015,228077,0.008367453083184285
"CRCNS: Information Flow in the Brain During Language and Meaning Comprehension DESCRIPTION (provided by applicant): Over the past ten years a good deal has been learned from fMRI studies about the spatial patterns of neural activation used by the human brain to represent meanings of words and concepts. Much less is understood about the time evolution of this neural activity, including the temporally interrelated sub-processes the brain employs during the hundreds of milliseconds it takes to comprehend a single word, or the more complex processes it uses to construct and encode meaning of entire sentences as the words arrive one by one. We propose research to study, and to build computational models of, the detailed spatial and temporal neural activity observed during the comprehension of single words, phrases, sentences, and stories. This proposed research will specifically target the following questions: ""What information is encoded by neural activity where and when, and by which subprocesses in the brain, during the time it takes to comprehend a single word in isolation?"" ""What is the flow of information encoded when a newly sensed word first activates sensory cortex, then later results in neural activation encoding the word meaning?"" ""How does the brain integrate a newly encountered word in the context of earlier words in the sentence or phrase, to compose the meaning representation of the multi-word phrase or sentence?"" and ""How do semantic expectations and demands, together with syntactic sentence structure alter the processing of words, compared to processing the same words in isolation, or as an unstructured set such as {kick, Joe, ball}?"" To study these questions we will (1) devise novel experimental protocols to probe the flow of information encoded in neural signals during word and sentence processing, (2) collect new brain image data using both fMRI to achieve spatial resolution of a few millimeters, and MEG to achieve temporal resolution of a few milliseconds, (3) develop and apply novel machine learning approaches to build computational models that integrate and that predict this combined experimental data. Our goal is to develop an increasingly accurate computational model of how the brain comprehends words, phrases and sentences - a model that makes testable predictions about the neural activity observed in response to novel language stimuli. Intellectual Merit: This collaborative research brings together advanced machine learning algorithms with novel experimental protocols for MEG and fMRI brain imaging to advance our understanding of two fundamental open questions about the human brain: how does the brain represent meaning, and what neuro-cognitive processes construct that meaning piece-by-piece from perceived language stimuli? Broader Impacts: If successful, this research will impact a broad range of communities, including (1) cognitive neuroscience and computational linguistics, providing improved understanding of language processing in the brain, (2) machine learning, by driving the development of new methods for time series and latent variable analysis, integrating multiple data sets, and incorporating diverse  background knowledge as priors, (3) clinical studies of brain pathologies, especially those related to language processing, and informing treatment strategies for developmental and acquired language disorders (4) education of graduates, undergraduates and the general public, through dissemination of technical articles, teaching materials, and news about our work in the public press. n/a",CRCNS: Information Flow in the Brain During Language and Meaning Comprehension,8692440,R01HD075328,"['Acquired Language Disorders', 'Algorithms', 'Automobile Driving', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Research', 'Cognitive', 'Communities', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Evolution', 'Functional Magnetic Resonance Imaging', 'General Population', 'Goals', 'Graduate Education', 'Human', 'Knowledge', 'Language', 'Learning', 'Machine Learning', 'Methods', 'Modeling', 'Process', 'Protocols documentation', 'Research', 'Resolution', 'Semantics', 'Series', 'Signal Transduction', 'Stimulus', 'Structure', 'Teaching Materials', 'Time', 'Word Processing', 'Work', 'cognitive neuroscience', 'expectation', 'improved', 'language processing', 'millimeter', 'millisecond', 'neural patterning', 'news', 'novel', 'phrases', 'relating to nervous system', 'response', 'sensory cortex', 'syntax', 'treatment strategy']",NICHD,CARNEGIE-MELLON UNIVERSITY,R01,2014,227375,0.008367453083184285
"CRCNS: Information Flow in the Brain During Language and Meaning Comprehension DESCRIPTION (provided by applicant): Over the past ten years a good deal has been learned from fMRI studies about the spatial patterns of neural activation used by the human brain to represent meanings of words and concepts. Much less is understood about the time evolution of this neural activity, including the temporally interrelated sub-processes the brain employs during the hundreds of milliseconds it takes to comprehend a single word, or the more complex processes it uses to construct and encode meaning of entire sentences as the words arrive one by one. We propose research to study, and to build computational models of, the detailed spatial and temporal neural activity observed during the comprehension of single words, phrases, sentences, and stories. This proposed research will specifically target the following questions: ""What information is encoded by neural activity where and when, and by which subprocesses in the brain, during the time it takes to comprehend a single word in isolation?"" ""What is the flow of information encoded when a newly sensed word first activates sensory cortex, then later results in neural activation encoding the word meaning?"" ""How does the brain integrate a newly encountered word in the context of earlier words in the sentence or phrase, to compose the meaning representation of the multi-word phrase or sentence?"" and ""How do semantic expectations and demands, together with syntactic sentence structure alter the processing of words, compared to processing the same words in isolation, or as an unstructured set such as {kick, Joe, ball}?"" To study these questions we will (1) devise novel experimental protocols to probe the flow of information encoded in neural signals during word and sentence processing, (2) collect new brain image data using both fMRI to achieve spatial resolution of a few millimeters, and MEG to achieve temporal resolution of a few milliseconds, (3) develop and apply novel machine learning approaches to build computational models that integrate and that predict this combined experimental data. Our goal is to develop an increasingly accurate computational model of how the brain comprehends words, phrases and sentences - a model that makes testable predictions about the neural activity observed in response to novel language stimuli. Intellectual Merit: This collaborative research brings together advanced machine learning algorithms with novel experimental protocols for MEG and fMRI brain imaging to advance our understanding of two fundamental open questions about the human brain: how does the brain represent meaning, and what neuro-cognitive processes construct that meaning piece-by-piece from perceived language stimuli? Broader Impacts: If successful, this research will impact a broad range of communities, including (1) cognitive neuroscience and computational linguistics, providing improved understanding of language processing in the brain, (2) machine learning, by driving the development of new methods for time series and latent variable analysis, integrating multiple data sets, and incorporating diverse  background knowledge as priors, (3) clinical studies of brain pathologies, especially those related to language processing, and informing treatment strategies for developmental and acquired language disorders (4) education of graduates, undergraduates and the general public, through dissemination of technical articles, teaching materials, and news about our work in the public press. n/a",CRCNS: Information Flow in the Brain During Language and Meaning Comprehension,8532012,R01HD075328,"['Acquired Language Disorders', 'Algorithms', 'Automobile Driving', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Research', 'Cognitive', 'Communities', 'Complex', 'Comprehension', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Evolution', 'Functional Magnetic Resonance Imaging', 'General Population', 'Goals', 'Graduate Education', 'Human', 'Knowledge', 'Language', 'Learning', 'Linguistics', 'Machine Learning', 'Methods', 'Modeling', 'Process', 'Protocols documentation', 'Research', 'Resolution', 'Semantics', 'Series', 'Signal Transduction', 'Stimulus', 'Structure', 'Teaching Materials', 'Time', 'Word Processing', 'Work', 'cognitive neuroscience', 'expectation', 'improved', 'language processing', 'millimeter', 'millisecond', 'neural patterning', 'news', 'novel', 'phrases', 'relating to nervous system', 'response', 'sensory cortex', 'syntax', 'treatment strategy']",NICHD,CARNEGIE-MELLON UNIVERSITY,R01,2013,227049,0.008367453083184285
"CRCNS: Information Flow in the Brain During Language and Meaning Comprehension DESCRIPTION (provided by applicant): Over the past ten years a good deal has been learned from fMRI studies about the spatial patterns of neural activation used by the human brain to represent meanings of words and concepts. Much less is understood about the time evolution of this neural activity, including the temporally interrelated sub-processes the brain employs during the hundreds of milliseconds it takes to comprehend a single word, or the more complex processes it uses to construct and encode meaning of entire sentences as the words arrive one by one. We propose research to study, and to build computational models of, the detailed spatial and temporal neural activity observed during the comprehension of single words, phrases, sentences, and stories. This proposed research will specifically target the following questions: ""What information is encoded by neural activity where and when, and by which subprocesses in the brain, during the time it takes to comprehend a single word in isolation?"" ""What is the flow of information encoded when a newly sensed word first activates sensory cortex, then later results in neural activation encoding the word meaning?"" ""How does the brain integrate a newly encountered word in the context of earlier words in the sentence or phrase, to compose the meaning representation of the multi-word phrase or sentence?"" and ""How do semantic expectations and demands, together with syntactic sentence structure alter the processing of words, compared to processing the same words in isolation, or as an unstructured set such as {kick, Joe, ball}?"" To study these questions we will (1) devise novel experimental protocols to probe the flow of information encoded in neural signals during word and sentence processing, (2) collect new brain image data using both fMRI to achieve spatial resolution of a few millimeters, and MEG to achieve temporal resolution of a few milliseconds, (3) develop and apply novel machine learning approaches to build computational models that integrate and that predict this combined experimental data. Our goal is to develop an increasingly accurate computational model of how the brain comprehends words, phrases and sentences - a model that makes testable predictions about the neural activity observed in response to novel language stimuli. Intellectual Merit: This collaborative research brings together advanced machine learning algorithms with novel experimental protocols for MEG and fMRI brain imaging to advance our understanding of two fundamental open questions about the human brain: how does the brain represent meaning, and what neuro-cognitive processes construct that meaning piece-by-piece from perceived language stimuli? Broader Impacts: If successful, this research will impact a broad range of communities, including (1) cognitive neuroscience and computational linguistics, providing improved understanding of language processing in the brain, (2) machine learning, by driving the development of new methods for time series and latent variable analysis, integrating multiple data sets, and incorporating diverse background knowledge as priors, (3) clinical studies of brain pathologies, especially those related to language processing, and informing treatment strategies for developmental and acquired language disorders (4) education of graduates, undergraduates and the general public, through dissemination of technical articles, teaching materials, and news about our work in the public press. n/a",CRCNS: Information Flow in the Brain During Language and Meaning Comprehension,8444787,R01HD075328,"['Acquired Language Disorders', 'Algorithms', 'Automobile Driving', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Research', 'Cognitive', 'Communities', 'Complex', 'Comprehension', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Evolution', 'Functional Magnetic Resonance Imaging', 'General Population', 'Goals', 'Graduate Education', 'Human', 'Knowledge', 'Language', 'Learning', 'Linguistics', 'Machine Learning', 'Methods', 'Modeling', 'Process', 'Protocols documentation', 'Research', 'Resolution', 'Semantics', 'Series', 'Signal Transduction', 'Stimulus', 'Structure', 'Teaching Materials', 'Time', 'Word Processing', 'Work', 'cognitive neuroscience', 'expectation', 'improved', 'language processing', 'millimeter', 'millisecond', 'neural patterning', 'news', 'novel', 'phrases', 'relating to nervous system', 'response', 'sensory cortex', 'syntax', 'treatment strategy']",NICHD,CARNEGIE-MELLON UNIVERSITY,R01,2012,246375,0.008367453083184285
"Analysis of Multi-Voxel Patterns of Activity in fMRI data    DESCRIPTION (provided by applicant):  fMRI experiments produce large, numerically rich, but noisy data sets that pose a challenge for extracting the signal variance and establishing the correspondence between that signal and cognitive variables.  Conventional analysis has reduced the dimensionality of fMRI data by searching for clusters of voxels that show similar responses to experimental manipulations and averaging the signal within those clusters.  We have introduced a new approach to fMRI data analysis, ""multi-voxel pattern analysis"", that examines higher spatial frequency patterns of activity - the voxel-by-voxel variation of response within a region - and have shown that this method greatly increases the sensitivity of fMRI (Haxby et al. 2001; Hanson et al. 2004; OToole et al. 2004; Polyn et al. 2004).  In the proposed investigations, we will develop new methods for analysis of spatially-distributed patterns of neural activity in relation to two specific problems in fMRI data analysis:  1.  accounting for inter-individual variation in functional neuroanatomy, and 2.  the relation between spatially-distributed neural population responses and cognitive representations.  This work will involve the efforts of a multidisciplinary team consisting of cognitive neuroscientists, applied mathematicians, and signal- processing engineers.  We propose the development of analytic methods for aligning the functional neuroanatomy of individual brains based on the patterns of neural activity that are elicited by a broad spectrum of cognitive activities.  We predict that these methods will enhance the sensitivity of group statistical tests of fMRI data, will allow the investigation of the inter-individual consistency of higher spatial frequency topographic representations, and will provide explicit measures of inter-individual variation in the location, organization, and spatial extent of functional maps, with potential applications for studies of clinical conditions.  We propose, further, to develop methods for detecting and analyzing distributed patterns of neural activity that make use of prior knowledge about the structure of the cognitive representations that are associated with those neural activities.  We predict that these methods will increase the sensitivity of multi-voxel pattern analysis and will allow the investigation of how cognitive information is represented in topographically-organized, spatially-distributed patterns of neural activity.         n/a",Analysis of Multi-Voxel Patterns of Activity in fMRI data,7480923,R01MH075706,"['Accounting', 'Address', 'Affect', 'Anatomy', 'Area', 'Base of the Brain', 'Biological Neural Networks', 'Brain', 'Categories', 'Classification', 'Clinical', 'Code', 'Cognitive', 'Condition', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Discriminant Analysis', 'Engineering', 'Facial Expression', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Human', 'Individual', 'Individual Differences', 'Investigation', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Neuroanatomy', 'Normal Range', 'Pattern', 'Performance', 'Population', 'Principal Investigator', 'Range', 'Signal Transduction', 'Stimulus', 'Structure', 'Surface', 'Testing', 'Variant', 'Work', 'base', 'computerized data processing', 'design', 'improved', 'innovation', 'method development', 'multidisciplinary', 'novel strategies', 'psychologic', 'relating to nervous system', 'research study', 'response', 'two-dimensional']",NIMH,DARTMOUTH COLLEGE,R01,2008,319340,-0.036560448460097955
"Analysis of Multi-Voxel Patterns of Activity in fMRI data    DESCRIPTION (provided by applicant):  fMRI experiments produce large, numerically rich, but noisy data sets that pose a challenge for extracting the signal variance and establishing the correspondence between that signal and cognitive variables.  Conventional analysis has reduced the dimensionality of fMRI data by searching for clusters of voxels that show similar responses to experimental manipulations and averaging the signal within those clusters.  We have introduced a new approach to fMRI data analysis, ""multi-voxel pattern analysis"", that examines higher spatial frequency patterns of activity - the voxel-by-voxel variation of response within a region - and have shown that this method greatly increases the sensitivity of fMRI (Haxby et al. 2001; Hanson et al. 2004; OToole et al. 2004; Polyn et al. 2004).  In the proposed investigations, we will develop new methods for analysis of spatially-distributed patterns of neural activity in relation to two specific problems in fMRI data analysis:  1.  accounting for inter-individual variation in functional neuroanatomy, and 2.  the relation between spatially-distributed neural population responses and cognitive representations.  This work will involve the efforts of a multidisciplinary team consisting of cognitive neuroscientists, applied mathematicians, and signal- processing engineers.  We propose the development of analytic methods for aligning the functional neuroanatomy of individual brains based on the patterns of neural activity that are elicited by a broad spectrum of cognitive activities.  We predict that these methods will enhance the sensitivity of group statistical tests of fMRI data, will allow the investigation of the inter-individual consistency of higher spatial frequency topographic representations, and will provide explicit measures of inter-individual variation in the location, organization, and spatial extent of functional maps, with potential applications for studies of clinical conditions.  We propose, further, to develop methods for detecting and analyzing distributed patterns of neural activity that make use of prior knowledge about the structure of the cognitive representations that are associated with those neural activities.  We predict that these methods will increase the sensitivity of multi-voxel pattern analysis and will allow the investigation of how cognitive information is represented in topographically-organized, spatially-distributed patterns of neural activity.         n/a",Analysis of Multi-Voxel Patterns of Activity in fMRI data,7846781,R01MH075706,"['Accounting', 'Address', 'Affect', 'Anatomy', 'Area', 'Base of the Brain', 'Biological Neural Networks', 'Brain', 'Categories', 'Classification', 'Clinical', 'Code', 'Cognitive', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Discriminant Analysis', 'Engineering', 'Facial Expression', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Human', 'Individual', 'Individual Differences', 'Investigation', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Neuroanatomy', 'Normal Range', 'Pattern', 'Performance', 'Population', 'Principal Investigator', 'Signal Transduction', 'Stimulus', 'Structure', 'Surface', 'Testing', 'Variant', 'Work', 'base', 'computerized data processing', 'design', 'improved', 'innovation', 'method development', 'multidisciplinary', 'neural patterning', 'novel strategies', 'psychologic', 'relating to nervous system', 'research study', 'response', 'two-dimensional']",NIMH,DARTMOUTH COLLEGE,R01,2010,317364,-0.036560448460097955
"Analysis of Multi-Voxel Patterns of Activity in fMRI data    DESCRIPTION (provided by applicant):  fMRI experiments produce large, numerically rich, but noisy data sets that pose a challenge for extracting the signal variance and establishing the correspondence between that signal and cognitive variables.  Conventional analysis has reduced the dimensionality of fMRI data by searching for clusters of voxels that show similar responses to experimental manipulations and averaging the signal within those clusters.  We have introduced a new approach to fMRI data analysis, ""multi-voxel pattern analysis"", that examines higher spatial frequency patterns of activity - the voxel-by-voxel variation of response within a region - and have shown that this method greatly increases the sensitivity of fMRI (Haxby et al. 2001; Hanson et al. 2004; OToole et al. 2004; Polyn et al. 2004).  In the proposed investigations, we will develop new methods for analysis of spatially-distributed patterns of neural activity in relation to two specific problems in fMRI data analysis:  1.  accounting for inter-individual variation in functional neuroanatomy, and 2.  the relation between spatially-distributed neural population responses and cognitive representations.  This work will involve the efforts of a multidisciplinary team consisting of cognitive neuroscientists, applied mathematicians, and signal- processing engineers.  We propose the development of analytic methods for aligning the functional neuroanatomy of individual brains based on the patterns of neural activity that are elicited by a broad spectrum of cognitive activities.  We predict that these methods will enhance the sensitivity of group statistical tests of fMRI data, will allow the investigation of the inter-individual consistency of higher spatial frequency topographic representations, and will provide explicit measures of inter-individual variation in the location, organization, and spatial extent of functional maps, with potential applications for studies of clinical conditions.  We propose, further, to develop methods for detecting and analyzing distributed patterns of neural activity that make use of prior knowledge about the structure of the cognitive representations that are associated with those neural activities.  We predict that these methods will increase the sensitivity of multi-voxel pattern analysis and will allow the investigation of how cognitive information is represented in topographically-organized, spatially-distributed patterns of neural activity.         n/a",Analysis of Multi-Voxel Patterns of Activity in fMRI data,7692174,R01MH075706,"['Accounting', 'Address', 'Affect', 'Anatomy', 'Area', 'Base of the Brain', 'Biological Neural Networks', 'Brain', 'Categories', 'Classification', 'Clinical', 'Code', 'Cognitive', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Discriminant Analysis', 'Engineering', 'Facial Expression', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Human', 'Individual', 'Individual Differences', 'Investigation', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Neuroanatomy', 'Normal Range', 'Pattern', 'Performance', 'Population', 'Principal Investigator', 'Signal Transduction', 'Stimulus', 'Structure', 'Surface', 'Testing', 'Variant', 'Work', 'base', 'computerized data processing', 'design', 'improved', 'innovation', 'method development', 'multidisciplinary', 'neural patterning', 'novel strategies', 'psychologic', 'relating to nervous system', 'research study', 'response', 'two-dimensional']",NIMH,DARTMOUTH COLLEGE,R01,2009,318428,-0.036560448460097955
"Analysis of Multi-Voxel Patterns of Activity in fMRI data    DESCRIPTION (provided by applicant):  fMRI experiments produce large, numerically rich, but noisy data sets that pose a challenge for extracting the signal variance and establishing the correspondence between that signal and cognitive variables.  Conventional analysis has reduced the dimensionality of fMRI data by searching for clusters of voxels that show similar responses to experimental manipulations and averaging the signal within those clusters.  We have introduced a new approach to fMRI data analysis, ""multi-voxel pattern analysis"", that examines higher spatial frequency patterns of activity - the voxel-by-voxel variation of response within a region - and have shown that this method greatly increases the sensitivity of fMRI (Haxby et al. 2001; Hanson et al. 2004; OToole et al. 2004; Polyn et al. 2004).  In the proposed investigations, we will develop new methods for analysis of spatially-distributed patterns of neural activity in relation to two specific problems in fMRI data analysis:  1.  accounting for inter-individual variation in functional neuroanatomy, and 2.  the relation between spatially-distributed neural population responses and cognitive representations.  This work will involve the efforts of a multidisciplinary team consisting of cognitive neuroscientists, applied mathematicians, and signal- processing engineers.  We propose the development of analytic methods for aligning the functional neuroanatomy of individual brains based on the patterns of neural activity that are elicited by a broad spectrum of cognitive activities.  We predict that these methods will enhance the sensitivity of group statistical tests of fMRI data, will allow the investigation of the inter-individual consistency of higher spatial frequency topographic representations, and will provide explicit measures of inter-individual variation in the location, organization, and spatial extent of functional maps, with potential applications for studies of clinical conditions.  We propose, further, to develop methods for detecting and analyzing distributed patterns of neural activity that make use of prior knowledge about the structure of the cognitive representations that are associated with those neural activities.  We predict that these methods will increase the sensitivity of multi-voxel pattern analysis and will allow the investigation of how cognitive information is represented in topographically-organized, spatially-distributed patterns of neural activity.         n/a",Analysis of Multi-Voxel Patterns of Activity in fMRI data,7613805,R01MH075706,"['Accounting', 'Address', 'Affect', 'Anatomy', 'Area', 'Base of the Brain', 'Biological Neural Networks', 'Brain', 'Categories', 'Classification', 'Clinical', 'Code', 'Cognitive', 'Condition', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Discriminant Analysis', 'Engineering', 'Facial Expression', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Human', 'Individual', 'Individual Differences', 'Investigation', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Neuroanatomy', 'Normal Range', 'Pattern', 'Performance', 'Population', 'Principal Investigator', 'Range', 'Signal Transduction', 'Stimulus', 'Structure', 'Surface', 'Testing', 'Variant', 'Work', 'base', 'computerized data processing', 'design', 'improved', 'innovation', 'method development', 'multidisciplinary', 'novel strategies', 'psychologic', 'relating to nervous system', 'research study', 'response', 'two-dimensional']",NIMH,DARTMOUTH COLLEGE,R01,2007,320177,-0.036560448460097955
"Analysis of Multi-Voxel Patterns of Activity in fMRI data    DESCRIPTION (provided by applicant):  fMRI experiments produce large, numerically rich, but noisy data sets that pose a challenge for extracting the signal variance and establishing the correspondence between that signal and cognitive variables.  Conventional analysis has reduced the dimensionality of fMRI data by searching for clusters of voxels that show similar responses to experimental manipulations and averaging the signal within those clusters.  We have introduced a new approach to fMRI data analysis, ""multi-voxel pattern analysis"", that examines higher spatial frequency patterns of activity - the voxel-by-voxel variation of response within a region - and have shown that this method greatly increases the sensitivity of fMRI (Haxby et al. 2001; Hanson et al. 2004; OToole et al. 2004; Polyn et al. 2004).  In the proposed investigations, we will develop new methods for analysis of spatially-distributed patterns of neural activity in relation to two specific problems in fMRI data analysis:  1.  accounting for inter-individual variation in functional neuroanatomy, and 2.  the relation between spatially-distributed neural population responses and cognitive representations.  This work will involve the efforts of a multidisciplinary team consisting of cognitive neuroscientists, applied mathematicians, and signal- processing engineers.  We propose the development of analytic methods for aligning the functional neuroanatomy of individual brains based on the patterns of neural activity that are elicited by a broad spectrum of cognitive activities.  We predict that these methods will enhance the sensitivity of group statistical tests of fMRI data, will allow the investigation of the inter-individual consistency of higher spatial frequency topographic representations, and will provide explicit measures of inter-individual variation in the location, organization, and spatial extent of functional maps, with potential applications for studies of clinical conditions.  We propose, further, to develop methods for detecting and analyzing distributed patterns of neural activity that make use of prior knowledge about the structure of the cognitive representations that are associated with those neural activities.  We predict that these methods will increase the sensitivity of multi-voxel pattern analysis and will allow the investigation of how cognitive information is represented in topographically-organized, spatially-distributed patterns of neural activity.         n/a",Analysis of Multi-Voxel Patterns of Activity in fMRI data,7146469,R01MH075706,"['artificial intelligence', 'bioimaging /biomedical imaging', 'brain mapping', 'cerebral cortex', 'clinical research', 'cognition', 'computational neuroscience', 'computer program /software', 'face expression', 'functional magnetic resonance imaging', 'human subject', 'neural information processing', 'neuroanatomy', 'neuroimaging', 'statistics /biometry', 'three dimensional imaging /topography', 'videotape /videodisc', 'visual perception']",NIMH,PRINCETON UNIVERSITY,R01,2006,329481,-0.036560448460097955
"HIGH RESOLUTION ROBOTIC TELEMAMMOGRAPHY SYSTEM Telemammography requires high spatial resolution, the transfer of large          image files and rapid lossless transmission of images.  Current                  teleradiology techniques fail to meet these requirements.  A novel               approach addressing these difficulties, involving robotic control of             image acquisition, optical magnification and unique transmission                 strategies licensed from BellSouth Telecommunications, is proposed.              Phase I goals include prototype development, software design and                 testing, objective characterization of the system and subjective system          evaluation by a board certified radiologist.  Following completion of            Phase I, a Phase II application will be filed.  Phase II will include            optimization of system configuration, extensive testing and evaluation,          both in-house and by consulting radiologists, culminating in clinical            trials at a minimum of three sites.  Following this we expect to file for        FDA clearance of the device.  This research should result in affordable,         high resolution telemammography system that will provide better                  service to remote locations.  With this system, even remote locations            could have access to expertise and rapid diagnosis, affording women              better care in a stressful time.  This novel approach will have                  application to all of teleradiology, especially those specialties requiring      a high spatial resolution.                                                                                                                                        PROPOSED COMMERCIAL APPLICATION:  The proposed project will result               in a telemammography system that allows re-acquisition of high spatial           resolution images by a remote radiologist via a roboticly controlled X-          ray viewer. This system will allow transmission of high quality                  mammographic images from a center to a remote physician or form a                remote hospital or clinic to a center for a more rapid referral and/or           diagnosis. This improved affordable system for the transmission of               radiologic images will have application to teleradiology in general.              n/a",HIGH RESOLUTION ROBOTIC TELEMAMMOGRAPHY SYSTEM,2422962,R43CA075766,"['artificial intelligence', ' biomedical equipment development', ' charge coupled device camera', ' clinical biomedical equipment', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' digital imaging', ' human data', ' image enhancement', ' mammography', ' phantom model', ' robotics', ' telemetry']",NCI,"DIVERSIFIED SCIENTIFIC, INC.",R43,1997,100000,-0.0008334307178780395
"DIGITAL CZT X FOR GAMMA RAY IMAGING DETECTOR Imaging detectors for photons in the 10 to 150 keV energy range have             many uses in medical technology including tumor imaging, SPECT, and              radiography.  Digital output is particularly useful since it allows              image enhancement, analysis, transmission and storage.  An approach is           proposed for developing a new technology, using CdZnTe detector                  material, which would allow images to be obtained with 100 u spatial             resolution and energy resolution of 2% or better. This capability would          facilitate entirely new classes of medical diagnostic procedures. For            example, ""dual energy"" angiography could now be done using a                     polychromatic x-ray source.                                                                                                                                       The present effort will demonstrate the feasibility of the conceptual            approach and create the technological foundation upon which later,               application specific instruments can be built.  Detectors will be                modeled, using Monte Carlo and electron transport, to predict their              signal outputs. These outputs will be compared to measured signals to            validate the models. The models will then be employed to develop signal          processing algorithms which achieve the desired energy and spatial               resolutions.  Finally, electronics will be designed to implement the             algorithms.  In Phase II a working model would be constructed and                tested.                                                                          PROPOSED COMMERCIAL APPLICATION                                                  As an energy resolved digital detector with 100 u spatial resolution,            the proposed detector technology could find many medical applications,           including SPECT, energy resolved angiography for small mammals, bone             densitometry on rodent bones, and small, hand-held gamma cameras.  Non-          medical applications would include non-destructive testing,                      astrophysical gamma imaging, nuclear cleanup uses, and scientific                instruments.                                                                      n/a",DIGITAL CZT X FOR GAMMA RAY IMAGING DETECTOR,2423599,R43CA075844,"['X ray', ' artificial intelligence', ' biomedical equipment development', ' cadmium', ' clinical biomedical equipment', ' digital imaging', ' gamma radiation', ' imaging /visualization /scanning', ' tellurium', ' zinc']",NCI,X-RAY INSTRUMENTATION ASSOCIATES,R43,1997,100000,0.005260642175620633
"Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches Project Summary The form (or shape) and function relationship of anatomical structures is a central theme in biology where abnor- mal shape changes are closely tied to pathological functions. Morphometrics has been an indispensable quan- titative tool in medical and biological sciences to study anatomical forms for more than 100 years. Recently, the increased availability of high-resolution in-vivo images of anatomy has led to the development of a new generation of morphometric approaches, called statistical shape modeling (SSM), that take advantage of modern computa- tional techniques to model anatomical shapes and their variability within populations with unprecedented detail. SSM stands to revolutionize morphometric analysis, but its widespread adoption is hindered by a number of sig- niﬁcant challenges, including the complexity of the approaches and their increased computational requirements, relative to traditional morphometrics. Arguably, however, the most important roadblock to more widespread adop- tion is the lack of user-friendly and scalable software tools for a variety of anatomical surfaces that can be readily incorporated into biomedical research labs. The goal of this proposal is thus to address these challenges in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM), which automat- ically constructs optimal statistical landmark-based shape models of ensembles of anatomical shapes without relying on any speciﬁc surface parameterization. The proposed research will provide an automated, general- purpose, and scalable computational solution for constructing shape models of general anatomy. In Aim 1, we will build computational and machine learning algorithms to model anatomies with complex surface topologies (e.g., surface openings and shared boundaries) and highly variable anatomical populations. In Aim 2, we will introduce an end-to-end machine learning approach to extract statistical shape representation directly from im- ages, requiring no parameter tuning, image pre-processing, or user assistance. In Aim 3, we will provide intuitive graphical user interfaces and visualization tools to incorporate user-deﬁned modeling preferences and promote the visual interpretation of shape models. We will also make use of recent advances in cloud computing to enable researchers with limited computational resources and/or large cohorts to build and execute custom SSM work- ﬂows using remote scalable computational resources. Algorithmic developments will be thoroughly evaluated and validated using existing, fully funded, large-scale, and constantly growing databases of CT and MRI images lo- cated on-site. Furthermore, we will develop and disseminate standard workﬂows and domain-speciﬁc use cases for complex anatomies to promote reproducibility. Efforts to develop the proposed technology are aligned with the mission of the National Institute of General Medical Sciences (NIGMS), and its third strategic goal: to bridge biology and quantitative science for better global health through supporting the development of and access to computational research tools for biomedical research. Our long-term goal is to increase the clinical utility and widespread adoption of SSM, and the proposed research will establish the groundwork for achieving this goal. Project Narrative This project will develop general-purpose, scalable, and open-source statistical shape modeling (SSM) tools, which will present unique capabilities for automated anatomy modeling with less user input. The proposed tech- nology will introduce a number of signiﬁcant improvements to current SSM approaches and tools, including the support for challenging modeling problems, inferring shapes directly from images (and hence bypassing the seg- mentation step), parallel optimizations for speed, and new user interfaces that will be much easier and scalable than the current tools. The proposed technology will constitute an indispensable resource for the biomedical and clinical communities that will enable new avenues for biomedical research and clinical investigations, provide new ways to answer biologically related questions, allow new types of questions to be asked, and open the door for the integration of SSM with clinical care.","Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches",9969467,R01AR076120,"['Address', 'Adoption', 'Age', 'Algorithms', 'Anatomic Models', 'Anatomic Surface', 'Anatomy', 'Area', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Brain', 'Bypass', 'Cardiology', 'Cessation of life', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collection', 'Communities', 'Complex', 'Complex Analysis', 'Computational Technique', 'Computer Models', 'Computer software', 'Computers', 'Custom', 'Data', 'Databases', 'Development', 'Disease', 'Felis catus', 'Funding', 'Generations', 'Geometry', 'Goals', 'Human', 'Ice', 'Image', 'Imagery', 'Injury', 'Intuition', 'Laboratory Research', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematical Computing', 'Measures', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Modernization', 'Modification', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Occupations', 'Online Systems', 'Organism', 'Orthopedics', 'Pathologic', 'Population', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Shapes', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Speed', 'Statistical Data Interpretation', 'Structure', 'Supervision', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Variant', 'Visual', 'Visualization software', 'Work', 'algorithm development', 'base', 'biomedical resource', 'clinical care', 'clinical investigation', 'clinically relevant', 'cohort', 'computerized tools', 'computing resources', 'deep learning', 'experience', 'flexibility', 'global health', 'graphical user interface', 'image archival system', 'image processing', 'imaging Segmentation', 'in vivo imaging', 'innovation', 'large datasets', 'machine learning algorithm', 'model development', 'multidisciplinary', 'open source', 'particle', 'preference', 'software development', 'tool', 'usability', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,614363,-0.032730203485249805
"ShapeWorks in the Cloud Project Summary This application is submitted in response to NOT-OD-20-073 as an administrative supplement to the parent award R01AR076120 titled: ""Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches."" The form (or shape) of anatomies is the clinical language that describes abnormal mor- phologies tied to pathologic functions. Quantifying such subtle morphological shape changes requires parsing the anatomy into a quantitative description that is consistent across the population in question. For more than 100 years, morphometrics has been an indispensable quantitative tool in medical and biological sciences to study anatomical forms. But its representation capacity is limited to linear distances, angles, and areas. Sta- tistical shape modeling (SSM) is the computational extension of classical morphometric techniques to analyze more detailed representations of complex anatomy and their variability within populations The parent award ad- dresses existing roadblocks for the widespread adoption of SSM computational tools in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM) and its associated suite of open-source software tools, ShapeWorks. ShapeWorks enables learning population-level shape representation via automatic dense placement of homologous landmarks on image segmentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread applicability and impact to medicine and biology are hindered by computational barriers that most existing shape modeling packages face. The goal of this supplement award is to provide supplemental support for Aim 3 of the parent award to leverage best practices in software development and advances in cloud computing to enable researchers with limited computational resources and/or large-scale cohorts to build and execute custom SSM workﬂows us- ing remote scalable computational resources. To achieve this goal, we have developed a plan to enhance the design, implementation, and cloud-readiness of ShapeWorks and augmented our scientiﬁc team to add senior, experienced software engineers/developers who have extensive experience in professional programming, code refactoring, and scientiﬁc computing. This award will provide our team with the support necessary to (Aim 1) de- sign ShapeWorks as a collection of modular and reusable services, (Aim 2) decouple ShapeWorks services from explicitly encoded data sources, and (Aim 3) refactor ShapeWorks to scale efﬁciently on the cloud. All software development will be performed in adherence to software engineering practices and design principles, including coding style, documentation, and version control. The proposed efforts will be released as open-source software in a manner consistent with the principles of reproducible research and the practices of open science. Our long- term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein in addition to the parent award will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. The impact and scientiﬁc value of ShapeWorks have been recognized in a range of applications, including psychology, biological phenotyping, car- diology, and orthopedics. If funded, this supplement will provide support to revise, refactor, and redeploy Shape- Works to take advantage of new cloud computing paradigms, to be robust, sustainable, scalable, and accessible to a broader community, and to address the growing need for shape modeling tools to handle large collections of clinical data and to obtain sufﬁcient statistical power for large shape studies.",ShapeWorks in the Cloud,10166337,R01AR076120,"['Address', 'Adherence', 'Administrative Supplement', 'Adoption', 'Anatomy', 'Applied Research', 'Architecture', 'Area', 'Award', 'Biological', 'Biological Sciences', 'Biology', 'Cardiology', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Cloud Computing', 'Cloud Service', 'Code', 'Collection', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computer Models', 'Computer software', 'Computers', 'Coupled', 'Custom', 'Data', 'Data Sources', 'Databases', 'Disabled Persons', 'Documentation', 'Environment', 'Face', 'Funding', 'Goals', 'Image', 'Imagery', 'Language', 'Learning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'Occupations', 'Online Systems', 'Orthopedics', 'Parents', 'Pathologic', 'Phenotype', 'Population', 'Privatization', 'Psychology', 'Readiness', 'Reproducibility', 'Research', 'Research Personnel', 'Running', 'Scientist', 'Services', 'Shapes', 'Software Design', 'Software Engineering', 'Software Tools', 'Source Code', 'Speed', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'base', 'cohort', 'computational platform', 'computerized tools', 'computing resources', 'data management', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'large datasets', 'model development', 'open data', 'open source', 'particle', 'response', 'scientific computing', 'shape analysis', 'software development', 'statistics', 'tool', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,210000,-0.02712657813100972
"Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches Project Summary The form (or shape) and function relationship of anatomical structures is a central theme in biology where abnor- mal shape changes are closely tied to pathological functions. Morphometrics has been an indispensable quan- titative tool in medical and biological sciences to study anatomical forms for more than 100 years. Recently, the increased availability of high-resolution in-vivo images of anatomy has led to the development of a new generation of morphometric approaches, called statistical shape modeling (SSM), that take advantage of modern computa- tional techniques to model anatomical shapes and their variability within populations with unprecedented detail. SSM stands to revolutionize morphometric analysis, but its widespread adoption is hindered by a number of sig- niﬁcant challenges, including the complexity of the approaches and their increased computational requirements, relative to traditional morphometrics. Arguably, however, the most important roadblock to more widespread adop- tion is the lack of user-friendly and scalable software tools for a variety of anatomical surfaces that can be readily incorporated into biomedical research labs. The goal of this proposal is thus to address these challenges in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM), which automat- ically constructs optimal statistical landmark-based shape models of ensembles of anatomical shapes without relying on any speciﬁc surface parameterization. The proposed research will provide an automated, general- purpose, and scalable computational solution for constructing shape models of general anatomy. In Aim 1, we will build computational and machine learning algorithms to model anatomies with complex surface topologies (e.g., surface openings and shared boundaries) and highly variable anatomical populations. In Aim 2, we will introduce an end-to-end machine learning approach to extract statistical shape representation directly from im- ages, requiring no parameter tuning, image pre-processing, or user assistance. In Aim 3, we will provide intuitive graphical user interfaces and visualization tools to incorporate user-deﬁned modeling preferences and promote the visual interpretation of shape models. We will also make use of recent advances in cloud computing to enable researchers with limited computational resources and/or large cohorts to build and execute custom SSM work- ﬂows using remote scalable computational resources. Algorithmic developments will be thoroughly evaluated and validated using existing, fully funded, large-scale, and constantly growing databases of CT and MRI images lo- cated on-site. Furthermore, we will develop and disseminate standard workﬂows and domain-speciﬁc use cases for complex anatomies to promote reproducibility. Efforts to develop the proposed technology are aligned with the mission of the National Institute of General Medical Sciences (NIGMS), and its third strategic goal: to bridge biology and quantitative science for better global health through supporting the development of and access to computational research tools for biomedical research. Our long-term goal is to increase the clinical utility and widespread adoption of SSM, and the proposed research will establish the groundwork for achieving this goal. Project Narrative This project will develop general-purpose, scalable, and open-source statistical shape modeling (SSM) tools, which will present unique capabilities for automated anatomy modeling with less user input. The proposed tech- nology will introduce a number of signiﬁcant improvements to current SSM approaches and tools, including the support for challenging modeling problems, inferring shapes directly from images (and hence bypassing the seg- mentation step), parallel optimizations for speed, and new user interfaces that will be much easier and scalable than the current tools. The proposed technology will constitute an indispensable resource for the biomedical and clinical communities that will enable new avenues for biomedical research and clinical investigations, provide new ways to answer biologically related questions, allow new types of questions to be asked, and open the door for the integration of SSM with clinical care.","Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches",9803774,R01AR076120,"['Address', 'Adoption', 'Age', 'Algorithms', 'Anatomic Models', 'Anatomic Surface', 'Anatomy', 'Area', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Brain', 'Bypass', 'Cardiology', 'Cessation of life', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collection', 'Communities', 'Complex', 'Complex Analysis', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Custom', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Felis catus', 'Funding', 'Generations', 'Geometry', 'Goals', 'Human', 'Ice', 'Image', 'Imagery', 'Injury', 'Intuition', 'Laboratory Research', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematical Computing', 'Measures', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Modernization', 'Modification', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Occupations', 'Online Systems', 'Organism', 'Orthopedics', 'Pathologic', 'Population', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Shapes', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Speed', 'Statistical Data Interpretation', 'Structure', 'Supervision', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Variant', 'Visual', 'Visualization software', 'Work', 'base', 'biomedical resource', 'clinical care', 'clinical investigation', 'clinically relevant', 'cohort', 'computerized tools', 'computing resources', 'deep learning', 'experience', 'flexibility', 'global health', 'graphical user interface', 'image archival system', 'image processing', 'imaging Segmentation', 'in vivo imaging', 'innovation', 'machine learning algorithm', 'model development', 'multidisciplinary', 'open source', 'particle', 'preference', 'software development', 'tool', 'usability', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2019,631809,-0.032730203485249805
"Fusion of Electromagnetic Brain Imaging and fMRI    DESCRIPTION (provided by applicant): Multimodal non-invasive functional brain imaging has made a tremendous impact in improving our understanding of the neural correlates of human behavior, and is now an indispensable tool for systems and cognitive neuroscientists. We propose to develop state-of-the-art multimodal functional imaging fusion algorithms for accurate visualization of the brain's dynamic activity and high spatial and temporal resolution. We propose to develop algorithms that combine complementary high spatial resolution of functional MRI (fMRI) and high-temporal resolution of magnetoencephalography (MEG) and electroencephalography (EEG) data for high-fidelity reconstruction of brain activity. In recent years, our research group has developed a suite of novel and powerful algorithms for MEG/EEG imaging superior to existing benchmark algorithms, and we have compared these results with electrocorticography (ECOG). Specifically, our algorithms can solve for many brain sources, including sources located far from the sensors, in the presence of large interference from unrelated brain sources using fast and robust probabilistic inference techniques. Here, we propose to extend this success in M/EEG inverse algorithms into the domain of multimodal imaging data fusion. Our overall goal here is to ultimately produce robust, high fidelity videos of event-related brain activation at a sub-millimeter and sub-millisecond resolution from noisy MEG/EEG and fMRI data using state-of-the-art machine learning algorithms. Specifically, we propose to extend a powerful new algorithm that we have recently developed, called Champagne, into two new fusion algorithms that combine fMRI, MEG and EEG data in different ways. Performance of both algorithms will first be rigorously evaluated in simulations, including performance comparisons with existing benchmark fusion algorithms. Algorithms will then tested for consistency on four fMRI-MEG+EEG datasets from healthy controls obtained for identical paradigms (auditory, motor, picture naming and verb-generation) and two fMRI-EEG datasets (face and motion perception). Additional validation studies will also be performed on fMRI-MEG/EEG datasets obtained from epilepsy patients and compared to electrocorticography (ECoG). Following successful testing and evaluation, all algorithms developed in this grant proposal, as well as example validation datasets, will be distributed using NUTMEG (nutmeg.berkeley.edu), an open-source software toolbox that we have developed.        Multimodal non-invasive functional brain imaging has made a tremendous impact in improving our understanding of the neural correlates of human behavior, and is now an indispensable tool for systems and cognitive neuroscientists. With the development of appropriate analytical tools, multimodal functional brain imaging is in the process of revolutionizing the diagnosis and treatment of a variety of neurological and psychiatric disorders such as autism, schizophrenia, dementia, and epilepsy that affect tens of millions of Americans.         ",Fusion of Electromagnetic Brain Imaging and fMRI,8320120,R21NS076171,"['Affect', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Autistic Disorder', 'Behavior', 'Benchmarking', 'Brain', 'Brain imaging', 'Cognitive', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Development', 'Diagnosis', 'Electrocorticogram', 'Electroencephalography', 'Electromagnetics', 'Epilepsy', 'Event', 'Functional Imaging', 'Functional Magnetic Resonance Imaging', 'Generations', 'Goals', 'Human', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Measurement', 'Measures', 'Mental disorders', 'Methods', 'Modality', 'Modeling', 'Motion Perception', 'Motor', 'Multimodal Imaging', 'Names', 'Nutmeg - dietary', 'Patients', 'Pattern', 'Performance', 'Process', 'Research', 'Resolution', 'Scalp structure', 'Schizophrenia', 'Signal Transduction', 'Source', 'Specific qualifier value', 'Surface', 'Techniques', 'Testing', 'Time', 'Validation', 'Variant', 'analytical tool', 'base', 'blood oxygen level dependent', 'cognitive system', 'evaluation/testing', 'face perception', 'foot', 'hemodynamics', 'imaging modality', 'improved', 'magnetic field', 'millimeter', 'millisecond', 'nervous system disorder', 'novel', 'open source', 'reconstruction', 'relating to nervous system', 'sensor', 'simulation', 'spatiotemporal', 'success', 'tool', 'validation studies']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R21,2012,193125,-0.04931682182723001
"Fusion of Electromagnetic Brain Imaging and fMRI    DESCRIPTION (provided by applicant): Multimodal non-invasive functional brain imaging has made a tremendous impact in improving our understanding of the neural correlates of human behavior, and is now an indispensable tool for systems and cognitive neuroscientists. We propose to develop state-of-the-art multimodal functional imaging fusion algorithms for accurate visualization of the brain's dynamic activity and high spatial and temporal resolution. We propose to develop algorithms that combine complementary high spatial resolution of functional MRI (fMRI) and high-temporal resolution of magnetoencephalography (MEG) and electroencephalography (EEG) data for high-fidelity reconstruction of brain activity. In recent years, our research group has developed a suite of novel and powerful algorithms for MEG/EEG imaging superior to existing benchmark algorithms, and we have compared these results with electrocorticography (ECOG). Specifically, our algorithms can solve for many brain sources, including sources located far from the sensors, in the presence of large interference from unrelated brain sources using fast and robust probabilistic inference techniques. Here, we propose to extend this success in M/EEG inverse algorithms into the domain of multimodal imaging data fusion. Our overall goal here is to ultimately produce robust, high fidelity videos of event-related brain activation at a sub-millimeter and sub-millisecond resolution from noisy MEG/EEG and fMRI data using state-of-the-art machine learning algorithms. Specifically, we propose to extend a powerful new algorithm that we have recently developed, called Champagne, into two new fusion algorithms that combine fMRI, MEG and EEG data in different ways. Performance of both algorithms will first be rigorously evaluated in simulations, including performance comparisons with existing benchmark fusion algorithms. Algorithms will then tested for consistency on four fMRI-MEG+EEG datasets from healthy controls obtained for identical paradigms (auditory, motor, picture naming and verb-generation) and two fMRI-EEG datasets (face and motion perception). Additional validation studies will also be performed on fMRI-MEG/EEG datasets obtained from epilepsy patients and compared to electrocorticography (ECoG). Following successful testing and evaluation, all algorithms developed in this grant proposal, as well as example validation datasets, will be distributed using NUTMEG (nutmeg.berkeley.edu), an open-source software toolbox that we have developed.      PUBLIC HEALTH RELEVANCE: Multimodal non-invasive functional brain imaging has made a tremendous impact in improving our understanding of the neural correlates of human behavior, and is now an indispensable tool for systems and cognitive neuroscientists. With the development of appropriate analytical tools, multimodal functional brain imaging is in the process of revolutionizing the diagnosis and treatment of a variety of neurological and psychiatric disorders such as autism, schizophrenia, dementia, and epilepsy that affect tens of millions of Americans.           Multimodal non-invasive functional brain imaging has made a tremendous impact in improving our understanding of the neural correlates of human behavior, and is now an indispensable tool for systems and cognitive neuroscientists. With the development of appropriate analytical tools, multimodal functional brain imaging is in the process of revolutionizing the diagnosis and treatment of a variety of neurological and psychiatric disorders such as autism, schizophrenia, dementia, and epilepsy that affect tens of millions of Americans.         ",Fusion of Electromagnetic Brain Imaging and fMRI,8247368,R21NS076171,"['Affect', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Autistic Disorder', 'Behavior', 'Benchmarking', 'Brain', 'Brain imaging', 'Cognitive', 'Computer software', 'Data', 'Data Set', 'Dementia', 'Development', 'Diagnosis', 'Electrocorticogram', 'Electroencephalography', 'Electromagnetics', 'Epilepsy', 'Event', 'Functional Imaging', 'Functional Magnetic Resonance Imaging', 'Generations', 'Goals', 'Human', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Measurement', 'Measures', 'Mental disorders', 'Methods', 'Modality', 'Modeling', 'Motion Perception', 'Motor', 'Multimodal Imaging', 'Names', 'Nutmeg - dietary', 'Patients', 'Pattern', 'Performance', 'Process', 'Research', 'Resolution', 'Scalp structure', 'Schizophrenia', 'Signal Transduction', 'Source', 'Specific qualifier value', 'Surface', 'Techniques', 'Testing', 'Time', 'Validation', 'Variant', 'analytical tool', 'base', 'blood oxygen level dependent', 'cognitive system', 'evaluation/testing', 'face perception', 'foot', 'hemodynamics', 'imaging modality', 'improved', 'magnetic field', 'millimeter', 'millisecond', 'nervous system disorder', 'novel', 'open source', 'reconstruction', 'relating to nervous system', 'sensor', 'simulation', 'spatiotemporal', 'success', 'tool', 'validation studies']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R21,2011,231750,-0.04429837275723424
"Noninvasive imaging-based electrophysiology using microelectronic devices DESCRIPTION (provided by applicant):  The goal of this project is to establish a strategy that will make neuronal electrical signaling detectable via magnetic resonance imaging (MRI) at a whole-brain level. Our approach is built on the novel concept of using cell-adhesive micron-scale electronic devices to transduce neuronal potentials across the brain into magnetic field fluctuations. As part of our validation of these voltage-sensing microprobes, we also propose to implement a new, scalable method for simultaneous recording of MRI and electrophysiological data. The methods we propose to develop will be broadly applicable to problems in neurobiology, and will transform neuroscientists' ability to study integrative functions of the brain. Our microprobe approach will also help establish a new paradigm in diagnostic medicine and molecular imaging, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology. Recent work has dem- onstrated that micron-scale electrodes, coated with cell-adhesive molecules and juxtaposed against cultured cells allow recording of millivolt-scale action potentials, comparable to intracellular recordings. The current induced in a microelectrode can be converted into a modest, transient magnetic field if it is channeled into an inductor. In Specific Aim 1, we will model the magnetic fields produced by feasible currents in spiral or solenoidal microcoils of defined geometry, compute predicted effects on MRI signal amplitude and phase as a function of microprobe distribution, and fabricate the microprobes themselves. Preliminary calculations indicate that localized, transient fields of about 10 nT could be produced in individual 10-turn microcoils of 1 5m diameter. Magnetic fields of this order are greater than endogenous neuronal fields detected in tech- nologies like magnetoencephalography, and have been shown previously to be measurable by MRI in some contexts. In Specific Aim 2, we will test the ability of our microprobes to report action potentials from neu- ronal populations in MRI. The microdevices wil first be applied to cultured neurons or neural tissue slices and placed in an MRI scanner. Data series will be obtained using multiple protocols to detect variations of MRI signal due to variations in neuronal activity. If experiments in culture are successful, microprobes will be site-specifically injected into the cerebral cortex of anesthetized rats, and tested in an somatosensory stimu- lation paradigm. In Specific Aim 3, we will establish a simultaneous MRI and conventional electrophysiology approach to validate the novel MRI voltage probes directly. Performing electrophysiology in an MRI scanner is complicated by artifacts induced by the scanning hardware, in particular due to switched gradient fields. To circumvent this problem, we will measure neuronal potentials using differential recording from pairs of channels on tetrodes or modified tetrodes. Once the in-scanner recording method has been refined, MRI- based and conventional electrophysiology data will be obtained and compared to assess performance of the voltage-sensing microprobes, and to guide further improvements, if necessary. Noninvasive MRI-based electrophysiology using microelectronic devices will have high impact in biology, and specifically in brain research, both through applications to the study of neurological disease and as tools for the analysis of neural network function in basic neuroscience. The microprobes we propose to develop represent a new paradigm in diagnostic medicine, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology.",Noninvasive imaging-based electrophysiology using microelectronic devices,8850914,R01NS076462,"['Action Potentials', 'Adhesives', 'Animals', 'Area', 'Artificial Intelligence', 'Bathing', 'Biological Neural Networks', 'Biology', 'Brain', 'Brain Diseases', 'Caliber', 'Cell Culture Techniques', 'Cell physiology', 'Cells', 'Cerebral cortex', 'Chemicals', 'Child Development', 'Contralateral', 'Contrast Media', 'Cultured Cells', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Education', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Electroplating', 'Engineering', 'Feedback', 'Geometry', 'Goals', 'Gold', 'Human', 'Imaging Techniques', 'Incubators', 'Individual', 'Informatics', 'Injury', 'Knowledge', 'Life', 'Magnetic Resonance Imaging', 'Magnetism', 'Magnetoencephalography', 'Measurable', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Microelectrodes', 'Modeling', 'Morphologic artifacts', 'Neurobiology', 'Neurons', 'Neurosciences', 'Organism', 'Performance', 'Personal Satisfaction', 'Phase', 'Physiology', 'Population', 'Positioning Attribute', 'Protocols documentation', 'Rattus', 'Relative (related person)', 'Reporting', 'Sampling', 'Scanning', 'Series', 'Signal Transduction', 'Site', 'Slice', 'Techniques', 'Technology', 'Testing', 'Tissues', 'Validation', 'Variant', 'Work', 'base', 'brain research', 'density', 'design', 'electrical potential', 'improved', 'information processing', 'magnetic field', 'molecular imaging', 'nervous system disorder', 'neural stimulation', 'non-invasive imaging', 'novel', 'optogenetics', 'programs', 'relating to nervous system', 'remediation', 'research study', 'sensor', 'somatosensory', 'tool', 'two-dimensional', 'voltage']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2015,392217,-0.020131167342975678
"Noninvasive imaging-based electrophysiology using microelectronic devices    DESCRIPTION (provided by applicant):  The goal of this project is to establish a strategy that will make neuronal electrical signaling detectable via magnetic resonance imaging (MRI) at a whole-brain level. Our approach is built on the novel concept of using cell-adhesive micron-scale electronic devices to transduce neuronal potentials across the brain into magnetic field fluctuations. As part of our validation of these voltage-sensing microprobes, we also propose to implement a new, scalable method for simultaneous recording of MRI and electrophysiological data. The methods we propose to develop will be broadly applicable to problems in neurobiology, and will transform neuroscientists' ability to study integrative functions of the brain. Our microprobe approach will also help establish a new paradigm in diagnostic medicine and molecular imaging, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology. Recent work has dem- onstrated that micron-scale electrodes, coated with cell-adhesive molecules and juxtaposed against cultured cells allow recording of millivolt-scale action potentials, comparable to intracellular recordings. The current induced in a microelectrode can be converted into a modest, transient magnetic field if it is channeled into an inductor. In Specific Aim 1, we will model the magnetic fields produced by feasible currents in spiral or solenoidal microcoils of defined geometry, compute predicted effects on MRI signal amplitude and phase as a function of microprobe distribution, and fabricate the microprobes themselves. Preliminary calculations indicate that localized, transient fields of about 10 nT could be produced in individual 10-turn microcoils of 1 5m diameter. Magnetic fields of this order are greater than endogenous neuronal fields detected in tech- nologies like magnetoencephalography, and have been shown previously to be measurable by MRI in some contexts. In Specific Aim 2, we will test the ability of our microprobes to report action potentials from neu- ronal populations in MRI. The microdevices wil first be applied to cultured neurons or neural tissue slices and placed in an MRI scanner. Data series will be obtained using multiple protocols to detect variations of MRI signal due to variations in neuronal activity. If experiments in culture are successful, microprobes will be site-specifically injected into the cerebral cortex of anesthetized rats, and tested in an somatosensory stimu- lation paradigm. In Specific Aim 3, we will establish a simultaneous MRI and conventional electrophysiology approach to validate the novel MRI voltage probes directly. Performing electrophysiology in an MRI scanner is complicated by artifacts induced by the scanning hardware, in particular due to switched gradient fields. To circumvent this problem, we will measure neuronal potentials using differential recording from pairs of channels on tetrodes or modified tetrodes. Once the in-scanner recording method has been refined, MRI- based and conventional electrophysiology data will be obtained and compared to assess performance of the voltage-sensing microprobes, and to guide further improvements, if necessary.         Noninvasive MRI-based electrophysiology using microelectronic devices will have high impact in biology, and specifically in brain research, both through applications to the study of neurological disease and as tools for the analysis of neural network function in basic neuroscience. The microprobes we propose to develop represent a new paradigm in diagnostic medicine, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology.         ",Noninvasive imaging-based electrophysiology using microelectronic devices,8658488,R01NS076462,"['Action Potentials', 'Adhesives', 'Animals', 'Area', 'Artificial Intelligence', 'Bathing', 'Biological Neural Networks', 'Biology', 'Brain', 'Brain Diseases', 'Caliber', 'Cell Culture Techniques', 'Cell physiology', 'Cells', 'Cerebral cortex', 'Chemicals', 'Child Development', 'Contralateral', 'Contrast Media', 'Cultured Cells', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Education', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Electroplating', 'Engineering', 'Feedback', 'Geometry', 'Goals', 'Gold', 'Human', 'Image', 'Imaging Techniques', 'Incubators', 'Individual', 'Informatics', 'Injury', 'Knowledge', 'Life', 'Magnetic Resonance Imaging', 'Magnetism', 'Magnetoencephalography', 'Measurable', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Microelectrodes', 'Modeling', 'Morphologic artifacts', 'Neurobiology', 'Neurons', 'Neurosciences', 'Organism', 'Performance', 'Personal Satisfaction', 'Phase', 'Physiology', 'Population', 'Positioning Attribute', 'Protocols documentation', 'Rattus', 'Relative (related person)', 'Reporting', 'Sampling', 'Scanning', 'Series', 'Signal Transduction', 'Site', 'Slice', 'Techniques', 'Technology', 'Testing', 'Tissues', 'Validation', 'Variant', 'Work', 'base', 'brain research', 'density', 'design', 'electrical potential', 'improved', 'information processing', 'magnetic field', 'molecular imaging', 'nervous system disorder', 'neural stimulation', 'novel', 'optogenetics', 'programs', 'relating to nervous system', 'remediation', 'research study', 'sensor', 'somatosensory', 'tool', 'two-dimensional', 'voltage']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2014,392158,-0.020131167342975678
"Noninvasive imaging-based electrophysiology using microelectronic devices    DESCRIPTION (provided by applicant):  The goal of this project is to establish a strategy that will make neuronal electrical signaling detectable via magnetic resonance imaging (MRI) at a whole-brain level. Our approach is built on the novel concept of using cell-adhesive micron-scale electronic devices to transduce neuronal potentials across the brain into magnetic field fluctuations. As part of our validation of these voltage-sensing microprobes, we also propose to implement a new, scalable method for simultaneous recording of MRI and electrophysiological data. The methods we propose to develop will be broadly applicable to problems in neurobiology, and will transform neuroscientists' ability to study integrative functions of the brain. Our microprobe approach will also help establish a new paradigm in diagnostic medicine and molecular imaging, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology. Recent work has dem- onstrated that micron-scale electrodes, coated with cell-adhesive molecules and juxtaposed against cultured cells allow recording of millivolt-scale action potentials, comparable to intracellular recordings. The current induced in a microelectrode can be converted into a modest, transient magnetic field if it is channeled into an inductor. In Specific Aim 1, we will model the magnetic fields produced by feasible currents in spiral or solenoidal microcoils of defined geometry, compute predicted effects on MRI signal amplitude and phase as a function of microprobe distribution, and fabricate the microprobes themselves. Preliminary calculations indicate that localized, transient fields of about 10 nT could be produced in individual 10-turn microcoils of 1 5m diameter. Magnetic fields of this order are greater than endogenous neuronal fields detected in tech- nologies like magnetoencephalography, and have been shown previously to be measurable by MRI in some contexts. In Specific Aim 2, we will test the ability of our microprobes to report action potentials from neu- ronal populations in MRI. The microdevices wil first be applied to cultured neurons or neural tissue slices and placed in an MRI scanner. Data series will be obtained using multiple protocols to detect variations of MRI signal due to variations in neuronal activity. If experiments in culture are successful, microprobes will be site-specifically injected into the cerebral cortex of anesthetized rats, and tested in an somatosensory stimu- lation paradigm. In Specific Aim 3, we will establish a simultaneous MRI and conventional electrophysiology approach to validate the novel MRI voltage probes directly. Performing electrophysiology in an MRI scanner is complicated by artifacts induced by the scanning hardware, in particular due to switched gradient fields. To circumvent this problem, we will measure neuronal potentials using differential recording from pairs of channels on tetrodes or modified tetrodes. Once the in-scanner recording method has been refined, MRI- based and conventional electrophysiology data will be obtained and compared to assess performance of the voltage-sensing microprobes, and to guide further improvements, if necessary.         Noninvasive MRI-based electrophysiology using microelectronic devices will have high impact in biology, and specifically in brain research, both through applications to the study of neurological disease and as tools for the analysis of neural network function in basic neuroscience. The microprobes we propose to develop represent a new paradigm in diagnostic medicine, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology.         ",Noninvasive imaging-based electrophysiology using microelectronic devices,8463264,R01NS076462,"['Action Potentials', 'Adhesives', 'Animals', 'Area', 'Artificial Intelligence', 'Bathing', 'Biological Neural Networks', 'Biology', 'Brain', 'Brain Diseases', 'Caliber', 'Cell Culture Techniques', 'Cell physiology', 'Cells', 'Cerebral cortex', 'Chemicals', 'Child Development', 'Contralateral', 'Contrast Media', 'Cultured Cells', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Education', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Electroplating', 'Engineering', 'Feedback', 'Goals', 'Gold', 'Human', 'Image', 'Imaging Techniques', 'Incubators', 'Individual', 'Informatics', 'Injury', 'Knowledge', 'Life', 'Magnetic Resonance Imaging', 'Magnetism', 'Magnetoencephalography', 'Measurable', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Microelectrodes', 'Modeling', 'Morphologic artifacts', 'Neurobiology', 'Neurons', 'Neurosciences', 'Organism', 'Performance', 'Personal Satisfaction', 'Phase', 'Physiology', 'Population', 'Positioning Attribute', 'Protocols documentation', 'Rattus', 'Relative (related person)', 'Reporting', 'Sampling', 'Scanning', 'Series', 'Signal Transduction', 'Site', 'Slice', 'Techniques', 'Technology', 'Testing', 'Tissues', 'Validation', 'Variant', 'Work', 'base', 'brain research', 'density', 'design', 'electrical potential', 'improved', 'information processing', 'magnetic field', 'molecular imaging', 'nervous system disorder', 'neural stimulation', 'novel', 'optogenetics', 'programs', 'relating to nervous system', 'remediation', 'research study', 'sensor', 'somatosensory', 'tool', 'two-dimensional', 'voltage']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2013,380336,-0.020131167342975678
"Noninvasive imaging-based electrophysiology using microelectronic devices    DESCRIPTION (provided by applicant):  The goal of this project is to establish a strategy that will make neuronal electrical signaling detectable via magnetic resonance imaging (MRI) at a whole-brain level. Our approach is built on the novel concept of using cell-adhesive micron-scale electronic devices to transduce neuronal potentials across the brain into magnetic field fluctuations. As part of our validation of these voltage-sensing microprobes, we also propose to implement a new, scalable method for simultaneous recording of MRI and electrophysiological data. The methods we propose to develop will be broadly applicable to problems in neurobiology, and will transform neuroscientists' ability to study integrative functions of the brain. Our microprobe approach will also help establish a new paradigm in diagnostic medicine and molecular imaging, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology. Recent work has dem- onstrated that micron-scale electrodes, coated with cell-adhesive molecules and juxtaposed against cultured cells allow recording of millivolt-scale action potentials, comparable to intracellular recordings. The current induced in a microelectrode can be converted into a modest, transient magnetic field if it is channeled into an inductor. In Specific Aim 1, we will model the magnetic fields produced by feasible currents in spiral or solenoidal microcoils of defined geometry, compute predicted effects on MRI signal amplitude and phase as a function of microprobe distribution, and fabricate the microprobes themselves. Preliminary calculations indicate that localized, transient fields of about 10 nT could be produced in individual 10-turn microcoils of 1 5m diameter. Magnetic fields of this order are greater than endogenous neuronal fields detected in tech- nologies like magnetoencephalography, and have been shown previously to be measurable by MRI in some contexts. In Specific Aim 2, we will test the ability of our microprobes to report action potentials from neu- ronal populations in MRI. The microdevices wil first be applied to cultured neurons or neural tissue slices and placed in an MRI scanner. Data series will be obtained using multiple protocols to detect variations of MRI signal due to variations in neuronal activity. If experiments in culture are successful, microprobes will be site-specifically injected into the cerebral cortex of anesthetized rats, and tested in an somatosensory stimu- lation paradigm. In Specific Aim 3, we will establish a simultaneous MRI and conventional electrophysiology approach to validate the novel MRI voltage probes directly. Performing electrophysiology in an MRI scanner is complicated by artifacts induced by the scanning hardware, in particular due to switched gradient fields. To circumvent this problem, we will measure neuronal potentials using differential recording from pairs of channels on tetrodes or modified tetrodes. Once the in-scanner recording method has been refined, MRI- based and conventional electrophysiology data will be obtained and compared to assess performance of the voltage-sensing microprobes, and to guide further improvements, if necessary.         Noninvasive MRI-based electrophysiology using microelectronic devices will have high impact in biology, and specifically in brain research, both through applications to the study of neurological disease and as tools for the analysis of neural network function in basic neuroscience. The microprobes we propose to develop represent a new paradigm in diagnostic medicine, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology.         ",Noninvasive imaging-based electrophysiology using microelectronic devices,8326617,R01NS076462,"['Action Potentials', 'Adhesives', 'Animals', 'Area', 'Artificial Intelligence', 'Bathing', 'Biological Neural Networks', 'Biology', 'Brain', 'Brain Diseases', 'Caliber', 'Cell Culture Techniques', 'Cell physiology', 'Cells', 'Cerebral cortex', 'Chemicals', 'Child Development', 'Contralateral', 'Contrast Media', 'Cultured Cells', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Education', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Electroplating', 'Engineering', 'Feedback', 'Goals', 'Gold', 'Human', 'Image', 'Imaging Techniques', 'Incubators', 'Individual', 'Informatics', 'Injury', 'Knowledge', 'Life', 'Magnetic Resonance Imaging', 'Magnetism', 'Magnetoencephalography', 'Measurable', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Microelectrodes', 'Modeling', 'Morphologic artifacts', 'Neurobiology', 'Neurons', 'Neurosciences', 'Organism', 'Performance', 'Personal Satisfaction', 'Phase', 'Physiology', 'Population', 'Positioning Attribute', 'Protocols documentation', 'Rattus', 'Relative (related person)', 'Reporting', 'Sampling', 'Scanning', 'Series', 'Signal Transduction', 'Site', 'Slice', 'Techniques', 'Technology', 'Testing', 'Tissues', 'Validation', 'Variant', 'Work', 'base', 'brain research', 'density', 'design', 'electrical potential', 'improved', 'information processing', 'magnetic field', 'molecular imaging', 'nervous system disorder', 'neural stimulation', 'novel', 'optogenetics', 'programs', 'relating to nervous system', 'remediation', 'research study', 'sensor', 'somatosensory', 'tool', 'two-dimensional', 'voltage']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2012,392042,-0.020131167342975678
"Noninvasive imaging-based electrophysiology using microelectronic devices    DESCRIPTION (provided by applicant):  The goal of this project is to establish a strategy that will make neuronal electrical signaling detectable via magnetic resonance imaging (MRI) at a whole-brain level. Our approach is built on the novel concept of using cell-adhesive micron-scale electronic devices to transduce neuronal potentials across the brain into magnetic field fluctuations. As part of our validation of these voltage-sensing microprobes, we also propose to implement a new, scalable method for simultaneous recording of MRI and electrophysiological data. The methods we propose to develop will be broadly applicable to problems in neurobiology, and will transform neuroscientists' ability to study integrative functions of the brain. Our microprobe approach will also help establish a new paradigm in diagnostic medicine and molecular imaging, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology. Recent work has dem- onstrated that micron-scale electrodes, coated with cell-adhesive molecules and juxtaposed against cultured cells allow recording of millivolt-scale action potentials, comparable to intracellular recordings. The current induced in a microelectrode can be converted into a modest, transient magnetic field if it is channeled into an inductor. In Specific Aim 1, we will model the magnetic fields produced by feasible currents in spiral or solenoidal microcoils of defined geometry, compute predicted effects on MRI signal amplitude and phase as a function of microprobe distribution, and fabricate the microprobes themselves. Preliminary calculations indicate that localized, transient fields of about 10 nT could be produced in individual 10-turn microcoils of 1 5m diameter. Magnetic fields of this order are greater than endogenous neuronal fields detected in tech- nologies like magnetoencephalography, and have been shown previously to be measurable by MRI in some contexts. In Specific Aim 2, we will test the ability of our microprobes to report action potentials from neu- ronal populations in MRI. The microdevices wil first be applied to cultured neurons or neural tissue slices and placed in an MRI scanner. Data series will be obtained using multiple protocols to detect variations of MRI signal due to variations in neuronal activity. If experiments in culture are successful, microprobes will be site-specifically injected into the cerebral cortex of anesthetized rats, and tested in an somatosensory stimu- lation paradigm. In Specific Aim 3, we will establish a simultaneous MRI and conventional electrophysiology approach to validate the novel MRI voltage probes directly. Performing electrophysiology in an MRI scanner is complicated by artifacts induced by the scanning hardware, in particular due to switched gradient fields. To circumvent this problem, we will measure neuronal potentials using differential recording from pairs of channels on tetrodes or modified tetrodes. Once the in-scanner recording method has been refined, MRI- based and conventional electrophysiology data will be obtained and compared to assess performance of the voltage-sensing microprobes, and to guide further improvements, if necessary.      PUBLIC HEALTH RELEVANCE:  Noninvasive MRI-based electrophysiology using microelectronic devices will have high impact in biology, and specifically in brain research, both through applications to the study of neurological disease and as tools for the analysis of neural network function in basic neuroscience. The microprobes we propose to develop represent a new paradigm in diagnostic medicine, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology.            Noninvasive MRI-based electrophysiology using microelectronic devices will have high impact in biology, and specifically in brain research, both through applications to the study of neurological disease and as tools for the analysis of neural network function in basic neuroscience. The microprobes we propose to develop represent a new paradigm in diagnostic medicine, where tiny machines, rather than conventional chemical contrast agents, will report on aspects of cellular physiology.         ",Noninvasive imaging-based electrophysiology using microelectronic devices,8180844,R01NS076462,"['Action Potentials', 'Adhesives', 'Animals', 'Area', 'Artificial Intelligence', 'Bathing', 'Biological Neural Networks', 'Biology', 'Brain', 'Brain Diseases', 'Caliber', 'Cell Culture Techniques', 'Cell physiology', 'Cells', 'Cerebral cortex', 'Chemicals', 'Child Development', 'Contralateral', 'Contrast Media', 'Cultured Cells', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Education', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Electroplating', 'Engineering', 'Feedback', 'Goals', 'Gold', 'Human', 'Image', 'Imaging Techniques', 'Incubators', 'Individual', 'Informatics', 'Injury', 'Knowledge', 'Life', 'Magnetic Resonance Imaging', 'Magnetism', 'Magnetoencephalography', 'Measurable', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Microelectrodes', 'Modeling', 'Morphologic artifacts', 'Neurobiology', 'Neurons', 'Neurosciences', 'Organism', 'Performance', 'Personal Satisfaction', 'Phase', 'Physiology', 'Population', 'Positioning Attribute', 'Protocols documentation', 'Rattus', 'Relative (related person)', 'Reporting', 'Sampling', 'Scanning', 'Series', 'Signal Transduction', 'Site', 'Slice', 'Techniques', 'Technology', 'Testing', 'Tissues', 'Validation', 'Variant', 'Work', 'base', 'brain research', 'density', 'design', 'electrical potential', 'improved', 'information processing', 'magnetic field', 'molecular imaging', 'nervous system disorder', 'neural stimulation', 'novel', 'programs', 'relating to nervous system', 'remediation', 'research study', 'sensor', 'somatosensory', 'tool', 'two-dimensional', 'voltage']",NINDS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2011,360321,-0.02007419531447559
"Improved Methods for Single Subject FMRI Analysis    DESCRIPTION (provided by applicant): Mental illness is a great burden for the affected individual and economically costly for society. The annual cost of mental disorders has been estimated to be $150 billion, increasing every year, and this total does not include more than three million people receiving disability benefits due to mental disorders. It is imperative that we prioritize research efforts focused on understanding brain function in order to improve diagnostic strategies and discover more effective therapies. Functional Magnetic Resonance Imaging (fMRI) is a powerful tool to visualize and measure typical and atypical cognitive processing. However, many important cognitive processing systems, such as those associated with memory, language, emotion and executive control, only produce small BOLD signals and thus measurements are noisy and have low statistical confidence. Hence, fMRI has not been readily adopted for clinical diagnosis of individual patients. I propose to develop greatly improved methods to suppress the noise sources in fMRI data in order to transform fMRI from a research tool about populations to a consistent and accurate diagnostic tool to study individual cognitive functions. Using the strategy that every noise suppression algorithm must perform well to reliably detect single trial fMRI BOLD signals, I developed visualization methods to ""see"" deeply into fMRI data to evaluate the quality of the data at every step of fMRI data processing. The preliminary studies indicate that there are clear opportunities to improve fMRI image analysis techniques. The proposed research will first develop and test methods to improve suppression of errors from motion and physiological fluctuations. Then it will translate this research by combining these techniques with pattern recognition to characterize individual cognitive activation patterns in typical and atypical populations. My quantitative science expertise is in image processing, algorithm design, and pattern recognition. The research directly supports my interdisciplinary career development with hands-on experience in experiment planning, fMRI scanner operation, neuroscience coursework, and new software methods for application to severely brain disordered populations. In particular, the subjects for this research will include important clinical psychiatric populations with disorders such as fragile X syndrome, Turner syndrome, autism, Williams syndrome, depression, and bipolar disorder, so that all newly developed methods can be immediately put into practice.           n/a",Improved Methods for Single Subject FMRI Analysis,7075928,K25MH077309,"['artificial intelligence', 'bioimaging /biomedical imaging', 'body movement', 'brain disorder diagnosis', 'clinical research', 'cognition', 'cognition disorders', 'computer assisted diagnosis', 'computer program /software', 'developmental disease /disorder', 'dyslexia', 'functional magnetic resonance imaging', 'human data', 'human subject', 'image enhancement', 'image processing', 'learning disorders', 'mental disorder diagnosis', 'mental retardation', 'patient oriented research']",NIMH,STANFORD UNIVERSITY,K25,2006,162340,-0.0318515084538241
"The Effect of Wrist Motion on Pattern-Recognition-Based Myoelectric Control     DESCRIPTION (provided by applicant): Myoelectric partial-hand prostheses offer the potential for a wide range of functional hand grasps not previously available to partial-hand amputees. Despite these advances, there is still an important need to develop more effective methods of controlling them. Many interesting studies investigating the use of pattern- recognition-based myoelectric control (PRMC) have shown that electromyographic signals (EMG) from the extrinsic muscles of the hand provide highly-accurate control of prostheses in individuals with high-level amputations. However, these studies fail to address an essential challenge that is unique to partial-hand amputees: the presence of a functional wrist. Not only can the wrist be in any position when the user initiates movement of their prosthesis, it can also be in active motion during the movements. Moreover, the muscle contractions responsible for these motions influence properties of the EMG from the extrinsic muscles, and consequently the performance of a PRMC system. In this study, we will use principal component analysis to describe how wrist kinematics affect the properties of EMG from the extrinsic muscles. We will incorporate recorded wrist kinematic information into both linear and non-linear pattern-recognition systems. Finally, we will test the real-time ability of these PRMC systems to accommodate the effect of wrist kinematics, preserve wrist function and improve partial-hand prosthetic control.         PUBLIC HEALTH RELEVANCE: Many partial-hand amputees, who have lost a part of their hand, are still able to move their wrist and this motion can interfere with the electric signals produced by the muscles that control the hand. The goal of this study is to determine how wrist motion affects these electric signals. We propose methods that account for the effect of wrist motion and allow partial-hand amputees to move their wrist while simultaneously successfully controlling their prosthetic hand.                ",The Effect of Wrist Motion on Pattern-Recognition-Based Myoelectric Control,8962088,F31HD078092,"['Accounting', 'Achievement', 'Activities of Daily Living', 'Address', 'Affect', 'Amputation', 'Amputees', 'Data', 'Devices', 'Digit structure', 'Goals', 'Hand', 'Human', 'Individual', 'Joints', 'Methods', 'Motion', 'Movement', 'Muscle', 'Muscle Contraction', 'Outcome', 'Pattern Recognition', 'Pattern Recognition Systems', 'Performance', 'Positioning Attribute', 'Principal Component Analysis', 'Property', 'Prosthesis', 'Research', 'Residual state', 'Signal Transduction', 'System', 'Technology', 'Testing', 'Time', 'Upper Extremity', 'Variant', 'Wrist', 'base', 'clinical application', 'finger movement', 'functional loss', 'hand grasp', 'improved', 'improved functioning', 'interest', 'invention', 'kinematics', 'limb amputation', 'myoelectric control', 'performance tests', 'prosthesis control', 'prosthetic hand', 'public health relevance', 'residual limb', 'vector', 'virtual', 'wrist function', 'wrist motion']",NICHD,NORTHWESTERN UNIVERSITY,F31,2016,31216,-0.008875111147761756
"The Effect of Wrist Motion on Pattern-Recognition-Based Myoelectric Control     DESCRIPTION (provided by applicant): Myoelectric partial-hand prostheses offer the potential for a wide range of functional hand grasps not previously available to partial-hand amputees. Despite these advances, there is still an important need to develop more effective methods of controlling them. Many interesting studies investigating the use of pattern- recognition-based myoelectric control (PRMC) have shown that electromyographic signals (EMG) from the extrinsic muscles of the hand provide highly-accurate control of prostheses in individuals with high-level amputations. However, these studies fail to address an essential challenge that is unique to partial-hand amputees: the presence of a functional wrist. Not only can the wrist be in any position when the user initiates movement of their prosthesis, it can also be in active motion during the movements. Moreover, the muscle contractions responsible for these motions influence properties of the EMG from the extrinsic muscles, and consequently the performance of a PRMC system. In this study, we will use principal component analysis to describe how wrist kinematics affect the properties of EMG from the extrinsic muscles. We will incorporate recorded wrist kinematic information into both linear and non-linear pattern-recognition systems. Finally, we will test the real-time ability of these PRMC systems to accommodate the effect of wrist kinematics, preserve wrist function and improve partial-hand prosthetic control.         PUBLIC HEALTH RELEVANCE: Many partial-hand amputees, who have lost a part of their hand, are still able to move their wrist and this motion can interfere with the electric signals produced by the muscles that control the hand. The goal of this study is to determine how wrist motion affects these electric signals. We propose methods that account for the effect of wrist motion and allow partial-hand amputees to move their wrist while simultaneously successfully controlling their prosthetic hand.                ",The Effect of Wrist Motion on Pattern-Recognition-Based Myoelectric Control,8737400,F31HD078092,"['Accounting', 'Achievement', 'Activities of Daily Living', 'Address', 'Affect', 'Amputation', 'Amputees', 'Data', 'Devices', 'Digit structure', 'Goals', 'Hand', 'Human', 'Individual', 'Joints', 'Methods', 'Motion', 'Movement', 'Muscle', 'Muscle Contraction', 'Outcome', 'Pattern Recognition', 'Pattern Recognition Systems', 'Performance', 'Positioning Attribute', 'Principal Component Analysis', 'Property', 'Prosthesis', 'Research', 'Residual state', 'Signal Transduction', 'System', 'Technology', 'Testing', 'Time', 'Upper Extremity', 'Variant', 'Wrist', 'base', 'clinical application', 'finger movement', 'functional loss', 'hand grasp', 'improved', 'improved functioning', 'interest', 'kinematics', 'limb amputation', 'myoelectric control', 'performance tests', 'prosthesis control', 'prosthetic hand', 'public health relevance', 'residual limb', 'vector', 'virtual', 'wrist function', 'wrist motion']",NICHD,NORTHWESTERN UNIVERSITY,F31,2015,47232,-0.008875111147761756
"AUTO SPECTRAL TOPOGRAPHY FOR CERVICAL TISSUE SCREENING LightForm, Inc., proposes to develop an adjunct to routine Pap smear             evaluation for automatic and simultaneous detection of Human                     Papillomavirus (HPV) and Chlamydia Trachomatis (CT) in cervicovaginal            tissue. The AutoCerSpex system will enable the detection and                     quantification of HPV and CT by multiple fluorophore tagging. The use of         Fluorescence In-Situ Hybridization (FISH) probes, type-specific for HPV,         will emit in the red. CT elementary bodies will be stained by Differential       ImmunoFluorescence (DIF) and will emit in the green. An inexpensive              imaging spectrometer system from the space program, utilizing a                  proprietary Neural Network, will automatically acquire and processes             spatial and spectral data to locate and map areas of infection. During           Phase I, HPV 16/18 and CT will be detected to demonstrate feasibility.           During Phase II, CT and multiple HPV-types, emitting many spectral               features will be added to the protocol.                                                                                                                           Video images of the specimen will be simultaneously acquired for                 morphology and clustering analysis. The Neural Network will correlate and        quantify spectra onto these images to provide a visual mapping of the            incidence of HPV and CT. AutoCerSpex is a logical symbiosis of techniques        known to be effective and will greatly enhance the scope of standard Pap         tests.                                                                                                                                                            PROPOSED COMMERCIAL APPLICATION:                                                 The rapid and accurate identification of the Human Papillomavirus and            Chlamydia in cervicovaginal tissue will result in earlier treatment, a           reduction in loss of life and reduced in-hospital and outpatient                 procedures. Estimated savings could amount to 1 billion dollars per year.        In addition the spectrometer could be used in many industrial applications       including general life science research, semi-conductor QC, plastics,            fiber optics and environmental monitoring.                                        n/a",AUTO SPECTRAL TOPOGRAPHY FOR CERVICAL TISSUE SCREENING,2795907,R43CA078255,"['Chlamydiaceae', ' artificial intelligence', ' biopsy', ' cell line', ' cervical /vaginal smear', ' communicable disease diagnosis', ' diagnosis design /evaluation', ' fluorescence spectrometry', ' fluorescent in situ hybridization', ' histopathology', ' human papillomavirus', ' human tissue', ' immunofluorescence technique', ' stainings']",NCI,"LIGHTFORM, INC.",R43,1999,100000,-0.03035196325279094
"FAST DYNAMIC 3D MRI USING ADAPTIVE SPATIAL ENCODING DESCRIPTION (Adapted from Applicant's Abstract):  The main goal of this          project is the development of a fast dynamic 3D MRI method using adaptive        spatial encoding that can acquire a high resolution MRI dates (256x256x256)      operating in near real-time as possible (1 dataset per 1-2 seconds) with         minimal hardware modifications to a standard MRI scanner.  This goal lies        well outside the possibilities of current MRI methods like echo planar           techniques that employ Fourier encoding and specialized gradient hardware.       A number of applications of interventional MRI, a focus in our hospital,         have the specific requirement for dynamic 3D MRI that can operate on an          ""open"" MR scanner with no specialized gradient coils.  The most important of     these applications is the MRI monitoring of the timecourse of thermal            therapies during which non-uniform heating of tissue occurs due to tissue        heterogeneity and nearby vessels.  Another important application is the near     real-time 3D tracking of probes and catheters used for minimally invasive        therapies.  Specifically, we propose to develop, implement, test and             optimize a dynamic 3D MRI method that encodes adaptively in two directions       using high flip angle 2D spatially selective RF excitations to implement a       minimal set of near-optimal encodes computed from the multidimensional           Singular Value Decomposition (MSVD) of a 3D image estimate (formed from          recently acquired data) computed per acquisition, combined with frequency        encoding in the third direction.  The accomplishment of the main objective       of this project is only possible due to three significant technological          advances.  First, and most important regarding spatial encoding, the             applicants reported recently having developed the MSVD, a powerful numerical     mathematical tool that can determine near-optimal 3D spatial encoding.           Second, a simple fast numerical procedure has been developed in their            laboratory for the computation of RF pulse waveforms for implementing            non-Fourier encodings using high flip angles ( 90=A1) for high SNR scans.        Third, at their facility, they have the operating capability for near            real-time adaptive 2D MRI using a commercial MR system with the minor            modification of an additional attached workstation.                               n/a",FAST DYNAMIC 3D MRI USING ADAPTIVE SPATIAL ENCODING,6174214,R01CA078299,"['bioimaging /biomedical imaging', ' biomedical equipment development', ' clinical biomedical equipment', ' computer assisted patient care', ' human data', ' magnetic resonance imaging', ' neoplasm /cancer thermotherapy', ' patient monitoring device', ' phantom model']",NCI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2000,169005,0.013524802448550874
"FAST DYNAMIC 3D MRI USING ADAPTIVE SPATIAL ENCODING DESCRIPTION (Adapted from Applicant's Abstract):  The main goal of this          project is the development of a fast dynamic 3D MRI method using adaptive        spatial encoding that can acquire a high resolution MRI dates (256x256x256)      operating in near real-time as possible (1 dataset per 1-2 seconds) with         minimal hardware modifications to a standard MRI scanner.  This goal lies        well outside the possibilities of current MRI methods like echo planar           techniques that employ Fourier encoding and specialized gradient hardware.       A number of applications of interventional MRI, a focus in our hospital,         have the specific requirement for dynamic 3D MRI that can operate on an          ""open"" MR scanner with no specialized gradient coils.  The most important of     these applications is the MRI monitoring of the timecourse of thermal            therapies during which non-uniform heating of tissue occurs due to tissue        heterogeneity and nearby vessels.  Another important application is the near     real-time 3D tracking of probes and catheters used for minimally invasive        therapies.  Specifically, we propose to develop, implement, test and             optimize a dynamic 3D MRI method that encodes adaptively in two directions       using high flip angle 2D spatially selective RF excitations to implement a       minimal set of near-optimal encodes computed from the multidimensional           Singular Value Decomposition (MSVD) of a 3D image estimate (formed from          recently acquired data) computed per acquisition, combined with frequency        encoding in the third direction.  The accomplishment of the main objective       of this project is only possible due to three significant technological          advances.  First, and most important regarding spatial encoding, the             applicants reported recently having developed the MSVD, a powerful numerical     mathematical tool that can determine near-optimal 3D spatial encoding.           Second, a simple fast numerical procedure has been developed in their            laboratory for the computation of RF pulse waveforms for implementing            non-Fourier encodings using high flip angles ( 90=A1) for high SNR scans.        Third, at their facility, they have the operating capability for near            real-time adaptive 2D MRI using a commercial MR system with the minor            modification of an additional attached workstation.                               n/a",FAST DYNAMIC 3D MRI USING ADAPTIVE SPATIAL ENCODING,2896543,R01CA078299,"['bioimaging /biomedical imaging', ' biomedical equipment development', ' clinical biomedical equipment', ' computer assisted patient care', ' human data', ' magnetic resonance imaging', ' neoplasm /cancer thermotherapy', ' patient monitoring device', ' phantom model']",NCI,BRIGHAM AND WOMEN'S HOSPITAL,R01,1999,164083,0.013524802448550874
"FAST DYNAMIC 3D MRI USING ADAPTIVE SPATIAL ENCODING DESCRIPTION (Adapted from Applicant's Abstract):  The main goal of this          project is the development of a fast dynamic 3D MRI method using adaptive        spatial encoding that can acquire a high resolution MRI dates (256x256x256)      operating in near real-time as possible (1 dataset per 1-2 seconds) with         minimal hardware modifications to a standard MRI scanner.  This goal lies        well outside the possibilities of current MRI methods like echo planar           techniques that employ Fourier encoding and specialized gradient hardware.       A number of applications of interventional MRI, a focus in our hospital,         have the specific requirement for dynamic 3D MRI that can operate on an          ""open"" MR scanner with no specialized gradient coils.  The most important of     these applications is the MRI monitoring of the timecourse of thermal            therapies during which non-uniform heating of tissue occurs due to tissue        heterogeneity and nearby vessels.  Another important application is the near     real-time 3D tracking of probes and catheters used for minimally invasive        therapies.  Specifically, we propose to develop, implement, test and             optimize a dynamic 3D MRI method that encodes adaptively in two directions       using high flip angle 2D spatially selective RF excitations to implement a       minimal set of near-optimal encodes computed from the multidimensional           Singular Value Decomposition (MSVD) of a 3D image estimate (formed from          recently acquired data) computed per acquisition, combined with frequency        encoding in the third direction.  The accomplishment of the main objective       of this project is only possible due to three significant technological          advances.  First, and most important regarding spatial encoding, the             applicants reported recently having developed the MSVD, a powerful numerical     mathematical tool that can determine near-optimal 3D spatial encoding.           Second, a simple fast numerical procedure has been developed in their            laboratory for the computation of RF pulse waveforms for implementing            non-Fourier encodings using high flip angles ( 90=A1) for high SNR scans.        Third, at their facility, they have the operating capability for near            real-time adaptive 2D MRI using a commercial MR system with the minor            modification of an additional attached workstation.                               n/a",FAST DYNAMIC 3D MRI USING ADAPTIVE SPATIAL ENCODING,2669409,R01CA078299,"['biomedical equipment development', ' clinical biomedical equipment', ' computer assisted patient care', ' human data', ' magnetic resonance imaging', ' neoplasm /cancer thermotherapy', ' patient monitoring device', ' phantom model']",NCI,BRIGHAM AND WOMEN'S HOSPITAL,R01,1998,162621,0.013524802448550874
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8426190,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'stress disorder', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2013,315672,-0.03328729438814681
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8238371,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2012,326844,-0.03328729438814681
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8044673,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,324859,-0.02631610541205384
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,7846105,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Internet', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Source', 'Stress', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'public health relevance', 'research study', 'text searching', 'tool', 'vector']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,326254,-0.02631610541205384
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,7582189,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Body of uterus', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Internet', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Source', 'Stress', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'public health relevance', 'research study', 'text searching', 'tool', 'vector']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,312453,-0.02631610541205384
"SUPPORT AND DEVELOPMENT OF EMAN FOR ELECTRON MICROSCOPY IMAGE PROCESSING EMAN is one of the most well-established and widely used scientific image processing suites targeting the rapidly growing CryoEM/CryoET community worldwide. In turn, the CryoEM and CryoET studies which it enables permit determination of the structures of interacting macromolecules both in-vitro and in-vivo, and are being used to better understand the biochemical processes taking place in cells, to better identify potential drug targets and develop novel diagnostics. With the higher resolutions now possible in this field, direct drug interaction structural studies are now possible, and being used to gain insight into the mode of action of drugs within the cell. Unlike many newer tools in the field, such as Relion, CisTEM and CryoSparc, which focus on specific refinement tasks, EMAN is a versatile, modular suite capable of performing a variety of image processing tasks with hundreds of algorithms supporting virtually all of the standard file formats and mathematical conventions used in the field, as well as other related imaging fields. It provides an ideal platform for prototyping fundamental new algorithm developments, while still able to achieve data-limited resolution in single particle reconstruction. While high resolution single particle refinement has become routine in recent years, thanks largely to the dramatic data quality improvements provided by new detector technology, there remain significant opportunities for improvements in mitigating model bias, efficient use of data, and analysis of complexes with compositional or conformational variability. Some of the most important problems from a biological perspective involve the sort of compositional and conformational variability which remain challenging problems. The field also remains susceptible to problems of initial model bias, which are exacerbated in systems exhibiting structural variability, and as a result many structures are still published with exaggerated resolution claims. The standard protocols used by many in the field typically involve discarding a very large fraction of the raw data (as much as 80-90% in some cases), often based on qualitative assessments, raising questions related to rigor and reproducibility of structural results. In this proposal, we will develop or adapt image processing techniques to help resolve these issues, based on developments or unrealized concepts from mathematics and computer science. CryoEM and CryoET are used to study the structures of interacting biomolecules in the cell at resolutions 100x better than the best possible light microscope. This methodology permits new insights into the biomolecules which underlie disease, can shed light on structural changes in diseased cells and provide direct information on how drugs interact with the molecules they target. This grant develops the software used to turn noisy 2-D electron microscope images into reliable 3-D structures of individual molecules extending to near-atomic resolution.",SUPPORT AND DEVELOPMENT OF EMAN FOR ELECTRON MICROSCOPY IMAGE PROCESSING,10021685,R01GM080139,"['3-Dimensional', 'Algorithms', 'Appearance', 'Biochemical Process', 'Biological', 'Cells', 'Classification', 'Communities', 'Complex', 'Cryoelectron Microscopy', 'Data', 'Data Analyses', 'Data Reporting', 'Development', 'Disease', 'Drug Interactions', 'Drug Targeting', 'Electron Microscope', 'Electron Microscopy', 'Exhibits', 'Grant', 'Image', 'In Vitro', 'Individual', 'Light', 'Light Microscope', 'Maps', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Molecular Conformation', 'Nature', 'Pathway interactions', 'Pharmaceutical Preparations', 'Process', 'Protocols documentation', 'Publishing', 'Reproducibility', 'Resolution', 'Roentgen Rays', 'Rotation', 'Scheme', 'Structure', 'System', 'Techniques', 'Technology', 'Training', 'Work', 'algorithm development', 'artificial neural network', 'autoencoder', 'base', 'case-based', 'computer science', 'data quality', 'deep learning', 'denoising', 'detector', 'drug action', 'file format', 'image processing', 'improved', 'in vivo', 'insight', 'macromolecule', 'mathematical sciences', 'microscopic imaging', 'neural network', 'novel diagnostics', 'particle', 'prototype', 'reconstruction', 'software development', 'symposium', 'three dimensional structure', 'tool', 'virtual']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2020,344862,-0.04518885490368794
"SUPPORT AND DEVELOPMENT OF EMAN FOR ELECTRON MICROSCOPY IMAGE PROCESSING EMAN is one of the most well-established and widely used scientific image processing suites targeting the rapidly growing CryoEM/CryoET community worldwide. In turn, the CryoEM and CryoET studies which it enables permit determination of the structures of interacting macromolecules both in-vitro and in-vivo, and are being used to better understand the biochemical processes taking place in cells, to better identify potential drug targets and develop novel diagnostics. With the higher resolutions now possible in this field, direct drug interaction structural studies are now possible, and being used to gain insight into the mode of action of drugs within the cell. Unlike many newer tools in the field, such as Relion, CisTEM and CryoSparc, which focus on specific refinement tasks, EMAN is a versatile, modular suite capable of performing a variety of image processing tasks with hundreds of algorithms supporting virtually all of the standard file formats and mathematical conventions used in the field, as well as other related imaging fields. It provides an ideal platform for prototyping fundamental new algorithm developments, while still able to achieve data-limited resolution in single particle reconstruction. While high resolution single particle refinement has become routine in recent years, thanks largely to the dramatic data quality improvements provided by new detector technology, there remain significant opportunities for improvements in mitigating model bias, efficient use of data, and analysis of complexes with compositional or conformational variability. Some of the most important problems from a biological perspective involve the sort of compositional and conformational variability which remain challenging problems. The field also remains susceptible to problems of initial model bias, which are exacerbated in systems exhibiting structural variability, and as a result many structures are still published with exaggerated resolution claims. The standard protocols used by many in the field typically involve discarding a very large fraction of the raw data (as much as 80-90% in some cases), often based on qualitative assessments, raising questions related to rigor and reproducibility of structural results. In this proposal, we will develop or adapt image processing techniques to help resolve these issues, based on developments or unrealized concepts from mathematics and computer science. CryoEM and CryoET are used to study the structures of interacting biomolecules in the cell at resolutions 100x better than the best possible light microscope. This methodology permits new insights into the biomolecules which underlie disease, can shed light on structural changes in diseased cells and provide direct information on how drugs interact with the molecules they target. This grant develops the software used to turn noisy 2-D electron microscope images into reliable 3-D structures of individual molecules extending to near-atomic resolution.",SUPPORT AND DEVELOPMENT OF EMAN FOR ELECTRON MICROSCOPY IMAGE PROCESSING,9886087,R01GM080139,"['3-Dimensional', 'Algorithms', 'Appearance', 'Biochemical Process', 'Biological', 'Cells', 'Classification', 'Communities', 'Complex', 'Cryoelectron Microscopy', 'Data', 'Data Analyses', 'Data Quality', 'Data Reporting', 'Development', 'Disease', 'Drug Interactions', 'Drug Targeting', 'Drug effect disorder', 'Electron Microscope', 'Electron Microscopy', 'Exhibits', 'Grant', 'Image', 'In Vitro', 'Individual', 'Light', 'Light Microscope', 'Maps', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Molecular Conformation', 'Nature', 'Pathway interactions', 'Pharmaceutical Preparations', 'Process', 'Protocols documentation', 'Publishing', 'Reproducibility', 'Resolution', 'Roentgen Rays', 'Rotation', 'Scheme', 'Structure', 'System', 'Techniques', 'Technology', 'Training', 'Work', 'artificial neural network', 'autoencoder', 'base', 'case-based', 'computer science', 'deep learning', 'denoising', 'detector', 'file format', 'image processing', 'improved', 'in vivo', 'insight', 'macromolecule', 'mathematical sciences', 'microscopic imaging', 'neural network', 'novel diagnostics', 'particle', 'prototype', 'reconstruction', 'software development', 'symposium', 'three dimensional structure', 'tool', 'virtual']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2019,344862,-0.04518885490368794
"High-Accuracy Protein Models Derived from Lower Resolution Data An outstanding international interdisciplinary team has been assembled that will bring a broad variety of expertise to bear on protein model building, bringing together researchers from chemistry, physics, computer science, mathematics, structural biology, and bioinformatics. The expertise ranges from quantum chemistry to machine learning, and from datamining to high performance computing. Input from collaborating NMR and crystallographers will be essential for validating the protein models. Improving abilities to model proteins can impact public health in important ways by enhancing our basic understanding of protein behavior and by facilitating a more efficient selection of protein targets for drug design. The overall goal is to improve a wide range of protein modeling approaches, both by developing new approaches, and by combining those previously been developed. The specific aims are to: 1) Improve existing comparative (homology) modeling and 2) Improve models obtained by fold-recognition and ab initio procedures to make them useful for molecular replacement. There will be some new methods development. Efforts are in four areas - databases, interaction potentials, conformational sampling, and optimization for combining approaches. We will develop ways to include constraints mined from sub-atomic resolution protein structures using a new HIRES Database (to include structures with resolution < 0.85 A). These will include a structure fragment database, as well as short-range distance distributions. These data can be used to compare modeled structures against the collected data. Uses of the high resolution data will ilclude selecting higher quality fragments to replace poor quality segments in the models, for mining interaction potentials, and as a source of a variety of other high quality information regarding protein structures. Better assessments of protein structural models will be developed, including the assessment of the quality of individual segments within a protein structure; the new metrics developed will be used for assessing the quality of computer-built models, crystal structures and NMR structures, and provide indicators of the expected quality of whole protein models as well as of its segments. New ways to sample protein motions will be pursued. Combining diverse methods will lead to significant gains in the computer modeling of protein structures. Extensive testing and validation will be carried out at each stage and in each part of the project to ensure large gains in model accuracy. n/a",High-Accuracy Protein Models Derived from Lower Resolution Data,7683856,R01GM081680,"['Amino Acid Sequence', 'Area', 'Arts', 'Behavior', 'Bioinformatics', 'Biology', 'CASP6 gene', 'Carbon', 'Categories', 'Cellular biology', 'Cereals', 'Chemistry', 'Code', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Simulation', 'Computers', 'Consensus', 'Crystallography', 'Data', 'Databases', 'Detection', 'Development', 'Drug Design', 'Ensure', 'Enzymes', 'Evaluation', 'Foundations', 'Future', 'Generations', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Guidelines', 'High Performance Computing', 'Homology Modeling', 'Individual', 'Institutes', 'International', 'Investments', 'Iowa', 'Laboratories', 'Lead', 'Letters', 'Machine Learning', 'Mathematics', 'Medal', 'Methodology', 'Methods', 'Metric', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Conformation', 'Molecular Models', 'Monsters', 'Motion', 'Organism', 'Peptide Sequence Determination', 'Performance', 'Pharmaceutical Preparations', 'Physics', 'Polishes', 'Principal Investigator', 'Procedures', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Public Health', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Sampling', 'Solutions', 'Source', 'Staging', 'Structural Models', 'Structure', 'Techniques', 'Testing', 'Torsion', 'Universities', 'Ursidae Family', 'Validation', 'Variant', 'Wisconsin', 'Work', 'base', 'comparative', 'computer science', 'data mining', 'database structure', 'experience', 'improved', 'insertion/deletion mutation', 'member', 'method development', 'molecular mechanics', 'molecular modeling', 'network models', 'novel strategies', 'numb protein', 'programs', 'protein structure', 'protein structure prediction', 'quantum', 'quantum chemistry', 'reconstruction', 'research study', 'restraint', 'scaffold', 'simulation', 'software development', 'structural biology', 'theories', 'three dimensional structure', 'tool', 'web site']",NIGMS,IOWA STATE UNIVERSITY,R01,2009,231392,0.028897491328495693
"High-Accuracy Protein Models Derived from Lower Resolution Data An outstanding international interdisciplinary team has been assembled that will bring a broad variety of expertise to bear on protein model building, bringing together researchers from chemistry, physics, computer science, mathematics, structural biology, and bioinformatics. The expertise ranges from quantum chemistry to machine learning, and from datamining to high performance computing. Input from collaborating NMR and crystallographers will be essential for validating the protein models. Improving abilities to model proteins can impact public health in important ways by enhancing our basic understanding of protein behavior and by facilitating a more efficient selection of protein targets for drug design. The overall goal is to improve a wide range of protein modeling approaches, both by developing new approaches, and by combining those previously been developed. The specific aims are to: 1) Improve existing comparative (homology) modeling and 2) Improve models obtained by fold-recognition and ab initio procedures to make them useful for molecular replacement. There will be some new methods development. Efforts are in four areas - databases, interaction potentials, conformational sampling, and optimization for combining approaches. We will develop ways to include constraints mined from sub-atomic resolution protein structures using a new HIRES Database (to include structures with resolution < 0.85 A). These will include a structure fragment database, as well as short-range distance distributions. These data can be used to compare modeled structures against the collected data. Uses of the high resolution data will ilclude selecting higher quality fragments to replace poor quality segments in the models, for mining interaction potentials, and as a source of a variety of other high quality information regarding protein structures. Better assessments of protein structural models will be developed, including the assessment of the quality of individual segments within a protein structure; the new metrics developed will be used for assessing the quality of computer-built models, crystal structures and NMR structures, and provide indicators of the expected quality of whole protein models as well as of its segments. New ways to sample protein motions will be pursued. Combining diverse methods will lead to significant gains in the computer modeling of protein structures. Extensive testing and validation will be carried out at each stage and in each part of the project to ensure large gains in model accuracy. n/a",High-Accuracy Protein Models Derived from Lower Resolution Data,7931242,R01GM081680,"['Amino Acid Sequence', 'Area', 'Arts', 'Behavior', 'Bioinformatics', 'Biology', 'CASP6 gene', 'Carbon', 'Categories', 'Cellular biology', 'Cereals', 'Chemistry', 'Code', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Simulation', 'Computers', 'Consensus', 'Crystallography', 'Data', 'Databases', 'Detection', 'Development', 'Drug Design', 'Ensure', 'Enzymes', 'Evaluation', 'Foundations', 'Future', 'Generations', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Guidelines', 'High Performance Computing', 'Homology Modeling', 'Individual', 'Institutes', 'International', 'Investments', 'Iowa', 'Laboratories', 'Lead', 'Letters', 'Machine Learning', 'Mathematics', 'Medal', 'Methodology', 'Methods', 'Metric', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Conformation', 'Molecular Models', 'Monsters', 'Motion', 'Organism', 'Peptide Sequence Determination', 'Performance', 'Pharmaceutical Preparations', 'Physics', 'Polishes', 'Principal Investigator', 'Procedures', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Public Health', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Sampling', 'Solutions', 'Source', 'Staging', 'Structural Models', 'Structure', 'Techniques', 'Testing', 'Torsion', 'Universities', 'Ursidae Family', 'Validation', 'Variant', 'Wisconsin', 'Work', 'base', 'comparative', 'computer science', 'data mining', 'database structure', 'experience', 'improved', 'insertion/deletion mutation', 'member', 'method development', 'molecular mechanics', 'molecular modeling', 'network models', 'novel strategies', 'numb protein', 'programs', 'protein structure', 'protein structure prediction', 'quantum', 'quantum chemistry', 'reconstruction', 'research study', 'restraint', 'scaffold', 'simulation', 'software development', 'structural biology', 'theories', 'three dimensional structure', 'tool', 'web site']",NIGMS,IOWA STATE UNIVERSITY,R01,2009,52007,0.028897491328495693
"High-Accuracy Protein Models Derived from Lower Resolution Data An outstanding international interdisciplinary team has been assembled that will bring a broad variety of expertise to bear on protein model building, bringing together researchers from chemistry, physics, computer science, mathematics, structural biology, and bioinformatics. The expertise ranges from quantum chemistry to machine learning, and from datamining to high performance computing. Input from collaborating NMR and crystallographers will be essential for validating the protein models. Improving abilities to model proteins can impact public health in important ways by enhancing our basic understanding of protein behavior and by facilitating a more efficient selection of protein targets for drug design. The overall goal is to improve a wide range of protein modeling approaches, both by developing new approaches, and by combining those previously been developed. The specific aims are to: 1) Improve existing comparative (homology) modeling and 2) Improve models obtained by fold-recognition and ab initio procedures to make them useful for molecular replacement. There will be some new methods development. Efforts are in four areas - databases, interaction potentials, conformational sampling, and optimization for combining approaches. We will develop ways to include constraints mined from sub-atomic resolution protein structures using a new HIRES Database (to include structures with resolution < 0.85 A). These will include a structure fragment database, as well as short-range distance distributions. These data can be used to compare modeled structures against the collected data. Uses of the high resolution data will ilclude selecting higher quality fragments to replace poor quality segments in the models, for mining interaction potentials, and as a source of a variety of other high quality information regarding protein structures. Better assessments of protein structural models will be developed, including the assessment of the quality of individual segments within a protein structure; the new metrics developed will be used for assessing the quality of computer-built models, crystal structures and NMR structures, and provide indicators of the expected quality of whole protein models as well as of its segments. New ways to sample protein motions will be pursued. Combining diverse methods will lead to significant gains in the computer modeling of protein structures. Extensive testing and validation will be carried out at each stage and in each part of the project to ensure large gains in model accuracy. n/a",High-Accuracy Protein Models Derived from Lower Resolution Data,7495009,R01GM081680,"['Amino Acid Sequence', 'Area', 'Arts', 'Behavior', 'Bioinformatics', 'Biology', 'CASP6 gene', 'Carbon', 'Categories', 'Cellular biology', 'Cereals', 'Chemistry', 'Code', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Simulation', 'Computers', 'Consensus', 'Crystallography', 'Data', 'Databases', 'Detection', 'Development', 'Drug Design', 'Ensure', 'Enzymes', 'Evaluation', 'Foundations', 'Future', 'Generations', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Guidelines', 'High Performance Computing', 'Homology Modeling', 'Individual', 'Institutes', 'International', 'Internet', 'Investments', 'Iowa', 'Laboratories', 'Lead', 'Letters', 'Machine Learning', 'Mathematics', 'Medal', 'Methodology', 'Methods', 'Metric', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Conformation', 'Monsters', 'Motion', 'Numbers', 'Organism', 'Peptide Sequence Determination', 'Performance', 'Pharmaceutical Preparations', 'Physics', 'Polishes', 'Principal Investigator', 'Procedures', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Public Health', 'Purpose', 'Range', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Sampling', 'Score', 'Site', 'Solutions', 'Source', 'Staging', 'Standards of Weights and Measures', 'Structural Models', 'Structural Protein', 'Structure', 'Techniques', 'Testing', 'Torsion', 'Universities', 'Ursidae Family', 'Validation', 'Variant', 'Wisconsin', 'Work', 'base', 'comparative', 'computer science', 'data mining', 'experience', 'improved', 'insertion/deletion mutation', 'member', 'method development', 'molecular mechanics', 'molecular modeling', 'network models', 'novel strategies', 'numb protein', 'programs', 'protein structure', 'protein structure prediction', 'quantum', 'quantum chemistry', 'reconstruction', 'research study', 'restraint', 'scaffold', 'simulation', 'size', 'software development', 'structural biology', 'theories', 'three dimensional structure', 'tool']",NIGMS,IOWA STATE UNIVERSITY,R01,2008,231555,0.028897491328495693
"High-Accuracy Protein Models Derived from Lower Resolution Data    DESCRIPTION (provided by applicant):  An outstanding international interdisciplinary team has been assembled that will bring a broad variety of expertise to bear on protein model building, bringing together researchers from chemistry, physics, computer science, mathematics, structural biology, and bioinformatics. The expertise ranges from quantum chemistry to machine learning, and from data-mining to high performance computing. Input from collaborators in NMR and crystallography will be essential for validating the protein models. Improving abilities to model proteins can impact public health in important ways by enhancing our basic understanding of protein behavior and by facilitating a more efficient selection of protein targets for drug design.      The overall goal is to improve a wide range of protein modeling approaches, both by developing new approaches and by combining those previously been developed. The specific aims are to: (1) improve existing comparative (homology) modeling and (2) improve models obtained by fold recognition and ab initio procedures to make them useful for molecular replacement. There will be some new methods development. Efforts are in four areas--databases, interaction potentials, conformational sampling, and optimization for combining approaches. We will develop ways to include constraints mined from sub-atomic resolution protein structures using a new HIRES database (to include structures with resolution <0.85 A). These will include a structure fragment database, as well as short-range distance distributions. These data can be used to compare modeled structures against the collected data. Uses of the high resolution data will include selecting higher quality fragments to replace poor quality segments in the models, for mining interaction potentials, and as a source of a variety of other high quality information regarding protein structures.       Better assessments of protein structural models will be developed, including the assessment of the quality of individual segments within a protein structure; the new metrics developed will be used for assessing the quality of computer-built models, crystal structures and NMR structures, and provide indicators of the expected quality of whole protein models as well as of their segments. New ways to sample protein motions will be pursued. Combining diverse methods will lead to significant gains in the computer modeling of protein structures. Extensive testing and validation will be carried out at each stage and in each part of the project to ensure large gains in model accuracy.          n/a",High-Accuracy Protein Models Derived from Lower Resolution Data,7304272,R01GM081680,"['Amino Acid Sequence', 'Area', 'Arts', 'Behavior', 'Bioinformatics', 'Biology', 'CASP6 gene', 'Carbon', 'Categories', 'Cellular biology', 'Cereals', 'Chemistry', 'Code', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Simulation', 'Computers', 'Consensus', 'Crystallography', 'Data', 'Databases', 'Detection', 'Development', 'Drug Design', 'Ensure', 'Enzymes', 'Evaluation', 'Foundations', 'Future', 'Generations', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Guidelines', 'High Performance Computing', 'Homology Modeling', 'Individual', 'Institutes', 'International', 'Internet', 'Investments', 'Iowa', 'Laboratories', 'Lead', 'Letters', 'Machine Learning', 'Mathematics', 'Medal', 'Methodology', 'Methods', 'Metric', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Conformation', 'Monsters', 'Motion', 'Numbers', 'Organism', 'Peptide Sequence Determination', 'Performance', 'Pharmaceutical Preparations', 'Physics', 'Polishes', 'Principal Investigator', 'Procedures', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Public Health', 'Purpose', 'Range', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Sampling', 'Score', 'Site', 'Solutions', 'Source', 'Staging', 'Standards of Weights and Measures', 'Structural Models', 'Structural Protein', 'Structure', 'Techniques', 'Testing', 'Torsion', 'Universities', 'Ursidae Family', 'Validation', 'Variant', 'Wisconsin', 'Work', 'base', 'comparative', 'computer science', 'data mining', 'experience', 'improved', 'insertion/deletion mutation', 'member', 'method development', 'molecular mechanics', 'molecular modeling', 'network models', 'novel strategies', 'numb protein', 'programs', 'protein structure', 'protein structure prediction', 'quantum', 'quantum chemistry', 'reconstruction', 'research study', 'restraint', 'scaffold', 'simulation', 'size', 'software development', 'structural biology', 'theories', 'three dimensional structure', 'tool']",NIGMS,IOWA STATE UNIVERSITY,R01,2007,248049,0.029456474323777057
"University of Chicago Autoimmunity Center of Excellence PROJECT SUMMARY The focus of the University of Chicago ACE (UCACE) is in situ human autoimmunity. During the last funding cycle, we successfully developed techniques to fully characterize the transcriptional state of single B cells and plasmablasts sorted from tissue samples and to pair this with analyses of functional immunoglobulin repertoire. Application of this approach to lupus tubulointerstitial inflammation (TII) and renal acute mixed allograft rejection (AMR) suggest that the activation state, antigenic repertoire and mechanisms of antigenic-driven B cell selection in human inflammation is fundamentally different than that typically observed in secondary lymphoid organs (Collaborative Project). In Celiac disease, B cells expressing transglutaminase 2 (TG2) specific antibodies are massively expanded in the duodenal mucosa. A highly-restricted repertoire of VH genes encode these antibodies, most notably VH5-51. Remarkably, this VH gene is also over-represented in the recirculating IgA+ B cell pool which is rich in anti-microbial activity. These findings suggest a model in which microbial antigens select for pathogenic TG2 reactive antibodies in susceptible hosts (Principle Project). Our second technical innovation is unique to the UCACE (Collaborative Project). Previous work has demonstrated that by quantifying the distance between T and B cells in multicolor confocal images (Cell Distance Mapping, CDM) we could identify competent TFH cells and functional relationships with B cells. We have now implemented a deep convolutional neural network (DCNN) that accurately identifies both cell position and shape in multicolor confocal images. In mice, analysis of the DCNN output (CDM3) indicates that T cell shape as a function of distance from dendritic cells (DCs) discriminates between cognate and non- cognate T cell:DC interactions with a sensitivity and specificity approaching that of two-photon emission microscopy (TPEM). In lupus TII, CDM3 both confirmed that myeloid DCs present antigen to CD4+ T cells in situ and identified plasmacytoid DCs as an important antigen presenting cell (APC) in severe TII. Finally, in the Pilot Project, we are applying microfluidics to develop in vitro culture systems capable of studying cognate interactions between single cells. These projects demonstrate a novel pipeline of methodologies to identify in situ cell populations, characterize their function and quantify the adaptive cell networks through which they cooperate to drive local adaptive immunity and inflammation. PROJECT NARRATIVE The focus of the University of Chicago Autoimmunity Center of Excellence is in situ adaptive autoimmunity. Across autoimmune diseases such as lupus nephritis, rheumatoid arthritis and Celiac disease, local immune responses in affected tissues drive tissue damage leading to overall morbidity and mortality. The purpose of the UCACE is to better understand these in situ adaptive mechanisms, which will enable the development of more effective and less toxic therapies.",University of Chicago Autoimmunity Center of Excellence,9920098,U19AI082724,"['Acute', 'Affect', 'Affinity', 'Anatomy', 'Antibodies', 'Antigen-Presenting Cells', 'Architecture', 'Autoantibodies', 'Autoantigens', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'B-Lymphocytes', 'Biological Assay', 'Blood', 'CD4 Positive T Lymphocytes', 'Celiac Disease', 'Cell Communication', 'Cell Shape', 'Cells', 'Chicago', 'Dendritic Cells', 'Development', 'Disease', 'Duodenum', 'Flow Cytometry', 'Funding', 'Genes', 'Genetic Transcription', 'Gluten', 'Helper-Inducer T-Lymphocyte', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunoglobulin A', 'Immunoglobulins', 'In Situ', 'In Vitro', 'Individual', 'Inflammation', 'Inflammatory', 'Kidney', 'Link', 'Lupus', 'Lupus Nephritis', 'Mediating', 'Memory', 'Methodology', 'Methods', 'Microfluidics', 'Microscopy', 'Modeling', 'Morbidity - disease rate', 'Mucous Membrane', 'Mus', 'Myelogenous', 'Organ', 'Output', 'Pathogenicity', 'Pilot Projects', 'Plasmablast', 'Population', 'Positioning Attribute', 'Process', 'Rheumatoid Arthritis', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Site', 'Specificity', 'System', 'T-Lymphocyte', 'Techniques', 'Testing', 'Tissue Sample', 'Tissues', 'Universities', 'Work', 'adaptive immunity', 'allograft rejection', 'antimicrobial', 'base', 'confocal imaging', 'convolutional neural network', 'disease heterogeneity', 'high throughput analysis', 'innovation', 'insight', 'microorganism antigen', 'molecular phenotype', 'mortality', 'novel', 'peripheral blood', 'response', 'secondary lymphoid organ', 'single-cell RNA sequencing', 'transglutaminase 2', 'two-photon']",NIAID,UNIVERSITY OF CHICAGO,U19,2020,444012,-0.04081686984186044
"University of Chicago Autoimmunity Center of Excellence PROJECT SUMMARY The focus of the University of Chicago ACE (UCACE) is in situ human autoimmunity. During the last funding cycle, we successfully developed techniques to fully characterize the transcriptional state of single B cells and plasmablasts sorted from tissue samples and to pair this with analyses of functional immunoglobulin repertoire. Application of this approach to lupus tubulointerstitial inflammation (TII) and renal acute mixed allograft rejection (AMR) suggest that the activation state, antigenic repertoire and mechanisms of antigenic-driven B cell selection in human inflammation is fundamentally different than that typically observed in secondary lymphoid organs (Collaborative Project). In Celiac disease, B cells expressing transglutaminase 2 (TG2) specific antibodies are massively expanded in the duodenal mucosa. A highly-restricted repertoire of VH genes encode these antibodies, most notably VH5-51. Remarkably, this VH gene is also over-represented in the recirculating IgA+ B cell pool which is rich in anti-microbial activity. These findings suggest a model in which microbial antigens select for pathogenic TG2 reactive antibodies in susceptible hosts (Principle Project). Our second technical innovation is unique to the UCACE (Collaborative Project). Previous work has demonstrated that by quantifying the distance between T and B cells in multicolor confocal images (Cell Distance Mapping, CDM) we could identify competent TFH cells and functional relationships with B cells. We have now implemented a deep convolutional neural network (DCNN) that accurately identifies both cell position and shape in multicolor confocal images. In mice, analysis of the DCNN output (CDM3) indicates that T cell shape as a function of distance from dendritic cells (DCs) discriminates between cognate and non- cognate T cell:DC interactions with a sensitivity and specificity approaching that of two-photon emission microscopy (TPEM). In lupus TII, CDM3 both confirmed that myeloid DCs present antigen to CD4+ T cells in situ and identified plasmacytoid DCs as an important antigen presenting cell (APC) in severe TII. Finally, in the Pilot Project, we are applying microfluidics to develop in vitro culture systems capable of studying cognate interactions between single cells. These projects demonstrate a novel pipeline of methodologies to identify in situ cell populations, characterize their function and quantify the adaptive cell networks through which they cooperate to drive local adaptive immunity and inflammation. PROJECT NARRATIVE The focus of the University of Chicago Autoimmunity Center of Excellence is in situ adaptive autoimmunity. Across autoimmune diseases such as lupus nephritis, rheumatoid arthritis and Celiac disease, local immune responses in affected tissues drive tissue damage leading to overall morbidity and mortality. The purpose of the UCACE is to better understand these in situ adaptive mechanisms, which will enable the development of more effective and less toxic therapies.",University of Chicago Autoimmunity Center of Excellence,9728508,U19AI082724,"['Acute', 'Affect', 'Affinity', 'Anatomy', 'Antibodies', 'Antigen-Presenting Cells', 'Architecture', 'Autoantibodies', 'Autoantigens', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'B-Lymphocytes', 'Biological Assay', 'Blood', 'CD4 Positive T Lymphocytes', 'Celiac Disease', 'Cell Communication', 'Cell Shape', 'Cells', 'Chicago', 'Dendritic Cells', 'Development', 'Disease', 'Duodenum', 'Flow Cytometry', 'Funding', 'Genes', 'Genetic Transcription', 'Gluten', 'Helper-Inducer T-Lymphocyte', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunoglobulin A', 'Immunoglobulins', 'In Situ', 'In Vitro', 'Individual', 'Inflammation', 'Inflammatory', 'Kidney', 'Link', 'Lupus', 'Lupus Nephritis', 'Lymphoid', 'Mediating', 'Memory', 'Methodology', 'Methods', 'Microfluidics', 'Microscopy', 'Modeling', 'Morbidity - disease rate', 'Mucous Membrane', 'Mus', 'Myelogenous', 'Organ', 'Output', 'Pathogenicity', 'Pilot Projects', 'Plasmablast', 'Population', 'Positioning Attribute', 'Process', 'Rheumatoid Arthritis', 'Sampling', 'Sensitivity and Specificity', 'Shapes', 'Site', 'Specificity', 'System', 'T-Lymphocyte', 'Techniques', 'Testing', 'Tissue Sample', 'Tissues', 'Universities', 'Work', 'adaptive immunity', 'allograft rejection', 'antimicrobial', 'base', 'confocal imaging', 'convolutional neural network', 'disease heterogeneity', 'high throughput analysis', 'innovation', 'insight', 'microorganism antigen', 'molecular phenotype', 'mortality', 'novel', 'peripheral blood', 'response', 'single-cell RNA sequencing', 'transglutaminase 2', 'two-photon']",NIAID,UNIVERSITY OF CHICAGO,U19,2019,525012,-0.04081686984186044
"Empirical conformation-dependent covalent geometry variation in proteins    DESCRIPTION (provided by applicant): A detailed and accurate understanding of the structure of proteins is one cornerstone of modern biomedical research, and an explicit goal of the NIH is to define the structure of all proteins either by accurate experimental determination or comparative model-building. The most successful structure prediction approaches employ empirical knowledge-based energy terms derived from features of known protein structures - most notably single-residue ???-distributions, backbone-dependent side chain rotamer preferences, and tight packing criteria. One known unrealistic feature of these prediction programs is the assumption of a fixed ideal geometry for the backbone. The driving hypothesis behind this proposal is that there exists a largely unappreciated but real, systematic, significant and pervasive variation in backbone bond angles and peptide planarity that occurs as a function of backbone torsion angles, and accounting properly for this variation will be required to achieve X-ray crystal structure quality for comparative models. The overall goal of this work is to generate accurate empirical values for this covalent variation that will lead to tangible improvements in the accuracy of structures produced by comparative modeling and de novo structure prediction as well as by X-ray crystallography. We propose to achieve this overall goal by pursuing the following three specific aims: 1) to design, develop, and make available a flexibly-searchable database containing bond lengths, bond angles, and torsion angles for all structures known at better than 1.75 ¿ resolution (currently ~500,000 residues); 2) to use conventional query-based and modern machine learning approaches to derive accurate empirical information from the database about the systematic correlation of local conformation with variations in covalent geometry; and 3) to create a modular conformation-dependent expected covalent geometry library and to facilitate its incorporation into leading applications for comparative and crystallographic protein structure modeling. With the dramatically increased number of ultrahigh-resolution resolution crystal structures now known, the time is ripe for construction of this Protein Geometry Database that will provide facile access to a massive treasure trove of reliable and detailed empirical information about protein structure. To be done well, this work will require painstaking attention to detail and an intimate familiarity with the limitations of crystallographic refinement and the principles of protein structure. Dr. Karplus is well-suited to lead this work as he has a 20+-year track record of quality crystallographic structure determinations combined with contributions of more general insights into protein structure, among them being the pioneering characterization of the conformation-dependent variations in covalent geometry that serves as this project's foundation. Collaborations with world-leading groups in structure prediction, in crystallographic refinement and structure validation, and in knowledge-based library development ensure a rapid and effective translation of the gleaned information into improvements in protein modeling. PUBLIC HEALTH RELEVANCE: Proteins are responsible for carrying out most of the processes of life and their function depends exquisitely on their structure, even on the tiniest structural details. For this reason, determining accurate structures of proteins is a cornerstone of modern biomedical research. This work is aimed at leading to a universal improvement in the accuracy with which protein structure can be built.           n/a",Empirical conformation-dependent covalent geometry variation in proteins,7525973,R01GM083136,"['Accounting', 'Attention', 'Automobile Driving', 'Biomedical Research', 'Catalysis', 'Classification', 'Collaborations', 'Communities', 'Crystallography', 'Databases', 'Development', 'Ensure', 'Enzymes', 'Facility Construction Funding Category', 'Familiarity', 'Foundations', 'Glean', 'Goals', 'Heart', 'Homology Modeling', 'Investments', 'Knowledge', 'Lead', 'Length', 'Letters', 'Libraries', 'Life', 'Machine Learning', 'Methodology', 'Mining', 'Modeling', 'Molecular Conformation', 'Numbers', 'Object Attachment', 'Online Systems', 'Other Resources', 'Pattern', 'Peptides', 'Personal Satisfaction', 'Pharmaceutical Preparations', 'Process', 'Protein Conformation', 'Protein Structure Initiative', 'Proteins', 'Public Health', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Roentgen Rays', 'Side', 'Structure', 'Technology', 'Time', 'Torsion', 'Translations', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vertebral column', 'Work', 'X-Ray Crystallography', 'base', 'comparative', 'cost', 'design', 'disease-causing mutation', 'falls', 'inhibitor/antagonist', 'innovation', 'insight', 'knowledge base', 'molecular mechanics', 'novel', 'predictive modeling', 'preference', 'programs', 'protein structure', 'protein structure prediction', 'software development', 'structural biology', 'structural genomics']",NIGMS,OREGON STATE UNIVERSITY,R01,2008,213756,-0.02135690727665672
"Improving Modeling by Learning from Details of High Accuracy Protein Structures DESCRIPTION (provided by applicant): The functions of proteins depend exquisitely on their structure, with details at the 0.1 � scale influencing enzyme catalysis, disease-causing mutations, and drug recognition. For this reason, having detailed and accurate structures of proteins is a cornerstone of modern biomedical research, and the NIH funded the Protein Structure Initiative with the goal of obtaining models for every protein structure with an accuracy approaching that of a high-resolution crystal structure. Current technology for template-based modeling is powerful, but cannot yet deliver ""near-crystal-structure"" quality. Tests show that the best minimization routines still fall short of consistently producing protein models for close homologs that approach within ~1 � rmsd of the 'native' structure as ultimately revealed by crystal structures. To help break through this 1 � barrier, during the previous period of support we used ultrahigh-resolution structures to create a library of conformation- dependent ideal geometry functions for the protein backbone, and showed that its use improves the quality of protein crystal structures and holds promise to improve template-based model refinement. We also discovered that ultrahigh-resolution crystal structures are a rich source of details about protein structure that are not accurately attainable from structures in the ~1.5-2 � resolution range and thus have not yet been fully accounted for in current energy functions. Here, our central hypothesis is that a major step forward in template-based modeling accuracy will come from identifying and explicitly taking into account detailed features of protein covalent geometry, conformation and non-covalent packing interactions that have not yet been characterized, and can now be gleaned from the study of highly accurate ultrahigh-resolution protein structures. The overall goal of our proposal is to mine such information so it can be used to improve the accuracy of predictive modeling. With many ultrahigh-resolution structures now available, the time is ripe to achieve this goal by pursuing three specific aims related to (1) extending the impact of the 'ideal geometry function' paradigm by creating, optimizing, and implementing conformation- dependent libraries accounting for peptide planarity, side chains, and cis-peptides, (2) mining ultrahigh- resolution crystal structures to glean information for next-generation empirical energy functions, and (3) analyzing ultrahigh-resolution protein structures solved in varying environments to produce a set of benchmark test cases and developing residue level assessment tools to use with these test cases to evaluate and hone template-based modeling refinement applications. This proposed work is low cost and low risk, and has a high likelihood of substantial impact as it provides basic information that can be widely incorporated into predictive and experimental modeling applications to improve their accuracy. It is also distinct from major efforts being invested into template-based modeling. Introducing this greater level of realism is a prerequisite to improving the refinement step of template-based modeling and achieving the goals of the Protein Structure Initiative. Proteins carry out the work that gets done inside of cells, so figuring out what they look like helps us understand things like how drugs work and how to design new drugs that will work even better. It is not practical to experimentally determine every protein structure, so having reliable ways to use computers to predict their structures is quite important. Current methods are not quite accurate enough, and the goal of this work is to look carefully at the best known protein structures to learn from their exact features how we can improve prediction technology to get the details right.",Improving Modeling by Learning from Details of High Accuracy Protein Structures,8895978,R01GM083136,"['Accounting', 'Automobile Driving', 'Behavior', 'Benchmarking', 'Biomedical Research', 'Catalysis', 'Cells', 'Computers', 'Crystallography', 'Databases', 'Development', 'Drug Formulations', 'Environment', 'Enzymes', 'Experimental Models', 'Funding', 'Generations', 'Geometry', 'Glean', 'Goals', 'Homologous Gene', 'Investments', 'Learning', 'Libraries', 'Life', 'Machine Learning', 'Maintenance', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular Conformation', 'Peptides', 'Pharmaceutical Preparations', 'Process', 'Protein Conformation', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Side', 'Source', 'Structure', 'Technology', 'Testing', 'Time', 'Torsion', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vertebral column', 'Work', 'base', 'cost', 'design', 'disease-causing mutation', 'falls', 'improved', 'inhibitor/antagonist', 'innovation', 'knowledge base', 'model building', 'molecular mechanics', 'next generation', 'predictive modeling', 'protein function', 'protein structure', 'protein structure prediction', 'tool', 'trend', 'ultra high resolution']",NIGMS,OREGON STATE UNIVERSITY,R01,2015,206832,-0.02496676018262803
"Improving Modeling by Learning from Details of High Accuracy Protein Structures     DESCRIPTION (provided by applicant): The functions of proteins depend exquisitely on their structure, with details at the 0.1 ¿ scale influencing enzyme catalysis, disease-causing mutations, and drug recognition. For this reason, having detailed and accurate structures of proteins is a cornerstone of modern biomedical research, and the NIH funded the Protein Structure Initiative with the goal of obtaining models for every protein structure with an accuracy approaching that of a high-resolution crystal structure. Current technology for template-based modeling is powerful, but cannot yet deliver ""near-crystal-structure"" quality. Tests show that the best minimization routines still fall short of consistently producing protein models for close homologs that approach within ~1 ¿ rmsd of the 'native' structure as ultimately revealed by crystal structures. To help break through this 1 ¿ barrier, during the previous period of support we used ultrahigh-resolution structures to create a library of conformation- dependent ideal geometry functions for the protein backbone, and showed that its use improves the quality of protein crystal structures and holds promise to improve template-based model refinement. We also discovered that ultrahigh-resolution crystal structures are a rich source of details about protein structure that are not accurately attainable from structures in the ~1.5-2 ¿ resolution range and thus have not yet been fully accounted for in current energy functions. Here, our central hypothesis is that a major step forward in template-based modeling accuracy will come from identifying and explicitly taking into account detailed features of protein covalent geometry, conformation and non-covalent packing interactions that have not yet been characterized, and can now be gleaned from the study of highly accurate ultrahigh-resolution protein structures. The overall goal of our proposal is to mine such information so it can be used to improve the accuracy of predictive modeling. With many ultrahigh-resolution structures now available, the time is ripe to achieve this goal by pursuing three specific aims related to (1) extending the impact of the 'ideal geometry function' paradigm by creating, optimizing, and implementing conformation- dependent libraries accounting for peptide planarity, side chains, and cis-peptides, (2) mining ultrahigh- resolution crystal structures to glean information for next-generation empirical energy functions, and (3) analyzing ultrahigh-resolution protein structures solved in varying environments to produce a set of benchmark test cases and developing residue level assessment tools to use with these test cases to evaluate and hone template-based modeling refinement applications. This proposed work is low cost and low risk, and has a high likelihood of substantial impact as it provides basic information that can be widely incorporated into predictive and experimental modeling applications to improve their accuracy. It is also distinct from major efforts being invested into template-based modeling. Introducing this greater level of realism is a prerequisite to improving the refinement step of template-based modeling and achieving the goals of the Protein Structure Initiative.           Proteins carry out the work that gets done inside of cells, so figuring out what they look like helps us understand things like how drugs work and how to design new drugs that will work even better. It is not practical to experimentally determine every protein structure, so having reliable ways to use computers to predict their structures is quite important. Current methods are not quite accurate enough, and the goal of this work is to look carefully at the best known protein structures to learn from their exact features how we can improve prediction technology to get the details right.            ",Improving Modeling by Learning from Details of High Accuracy Protein Structures,8708105,R01GM083136,"['Accounting', 'Automobile Driving', 'Behavior', 'Benchmarking', 'Biomedical Research', 'Catalysis', 'Cells', 'Computers', 'Crystallography', 'Databases', 'Development', 'Drug Formulations', 'Environment', 'Enzymes', 'Experimental Models', 'Funding', 'Generations', 'Geometry', 'Glean', 'Goals', 'Homologous Gene', 'Investments', 'Learning', 'Libraries', 'Life', 'Machine Learning', 'Maintenance', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular Conformation', 'Peptides', 'Pharmaceutical Preparations', 'Process', 'Protein Conformation', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Side', 'Source', 'Structure', 'Technology', 'Testing', 'Time', 'Torsion', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vertebral column', 'Work', 'base', 'cost', 'design', 'disease-causing mutation', 'falls', 'improved', 'inhibitor/antagonist', 'innovation', 'knowledge base', 'molecular mechanics', 'next generation', 'predictive modeling', 'protein function', 'protein structure', 'protein structure prediction', 'tool', 'trend', 'ultra high resolution']",NIGMS,OREGON STATE UNIVERSITY,R01,2014,207408,-0.02496676018262803
"Improving Modeling by Learning from Details of High Accuracy Protein Structures     DESCRIPTION (provided by applicant): The functions of proteins depend exquisitely on their structure, with details at the 0.1 ¿ scale influencing enzyme catalysis, disease-causing mutations, and drug recognition. For this reason, having detailed and accurate structures of proteins is a cornerstone of modern biomedical research, and the NIH funded the Protein Structure Initiative with the goal of obtaining models for every protein structure with an accuracy approaching that of a high-resolution crystal structure. Current technology for template-based modeling is powerful, but cannot yet deliver ""near-crystal-structure"" quality. Tests show that the best minimization routines still fall short of consistently producing protein models for close homologs that approach within ~1 ¿ rmsd of the 'native' structure as ultimately revealed by crystal structures. To help break through this 1 ¿ barrier, during the previous period of support we used ultrahigh-resolution structures to create a library of conformation- dependent ideal geometry functions for the protein backbone, and showed that its use improves the quality of protein crystal structures and holds promise to improve template-based model refinement. We also discovered that ultrahigh-resolution crystal structures are a rich source of details about protein structure that are not accurately attainable from structures in the ~1.5-2 ¿ resolution range and thus have not yet been fully accounted for in current energy functions. Here, our central hypothesis is that a major step forward in template-based modeling accuracy will come from identifying and explicitly taking into account detailed features of protein covalent geometry, conformation and non-covalent packing interactions that have not yet been characterized, and can now be gleaned from the study of highly accurate ultrahigh-resolution protein structures. The overall goal of our proposal is to mine such information so it can be used to improve the accuracy of predictive modeling. With many ultrahigh-resolution structures now available, the time is ripe to achieve this goal by pursuing three specific aims related to (1) extending the impact of the 'ideal geometry function' paradigm by creating, optimizing, and implementing conformation- dependent libraries accounting for peptide planarity, side chains, and cis-peptides, (2) mining ultrahigh- resolution crystal structures to glean information for next-generation empirical energy functions, and (3) analyzing ultrahigh-resolution protein structures solved in varying environments to produce a set of benchmark test cases and developing residue level assessment tools to use with these test cases to evaluate and hone template-based modeling refinement applications. This proposed work is low cost and low risk, and has a high likelihood of substantial impact as it provides basic information that can be widely incorporated into predictive and experimental modeling applications to improve their accuracy. It is also distinct from major efforts being invested into template-based modeling. Introducing this greater level of realism is a prerequisite to improving the refinement step of template-based modeling and achieving the goals of the Protein Structure Initiative.           Proteins carry out the work that gets done inside of cells, so figuring out what they look like helps us understand things like how drugs work and how to design new drugs that will work even better. It is not practical to experimentally determine every protein structure, so having reliable ways to use computers to predict their structures is quite important. Current methods are not quite accurate enough, and the goal of this work is to look carefully at the best known protein structures to learn from their exact features how we can improve prediction technology to get the details right.            ",Improving Modeling by Learning from Details of High Accuracy Protein Structures,8547080,R01GM083136,"['Accounting', 'Automobile Driving', 'Behavior', 'Benchmarking', 'Biomedical Research', 'Catalysis', 'Cells', 'Computers', 'Crystallography', 'Databases', 'Development', 'Drug Formulations', 'Environment', 'Enzymes', 'Experimental Models', 'Funding', 'Generations', 'Glean', 'Goals', 'Homologous Gene', 'Investments', 'Learning', 'Libraries', 'Life', 'Machine Learning', 'Maintenance', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular Conformation', 'Peptides', 'Pharmaceutical Preparations', 'Process', 'Protein Conformation', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Side', 'Source', 'Structure', 'Technology', 'Testing', 'Time', 'Torsion', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vertebral column', 'Work', 'base', 'cost', 'design', 'disease-causing mutation', 'falls', 'improved', 'inhibitor/antagonist', 'innovation', 'knowledge base', 'molecular mechanics', 'next generation', 'predictive modeling', 'protein function', 'protein structure', 'protein structure prediction', 'tool', 'trend', 'ultra high resolution']",NIGMS,OREGON STATE UNIVERSITY,R01,2013,200665,-0.02496676018262803
"Improving Modeling by Learning from Details of High Accuracy Protein Structures     DESCRIPTION (provided by applicant): The functions of proteins depend exquisitely on their structure, with details at the 0.1 ¿ scale influencing enzyme catalysis, disease-causing mutations, and drug recognition. For this reason, having detailed and accurate structures of proteins is a cornerstone of modern biomedical research, and the NIH funded the Protein Structure Initiative with the goal of obtaining models for every protein structure with an accuracy approaching that of a high-resolution crystal structure. Current technology for template-based modeling is powerful, but cannot yet deliver ""near-crystal-structure"" quality. Tests show that the best minimization routines still fall short of consistently producing protein models for close homologs that approach within ~1 ¿ rmsd of the 'native' structure as ultimately revealed by crystal structures. To help break through this 1 ¿ barrier, during the previous period of support we used ultrahigh-resolution structures to create a library of conformation- dependent ideal geometry functions for the protein backbone, and showed that its use improves the quality of protein crystal structures and holds promise to improve template-based model refinement. We also discovered that ultrahigh-resolution crystal structures are a rich source of details about protein structure that are not accurately attainable from structures in the ~1.5-2 ¿ resolution range and thus have not yet been fully accounted for in current energy functions. Here, our central hypothesis is that a major step forward in template-based modeling accuracy will come from identifying and explicitly taking into account detailed features of protein covalent geometry, conformation and non-covalent packing interactions that have not yet been characterized, and can now be gleaned from the study of highly accurate ultrahigh-resolution protein structures. The overall goal of our proposal is to mine such information so it can be used to improve the accuracy of predictive modeling. With many ultrahigh-resolution structures now available, the time is ripe to achieve this goal by pursuing three specific aims related to (1) extending the impact of the 'ideal geometry function' paradigm by creating, optimizing, and implementing conformation- dependent libraries accounting for peptide planarity, side chains, and cis-peptides, (2) mining ultrahigh- resolution crystal structures to glean information for next-generation empirical energy functions, and (3) analyzing ultrahigh-resolution protein structures solved in varying environments to produce a set of benchmark test cases and developing residue level assessment tools to use with these test cases to evaluate and hone template-based modeling refinement applications. This proposed work is low cost and low risk, and has a high likelihood of substantial impact as it provides basic information that can be widely incorporated into predictive and experimental modeling applications to improve their accuracy. It is also distinct from major efforts being invested into template-based modeling. Introducing this greater level of realism is a prerequisite to improving the refinement step of template-based modeling and achieving the goals of the Protein Structure Initiative.        PUBLIC HEALTH RELEVANCE:  Proteins carry out the work that gets done inside of cells, so figuring out what they look like helps us understand things like how drugs work and how to design new drugs that will work even better. It is not practical to experimentally determine every protein structure, so having reliable ways to use computers to predict their structures is quite important. Current methods are not quite accurate enough, and the goal of this work is to look carefully at the best known protein structures to learn from their exact features how we can improve prediction technology to get the details right.               Proteins carry out the work that gets done inside of cells, so figuring out what they look like helps us understand things like how drugs work and how to design new drugs that will work even better. It is not practical to experimentally determine every protein structure, so having reliable ways to use computers to predict their structures is quite important. Current methods are not quite accurate enough, and the goal of this work is to look carefully at the best known protein structures to learn from their exact features how we can improve prediction technology to get the details right.            ",Improving Modeling by Learning from Details of High Accuracy Protein Structures,8438862,R01GM083136,"['Accounting', 'Automobile Driving', 'Behavior', 'Benchmarking', 'Biomedical Research', 'Catalysis', 'Cells', 'Computers', 'Crystallography', 'Databases', 'Development', 'Drug Formulations', 'Environment', 'Enzymes', 'Experimental Models', 'Funding', 'Generations', 'Glean', 'Goals', 'Homologous Gene', 'Investments', 'Learning', 'Libraries', 'Life', 'Machine Learning', 'Maintenance', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular Conformation', 'Peptides', 'Pharmaceutical Preparations', 'Process', 'Protein Conformation', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Side', 'Source', 'Structure', 'Technology', 'Testing', 'Time', 'Torsion', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vertebral column', 'Work', 'base', 'cost', 'design', 'disease-causing mutation', 'falls', 'improved', 'inhibitor/antagonist', 'innovation', 'knowledge base', 'molecular mechanics', 'next generation', 'predictive modeling', 'protein function', 'protein structure', 'protein structure prediction', 'tool', 'trend', 'ultra high resolution']",NIGMS,OREGON STATE UNIVERSITY,R01,2012,208438,-0.02166364274697327
"Empirical conformation-dependent covalent geometry variation in proteins    DESCRIPTION (provided by applicant): A detailed and accurate understanding of the structure of proteins is one cornerstone of modern biomedical research, and an explicit goal of the NIH is to define the structure of all proteins either by accurate experimental determination or comparative model-building. The most successful structure prediction approaches employ empirical knowledge-based energy terms derived from features of known protein structures - most notably single-residue ???-distributions, backbone-dependent side chain rotamer preferences, and tight packing criteria. One known unrealistic feature of these prediction programs is the assumption of a fixed ideal geometry for the backbone. The driving hypothesis behind this proposal is that there exists a largely unappreciated but real, systematic, significant and pervasive variation in backbone bond angles and peptide planarity that occurs as a function of backbone torsion angles, and accounting properly for this variation will be required to achieve X-ray crystal structure quality for comparative models. The overall goal of this work is to generate accurate empirical values for this covalent variation that will lead to tangible improvements in the accuracy of structures produced by comparative modeling and de novo structure prediction as well as by X-ray crystallography. We propose to achieve this overall goal by pursuing the following three specific aims: 1) to design, develop, and make available a flexibly-searchable database containing bond lengths, bond angles, and torsion angles for all structures known at better than 1.75 ¿ resolution (currently ~500,000 residues); 2) to use conventional query-based and modern machine learning approaches to derive accurate empirical information from the database about the systematic correlation of local conformation with variations in covalent geometry; and 3) to create a modular conformation-dependent expected covalent geometry library and to facilitate its incorporation into leading applications for comparative and crystallographic protein structure modeling. With the dramatically increased number of ultrahigh-resolution resolution crystal structures now known, the time is ripe for construction of this Protein Geometry Database that will provide facile access to a massive treasure trove of reliable and detailed empirical information about protein structure. To be done well, this work will require painstaking attention to detail and an intimate familiarity with the limitations of crystallographic refinement and the principles of protein structure. Dr. Karplus is well-suited to lead this work as he has a 20+-year track record of quality crystallographic structure determinations combined with contributions of more general insights into protein structure, among them being the pioneering characterization of the conformation-dependent variations in covalent geometry that serves as this project's foundation. Collaborations with world-leading groups in structure prediction, in crystallographic refinement and structure validation, and in knowledge-based library development ensure a rapid and effective translation of the gleaned information into improvements in protein modeling. PUBLIC HEALTH RELEVANCE: Proteins are responsible for carrying out most of the processes of life and their function depends exquisitely on their structure, even on the tiniest structural details. For this reason, determining accurate structures of proteins is a cornerstone of modern biomedical research. This work is aimed at leading to a universal improvement in the accuracy with which protein structure can be built.           n/a",Empirical conformation-dependent covalent geometry variation in proteins,8111114,R01GM083136,"['Accounting', 'Attention', 'Automobile Driving', 'Biomedical Research', 'Catalysis', 'Collaborations', 'Communities', 'Crystallography', 'Databases', 'Development', 'Ensure', 'Enzymes', 'Familiarity', 'Foundations', 'Glean', 'Goals', 'Health', 'Heart', 'Homology Modeling', 'Investments', 'Knowledge', 'Lead', 'Length', 'Letters', 'Libraries', 'Life', 'Machine Learning', 'Methodology', 'Mining', 'Modeling', 'Molecular Conformation', 'Online Systems', 'Pattern', 'Peptides', 'Pharmaceutical Preparations', 'Process', 'Protein Conformation', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Roentgen Rays', 'Side', 'Structure', 'Technology', 'Time', 'Torsion', 'Translations', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vertebral column', 'Work', 'X-Ray Crystallography', 'base', 'comparative', 'cost', 'design', 'disease-causing mutation', 'falls', 'flexibility', 'inhibitor/antagonist', 'innovation', 'insight', 'knowledge base', 'molecular mechanics', 'novel', 'predictive modeling', 'preference', 'programs', 'protein structure', 'protein structure prediction', 'software development', 'structural biology', 'structural genomics']",NIGMS,OREGON STATE UNIVERSITY,R01,2011,208186,-0.02135690727665672
"Empirical conformation-dependent covalent geometry variation in proteins    DESCRIPTION (provided by applicant): A detailed and accurate understanding of the structure of proteins is one cornerstone of modern biomedical research, and an explicit goal of the NIH is to define the structure of all proteins either by accurate experimental determination or comparative model-building. The most successful structure prediction approaches employ empirical knowledge-based energy terms derived from features of known protein structures - most notably single-residue ???-distributions, backbone-dependent side chain rotamer preferences, and tight packing criteria. One known unrealistic feature of these prediction programs is the assumption of a fixed ideal geometry for the backbone. The driving hypothesis behind this proposal is that there exists a largely unappreciated but real, systematic, significant and pervasive variation in backbone bond angles and peptide planarity that occurs as a function of backbone torsion angles, and accounting properly for this variation will be required to achieve X-ray crystal structure quality for comparative models. The overall goal of this work is to generate accurate empirical values for this covalent variation that will lead to tangible improvements in the accuracy of structures produced by comparative modeling and de novo structure prediction as well as by X-ray crystallography. We propose to achieve this overall goal by pursuing the following three specific aims: 1) to design, develop, and make available a flexibly-searchable database containing bond lengths, bond angles, and torsion angles for all structures known at better than 1.75 ¿ resolution (currently ~500,000 residues); 2) to use conventional query-based and modern machine learning approaches to derive accurate empirical information from the database about the systematic correlation of local conformation with variations in covalent geometry; and 3) to create a modular conformation-dependent expected covalent geometry library and to facilitate its incorporation into leading applications for comparative and crystallographic protein structure modeling. With the dramatically increased number of ultrahigh-resolution resolution crystal structures now known, the time is ripe for construction of this Protein Geometry Database that will provide facile access to a massive treasure trove of reliable and detailed empirical information about protein structure. To be done well, this work will require painstaking attention to detail and an intimate familiarity with the limitations of crystallographic refinement and the principles of protein structure. Dr. Karplus is well-suited to lead this work as he has a 20+-year track record of quality crystallographic structure determinations combined with contributions of more general insights into protein structure, among them being the pioneering characterization of the conformation-dependent variations in covalent geometry that serves as this project's foundation. Collaborations with world-leading groups in structure prediction, in crystallographic refinement and structure validation, and in knowledge-based library development ensure a rapid and effective translation of the gleaned information into improvements in protein modeling. PUBLIC HEALTH RELEVANCE: Proteins are responsible for carrying out most of the processes of life and their function depends exquisitely on their structure, even on the tiniest structural details. For this reason, determining accurate structures of proteins is a cornerstone of modern biomedical research. This work is aimed at leading to a universal improvement in the accuracy with which protein structure can be built.           n/a",Empirical conformation-dependent covalent geometry variation in proteins,7905142,R01GM083136,"['Accounting', 'Attention', 'Automobile Driving', 'Biomedical Research', 'Catalysis', 'Collaborations', 'Communities', 'Crystallography', 'Databases', 'Development', 'Ensure', 'Enzymes', 'Familiarity', 'Foundations', 'Glean', 'Goals', 'Heart', 'Homology Modeling', 'Investments', 'Knowledge', 'Lead', 'Length', 'Letters', 'Libraries', 'Life', 'Machine Learning', 'Methodology', 'Mining', 'Modeling', 'Molecular Conformation', 'Online Systems', 'Pattern', 'Peptides', 'Pharmaceutical Preparations', 'Process', 'Protein Conformation', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Roentgen Rays', 'Side', 'Structure', 'Technology', 'Time', 'Torsion', 'Translations', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vertebral column', 'Work', 'X-Ray Crystallography', 'base', 'comparative', 'cost', 'design', 'disease-causing mutation', 'falls', 'flexibility', 'inhibitor/antagonist', 'innovation', 'insight', 'knowledge base', 'molecular mechanics', 'novel', 'predictive modeling', 'preference', 'programs', 'protein structure', 'protein structure prediction', 'public health relevance', 'software development', 'structural biology', 'structural genomics']",NIGMS,OREGON STATE UNIVERSITY,R01,2010,210764,-0.02135690727665672
"Empirical conformation-dependent covalent geometry variation in proteins    DESCRIPTION (provided by applicant): A detailed and accurate understanding of the structure of proteins is one cornerstone of modern biomedical research, and an explicit goal of the NIH is to define the structure of all proteins either by accurate experimental determination or comparative model-building. The most successful structure prediction approaches employ empirical knowledge-based energy terms derived from features of known protein structures - most notably single-residue ???-distributions, backbone-dependent side chain rotamer preferences, and tight packing criteria. One known unrealistic feature of these prediction programs is the assumption of a fixed ideal geometry for the backbone. The driving hypothesis behind this proposal is that there exists a largely unappreciated but real, systematic, significant and pervasive variation in backbone bond angles and peptide planarity that occurs as a function of backbone torsion angles, and accounting properly for this variation will be required to achieve X-ray crystal structure quality for comparative models. The overall goal of this work is to generate accurate empirical values for this covalent variation that will lead to tangible improvements in the accuracy of structures produced by comparative modeling and de novo structure prediction as well as by X-ray crystallography. We propose to achieve this overall goal by pursuing the following three specific aims: 1) to design, develop, and make available a flexibly-searchable database containing bond lengths, bond angles, and torsion angles for all structures known at better than 1.75 ¿ resolution (currently ~500,000 residues); 2) to use conventional query-based and modern machine learning approaches to derive accurate empirical information from the database about the systematic correlation of local conformation with variations in covalent geometry; and 3) to create a modular conformation-dependent expected covalent geometry library and to facilitate its incorporation into leading applications for comparative and crystallographic protein structure modeling. With the dramatically increased number of ultrahigh-resolution resolution crystal structures now known, the time is ripe for construction of this Protein Geometry Database that will provide facile access to a massive treasure trove of reliable and detailed empirical information about protein structure. To be done well, this work will require painstaking attention to detail and an intimate familiarity with the limitations of crystallographic refinement and the principles of protein structure. Dr. Karplus is well-suited to lead this work as he has a 20+-year track record of quality crystallographic structure determinations combined with contributions of more general insights into protein structure, among them being the pioneering characterization of the conformation-dependent variations in covalent geometry that serves as this project's foundation. Collaborations with world-leading groups in structure prediction, in crystallographic refinement and structure validation, and in knowledge-based library development ensure a rapid and effective translation of the gleaned information into improvements in protein modeling. PUBLIC HEALTH RELEVANCE: Proteins are responsible for carrying out most of the processes of life and their function depends exquisitely on their structure, even on the tiniest structural details. For this reason, determining accurate structures of proteins is a cornerstone of modern biomedical research. This work is aimed at leading to a universal improvement in the accuracy with which protein structure can be built.           n/a",Empirical conformation-dependent covalent geometry variation in proteins,7656854,R01GM083136,"['Accounting', 'Attention', 'Automobile Driving', 'Biomedical Research', 'Catalysis', 'Classification', 'Collaborations', 'Communities', 'Crystallography', 'Databases', 'Development', 'Ensure', 'Enzymes', 'Familiarity', 'Foundations', 'Glean', 'Goals', 'Heart', 'Homology Modeling', 'Investments', 'Knowledge', 'Lead', 'Length', 'Letters', 'Libraries', 'Life', 'Machine Learning', 'Methodology', 'Mining', 'Modeling', 'Molecular Conformation', 'Online Systems', 'Pattern', 'Peptides', 'Pharmaceutical Preparations', 'Process', 'Protein Conformation', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Roentgen Rays', 'Side', 'Structure', 'Technology', 'Time', 'Torsion', 'Translations', 'United States National Institutes of Health', 'Validation', 'Variant', 'Vertebral column', 'Work', 'X-Ray Crystallography', 'base', 'comparative', 'cost', 'design', 'disease-causing mutation', 'falls', 'flexibility', 'inhibitor/antagonist', 'innovation', 'insight', 'knowledge base', 'molecular mechanics', 'novel', 'predictive modeling', 'preference', 'programs', 'protein structure', 'protein structure prediction', 'public health relevance', 'software development', 'structural biology', 'structural genomics']",NIGMS,OREGON STATE UNIVERSITY,R01,2009,286440,-0.02135690727665672
"Wrist/Grasp Muscle Activity Patterns for EMG-neural Interface Prosthesis Control DESCRIPTION (provided by applicant):    Simultaneous wrist/hand control (where a person simultaneously moves his/her wrist while grasping or  releasing), is essential in activities of daiy living, but cannot be performed by amputees using clinical surface  electromyography (sEMG) based neural interfaces (myoelectric control). Transradial amputees, who make up 40% of major amputations (proximal to the wrist), cite simultaneous control as a necessary element to improve upper-limb prostheses. Our long-term goal is to develop a clinically viable EMG-based neural interface for transradial amputees that is suitable for restoring simultaneous control of prosthetic wrist/hand movements.   One proposed method for intuitive simultaneous wrist/hand prosthesis control uses intramuscular EMG  (imEMG) amplitude estimates from multiple agonist/ antagonist forearm muscle pairs to control physiologically  appropriate degrees of freedom DOFs (e.g. flexor digitorum profundus / extensor digitorum control  grasp/release). Known as 'direct control', this approach does not require burdensome levels of machine  learning algorithm training, as is required for experimental sEMG-based simultaneous control methods.  However, this method assumes independently modulated muscle activation patterns for each DOF - that the activation patterns of muscles controlling a DOF (i.e. finger flexors/extensors for grasp/release) do not change when a second DOF is simultaneously attempted (simultaneous pronation/supination and grasp/release). The biologic foundation of such an assumption has not been explored to date. Little is known of how the central nervous system (CNS) coordinates wrist and extrinsic hand muscle activation when independent of biomechanical effects and joint position feedback. Muscle activation patterns under such conditions are particularly relevant to amputee populations, who lack distal joints. Therefore, the objective of this research is to better understand how the CNS coordinates muscle activation patterns in healthy individuals to produce simultaneous wrist/grasp torques when independent of the biomechanical effects of joint movement. We will then apply this information to develop an imEMG simultaneous myoelectric prosthesis controller.  imEMG patterns will be measured as healthy subjects produce single-DOF and simultaneous multi-DOF (SMD) wrist and/or grasp/release torques in isometric, neutral posture conditions. Multivariate models predicting imEMG activity from measured wrist/grasp torques will be developed and will suggest which DOFs have independently modulated muscle activation patterns. This information will then allow for the development of a 'biologically-inspired' simultaneous controller. DOFs with independently modulated muscle activation patterns will be controlled using direct control, while experimental pattern recognition algorithms currently used for sEMG-based simultaneous control will be used for all other DOFs. We will evaluate the performance of this hybrid controller in healthy subjects both offline and using online functional tests in a virtual environment. PUBLIC HEALTH RELEVANCE:    Various current prosthetic arms use the electrical signals produced when muscles contract to control prosthesis movement, but cannot provide simultaneous control of both the wrist and the hand. This study will investigate how the nervous system activates muscles to simultaneously control the wrist and hand in healthy individuals. This information can then be used to provide clinically viable methods of controlling simultaneous wrist and hand movements of prostheses to amputees.",Wrist/Grasp Muscle Activity Patterns for EMG-neural Interface Prosthesis Control,8806616,F31NS083166,"['Activities of Daily Living', 'Agonist', 'Algorithms', 'Amputation', 'Amputees', 'Anatomy', 'Artificial Arm', 'Biological', 'Biomechanics', 'Clinical', 'Contracts', 'Data', 'Development', 'Distal', 'Educational Status', 'Electromyography', 'Elements', 'Environment', 'Feedback', 'Fingers', 'Flexor', 'Forearm', 'Foundations', 'Freedom', 'Goals', 'Hand', 'Health', 'Human', 'Hybrids', 'Individual', 'Intramuscular', 'Isometric Exercise', 'Joints', 'Knowledge', 'Life', 'Limb Prosthesis', 'Literature', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Movement', 'Muscle', 'Muscle Contraction', 'Myoelectric prosthesis', 'Nervous system structure', 'Neuraxis', 'Outcome', 'Pattern', 'Pattern Recognition', 'Performance', 'Persons', 'Population', 'Positioning Attribute', 'Posture', 'Pronation', 'Prosthesis', 'Rehabilitation therapy', 'Research', 'Signal Transduction', 'Supination', 'Surface', 'Testing', 'Torque', 'Training', 'Upper Extremity', 'Work', 'Wrist', 'arm', 'base', 'design', 'extensor digitorum', 'grasp', 'improved', 'joint mobilization', 'myoelectric control', 'prosthesis control', 'prosthetic hand', 'relating to nervous system', 'residual limb', 'success', 'virtual']",NINDS,NORTHWESTERN UNIVERSITY,F31,2015,6800,-0.042490034169130095
"Wrist/Grasp Muscle Activity Patterns for EMG-neural Interface Prosthesis Control     DESCRIPTION (provided by applicant):    Simultaneous wrist/hand control (where a person simultaneously moves his/her wrist while grasping or  releasing), is essential in activities of daiy living, but cannot be performed by amputees using clinical surface  electromyography (sEMG) based neural interfaces (myoelectric control). Transradial amputees, who make up 40% of major amputations (proximal to the wrist), cite simultaneous control as a necessary element to improve upper-limb prostheses. Our long-term goal is to develop a clinically viable EMG-based neural interface for transradial amputees that is suitable for restoring simultaneous control of prosthetic wrist/hand movements.   One proposed method for intuitive simultaneous wrist/hand prosthesis control uses intramuscular EMG  (imEMG) amplitude estimates from multiple agonist/ antagonist forearm muscle pairs to control physiologically  appropriate degrees of freedom DOFs (e.g. flexor digitorum profundus / extensor digitorum control  grasp/release). Known as 'direct control', this approach does not require burdensome levels of machine  learning algorithm training, as is required for experimental sEMG-based simultaneous control methods.  However, this method assumes independently modulated muscle activation patterns for each DOF - that the activation patterns of muscles controlling a DOF (i.e. finger flexors/extensors for grasp/release) do not change when a second DOF is simultaneously attempted (simultaneous pronation/supination and grasp/release). The biologic foundation of such an assumption has not been explored to date. Little is known of how the central nervous system (CNS) coordinates wrist and extrinsic hand muscle activation when independent of biomechanical effects and joint position feedback. Muscle activation patterns under such conditions are particularly relevant to amputee populations, who lack distal joints. Therefore, the objective of this research is to better understand how the CNS coordinates muscle activation patterns in healthy individuals to produce simultaneous wrist/grasp torques when independent of the biomechanical effects of joint movement. We will then apply this information to develop an imEMG simultaneous myoelectric prosthesis controller.  imEMG patterns will be measured as healthy subjects produce single-DOF and simultaneous multi-DOF (SMD) wrist and/or grasp/release torques in isometric, neutral posture conditions. Multivariate models predicting imEMG activity from measured wrist/grasp torques will be developed and will suggest which DOFs have independently modulated muscle activation patterns. This information will then allow for the development of a 'biologically-inspired' simultaneous controller. DOFs with independently modulated muscle activation patterns will be controlled using direct control, while experimental pattern recognition algorithms currently used for sEMG-based simultaneous control will be used for all other DOFs. We will evaluate the performance of this hybrid controller in healthy subjects both offline and using online functional tests in a virtual environment.            PUBLIC HEALTH RELEVANCE:    Various current prosthetic arms use the electrical signals produced when muscles contract to control prosthesis movement, but cannot provide simultaneous control of both the wrist and the hand. This study will investigate how the nervous system activates muscles to simultaneously control the wrist and hand in healthy individuals. This information can then be used to provide clinically viable methods of controlling simultaneous wrist and hand movements of prostheses to amputees.                 ",Wrist/Grasp Muscle Activity Patterns for EMG-neural Interface Prosthesis Control,8702940,F31NS083166,"['Activities of Daily Living', 'Agonist', 'Algorithms', 'Amputation', 'Amputees', 'Anatomy', 'Biological', 'Biomechanics', 'Clinical', 'Contracts', 'Data', 'Development', 'Distal', 'Educational Status', 'Electromyography', 'Elements', 'Environment', 'Feedback', 'Fingers', 'Flexor', 'Forearm', 'Foundations', 'Freedom', 'Goals', 'Hand', 'Human', 'Hybrids', 'Individual', 'Intramuscular', 'Isometric Exercise', 'Joints', 'Knowledge', 'Life', 'Limb Prosthesis', 'Limb structure', 'Literature', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Movement', 'Muscle', 'Muscle Contraction', 'Myoelectric prosthesis', 'Nervous system structure', 'Neuraxis', 'Outcome', 'Pattern', 'Pattern Recognition', 'Performance', 'Persons', 'Population', 'Positioning Attribute', 'Posture', 'Pronation', 'Prosthesis', 'Rehabilitation therapy', 'Research', 'Residual state', 'Signal Transduction', 'Supination', 'Surface', 'Testing', 'Torque', 'Training', 'Upper Extremity', 'Work', 'Wrist', 'arm', 'base', 'design', 'extensor digitorum', 'grasp', 'improved', 'joint mobilization', 'public health relevance', 'relating to nervous system', 'success', 'virtual']",NINDS,NORTHWESTERN UNIVERSITY,F31,2014,47676,-0.042490034169130095
"Wrist/Grasp Muscle Activity Patterns for EMG-neural Interface Prosthesis Control     DESCRIPTION (provided by applicant):    Simultaneous wrist/hand control (where a person simultaneously moves his/her wrist while grasping or  releasing), is essential in activities of daiy living, but cannot be performed by amputees using clinical surface  electromyography (sEMG) based neural interfaces (myoelectric control). Transradial amputees, who make up 40% of major amputations (proximal to the wrist), cite simultaneous control as a necessary element to improve upper-limb prostheses. Our long-term goal is to develop a clinically viable EMG-based neural interface for transradial amputees that is suitable for restoring simultaneous control of prosthetic wrist/hand movements.   One proposed method for intuitive simultaneous wrist/hand prosthesis control uses intramuscular EMG  (imEMG) amplitude estimates from multiple agonist/ antagonist forearm muscle pairs to control physiologically  appropriate degrees of freedom DOFs (e.g. flexor digitorum profundus / extensor digitorum control  grasp/release). Known as 'direct control', this approach does not require burdensome levels of machine  learning algorithm training, as is required for experimental sEMG-based simultaneous control methods.  However, this method assumes independently modulated muscle activation patterns for each DOF - that the activation patterns of muscles controlling a DOF (i.e. finger flexors/extensors for grasp/release) do not change when a second DOF is simultaneously attempted (simultaneous pronation/supination and grasp/release). The biologic foundation of such an assumption has not been explored to date. Little is known of how the central nervous system (CNS) coordinates wrist and extrinsic hand muscle activation when independent of biomechanical effects and joint position feedback. Muscle activation patterns under such conditions are particularly relevant to amputee populations, who lack distal joints. Therefore, the objective of this research is to better understand how the CNS coordinates muscle activation patterns in healthy individuals to produce simultaneous wrist/grasp torques when independent of the biomechanical effects of joint movement. We will then apply this information to develop an imEMG simultaneous myoelectric prosthesis controller.  imEMG patterns will be measured as healthy subjects produce single-DOF and simultaneous multi-DOF (SMD) wrist and/or grasp/release torques in isometric, neutral posture conditions. Multivariate models predicting imEMG activity from measured wrist/grasp torques will be developed and will suggest which DOFs have independently modulated muscle activation patterns. This information will then allow for the development of a 'biologically-inspired' simultaneous controller. DOFs with independently modulated muscle activation patterns will be controlled using direct control, while experimental pattern recognition algorithms currently used for sEMG-based simultaneous control will be used for all other DOFs. We will evaluate the performance of this hybrid controller in healthy subjects both offline and using online functional tests in a virtual environment.            PUBLIC HEALTH RELEVANCE:    Various current prosthetic arms use the electrical signals produced when muscles contract to control prosthesis movement, but cannot provide simultaneous control of both the wrist and the hand. This study will investigate how the nervous system activates muscles to simultaneously control the wrist and hand in healthy individuals. This information can then be used to provide clinically viable methods of controlling simultaneous wrist and hand movements of prostheses to amputees.                 ",Wrist/Grasp Muscle Activity Patterns for EMG-neural Interface Prosthesis Control,8526724,F31NS083166,"['Activities of Daily Living', 'Agonist', 'Algorithms', 'Amputation', 'Amputees', 'Anatomy', 'Biological', 'Biomechanics', 'Clinical', 'Contracts', 'Data', 'Development', 'Distal', 'Educational Status', 'Electromyography', 'Elements', 'Environment', 'Feedback', 'Fingers', 'Flexor', 'Forearm', 'Foundations', 'Freedom', 'Goals', 'Hand', 'Human', 'Hybrids', 'Individual', 'Intramuscular', 'Isometric Exercise', 'Joints', 'Knowledge', 'Life', 'Limb Prosthesis', 'Limb structure', 'Literature', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Movement', 'Muscle', 'Muscle Contraction', 'Myoelectric prosthesis', 'Nervous system structure', 'Neuraxis', 'Outcome', 'Pattern', 'Pattern Recognition', 'Performance', 'Persons', 'Population', 'Positioning Attribute', 'Posture', 'Pronation', 'Prosthesis', 'Rehabilitation therapy', 'Research', 'Residual state', 'Signal Transduction', 'Supination', 'Surface', 'Testing', 'Torque', 'Training', 'Upper Extremity', 'Work', 'Wrist', 'arm', 'base', 'design', 'extensor digitorum', 'grasp', 'improved', 'joint mobilization', 'public health relevance', 'relating to nervous system', 'success', 'virtual']",NINDS,NORTHWESTERN UNIVERSITY,F31,2013,47232,-0.042490034169130095
"Methods for Dynamic Causal Interactions in Human Brain Function and Dysfunction DESCRIPTION (provided by applicant): In the past two decades, functional magnetic resonance imaging (fMRI) has emerged as a powerful tool for investigating human brain function. Although fMRI research has primarily focused on identifying brain regions that are activated during performance of cognitive tasks, there is growing interest in examining how cognitive functions emerge as a result of context-dependent, dynamic causal interactions between distributed brain regions. Devising and validating methods for investigating such interactions has therefore taken on great significance. The first major goal of this proposal is to address a critical need in fMRI research by developing novel algorithms for identifying context-dependent dynamic causal interactions between distributed brain regions. To this end, we will develop and validate novel computational methods using Multivariate Dynamical Systems based Markov chain Monte Carlo (MDS-MCMC) algorithms that overcome major limitations of existing methods for investigating dynamic causal interactions and connectivity in the human brain. A comprehensive validation framework will be use evaluate MDS-MCMC and compare it with existing dynamic causal estimation methods. The second major goal of this proposal is to use the MDS-MCMC framework to investigate dynamic causal interactions underlying cognition in normal healthy adults, and in patients with Parkinson's disease (PD). Cognitive impairment is one of the most devastating symptoms in PD. Once thought of as an insignificant feature of the disease, it is now clear that cognitive impairment is present in the majority of PD patients and that this impairment is significantly linked to increased disability and the risk of mortality, yetlittle is known about the brain basis of cognitive impairment in PD. The computational algorithms we develop, validate, and apply here will allow us to rigorously investigate brain dynamics support critical cognitive processes in the human brain, leading to a more complete understanding of fundamental mechanisms underlying human brain function and dysfunction. Our proposed studies will also, for the first time, examine casual interactions in simulated, open-source, opto-genetic, experimental and clinical brain imaging data using state-of-the-art sub-second high-temporal resolution fMRI, based on the Human Connectome Project (HCP). Critically, we will maintain a tight link between our computational and systems neuroscience goals algorithms to solve important problems in cognitive, systems and clinical neuroscience. Together, our proposed studies will lead to new and improved computational tools for examining dynamical causal interactions between distributed brain regions, with broad applications to the HCP and clinical neuroscience. The proposed studies are highly relevant to the mission of the NIH Innovations in Biomedical Computational Science and Technology and the Big Data to Knowledge Programs, which seek to encourage development and dissemination of innovative advanced computational tools for brain imaging and neuroscience. We will disseminate our algorithms and software to the research community via NITRC . PUBLIC HEALTH RELEVANCE: In the past two decades, functional magnetic resonance imaging (fMRI) has emerged as a powerful tool for investigating human brain function and dysfunction. Although fMRI studies of brain function have primarily focused on identifying brain regions that are activated during performance of perceptual or cognitive tasks, there is growing consensus that cognitive functions emerge as a result of dynamic context-dependent interactions between multiple brain areas. Developing new computational methods for investigating causal interactions in fMRI data has therefore taken added significance; the overall goal of this proposal is to address this critical need by developing new methods for studying causal interactions and brain connectivity between distributed brain regions during cognition.",Methods for Dynamic Causal Interactions in Human Brain Function and Dysfunction,9521866,R01NS086085,"['Address', 'Adult', 'Algorithmic Software', 'Algorithms', 'Area', 'Basal Ganglia', 'Benchmarking', 'Big Data to Knowledge', 'Biomedical Computing', 'Brain', 'Brain imaging', 'Brain region', 'Clinical', 'Cognition', 'Cognitive deficits', 'Communities', 'Computational Science', 'Computational algorithm', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Development', 'Disease', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Genetic', 'Goals', 'Human', 'Imaging Techniques', 'Impaired cognition', 'Impairment', 'Individual Differences', 'Link', 'Machine Learning', 'Markov Chains', 'Markov chain Monte Carlo methodology', 'Mediating', 'Methods', 'Mission', 'Modeling', 'Neurosciences', 'Parietal', 'Parkinson Disease', 'Participant', 'Patients', 'Performance', 'Play', 'Prefrontal Cortex', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Resource Sharing', 'Risk', 'Role', 'Short-Term Memory', 'Software Tools', 'Symptoms', 'System', 'Technology', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'brain dysfunction', 'cognitive function', 'cognitive load', 'cognitive performance', 'cognitive process', 'cognitive system', 'cognitive task', 'computerized tools', 'connectome', 'disability', 'dynamic system', 'imaging study', 'improved', 'in vivo', 'innovation', 'interest', 'mortality', 'neurophysiology', 'novel', 'open data', 'open source', 'optogenetics', 'programs', 'public health relevance', 'simulation', 'temporal measurement', 'tool']",NINDS,STANFORD UNIVERSITY,R01,2018,554411,-0.02004278722241571
"Methods for Dynamic Causal Interactions in Human Brain Function and Dysfunction DESCRIPTION (provided by applicant): In the past two decades, functional magnetic resonance imaging (fMRI) has emerged as a powerful tool for investigating human brain function. Although fMRI research has primarily focused on identifying brain regions that are activated during performance of cognitive tasks, there is growing interest in examining how cognitive functions emerge as a result of context-dependent, dynamic causal interactions between distributed brain regions. Devising and validating methods for investigating such interactions has therefore taken on great significance. The first major goal of this proposal is to address a critical need in fMRI research by developing novel algorithms for identifying context-dependent dynamic causal interactions between distributed brain regions. To this end, we will develop and validate novel computational methods using Multivariate Dynamical Systems based Markov chain Monte Carlo (MDS-MCMC) algorithms that overcome major limitations of existing methods for investigating dynamic causal interactions and connectivity in the human brain. A comprehensive validation framework will be use evaluate MDS-MCMC and compare it with existing dynamic causal estimation methods. The second major goal of this proposal is to use the MDS-MCMC framework to investigate dynamic causal interactions underlying cognition in normal healthy adults, and in patients with Parkinson's disease (PD). Cognitive impairment is one of the most devastating symptoms in PD. Once thought of as an insignificant feature of the disease, it is now clear that cognitive impairment is present in the majority of PD patients and that this impairment is significantly linked to increased disability and the risk of mortality, yetlittle is known about the brain basis of cognitive impairment in PD. The computational algorithms we develop, validate, and apply here will allow us to rigorously investigate brain dynamics support critical cognitive processes in the human brain, leading to a more complete understanding of fundamental mechanisms underlying human brain function and dysfunction. Our proposed studies will also, for the first time, examine casual interactions in simulated, open-source, opto-genetic, experimental and clinical brain imaging data using state-of-the-art sub-second high-temporal resolution fMRI, based on the Human Connectome Project (HCP). Critically, we will maintain a tight link between our computational and systems neuroscience goals algorithms to solve important problems in cognitive, systems and clinical neuroscience. Together, our proposed studies will lead to new and improved computational tools for examining dynamical causal interactions between distributed brain regions, with broad applications to the HCP and clinical neuroscience. The proposed studies are highly relevant to the mission of the NIH Innovations in Biomedical Computational Science and Technology and the Big Data to Knowledge Programs, which seek to encourage development and dissemination of innovative advanced computational tools for brain imaging and neuroscience. We will disseminate our algorithms and software to the research community via NITRC . PUBLIC HEALTH RELEVANCE: In the past two decades, functional magnetic resonance imaging (fMRI) has emerged as a powerful tool for investigating human brain function and dysfunction. Although fMRI studies of brain function have primarily focused on identifying brain regions that are activated during performance of perceptual or cognitive tasks, there is growing consensus that cognitive functions emerge as a result of dynamic context-dependent interactions between multiple brain areas. Developing new computational methods for investigating causal interactions in fMRI data has therefore taken added significance; the overall goal of this proposal is to address this critical need by developing new methods for studying causal interactions and brain connectivity between distributed brain regions during cognition.",Methods for Dynamic Causal Interactions in Human Brain Function and Dysfunction,9301657,R01NS086085,"['Address', 'Adult', 'Algorithmic Software', 'Algorithms', 'Area', 'Basal Ganglia', 'Benchmarking', 'Big Data to Knowledge', 'Biomedical Computing', 'Brain', 'Brain imaging', 'Brain region', 'Clinical', 'Cognition', 'Cognitive deficits', 'Communities', 'Computational Science', 'Computational algorithm', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Development', 'Disease', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Genetic', 'Goals', 'Human', 'Imaging Techniques', 'Impaired cognition', 'Impairment', 'Individual Differences', 'Link', 'Machine Learning', 'Markov Chains', 'Markov chain Monte Carlo methodology', 'Mediating', 'Methods', 'Mission', 'Modeling', 'Neurosciences', 'Parietal', 'Parkinson Disease', 'Participant', 'Patients', 'Performance', 'Play', 'Prefrontal Cortex', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Resource Sharing', 'Risk', 'Role', 'Short-Term Memory', 'Software Tools', 'Symptoms', 'System', 'Task Performances', 'Technology', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'brain dysfunction', 'cognitive function', 'cognitive load', 'cognitive performance', 'cognitive process', 'cognitive system', 'cognitive task', 'computerized tools', 'connectome', 'disability', 'dynamic system', 'imaging study', 'improved', 'in vivo', 'innovation', 'interest', 'mortality', 'neurophysiology', 'novel', 'open data', 'open source', 'optogenetics', 'programs', 'public health relevance', 'simulation', 'temporal measurement', 'tool']",NINDS,STANFORD UNIVERSITY,R01,2017,554411,-0.02004278722241571
"Methods for Dynamic Causal Interactions in Human Brain Function and Dysfunction DESCRIPTION (provided by applicant): In the past two decades, functional magnetic resonance imaging (fMRI) has emerged as a powerful tool for investigating human brain function. Although fMRI research has primarily focused on identifying brain regions that are activated during performance of cognitive tasks, there is growing interest in examining how cognitive functions emerge as a result of context-dependent, dynamic causal interactions between distributed brain regions. Devising and validating methods for investigating such interactions has therefore taken on great significance. The first major goal of this proposal is to address a critical need in fMRI research by developing novel algorithms for identifying context-dependent dynamic causal interactions between distributed brain regions. To this end, we will develop and validate novel computational methods using Multivariate Dynamical Systems based Markov chain Monte Carlo (MDS-MCMC) algorithms that overcome major limitations of existing methods for investigating dynamic causal interactions and connectivity in the human brain. A comprehensive validation framework will be use evaluate MDS-MCMC and compare it with existing dynamic causal estimation methods. The second major goal of this proposal is to use the MDS-MCMC framework to investigate dynamic causal interactions underlying cognition in normal healthy adults, and in patients with Parkinson's disease (PD). Cognitive impairment is one of the most devastating symptoms in PD. Once thought of as an insignificant feature of the disease, it is now clear that cognitive impairment is present in the majority of PD patients and that this impairment is significantly linked to increased disability and the risk of mortality, yetlittle is known about the brain basis of cognitive impairment in PD. The computational algorithms we develop, validate, and apply here will allow us to rigorously investigate brain dynamics support critical cognitive processes in the human brain, leading to a more complete understanding of fundamental mechanisms underlying human brain function and dysfunction. Our proposed studies will also, for the first time, examine casual interactions in simulated, open-source, opto-genetic, experimental and clinical brain imaging data using state-of-the-art sub-second high-temporal resolution fMRI, based on the Human Connectome Project (HCP). Critically, we will maintain a tight link between our computational and systems neuroscience goals algorithms to solve important problems in cognitive, systems and clinical neuroscience. Together, our proposed studies will lead to new and improved computational tools for examining dynamical causal interactions between distributed brain regions, with broad applications to the HCP and clinical neuroscience. The proposed studies are highly relevant to the mission of the NIH Innovations in Biomedical Computational Science and Technology and the Big Data to Knowledge Programs, which seek to encourage development and dissemination of innovative advanced computational tools for brain imaging and neuroscience. We will disseminate our algorithms and software to the research community via NITRC . PUBLIC HEALTH RELEVANCE: In the past two decades, functional magnetic resonance imaging (fMRI) has emerged as a powerful tool for investigating human brain function and dysfunction. Although fMRI studies of brain function have primarily focused on identifying brain regions that are activated during performance of perceptual or cognitive tasks, there is growing consensus that cognitive functions emerge as a result of dynamic context-dependent interactions between multiple brain areas. Developing new computational methods for investigating causal interactions in fMRI data has therefore taken added significance; the overall goal of this proposal is to address this critical need by developing new methods for studying causal interactions and brain connectivity between distributed brain regions during cognition.",Methods for Dynamic Causal Interactions in Human Brain Function and Dysfunction,9086441,R01NS086085,"['Address', 'Adult', 'Algorithmic Software', 'Algorithms', 'Area', 'Basal Ganglia', 'Base of the Brain', 'Benchmarking', 'Big Data to Knowledge', 'Brain', 'Brain imaging', 'Brain region', 'Clinical', 'Cognition', 'Cognitive deficits', 'Communities', 'Computational Science', 'Computational algorithm', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Development', 'Disease', 'Experimental Genetics', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Goals', 'Human', 'Impaired cognition', 'Impairment', 'Individual Differences', 'Lead', 'Link', 'Machine Learning', 'Markov Chains', 'Markov chain Monte Carlo methodology', 'Mediating', 'Methods', 'Mission', 'Modeling', 'Neurosciences', 'Parietal', 'Parkinson Disease', 'Participant', 'Patients', 'Performance', 'Play', 'Prefrontal Cortex', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Resource Sharing', 'Risk', 'Role', 'Short-Term Memory', 'Software Tools', 'Symptoms', 'System', 'Techniques', 'Technology', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'brain dysfunction', 'cognitive function', 'cognitive load', 'cognitive process', 'cognitive system', 'cognitive task', 'computerized tools', 'connectome', 'disability', 'dynamic system', 'improved', 'in vivo', 'innovation', 'interest', 'mortality', 'novel', 'open data', 'open source', 'optogenetics', 'programs', 'public health relevance', 'simulation', 'temporal measurement', 'tool']",NINDS,STANFORD UNIVERSITY,R01,2016,554411,-0.02004278722241571
"Methods for Dynamic Causal Interactions in Human Brain Function and Dysfunction DESCRIPTION (provided by applicant): In the past two decades, functional magnetic resonance imaging (fMRI) has emerged as a powerful tool for investigating human brain function. Although fMRI research has primarily focused on identifying brain regions that are activated during performance of cognitive tasks, there is growing interest in examining how cognitive functions emerge as a result of context-dependent, dynamic causal interactions between distributed brain regions. Devising and validating methods for investigating such interactions has therefore taken on great significance. The first major goal of this proposal is to address a critical need in fMRI research by developing novel algorithms for identifying context-dependent dynamic causal interactions between distributed brain regions. To this end, we will develop and validate novel computational methods using Multivariate Dynamical Systems based Markov chain Monte Carlo (MDS-MCMC) algorithms that overcome major limitations of existing methods for investigating dynamic causal interactions and connectivity in the human brain. A comprehensive validation framework will be use evaluate MDS-MCMC and compare it with existing dynamic causal estimation methods. The second major goal of this proposal is to use the MDS-MCMC framework to investigate dynamic causal interactions underlying cognition in normal healthy adults, and in patients with Parkinson's disease (PD). Cognitive impairment is one of the most devastating symptoms in PD. Once thought of as an insignificant feature of the disease, it is now clear that cognitive impairment is present in the majority of PD patients and that this impairment is significantly linked to increased disability and the risk of mortality, yetlittle is known about the brain basis of cognitive impairment in PD. The computational algorithms we develop, validate, and apply here will allow us to rigorously investigate brain dynamics support critical cognitive processes in the human brain, leading to a more complete understanding of fundamental mechanisms underlying human brain function and dysfunction. Our proposed studies will also, for the first time, examine casual interactions in simulated, open-source, opto-genetic, experimental and clinical brain imaging data using state-of-the-art sub-second high-temporal resolution fMRI, based on the Human Connectome Project (HCP). Critically, we will maintain a tight link between our computational and systems neuroscience goals algorithms to solve important problems in cognitive, systems and clinical neuroscience. Together, our proposed studies will lead to new and improved computational tools for examining dynamical causal interactions between distributed brain regions, with broad applications to the HCP and clinical neuroscience. The proposed studies are highly relevant to the mission of the NIH Innovations in Biomedical Computational Science and Technology and the Big Data to Knowledge Programs, which seek to encourage development and dissemination of innovative advanced computational tools for brain imaging and neuroscience. We will disseminate our algorithms and software to the research community via NITRC . PUBLIC HEALTH RELEVANCE: In the past two decades, functional magnetic resonance imaging (fMRI) has emerged as a powerful tool for investigating human brain function and dysfunction. Although fMRI studies of brain function have primarily focused on identifying brain regions that are activated during performance of perceptual or cognitive tasks, there is growing consensus that cognitive functions emerge as a result of dynamic context-dependent interactions between multiple brain areas. Developing new computational methods for investigating causal interactions in fMRI data has therefore taken added significance; the overall goal of this proposal is to address this critical need by developing new methods for studying causal interactions and brain connectivity between distributed brain regions during cognition.",Methods for Dynamic Causal Interactions in Human Brain Function and Dysfunction,8866489,R01NS086085,"['Address', 'Adult', 'Algorithmic Software', 'Algorithms', 'Area', 'Basal Ganglia', 'Base of the Brain', 'Benchmarking', 'Big Data', 'Brain', 'Brain imaging', 'Brain region', 'Clinical', 'Cognition', 'Cognitive deficits', 'Communities', 'Computational Science', 'Computational algorithm', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Development', 'Disease', 'Experimental Genetics', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Goals', 'Human', 'Impaired cognition', 'Impairment', 'Individual Differences', 'Knowledge', 'Lead', 'Link', 'Machine Learning', 'Markov Chains', 'Markov chain Monte Carlo methodology', 'Mediating', 'Methods', 'Mission', 'Modeling', 'Neurosciences', 'Parietal', 'Parkinson Disease', 'Participant', 'Patients', 'Performance', 'Play', 'Prefrontal Cortex', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Resource Sharing', 'Risk', 'Role', 'Short-Term Memory', 'Software Tools', 'Symptoms', 'System', 'Techniques', 'Technology', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'cognitive function', 'cognitive load', 'cognitive process', 'cognitive system', 'cognitive task', 'computerized tools', 'disability', 'improved', 'in vivo', 'innovation', 'interest', 'mortality', 'novel', 'open source', 'optogenetics', 'programs', 'public health relevance', 'simulation', 'temporal measurement', 'tool']",NINDS,STANFORD UNIVERSITY,R01,2015,507610,-0.02004278722241571
"Methods for Dynamic Causal Interactions in Human Brain Function and Dysfunction     DESCRIPTION (provided by applicant): In the past two decades, functional magnetic resonance imaging (fMRI) has emerged as a powerful tool for investigating human brain function. Although fMRI research has primarily focused on identifying brain regions that are activated during performance of cognitive tasks, there is growing interest in examining how cognitive functions emerge as a result of context-dependent, dynamic causal interactions between distributed brain regions. Devising and validating methods for investigating such interactions has therefore taken on great significance. The first major goal of this proposal is to address a critical need in fMRI research by developing novel algorithms for identifying context-dependent dynamic causal interactions between distributed brain regions. To this end, we will develop and validate novel computational methods using Multivariate Dynamical Systems based Markov chain Monte Carlo (MDS-MCMC) algorithms that overcome major limitations of existing methods for investigating dynamic causal interactions and connectivity in the human brain. A comprehensive validation framework will be use evaluate MDS-MCMC and compare it with existing dynamic causal estimation methods. The second major goal of this proposal is to use the MDS-MCMC framework to investigate dynamic causal interactions underlying cognition in normal healthy adults, and in patients with Parkinson's disease (PD). Cognitive impairment is one of the most devastating symptoms in PD. Once thought of as an insignificant feature of the disease, it is now clear that cognitive impairment is present in the majority of PD patients and that this impairment is significantly linked to increased disability and the risk of mortality, yetlittle is known about the brain basis of cognitive impairment in PD. The computational algorithms we develop, validate, and apply here will allow us to rigorously investigate brain dynamics support critical cognitive processes in the human brain, leading to a more complete understanding of fundamental mechanisms underlying human brain function and dysfunction. Our proposed studies will also, for the first time, examine casual interactions in simulated, open-source, opto-genetic, experimental and clinical brain imaging data using state-of-the-art sub-second high-temporal resolution fMRI, based on the Human Connectome Project (HCP). Critically, we will maintain a tight link between our computational and systems neuroscience goals algorithms to solve important problems in cognitive, systems and clinical neuroscience. Together, our proposed studies will lead to new and improved computational tools for examining dynamical causal interactions between distributed brain regions, with broad applications to the HCP and clinical neuroscience. The proposed studies are highly relevant to the mission of the NIH Innovations in Biomedical Computational Science and Technology and the Big Data to Knowledge Programs, which seek to encourage development and dissemination of innovative advanced computational tools for brain imaging and neuroscience. We will disseminate our algorithms and software to the research community via NITRC .         PUBLIC HEALTH RELEVANCE: In the past two decades, functional magnetic resonance imaging (fMRI) has emerged as a powerful tool for investigating human brain function and dysfunction. Although fMRI studies of brain function have primarily focused on identifying brain regions that are activated during performance of perceptual or cognitive tasks, there is growing consensus that cognitive functions emerge as a result of dynamic context-dependent interactions between multiple brain areas. Developing new computational methods for investigating causal interactions in fMRI data has therefore taken added significance; the overall goal of this proposal is to address this critical need by developing new methods for studying causal interactions and brain connectivity between distributed brain regions during cognition.                ",Methods for Dynamic Causal Interactions in Human Brain Function and Dysfunction,8774725,R01NS086085,"['Address', 'Adult', 'Algorithmic Software', 'Algorithms', 'Area', 'Basal Ganglia', 'Base of the Brain', 'Benchmarking', 'Big Data', 'Brain', 'Brain imaging', 'Brain region', 'Clinical', 'Cognition', 'Cognitive', 'Cognitive deficits', 'Communities', 'Computational Science', 'Computational algorithm', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Development', 'Disease', 'Experimental Genetics', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Goals', 'Human', 'Impaired cognition', 'Impairment', 'Individual Differences', 'Knowledge', 'Lead', 'Link', 'Machine Learning', 'Markov Chains', 'Markov chain Monte Carlo methodology', 'Mediating', 'Methods', 'Mission', 'Modeling', 'Neurosciences', 'Parietal', 'Parkinson Disease', 'Participant', 'Patients', 'Performance', 'Play', 'Prefrontal Cortex', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Resolution', 'Resource Sharing', 'Risk', 'Role', 'Short-Term Memory', 'Simulate', 'Software Tools', 'Symptoms', 'System', 'Techniques', 'Technology', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'cognitive function', 'cognitive system', 'computerized tools', 'disability', 'improved', 'in vivo', 'innovation', 'interest', 'mortality', 'novel', 'open source', 'optogenetics', 'programs', 'public health relevance', 'simulation', 'tool']",NINDS,STANFORD UNIVERSITY,R01,2014,449410,-0.02004278722241571
"Structural Dynamics of Biomolecular Systems Many proteins function as molecular machines. Understanding the principles that control the machinery of biomolecular systems is a computational challenge in many cases due to the involvement of macromolecular structures composed of multiple subunits and cooperative interactions manifested by allosteric changes in conformations, which are beyond the range of atomic simulations. Our goal in a recently funded R33 has been to develop and utilize low resolution models for exploring the collective dynamics of such complex systems, and bridging between structure and function, based on the paradigm structure-encodes-dynamics-encodes-function. The elastic network models and methods we introduced to this aim have found utility in many applications and helped us gain insights into the intrinsic, structure- encoded ability of proteins to favor the reconfiguration of native structures between functional substates. In the present R01, we are proposing to build on our previous work, to further explore the structure -> dynamics -> function mapping of allosteric and/or multimeric proteins using physically-based and computationally efficient models in collaboration with the NCBC Simbios at Stanford U (PI: Altman). The Simbios group has already started to construct a new simulation package, Simbody, the utility of which is expected to be significantly enhanced by a collaborative work. Our specific aims are (1) to build models and methods for automated coarse-graining of complex structures at multiple levels of resolution and assessing their collective dynamics, toward using the resulting models (structure) and data (motions) in Simbody; (2) to complement the physics-based approach developed in Aim 1 by information-theoretic approaches toward delineating signal transduction pathways/mechanisms in allosteric systems, and establishing the connection between these pathways and structural dynamics, and (3) to gain insights into the machinery of molecular chaperones, using as prototypes the bacterial chaperonin GroEL-GroES and the DnaK chaperone system, in collaboration with the Gierasch lab currently doing NIH-supported experiments for understanding the allosteric dynamics of the DnaK system. An important outcome of this project will be the establishment of a methodology for simulating the machinery of biomolecular systems on the order of Megadaltons, which will be achieved in collaboration with the Schulten lab, in addition to our partnership with the Simbios team. Project Narrative  Many proteins function as molecular machines. Understanding the dynamics or underlying molecular principles of these machines is a challenge due to the highly cooperative nature of interactions, which simultaneously involve multiple molecules. The aim of this project is twofold: to develop and implement computational models and methods to improve our understanding of the collective dynamics of proteins; and to elucidate the machinery of molecular chaperones - molecular systems that play an essential role in cellular physiology by assisting the folding and assembly/disassembly of proteins and directing them to transport and degradation pathways. These aims will be pursued in collaboration with the National Center for Biomedical Computing Simbios at Stanford U.",Structural Dynamics of Biomolecular Systems,8212307,R01GM086238,"['Algorithms', 'Allosteric Regulation', 'Biological', 'Biological Process', 'Biomedical Computing', 'Cell physiology', 'Cereals', 'Collaborations', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Databases', 'Degradation Pathway', 'Elements', 'Engineering', 'Environment', 'Funding', 'Goals', 'Grant', 'Graph', 'Hybrids', 'Information Sciences', 'Knowledge', 'Length', 'Libraries', 'Machine Learning', 'Maps', 'Mediating', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Chaperones', 'Molecular Conformation', 'Molecular Machines', 'Molecular Structure', 'Motion', 'Motor', 'Myosin ATPase', 'Nature', 'Nonlinear Dynamics', 'Outcome', 'Pathway interactions', 'Physics', 'Physiological', 'Play', 'Polymers', 'Process', 'Property', 'Protein Dynamics', 'Proteins', 'Research', 'Resolution', 'Role', 'Shapes', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Simulate', 'Site', 'Statistical Mechanics', 'Structure', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'chaperonin', 'computer framework', 'data modeling', 'design', 'improved', 'insight', 'interest', 'intermolecular interaction', 'molecular dynamics', 'network models', 'novel', 'protein folding', 'protein function', 'prototype', 'research study', 'simulation', 'structural genomics', 'theories']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2012,328628,-0.024219313261815395
"Structural Dynamics of Biomolecular Systems Many proteins function as molecular machines. Understanding the principles that control the machinery of biomolecular systems is a computational challenge in many cases due to the involvement of macromolecular structures composed of multiple subunits and cooperative interactions manifested by allosteric changes in conformations, which are beyond the range of atomic simulations. Our goal in a recently funded R33 has been to develop and utilize low resolution models for exploring the collective dynamics of such complex systems, and bridging between structure and function, based on the paradigm structure-encodes-dynamics-encodes-function. The elastic network models and methods we introduced to this aim have found utility in many applications and helped us gain insights into the intrinsic, structure- encoded ability of proteins to favor the reconfiguration of native structures between functional substates. In the present R01, we are proposing to build on our previous work, to further explore the structure -> dynamics -> function mapping of allosteric and/or multimeric proteins using physically-based and computationally efficient models in collaboration with the NCBC Simbios at Stanford U (PI: Altman). The Simbios group has already started to construct a new simulation package, Simbody, the utility of which is expected to be significantly enhanced by a collaborative work. Our specific aims are (1) to build models and methods for automated coarse-graining of complex structures at multiple levels of resolution and assessing their collective dynamics, toward using the resulting models (structure) and data (motions) in Simbody; (2) to complement the physics-based approach developed in Aim 1 by information-theoretic approaches toward delineating signal transduction pathways/mechanisms in allosteric systems, and establishing the connection between these pathways and structural dynamics, and (3) to gain insights into the machinery of molecular chaperones, using as prototypes the bacterial chaperonin GroEL-GroES and the DnaK chaperone system, in collaboration with the Gierasch lab currently doing NIH-supported experiments for understanding the allosteric dynamics of the DnaK system. An important outcome of this project will be the establishment of a methodology for simulating the machinery of biomolecular systems on the order of Megadaltons, which will be achieved in collaboration with the Schulten lab, in addition to our partnership with the Simbios team. Project Narrative  Many proteins function as molecular machines. Understanding the dynamics or underlying molecular principles of these machines is a challenge due to the highly cooperative nature of interactions, which simultaneously involve multiple molecules. The aim of this project is twofold: to develop and implement computational models and methods to improve our understanding of the collective dynamics of proteins; and to elucidate the machinery of molecular chaperones - molecular systems that play an essential role in cellular physiology by assisting the folding and assembly/disassembly of proteins and directing them to transport and degradation pathways. These aims will be pursued in collaboration with the National Center for Biomedical Computing Simbios at Stanford U.",Structural Dynamics of Biomolecular Systems,8017445,R01GM086238,"['Algorithms', 'Allosteric Regulation', 'Biological', 'Biological Process', 'Biomedical Computing', 'Cell physiology', 'Cereals', 'Collaborations', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Databases', 'Degradation Pathway', 'Elements', 'Engineering', 'Environment', 'Funding', 'Goals', 'Grant', 'Graph', 'Hybrids', 'Information Sciences', 'Knowledge', 'Length', 'Libraries', 'Machine Learning', 'Maps', 'Mediating', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Chaperones', 'Molecular Conformation', 'Molecular Machines', 'Molecular Structure', 'Motion', 'Motor', 'Myosin ATPase', 'Nature', 'Nonlinear Dynamics', 'Outcome', 'Pathway interactions', 'Physics', 'Physiological', 'Play', 'Polymers', 'Process', 'Property', 'Protein Dynamics', 'Proteins', 'Research', 'Resolution', 'Role', 'Shapes', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Simulate', 'Site', 'Statistical Mechanics', 'Structure', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'chaperonin', 'computer framework', 'data modeling', 'design', 'improved', 'insight', 'interest', 'intermolecular interaction', 'molecular dynamics', 'network models', 'novel', 'protein folding', 'protein function', 'prototype', 'research study', 'simulation', 'structural genomics', 'theories']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2011,328558,-0.024219313261815395
"Structural Dynamics of Biomolecular Systems Many proteins function as molecular machines. Understanding the principles that control the machinery of biomolecular systems is a computational challenge in many cases due to the involvement of macromolecular structures composed of multiple subunits and cooperative interactions manifested by allosteric changes in conformations, which are beyond the range of atomic simulations. Our goal in a recently funded R33 has been to develop and utilize low resolution models for exploring the collective dynamics of such complex systems, and bridging between structure and function, based on the paradigm structure-encodes-dynamics-encodes-function. The elastic network models and methods we introduced to this aim have found utility in many applications and helped us gain insights into the intrinsic, structure- encoded ability of proteins to favor the reconfiguration of native structures between functional substates. In the present R01, we are proposing to build on our previous work, to further explore the structure -> dynamics -> function mapping of allosteric and/or multimeric proteins using physically-based and computationally efficient models in collaboration with the NCBC Simbios at Stanford U (PI: Altman). The Simbios group has already started to construct a new simulation package, Simbody, the utility of which is expected to be significantly enhanced by a collaborative work. Our specific aims are (1) to build models and methods for automated coarse-graining of complex structures at multiple levels of resolution and assessing their collective dynamics, toward using the resulting models (structure) and data (motions) in Simbody; (2) to complement the physics-based approach developed in Aim 1 by information-theoretic approaches toward delineating signal transduction pathways/mechanisms in allosteric systems, and establishing the connection between these pathways and structural dynamics, and (3) to gain insights into the machinery of molecular chaperones, using as prototypes the bacterial chaperonin GroEL-GroES and the DnaK chaperone system, in collaboration with the Gierasch lab currently doing NIH-supported experiments for understanding the allosteric dynamics of the DnaK system. An important outcome of this project will be the establishment of a methodology for simulating the machinery of biomolecular systems on the order of Megadaltons, which will be achieved in collaboration with the Schulten lab, in addition to our partnership with the Simbios team. Project Narrative  Many proteins function as molecular machines. Understanding the dynamics or underlying molecular principles of these machines is a challenge due to the highly cooperative nature of interactions, which simultaneously involve multiple molecules. The aim of this project is twofold: to develop and implement computational models and methods to improve our understanding of the collective dynamics of proteins; and to elucidate the machinery of molecular chaperones - molecular systems that play an essential role in cellular physiology by assisting the folding and assembly/disassembly of proteins and directing them to transport and degradation pathways. These aims will be pursued in collaboration with the National Center for Biomedical Computing Simbios at Stanford U.",Structural Dynamics of Biomolecular Systems,7751334,R01GM086238,"['Administrator', 'Algorithms', 'Allosteric Regulation', 'Area', 'Automobile Driving', 'Benchmarking', 'Bioinformatics', 'Biological', 'Biological Process', 'Biology', 'Biomedical Computing', 'Biomedical Engineering', 'Biomedical Research', 'Cardiovascular system', 'Cell physiology', 'Cells', 'Cereals', 'Chemicals', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Databases', 'Degradation Pathway', 'Development', 'Disease', 'Doctor of Philosophy', 'Documentation', 'Drug Delivery Systems', 'Elements', 'Engineering', 'Environment', 'Equation', 'Exhibits', 'Faculty', 'Family member', 'Fostering', 'Funding', 'Future', 'Generations', 'Genetic Medicine', 'Goals', 'Grant', 'Graph', 'Hybrids', 'Imagery', 'Individual', 'Information Sciences', 'Institutes', 'Institution', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Length', 'Libraries', 'Licensing', 'Link', 'Literature', 'Machine Learning', 'Maps', 'Mechanics', 'Mediating', 'Medical Device', 'Medical Research', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Modification', 'Molecular', 'Molecular Chaperones', 'Molecular Conformation', 'Molecular Machines', 'Molecular Structure', 'Motion', 'Motor', 'Movement', 'Myopathy', 'Myosin ATPase', 'Nature', 'Newsletter', 'Nonlinear Dynamics', 'Operative Surgical Procedures', 'Organism', 'Outcome', 'Paper', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Physics', 'Physiological', 'Play', 'Polymers', 'Process', 'Property', 'Protein Dynamics', 'Proteins', 'Publishing', 'RNA', 'RNA Folding', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Role', 'Scheme', 'Scientist', 'Sequence Analysis', 'Shapes', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Simulate', 'Site', 'Software Engineering', 'Source', 'Statistical Mechanics', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Training', 'Translating', 'United States National Institutes of Health', 'Universities', 'Ursidae Family', 'Validation', 'Work', 'advanced simulation', 'analytical tool', 'base', 'biological systems', 'biomedical scientist', 'body system', 'chaperone machinery', 'chaperonin', 'commercialization', 'computer framework', 'computer science', 'computerized tools', 'data modeling', 'data sharing', 'design', 'dissemination research', 'flexibility', 'graphical user interface', 'image visualization', 'improved', 'innovation', 'insight', 'interest', 'intermolecular interaction', 'macromolecule', 'mathematical model', 'meetings', 'member', 'models and simulation', 'molecular dynamics', 'nanometer', 'network models', 'neuromuscular', 'novel', 'open source', 'programs', 'protein folding', 'protein function', 'prototype', 'repository', 'research study', 'response', 'simulation', 'software development', 'structural genomics', 'theories', 'tool', 'user-friendly', 'web site']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2010,324772,-0.024219313261815395
"Structural Dynamics of Biomolecular Systems    DESCRIPTION (provided by applicant):  Many proteins function as molecular machines. Understanding the principles that control the machinery of biomolecular systems is a computational challenge in many cases due to the involvement of macromolecular structures composed of multiple subunits and cooperative interactions manifested by allosteric changes in conformations, which are beyond the range of atomic simulations. Our goal in a recently funded R33 has been to develop and utilize low resolution models for exploring the collective dynamics of such complex systems, and bridging between structure and function, based on the paradigm structure-encodes-dynamics-encodes-function. The elastic network models and methods we introduced to this aim have found utility in many applications and helped us gain insights into the intrinsic, structure- encoded ability of proteins to favor the reconfiguration of native structures between functional substates. In the present R01, we are proposing to build on our previous work, to further explore the structure -> dynamics -> function mapping of allosteric and/or multimeric proteins using physically-based and computationally efficient models in collaboration with the NCBC Simbios at Stanford U (PI: Altman). The Simbios group has already started to construct a new simulation package, Simbody, the utility of which is expected to be significantly enhanced by a collaborative work. Our specific aims are (1) to build models and methods for automated coarse-graining of complex structures at multiple levels of resolution and assessing their collective dynamics, toward using the resulting models (structure) and data (motions) in Simbody; (2) to complement the physics-based approach developed in Aim 1 by information-theoretic approaches toward delineating signal transduction pathways/mechanisms in allosteric systems, and establishing the connection between these pathways and structural dynamics, and (3) to gain insights into the machinery of molecular chaperones, using as prototypes the bacterial chaperonin GroEL-GroES and the DnaK chaperone system, in collaboration with the Gierasch lab currently doing NIH-supported experiments for understanding the allosteric dynamics of the DnaK system. An important outcome of this project will be the establishment of a methodology for simulating the machinery of biomolecular systems on the order of Megadaltons, which will be achieved in collaboration with the Schulten lab, in addition to our partnership with the Simbios team. Project Narrative Many proteins function as molecular machines. Understanding the dynamics or underlying molecular principles of these machines is a challenge due to the highly cooperative nature of interactions, which simultaneously involve multiple molecules. The aim of this project is twofold: to develop and implement computational models and methods to improve our understanding of the collective dynamics of proteins; and to elucidate the machinery of molecular chaperones - molecular systems that play an essential role in cellular physiology by assisting the folding and assembly/disassembly of proteins and directing them to transport and degradation pathways. These aims will be pursued in collaboration with the National Center for Biomedical Computing Simbios at Stanford U.          n/a",Structural Dynamics of Biomolecular Systems,7556196,R01GM086238,"['Algorithms', 'Allosteric Regulation', 'Biological', 'Biological Process', 'Biomedical Computing', 'Cell physiology', 'Cereals', 'Collaborations', 'Communication', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Databases', 'Degradation Pathway', 'Elements', 'Engineering', 'Environment', 'Funding', 'Goals', 'Grant', 'Graph', 'Hybrids', 'Information Sciences', 'Knowledge', 'Length', 'Libraries', 'Machine Learning', 'Maps', 'Mediating', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Chaperones', 'Molecular Conformation', 'Molecular Machines', 'Molecular Structure', 'Motion', 'Motor', 'Myosin ATPase', 'Nature', 'Nonlinear Dynamics', 'Outcome', 'Pathway interactions', 'Physics', 'Physiological', 'Play', 'Polymers', 'Process', 'Property', 'Protein Dynamics', 'Proteins', 'Research', 'Resolution', 'Role', 'Shapes', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Simulate', 'Site', 'Statistical Mechanics', 'Structure', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'chaperonin', 'computer framework', 'data modeling', 'design', 'improved', 'insight', 'interest', 'intermolecular interaction', 'molecular dynamics', 'network models', 'novel', 'protein folding', 'protein function', 'prototype', 'research study', 'simulation', 'structural genomics', 'theories']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,327483,-0.024219313261815395
"Neural Mechanisms for the Construction of Subjective Value and Preferences    DESCRIPTION (provided by applicant): Adaptive decision making relies on the ability to evaluate and integrate potential rewards. Data from primate electrophysiology and human functional magnetic resonance imaging (fMRI) studies implicate key structures including the orbitofrontal cortex (OFC) and striatum in valuation and reward-driven learning. However, how value signals from distinct rewards are integrated into a decision value remains relatively unknown. My research proposal will investigate the neural underpinnings of value computation and construction. In particular, I will conduct several experiments that will lead to a better understanding of these regions' separate contributions and functional interactions. First, I will delineate contributions of the striatum to making valuations involving only positive information, only negative information, or both (Specific Aim 1). Second, I investigate the influence of the striatum on hierarchical value computations, as necessary for complex real-world decisions (Specific Aim 2). Third, I will identify a causal role of the striatum in the manipulation of valuation processes (Specific Aim 3). Despite the large fMRI literature involving the striatum and other reward-processing regions of the brain, these aims have been unaddressed with standard fMRI analyses, which are generally limited in spatial accuracy and ill-suited to make strong conclusions regarding relationships between brain regions. My proposal will employ analytic techniques drawn from the machine- learning and biophysical literatures that are well-equipped to predict individual behavior, identify fine-scale differences in brain computational and cognitive processes, and draw conclusions about causal relationships between brain regions. These analytic techniques, combined with novel experimental designs, will allow me to make a significant contribution to an important area of the neuroscience literature. This proposal focuses on subjective value construction, deficits of which are endemic in clinical disorders. Previous research demonstrates that depressed or drug-deprived individuals differentially process positive and negative material and rewards; the research proposed here will tie those results to value computation and decisions involving risk. Additionally, a better understanding of how individuals make complex choices should prove useful for treatment of mental health disorders like depression or anxiety.           n/a",Neural Mechanisms for the Construction of Subjective Value and Preferences,7883451,F31MH086255,"['Affect', 'Animals', 'Anxiety', 'Area', 'Attenuated', 'Behavior', 'Behavioral', 'Brain', 'Brain region', 'Clinical', 'Cognitive', 'Complex', 'Corpus striatum structure', 'Data', 'Decision Making', 'Depressed mood', 'Disease', 'Electrophysiology (science)', 'Environment', 'Evaluation', 'Experimental Designs', 'Food', 'Functional Magnetic Resonance Imaging', 'Gambling', 'Goals', 'Human', 'Image Analysis', 'Individual', 'Lead', 'Learning', 'Literature', 'Machine Learning', 'Mental Depression', 'Mental disorders', 'Neurobiology', 'Neurosciences', 'Nicotine', 'Outcome', 'Pain', 'Pharmaceutical Preparations', 'Phase', 'Physiological', 'Play', 'Population', 'Primates', 'Process', 'Proxy', 'Psychology', 'Research', 'Research Proposals', 'Rewards', 'Risk', 'Role', 'Signal Transduction', 'Smoker', 'Sorting - Cell Movement', 'Specificity', 'Staging', 'Structure', 'Sum', 'System', 'Techniques', 'Testing', 'Uncertainty', 'Ventral Striatum', 'Work', 'addiction', 'base', 'deprivation', 'improved', 'neuromechanism', 'novel', 'pleasure', 'preference', 'relating to nervous system', 'research study', 'response', 'reward processing']",NIMH,DUKE UNIVERSITY,F31,2010,28699,-0.03617520809803944
"Neural Mechanisms for the Construction of Subjective Value and Preferences    DESCRIPTION (provided by applicant): Adaptive decision making relies on the ability to evaluate and integrate potential rewards. Data from primate electrophysiology and human functional magnetic resonance imaging (fMRI) studies implicate key structures including the orbitofrontal cortex (OFC) and striatum in valuation and reward-driven learning. However, how value signals from distinct rewards are integrated into a decision value remains relatively unknown. My research proposal will investigate the neural underpinnings of value computation and construction. In particular, I will conduct several experiments that will lead to a better understanding of these regions' separate contributions and functional interactions. First, I will delineate contributions of the striatum to making valuations involving only positive information, only negative information, or both (Specific Aim 1). Second, I investigate the influence of the striatum on hierarchical value computations, as necessary for complex real-world decisions (Specific Aim 2). Third, I will identify a causal role of the striatum in the manipulation of valuation processes (Specific Aim 3). Despite the large fMRI literature involving the striatum and other reward-processing regions of the brain, these aims have been unaddressed with standard fMRI analyses, which are generally limited in spatial accuracy and ill-suited to make strong conclusions regarding relationships between brain regions. My proposal will employ analytic techniques drawn from the machine- learning and biophysical literatures that are well-equipped to predict individual behavior, identify fine-scale differences in brain computational and cognitive processes, and draw conclusions about causal relationships between brain regions. These analytic techniques, combined with novel experimental designs, will allow me to make a significant contribution to an important area of the neuroscience literature. This proposal focuses on subjective value construction, deficits of which are endemic in clinical disorders. Previous research demonstrates that depressed or drug-deprived individuals differentially process positive and negative material and rewards; the research proposed here will tie those results to value computation and decisions involving risk. Additionally, a better understanding of how individuals make complex choices should prove useful for treatment of mental health disorders like depression or anxiety.           n/a",Neural Mechanisms for the Construction of Subjective Value and Preferences,7677589,F31MH086255,"['Affect', 'Animals', 'Anxiety', 'Area', 'Attenuated', 'Behavior', 'Behavioral', 'Brain', 'Brain region', 'Clinical', 'Cognitive', 'Complex', 'Corpus striatum structure', 'Data', 'Decision Making', 'Disease', 'Electrophysiology (science)', 'Environment', 'Evaluation', 'Experimental Designs', 'Food', 'Functional Magnetic Resonance Imaging', 'Gambling', 'Goals', 'Human', 'Image Analysis', 'Individual', 'Lead', 'Learning', 'Literature', 'Machine Learning', 'Mental disorders', 'Neurobiology', 'Neurosciences', 'Nicotine', 'Outcome', 'Pain', 'Pharmaceutical Preparations', 'Phase', 'Physiological', 'Play', 'Population', 'Primates', 'Process', 'Proxy', 'Psychology', 'Research', 'Research Proposals', 'Rewards', 'Risk', 'Role', 'Signal Transduction', 'Smoker', 'Sorting - Cell Movement', 'Specificity', 'Staging', 'Structure', 'Sum', 'System', 'Techniques', 'Testing', 'Uncertainty', 'Ventral Striatum', 'Work', 'addiction', 'base', 'depressed', 'depression', 'deprivation', 'improved', 'neuromechanism', 'novel', 'pleasure', 'preference', 'relating to nervous system', 'research study', 'response', 'reward processing']",NIMH,DUKE UNIVERSITY,F31,2009,28398,-0.03617520809803944
"Spectral imaging for automated malignant blast counting    DESCRIPTION (provided by applicant):    We propose to extend our successful development of an agile spectral light source for light microscopy, funded through the NCI-IMAT initiative, into FDA trials. The SpectraLamp(tm) device enables the automated and quantitative analysis of double-immunostained samples in brightfield (non-fluorescence-based) microscopy, with particular utility for hematopathology applications. A clinically compelling area is the enumeration of malignant blasts in bone marrow biopsy specimens of patients with acute leukemias, myelodysplastic syndromes and chronic myleloproliferative diseases for the purpose of staging and evaluating therapeutic responses. Current methods are inadequate due to poor sampling (typical of bone marrow aspirates) or difficulty in identifying true blasts (bone marrow biopsies and single-color immunohistochemical phenotyping). Double-immunophenotyping permits largely unambiguous detection, but such labeling strategies are not supported by standard (non-multispectral) color imaging systems. To overcome this limitation, we will combine our multispectral imaging system with a high-speed slide-scanning platform that can scan an entire bone-marrow biopsy at high resolution in under 3 minutes. Machine-learning software tools and spectral imaging will identify blasts, combining morphology parameters and double or even triple immunophenotyping to ensure accuracy and precision. After blasts are identified, flow-cytometrylike software will present the data in histograms and other modalities, allowing the user to ""gate"" on particular cellular populations and pull up panels of cell images for confirmation.      Time-line: Final equipment-related tasks will be accomplished in the first year and preliminary testing of the instrumentation and software features will occur in the second year, as will development of qualified panels of immunoreagents and staining protocols (by DAKOCytomation). FDA-sanctioned clinical trials will be conducted in the third year, leading to submission of an application for a 510(k) clearance.         n/a",Spectral imaging for automated malignant blast counting,7278340,R44CA088684,"['Acute leukemia', 'Antibodies', 'Area', 'Aspirate substance', 'Biopsy Specimen', 'Blast Cell', 'Bone Marrow', 'Bone marrow biopsy', 'Cells', 'Chronic', 'Clinical', 'Clinical Trials', 'Collaborations', 'Color', 'Compatible', 'Computer software', 'Count', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Disease', 'Dysmyelopoietic Syndromes', 'Ensure', 'Equipment', 'Evaluation', 'Evaluation Reports', 'Event', 'Experimental Designs', 'Florida', 'Fluorescence', 'Funding', 'Future', 'Generations', 'Goals', 'Grant', 'Hematopathology', 'Histology', 'Image', 'Image Analysis', 'Immunophenotyping', 'Label', 'Laboratories', 'Legal patent', 'Light', 'Lighting', 'Machine Learning', 'Malignant - descriptor', 'Manuals', 'Mechanics', 'Medical Device', 'Methodology', 'Methods', 'Microscope', 'Microscopy', 'Modality', 'Morphology', 'Myelodysplastic/Myeloproliferative Disease', 'Optics', 'Pathologist', 'Pathology', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Procedures', 'Protocols documentation', 'Public Health Schools', 'Purpose', 'Qualifying', 'Range', 'Reagent', 'Reporting', 'Research Personnel', 'Resolution', 'Sampling', 'Scanning', 'Series', 'Side', 'Slide', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Software Engineering', 'Software Tools', 'Software Validation', 'Source', 'Speed', 'Staging', 'Staining method', 'Stains', 'Standards of Weights and Measures', 'System', 'Testing', 'Therapeutic', 'Time', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Washington', 'Work', 'base', 'cellular imaging', 'design', 'experience', 'innovation', 'instrument', 'instrumentation', 'leukemia', 'light microscopy', 'novel', 'programs', 'research study', 'response', 'software development']",NCI,CAMBRIDGE RESEARCH AND INSTRUMENTATION,R44,2007,1006481,-0.026904461953389386
"Spectral imaging for automated malignant blast counting    DESCRIPTION (provided by applicant):    We propose to extend our successful development of an agile spectral light source for light microscopy, funded through the NCI-IMAT initiative, into FDA trials. The SpectraLamp(tm) device enables the automated and quantitative analysis of double-immunostained samples in brightfield (non-fluorescence-based) microscopy, with particular utility for hematopathology applications. A clinically compelling area is the enumeration of malignant blasts in bone marrow biopsy specimens of patients with acute leukemias, myelodysplastic syndromes and chronic myleloproliferative diseases for the purpose of staging and evaluating therapeutic responses. Current methods are inadequate due to poor sampling (typical of bone marrow aspirates) or difficulty in identifying true blasts (bone marrow biopsies and single-color immunohistochemical phenotyping). Double-immunophenotyping permits largely unambiguous detection, but such labeling strategies are not supported by standard (non-multispectral) color imaging systems. To overcome this limitation, we will combine our multispectral imaging system with a high-speed slide-scanning platform that can scan an entire bone-marrow biopsy at high resolution in under 3 minutes. Machine-learning software tools and spectral imaging will identify blasts, combining morphology parameters and double or even triple immunophenotyping to ensure accuracy and precision. After blasts are identified, flow-cytometrylike software will present the data in histograms and other modalities, allowing the user to ""gate"" on particular cellular populations and pull up panels of cell images for confirmation.      Time-line: Final equipment-related tasks will be accomplished in the first year and preliminary testing of the instrumentation and software features will occur in the second year, as will development of qualified panels of immunoreagents and staining protocols (by DAKOCytomation). FDA-sanctioned clinical trials will be conducted in the third year, leading to submission of an application for a 510(k) clearance.         n/a",Spectral imaging for automated malignant blast counting,7096660,R44CA088684,"['bioimaging /biomedical imaging', 'biomedical equipment development', 'biopsy', 'blood cell count', 'bone marrow', 'bone marrow exam', 'cell morphology', 'cell population study', 'clinical research', 'computer program /software', 'computer system design /evaluation', 'flow cytometry', 'hematopoietic stem cells', 'high throughput technology', 'histopathology', 'human tissue', 'image enhancement', 'immunocytochemistry', 'leukemia', 'light emission', 'light microscopy', 'lighting', 'molecular /cellular imaging', 'neoplasm /cancer diagnosis', 'spectrometry']",NCI,CAMBRIDGE RESEARCH AND INSTRUMENTATION,R44,2006,1029373,-0.026904461953389386
"Spectral imaging for automated malignant blast counting    DESCRIPTION (provided by applicant):    We propose to extend our successful development of an agile spectral light source for light microscopy, funded through the NCI-IMAT initiative, into FDA trials. The SpectraLamp(tm) device enables the automated and quantitative analysis of double-immunostained samples in brightfield (non-fluorescence-based) microscopy, with particular utility for hematopathology applications. A clinically compelling area is the enumeration of malignant blasts in bone marrow biopsy specimens of patients with acute leukemias, myelodysplastic syndromes and chronic myleloproliferative diseases for the purpose of staging and evaluating therapeutic responses. Current methods are inadequate due to poor sampling (typical of bone marrow aspirates) or difficulty in identifying true blasts (bone marrow biopsies and single-color immunohistochemical phenotyping). Double-immunophenotyping permits largely unambiguous detection, but such labeling strategies are not supported by standard (non-multispectral) color imaging systems. To overcome this limitation, we will combine our multispectral imaging system with a high-speed slide-scanning platform that can scan an entire bone-marrow biopsy at high resolution in under 3 minutes. Machine-learning software tools and spectral imaging will identify blasts, combining morphology parameters and double or even triple immunophenotyping to ensure accuracy and precision. After blasts are identified, flow-cytometrylike software will present the data in histograms and other modalities, allowing the user to ""gate"" on particular cellular populations and pull up panels of cell images for confirmation.      Time-line: Final equipment-related tasks will be accomplished in the first year and preliminary testing of the instrumentation and software features will occur in the second year, as will development of qualified panels of immunoreagents and staining protocols (by DAKOCytomation). FDA-sanctioned clinical trials will be conducted in the third year, leading to submission of an application for a 510(k) clearance.         n/a",Spectral imaging for automated malignant blast counting,6938339,R44CA088684,"['bioimaging /biomedical imaging', 'biomedical equipment development', 'biopsy', 'blood cell count', 'bone marrow', 'bone marrow exam', 'cell morphology', 'cell population study', 'clinical research', 'computer program /software', 'computer system design /evaluation', 'flow cytometry', 'hematopoietic stem cells', 'high throughput technology', 'histopathology', 'human tissue', 'image enhancement', 'immunocytochemistry', 'leukemia', 'light emission', 'light microscopy', 'lighting', 'molecular /cellular imaging', 'neoplasm /cancer diagnosis', 'spectrometry']",NCI,CAMBRIDGE RESEARCH AND INSTRUMENTATION,R44,2005,1000582,-0.026904461953389386
"Investigating emotional reactivity in depression using event-related potentials    DESCRIPTION (provided by applicant): Major depressive disorder (MDD) is of immense public health importance because of its high prevalence, early onset, chronicity, and functional impairment. While behavioral and peripheral physiological studies have identified a deficit in emotional reactivity associated with MDD, leading to an emotion context insensitivity (ECI) theoretical model, neuro imaging studies have yielded evidence of both hypo- and hyperactivity depending on the region of interest and the type of affective stimuli used. One possibility is that emotional reactivity in MDD is moderated by the personal relevance of stimuli, such that reactivity to normative stimuli is blunted and reactivity to idiographic stimuli is increased, although this has yet to be tested within a single sample. The proposed study seeks to examine this issue using three direct measures of brain activity: event-related potentials (ERPs), spectral analysis of the electroencephalogram (EEG), and functional magnetic resonance imaging (fMRI). By integrating these three methods, this study will be well-equipped to enhance our understanding of the pathophysiological mechanisms underlying abnormal emotional processing in MDD, which is consistent with the mission of the Division of Neuroscience and Basic Behavioral Science at the National Institute of Mental Health. Further, by seeking to systematically characterize specific neural abnormalities in MDD, the current study may provide objective biological measures that can inform future work on clinical issues such as treatment selection, tracking treatment progress, and predicting recovery. To examine the influence of personal relevance on abnormal emotional processing in MDD, the proposed research will consider neural activity among 30 adults with current MDD and 30 never-depressed controls, with three specific aims: (a) to compare ERP/EEG responses to normative and idiographic emotional stimuli; (b) to compare ERP/EEG responses to reward- and performance-based feedback; and (c) to use fMRI to examine neural activation to emotional faces and rewards. This research also incorporates several essential training components that will provide the opportunity for the applicant to attain further clinical and methodological expertise in several domains. As a co-sponsor of this application, Dr. Daniel Klein will provide advanced training in the assessment of mood disorders. Dr. Lilianne Mujica-Parodi, also a co-sponsor, will provide training in the acquisition, analysis, and interpretation of fMRI data. Two consultants are also included in this proposal to provide advanced statistical training: Dr. Joseph Dien has expertise in the application of principal components analysis and source localization techniques to ERP/EEG data sets, and Dr. Andreas Keil has expertise in using time-frequency approaches to analyze oscillatory EEG data. Finally, the implementation of the proposed research and training will be overseen by Dr. Greg Hajcak, the sponsor of this proposal and the primary academic mentor of the applicant.      PUBLIC HEALTH RELEVANCE: Major depressive disorder (MDD) is an illness that is of immense public health importance because of its high prevalence rate, early onset, chronicity, and functional impairment. The current project seeks to use examine specific patterns of brain activity in response to emotional stimuli, in order to shed light on the neural mechanisms underlying abnormal emotional reactivity in MDD. In doing so, this will aid recent research efforts to identify biological markers associated with MDD that can be used to facilitate effective treatment selection, track treatment progress, and predict recovery.           Project Narrative: Relevance  Major depressive disorder (MDD) is an illness that is of immense public health importance because of its high prevalence rate, early onset, chronicity, and functional impairment. The current project seeks to use examine specific patterns of brain activity in response to emotional stimuli, in order to shed light on the neural mechanisms underlying abnormal emotional reactivity in MDD. In doing so, this will aid recent research efforts to identify biological markers associated with MDD that can be used to facilitate effective treatment selection, track treatment progress, and predict recovery.",Investigating emotional reactivity in depression using event-related potentials,8214710,F31MH090658,"['Adult', 'Affective', 'Amygdaloid structure', 'Anger', 'Anterior', 'Attention', 'Basic Behavioral Science', 'Behavioral', 'Biological', 'Biological Markers', 'Brain', 'Clinical', 'Complement', 'Control Groups', 'Corpus striatum structure', 'Data', 'Data Set', 'Depressed mood', 'Dopamine', 'Electroencephalogram', 'Emotional', 'Emotions', 'Event-Related Potentials', 'Exhibits', 'Face', 'Feedback', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Future', 'High Prevalence', 'Hyperactive behavior', 'Image', 'Individual', 'Light', 'Link', 'Literature', 'Major Depressive Disorder', 'Measures', 'Mental Depression', 'Mentors', 'Methods', 'Mission', 'Mood Disorders', 'National Institute of Mental Health', 'Nature', 'Neurosciences', 'Outcome', 'Parietal', 'Pattern', 'Performance', 'Peripheral', 'Physiological', 'Physiology', 'Principal Component Analysis', 'Process', 'Public Health', 'Reaction Time', 'Recovery', 'Reporting', 'Research', 'Research Training', 'Resolution', 'Rewards', 'Sampling', 'Selection for Treatments', 'Source', 'Stimulus', 'Techniques', 'Testing', 'Theoretical model', 'Training', 'Visual', 'Work', 'base', 'cingulate cortex', 'depressive symptoms', 'early onset', 'effective therapy', 'emotional stimulus', 'functional disability', 'indexing', 'interest', 'neuroimaging', 'neuromechanism', 'public health relevance', 'relating to nervous system', 'response', 'reward processing', 'self reported behavior', 'time use', 'visual stimulus']",NIMH,STATE UNIVERSITY NEW YORK STONY BROOK,F31,2012,18913,-0.0541183268567759
"Investigating emotional reactivity in depression using event-related potentials    DESCRIPTION (provided by applicant): Major depressive disorder (MDD) is of immense public health importance because of its high prevalence, early onset, chronicity, and functional impairment. While behavioral and peripheral physiological studies have identified a deficit in emotional reactivity associated with MDD, leading to an emotion context insensitivity (ECI) theoretical model, neuro imaging studies have yielded evidence of both hypo- and hyperactivity depending on the region of interest and the type of affective stimuli used. One possibility is that emotional reactivity in MDD is moderated by the personal relevance of stimuli, such that reactivity to normative stimuli is blunted and reactivity to idiographic stimuli is increased, although this has yet to be tested within a single sample. The proposed study seeks to examine this issue using three direct measures of brain activity: event-related potentials (ERPs), spectral analysis of the electroencephalogram (EEG), and functional magnetic resonance imaging (fMRI). By integrating these three methods, this study will be well-equipped to enhance our understanding of the pathophysiological mechanisms underlying abnormal emotional processing in MDD, which is consistent with the mission of the Division of Neuroscience and Basic Behavioral Science at the National Institute of Mental Health. Further, by seeking to systematically characterize specific neural abnormalities in MDD, the current study may provide objective biological measures that can inform future work on clinical issues such as treatment selection, tracking treatment progress, and predicting recovery. To examine the influence of personal relevance on abnormal emotional processing in MDD, the proposed research will consider neural activity among 30 adults with current MDD and 30 never-depressed controls, with three specific aims: (a) to compare ERP/EEG responses to normative and idiographic emotional stimuli; (b) to compare ERP/EEG responses to reward- and performance-based feedback; and (c) to use fMRI to examine neural activation to emotional faces and rewards. This research also incorporates several essential training components that will provide the opportunity for the applicant to attain further clinical and methodological expertise in several domains. As a co-sponsor of this application, Dr. Daniel Klein will provide advanced training in the assessment of mood disorders. Dr. Lilianne Mujica-Parodi, also a co-sponsor, will provide training in the acquisition, analysis, and interpretation of fMRI data. Two consultants are also included in this proposal to provide advanced statistical training: Dr. Joseph Dien has expertise in the application of principal components analysis and source localization techniques to ERP/EEG data sets, and Dr. Andreas Keil has expertise in using time-frequency approaches to analyze oscillatory EEG data. Finally, the implementation of the proposed research and training will be overseen by Dr. Greg Hajcak, the sponsor of this proposal and the primary academic mentor of the applicant.      PUBLIC HEALTH RELEVANCE: Major depressive disorder (MDD) is an illness that is of immense public health importance because of its high prevalence rate, early onset, chronicity, and functional impairment. The current project seeks to use examine specific patterns of brain activity in response to emotional stimuli, in order to shed light on the neural mechanisms underlying abnormal emotional reactivity in MDD. In doing so, this will aid recent research efforts to identify biological markers associated with MDD that can be used to facilitate effective treatment selection, track treatment progress, and predict recovery.           Project Narrative: Relevance  Major depressive disorder (MDD) is an illness that is of immense public health importance because of its high prevalence rate, early onset, chronicity, and functional impairment. The current project seeks to use examine specific patterns of brain activity in response to emotional stimuli, in order to shed light on the neural mechanisms underlying abnormal emotional reactivity in MDD. In doing so, this will aid recent research efforts to identify biological markers associated with MDD that can be used to facilitate effective treatment selection, track treatment progress, and predict recovery.",Investigating emotional reactivity in depression using event-related potentials,8059070,F31MH090658,"['Adult', 'Affective', 'Amygdaloid structure', 'Anger', 'Anterior', 'Attention', 'Basic Behavioral Science', 'Behavioral', 'Biological', 'Biological Markers', 'Brain', 'Clinical', 'Complement', 'Control Groups', 'Corpus striatum structure', 'Data', 'Data Set', 'Depressed mood', 'Dopamine', 'Electroencephalogram', 'Emotional', 'Emotions', 'Event-Related Potentials', 'Exhibits', 'Face', 'Feedback', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Future', 'High Prevalence', 'Hyperactive behavior', 'Image', 'Individual', 'Light', 'Link', 'Literature', 'Major Depressive Disorder', 'Measures', 'Mental Depression', 'Mentors', 'Methods', 'Mission', 'Mood Disorders', 'National Institute of Mental Health', 'Nature', 'Neurosciences', 'Outcome', 'Parietal', 'Pattern', 'Performance', 'Peripheral', 'Physiological', 'Physiology', 'Principal Component Analysis', 'Process', 'Public Health', 'Reaction Time', 'Recovery', 'Reporting', 'Research', 'Research Training', 'Resolution', 'Rewards', 'Sampling', 'Selection for Treatments', 'Source', 'Stimulus', 'Techniques', 'Testing', 'Theoretical model', 'Training', 'Visual', 'Work', 'base', 'cingulate cortex', 'depressive symptoms', 'early onset', 'effective therapy', 'emotional stimulus', 'functional disability', 'indexing', 'interest', 'neuroimaging', 'neuromechanism', 'public health relevance', 'relating to nervous system', 'response', 'reward processing', 'self reported behavior', 'time use', 'visual stimulus']",NIMH,STATE UNIVERSITY NEW YORK STONY BROOK,F31,2010,29795,-0.0541183268567759
"Neonatal Endotracheal Intubation: Enhancing Training Through Computer Simulation and Automated Evaluation Project Summary Neonatal endotracheal intubation (ETI) is a time-sensitive resuscitation procedure! essential for ventilation of newborns. It requires an unusually high level of skill due to the narrow airways, relatively large tongue, anterior glottic position, and low respiratory reserve of neonates (Bercic, Pocajt et al. 1978). Given the difficulty of the procedure and the high rate of complications in untrained hands, effective training is crucial. However, intubation success rates for pediatric residents are low under current resuscitation training programs and show little improvement between years 1-3 of residency (23-25%) (O'Donnell, Kamlin et al. 2006; Haubner, Barry et al. 2013). There is a pressing need to understand the factors that lead to poor training results and for innovative training modalities that can bridge the gap left by traditional training and thereby allow rapid skill acquisition. We hypothesize that current training and assessment methods suffer from 4 key weaknesses: (1) Poor realism: manikin and simulator-based training typically provide little variation in anatomy or difficulty level—key requirements for developing expertise (Dreyfus, Athanasiou et al. 1986)—and do not realistically model the look, feel, and motions of real tissue. (2) Subjective, highly variable, and resource-intensive assessment methods: training opportunities are limited by the availability of expert instructors. (3) Poor visualization: learners have poor knowledge about what went wrong and how to improve; they cannot see exactly what is going on inside the manikin or the patient and cannot directly monitor their actions relative to idealized, expert performance. (4) Assessment under artificially ideal conditions: assessments of ETI performance in classroom settings likely overestimate trainees' skill level because they do not mimic the stressors and distractions that are inherent in the real clinical environment. Technology-enhanced ETI simulators can resolve all of these key weaknesses: We have conducted preliminary work (Hahn, Li et al. 2016; Soghier, Li et al. 2014) on an augmented reality (AR (Azuma 1997)) manikin simulator driven by the motions of the trainee and physical manikin in real time that 1) provides a quantitative assessment of ETI technique and 2) allows the trainee to visualize the motion of the laryngoscope inside the manikin. The assessment score can provide feedback during the performance, as well as constitute part of the evaluation of the trainee's skill. Work under this proposal will build on this preliminary work. The specific aims are to: extend the current augmented reality (AR) manikin simulator to a virtual reality (VR) computer simulator and validate, extend and validate automated assessment and visualization algorithm for ETI, study training effectiveness by testing groups of pediatric residents across 3 years to quantify the effect of technology-enhanced methods relative to the current training regimen in terms of both intubation performance on simulators and clinical outcomes in patients, and assess performance under more realistic conditions. Project Narrative The current training and assessment of neonatal endotracheal intubation (ETI) suffers from key weaknesses of 1) poor realism of task simulators, 2) subjective, highly variable, and resource-intensive assessment methods, 3) poor visualization during simulation, and 4) assessment methods under artificially ideal conditions. This high-yield proposal will bridge the gap between training and clinical practice by using quantitative assessment tools and technology-enhanced simulation to improve ETI performance prior to patient care.",Neonatal Endotracheal Intubation: Enhancing Training Through Computer Simulation and Automated Evaluation,9967059,R01HD091179,"['Algorithms', 'Anatomy', 'Anterior', 'Assessment tool', 'Attention', 'Augmented Reality', 'Biomedical Engineering', 'Biometry', 'Childhood', 'Clinical', 'Cognitive Science', 'Computer Simulation', 'Computers', 'Data', 'Effectiveness', 'Enhancement Technology', 'Environment', 'Evaluation', 'Feedback', 'Hand', 'Intratracheal Intubation', 'Intubation', 'Knowledge', 'Laryngoscopes', 'Lead', 'Learning', 'Left', 'Libraries', 'Machine Learning', 'Manikins', 'Measures', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Motion', 'Neonatal', 'Neonatology', 'Newborn Infant', 'Outcome', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Positioning Attribute', 'Procedures', 'Regimen', 'Residencies', 'Resources', 'Resuscitation', 'Scanning', 'Standardization', 'System', 'Techniques', 'Testing', 'Time', 'Tissues', 'Tongue', 'Training', 'Training Programs', 'Validation', 'Variant', 'Visualization', 'Work', 'base', 'clinical practice', 'computer science', 'distraction', 'effectiveness testing', 'evidence base', 'haptics', 'improved', 'innovation', 'instructor', 'kinematics', 'multidisciplinary', 'neonate', 'respiratory', 'simulation', 'skill acquisition', 'skills', 'stressor', 'success', 'training opportunity', 'ventilation', 'virtual reality', 'virtual reality simulator']",NICHD,GEORGE WASHINGTON UNIVERSITY,R01,2020,319874,-0.046341384054548275
"Neonatal Endotracheal Intubation: Enhancing Training Through Computer Simulation and Automated Evaluation Project Summary Neonatal endotracheal intubation (ETI) is a time-sensitive resuscitation procedure! essential for ventilation of newborns. It requires an unusually high level of skill due to the narrow airways, relatively large tongue, anterior glottic position, and low respiratory reserve of neonates (Bercic, Pocajt et al. 1978). Given the difficulty of the procedure and the high rate of complications in untrained hands, effective training is crucial. However, intubation success rates for pediatric residents are low under current resuscitation training programs and show little improvement between years 1-3 of residency (23-25%) (O'Donnell, Kamlin et al. 2006; Haubner, Barry et al. 2013). There is a pressing need to understand the factors that lead to poor training results and for innovative training modalities that can bridge the gap left by traditional training and thereby allow rapid skill acquisition. We hypothesize that current training and assessment methods suffer from 4 key weaknesses: (1) Poor realism: manikin and simulator-based training typically provide little variation in anatomy or difficulty level—key requirements for developing expertise (Dreyfus, Athanasiou et al. 1986)—and do not realistically model the look, feel, and motions of real tissue. (2) Subjective, highly variable, and resource-intensive assessment methods: training opportunities are limited by the availability of expert instructors. (3) Poor visualization: learners have poor knowledge about what went wrong and how to improve; they cannot see exactly what is going on inside the manikin or the patient and cannot directly monitor their actions relative to idealized, expert performance. (4) Assessment under artificially ideal conditions: assessments of ETI performance in classroom settings likely overestimate trainees' skill level because they do not mimic the stressors and distractions that are inherent in the real clinical environment. Technology-enhanced ETI simulators can resolve all of these key weaknesses: We have conducted preliminary work (Hahn, Li et al. 2016; Soghier, Li et al. 2014) on an augmented reality (AR (Azuma 1997)) manikin simulator driven by the motions of the trainee and physical manikin in real time that 1) provides a quantitative assessment of ETI technique and 2) allows the trainee to visualize the motion of the laryngoscope inside the manikin. The assessment score can provide feedback during the performance, as well as constitute part of the evaluation of the trainee's skill. Work under this proposal will build on this preliminary work. The specific aims are to: extend the current augmented reality (AR) manikin simulator to a virtual reality (VR) computer simulator and validate, extend and validate automated assessment and visualization algorithm for ETI, study training effectiveness by testing groups of pediatric residents across 3 years to quantify the effect of technology-enhanced methods relative to the current training regimen in terms of both intubation performance on simulators and clinical outcomes in patients, and assess performance under more realistic conditions. Project Narrative The current training and assessment of neonatal endotracheal intubation (ETI) suffers from key weaknesses of 1) poor realism of task simulators, 2) subjective, highly variable, and resource-intensive assessment methods, 3) poor visualization during simulation, and 4) assessment methods under artificially ideal conditions. This high-yield proposal will bridge the gap between training and clinical practice by using quantitative assessment tools and technology-enhanced simulation to improve ETI performance prior to patient care.",Neonatal Endotracheal Intubation: Enhancing Training Through Computer Simulation and Automated Evaluation,9732337,R01HD091179,"['Algorithms', 'Anatomy', 'Anterior', 'Assessment tool', 'Attention', 'Augmented Reality', 'Biomedical Engineering', 'Biometry', 'Childhood', 'Clinical', 'Cognitive Science', 'Computer Simulation', 'Computers', 'Data', 'Effectiveness', 'Enhancement Technology', 'Environment', 'Environmental air flow', 'Evaluation', 'Feedback', 'Hand', 'Imagery', 'Intratracheal Intubation', 'Intubation', 'Knowledge', 'Laryngoscopes', 'Lead', 'Learning', 'Left', 'Libraries', 'Machine Learning', 'Manikins', 'Measures', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Motion', 'Neonatal', 'Neonatology', 'Newborn Infant', 'Outcome', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Positioning Attribute', 'Procedures', 'Regimen', 'Residencies', 'Resources', 'Resuscitation', 'Scanning', 'Standardization', 'System', 'Techniques', 'Testing', 'Time', 'Tissues', 'Tongue', 'Training', 'Training Programs', 'Validation', 'Variant', 'Work', 'base', 'clinical practice', 'computer science', 'distraction', 'evidence base', 'haptics', 'improved', 'innovation', 'instructor', 'kinematics', 'multidisciplinary', 'neonate', 'respiratory', 'simulation', 'skill acquisition', 'skills', 'stressor', 'success', 'training opportunity', 'virtual reality']",NICHD,GEORGE WASHINGTON UNIVERSITY,R01,2019,318886,-0.046341384054548275
"Neonatal Endotracheal Intubation: Enhancing Training Through Computer Simulation and Automated Evaluation Project Summary Neonatal endotracheal intubation (ETI) is a time-sensitive resuscitation procedure! essential for ventilation of newborns. It requires an unusually high level of skill due to the narrow airways, relatively large tongue, anterior glottic position, and low respiratory reserve of neonates (Bercic, Pocajt et al. 1978). Given the difficulty of the procedure and the high rate of complications in untrained hands, effective training is crucial. However, intubation success rates for pediatric residents are low under current resuscitation training programs and show little improvement between years 1-3 of residency (23-25%) (O'Donnell, Kamlin et al. 2006; Haubner, Barry et al. 2013). There is a pressing need to understand the factors that lead to poor training results and for innovative training modalities that can bridge the gap left by traditional training and thereby allow rapid skill acquisition. We hypothesize that current training and assessment methods suffer from 4 key weaknesses: (1) Poor realism: manikin and simulator-based training typically provide little variation in anatomy or difficulty level—key requirements for developing expertise (Dreyfus, Athanasiou et al. 1986)—and do not realistically model the look, feel, and motions of real tissue. (2) Subjective, highly variable, and resource-intensive assessment methods: training opportunities are limited by the availability of expert instructors. (3) Poor visualization: learners have poor knowledge about what went wrong and how to improve; they cannot see exactly what is going on inside the manikin or the patient and cannot directly monitor their actions relative to idealized, expert performance. (4) Assessment under artificially ideal conditions: assessments of ETI performance in classroom settings likely overestimate trainees' skill level because they do not mimic the stressors and distractions that are inherent in the real clinical environment. Technology-enhanced ETI simulators can resolve all of these key weaknesses: We have conducted preliminary work (Hahn, Li et al. 2016; Soghier, Li et al. 2014) on an augmented reality (AR (Azuma 1997)) manikin simulator driven by the motions of the trainee and physical manikin in real time that 1) provides a quantitative assessment of ETI technique and 2) allows the trainee to visualize the motion of the laryngoscope inside the manikin. The assessment score can provide feedback during the performance, as well as constitute part of the evaluation of the trainee's skill. Work under this proposal will build on this preliminary work. The specific aims are to: extend the current augmented reality (AR) manikin simulator to a virtual reality (VR) computer simulator and validate, extend and validate automated assessment and visualization algorithm for ETI, study training effectiveness by testing groups of pediatric residents across 3 years to quantify the effect of technology-enhanced methods relative to the current training regimen in terms of both intubation performance on simulators and clinical outcomes in patients, and assess performance under more realistic conditions. Project Narrative The current training and assessment of neonatal endotracheal intubation (ETI) suffers from key weaknesses of 1) poor realism of task simulators, 2) subjective, highly variable, and resource-intensive assessment methods, 3) poor visualization during simulation, and 4) assessment methods under artificially ideal conditions. This high-yield proposal will bridge the gap between training and clinical practice by using quantitative assessment tools and technology-enhanced simulation to improve ETI performance prior to patient care.",Neonatal Endotracheal Intubation: Enhancing Training Through Computer Simulation and Automated Evaluation,9547897,R01HD091179,"['Algorithms', 'Anatomy', 'Anterior', 'Assessment tool', 'Attention', 'Augmented Reality', 'Biomedical Engineering', 'Biometry', 'Childhood', 'Clinical', 'Cognitive Science', 'Computer Simulation', 'Computers', 'Data', 'Effectiveness', 'Enhancement Technology', 'Environment', 'Environmental air flow', 'Evaluation', 'Feedback', 'Hand', 'Imagery', 'Intratracheal Intubation', 'Intubation', 'Knowledge', 'Laryngoscopes', 'Lead', 'Learning', 'Left', 'Libraries', 'Machine Learning', 'Manikins', 'Measures', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Motion', 'Neonatal', 'Neonatology', 'Newborn Infant', 'Outcome', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Positioning Attribute', 'Procedures', 'Regimen', 'Residencies', 'Resources', 'Resuscitation', 'Scanning', 'Standardization', 'System', 'Techniques', 'Testing', 'Time', 'Tissues', 'Tongue', 'Training', 'Training Programs', 'Validation', 'Variant', 'Work', 'base', 'clinical practice', 'computer science', 'distraction', 'evidence base', 'haptics', 'improved', 'innovation', 'instructor', 'kinematics', 'multidisciplinary', 'neonate', 'respiratory', 'simulation', 'skill acquisition', 'skills', 'stressor', 'success', 'training opportunity', 'virtual reality']",NICHD,GEORGE WASHINGTON UNIVERSITY,R01,2018,317404,-0.046341384054548275
"Neonatal Endotracheal Intubation: Enhancing Training Through Computer Simulation and Automated Evaluation Project Summary Neonatal endotracheal intubation (ETI) is a time-sensitive resuscitation procedure! essential for ventilation of newborns. It requires an unusually high level of skill due to the narrow airways, relatively large tongue, anterior glottic position, and low respiratory reserve of neonates (Bercic, Pocajt et al. 1978). Given the difficulty of the procedure and the high rate of complications in untrained hands, effective training is crucial. However, intubation success rates for pediatric residents are low under current resuscitation training programs and show little improvement between years 1-3 of residency (23-25%) (O'Donnell, Kamlin et al. 2006; Haubner, Barry et al. 2013). There is a pressing need to understand the factors that lead to poor training results and for innovative training modalities that can bridge the gap left by traditional training and thereby allow rapid skill acquisition. We hypothesize that current training and assessment methods suffer from 4 key weaknesses: (1) Poor realism: manikin and simulator-based training typically provide little variation in anatomy or difficulty level—key requirements for developing expertise (Dreyfus, Athanasiou et al. 1986)—and do not realistically model the look, feel, and motions of real tissue. (2) Subjective, highly variable, and resource-intensive assessment methods: training opportunities are limited by the availability of expert instructors. (3) Poor visualization: learners have poor knowledge about what went wrong and how to improve; they cannot see exactly what is going on inside the manikin or the patient and cannot directly monitor their actions relative to idealized, expert performance. (4) Assessment under artificially ideal conditions: assessments of ETI performance in classroom settings likely overestimate trainees' skill level because they do not mimic the stressors and distractions that are inherent in the real clinical environment. Technology-enhanced ETI simulators can resolve all of these key weaknesses: We have conducted preliminary work (Hahn, Li et al. 2016; Soghier, Li et al. 2014) on an augmented reality (AR (Azuma 1997)) manikin simulator driven by the motions of the trainee and physical manikin in real time that 1) provides a quantitative assessment of ETI technique and 2) allows the trainee to visualize the motion of the laryngoscope inside the manikin. The assessment score can provide feedback during the performance, as well as constitute part of the evaluation of the trainee's skill. Work under this proposal will build on this preliminary work. The specific aims are to: extend the current augmented reality (AR) manikin simulator to a virtual reality (VR) computer simulator and validate, extend and validate automated assessment and visualization algorithm for ETI, study training effectiveness by testing groups of pediatric residents across 3 years to quantify the effect of technology-enhanced methods relative to the current training regimen in terms of both intubation performance on simulators and clinical outcomes in patients, and assess performance under more realistic conditions. Project Narrative The current training and assessment of neonatal endotracheal intubation (ETI) suffers from key weaknesses of 1) poor realism of task simulators, 2) subjective, highly variable, and resource-intensive assessment methods, 3) poor visualization during simulation, and 4) assessment methods under artificially ideal conditions. This high-yield proposal will bridge the gap between training and clinical practice by using quantitative assessment tools and technology-enhanced simulation to improve ETI performance prior to patient care.",Neonatal Endotracheal Intubation: Enhancing Training Through Computer Simulation and Automated Evaluation,9288629,R01HD091179,"['Algorithms', 'Anatomy', 'Anterior', 'Assessment tool', 'Attention', 'Augmented Reality', 'Biomedical Engineering', 'Biometry', 'Childhood', 'Clinical', 'Cognitive Science', 'Computer Simulation', 'Computers', 'Data', 'Effectiveness', 'Enhancement Technology', 'Environment', 'Environmental air flow', 'Evaluation', 'Feedback', 'Hand', 'Imagery', 'Intratracheal Intubation', 'Intubation', 'Knowledge', 'Laryngoscopes', 'Lead', 'Learning', 'Left', 'Libraries', 'Machine Learning', 'Manikins', 'Measures', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Motion', 'Neonatal', 'Neonatology', 'Newborn Infant', 'Outcome', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Positioning Attribute', 'Procedures', 'Regimen', 'Residencies', 'Resources', 'Resuscitation', 'Scanning', 'Standardization', 'System', 'Techniques', 'Testing', 'Time', 'Tissues', 'Tongue', 'Training', 'Training Programs', 'Validation', 'Variant', 'Work', 'base', 'clinical practice', 'computer science', 'distraction', 'evidence base', 'haptics', 'improved', 'innovation', 'instructor', 'kinematics', 'multidisciplinary', 'neonate', 'respiratory', 'simulation', 'skill acquisition', 'skills', 'stressor', 'success', 'training opportunity', 'virtual reality']",NICHD,GEORGE WASHINGTON UNIVERSITY,R01,2017,313412,-0.046341384054548275
"Simulation Algorithms for Spatial Pattern Recognition    DESCRIPTION (provided by applicant):    This SBIR project is developing methods and software for the specification, construction and simulation of neutral spatial models, and for applying these neutral models within the framework of probabilistic pattern recognition. Results will allow epidemiologists, environmental scientists and image analysts across a broad range of commercial disciplines to more accurately identify patterns in spatial data by removing the bias towards false positives that is caused by unrealistic null hypotheses such as ""complete spatial randomness"" (CSR). This project will accomplish 5 aims:      1. Conduct a requirements analysis to specify the neutral models and functionality to incorporate in the software.   2. Develop and test a software prototype to evaluate feasibility of the proposed models.   3. Propose a topology of neutral models and develop strategies to generate them and to conduct sensitivity analysis for investigating the impact of implicit assumptions (i.e. spatial autocorrelation or non-uniform risk) and number of realizations on test results.   4. Incorporate the neutral models in the first commercially established software package that allows for user-specified alternate hypothesis in spatial statistical tests.   5. Apply the software and methods to demonstrate the approach and its unique benefits for exposure and health risk assessment.      Feasibility of this project was demonstrated in the Phase I. This Phase II project will accomplish aims three through five. These technologic, scientific and commercial innovations will revolutionize our ability to identify, document and assess the probability of spatial patterns relative to neutral models that incorporate realistic local, spatial and multivariate dependencies. The neutral models and methods in this proposal make possible, for the first time ever, evaluation of the sensitivity of the results of cluster or boundary analyses to specification of the null hypothesis.         n/a",Simulation Algorithms for Spatial Pattern Recognition,7015648,R44CA092807,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'human data', 'image processing', 'imaging /visualization /scanning', 'statistics /biometry', 'visual cortex']",NCI,BIOMEDWARE,R44,2006,500182,0.007262770267621003
"Simulation Algorithms for Spatial Pattern Recognition    DESCRIPTION (provided by applicant):    This SBIR project is developing methods and software for the specification, construction and simulation of neutral spatial models, and for applying these neutral models within the framework of probabilistic pattern recognition. Results will allow epidemiologists, environmental scientists and image analysts across a broad range of commercial disciplines to more accurately identify patterns in spatial data by removing the bias towards false positives that is caused by unrealistic null hypotheses such as ""complete spatial randomness"" (CSR). This project will accomplish 5 aims:      1. Conduct a requirements analysis to specify the neutral models and functionality to incorporate in the software.   2. Develop and test a software prototype to evaluate feasibility of the proposed models.   3. Propose a topology of neutral models and develop strategies to generate them and to conduct sensitivity analysis for investigating the impact of implicit assumptions (i.e. spatial autocorrelation or non-uniform risk) and number of realizations on test results.   4. Incorporate the neutral models in the first commercially established software package that allows for user-specified alternate hypothesis in spatial statistical tests.   5. Apply the software and methods to demonstrate the approach and its unique benefits for exposure and health risk assessment.      Feasibility of this project was demonstrated in the Phase I. This Phase II project will accomplish aims three through five. These technologic, scientific and commercial innovations will revolutionize our ability to identify, document and assess the probability of spatial patterns relative to neutral models that incorporate realistic local, spatial and multivariate dependencies. The neutral models and methods in this proposal make possible, for the first time ever, evaluation of the sensitivity of the results of cluster or boundary analyses to specification of the null hypothesis.         n/a",Simulation Algorithms for Spatial Pattern Recognition,6863029,R44CA092807,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'human data', 'image processing', 'imaging /visualization /scanning', 'statistics /biometry', 'visual cortex']",NCI,BIOMEDWARE,R44,2005,498368,0.007262770267621003
"Simulation Algorithms for Spatial Pattern Recognition   DESCRIPTION (provided by applicant): A new generation of satellites is imaging       the earth's surface with unprecedented spatial and spectral resolution. With         the ability to identify local features related to environmental exposures, this      high-resolution imagery is gong to revolutionize health risk assessment. The         realization of this potential depends critically on our ability to recognize         spatial patterns on these large images. This project will develop fast spatial       null models for use in statistical pattern recognition, and will accomplish 4        aims.                                                                                                                                                                     (1) Implement fast simulation algorithms conditioned on properties of the data,      and on spatial functions;                                                            (2) Assess project feasibility by evaluating the performance of these                algorithms on existing high-resolution, hyperspectral imagery;                       (3) Implement the simulation algorithms in 2 commercial spatial analysis             software packages;                                                                   (4) Apply the software and methods to demonstrate the approach and unique            benefits for risk assessment.                                                                                                                                             The phase 1 research will address the first two aims; aims three and four will       be accomplished in phase 2 once feasibility is demonstrated. The technologic         and scientific innovations from this project are expected to greatly enhance         our ability to extract knowledge from high resolution imagery.                       PROPOSED COMMERCIAL APPLICATION:  The imminent launch of over a dozen satellites capable of high-resolution imagery is giving  health researchers powerful new data for relating environmental features to health   outcomes, but existing software packages cannot undertake spatial analysis of these  extraordinarly large data sets.   The fast simulation algorithms from this research will  be incorporated into 2 commercial software packages, providing advanced spatial  analysis for large imagery.                                                                                     n/a",Simulation Algorithms for Spatial Pattern Recognition,6401389,R43CA092807,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' image processing', ' imaging /visualization /scanning', ' statistics /biometry']",NCI,BIOMEDWARE,R43,2001,170490,0.00971270628157952
"Applied Imagery Pattern Recognition Workshop   DESCRIPTION (Provided by Applicant):                                                 The Applied Imagery, Pattern Recognition (AIPR) Workshop is held annually in         Washington, D.C., at the Cosmos Club.  This workshop brings together 60-80           members of academia, industry, and other federal agencies, with a particular         history of involvement with the broader intelligence community in image              processing and analysis.  The workshop format for the last decade has                generally involved three days of high-quality presentations and investigator         interaction, both formally and informally at evening meeting receptions held         at the Cosmos Club.  The 30th meeting will be held from October 10-12, 2001,         and focus on Time-Varying Imagery.  This has particular relevance to medical         imaging, as it is increasingly being used to describe intrinsically-varying          physiologic phenomena (e.g., blood flow, intra-operative conditions).  The           goals of the meeting are the following: to provide technology transfer among         the three groups and to demonstrate the complementary capabilities of the            groups.  This is especially important in the area of validation of algorithms,       and the consequent need for databases that permit that validation.  Among the        specific topics to be covered at this meeting are: Real-time event                   understanding,  Extraction of information from video, video compression and          decompression, and hand and body gesture recognition.  A five-year period of         support is requested, to enable the meeting to grow in participation and             breadth of disciplines attracted.                                                                                                                                                                                                                              n/a",Applied Imagery Pattern Recognition Workshop,6944025,R13CA093819,"['artificial intelligence', 'computer human interaction', 'videotape /videodisc', 'workshop']",NCI,GEORGE WASHINGTON UNIVERSITY,R13,2005,5000,-0.007571157664503994
"Applied Imagery Pattern Recognition Workshop   DESCRIPTION (Provided by Applicant):                                                 The Applied Imagery, Pattern Recognition (AIPR) Workshop is held annually in         Washington, D.C., at the Cosmos Club.  This workshop brings together 60-80           members of academia, industry, and other federal agencies, with a particular         history of involvement with the broader intelligence community in image              processing and analysis.  The workshop format for the last decade has                generally involved three days of high-quality presentations and investigator         interaction, both formally and informally at evening meeting receptions held         at the Cosmos Club.  The 30th meeting will be held from October 10-12, 2001,         and focus on Time-Varying Imagery.  This has particular relevance to medical         imaging, as it is increasingly being used to describe intrinsically-varying          physiologic phenomena (e.g., blood flow, intra-operative conditions).  The           goals of the meeting are the following: to provide technology transfer among         the three groups and to demonstrate the complementary capabilities of the            groups.  This is especially important in the area of validation of algorithms,       and the consequent need for databases that permit that validation.  Among the        specific topics to be covered at this meeting are: Real-time event                   understanding,  Extraction of information from video, video compression and          decompression, and hand and body gesture recognition.  A five-year period of         support is requested, to enable the meeting to grow in participation and             breadth of disciplines attracted.                                                                                                                                                                                                                              n/a",Applied Imagery Pattern Recognition Workshop,6793307,R13CA093819,"['artificial intelligence', 'computer human interaction', 'videotape /videodisc', 'workshop']",NCI,GEORGE WASHINGTON UNIVERSITY,R13,2004,5000,-0.007571157664503994
"Applied Imagery Pattern Recognition Workshop   DESCRIPTION (Provided by Applicant):                                                 The Applied Imagery, Pattern Recognition (AIPR) Workshop is held annually in         Washington, D.C., at the Cosmos Club.  This workshop brings together 60-80           members of academia, industry, and other federal agencies, with a particular         history of involvement with the broader intelligence community in image              processing and analysis.  The workshop format for the last decade has                generally involved three days of high-quality presentations and investigator         interaction, both formally and informally at evening meeting receptions held         at the Cosmos Club.  The 30th meeting will be held from October 10-12, 2001,         and focus on Time-Varying Imagery.  This has particular relevance to medical         imaging, as it is increasingly being used to describe intrinsically-varying          physiologic phenomena (e.g., blood flow, intra-operative conditions).  The           goals of the meeting are the following: to provide technology transfer among         the three groups and to demonstrate the complementary capabilities of the            groups.  This is especially important in the area of validation of algorithms,       and the consequent need for databases that permit that validation.  Among the        specific topics to be covered at this meeting are: Real-time event                   understanding,  Extraction of information from video, video compression and          decompression, and hand and body gesture recognition.  A five-year period of         support is requested, to enable the meeting to grow in participation and             breadth of disciplines attracted.                                                                                                                                                                                                                              n/a",Applied Imagery Pattern Recognition Workshop,6644867,R13CA093819,"['artificial intelligence', ' computer human interaction', ' videotape /videodisc', ' workshop']",NCI,GEORGE WASHINGTON UNIVERSITY,R13,2003,5000,-0.007571157664503994
"Applied Imagery Pattern Recognition Workshop   DESCRIPTION (Provided by Applicant):                                                 The Applied Imagery, Pattern Recognition (AIPR) Workshop is held annually in         Washington, D.C., at the Cosmos Club.  This workshop brings together 60-80           members of academia, industry, and other federal agencies, with a particular         history of involvement with the broader intelligence community in image              processing and analysis.  The workshop format for the last decade has                generally involved three days of high-quality presentations and investigator         interaction, both formally and informally at evening meeting receptions held         at the Cosmos Club.  The 30th meeting will be held from October 10-12, 2001,         and focus on Time-Varying Imagery.  This has particular relevance to medical         imaging, as it is increasingly being used to describe intrinsically-varying          physiologic phenomena (e.g., blood flow, intra-operative conditions).  The           goals of the meeting are the following: to provide technology transfer among         the three groups and to demonstrate the complementary capabilities of the            groups.  This is especially important in the area of validation of algorithms,       and the consequent need for databases that permit that validation.  Among the        specific topics to be covered at this meeting are: Real-time event                   understanding,  Extraction of information from video, video compression and          decompression, and hand and body gesture recognition.  A five-year period of         support is requested, to enable the meeting to grow in participation and             breadth of disciplines attracted.                                                                                                                                                                                                                              n/a",Applied Imagery Pattern Recognition Workshop,6522796,R13CA093819,"['artificial intelligence', ' computer human interaction', ' videotape /videodisc', ' workshop']",NCI,GEORGE WASHINGTON UNIVERSITY,R13,2002,5000,-0.007571157664503994
"Applied Imagery Pattern Recognition Workshop   DESCRIPTION (Provided by Applicant):                                                 The Applied Imagery, Pattern Recognition (AIPR) Workshop is held annually in         Washington, D.C., at the Cosmos Club.  This workshop brings together 60-80           members of academia, industry, and other federal agencies, with a particular         history of involvement with the broader intelligence community in image              processing and analysis.  The workshop format for the last decade has                generally involved three days of high-quality presentations and investigator         interaction, both formally and informally at evening meeting receptions held         at the Cosmos Club.  The 30th meeting will be held from October 10-12, 2001,         and focus on Time-Varying Imagery.  This has particular relevance to medical         imaging, as it is increasingly being used to describe intrinsically-varying          physiologic phenomena (e.g., blood flow, intra-operative conditions).  The           goals of the meeting are the following: to provide technology transfer among         the three groups and to demonstrate the complementary capabilities of the            groups.  This is especially important in the area of validation of algorithms,       and the consequent need for databases that permit that validation.  Among the        specific topics to be covered at this meeting are: Real-time event                   understanding,  Extraction of information from video, video compression and          decompression, and hand and body gesture recognition.  A five-year period of         support is requested, to enable the meeting to grow in participation and             breadth of disciplines attracted.                                                                                                                                                                                                                              n/a",Applied Imagery Pattern Recognition Workshop,6421360,R13CA093819,"['artificial intelligence', ' computer human interaction', ' videotape /videodisc', ' workshop']",NCI,GEORGE WASHINGTON UNIVERSITY,R13,2001,5000,-0.007571157664503994
"Estimating Neural Network Precision for Cancer Diagnosis   DESCRIPTION (Provided by Applicant): This application's broad, long-term             objective is to diagnose cancer accurately, reducing both false-negative and         false-positive diagnoses. The research proposed in this application concerns         artificial neural networks (ANNs) which are important statistical tools that         are used frequently in computer-aided diagnosis (CAD) methods intended to            improve cancer diagnosis. The goal of this research is to provide error bars         for ANN output. The significance and health-relatedness of this research is          that the goal, if achieved, could represent a fundamental advance to ANNs in         CAD applications, by making ANN output more reliable for subsequent computer         processing and more intuitive to understand for radiologists to interpret and        incorporate (the diagnostic predictions made by ANNs) in their own diagnoses.        The proposed method could become the standard of practice, replacing the             conventional, one-ANN approach. The hypothesis to be tested is that artificial       neural network output has finite uncertainty which can be estimated and              expressed in terms of confidence intervals. The specific aims are:                                                                                                        (1) To demonstrate qualitatively the concept of uncertainty in ANN output and        the feasibility of developing multiple ANNs from a single set of training            cases.                                                                                                                                                                    (2) To develop and validate quantitative method(s) of ANN-precision estimation.                                                                                           (3) To apply ANN-precision estimation to a real-world CAD problem: computer          classification of malignant and benign clustered microcalcifications.                                                                                                     The research design is to use primarily computer simulations to investigate          properties of the output from multiple ANNs obtained from the same training          cases and to develop and validate practical method(s) for computing confidence       intervals in ANN output from these multiple ANNs, then to apply the new methods      to a real-world CAD task. The methods to be used include computer simulation,        ROC analysis, computation of confidence intervals, bootstrapping, parametric         estimation of statistical distributions, analytical analysis, and computer           analysis of breast lesions in mammograms.                                                                                                                                 n/a",Estimating Neural Network Precision for Cancer Diagnosis,6620862,R21CA093989,"['artificial intelligence', ' breast neoplasm /cancer diagnosis', ' breast neoplasms', ' calcification', ' calcium disorder', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' female', ' human data', ' neoplasm /cancer classification /staging', "" women's health""]",NCI,UNIVERSITY OF CHICAGO,R21,2003,185585,-0.047886029561805765
"Estimating Neural Network Precision for Cancer Diagnosis   DESCRIPTION (Provided by Applicant): This application's broad, long-term             objective is to diagnose cancer accurately, reducing both false-negative and         false-positive diagnoses. The research proposed in this application concerns         artificial neural networks (ANNs) which are important statistical tools that         are used frequently in computer-aided diagnosis (CAD) methods intended to            improve cancer diagnosis. The goal of this research is to provide error bars         for ANN output. The significance and health-relatedness of this research is          that the goal, if achieved, could represent a fundamental advance to ANNs in         CAD applications, by making ANN output more reliable for subsequent computer         processing and more intuitive to understand for radiologists to interpret and        incorporate (the diagnostic predictions made by ANNs) in their own diagnoses.        The proposed method could become the standard of practice, replacing the             conventional, one-ANN approach. The hypothesis to be tested is that artificial       neural network output has finite uncertainty which can be estimated and              expressed in terms of confidence intervals. The specific aims are:                                                                                                        (1) To demonstrate qualitatively the concept of uncertainty in ANN output and        the feasibility of developing multiple ANNs from a single set of training            cases.                                                                                                                                                                    (2) To develop and validate quantitative method(s) of ANN-precision estimation.                                                                                           (3) To apply ANN-precision estimation to a real-world CAD problem: computer          classification of malignant and benign clustered microcalcifications.                                                                                                     The research design is to use primarily computer simulations to investigate          properties of the output from multiple ANNs obtained from the same training          cases and to develop and validate practical method(s) for computing confidence       intervals in ANN output from these multiple ANNs, then to apply the new methods      to a real-world CAD task. The methods to be used include computer simulation,        ROC analysis, computation of confidence intervals, bootstrapping, parametric         estimation of statistical distributions, analytical analysis, and computer           analysis of breast lesions in mammograms.                                                                                                                                 n/a",Estimating Neural Network Precision for Cancer Diagnosis,6422575,R21CA093989,"['artificial intelligence', ' breast neoplasm /cancer diagnosis', ' breast neoplasms', ' calcification', ' calcium disorder', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' female', ' human data', ' neoplasm /cancer classification /staging', "" women's health""]",NCI,UNIVERSITY OF CHICAGO,R21,2002,182408,-0.047886029561805765
"Multiscale modeling of G protein-coupled receptors DESCRIPTION (provided by applicant): The G protein-coupled receptors (GPCRs) are the largest family in the mammalian genome, and are critical to a number of cell signaling processes. As a result, they are of enormous biomedical importance; by some estimates, as many as 50% of new pharmaceuticals target GPCRs. Unsurprisingly, there has been a huge research investment in understanding their biophysics. However, integral membrane proteins are challenging to work with experimentally, leaving an opportunity for computational methods to make a significant contribution. We will use multiscale modeling techniques, including all-atom molecular dynamics simulations and elastic network models, to explore the behavior of several GPCRs, including rhodopsin (and its retinal-free form, opsin) and the �2-adrenergic receptor (B2AR). Specifically, we will investigate the role of ligand binding in modulating GPCR function, via two separate all-atom molecular dynamics calculations. Microsecond-scale simulations of opsin will, when contrasted with our previous work on rhodopsin in the dark state and during the early stages of activation, allow us to see which interactions in rhodopsin are determined by the presence of the ligand, while the planned simulations of the full activation process will give the first atomic-level view of the structural changes involved in GPCR activation; this knowledge could be critical to the design of novel inhibitors to other GPCRs. The second goal of this proposal is to clarify the role of internal waters in the activation mechanism of GPCRs; our previous simulations described significant increases in the internal hydration of rhodopsin and B2AR. Here, we propose to pursue those observations more rigorously, using automatic pattern recognition methods to correlate hydration changes with functionally interesting protein motions in simulations of rhodopsin, B2AR, and the cannabinoid-2 receptor (CB2). The third goal of the proposal is to develop elastic network models - a simple, computationally inexpensive approach where the protein's interactions are represented as a network of springs - in order to explore larger scale problems not readily amenable to all-atom molecular dynamics, like the modulation of protein motions by G protein binding and GPCR oligomerization. A number of possible network model implementations will be considered, and the models will be carefully validated by quantitative comparison to extensive molecular dynamics simulations, including those proposed for the first aim. The fourth and final goal of the proposal is to assess the validity of a common assumption, that rhodopsin is a good template for understanding GPCR activation in general. To test this hypothesis we will apply multiple computational methods, including long timescale molecular dynamics and elastic network models, to a series of GPCRs, including rhodopsin, opsin, B2AR, and CB2. We will quantitatively correlate the fluctuations of the different GPCRs, with the hypothesis that motions conserved across multiple GPCRs are likely to be functionally significant. G protein-coupled receptors are the largest family of proteins in the human genome, and are the most targeted proteins for therapeutics development. We will use multiscale modeling methods to connect their structure and dynamics to function, in order to aid in the design and refinement of novel drugs.",Multiscale modeling of G protein-coupled receptors,8895982,R01GM095496,"['ADRB2 gene', 'Adrenergic Receptor', 'Behavior', 'Biophysics', 'CNR2 gene', 'Cell Signaling Process', 'Collaborations', 'Computing Methodologies', 'Data', 'Dimerization', 'Drug Targeting', 'Elements', 'Event', 'Family', 'G-Protein-Coupled Receptors', 'GTP-Binding Proteins', 'Goals', 'Human Genome', 'Hydration status', 'Integral Membrane Protein', 'Investments', 'Knowledge', 'Left', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Membrane Proteins', 'Methods', 'Modeling', 'Motion', 'Opsin', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Play', 'Process', 'Protein Binding', 'Protein Family', 'Proteins', 'Receptor Activation', 'Research', 'Research Personnel', 'Resources', 'Retinal', 'Rhodopsin', 'Role', 'Running', 'Series', 'Staging', 'Structure', 'Techniques', 'Testing', 'Water', 'Work', 'cost', 'design', 'follow-up', 'inhibitor/antagonist', 'insight', 'interest', 'laptop', 'mammalian genome', 'metarhodopsin I', 'metarhodopsin II', 'molecular dynamics', 'multi-scale modeling', 'network models', 'novel', 'protein structure', 'receptor function', 'research study', 'signal processing', 'simulation', 'supercomputer', 'temporal measurement', 'therapeutic development']",NIGMS,UNIVERSITY OF ROCHESTER,R01,2015,262650,-0.05969663954138769
"Multiscale modeling of G protein-coupled receptors    DESCRIPTION (provided by applicant): The G protein-coupled receptors (GPCRs) are the largest family in the mammalian genome, and are critical to a number of cell signaling processes. As a result, they are of enormous biomedical importance; by some estimates, as many as 50% of new pharmaceuticals target GPCRs. Unsurprisingly, there has been a huge research investment in understanding their biophysics. However, integral membrane proteins are challenging to work with experimentally, leaving an opportunity for computational methods to make a significant contribution. We will use multiscale modeling techniques, including all-atom molecular dynamics simulations and elastic network models, to explore the behavior of several GPCRs, including rhodopsin (and its retinal-free form, opsin) and the ¿2-adrenergic receptor (B2AR). Specifically, we will investigate the role of ligand binding in modulating GPCR function, via two separate all-atom molecular dynamics calculations. Microsecond-scale simulations of opsin will, when contrasted with our previous work on rhodopsin in the dark state and during the early stages of activation, allow us to see which interactions in rhodopsin are determined by the presence of the ligand, while the planned simulations of the full activation process will give the first atomic-level view of the structural changes involved in GPCR activation; this knowledge could be critical to the design of novel inhibitors to other GPCRs. The second goal of this proposal is to clarify the role of internal waters in the activation mechanism of GPCRs; our previous simulations described significant increases in the internal hydration of rhodopsin and B2AR. Here, we propose to pursue those observations more rigorously, using automatic pattern recognition methods to correlate hydration changes with functionally interesting protein motions in simulations of rhodopsin, B2AR, and the cannabinoid-2 receptor (CB2). The third goal of the proposal is to develop elastic network models - a simple, computationally inexpensive approach where the protein's interactions are represented as a network of springs - in order to explore larger scale problems not readily amenable to all-atom molecular dynamics, like the modulation of protein motions by G protein binding and GPCR oligomerization. A number of possible network model implementations will be considered, and the models will be carefully validated by quantitative comparison to extensive molecular dynamics simulations, including those proposed for the first aim. The fourth and final goal of the proposal is to assess the validity of a common assumption, that rhodopsin is a good template for understanding GPCR activation in general. To test this hypothesis we will apply multiple computational methods, including long timescale molecular dynamics and elastic network models, to a series of GPCRs, including rhodopsin, opsin, B2AR, and CB2. We will quantitatively correlate the fluctuations of the different GPCRs, with the hypothesis that motions conserved across multiple GPCRs are likely to be functionally significant.        G protein-coupled receptors are the largest family of proteins in the human genome, and are the most targeted proteins for therapeutics development. We will use multiscale modeling methods to connect their structure and dynamics to function, in order to aid in the design and refinement of novel drugs.              ",Multiscale modeling of G protein-coupled receptors,8689100,R01GM095496,"['ADRB2 gene', 'Adrenergic Receptor', 'Behavior', 'Biophysics', 'Cannabinoids', 'Cell Signaling Process', 'Collaborations', 'Computing Methodologies', 'Data', 'Dimerization', 'Drug Targeting', 'Elements', 'Event', 'Family', 'G-Protein-Coupled Receptors', 'GTP-Binding Proteins', 'Goals', 'Human Genome', 'Hydration status', 'Integral Membrane Protein', 'Investments', 'Knowledge', 'Left', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Membrane Proteins', 'Methods', 'Modeling', 'Motion', 'Opsin', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Play', 'Process', 'Protein Binding', 'Protein Family', 'Proteins', 'Receptor Activation', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Retinal', 'Rhodopsin', 'Role', 'Running', 'Series', 'Staging', 'Structure', 'Techniques', 'Testing', 'Water', 'Work', 'cost', 'design', 'follow-up', 'inhibitor/antagonist', 'insight', 'interest', 'laptop', 'mammalian genome', 'metarhodopsin I', 'metarhodopsin II', 'molecular dynamics', 'multi-scale modeling', 'network models', 'novel', 'protein structure', 'receptor', 'receptor function', 'research study', 'signal processing', 'simulation', 'supercomputer', 'therapeutic development']",NIGMS,UNIVERSITY OF ROCHESTER,R01,2014,262650,-0.05969663954138769
"Multiscale modeling of G protein-coupled receptors    DESCRIPTION (provided by applicant): The G protein-coupled receptors (GPCRs) are the largest family in the mammalian genome, and are critical to a number of cell signaling processes. As a result, they are of enormous biomedical importance; by some estimates, as many as 50% of new pharmaceuticals target GPCRs. Unsurprisingly, there has been a huge research investment in understanding their biophysics. However, integral membrane proteins are challenging to work with experimentally, leaving an opportunity for computational methods to make a significant contribution. We will use multiscale modeling techniques, including all-atom molecular dynamics simulations and elastic network models, to explore the behavior of several GPCRs, including rhodopsin (and its retinal-free form, opsin) and the ¿2-adrenergic receptor (B2AR). Specifically, we will investigate the role of ligand binding in modulating GPCR function, via two separate all-atom molecular dynamics calculations. Microsecond-scale simulations of opsin will, when contrasted with our previous work on rhodopsin in the dark state and during the early stages of activation, allow us to see which interactions in rhodopsin are determined by the presence of the ligand, while the planned simulations of the full activation process will give the first atomic-level view of the structural changes involved in GPCR activation; this knowledge could be critical to the design of novel inhibitors to other GPCRs. The second goal of this proposal is to clarify the role of internal waters in the activation mechanism of GPCRs; our previous simulations described significant increases in the internal hydration of rhodopsin and B2AR. Here, we propose to pursue those observations more rigorously, using automatic pattern recognition methods to correlate hydration changes with functionally interesting protein motions in simulations of rhodopsin, B2AR, and the cannabinoid-2 receptor (CB2). The third goal of the proposal is to develop elastic network models - a simple, computationally inexpensive approach where the protein's interactions are represented as a network of springs - in order to explore larger scale problems not readily amenable to all-atom molecular dynamics, like the modulation of protein motions by G protein binding and GPCR oligomerization. A number of possible network model implementations will be considered, and the models will be carefully validated by quantitative comparison to extensive molecular dynamics simulations, including those proposed for the first aim. The fourth and final goal of the proposal is to assess the validity of a common assumption, that rhodopsin is a good template for understanding GPCR activation in general. To test this hypothesis we will apply multiple computational methods, including long timescale molecular dynamics and elastic network models, to a series of GPCRs, including rhodopsin, opsin, B2AR, and CB2. We will quantitatively correlate the fluctuations of the different GPCRs, with the hypothesis that motions conserved across multiple GPCRs are likely to be functionally significant.        G protein-coupled receptors are the largest family of proteins in the human genome, and are the most targeted proteins for therapeutics development. We will use multiscale modeling methods to connect their structure and dynamics to function, in order to aid in the design and refinement of novel drugs.              ",Multiscale modeling of G protein-coupled receptors,8502702,R01GM095496,"['ADRB2 gene', 'Adrenergic Receptor', 'Behavior', 'Biophysics', 'Cannabinoids', 'Cell Signaling Process', 'Collaborations', 'Computing Methodologies', 'Data', 'Dimerization', 'Drug Targeting', 'Elements', 'Event', 'Family', 'G-Protein-Coupled Receptors', 'GTP-Binding Proteins', 'Goals', 'Human Genome', 'Hydration status', 'Integral Membrane Protein', 'Investments', 'Knowledge', 'Left', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Membrane Proteins', 'Methods', 'Modeling', 'Motion', 'Opsin', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Play', 'Process', 'Protein Binding', 'Protein Family', 'Proteins', 'Receptor Activation', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Retinal', 'Rhodopsin', 'Role', 'Running', 'Series', 'Staging', 'Structure', 'Techniques', 'Testing', 'Water', 'Work', 'computerized data processing', 'cost', 'design', 'follow-up', 'inhibitor/antagonist', 'insight', 'interest', 'laptop', 'mammalian genome', 'metarhodopsin I', 'metarhodopsin II', 'molecular dynamics', 'multi-scale modeling', 'network models', 'novel', 'protein structure', 'receptor', 'receptor function', 'research study', 'simulation', 'supercomputer', 'therapeutic development']",NIGMS,UNIVERSITY OF ROCHESTER,R01,2013,253457,-0.05969663954138769
"Multiscale modeling of G protein-coupled receptors    DESCRIPTION (provided by applicant): The G protein-coupled receptors (GPCRs) are the largest family in the mammalian genome, and are critical to a number of cell signaling processes. As a result, they are of enormous biomedical importance; by some estimates, as many as 50% of new pharmaceuticals target GPCRs. Unsurprisingly, there has been a huge research investment in understanding their biophysics. However, integral membrane proteins are challenging to work with experimentally, leaving an opportunity for computational methods to make a significant contribution. We will use multiscale modeling techniques, including all-atom molecular dynamics simulations and elastic network models, to explore the behavior of several GPCRs, including rhodopsin (and its retinal-free form, opsin) and the ¿2-adrenergic receptor (B2AR). Specifically, we will investigate the role of ligand binding in modulating GPCR function, via two separate all-atom molecular dynamics calculations. Microsecond-scale simulations of opsin will, when contrasted with our previous work on rhodopsin in the dark state and during the early stages of activation, allow us to see which interactions in rhodopsin are determined by the presence of the ligand, while the planned simulations of the full activation process will give the first atomic-level view of the structural changes involved in GPCR activation; this knowledge could be critical to the design of novel inhibitors to other GPCRs. The second goal of this proposal is to clarify the role of internal waters in the activation mechanism of GPCRs; our previous simulations described significant increases in the internal hydration of rhodopsin and B2AR. Here, we propose to pursue those observations more rigorously, using automatic pattern recognition methods to correlate hydration changes with functionally interesting protein motions in simulations of rhodopsin, B2AR, and the cannabinoid-2 receptor (CB2). The third goal of the proposal is to develop elastic network models - a simple, computationally inexpensive approach where the protein's interactions are represented as a network of springs - in order to explore larger scale problems not readily amenable to all-atom molecular dynamics, like the modulation of protein motions by G protein binding and GPCR oligomerization. A number of possible network model implementations will be considered, and the models will be carefully validated by quantitative comparison to extensive molecular dynamics simulations, including those proposed for the first aim. The fourth and final goal of the proposal is to assess the validity of a common assumption, that rhodopsin is a good template for understanding GPCR activation in general. To test this hypothesis we will apply multiple computational methods, including long timescale molecular dynamics and elastic network models, to a series of GPCRs, including rhodopsin, opsin, B2AR, and CB2. We will quantitatively correlate the fluctuations of the different GPCRs, with the hypothesis that motions conserved across multiple GPCRs are likely to be functionally significant.        G protein-coupled receptors are the largest family of proteins in the human genome, and are the most targeted proteins for therapeutics development. We will use multiscale modeling methods to connect their structure and dynamics to function, in order to aid in the design and refinement of novel drugs.              ",Multiscale modeling of G protein-coupled receptors,8324207,R01GM095496,"['ADRB2 gene', 'Adrenergic Receptor', 'Behavior', 'Biophysics', 'Cannabinoids', 'Cell Signaling Process', 'Collaborations', 'Computing Methodologies', 'Data', 'Dimerization', 'Drug Delivery Systems', 'Elements', 'Event', 'Family', 'G-Protein-Coupled Receptors', 'GTP-Binding Proteins', 'Goals', 'Human Genome', 'Hydration status', 'Integral Membrane Protein', 'Investments', 'Knowledge', 'Left', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Membrane Proteins', 'Methods', 'Modeling', 'Motion', 'Opsin', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Play', 'Process', 'Protein Binding', 'Protein Family', 'Proteins', 'Receptor Activation', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Retinal', 'Rhodopsin', 'Role', 'Running', 'Series', 'Staging', 'Structure', 'Techniques', 'Testing', 'Water', 'Work', 'computerized data processing', 'cost', 'design', 'follow-up', 'inhibitor/antagonist', 'insight', 'interest', 'laptop', 'mammalian genome', 'metarhodopsin I', 'metarhodopsin II', 'molecular dynamics', 'multi-scale modeling', 'network models', 'novel', 'protein structure', 'receptor', 'receptor function', 'research study', 'simulation', 'supercomputer', 'therapeutic development']",NIGMS,UNIVERSITY OF ROCHESTER,R01,2012,262650,-0.05969663954138769
"Multiscale modeling of G protein-coupled receptors    DESCRIPTION (provided by applicant): The G protein-coupled receptors (GPCRs) are the largest family in the mammalian genome, and are critical to a number of cell signaling processes. As a result, they are of enormous biomedical importance; by some estimates, as many as 50% of new pharmaceuticals target GPCRs. Unsurprisingly, there has been a huge research investment in understanding their biophysics. However, integral membrane proteins are challenging to work with experimentally, leaving an opportunity for computational methods to make a significant contribution. We will use multiscale modeling techniques, including all-atom molecular dynamics simulations and elastic network models, to explore the behavior of several GPCRs, including rhodopsin (and its retinal-free form, opsin) and the ¿2-adrenergic receptor (B2AR). Specifically, we will investigate the role of ligand binding in modulating GPCR function, via two separate all-atom molecular dynamics calculations. Microsecond-scale simulations of opsin will, when contrasted with our previous work on rhodopsin in the dark state and during the early stages of activation, allow us to see which interactions in rhodopsin are determined by the presence of the ligand, while the planned simulations of the full activation process will give the first atomic-level view of the structural changes involved in GPCR activation; this knowledge could be critical to the design of novel inhibitors to other GPCRs. The second goal of this proposal is to clarify the role of internal waters in the activation mechanism of GPCRs; our previous simulations described significant increases in the internal hydration of rhodopsin and B2AR. Here, we propose to pursue those observations more rigorously, using automatic pattern recognition methods to correlate hydration changes with functionally interesting protein motions in simulations of rhodopsin, B2AR, and the cannabinoid-2 receptor (CB2). The third goal of the proposal is to develop elastic network models - a simple, computationally inexpensive approach where the protein's interactions are represented as a network of springs - in order to explore larger scale problems not readily amenable to all-atom molecular dynamics, like the modulation of protein motions by G protein binding and GPCR oligomerization. A number of possible network model implementations will be considered, and the models will be carefully validated by quantitative comparison to extensive molecular dynamics simulations, including those proposed for the first aim. The fourth and final goal of the proposal is to assess the validity of a common assumption, that rhodopsin is a good template for understanding GPCR activation in general. To test this hypothesis we will apply multiple computational methods, including long timescale molecular dynamics and elastic network models, to a series of GPCRs, including rhodopsin, opsin, B2AR, and CB2. We will quantitatively correlate the fluctuations of the different GPCRs, with the hypothesis that motions conserved across multiple GPCRs are likely to be functionally significant.      PUBLIC HEALTH RELEVANCE: G protein-coupled receptors are the largest family of proteins in the human genome, and are the most targeted proteins for therapeutics development. We will use multiscale modeling methods to connect their structure and dynamics to function, in order to aid in the design and refinement of novel drugs.                G protein-coupled receptors are the largest family of proteins in the human genome, and are the most targeted proteins for therapeutics development. We will use multiscale modeling methods to connect their structure and dynamics to function, in order to aid in the design and refinement of novel drugs.              ",Multiscale modeling of G protein-coupled receptors,8020805,R01GM095496,"['ADRB2 gene', 'Adrenergic Receptor', 'Behavior', 'Biophysics', 'Cannabinoids', 'Cell Signaling Process', 'Collaborations', 'Computing Methodologies', 'Data', 'Dimerization', 'Drug Delivery Systems', 'Elements', 'Event', 'Family', 'G-Protein-Coupled Receptors', 'GTP-Binding Proteins', 'Goals', 'Human Genome', 'Hydration status', 'Integral Membrane Protein', 'Investments', 'Knowledge', 'Left', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Membrane Proteins', 'Methods', 'Modeling', 'Motion', 'Opsin', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Play', 'Process', 'Protein Binding', 'Protein Family', 'Proteins', 'Receptor Activation', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Retinal', 'Rhodopsin', 'Role', 'Running', 'Series', 'Staging', 'Structure', 'Techniques', 'Testing', 'Water', 'Work', 'computerized data processing', 'cost', 'design', 'follow-up', 'inhibitor/antagonist', 'insight', 'interest', 'laptop', 'mammalian genome', 'metarhodopsin I', 'metarhodopsin II', 'molecular dynamics', 'multi-scale modeling', 'network models', 'novel', 'protein structure', 'receptor', 'receptor function', 'research study', 'simulation', 'supercomputer', 'therapeutic development']",NIGMS,UNIVERSITY OF ROCHESTER,R01,2011,251069,-0.06028986644709854
"Modulation of Lung Disease by Genetic/Epigenetic Profiling Project Summary/Abstract Therapeutic management of lung disorders hallmarked by the loss-of-function of the Cystic Fibrosis (CF) Transmembrane conductance Regulator (CFTR) leading to CF are challenged by genetic and epigenetic diversity found in the CF population. Given the Precision Medicine Initiative (All of Us for You (https://allofus.nih.gov/) and the large amount of genomic and phenomic diversity found in patients, it is now generally recognized that we must find new approaches to address the complexity in CF presentation in the clinic. This will require an understanding of fundamental principles dictating disease onset at birth, defined by familial genetic variation, and its progression, influenced by epigenetic programs, both unique to the individual. This proposal is about understanding the role of genetic and epigenetic diversity in CF in response to Histone DeACetylase (HDAC) activity. We have shown these relationships to be responsive to the activity of HDACs, proteins that manage the acetylation/deacetylation balance of the genome and the proteome (the epigenome) to integrate the complex functions linking the genome to the proteome and phenome. Based on the premise that the genome and epigenome are sensitive to manipulation(s) that will favor increased functionality of the CFTR variant fold, the objective of this proposal is to mechanistically define the impact of HDAC modulation on CFTR function observed at the bench and the bedside. We hypothesize that CF can be best understood based on the rationale that disease can be defined by the collective of variation found in the CF population that alters CFTR sequence-to-function-to-structure relationships in the individual as now described using Variation Spatial Profiling (VSP) and the new principle of Spatial CoVariance (SCV) (Wang and Balch, 2018, In press). It is the objective of this proposal to apply VSP/SCV to analysis of the role of the epigenome in CF. Key goals to be achieved in this proposal are to 1) define molecular, cellular and physiological states that 2) describe the role of genetic/epigenetic/proteomic diversity in the CF population to 3) provide a sequence-to-function-to-structure characterization of disease in the individual. Aim 1 will explore the impact of HDAC inhibitors (HDACi) to define, from a biochemical/genetic diversity perspective, how variation across the entire CF population will respond to rebalancing of acetylation/deacetylation dynamics. Aim 2 will focus on the role of HDAC7 in the management of CF genetic diversity using molecular, biochemical and cellular approaches. Aim 3 will analyze the role of select HDAC7-sensitive CFTR interactors to address their role in the management of CF variation from an epigenetic perspective. We hypothesize that the completion of these Aims will describe relationships in the population that define the epigenome-linked genome features that impact progression of CF in the individual. Our integrated genome/epigenome/proteome platform will advance our understanding of the contribution of genetic diversity in the progression and management of CF as a complex disease. Project Narrative CF is a complex loss-of-function disease caused by genetic and epigenetic variation in the Cystic Fibrosis Transmembrane conductance Regulator (CFTR). We will focus on understanding spatial relationships defined by genetic diversity across the CF population that are sensitive to Histone DeACetylase (HDAC) activity to understand the role of the acetylation/deacetylation balance in facilitating function in the individual. We will use a combination of genomic/epigenomic/proteomic approaches based on the principles of Variation Spatial Profiling (VSP) and Spatial CoVariance (SCV) to dissect the role of HDAC in integrated pathways that affect CFTR variant synthesis, folding, trafficking and stability/function at the cell surface that may be responsive to chemical and/or biological manipulation of the epigenome.",Modulation of Lung Disease by Genetic/Epigenetic Profiling,9928091,R01HL095524,"['Acetylation', 'Address', 'Affect', 'Amino Acids', 'Automobile Driving', 'Biochemical', 'Biochemical Genetics', 'Biological', 'Biology', 'Birth', 'Cell Death', 'Cell surface', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Collection', 'Complex', 'Cystic Fibrosis', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Deacetylation', 'Disease', 'Disease Progression', 'Environment', 'Epigenetic Process', 'Equilibrium', 'Fibrosis', 'Funding', 'Gaussian model', 'Genetic', 'Genetic Diseases', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'HDAC7 histone deacetylase', 'Health', 'Histone Deacetylase', 'Histone Deacetylase Inhibitor', 'Human', 'Immune', 'Individual', 'Inflammatory Response', 'Lead', 'Link', 'Lung diseases', 'Machine Learning', 'Membrane', 'Mendelian disorder', 'Modification', 'Molecular', 'Mucous body substance', 'Onset of illness', 'Pathology', 'Pathway interactions', 'Patients', 'Phenotype', 'Physiological', 'Physiology', 'Population', 'Precision Medicine Initiative', 'Process', 'Proteins', 'Proteome', 'Proteomics', 'Publications', 'Role', 'Structure', 'System', 'Therapeutic', 'Tissues', 'Variant', 'base', 'bench to bedside', 'cystic fibrosis patients', 'epigenetic profiling', 'epigenetic variation', 'epigenome', 'epigenomics', 'genomic platform', 'healthspan', 'insight', 'loss of function', 'novel strategies', 'phenome', 'phenomics', 'programs', 'response', 'spatial relationship', 'success', 'trafficking', 'transcription factor']",NHLBI,SCRIPPS RESEARCH INSTITUTE,R01,2020,483750,-0.07355765395211022
"Modulation of Lung Disease by Genetic/Epigenetic Profiling Project Summary/Abstract Therapeutic management of lung disorders hallmarked by the loss-of-function of the Cystic Fibrosis (CF) Transmembrane conductance Regulator (CFTR) leading to CF are challenged by genetic and epigenetic diversity found in the CF population. Given the Precision Medicine Initiative (All of Us for You (https://allofus.nih.gov/) and the large amount of genomic and phenomic diversity found in patients, it is now generally recognized that we must find new approaches to address the complexity in CF presentation in the clinic. This will require an understanding of fundamental principles dictating disease onset at birth, defined by familial genetic variation, and its progression, influenced by epigenetic programs, both unique to the individual. This proposal is about understanding the role of genetic and epigenetic diversity in CF in response to Histone DeACetylase (HDAC) activity. We have shown these relationships to be responsive to the activity of HDACs, proteins that manage the acetylation/deacetylation balance of the genome and the proteome (the epigenome) to integrate the complex functions linking the genome to the proteome and phenome. Based on the premise that the genome and epigenome are sensitive to manipulation(s) that will favor increased functionality of the CFTR variant fold, the objective of this proposal is to mechanistically define the impact of HDAC modulation on CFTR function observed at the bench and the bedside. We hypothesize that CF can be best understood based on the rationale that disease can be defined by the collective of variation found in the CF population that alters CFTR sequence-to-function-to-structure relationships in the individual as now described using Variation Spatial Profiling (VSP) and the new principle of Spatial CoVariance (SCV) (Wang and Balch, 2018, In press). It is the objective of this proposal to apply VSP/SCV to analysis of the role of the epigenome in CF. Key goals to be achieved in this proposal are to 1) define molecular, cellular and physiological states that 2) describe the role of genetic/epigenetic/proteomic diversity in the CF population to 3) provide a sequence-to-function-to-structure characterization of disease in the individual. Aim 1 will explore the impact of HDAC inhibitors (HDACi) to define, from a biochemical/genetic diversity perspective, how variation across the entire CF population will respond to rebalancing of acetylation/deacetylation dynamics. Aim 2 will focus on the role of HDAC7 in the management of CF genetic diversity using molecular, biochemical and cellular approaches. Aim 3 will analyze the role of select HDAC7-sensitive CFTR interactors to address their role in the management of CF variation from an epigenetic perspective. We hypothesize that the completion of these Aims will describe relationships in the population that define the epigenome-linked genome features that impact progression of CF in the individual. Our integrated genome/epigenome/proteome platform will advance our understanding of the contribution of genetic diversity in the progression and management of CF as a complex disease. Project Narrative CF is a complex loss-of-function disease caused by genetic and epigenetic variation in the Cystic Fibrosis Transmembrane conductance Regulator (CFTR). We will focus on understanding spatial relationships defined by genetic diversity across the CF population that are sensitive to Histone DeACetylase (HDAC) activity to understand the role of the acetylation/deacetylation balance in facilitating function in the individual. We will use a combination of genomic/epigenomic/proteomic approaches based on the principles of Variation Spatial Profiling (VSP) and Spatial CoVariance (SCV) to dissect the role of HDAC in integrated pathways that affect CFTR variant synthesis, folding, trafficking and stability/function at the cell surface that may be responsive to chemical and/or biological manipulation of the epigenome.",Modulation of Lung Disease by Genetic/Epigenetic Profiling,9739114,R01HL095524,"['Acetylation', 'Address', 'Affect', 'Amino Acids', 'Automobile Driving', 'Biochemical', 'Biochemical Genetics', 'Biological', 'Biology', 'Birth', 'Cell Death', 'Cell surface', 'Cells', 'Chemicals', 'Clinic', 'Clinical', 'Collection', 'Complex', 'Cystic Fibrosis', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Deacetylation', 'Disease', 'Disease Progression', 'Environment', 'Epigenetic Process', 'Equilibrium', 'Fibrosis', 'Funding', 'Gaussian model', 'Genetic', 'Genetic Diseases', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'HDAC7 histone deacetylase', 'Health', 'Histone Deacetylase', 'Histone Deacetylase Inhibitor', 'Human', 'Immune', 'Individual', 'Inflammatory Response', 'Lead', 'Link', 'Lung diseases', 'Machine Learning', 'Membrane', 'Mendelian disorder', 'Modification', 'Molecular', 'Mucous body substance', 'Onset of illness', 'Pathology', 'Pathway interactions', 'Patients', 'Phenotype', 'Physiological', 'Physiology', 'Population', 'Precision Medicine Initiative', 'Process', 'Proteins', 'Proteome', 'Proteomics', 'Publications', 'Role', 'Structure', 'System', 'Therapeutic', 'Tissues', 'Variant', 'base', 'bench to bedside', 'cystic fibrosis patients', 'epigenetic profiling', 'epigenetic variation', 'epigenome', 'epigenomics', 'genomic platform', 'healthspan', 'insight', 'loss of function', 'novel strategies', 'phenome', 'phenomics', 'programs', 'response', 'spatial relationship', 'success', 'trafficking', 'transcription factor']",NHLBI,SCRIPPS RESEARCH INSTITUTE,R01,2019,483750,-0.07355765395211022
"Advaced Cross-Correlator Development of a general purpose ultrasound cross-correlator module that is proposed for a) blood flow estimation in one, two and three dimensions b) blood flow estimation in an overlapped mode for use in high frequency small vessels c) coded excitation deconvolution d) A-Mode tissue characteristic correlation quantification The module would be capable of accepting Digitized RF data at rates up to 4o million 12 bit samples per second from a beamformer and returning the Sum of the Products (SOP) of multiple selectable ranges of up to 48 samples with a theoretical accuracy of 1/128th of a sample in the range dimension and a dynamic range of 36 bits in the intensity dimension at the rate of the input data. Multiple results based upon the SOP would also be output. The chosen algorithms would be loaded through a Firewire interface to a personal computer (PC) in a sub second rates. The correlator modules would output its results again through the Firewire interface into the PC for further image optimization and viewing. Initially the module would be tested with the company's Beamformer but efforts would be made to offer a universal interface so researchers could utilize the computing power of the correlator on other instruments. It is also intended to make available the parameters of the algorithms for researchers to use this tool for further developments. PROPOSED COMMERCIAL APPLICATIONS: This proposed tool would be applicable in the research then clinical evaluation of true three-dimensional real time blood flow in the major vessels of the body down to the capillary vessels and in tissue flow as in angiogenesis. The correlator potentially will be used in improving he dynamic range, and quality of ultrasonic imaging. A possible application to be investigated is the correlators potential for recognizing tissue characteristics in real time. n/a",Advaced Cross-Correlator,6479214,R43CA096018,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer human interaction', ' computer network', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' digital imaging', ' phantom model', ' radiowave radiation', ' ultrasound blood flow measurement']",NCI,WINPROBE CORPORATION,R43,2002,127797,0.0013595825416733915
"Robust biomimetic models of human legs to solve high-dimensional real-time control problems Project Summary  Anatomically validated musculoskeletal models of human limbs computed faster than real-time are tools that will help advance the control of neuroprosthetics, rehabilitation, and the study of motor control principles. However, the current state-of-the-art models cannot be both accurate and fast. We propose to develop a new generation of validated real-time human leg models with musculoskeletal dynamics that are robust over the full range of multidimensional motion. The first aim is to validate a lower-limb model in the full range of static postures. The second aim is to validate a lower-limb model during dynamic locomotor tasks.  Building on our previous work, we will use OpenSim model repository as a starting point for the iterative process of validating the muscle anatomy and function using published anatomical data. We expect to recreate the full range of leg postures with the validated model. We will then collect data during locomotor tasks performed by healthy humans on the split-belt treadmill with simultaneous re- cordings of ground reaction forces, full-body motion capture, and surface electromyography from rep- resentative leg muscles. The model will be validated over a rich experimental dataset for locomotor pat- terns required in asymmetric stepping on a self-paced treadmill. We expect to validate the dynamic model by estimating in real-time the observed full body kinematics from muscle activity and ground reaction forces. The inverse solutions will allow us to estimate the ongoing spatiotemporal patterns of muscle activity.  At the conclusion of this study we will develop the detailed lower-limb model with high-di- mensional robust muscle path simulations to predict limb motion in real-time. The outcomes of this proposal will inform future work on the use of the real-time musculoskeletal models for the develop- ment of augmentation devices and the clinical assessment of locomotor deficits. Project Narrative The significance of this project is that it will address the major challenges in achieving intuitive human-computer interactions by allowing the simulation of high-dimensional muscle dynamics in real-time. The main innovation of this proposal is the rigorous validation of human leg model across large anthropomorphic variations (due to age and sex) and the robust performance using novel method for musculoskeletal simulations.",Robust biomimetic models of human legs to solve high-dimensional real-time control problems,9979392,R03HD099426,"['Address', 'Age', 'Anatomy', 'Articular Range of Motion', 'Basic Science', 'Behavioral', 'Biological', 'Biomimetics', 'Clinical', 'Clinical assessments', 'Data', 'Data Set', 'Development', 'Devices', 'Electric Stimulation', 'Electromyography', 'Engineering', 'Ensure', 'Evaluation', 'Exercise Physiology', 'Failure', 'Funding Opportunities', 'Future', 'Generations', 'Goals', 'Grant', 'Hand', 'Human', 'Intuition', 'Joints', 'Leg', 'Length', 'Limb Prosthesis', 'Limb structure', 'Lower Extremity', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Morphology', 'Motion', 'Movement', 'Muscle', 'Musculoskeletal', 'Musculoskeletal System', 'Neuromechanics', 'Orthopedics', 'Outcome', 'Participant', 'Pathway interactions', 'Pattern', 'Performance', 'Physical therapy', 'Polynomial Models', 'Posture', 'Process', 'Prosthesis', 'Publishing', 'Reaction', 'Rehabilitation therapy', 'Reproducibility', 'Research', 'Signal Transduction', 'Slide', 'Speed', 'Surface', 'Technology', 'Testing', 'Time', 'Upper Extremity', 'Validation', 'Variant', 'Work', 'arm', 'base', 'computer human interaction', 'high dimensionality', 'human model', 'innovation', 'kinematics', 'locomotor deficit', 'locomotor tasks', 'model development', 'motor control', 'neuroprosthesis', 'novel', 'real time model', 'relating to nervous system', 'repository', 'sex', 'simulation', 'spatiotemporal', 'tool', 'treadmill']",NICHD,WEST VIRGINIA UNIVERSITY,R03,2020,76000,-0.003978968113879334
"Perception and Inter-Observer Variability in Mammography DESCRIPTION (provided by applicant):    Inter-observer variability in mammogram reading has been well documented in the literature. Various factors have been used to explain this variability; among them, the most significant are related to the management of perceived findings.  However, the nature of this inter-observer variability has not been explored. Namely, were the lesions that were consistently reported by the radiologists any different from the ones that yield disagreement? Furthermore, could these differences be quantitatively assessed? Moreover, were these differences in any way related with the experience level of the observer? In addition, the interpretation of perceived findings is closely related with the visual search strategy used to scan the breast tissue, because observers compare perceived findings with the background, in order to determine their uniqueness. Hence, what is the effect of visual search strategy on inter-observer variability? Can this effect be modeled using Artificial Neural Networks (ANNs)? Can inferences be made regarding the observers' decision patterns by analyzing the results of simulations run on the ANNs?  The work described here aims at answering these questions. We will use spatial frequency analysis to characterize the areas on mammogram cases where mammographers, chest radiologists with experience reading mammograms and radiology residents at the end of their mammography rotation, indicate the presence of a finding, or fail to do so. We will assess inter-observer agreement, as well as intra- and inter-group agreement for the various groups of observers. In addition, we will train artificial neural networks to represent each observer, in such a way that by changing the nature of the features input to the ANNs we will be able to simulate how such changes would have affected the actual observer. We will assess the effects on inter-observer variability of changing the search strategy used by the observer to sample the breast tissue. In our setting, the inter-observer variability will be assessed by comparing the outputs of the ANNs that represent each observer. In addition, the changes in sampling strategy will correspond to actual possible strategies for the human observers themselves. n/a",Perception and Inter-Observer Variability in Mammography,6924688,R21CA100107,"['artificial intelligence', 'bioimaging /biomedical imaging', 'breast neoplasm /cancer diagnosis', 'clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'human data', 'human therapy evaluation', 'mammography', 'visual perception']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2005,167063,-0.07741582412805423
"Perception and Inter-Observer Variability in Mammography DESCRIPTION (provided by applicant):    Inter-observer variability in mammogram reading has been well documented in the literature. Various factors have been used to explain this variability; among them, the most significant are related to the management of perceived findings.  However, the nature of this inter-observer variability has not been explored. Namely, were the lesions that were consistently reported by the radiologists any different from the ones that yield disagreement? Furthermore, could these differences be quantitatively assessed? Moreover, were these differences in any way related with the experience level of the observer? In addition, the interpretation of perceived findings is closely related with the visual search strategy used to scan the breast tissue, because observers compare perceived findings with the background, in order to determine their uniqueness. Hence, what is the effect of visual search strategy on inter-observer variability? Can this effect be modeled using Artificial Neural Networks (ANNs)? Can inferences be made regarding the observers' decision patterns by analyzing the results of simulations run on the ANNs?  The work described here aims at answering these questions. We will use spatial frequency analysis to characterize the areas on mammogram cases where mammographers, chest radiologists with experience reading mammograms and radiology residents at the end of their mammography rotation, indicate the presence of a finding, or fail to do so. We will assess inter-observer agreement, as well as intra- and inter-group agreement for the various groups of observers. In addition, we will train artificial neural networks to represent each observer, in such a way that by changing the nature of the features input to the ANNs we will be able to simulate how such changes would have affected the actual observer. We will assess the effects on inter-observer variability of changing the search strategy used by the observer to sample the breast tissue. In our setting, the inter-observer variability will be assessed by comparing the outputs of the ANNs that represent each observer. In addition, the changes in sampling strategy will correspond to actual possible strategies for the human observers themselves. n/a",Perception and Inter-Observer Variability in Mammography,6821032,R21CA100107,"['artificial intelligence', 'bioimaging /biomedical imaging', 'breast neoplasm /cancer diagnosis', 'clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'human data', 'human therapy evaluation', 'mammography', 'visual perception']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2004,153968,-0.07741582412805423
"Modeling and Assessment of Maternal Uteroplacental Circulation During Pregnancy ABSTRACT In this proposal I am interested in developing a computational fluid dynamic (CFD) modeling approach for volumetric assessment of human uteroplacental blood flow in vivo. The placenta is an organ that exchanges nutrients and oxygen between the maternal circulation and the growing fetus. In the United States, about 3.4% of pregnancies per year are affected by hypertensive pregnancy disorders (HPD) such as preeclampsia (PE), which have been found to carry both fetal and maternal risk. Associated placental pathologies are believed to be linked to alterations in maternal arterial remodeling. Currently, Doppler ultrasound (US) is the primary method of assessing flow to the placenta from the uterine artery (UtA). Clinical studies have shown a relationship between high UtA flow resistance and risk of adverse pregnancy outcome late in gestation, but it has not been reliable as an early gestation screening tool. In order to improve predictive technologies for reliable risk assessment assessment of HPD, more research into the relationship between vessel structure and function in the UtA is needed. I hypothesize that CFD modeling can be a useful tool for investigating possible pathophysiological mechanisms of HPD by simulating complex hemodynamics of the maternal vascular system including pressure, wall shear stress, and pulse wave velocity. I plan to set up various 1D CFD simulations to understand the hemodynamic parameters of normal versus abnormal pregnancies. Then, I will validate the simulations with 4D flow MRI acquired from an in vitro flow phantom and a cohort of normal and hypertensive pregnant women. I anticipate that the results of this investigation can advance scientific knowledge regarding the progression of early HPD phenotypes to adverse pregnancy outcomes. This CFD study can also demonstrate the extent to which patient-specific hemodynamic simulations can be reliable for future improvement of clinical management. This training experience will provide opportunities to build my expertise in cardiovascular physiology, fluid mechanics, medical imaging, data analysis, and clinical care. I will be publishing articles on my CFD/4D flow MRI findings and communicating the impact of this work in medicine at various internal and external symposia. This research will be conducted under the mentorship of award-winning experts and the University of Pennsylvania in cardiovascular MRI (Walter Witschey), fluid mechanics and computational modeling (Paris Perdikaris), maternal fetal medicine (Nadav Schwartz) and cardiovascular physiology (Victor Ferrari). PROJECT NARRATIVE The human placenta is an understudied organ believed to play a critical role in pregnancy health and the pathophysiology of hypertensive pregnancy disorders. Current technologies like ultrasound are limited in its ability to assess complex hemodynamics, and clinical studies have not yet found reliable biomarkers of adverse pregnancy outcomes. We will develop a computational fluid dynamic model of the maternal vessels delivering blood to the placenta and validate it with magnetic resonance imaging to characterize uteroplacental structure and function, providing future avenues for new predictive biomarkers of pregnancy health.",Modeling and Assessment of Maternal Uteroplacental Circulation During Pregnancy,9991192,F31HD100171,"['3-Dimensional', '4D MRI', 'Abdomen', 'Affect', 'Agreement', 'American College of Obstetricians and Gynecologists', 'Angiography', 'Arteries', 'Award', 'Bayesian Analysis', 'Biological Markers', 'Blood', 'Blood Circulation', 'Blood Vessels', 'Blood flow', 'Cardiovascular Physiology', 'Cardiovascular system', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Complex', 'Computer Models', 'Data', 'Data Analyses', 'Development', 'Disease', 'Doppler Ultrasound', 'Early Diagnosis', 'Electric Capacitance', 'Engineering', 'Environment', 'Fetus', 'Functional disorder', 'Future', 'Geometry', 'Goals', 'Health', 'Heterogeneity', 'Human', 'In Vitro', 'Intervention', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Link', 'Liquid substance', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maternal-fetal medicine', 'Measurement', 'Measures', 'Mechanics', 'Medical Imaging', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Monitor', 'Morphology', 'Navier–Stokes equations', 'Nutrient', 'Organ', 'Outcome', 'Oxygen', 'Paris, France', 'Pathologic', 'Pathology', 'Patients', 'Pennsylvania', 'Phenotype', 'Physics', 'Physiologic pulse', 'Physiological', 'Physiology', 'Placenta', 'Placenta Diseases', 'Placentation', 'Play', 'Pre-Eclampsia', 'Predictive Value', 'Pregnancy', 'Pregnancy Outcome', 'Pregnant Women', 'Property', 'Publishing', 'Radiology Specialty', 'Reporting', 'Reproducibility', 'Research', 'Resistance', 'Resolution', 'Risk', 'Risk Assessment', 'Role', 'Scientific Advances and Accomplishments', 'Scientist', 'Screening procedure', 'Structure', 'Techniques', 'Technology', 'Testing', 'Texture', 'Third Pregnancy Trimester', 'Training', 'Tube', 'Ultrasonography', 'Uncertainty', 'United States', 'Universities', 'Uteroplacental Circulation', 'Uterus', 'Variant', 'Vascular System', 'Velocimetries', 'Woman', 'Work', 'adverse pregnancy outcome', 'arterial remodeling', 'base', 'cardiovascular imaging', 'clinical care', 'cohort', 'disorder risk', 'experience', 'falls', 'fetal', 'hemodynamics', 'high risk', 'imaging approach', 'improved', 'in vivo', 'insight', 'interest', 'maternal risk', 'neural network', 'non-invasive imaging', 'phantom model', 'predictive marker', 'pregnancy disorder', 'pressure', 'screening', 'shear stress', 'simulation', 'spatiotemporal', 'symposium', 'tool']",NICHD,UNIVERSITY OF PENNSYLVANIA,F31,2020,45520,0.005776322642048062
"Retinal eye-tracking for the prognosis and monitoring of multiple sclerosis Abstract The goal of this project is to validate an innovative, highly sensitive retinal eye-tracking technology, the tracking scanning laser ophthalmoscope (TSLO), as a prognostic and monitoring tool for neurodegenerative disorders, namely multiple sclerosis (MS). The applications of effective treatments for multiple sclerosis are constrained by (1) the absence of methods for early detection and (2) quantitative, highly sensitive methods monitoring deficits early in disease course when treatment may have a better chance of success. As already demonstrated, the TSLO is capable of rapidly assessing and measuring the extraordinarily fine, microscopic motion of the human eye during fixation in MS patients. Fixational eye movements are neurally-encoded, involuntary movements that require the coordination of many areas of the central nervous system. Given the TSLO’s theoretical sensitivity to change (0.2 arcminutes) and its precision of measurement - fixational eye movements have the potential utility for tracking neurodegenerative disease progression at an unprecedented scale. With the advent of the new FDA-approved MS treatment targeting B-cells (ocrelizumab), clinical tools are now desperately needed to not only assess treatment efficacy, but to objectively assess patient disability at the earliest stage of disease in order to cut relapse rates and prevent irrevocable disability. In this project, we will determine the optimal fixational eye motion metrics to distinguish patients from controls, establish the relationship between clinical disease severity measures and fixational eye movement deficits as defined by the TSLO system, and to use machine learning algorithms to further strengthen our fixational metrics. Narrative The goal of our project is to clinically validate a retinal imaging and eye-tracking technology, the tracking scanning laser ophthalmoscope (TSLO), as a prognostic and monitoring tool for neurodegenerative disorders, particularly Multiple Sclerosis (MS). The TSLO system is capable of rapidly assessing and measuring the extraordinarily fine, microscopic motion of the human eye during fixation with an accuracy of 0.2 arcminutes. Given the numerous brain regions involved with eye motion and the TSLO’s precision of measurement - fixational eye movements now have the potential utility for tracking neurodegenerative disease progression for the MS patient population.",Retinal eye-tracking for the prognosis and monitoring of multiple sclerosis,9465579,R41NS100222,"['Age', 'Algorithms', 'Area', 'B-Lymphocytes', 'Brain', 'Brain region', 'Characteristics', 'Clinical', 'Cognition', 'Color Visions', 'Databases', 'Digit structure', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Early Intervention', 'Electrocardiogram', 'Eye', 'Eye Movements', 'FDA approved', 'Fatigue', 'Fingers', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Involuntary Movements', 'Lasers', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Microscopic', 'Modality', 'Monitor', 'Motion', 'Movement', 'Multiple Sclerosis', 'Muscle', 'Nerve Degeneration', 'Neuraxis', 'Neurodegenerative Disorders', 'Neurons', 'Ophthalmoscopes', 'Patient Care Team', 'Patients', 'Physicians', 'Pilot Projects', 'Population', 'Population Control', 'Relapse', 'Retinal', 'Running', 'Scanning', 'Series', 'Severity of illness', 'Signal Transduction', 'Small Business Technology Transfer Research', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Treatment Efficacy', 'Vision', 'Visual Fields', 'Visual system structure', 'Walking', 'base', 'cohort', 'cost', 'disability', 'effective therapy', 'foot', 'innovation', 'instrument', 'multiple sclerosis patient', 'multiple sclerosis treatment', 'neuroadaptation', 'oculomotor', 'outcome forecast', 'patient population', 'prevent', 'prognostic', 'programs', 'receptive field', 'relating to nervous system', 'retinal imaging', 'sample fixation', 'standard measure', 'success', 'tool', 'walking speed']",NINDS,"C. LIGHT TECHNOLOGIES, INC.",R41,2018,225392,-0.04703984034887221
"Label-free RF Imaging of Cell Membrane Heterogeneity in Liquid DESCRIPTION (provided by applicant): Cell membrane heterogeneity, such as raft (-like) domains, plays an essential and active role in various cellular processes and pathogeneses. Yet, the heterogeneity is poorly understood. A main difficulty is the lack of appropriate techniques for molecular organization examination of living cell plasma membranes. Current technical approaches often need labeling, which induce artifacts, such as photo-oxidation. Their temporal and spatial resolutions are also often limited. In this project, label-free, non-intrusive radio frequency (RF) scanning techniques will be developed to fill the instrumentation gap. The obtained RF scanners have 100-nm or better spatial resolution and ~ 10 μs temporal resolution. The relative concentration levels of cholesterol, n-3 polyunsaturated fatty acids (PUFA) and sphingomyelin (SPM) in raft (-like) lipid domains of living B cells will be obtained.  The functionality of the developed ultra-sensitive RF scanners is based on measuring local capacitance and conductance of membrane organizations at multiple frequencies. It is hypothesized that measurements over a wide-frequency-range capture molecular- and structure-specific information, such as the relaxation time and dispersion of molecular organizations. Then inverse effective-medium-theory enables the identification and analysis of the targeted molecular components and structures. Thus, the obtained RF specificity enables label-free and non-intrusive imaging. To achieve sufficient RF sensitivity, it is hypothesized that an interference process eliminates RF probing waves at the output-port while preserving minute membrane domain signals. The two working hypotheses are based on the PI's preliminary results and the results from other research groups. To test the central hypothesis, which consists of the two working hypotheses, super- sensitive, high-resolution, multi-frequency RF scanners will be designed, fabricated, and tested. In addition to hydraulic approaches, a 3D electrode structure will be included for electrical manipulation and control of cell positions for F scanning. To identify the quantitative relationship between plasma membrane organizations and their RF properties, the raft (-like) domains of ternary giant-unilamellar-vesicle (GUV) lipid bilayers and B cell plasma membranes will be characterized. The domains will be modified by adjusting molecular compositions during synthesis for GUVs and feeding n-3 polyunsaturated fatty acids of different doses for B cells. Comparisons with the results obtained from conventional imaging and molecular composition analysis methods will be conducted to help verify the RF techniques.  The main contribution of the proposed research is a label-free, non-invasive and ultra-sensitive RF scanning technique with RF specificity for living cell membrane heterogeneity studies. The spatial resolution is 100 nm or better, and the temporal resolution is ~10 μs. Additionally, the concentration of cholesterol, SPM, and n-3 PUFA in the lipid raft (-like) domains of living B cells will be obtained with the RF scanners. The RF scanners provide a critical technique for quantitative characterization of living-cell-plasma-membrane heterogeneity and facilitate the understanding of diverse cellular processes and pathogeneses. The obtained knowledge on B cells helps identify new mechanisms through which n-3 PUFAs exert their effects.",Label-free RF Imaging of Cell Membrane Heterogeneity in Liquid,9130189,K25GM100480,"['B-Lymphocytes', 'Back', 'Biological Models', 'Biology', 'Cell Differentiation process', 'Cell Membrane Structures', 'Cell membrane', 'Cell physiology', 'Cells', 'Cholesterol', 'Complement', 'Data', 'Detergents', 'Disease', 'Dose', 'Electric Capacitance', 'Electrodes', 'Engineering', 'Frequencies', 'Goals', 'HIV', 'Heterogeneity', 'Image', 'Imaging Device', 'Investigation', 'Knowledge', 'Label', 'Life', 'Lipid Bilayers', 'Lipids', 'Liquid substance', 'Measurement', 'Measures', 'Mediating', 'Medical', 'Membrane', 'Membrane Fusion', 'Membrane Lipids', 'Membrane Microdomains', 'Membrane Structure and Function', 'Methods', 'Molecular', 'Molecular Structure', 'Molecular Target', 'Morphologic artifacts', 'N-3 polyunsaturated fatty acid', 'Outcome', 'Output', 'Pathogenesis', 'Physics', 'Play', 'Polyunsaturated Fatty Acids', 'Positioning Attribute', 'Process', 'Property', 'Proteins', 'Publications', 'Relaxation', 'Research', 'Resolution', 'Role', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'Spectrum Analysis', 'Sphingomyelins', 'Structure', 'Support Groups', 'Suspension substance', 'Suspensions', 'Techniques', 'Testing', 'Time', 'Vesicle', 'Virus', 'Work', 'Yeasts', 'base', 'design', 'electrical property', 'experience', 'feeding', 'information organization', 'insight', 'instrument', 'instrumentation', 'mathematical model', 'membrane model', 'molecular imaging', 'nanometer', 'optical imaging', 'oxidation', 'particle', 'radio frequency', 'sensor', 'single molecule', 'temporal measurement', 'theories', 'unilamellar vesicle', 'web site']",NIGMS,CLEMSON UNIVERSITY,K25,2016,144703,0.023681202502901438
"Label-free RF Imaging of Cell Membrane Heterogeneity in Liquid DESCRIPTION (provided by applicant): Cell membrane heterogeneity, such as raft (-like) domains, plays an essential and active role in various cellular processes and pathogeneses. Yet, the heterogeneity is poorly understood. A main difficulty is the lack of appropriate techniques for molecular organization examination of living cell plasma membranes. Current technical approaches often need labeling, which induce artifacts, such as photo-oxidation. Their temporal and spatial resolutions are also often limited. In this project, label-free, non-intrusive radio frequency (RF) scanning techniques will be developed to fill the instrumentation gap. The obtained RF scanners have 100-nm or better spatial resolution and ~ 10 μs temporal resolution. The relative concentration levels of cholesterol, n-3 polyunsaturated fatty acids (PUFA) and sphingomyelin (SPM) in raft (-like) lipid domains of living B cells will be obtained.  The functionality of the developed ultra-sensitive RF scanners is based on measuring local capacitance and conductance of membrane organizations at multiple frequencies. It is hypothesized that measurements over a wide-frequency-range capture molecular- and structure-specific information, such as the relaxation time and dispersion of molecular organizations. Then inverse effective-medium-theory enables the identification and analysis of the targeted molecular components and structures. Thus, the obtained RF specificity enables label-free and non-intrusive imaging. To achieve sufficient RF sensitivity, it is hypothesized that an interference process eliminates RF probing waves at the output-port while preserving minute membrane domain signals. The two working hypotheses are based on the PI's preliminary results and the results from other research groups. To test the central hypothesis, which consists of the two working hypotheses, super- sensitive, high-resolution, multi-frequency RF scanners will be designed, fabricated, and tested. In addition to hydraulic approaches, a 3D electrode structure will be included for electrical manipulation and control of cell positions for F scanning. To identify the quantitative relationship between plasma membrane organizations and their RF properties, the raft (-like) domains of ternary giant-unilamellar-vesicle (GUV) lipid bilayers and B cell plasma membranes will be characterized. The domains will be modified by adjusting molecular compositions during synthesis for GUVs and feeding n-3 polyunsaturated fatty acids of different doses for B cells. Comparisons with the results obtained from conventional imaging and molecular composition analysis methods will be conducted to help verify the RF techniques.  The main contribution of the proposed research is a label-free, non-invasive and ultra-sensitive RF scanning technique with RF specificity for living cell membrane heterogeneity studies. The spatial resolution is 100 nm or better, and the temporal resolution is ~10 μs. Additionally, the concentration of cholesterol, SPM, and n-3 PUFA in the lipid raft (-like) domains of living B cells will be obtained with the RF scanners. The RF scanners provide a critical technique for quantitative characterization of living-cell-plasma-membrane heterogeneity and facilitate the understanding of diverse cellular processes and pathogeneses. The obtained knowledge on B cells helps identify new mechanisms through which n-3 PUFAs exert their effects.",Label-free RF Imaging of Cell Membrane Heterogeneity in Liquid,8918671,K25GM100480,"['B-Lymphocytes', 'Back', 'Biological Models', 'Biology', 'Cell Differentiation process', 'Cell Membrane Structures', 'Cell membrane', 'Cell physiology', 'Cells', 'Cholesterol', 'Complement', 'Data', 'Detergents', 'Disease', 'Dose', 'Electric Capacitance', 'Electrodes', 'Engineering', 'Frequencies', 'Goals', 'HIV', 'Heterogeneity', 'Image', 'Investigation', 'Knowledge', 'Label', 'Life', 'Lipid Bilayers', 'Lipids', 'Liquid substance', 'Measurement', 'Measures', 'Mediating', 'Medical', 'Membrane', 'Membrane Fusion', 'Membrane Lipids', 'Membrane Microdomains', 'Membrane Structure and Function', 'Methods', 'Molecular', 'Molecular Structure', 'Molecular Target', 'Morphologic artifacts', 'N-3 polyunsaturated fatty acid', 'Outcome', 'Output', 'Pathogenesis', 'Physics', 'Play', 'Polyunsaturated Fatty Acids', 'Positioning Attribute', 'Process', 'Property', 'Proteins', 'Publications', 'Relative (related person)', 'Relaxation', 'Research', 'Resolution', 'Role', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'Spectrum Analysis', 'Sphingomyelins', 'Structure', 'Support Groups', 'Suspension substance', 'Suspensions', 'Techniques', 'Testing', 'Time', 'Vesicle', 'Virus', 'Work', 'Yeasts', 'base', 'design', 'electrical property', 'experience', 'feeding', 'information organization', 'insight', 'instrument', 'instrumentation', 'mathematical model', 'membrane model', 'molecular imaging', 'nanometer', 'optical imaging', 'oxidation', 'particle', 'radio frequency', 'sensor', 'single molecule', 'spectroscopic imaging', 'temporal measurement', 'theories', 'unilamellar vesicle', 'web site']",NIGMS,CLEMSON UNIVERSITY,K25,2015,144703,0.023681202502901438
"Label-free RF Imaging of Cell Membrane Heterogeneity in Liquid     DESCRIPTION (provided by applicant): Cell membrane heterogeneity, such as raft (-like) domains, plays an essential and active role in various cellular processes and pathogeneses. Yet, the heterogeneity is poorly understood. A main difficulty is the lack of appropriate techniques for molecular organization examination of living cell plasma membranes. Current technical approaches often need labeling, which induce artifacts, such as photo-oxidation. Their temporal and spatial resolutions are also often limited. In this project, label-free, non-intrusive radio frequency (RF) scanning techniques will be developed to fill the instrumentation gap. The obtained RF scanners have 100-nm or better spatial resolution and ~ 10 ?s temporal resolution. The relative concentration levels of cholesterol, n-3 polyunsaturated fatty acids (PUFA) and sphingomyelin (SPM) in raft (-like) lipid domains of living B cells will be obtained.  The functionality of the developed ultra-sensitive RF scanners is based on measuring local capacitance and conductance of membrane organizations at multiple frequencies. It is hypothesized that measurements over a wide-frequency-range capture molecular- and structure-specific information, such as the relaxation time and dispersion of molecular organizations. Then inverse effective-medium-theory enables the identification and analysis of the targeted molecular components and structures. Thus, the obtained RF specificity enables label-free and non-intrusive imaging. To achieve sufficient RF sensitivity, it is hypothesized that an interference process eliminates RF probing waves at the output-port while preserving minute membrane domain signals. The two working hypotheses are based on the PI's preliminary results and the results from other research groups. To test the central hypothesis, which consists of the two working hypotheses, super- sensitive, high-resolution, multi-frequency RF scanners will be designed, fabricated, and tested. In addition to hydraulic approaches, a 3D electrode structure will be included for electrical manipulation and control of cell positions for F scanning. To identify the quantitative relationship between plasma membrane organizations and their RF properties, the raft (-like) domains of ternary giant-unilamellar-vesicle (GUV) lipid bilayers and B cell plasma membranes will be characterized. The domains will be modified by adjusting molecular compositions during synthesis for GUVs and feeding n-3 polyunsaturated fatty acids of different doses for B cells. Comparisons with the results obtained from conventional imaging and molecular composition analysis methods will be conducted to help verify the RF techniques.  The main contribution of the proposed research is a label-free, non-invasive and ultra-sensitive RF scanning technique with RF specificity for living cell membrane heterogeneity studies. The spatial resolution is 100 nm or better, and the temporal resolution is ~10 ?s. Additionally, the concentration of cholesterol, SPM, and n-3 PUFA in the lipid raft (-like) domains of living B cells will be obtained with the RF scanners.          The RF scanners provide a critical technique for quantitative characterization of living-cell-plasma-membrane heterogeneity and facilitate the understanding of diverse cellular processes and pathogeneses. The obtained knowledge on B cells helps identify new mechanisms through which n-3 PUFAs exert their effects.            ",Label-free RF Imaging of Cell Membrane Heterogeneity in Liquid,8733177,K25GM100480,"['B-Lymphocytes', 'Back', 'Biological Models', 'Biology', 'Cell Differentiation process', 'Cell Membrane Structures', 'Cell membrane', 'Cell physiology', 'Cells', 'Cholesterol', 'Complement', 'Data', 'Detergents', 'Disease', 'Dose', 'Electric Capacitance', 'Electrodes', 'Engineering', 'Frequencies', 'Goals', 'HIV', 'Heterogeneity', 'Image', 'Investigation', 'Knowledge', 'Label', 'Life', 'Lipid Bilayers', 'Lipids', 'Liquid substance', 'Measurement', 'Measures', 'Mediating', 'Medical', 'Membrane', 'Membrane Fusion', 'Membrane Lipids', 'Membrane Microdomains', 'Membrane Structure and Function', 'Methods', 'Molecular', 'Molecular Structure', 'Molecular Target', 'Morphologic artifacts', 'N-3 polyunsaturated fatty acid', 'Outcome', 'Output', 'Pathogenesis', 'Physics', 'Play', 'Polyunsaturated Fatty Acids', 'Positioning Attribute', 'Process', 'Property', 'Proteins', 'Publications', 'Radio', 'Relative (related person)', 'Relaxation', 'Research', 'Resolution', 'Role', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'Spectrum Analysis', 'Sphingomyelins', 'Structure', 'Support Groups', 'Suspension substance', 'Suspensions', 'Techniques', 'Testing', 'Time', 'Virus', 'Work', 'Yeasts', 'base', 'design', 'electrical property', 'experience', 'feeding', 'information organization', 'insight', 'instrument', 'instrumentation', 'mathematical model', 'membrane model', 'molecular imaging', 'nanometer', 'optical imaging', 'oxidation', 'particle', 'sensor', 'single molecule', 'theories', 'unilamellar vesicle', 'web site']",NIGMS,CLEMSON UNIVERSITY,K25,2014,145103,0.023681202502901438
"Label-free RF Imaging of Cell Membrane Heterogeneity in Liquid     DESCRIPTION (provided by applicant): Cell membrane heterogeneity, such as raft (-like) domains, plays an essential and active role in various cellular processes and pathogeneses. Yet, the heterogeneity is poorly understood. A main difficulty is the lack of appropriate techniques for molecular organization examination of living cell plasma membranes. Current technical approaches often need labeling, which induce artifacts, such as photo-oxidation. Their temporal and spatial resolutions are also often limited. In this project, label-free, non-intrusive radio frequency (RF) scanning techniques will be developed to fill the instrumentation gap. The obtained RF scanners have 100-nm or better spatial resolution and ~ 10 ?s temporal resolution. The relative concentration levels of cholesterol, n-3 polyunsaturated fatty acids (PUFA) and sphingomyelin (SPM) in raft (-like) lipid domains of living B cells will be obtained.  The functionality of the developed ultra-sensitive RF scanners is based on measuring local capacitance and conductance of membrane organizations at multiple frequencies. It is hypothesized that measurements over a wide-frequency-range capture molecular- and structure-specific information, such as the relaxation time and dispersion of molecular organizations. Then inverse effective-medium-theory enables the identification and analysis of the targeted molecular components and structures. Thus, the obtained RF specificity enables label-free and non-intrusive imaging. To achieve sufficient RF sensitivity, it is hypothesized that an interference process eliminates RF probing waves at the output-port while preserving minute membrane domain signals. The two working hypotheses are based on the PI's preliminary results and the results from other research groups. To test the central hypothesis, which consists of the two working hypotheses, super- sensitive, high-resolution, multi-frequency RF scanners will be designed, fabricated, and tested. In addition to hydraulic approaches, a 3D electrode structure will be included for electrical manipulation and control of cell positions for F scanning. To identify the quantitative relationship between plasma membrane organizations and their RF properties, the raft (-like) domains of ternary giant-unilamellar-vesicle (GUV) lipid bilayers and B cell plasma membranes will be characterized. The domains will be modified by adjusting molecular compositions during synthesis for GUVs and feeding n-3 polyunsaturated fatty acids of different doses for B cells. Comparisons with the results obtained from conventional imaging and molecular composition analysis methods will be conducted to help verify the RF techniques.  The main contribution of the proposed research is a label-free, non-invasive and ultra-sensitive RF scanning technique with RF specificity for living cell membrane heterogeneity studies. The spatial resolution is 100 nm or better, and the temporal resolution is ~10 ?s. Additionally, the concentration of cholesterol, SPM, and n-3 PUFA in the lipid raft (-like) domains of living B cells will be obtained with the RF scanners.          The RF scanners provide a critical technique for quantitative characterization of living-cell-plasma-membrane heterogeneity and facilitate the understanding of diverse cellular processes and pathogeneses. The obtained knowledge on B cells helps identify new mechanisms through which n-3 PUFAs exert their effects.            ",Label-free RF Imaging of Cell Membrane Heterogeneity in Liquid,8536874,K25GM100480,"['B-Lymphocytes', 'Back', 'Biological Models', 'Biology', 'Cell Differentiation process', 'Cell Membrane Structures', 'Cell membrane', 'Cell physiology', 'Cells', 'Cholesterol', 'Complement', 'Data', 'Detergents', 'Disease', 'Dose', 'Electric Capacitance', 'Electrodes', 'Engineering', 'Frequencies', 'Goals', 'HIV', 'Heterogeneity', 'Image', 'Investigation', 'Knowledge', 'Label', 'Life', 'Lipid Bilayers', 'Lipids', 'Liquid substance', 'Measurement', 'Measures', 'Mediating', 'Medical', 'Membrane', 'Membrane Fusion', 'Membrane Lipids', 'Membrane Microdomains', 'Membrane Structure and Function', 'Methods', 'Molecular', 'Molecular Structure', 'Molecular Target', 'Morphologic artifacts', 'N-3 polyunsaturated fatty acid', 'Outcome', 'Output', 'Pathogenesis', 'Physics', 'Play', 'Polyunsaturated Fatty Acids', 'Positioning Attribute', 'Process', 'Property', 'Proteins', 'Publications', 'Radio', 'Relative (related person)', 'Relaxation', 'Research', 'Resolution', 'Role', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'Spectrum Analysis', 'Sphingomyelins', 'Structure', 'Support Groups', 'Suspension substance', 'Suspensions', 'Techniques', 'Testing', 'Time', 'Virus', 'Work', 'Yeasts', 'base', 'design', 'electrical property', 'experience', 'feeding', 'information organization', 'insight', 'instrument', 'instrumentation', 'mathematical model', 'membrane model', 'molecular imaging', 'nanometer', 'optical imaging', 'oxidation', 'particle', 'sensor', 'single molecule', 'theories', 'unilamellar vesicle', 'web site']",NIGMS,CLEMSON UNIVERSITY,K25,2013,145503,0.023681202502901438
"Label-free RF Imaging of Cell Membrane Heterogeneity in Liquid     DESCRIPTION (provided by applicant): Cell membrane heterogeneity, such as raft (-like) domains, plays an essential and active role in various cellular processes and pathogeneses. Yet, the heterogeneity is poorly understood. A main difficulty is the lack of appropriate techniques for molecular organization examination of living cell plasma membranes. Current technical approaches often need labeling, which induce artifacts, such as photo-oxidation. Their temporal and spatial resolutions are also often limited. In this project, label-free, non-intrusive radio frequency (RF) scanning techniques will be developed to fill the instrumentation gap. The obtained RF scanners have 100-nm or better spatial resolution and ~ 10 ?s temporal resolution. The relative concentration levels of cholesterol, n-3 polyunsaturated fatty acids (PUFA) and sphingomyelin (SPM) in raft (-like) lipid domains of living B cells will be obtained.  The functionality of the developed ultra-sensitive RF scanners is based on measuring local capacitance and conductance of membrane organizations at multiple frequencies. It is hypothesized that measurements over a wide-frequency-range capture molecular- and structure-specific information, such as the relaxation time and dispersion of molecular organizations. Then inverse effective-medium-theory enables the identification and analysis of the targeted molecular components and structures. Thus, the obtained RF specificity enables label-free and non-intrusive imaging. To achieve sufficient RF sensitivity, it is hypothesized that an interference process eliminates RF probing waves at the output-port while preserving minute membrane domain signals. The two working hypotheses are based on the PI's preliminary results and the results from other research groups. To test the central hypothesis, which consists of the two working hypotheses, super- sensitive, high-resolution, multi-frequency RF scanners will be designed, fabricated, and tested. In addition to hydraulic approaches, a 3D electrode structure will be included for electrical manipulation and control of cell positions for F scanning. To identify the quantitative relationship between plasma membrane organizations and their RF properties, the raft (-like) domains of ternary giant-unilamellar-vesicle (GUV) lipid bilayers and B cell plasma membranes will be characterized. The domains will be modified by adjusting molecular compositions during synthesis for GUVs and feeding n-3 polyunsaturated fatty acids of different doses for B cells. Comparisons with the results obtained from conventional imaging and molecular composition analysis methods will be conducted to help verify the RF techniques.  The main contribution of the proposed research is a label-free, non-invasive and ultra-sensitive RF scanning technique with RF specificity for living cell membrane heterogeneity studies. The spatial resolution is 100 nm or better, and the temporal resolution is ~10 ?s. Additionally, the concentration of cholesterol, SPM, and n-3 PUFA in the lipid raft (-like) domains of living B cells will be obtained with the RF scanners.        PUBLIC HEALTH RELEVANCE: The RF scanners provide a critical technique for quantitative characterization of living-cell-plasma-membrane heterogeneity and facilitate the understanding of diverse cellular processes and pathogeneses. The obtained knowledge on B cells helps identify new mechanisms through which n-3 PUFAs exert their effects.              The RF scanners provide a critical technique for quantitative characterization of living-cell-plasma-membrane heterogeneity and facilitate the understanding of diverse cellular processes and pathogeneses. The obtained knowledge on B cells helps identify new mechanisms through which n-3 PUFAs exert their effects.            ",Label-free RF Imaging of Cell Membrane Heterogeneity in Liquid,8383194,K25GM100480,"['B-Lymphocytes', 'Back', 'Biological Models', 'Biology', 'Cell Differentiation process', 'Cell Membrane Structures', 'Cell membrane', 'Cell physiology', 'Cells', 'Cholesterol', 'Complement', 'Data', 'Detergents', 'Disease', 'Dose', 'Electric Capacitance', 'Electrodes', 'Engineering', 'Frequencies', 'Goals', 'HIV', 'Heterogeneity', 'Image', 'Investigation', 'Knowledge', 'Label', 'Life', 'Lipid Bilayers', 'Lipids', 'Liquid substance', 'Measurement', 'Measures', 'Mediating', 'Medical', 'Membrane', 'Membrane Fusion', 'Membrane Lipids', 'Membrane Microdomains', 'Membrane Structure and Function', 'Methods', 'Molecular', 'Molecular Structure', 'Molecular Target', 'Morphologic artifacts', 'N-3 polyunsaturated fatty acid', 'Outcome', 'Output', 'Pathogenesis', 'Physics', 'Play', 'Polyunsaturated Fatty Acids', 'Positioning Attribute', 'Process', 'Property', 'Proteins', 'Publications', 'Radio', 'Relative (related person)', 'Relaxation', 'Research', 'Resolution', 'Role', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'Spectrum Analysis', 'Sphingomyelins', 'Structure', 'Support Groups', 'Suspension substance', 'Suspensions', 'Techniques', 'Testing', 'Time', 'Virus', 'Work', 'Yeasts', 'base', 'design', 'electrical property', 'experience', 'feeding', 'information organization', 'insight', 'instrument', 'instrumentation', 'mathematical model', 'membrane model', 'molecular imaging', 'nanometer', 'optical imaging', 'oxidation', 'particle', 'sensor', 'single molecule', 'theories', 'unilamellar vesicle', 'web site']",NIGMS,CLEMSON UNIVERSITY,K25,2012,145503,0.02123169347029003
"Fast motion-robust fetal neuroimaging with MRI PROJECT SUMMARY/ABSTRACT Fetal-brain magnetic resonance imaging (MRI) has become an invaluable tool for studying the early development of the brain and can resolve diagnostic ambiguities that may remain after routine ultrasound exams. Unfortunately, high levels of fetal and maternal motion (1) limit fetal MRI to rapid two-dimensional (2D) sequences and frequently introduce dramatic artifacts such as (2) image misorientation relative to the standard sagittal, coronal, axial planes needed for clinical assessment and (3) partial to complete signal loss. These factors lead to the inefficient practice of repeating ~30 s stack-of-slices acquisitions until motion-free images have been obtained. Throughout the session, technologists manually adjust the orientation of scans in response to motion, and about 38% of datasets are typically discarded. Thus, subject motion is the fundamental impediment to reaping the full benefits of MRI for answering clinical and investigational questions in the fetus. The overarching goal of this project is to overcome the challenges posed by motion by exploiting innovations in deep learning, which have enabled image-analysis algorithms with unprecedented speed and reliability. We propose to integrate these into the MRI acquisition pipeline to unlock the potential of fetal MRI. We will develop practical pulse-sequence technology for automated and dynamically motion-corrected fetal neuroimaging without the need for external hardware or calibration. We hypothesize that this will radically improve the quality and success rates of clinical and research studies, while dramatically reducing patient discomfort and cost. We propose as Aim 1 to eradicate (2) the vulnerability of acquisitions to image-brain misorientation with rapid, automated prescription of standard anatomical planes. In Aim 2, we propose to address (3) motion during the scan with real-time correction of fetal-head motion. An anatomical stack-of-slices acquisition will be interleaved with volumetric navigators. These will be used to measure motion as it happens in the scanner and to adaptively update the slice tilt/position. We propose as Aim 3 to develop a 3D radial sequence and estimate motion between subsets of radial spokes for real-time self-navigation. Adaptively updating the orientation of spokes and selectively re-acquiring corrupted subsets at the end of the scan will enable 3D imaging of the fetal brain (1). Since the applicant has a physics background, the proposed training program at MIT and HMS will focus on deep learning and fetal development/neuroscience during the K99 phase to develop the skills needed for transitioning to independence in the R00 phase. The applicant’s goal is to become a fetal image acquisition and analysis scientist acting as bridge between deep learning, MRI and clinical fetal-imaging applications to shift the boundaries of what is currently possible with state-of-the-art technology. Fulfilling the research aims will promote this, as it will result in a practical framework for automation and motion correction, applicable to a wide variety of fetal neuroimaging sequences. PROJECT NARRATIVE Subject motion is the fundamental impediment to reaping the full benefits of fetal-brain magnetic resonance imaging, as it frequently produces images with dramatic artifacts. The goal of this project is to exploit innovations in deep learning and integrate them into the acquisition pipeline to overcome the challenges posed by motion in fetal neuroimaging studies. This will be achieved by using fast, automated scan prescription of standard anatomical planes and by adaptively updating the acquisition as motion happens in the scanner, based on sub-second navigator scans interleaved with the imaging sequence.",Fast motion-robust fetal neuroimaging with MRI,9950474,K99HD101553,"['3-Dimensional', 'Address', 'Algorithmic Analysis', 'Amniotic Fluid', 'Anatomy', 'Automation', 'Brain', 'Brain imaging', 'Calibration', 'Childhood', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Data Set', 'Development', 'Diagnostic', 'Echo-Planar Imaging', 'Fetal Development', 'Fetus', 'Geometry', 'Goals', 'Head', 'Image', 'Individual', 'Label', 'Lead', 'Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Masks', 'Measures', 'Morphologic artifacts', 'Motion', 'Neurosciences', 'Patients', 'Phase', 'Physics', 'Physiologic pulse', 'Population', 'Positioning Attribute', 'Radial', 'Recording of previous events', 'Research', 'Residual state', 'Resolution', 'Sampling', 'Scanning', 'Scientist', 'Signal Transduction', 'Slice', 'Speed', 'Technology', 'Thick', 'Three-Dimensional Imaging', 'Time', 'Tissues', 'Training', 'Training Programs', 'Translating', 'Ultrasonography', 'Update', 'Work', 'base', 'clinical investigation', 'convolutional neural network', 'cost', 'deep learning', 'echo detection', 'experience', 'fetal', 'image archival system', 'improved', 'innovation', 'interest', 'neuroimaging', 'novel', 'prospective', 'radiologist', 'reconstruction', 'repaired', 'research study', 'response', 'skills', 'success', 'tool', 'two-dimensional']",NICHD,MASSACHUSETTS GENERAL HOSPITAL,K99,2020,108601,-0.02739288727261015
"Multi-Criteria IMRT Optimization    DESCRIPTION (provided by applicant): The overall long-term objective of this project is to incorporate clinical decisions through interactive feedback into the inverse treatment planning process for intensity-modulated radiotherapy. The hypotheses are that this will (i) make the inverse planning process more effective and (ii) increase the clinical relevancy of optimized plans, introducing better tradeoffs between target coverage and sparing of healthy tissues. The new approaches to tackle this problem include multi-criteria optimization techniques and an interactive plan navigation tool for searching a pre-calculated treatment plan database. The idea of multi-criteria optimization in treatment planning is that multiple planning criteria in different critical structures and in the target volume can be controlled simultaneously. In contrast, in current inverse planning algorithms a single objective (score function) is maximized or minimized. This conventional optimization gives only limited control of the planning result, and major manual plan tweaking using trial and error is often necessary. In the previous funding period the feasibility of the approach has been demonstrated in theoretical example cases and retrospective treatment planning studies. This showed the potential of the multi-criteria planning approach and a potential for improvement in some areas. The first aim of the current project is to improve the usability of the method. This will include a reduction of the dimensionality of the problem, to make the tradeoff decision easier and faster for the clinician. It will also include the generation of plan databases with minimal user intervention, and developments of the plan navigation approach. The second aim is to extend functionality. Hardware and delivery aspects will be incorporated. A means to interactively optimize beam orientations will be provided. Dose conformality will be included in the tradeoff discussion, and the user will be able to make tradeoff decisions based on alternative biologically motivated criteria. Finally, in aim 3, the hypothesis that the multi-criteria planning paradigm will lead to better treatment plans in less time will be tested in the clinic. This work has the potential to lead to clinically more suitable and more individualized radiation treatment plans, with better dose coverage of the tumor target volume and/or reduced radiation dose to surrounding healthy tissues. Furthermore, shorter treatment planning times will reduce the overall cost. PUBLIC HEALTH RELEVANCE: This work has the potential to lead to clinically more suitable and more individualized radiation treatment plans, with better dose coverage of the tumor target volume and/or reduced radiation dose to surrounding healthy tissues. Furthermore, shorter treatment planning times will reduce the overall cost.          n/a",Multi-Criteria IMRT Optimization,8129771,R01CA103904,"['Algorithms', 'Area', 'Brain', 'Clinic', 'Clinical', 'Collimator', 'Complex', 'Complication', 'Conflict (Psychology)', 'Data', 'Databases', 'Development Plans', 'Dimensions', 'Dose', 'Feedback', 'Funding', 'Generations', 'Goals', 'Head and neck structure', 'Health', 'Hour', 'Housing', 'Human', 'Intensity-Modulated Radiotherapy', 'Intervention', 'Joints', 'Lead', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Normal tissue morphology', 'Pancreas', 'Patients', 'Physicians', 'Plant Leaves', 'Principal Component Analysis', 'Probability', 'Procedures', 'Process', 'Radiation', 'Radiation therapy', 'Resistance', 'Site', 'Slide', 'Solutions', 'Structure', 'Surface', 'System', 'Tail', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Weight', 'Work', 'base', 'cost', 'improved', 'interest', 'novel strategies', 'therapy development', 'tool', 'touchscreen', 'treatment planning', 'treatment site', 'tumor', 'usability']",NCI,MASSACHUSETTS GENERAL HOSPITAL,R01,2011,269163,-0.022446342711899873
"Multi-Criteria IMRT Optimization    DESCRIPTION (provided by applicant): The overall long-term objective of this project is to incorporate clinical decisions through interactive feedback into the inverse treatment planning process for intensity-modulated radiotherapy. The hypotheses are that this will (i) make the inverse planning process more effective and (ii) increase the clinical relevancy of optimized plans, introducing better tradeoffs between target coverage and sparing of healthy tissues. The new approaches to tackle this problem include multi-criteria optimization techniques and an interactive plan navigation tool for searching a pre-calculated treatment plan database. The idea of multi-criteria optimization in treatment planning is that multiple planning criteria in different critical structures and in the target volume can be controlled simultaneously. In contrast, in current inverse planning algorithms a single objective (score function) is maximized or minimized. This conventional optimization gives only limited control of the planning result, and major manual plan tweaking using trial and error is often necessary. In the previous funding period the feasibility of the approach has been demonstrated in theoretical example cases and retrospective treatment planning studies. This showed the potential of the multi-criteria planning approach and a potential for improvement in some areas. The first aim of the current project is to improve the usability of the method. This will include a reduction of the dimensionality of the problem, to make the tradeoff decision easier and faster for the clinician. It will also include the generation of plan databases with minimal user intervention, and developments of the plan navigation approach. The second aim is to extend functionality. Hardware and delivery aspects will be incorporated. A means to interactively optimize beam orientations will be provided. Dose conformality will be included in the tradeoff discussion, and the user will be able to make tradeoff decisions based on alternative biologically motivated criteria. Finally, in aim 3, the hypothesis that the multi-criteria planning paradigm will lead to better treatment plans in less time will be tested in the clinic. This work has the potential to lead to clinically more suitable and more individualized radiation treatment plans, with better dose coverage of the tumor target volume and/or reduced radiation dose to surrounding healthy tissues. Furthermore, shorter treatment planning times will reduce the overall cost. PUBLIC HEALTH RELEVANCE: This work has the potential to lead to clinically more suitable and more individualized radiation treatment plans, with better dose coverage of the tumor target volume and/or reduced radiation dose to surrounding healthy tissues. Furthermore, shorter treatment planning times will reduce the overall cost.          n/a",Multi-Criteria IMRT Optimization,7917353,R01CA103904,"['Algorithms', 'Area', 'Binding', 'Brain', 'Clinic', 'Clinical', 'Collimator', 'Complex', 'Complication', 'Conflict (Psychology)', 'Data', 'Databases', 'Development Plans', 'Dimensions', 'Dose', 'Feedback', 'Funding', 'Generations', 'Goals', 'Head and neck structure', 'Hour', 'Housing', 'Human', 'Intensity-Modulated Radiotherapy', 'Intervention', 'Joints', 'Lead', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Normal tissue morphology', 'Pancreas', 'Patients', 'Physicians', 'Plant Leaves', 'Principal Component Analysis', 'Probability', 'Procedures', 'Process', 'Radiation', 'Radiation therapy', 'Resistance', 'Site', 'Solutions', 'Structure', 'Surface', 'System', 'Tail', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Weight', 'Work', 'base', 'cost', 'improved', 'interest', 'novel strategies', 'public health relevance', 'therapy development', 'tool', 'touchscreen', 'treatment planning', 'treatment site', 'tumor', 'usability']",NCI,MASSACHUSETTS GENERAL HOSPITAL,R01,2010,277488,-0.022446342711899873
"Multi-Criteria IMRT Optimization    DESCRIPTION (provided by applicant): The overall long-term objective of this project is to incorporate clinical decisions through interactive feedback into the inverse treatment planning process for intensity-modulated radiotherapy. The hypotheses are that this will (i) make the inverse planning process more effective and (ii) increase the clinical relevancy of optimized plans, introducing better tradeoffs between target coverage and sparing of healthy tissues. The new approaches to tackle this problem include multi-criteria optimization techniques and an interactive plan navigation tool for searching a pre-calculated treatment plan database. The idea of multi-criteria optimization in treatment planning is that multiple planning criteria in different critical structures and in the target volume can be controlled simultaneously. In contrast, in current inverse planning algorithms a single objective (score function) is maximized or minimized. This conventional optimization gives only limited control of the planning result, and major manual plan tweaking using trial and error is often necessary. In the previous funding period the feasibility of the approach has been demonstrated in theoretical example cases and retrospective treatment planning studies. This showed the potential of the multi-criteria planning approach and a potential for improvement in some areas. The first aim of the current project is to improve the usability of the method. This will include a reduction of the dimensionality of the problem, to make the tradeoff decision easier and faster for the clinician. It will also include the generation of plan databases with minimal user intervention, and developments of the plan navigation approach. The second aim is to extend functionality. Hardware and delivery aspects will be incorporated. A means to interactively optimize beam orientations will be provided. Dose conformality will be included in the tradeoff discussion, and the user will be able to make tradeoff decisions based on alternative biologically motivated criteria. Finally, in aim 3, the hypothesis that the multi-criteria planning paradigm will lead to better treatment plans in less time will be tested in the clinic. This work has the potential to lead to clinically more suitable and more individualized radiation treatment plans, with better dose coverage of the tumor target volume and/or reduced radiation dose to surrounding healthy tissues. Furthermore, shorter treatment planning times will reduce the overall cost. PUBLIC HEALTH RELEVANCE: This work has the potential to lead to clinically more suitable and more individualized radiation treatment plans, with better dose coverage of the tumor target volume and/or reduced radiation dose to surrounding healthy tissues. Furthermore, shorter treatment planning times will reduce the overall cost.          n/a",Multi-Criteria IMRT Optimization,7688128,R01CA103904,"['Algorithms', 'Area', 'Binding', 'Brain', 'Clinic', 'Clinical', 'Collimator', 'Complex', 'Complication', 'Conflict (Psychology)', 'Data', 'Databases', 'Development Plans', 'Dimensions', 'Dose', 'Feedback', 'Funding', 'Generations', 'Goals', 'Head and neck structure', 'Hour', 'Housing', 'Human', 'Intensity-Modulated Radiotherapy', 'Intervention', 'Joints', 'Lead', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Normal tissue morphology', 'Pancreas', 'Patients', 'Physicians', 'Plant Leaves', 'Principal Component Analysis', 'Probability', 'Procedures', 'Process', 'Radiation', 'Radiation therapy', 'Resistance', 'Site', 'Solutions', 'Structure', 'Surface', 'System', 'Tail', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Weight', 'Work', 'base', 'cost', 'improved', 'interest', 'novel strategies', 'public health relevance', 'therapy development', 'tool', 'touchscreen', 'treatment planning', 'treatment site', 'tumor', 'usability']",NCI,MASSACHUSETTS GENERAL HOSPITAL,R01,2009,277389,-0.022446342711899873
"Multi-Criteria IMRT Optimization    DESCRIPTION (provided by applicant): The overall long-term objective of this project is to incorporate clinical decisions through interactive feedback into the inverse treatment planning process for intensity-modulated radiotherapy. The hypotheses are that this will (i) make the inverse planning process more effective and (ii) increase the clinical relevancy of optimized plans, introducing better tradeoffs between target coverage and sparing of healthy tissues. The new approaches to tackle this problem include multi-criteria optimization techniques and an interactive plan navigation tool for searching a pre-calculated treatment plan database. The idea of multi-criteria optimization in treatment planning is that multiple planning criteria in different critical structures and in the target volume can be controlled simultaneously. In contrast, in current inverse planning algorithms a single objective (score function) is maximized or minimized. This conventional optimization gives only limited control of the planning result, and major manual plan tweaking using trial and error is often necessary. In the previous funding period the feasibility of the approach has been demonstrated in theoretical example cases and retrospective treatment planning studies. This showed the potential of the multi-criteria planning approach and a potential for improvement in some areas. The first aim of the current project is to improve the usability of the method. This will include a reduction of the dimensionality of the problem, to make the tradeoff decision easier and faster for the clinician. It will also include the generation of plan databases with minimal user intervention, and developments of the plan navigation approach. The second aim is to extend functionality. Hardware and delivery aspects will be incorporated. A means to interactively optimize beam orientations will be provided. Dose conformality will be included in the tradeoff discussion, and the user will be able to make tradeoff decisions based on alternative biologically motivated criteria. Finally, in aim 3, the hypothesis that the multi-criteria planning paradigm will lead to better treatment plans in less time will be tested in the clinic. This work has the potential to lead to clinically more suitable and more individualized radiation treatment plans, with better dose coverage of the tumor target volume and/or reduced radiation dose to surrounding healthy tissues. Furthermore, shorter treatment planning times will reduce the overall cost. PUBLIC HEALTH RELEVANCE: This work has the potential to lead to clinically more suitable and more individualized radiation treatment plans, with better dose coverage of the tumor target volume and/or reduced radiation dose to surrounding healthy tissues. Furthermore, shorter treatment planning times will reduce the overall cost.          n/a",Multi-Criteria IMRT Optimization,7528695,R01CA103904,"['Algorithms', 'Area', 'Binding', 'Brain', 'Clinic', 'Clinical', 'Collimator', 'Complex', 'Complication', 'Conflict (Psychology)', 'Data', 'Databases', 'Dimensions', 'Dose', 'Feedback', 'Funding', 'Generations', 'Goals', 'Head and neck structure', 'Hour', 'Housing', 'Human', 'Intensity-Modulated Radiotherapy', 'Intervention', 'Joints', 'Lead', 'Left', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Normal tissue morphology', 'Numbers', 'Pancreas', 'Patients', 'Physicians', 'Principal Component Analysis', 'Probability', 'Procedures', 'Process', 'Public Health', 'Purpose', 'Radiation', 'Radiation therapy', 'Range', 'Resistance', 'Score', 'Site', 'Solutions', 'Standards of Weights and Measures', 'Structure', 'Surface', 'System', 'Tail', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Weight', 'Work', 'base', 'concept', 'cost', 'improved', 'interest', 'novel strategies', 'therapy development', 'tool', 'touchscreen', 'treatment planning', 'treatment site', 'tumor', 'usability']",NCI,MASSACHUSETTS GENERAL HOSPITAL,R01,2008,295167,-0.022446342711899873
"Cytometry Reagent Database and Automated Panel Design Software.     DESCRIPTION (provided by applicant):  The proposed project described in this grant application, combined with existing Fluorish tools, will create a robust cloud-based e-commerce marketplace for cytometry research reagents, addressing key needs in the research community. Flow Cytometry is widely used in basic and clinical research. Fluorish LLC will streamline the experimental design process and enhance the quality of scientific inquiry for flow cytometry research. Through iterative development, we will begin implementing an automated panel design system, and a comprehensive reagent and panel database. Linking these components in the cloud with Fluorish's social computing capabilities, will help users pursue greater complexity in multiparametric flow cytometry experimental design and significantly enhance their scientific exploration.    Objectives: {(1) Provide an online cloud environment that allows researchers to develop valuable social networks, while integrating robust scientific experimental design tools, with core/lab management systems, and an e-commerce online marketplace.} (2) Develop and implement an experimental design schema, with integrated evaluation metrics, that will automatically characterize the interaction between reagent and instrument dependencies, to account for the complexities of fluorophore assignments in panel design. (3) Create heuristics with a common ontology for flow cytometry reagents and panels that will allow for greater transparency on use of reagents, and sharing capabilities of optimized flow cytometry panels.    Methods: Building on our existing core and lab management systems, we will continue to add functionality to our site, allowing scientists to readily manage and distribute critical informatio regarding their flow cytometry experience. With our aggregated database of numerous reagent vendors' catalogs, we will create a database of designed panels from published sources to facilitate efficient reuse of panels and promote data sharing. We will base our work on nascent standards being created in the field, specifically MIFlowCyt, OMIP, ACS, and NetCDF, as well as innovation in the creation of a reagent ontology for cytometry. We will also define requirements for the development of an expert system approach to panel design, that can include metrics from technical aspects of antigen density, fluorophore brightness, spectral overlap, and spillover spreading, as well as, the social computing aspects of factoring in specific antibody conjugate use cases with titration data. {All of these components can be accessed in the cloud online, providing a dynamic, 'living' environment for the flow cytometry community.}     PUBLIC HEALTH RELEVANCE: Flow cytometry is a research tool used widely in basic and clinical research to examine the immune system. This technology is critical in the design of new vaccines and development of therapies, which rely upon and target immune cells. {Fluorish will create an innovative cloud-based community for scientists using flow cytometry by integrating current Fluorish user networks, and the existing web-based Fluorish core and laboratory management tools, with the proposed Cytometry Panel Database, Automated Panel Design Software, and MyFluorish project described in this grant application.} The resulting Fluorish feature set will advance the starting position for scientists using flow cytometry, streamline the experimental design process, strengthen material management capabilites, and enhance the quality of scientific inquiry. {Furthermore, Fluorish's new model of an online marketplace can be applied to other areas of scientific inquiry which provides an opportunity for Fluorish to easily expand beyond flow cytometry.}                ",Cytometry Reagent Database and Automated Panel Design Software.,8646039,R43GM105262,"['Accounting', 'Achievement', 'Address', 'Algorithm Design', 'Algorithms', 'Antibodies', 'Antigens', 'Applications Grants', 'Area', 'Basic Science', 'Behavior', 'Biological Assay', 'Cataloging', 'Catalogs', 'Cells', 'Clinical Immunology', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Cytometry', 'Data', 'Databases', 'Dependency', 'Development', 'Environment', 'Evaluation', 'Experimental Designs', 'Expert Systems', 'Flow Cytometry', 'Fluorochrome', 'Goals', 'Growth', 'Immune Targeting', 'Immune system', 'Immunophenotyping', 'Indium', 'Knowledge', 'Laboratories', 'Life', 'Link', 'Marketing', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Online Systems', 'Ontology', 'Phenotype', 'Positioning Attribute', 'Process', 'Published Comment', 'Publishing', 'Quality Control', 'Reagent', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Site', 'Social Network', 'Software Design', 'Solutions', 'Source', 'Staining method', 'Stains', 'System', 'Technology', 'Testing', 'Time', 'Titrations', 'Training Support', 'Vendor', 'Work', 'antibody conjugate', 'base', 'cloud based', 'commercial application', 'data sharing', 'database design', 'density', 'design', 'e-commerce', 'experience', 'experimental analysis', 'fluorophore', 'heuristics', 'indexing', 'innovation', 'instrument', 'interest', 'novel', 'novel vaccines', 'public health relevance', 'real world application', 'social', 'success', 'tool', 'vaccine development', 'wasting']",NIGMS,"FLUORISH, LLC",R43,2014,185858,-0.03808769945078908
"Neural Signature of Fear Overgeneralization in Trauma Exposed Adults ﻿    DESCRIPTION (provided by applicant): Populations exposed to severe trauma such as war veterans, disaster survivors, and assault victims are at high risk for a range of trauma related psychopathology. Over one third of the exposed individuals are likely to develop significant and disabling psychopathology including posttraumatic stress disorder (PTSD), generalized anxiety disorder (GAD), panic disorder (PD), and major depression. However, efforts to advance knowledge on neural mechanisms of trauma related abnormalities have been hampered by a diagnostic categorical approach that has been focused almost solely on PTSD, neglecting a critical need to identify neural mechanisms of shared domains/constructs across anxiety, mood, and trauma-related disorders, which can be reliably measured and serve as physiological targets for novel treatments. Conditioned fear overgeneralization, in which there is deficiency in distinguishing learned danger cues from resembling safe stimuli, has been recently proposed as a potential endophenotype that crosses diagnostic boundaries including patients with PTSD, GAD and PD. The neural circuitry of fear overgeneralization has been elucidated in both animals and healthy humans, suggesting deficient functioning of the hippocampus and fear network regions. To advance identification of a neural signature of trauma related psychopathology that can potentially serve as a novel, neuroscience informed treatment target, and consistent with the RDoC emphasis on neurally-based domains of function across disorders, the current 4-year R01 application will use an fMRI paradigm for assessing fear overgeneralization among subjects with and without exposure to trauma (N=120). Eighty adults with well-ascertained, severe exposure to trauma and 40 matched non-trauma exposed healthy adults will be assessed. Rings of gradually increasing size will be visually presented during fMRI, with extreme sizes serving as conditioned danger cues (CS+; paired with electric shock) and conditioned safety cues (CS-). The rings of intermediary size serve as `generalization stimuli' (GS) and create a continuum-of-size between CS+ and CS-. Graphically represented conditioned-fear responses across this continuum, known as generalization gradients, have been reliably shown to have a steepness (or slope) that reflects the degree of generalization. For skin conductance response (SCR), a less steep slope indicates greater generalization. Separate generalization gradients will be generated by the paradigm for: a) regional fMRI activations; b) SCR; and 3) self-reported risk of shock. Machine-learning methodology will be applied to identify an fMRI-based neural signature of overgeneralization and test its associations with functional impairment and symptom severity across trauma- associated diagnoses. If successful, findings from this study will significantly advance the characterization of an endophenotype that will serve as an objective measure of pathology and a novel target for interventions specifically designed to target the impaired mechanism of overgeneralization and its neural signature. PUBLIC HEALTH RELEVANCE: Trauma exposed populations such as war veterans, disaster survivors, and assault victims are at high risk for a range of trauma-related psychopathology. Over one third of the exposed individuals are likely to develop significant and disabling psychopathology including posttraumatic stress disorder (PTSD), generalized anxiety disorder (GAD), panic disorder (PD), depression and functional impairment. The overarching goal of the study is to use a functional magnetic resonance imaging (fMRI) paradigm of fear overgeneralization, skin conductance response (SCR), and machine learning analytic methods in order to identify a neural signature of trauma-related psychopathology that can serve as an objective measure of pathology and functional impairment, and a novel target for neuroscience informed treatments of functionally impaired trauma exposed populations.",Neural Signature of Fear Overgeneralization in Trauma Exposed Adults,9492704,R01MH105355,"['Adult', 'Animals', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Biological Markers', 'Brain', 'Categories', 'Clinical', 'Control Groups', 'Cues', 'Data', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disasters', 'Disease', 'Exhibits', 'Exposure to', 'Fright', 'Functional Magnetic Resonance Imaging', 'Galvanic Skin Response', 'Generalized Anxiety Disorder', 'Goals', 'Hamilton Rating Scale for Depression', 'Hippocampus (Brain)', 'Human', 'Impairment', 'Individual', 'Insula of Reil', 'Intervention', 'Knowledge', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental Depression', 'Mental disorders', 'Methodology', 'Mood Disorders', 'Moods', 'Neurosciences', 'Panic Disorder', 'Participant', 'Pathology', 'Patient Self-Report', 'Patients', 'Pattern', 'Physiological', 'Physiology', 'Population', 'Post-Traumatic Stress Disorders', 'Prefrontal Cortex', 'Process', 'Psychopathology', 'Research', 'Research Domain Criteria', 'Risk', 'Risk Assessment', 'SF-36', 'Safety', 'Secondary to', 'Severities', 'Sex Characteristics', 'Shock', 'Stimulus', 'Stimulus Generalization', 'Symptoms', 'Testing', 'Trauma', 'Trauma Research', 'Veterans', 'War', 'Work', 'analytical method', 'anxiety-related disorders', 'assault', 'base', 'blood oxygenation level dependent response', 'brain circuitry', 'conditioned fear', 'design', 'disaster survivor', 'endophenotype', 'falls', 'functional disability', 'high risk', 'innovation', 'neglect', 'neural circuit', 'neuromechanism', 'novel', 'public health relevance', 'relating to nervous system', 'response', 'sex', 'social anxiety', 'trauma exposure', 'traumatic event']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2018,423903,-0.05806969555480724
"Neural Signature of Fear Overgeneralization in Trauma Exposed Adults ﻿    DESCRIPTION (provided by applicant): Populations exposed to severe trauma such as war veterans, disaster survivors, and assault victims are at high risk for a range of trauma related psychopathology. Over one third of the exposed individuals are likely to develop significant and disabling psychopathology including posttraumatic stress disorder (PTSD), generalized anxiety disorder (GAD), panic disorder (PD), and major depression. However, efforts to advance knowledge on neural mechanisms of trauma related abnormalities have been hampered by a diagnostic categorical approach that has been focused almost solely on PTSD, neglecting a critical need to identify neural mechanisms of shared domains/constructs across anxiety, mood, and trauma-related disorders, which can be reliably measured and serve as physiological targets for novel treatments. Conditioned fear overgeneralization, in which there is deficiency in distinguishing learned danger cues from resembling safe stimuli, has been recently proposed as a potential endophenotype that crosses diagnostic boundaries including patients with PTSD, GAD and PD. The neural circuitry of fear overgeneralization has been elucidated in both animals and healthy humans, suggesting deficient functioning of the hippocampus and fear network regions. To advance identification of a neural signature of trauma related psychopathology that can potentially serve as a novel, neuroscience informed treatment target, and consistent with the RDoC emphasis on neurally-based domains of function across disorders, the current 4-year R01 application will use an fMRI paradigm for assessing fear overgeneralization among subjects with and without exposure to trauma (N=120). Eighty adults with well-ascertained, severe exposure to trauma and 40 matched non-trauma exposed healthy adults will be assessed. Rings of gradually increasing size will be visually presented during fMRI, with extreme sizes serving as conditioned danger cues (CS+; paired with electric shock) and conditioned safety cues (CS-). The rings of intermediary size serve as `generalization stimuli' (GS) and create a continuum-of-size between CS+ and CS-. Graphically represented conditioned-fear responses across this continuum, known as generalization gradients, have been reliably shown to have a steepness (or slope) that reflects the degree of generalization. For skin conductance response (SCR), a less steep slope indicates greater generalization. Separate generalization gradients will be generated by the paradigm for: a) regional fMRI activations; b) SCR; and 3) self-reported risk of shock. Machine-learning methodology will be applied to identify an fMRI-based neural signature of overgeneralization and test its associations with functional impairment and symptom severity across trauma- associated diagnoses. If successful, findings from this study will significantly advance the characterization of an endophenotype that will serve as an objective measure of pathology and a novel target for interventions specifically designed to target the impaired mechanism of overgeneralization and its neural signature. PUBLIC HEALTH RELEVANCE: Trauma exposed populations such as war veterans, disaster survivors, and assault victims are at high risk for a range of trauma-related psychopathology. Over one third of the exposed individuals are likely to develop significant and disabling psychopathology including posttraumatic stress disorder (PTSD), generalized anxiety disorder (GAD), panic disorder (PD), depression and functional impairment. The overarching goal of the study is to use a functional magnetic resonance imaging (fMRI) paradigm of fear overgeneralization, skin conductance response (SCR), and machine learning analytic methods in order to identify a neural signature of trauma-related psychopathology that can serve as an objective measure of pathology and functional impairment, and a novel target for neuroscience informed treatments of functionally impaired trauma exposed populations.",Neural Signature of Fear Overgeneralization in Trauma Exposed Adults,9281088,R01MH105355,"['Adult', 'Animals', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Biological Markers', 'Brain', 'Categories', 'Clinical', 'Control Groups', 'Cues', 'Data', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disasters', 'Disease', 'Exhibits', 'Exposure to', 'Fright', 'Functional Magnetic Resonance Imaging', 'Galvanic Skin Response', 'Generalized Anxiety Disorder', 'Goals', 'Hamilton Rating Scale for Depression', 'Hippocampus (Brain)', 'Human', 'Impairment', 'Individual', 'Insula of Reil', 'Intervention', 'Knowledge', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental Depression', 'Mental disorders', 'Methodology', 'Mood Disorders', 'Moods', 'Neurosciences', 'Panic Disorder', 'Participant', 'Pathology', 'Patient Self-Report', 'Patients', 'Pattern', 'Physiological', 'Physiology', 'Population', 'Post-Traumatic Stress Disorders', 'Prefrontal Cortex', 'Process', 'Psychopathology', 'Research', 'Research Domain Criteria', 'Risk', 'Risk Assessment', 'SF-36', 'Safety', 'Secondary to', 'Severities', 'Sex Characteristics', 'Shock', 'Stimulus', 'Stimulus Generalization', 'Symptoms', 'Testing', 'Trauma', 'Trauma Research', 'Veterans', 'War', 'Work', 'analytical method', 'anxiety-related disorders', 'assault', 'base', 'blood oxygenation level dependent response', 'brain circuitry', 'conditioned fear', 'design', 'disaster survivor', 'endophenotype', 'falls', 'functional disability', 'high risk', 'innovation', 'neglect', 'neural circuit', 'neuromechanism', 'novel', 'public health relevance', 'relating to nervous system', 'response', 'sex', 'social anxiety', 'traumatic event']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2017,435678,-0.05806969555480724
"Neural Signature of Fear Overgeneralization in Trauma Exposed Adults ﻿    DESCRIPTION (provided by applicant): Populations exposed to severe trauma such as war veterans, disaster survivors, and assault victims are at high risk for a range of trauma related psychopathology. Over one third of the exposed individuals are likely to develop significant and disabling psychopathology including posttraumatic stress disorder (PTSD), generalized anxiety disorder (GAD), panic disorder (PD), and major depression. However, efforts to advance knowledge on neural mechanisms of trauma related abnormalities have been hampered by a diagnostic categorical approach that has been focused almost solely on PTSD, neglecting a critical need to identify neural mechanisms of shared domains/constructs across anxiety, mood, and trauma-related disorders, which can be reliably measured and serve as physiological targets for novel treatments. Conditioned fear overgeneralization, in which there is deficiency in distinguishing learned danger cues from resembling safe stimuli, has been recently proposed as a potential endophenotype that crosses diagnostic boundaries including patients with PTSD, GAD and PD. The neural circuitry of fear overgeneralization has been elucidated in both animals and healthy humans, suggesting deficient functioning of the hippocampus and fear network regions. To advance identification of a neural signature of trauma related psychopathology that can potentially serve as a novel, neuroscience informed treatment target, and consistent with the RDoC emphasis on neurally-based domains of function across disorders, the current 4-year R01 application will use an fMRI paradigm for assessing fear overgeneralization among subjects with and without exposure to trauma (N=120). Eighty adults with well-ascertained, severe exposure to trauma and 40 matched non-trauma exposed healthy adults will be assessed. Rings of gradually increasing size will be visually presented during fMRI, with extreme sizes serving as conditioned danger cues (CS+; paired with electric shock) and conditioned safety cues (CS-). The rings of intermediary size serve as `generalization stimuli' (GS) and create a continuum-of-size between CS+ and CS-. Graphically represented conditioned-fear responses across this continuum, known as generalization gradients, have been reliably shown to have a steepness (or slope) that reflects the degree of generalization. For skin conductance response (SCR), a less steep slope indicates greater generalization. Separate generalization gradients will be generated by the paradigm for: a) regional fMRI activations; b) SCR; and 3) self-reported risk of shock. Machine-learning methodology will be applied to identify an fMRI-based neural signature of overgeneralization and test its associations with functional impairment and symptom severity across trauma- associated diagnoses. If successful, findings from this study will significantly advance the characterization of an endophenotype that will serve as an objective measure of pathology and a novel target for interventions specifically designed to target the impaired mechanism of overgeneralization and its neural signature. PUBLIC HEALTH RELEVANCE: Trauma exposed populations such as war veterans, disaster survivors, and assault victims are at high risk for a range of trauma-related psychopathology. Over one third of the exposed individuals are likely to develop significant and disabling psychopathology including posttraumatic stress disorder (PTSD), generalized anxiety disorder (GAD), panic disorder (PD), depression and functional impairment. The overarching goal of the study is to use a functional magnetic resonance imaging (fMRI) paradigm of fear overgeneralization, skin conductance response (SCR), and machine learning analytic methods in order to identify a neural signature of trauma-related psychopathology that can serve as an objective measure of pathology and functional impairment, and a novel target for neuroscience informed treatments of functionally impaired trauma exposed populations.",Neural Signature of Fear Overgeneralization in Trauma Exposed Adults,9096246,R01MH105355,"['Adult', 'Animals', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Biological Markers', 'Brain', 'Clinical', 'Control Groups', 'Cues', 'Data', 'Diagnosis', 'Diagnostic', 'Disasters', 'Disease', 'Exhibits', 'Exposure to', 'Fright', 'Functional Magnetic Resonance Imaging', 'Galvanic Skin Response', 'Generalized Anxiety Disorder', 'Goals', 'Hamilton Rating Scale for Depression', 'Health', 'Hippocampus (Brain)', 'Human', 'Impairment', 'Individual', 'Insula of Reil', 'Intervention', 'Knowledge', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental Depression', 'Mental disorders', 'Methodology', 'Methods', 'Mood Disorders', 'Moods', 'Neurosciences', 'Panic Disorder', 'Participant', 'Pathology', 'Patient Self-Report', 'Patients', 'Pattern', 'Physiological', 'Physiology', 'Population', 'Post-Traumatic Stress Disorders', 'Prefrontal Cortex', 'Process', 'Psychopathology', 'Research', 'Research Domain Criteria', 'Risk', 'Risk Assessment', 'SF-36', 'Safety', 'Secondary to', 'Severities', 'Sex Characteristics', 'Shock', 'Stimulus', 'Stimulus Generalization', 'Symptoms', 'Testing', 'Trauma', 'Trauma Research', 'Veterans', 'War', 'Work', 'anxiety-related disorders', 'assault', 'base', 'blood oxygenation level dependent response', 'brain circuitry', 'conditioned fear', 'design', 'disaster survivor', 'endophenotype', 'falls', 'functional disability', 'high risk', 'innovation', 'neglect', 'neural circuit', 'neuromechanism', 'novel', 'relating to nervous system', 'response', 'sex', 'social anxiety', 'traumatic event']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2016,399327,-0.05806969555480724
"Neural Signature of Fear Overgeneralization in Trauma Exposed Adults ﻿    DESCRIPTION (provided by applicant): Populations exposed to severe trauma such as war veterans, disaster survivors, and assault victims are at high risk for a range of trauma related psychopathology. Over one third of the exposed individuals are likely to develop significant and disabling psychopathology including posttraumatic stress disorder (PTSD), generalized anxiety disorder (GAD), panic disorder (PD), and major depression. However, efforts to advance knowledge on neural mechanisms of trauma related abnormalities have been hampered by a diagnostic categorical approach that has been focused almost solely on PTSD, neglecting a critical need to identify neural mechanisms of shared domains/constructs across anxiety, mood, and trauma-related disorders, which can be reliably measured and serve as physiological targets for novel treatments. Conditioned fear overgeneralization, in which there is deficiency in distinguishing learned danger cues from resembling safe stimuli, has been recently proposed as a potential endophenotype that crosses diagnostic boundaries including patients with PTSD, GAD and PD. The neural circuitry of fear overgeneralization has been elucidated in both animals and healthy humans, suggesting deficient functioning of the hippocampus and fear network regions. To advance identification of a neural signature of trauma related psychopathology that can potentially serve as a novel, neuroscience informed treatment target, and consistent with the RDoC emphasis on neurally-based domains of function across disorders, the current 4-year R01 application will use an fMRI paradigm for assessing fear overgeneralization among subjects with and without exposure to trauma (N=120). Eighty adults with well-ascertained, severe exposure to trauma and 40 matched non-trauma exposed healthy adults will be assessed. Rings of gradually increasing size will be visually presented during fMRI, with extreme sizes serving as conditioned danger cues (CS+; paired with electric shock) and conditioned safety cues (CS-). The rings of intermediary size serve as `generalization stimuli' (GS) and create a continuum-of-size between CS+ and CS-. Graphically represented conditioned-fear responses across this continuum, known as generalization gradients, have been reliably shown to have a steepness (or slope) that reflects the degree of generalization. For skin conductance response (SCR), a less steep slope indicates greater generalization. Separate generalization gradients will be generated by the paradigm for: a) regional fMRI activations; b) SCR; and 3) self-reported risk of shock. Machine-learning methodology will be applied to identify an fMRI-based neural signature of overgeneralization and test its associations with functional impairment and symptom severity across trauma- associated diagnoses. If successful, findings from this study will significantly advance the characterization of an endophenotype that will serve as an objective measure of pathology and a novel target for interventions specifically designed to target the impaired mechanism of overgeneralization and its neural signature.         PUBLIC HEALTH RELEVANCE: Trauma exposed populations such as war veterans, disaster survivors, and assault victims are at high risk for a range of trauma-related psychopathology. Over one third of the exposed individuals are likely to develop significant and disabling psychopathology including posttraumatic stress disorder (PTSD), generalized anxiety disorder (GAD), panic disorder (PD), depression and functional impairment. The overarching goal of the study is to use a functional magnetic resonance imaging (fMRI) paradigm of fear overgeneralization, skin conductance response (SCR), and machine learning analytic methods in order to identify a neural signature of trauma-related psychopathology that can serve as an objective measure of pathology and functional impairment, and a novel target for neuroscience informed treatments of functionally impaired trauma exposed populations.            ",Neural Signature of Fear Overgeneralization in Trauma Exposed Adults,8960792,R01MH105355,"['Adult', 'Animals', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Biological Markers', 'Brain', 'Clinical', 'Control Groups', 'Cues', 'Data', 'Diagnosis', 'Diagnostic', 'Disasters', 'Disease', 'Exhibits', 'Exposure to', 'Fright', 'Functional Magnetic Resonance Imaging', 'Galvanic Skin Response', 'Generalized Anxiety Disorder', 'Goals', 'Hamilton Rating Scale for Depression', 'Hippocampus (Brain)', 'Human', 'Impairment', 'Individual', 'Insula of Reil', 'Intervention', 'Knowledge', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental Depression', 'Mental disorders', 'Methodology', 'Methods', 'Mood Disorders', 'Moods', 'Neurosciences', 'Panic Disorder', 'Participant', 'Pathology', 'Patient Self-Report', 'Patients', 'Pattern', 'Physiological', 'Physiology', 'Population', 'Post-Traumatic Stress Disorders', 'Prefrontal Cortex', 'Process', 'Psychopathology', 'Research', 'Research Domain Criteria', 'Risk', 'Risk Assessment', 'SF-36', 'Safety', 'Secondary to', 'Severities', 'Sex Characteristics', 'Shock', 'Stimulus', 'Stimulus Generalization', 'Symptoms', 'Testing', 'Trauma', 'Trauma Research', 'Veterans', 'War', 'Work', 'anxiety-related disorders', 'assault', 'base', 'blood oxygenation level dependent response', 'brain circuitry', 'conditioned fear', 'design', 'disaster survivor', 'endophenotype', 'falls', 'functional disability', 'high risk', 'innovation', 'neglect', 'neural circuit', 'neuromechanism', 'novel', 'public health relevance', 'relating to nervous system', 'response', 'sex', 'social anxiety', 'traumatic event']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2015,437426,-0.05806969555480724
"Accelerated x-ray therapy planning system PEREGRINE DESCRIPTION (provided by applicant): Computerized radiation therapy planning systems (RTP) are essential in Radiation Oncology for quantitative evaluation of radiation doses prior to patient treatment. Among currently available computational methods, the Monte Carlo method of calculating dose distributionsis most universal and accurate. It is believed that Monte Carlo software packages will become a central part of future RTP systems. The limiting problem with current Monte Carlo codes is the length of time (CPU time) required for calculations even when using state-of-the-art hardware. An increase in efficiency of Monte Carlo codes has been demonstrated using algorithms known as variance-reduction techniques (VR techniques), but the calculation times are still too long for routine  clinical use. While there is no single VR technique that would make Monte Carlo code clinically viable, a combination of such techniques usually results in improved performance. At present, the only commercial RTP system using Monte Carlo code for photon dose calculations is PEREGRINE from Lawrence Livermore National Laboratory and NOMOS Corporation. PEREGRINE uses several VR techniques. However, it is estimated that a further reduction in CPU time by a factor of 10 would be required to have a clinically efficient system.  Our theoretical study and subsequent Monte Carlo results support a new variance-reduction technique (NVR) for photon-beam dose calculations. It is shown that NVR yields up to a 5-fold reduction in CPU time. The long-term objective of the project is to reduce PEREGRIN's CPU time from currently several hours to several; minutes. This will require a combination of NVR with other VR techniques. Within this scope, the specific aims of Phase I are:  1. Development, implementation and validation of a PEREGRINE-specific NVR algorithm.  2. Benchmarking of NVR over the range of clinically useful energies in homogeneous and heterogeneous phantoms.  3. Validation of NVR in the case of patient-specific Monte Carlo calculations using CT based patient anatomy. n/a",Accelerated x-ray therapy planning system PEREGRINE,6787955,R41CA108088,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computed axial tomography', 'computer assisted medical decision making', 'computer assisted patient care', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'electromagnetic radiation', 'mathematics', 'patient care planning', 'radiation dosage', 'radiation therapy', 'technology /technique development']",NCI,NOMOS CORPORATION,R41,2004,100000,0.0055266626756670404
"Detection and characterization of critical under-immunized hotspots Detection and characterization of critical under-immunized hotspots  Emergence of undervaccinated geographical clusters for diseases like measles has become a national concern. A number of measles outbreaks have occurred in recent months, despite high MMR coverage in the United States ( 95%). Such undervaccinated clusters can act as reservoirs of infection that can transmit the disease to a wider population, magnifying their importance far beyond what their absolute numbers might indicate. The existence and growth of such undervaccinated clusters is often known to public health agencies and health provider networks, but they typically do not have enough resources to target people in each such cluster, to attempt to improve the vaccination rate. Preliminary results show that not all undervaccinated clusters are “equal” in terms of their potential for causing a big outbreak (referred to as its “criticality”), and the rate of undervaccination in a cluster does not necessarily correlate with its criticality.  However, there are no existing methods to estimate the potential risk of such clusters, and to identify the most “critical” ones. Some of the key reasons are: (i) purely data-driven spatial statistics methods rely only on immunization coverage, which does not give any indication of the risk of an outbreak; and (ii) current causal epidemic models need to be combined with detailed incidence data, which has not been easily available.  This proposal brings together a systems science approach, combining agent-based stochastic epidemic models, and techniques from machine learning, high performance computing, data mining, and spatial statistics, along with novel public and private datasets on immunization and incidence, to develop a novel methodology for identifying critical clusters, through the following tasks: (i) Identify spatial clusters with signiﬁcantly low immunization rates, or strong anti-vaccine sentiment; (ii) Develop an agent based model for the spread of measles that incorporates detailed immunization data, and is calibrated using a novel source of incidence data; (iii) Develop methods to ﬁnd and characterize critical spatial clusters, with respect to different metrics, which capture both epidemic and economic burden, and order underimmunized clusters based on their criticality; and (iv) Use the methodology to evaluate interventions in terms of their effect on criticality. A highly interdisciplinary team involving two universities, a health care delivery organization and a state department of Health, will work together to develop this methodology. Characterization of such clusters will enable public health departments and policy makers in targeted surveillance of their regions and a more efﬁcient allocation of resources. Project Narrative  This project will develop a new methodology to quantify the potential risks of under-vaccinated spatial clusters for highly infectious diseases. It will rank the clusters based on their economic and epidemic burden which will enable public health ofﬁcials in targeted surveillance and interventions, to mitigate their risk.",Detection and characterization of critical under-immunized hotspots,9887876,R01GM109718,"['Affect', 'Bayesian Method', 'Behavioral Model', 'California', 'Characteristics', 'Communicable Diseases', 'Communities', 'Computer Models', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Detection', 'Disease', 'Disease Clusterings', 'Disease Outbreaks', 'Disease model', 'Economic Burden', 'Economics', 'Epidemic', 'Epidemiology', 'Exhibits', 'Funding', 'Geography', 'Growth', 'Health', 'Health Personnel', 'Herd Immunity', 'High Performance Computing', 'Immunization', 'Immunize', 'Incidence', 'Individual', 'Infection', 'Intervention', 'Machine Learning', 'Measles', 'Measles-Mumps-Rubella Vaccine', 'Medical', 'Methodology', 'Methods', 'Minnesota', 'Modeling', 'New Jersey', 'New York', 'Oregon', 'Outcome', 'Pathway interactions', 'Policies', 'Policy Maker', 'Population', 'Population Analysis', 'Privatization', 'Public Health', 'Records', 'Registries', 'Resolution', 'Resource Allocation', 'Resources', 'Risk', 'Scanning', 'Schools', 'Science', 'Source', 'System', 'Systems Analysis', 'Techniques', 'Time', 'Uncertainty', 'United States', 'Universities', 'Vaccinated', 'Vaccination', 'Vaccines', 'Washington', 'Work', 'base', 'data mining', 'demographics', 'diverse data', 'economic cost', 'economic outcome', 'health care delivery', 'health disparity', 'health organization', 'improved', 'interest', 'novel', 'novel strategies', 'population based', 'provider networks', 'public health intervention', 'social', 'social media', 'spatiotemporal', 'statistics', 'tool', 'transmission process']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2020,324178,-0.01783654315338522
"Spiking network models of sharp-wave ripple sequences with gamma-locked attractor dynamics The hippocampus is a critical structure for learning and memory in the mammalian brain. During  active exploration, hippocampal circuits support the activity of place cells that track an animal’s  position as it moves. The spatial representations of place cells in a particular environment  collectively form a spatial map, the function of which remains largely unclear. During periods of  rest, the hippocampus switches to a network state characterized by stochastic and highly variable  activity patterns. A prominent feature of this irregular activity is recurring strong bursts of  excitation, called sharp waves, that recruit large proportions of hippocampal neurons and sweep  across the circuits of the hippocampus. Sequentially ordered activation of place cells during sharp  waves is considered to serve a critical role in memory consolidation. In addition to replaying  recently experienced routes, sharp wave sequences can follow unexplored paths through space and can  be biased to goal destinations according to reward value. This project will investigate an explicit  theoretical framework for flexible sequence generation in service of prospective route planning for  navigation. The theory rests on the observation that the same network that initiates sharp waves,  the hippocampal CA3 region, also prominently carries a distinct lower-frequency band of gamma  oscillation. One study has shown that the slow gamma rhythm modulated spiking activity during sharp  wave sequences, such that the phase of gamma with the strongest activity corresponded to periods  when the decoded sequence would dwell, or ‘hover’, at discrete spatial locations. Informed by  recording data from that study, this project will develop realistic hippocampal network models to  study how sharp waves and slow gamma oscillations might emerge simultaneously. On the basis of that  physiological model, spatial activity will be embedded into the synaptic weights of the network to  assess whether sharp waves and slow gamma interact to support gamma-locked attractor dynamics, and  how that interaction depends quantitatively on the synaptic modifications for spatial learning. In  particular, focus will be placed on a puzzling experimental demonstration that sharp wave sequence  recall slows down during learning. The long-term objective is to fully evaluate whether these  emergent phenomena support a unified framework for the function of spatial maps in navigation on  the basis of enabling flexible generation of novel sequences. A clear theoretical understanding of  these key hippocampal phenomena in realistic networks will elucidate their role in memory,  planning, and disease states. How do the neuronal networks of the hippocampus learn to generate sequences of spatial activity  that help animals find their way in a changing world? We propose computational models and  mathematical analysis to investigate a theory in which the tendency of recurrent networks to  converge to a small number of low-energy states is rhythmically destabilized by gamma oscillations  concurrently generated by the network. Our project integrates theory with benchmarks from  high-density recording data to reveal neurocomputational functions of hippocampal activity patterns  that serve critical roles in health and disease.",Spiking network models of sharp-wave ripple sequences with gamma-locked attractor dynamics,9775500,R03NS109923,"['Animals', 'Benchmarking', 'Biological', 'Brain', 'Cells', 'Code', 'Complex', 'Computer Simulation', 'Coupling', 'Data', 'Data Analyses', 'Data Set', 'Destinations', 'Development', 'Disease', 'Encapsulated', 'Environment', 'Event', 'Fostering', 'Frequencies', 'Gap Junctions', 'Generations', 'Goals', 'Health', 'Hippocampus (Brain)', 'Interneurons', 'Learning', 'Location', 'Maps', 'Memory', 'Modeling', 'Modification', 'Neural Network Simulation', 'Neurons', 'Output', 'Pattern', 'Periodicity', 'Phase', 'Physiological', 'Physiology', 'Positioning Attribute', 'Property', 'Rattus', 'Recurrence', 'Rest', 'Rewards', 'Role', 'Route', 'Services', 'Slow-Wave Sleep', 'Speed', 'Structure', 'Synapses', 'Theta Rhythm', 'Validation', 'Weight', 'artificial neural network', 'awake', 'classical conditioning', 'density', 'dentate gyrus', 'expectation', 'experience', 'falls', 'flexibility', 'mathematical analysis', 'memory consolidation', 'model development', 'network models', 'novel', 'physiologic model', 'predictive modeling', 'preservation', 'prospective', 'recruit', 'relating to nervous system', 'simulation', 'spatial relationship', 'statistics', 'theories', 'virtual']",NINDS,JOHNS HOPKINS UNIVERSITY,R03,2019,81875,-0.08385763563983822
"CRCNS: Real-time neural decoding for calcium imaging Program Director/Principal Investigator (Last, First, Middle): Chen, Rong PROJECT DESCRIPTION A. BACKGROUND AND SIGNIFICANCE Real-time neural decoding centers on predicting behavior variables based on neural activity data, where the prediction is performed at a pace that reliably keeps up with the speed of the activity that is being monitored. Neuromodulation devices are becoming one of the most powerful tools for the treatment of brain disorders, enhancing neurocognitive performance, and demonstrating causality (Bergmann et al., 2016; Knotkova and Rasche, 2015). A precise neuromodulation system (Figure 1) integrates neural activity monitoring, real-time neural decoding, and neuromodulation. In precise neuromodulation, a decoding device predicts a behavior variable based on neural data streams in real-time. Based on the decoding results, neuromodulation parameters such as timing, frequency, duration, and amplitude are changed. Precise neuromodulation systems with closed-loop real-time feedback are superior to the fixed (open- loop) neuromodulation paradigm (Brocker et al., 2017; deBettencourt et al., 2015; Ezzyat et al., 2017). A recent direct brain stimulation study (Ezzyat et al., 2017) demonstrated significant advantages of precise neuromodulation over open-loop neuromodulation. Ezzyat et al. applied direct brain stimulation with decoding capability to patients with epilepsy to improve their memory. They found that stimulation increased memory function only if delivered when the decoding device indicated low encoding efficiency while stimulation decreased memory function if delivered when the decoding device indicated high encoding efficiency. An open-loop neuromodulation system with a fixed stimulation paradigm may not always facilitate memory function.  Miniature cellular imaging (Ghosh et al., 2011; Kerr and Nimmerjahn, 2012; Scott et al., 2013) is one of the most powerful ways to study neural circuits. It enables us to investigate neural circuits during behaviors for an understanding of network architecture of behavior, cognition, and emotion. Miniature cellular imaging records neuronal activity at cellular and sub-second levels of spatial and temporal resolution in freely moving animals. Miniature cellular imaging has many advantages. First, compared with in vivo multi-electrode recording, miniature calcium imaging can probe all cells in the field of view, and visualize the spatial location of monitored cells (Kerr et al., 2005). Second, compared with magnetic resonance imaging, which measures brain activity at the macroscopic scale and with low temporal resolution, miniature cellular imaging provides high spatial and temporal resolution. Third, fiber photometry (Cui et al., 2014) lacks cellular-level resolution, while miniature cellular imaging allows concurrent tracking of neural calcium activities at cellular spatial resolution. Simultaneous neural activity monitoring and intetvention Stimulation Calcium imaging Real-time decoding system Figure 1 A precise neuromodulation system. Our project centers on developing RNDC-Lab. PHS 398 (Rev. 01 /18 Approved Through 03/31/2020) Page 26  Miniature cellular imaging with real- time decoding capability captures the central vision of brain science, (The brain initiative, 2014). Combined with optogenetics, it is a tremendous asset to studying neural mechanisms underlying normal and disease states, and leads to precise neuromodulation. However, developing such systems is a challenging task. A major obstacle is the analysis of the large imaging streams that are generated. The massive high-dimensional data streams that are generated include 0MB No. 0925-0001 n/a",CRCNS: Real-time neural decoding for calcium imaging,10001622,R01NS110421,"['Address', 'Algorithms', 'Animals', 'Attention', 'BRAIN initiative', 'Behavior', 'Biological Models', 'Brain', 'Brain Diseases', 'Calcium', 'Cells', 'Cognition', 'Collection', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Detection', 'Devices', 'Disease', 'Electrodes', 'Emotions', 'Encapsulated', 'Epilepsy', 'Esthesia', 'Etiology', 'Event', 'Feedback', 'Fiber', 'Frequencies', 'Generations', 'Image', 'Knowledge', 'Laboratories', 'Language', 'Learning', 'Link', 'Location', 'Magnetic Resonance Imaging', 'Masks', 'Measures', 'Memory', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Network-based', 'Neurocognitive', 'Neurons', 'Neurosciences', 'Pain', 'Patients', 'Perception', 'Performance', 'Photometry', 'Principal Investigator', 'Process', 'Records', 'Research', 'Resolution', 'Science', 'Scientist', 'Sleep', 'Speed', 'Stream', 'System', 'Techniques', 'Time', 'Training', 'Vision', 'Wakefulness', 'Work', 'automated algorithm', 'base', 'cellular imaging', 'cost efficient', 'data acquisition', 'data streams', 'deep neural network', 'design', 'experimental study', 'high dimensionality', 'improved', 'in vivo', 'innovation', 'multidimensional data', 'nervous system disorder', 'network architecture', 'neural circuit', 'neural stimulation', 'neuromechanism', 'neuroregulation', 'neurotransmission', 'novel', 'optical imaging', 'optogenetics', 'predictive modeling', 'programs', 'prototype', 'real-time images', 'relating to nervous system', 'signal processing', 'spatiotemporal', 'temporal measurement', 'tool']",NINDS,UNIVERSITY OF MARYLAND BALTIMORE,R01,2020,227854,-0.07397141856030537
"CRCNS: Real-time neural decoding for calcium imaging Program Director/Principal Investigator (Last, First, Middle): Chen, Rong PROJECT DESCRIPTION A. BACKGROUND AND SIGNIFICANCE Real-time neural decoding centers on predicting behavior variables based on neural activity data, where the prediction is performed at a pace that reliably keeps up with the speed of the activity that is being monitored. Neuromodulation devices are becoming one of the most powerful tools for the treatment of brain disorders, enhancing neurocognitive performance, and demonstrating causality (Bergmann et al., 2016; Knotkova and Rasche, 2015). A precise neuromodulation system (Figure 1) integrates neural activity monitoring, real-time neural decoding, and neuromodulation. In precise neuromodulation, a decoding device predicts a behavior variable based on neural data streams in real-time. Based on the decoding results, neuromodulation parameters such as timing, frequency, duration, and amplitude are changed. Precise neuromodulation systems with closed-loop real-time feedback are superior to the fixed (open- loop) neuromodulation paradigm (Brocker et al., 2017; deBettencourt et al., 2015; Ezzyat et al., 2017). A recent direct brain stimulation study (Ezzyat et al., 2017) demonstrated significant advantages of precise neuromodulation over open-loop neuromodulation. Ezzyat et al. applied direct brain stimulation with decoding capability to patients with epilepsy to improve their memory. They found that stimulation increased memory function only if delivered when the decoding device indicated low encoding efficiency while stimulation decreased memory function if delivered when the decoding device indicated high encoding efficiency. An open-loop neuromodulation system with a fixed stimulation paradigm may not always facilitate memory function.  Miniature cellular imaging (Ghosh et al., 2011; Kerr and Nimmerjahn, 2012; Scott et al., 2013) is one of the most powerful ways to study neural circuits. It enables us to investigate neural circuits during behaviors for an understanding of network architecture of behavior, cognition, and emotion. Miniature cellular imaging records neuronal activity at cellular and sub-second levels of spatial and temporal resolution in freely moving animals. Miniature cellular imaging has many advantages. First, compared with in vivo multi-electrode recording, miniature calcium imaging can probe all cells in the field of view, and visualize the spatial location of monitored cells (Kerr et al., 2005). Second, compared with magnetic resonance imaging, which measures brain activity at the macroscopic scale and with low temporal resolution, miniature cellular imaging provides high spatial and temporal resolution. Third, fiber photometry (Cui et al., 2014) lacks cellular-level resolution, while miniature cellular imaging allows concurrent tracking of neural calcium activities at cellular spatial resolution. Simultaneous neural activity monitoring and intetvention Stimulation Calcium imaging Real-time decoding system Figure 1 A precise neuromodulation system. Our project centers on developing RNDC-Lab. PHS 398 (Rev. 01 /18 Approved Through 03/31/2020) Page 26  Miniature cellular imaging with real- time decoding capability captures the central vision of brain science, (The brain initiative, 2014). Combined with optogenetics, it is a tremendous asset to studying neural mechanisms underlying normal and disease states, and leads to precise neuromodulation. However, developing such systems is a challenging task. A major obstacle is the analysis of the large imaging streams that are generated. The massive high-dimensional data streams that are generated include 0MB No. 0925-0001 n/a",CRCNS: Real-time neural decoding for calcium imaging,9769912,R01NS110421,"['Address', 'Algorithms', 'Animals', 'Attention', 'BRAIN initiative', 'Behavior', 'Biological Models', 'Brain', 'Brain Diseases', 'Calcium', 'Cells', 'Cognition', 'Collection', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Detection', 'Devices', 'Disease', 'Electrodes', 'Emotions', 'Encapsulated', 'Epilepsy', 'Esthesia', 'Etiology', 'Event', 'Feedback', 'Fiber', 'Frequencies', 'Generations', 'Image', 'Knowledge', 'Laboratories', 'Language', 'Learning', 'Link', 'Location', 'Magnetic Resonance Imaging', 'Masks', 'Measures', 'Memory', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Network-based', 'Neurocognitive', 'Neurons', 'Neurosciences', 'Pain', 'Patients', 'Perception', 'Performance', 'Photometry', 'Principal Investigator', 'Process', 'Records', 'Research', 'Resolution', 'Science', 'Scientist', 'Sleep', 'Speed', 'Stream', 'System', 'Techniques', 'Time', 'Training', 'Vision', 'Wakefulness', 'Work', 'base', 'cellular imaging', 'cost efficient', 'data acquisition', 'deep neural network', 'design', 'experimental study', 'high dimensionality', 'improved', 'in vivo', 'innovation', 'multidimensional data', 'nervous system disorder', 'network architecture', 'neural circuit', 'neural stimulation', 'neuromechanism', 'neuroregulation', 'neurotransmission', 'novel', 'optical imaging', 'optogenetics', 'predictive modeling', 'programs', 'prototype', 'real-time images', 'relating to nervous system', 'signal processing', 'spatiotemporal', 'temporal measurement', 'tool']",NINDS,UNIVERSITY OF MARYLAND BALTIMORE,R01,2019,228265,-0.07397141856030537
"A Scalable Platform for Exploring and Analyzing Whole Brain Tissue Cleared Images Abstract  The ability of accurate localize and characterize cells in light sheet fluorescence microscopy (LSFM) image is indispensable for shedding new light on the understanding of three dimensional structures of the whole brain. In our previous work, we have successfully developed a 2D nuclear segmentation method for the nuclear cleared microscopy images using deep learning techniques. Although the convolutional neural networks show promise in segmenting cells in LSFM images, our previous work is confined in 2D segmentation scenario and suffers from the limited number of annotated data. In this project, we aim to develop a high throughput 3D cell segmentation engine, with the focus on improving the segmentation accuracy and generality. First, we will develop a cloud based semi-automatic annotation platform using the strength of virtual reality (VR) and crowd sourcing. The user-friendly annotation environment and stereoscopic view in VR can significantly improve the efficiency of manual annotation. We design a semi-automatic annotation workflow to largely reduce human intervention, and thus improve both the accuracy and the replicability of annotation across different users. Enlightened by the spirit of citizen science, we will extend the annotation software into a crowd sourcing platform which allows us to obtain a massive number of manual annotations in short time. Second, we will develop a fully 3D cell segmentation engine using 3D convolutional neural networks trained with the 3D annotated samples. Since it is often difficult to acquire isotropic LSFM images, we will further develop a super resolution method to impute a high resolution image to facilitate the 3D cell segmentation. Third, we will develop a transfer learning framework to make our 3D cell segmentation engine general enough to the application of novel LSFM data which might have significant gap of image appearance due to different imaging setup or clearing/staining protocol. This general framework will allow us to rapidly develop a specific cell segmentation solution for new LSFM data with very few or even no manual annotations, by transferring the existing 3D segmentation engine that has been trained with a sufficient number of annotated samples. Fourth, we will apply our computational tools to several pilot neuroscience studies: (1) Investigating how topoisomerase I (one of the autism linked transcriptional regulators) regulates brain structure, and (2) Investigating genetic influence on cell types in the developing human brain by quantifying the number of progenitor cells in fetal cortical tissue. Successful carrying out our project will have wide-reaching impact in neuroscience community in visualizing and analyzing complete cellular resolution maps of individual cell types within healthy and disease brain. The improved cell segmentation engine in 3D allows scientists from all over the world to share and process each other’s data accurately and efficiently, thus increasing reproducibility and power. Project Narrative This proposal aims to develop a next generation cell segmentation engine for the whole brain tissue cleared images. Our proposed work is built upon our previous 2D nuclear segmentation project using deep learning techniques. However, we found that our current computational tool is limited in 2D segmentation scenario and insufficient of annotated training samples. To address these limitations, we will first develop a cloud-based semi-automatic annotation tool with the capacity of virtual reality. Our annotation tool is designed to be cross- platform, which allows us to partner with “SciStarter” (the largest citizen science projects in the world) and acquire large amount of cell annotations from the science enthusiastic volunteers. Meanwhile, we will develop next generation 3D cell segmentation engine using an end-to-end fully connected convolution neural network. To facilitate 3D cell segmentation, we will also develop a super resolution method to impute an isotropic high- resolution image from a low-resolution microscopy image. After the development of 3D cell segmentation engine, we will continue to improve its generality by developing a transfer learning framework which enables us to rapidly deploy our 3D cell segmentation engine to the novel microscopy images without the time-consuming manual annotation step. Finally, we will apply our segmentation tool to visualize and quantify brain structure differences within genetically characterized mouse and human brain tissue at UNC neuroscience center. In the end of this project, we will release the software (both binary program and source code) and the 3D cell annotations, in order to facilitate the similar neuroscience studies in other institutes. Considering the importance of high throughput computational tools in quantifying three dimensional brain structure, this cutting- edge technique will be very useful in neuroscience research community.",A Scalable Platform for Exploring and Analyzing Whole Brain Tissue Cleared Images,9923760,R01NS110791,"['3-Dimensional', 'Address', 'Affect', 'Anecdotes', 'Appearance', 'Area', 'Biological', 'Brain', 'Brain Diseases', 'Brain region', 'Cell Nucleus', 'Cells', 'Communities', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Consumption', 'Data', 'Development', 'Environment', 'Evaluation', 'Fluorescence Microscopy', 'Genetic', 'Genetic Transcription', 'Genotype', 'Gold', 'Human', 'Image', 'Individual', 'Institutes', 'Intervention', 'Knock-out', 'Label', 'Lead', 'Learning', 'Light', 'Link', 'Manuals', 'Maps', 'Methods', 'Microscopy', 'Modeling', 'Mus', 'Neurosciences', 'Neurosciences Research', 'Noise', 'Nuclear', 'Performance', 'Process', 'Protocols documentation', 'Psychological Transfer', 'Reproducibility', 'Resolution', 'Sampling', 'Science', 'Scientist', 'Shapes', 'Slice', 'Source Code', 'Stains', 'Structure', 'Techniques', 'Technology', 'Time', 'Tissues', 'Training', 'Type I DNA Topoisomerases', 'Visual', 'Visualization', 'Work', 'annotation  system', 'autism spectrum disorder', 'base', 'bioimaging', 'brain tissue', 'cell type', 'citizen science', 'cloud based', 'computerized tools', 'contrast imaging', 'convolutional neural network', 'crowdsourcing', 'deep learning', 'design', 'fetal', 'flexibility', 'high resolution imaging', 'improved', 'microscopic imaging', 'next generation', 'novel', 'programs', 'stem cells', 'stereoscopic', 'success', 'three dimensional structure', 'tissue processing', 'tool', 'two-dimensional', 'user-friendly', 'virtual reality', 'volunteer']",NINDS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2020,335344,-0.04715641948498722
"A Scalable Platform for Exploring and Analyzing Whole Brain Tissue Cleared Images Abstract  The ability of accurate localize and characterize cells in light sheet fluorescence microscopy (LSFM) image is indispensable for shedding new light on the understanding of three dimensional structures of the whole brain. In our previous work, we have successfully developed a 2D nuclear segmentation method for the nuclear cleared microscopy images using deep learning techniques. Although the convolutional neural networks show promise in segmenting cells in LSFM images, our previous work is confined in 2D segmentation scenario and suffers from the limited number of annotated data. In this project, we aim to develop a high throughput 3D cell segmentation engine, with the focus on improving the segmentation accuracy and generality. First, we will develop a cloud based semi-automatic annotation platform using the strength of virtual reality (VR) and crowd sourcing. The user-friendly annotation environment and stereoscopic view in VR can significantly improve the efficiency of manual annotation. We design a semi-automatic annotation workflow to largely reduce human intervention, and thus improve both the accuracy and the replicability of annotation across different users. Enlightened by the spirit of citizen science, we will extend the annotation software into a crowd sourcing platform which allows us to obtain a massive number of manual annotations in short time. Second, we will develop a fully 3D cell segmentation engine using 3D convolutional neural networks trained with the 3D annotated samples. Since it is often difficult to acquire isotropic LSFM images, we will further develop a super resolution method to impute a high resolution image to facilitate the 3D cell segmentation. Third, we will develop a transfer learning framework to make our 3D cell segmentation engine general enough to the application of novel LSFM data which might have significant gap of image appearance due to different imaging setup or clearing/staining protocol. This general framework will allow us to rapidly develop a specific cell segmentation solution for new LSFM data with very few or even no manual annotations, by transferring the existing 3D segmentation engine that has been trained with a sufficient number of annotated samples. Fourth, we will apply our computational tools to several pilot neuroscience studies: (1) Investigating how topoisomerase I (one of the autism linked transcriptional regulators) regulates brain structure, and (2) Investigating genetic influence on cell types in the developing human brain by quantifying the number of progenitor cells in fetal cortical tissue. Successful carrying out our project will have wide-reaching impact in neuroscience community in visualizing and analyzing complete cellular resolution maps of individual cell types within healthy and disease brain. The improved cell segmentation engine in 3D allows scientists from all over the world to share and process each other’s data accurately and efficiently, thus increasing reproducibility and power. Project Narrative This proposal aims to develop a next generation cell segmentation engine for the whole brain tissue cleared images. Our proposed work is built upon our previous 2D nuclear segmentation project using deep learning techniques. However, we found that our current computational tool is limited in 2D segmentation scenario and insufficient of annotated training samples. To address these limitations, we will first develop a cloud-based semi-automatic annotation tool with the capacity of virtual reality. Our annotation tool is designed to be cross- platform, which allows us to partner with “SciStarter” (the largest citizen science projects in the world) and acquire large amount of cell annotations from the science enthusiastic volunteers. Meanwhile, we will develop next generation 3D cell segmentation engine using an end-to-end fully connected convolution neural network. To facilitate 3D cell segmentation, we will also develop a super resolution method to impute an isotropic high- resolution image from a low-resolution microscopy image. After the development of 3D cell segmentation engine, we will continue to improve its generality by developing a transfer learning framework which enables us to rapidly deploy our 3D cell segmentation engine to the novel microscopy images without the time-consuming manual annotation step. Finally, we will apply our segmentation tool to visualize and quantify brain structure differences within genetically characterized mouse and human brain tissue at UNC neuroscience center. In the end of this project, we will release the software (both binary program and source code) and the 3D cell annotations, in order to facilitate the similar neuroscience studies in other institutes. Considering the importance of high throughput computational tools in quantifying three dimensional brain structure, this cutting- edge technique will be very useful in neuroscience research community.",A Scalable Platform for Exploring and Analyzing Whole Brain Tissue Cleared Images,9714223,R01NS110791,"['3-Dimensional', 'Address', 'Affect', 'Anecdotes', 'Appearance', 'Area', 'Biological', 'Brain', 'Brain Diseases', 'Brain region', 'Cell Nucleus', 'Cells', 'Communities', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Consumption', 'Data', 'Development', 'Dimensions', 'Environment', 'Evaluation', 'Fluorescence Microscopy', 'Genetic', 'Genetic Transcription', 'Genotype', 'Gold', 'Human', 'Image', 'Imagery', 'Individual', 'Institutes', 'Intervention', 'Knock-out', 'Label', 'Lead', 'Learning', 'Light', 'Link', 'Manuals', 'Maps', 'Methods', 'Microscopy', 'Modeling', 'Mus', 'Neurosciences', 'Neurosciences Research', 'Noise', 'Nuclear', 'Performance', 'Process', 'Protocols documentation', 'Psychological Transfer', 'Reproducibility', 'Resolution', 'Sampling', 'Science', 'Scientist', 'Shapes', 'Slice', 'Source Code', 'Stains', 'Stem cells', 'Structure', 'Techniques', 'Technology', 'Time', 'Tissues', 'Training', 'Type I DNA Topoisomerases', 'Visual', 'Work', 'annotation  system', 'autism spectrum disorder', 'base', 'bioimaging', 'brain tissue', 'cell type', 'citizen science', 'cloud based', 'computerized tools', 'contrast imaging', 'convolutional neural network', 'crowdsourcing', 'deep learning', 'design', 'fetal', 'flexibility', 'high resolution imaging', 'improved', 'microscopic imaging', 'next generation', 'novel', 'programs', 'stereoscopic', 'success', 'three dimensional structure', 'tissue processing', 'tool', 'two-dimensional', 'user-friendly', 'virtual reality', 'volunteer']",NINDS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,335573,-0.04715641948498722
"Quantitative analysis of estrogen and sleep deprivation-induced blood and lymphatic vascular remodeling in the brain system Abstract Numerous small vessels making up the central nervous system blood and lymphatic vascular networks are heterogeneous and region-specific dynamic structures, whose segments, position, shape and function can change in response to physiological and pathophysiological conditions. To date it has not been possible to integrate blood and lymphatic vascular elements and their microenvironment to achieve a holistic quantitative characterization of the combined brain and meningeal tissue-scale vascular networks, its structure and function in normal and disease states. This application proposes to develop microscopy- based high-throughput image analysis techniques for automated extraction of blood and lymphatic vascular networks enabling quantitative morphodynamic characterization of cerebrovascular microenvironment changes in two intracranial compartments – the brain and dura mater. The study will focus on new algorithms for precise region-specific microvessel registration, mosaicing, segmentation, fusion and colocalization for constructing large tissue scale spatially aligned dual blood/lymphatic vascular network structural maps in the animals of both sexes, as well as characterization of heterogeneities of microvascular networks, including blood and lymphatic vasculature, under estrogen and sleep deprivation (the conditions relevant to multiple cerebrovascular disorders) compared to physiological settings. In other words, advanced microscopy-based techniques will be used to image blood and lymphatic vessels at sub-micron resolution in dura mater and the brain, and then cutting-edge deep machine learning imaging analysis methods will be employed to segment and quantify these vessels, their geometry, vessel wall structure, functionality, and interrelationship. Detailed structural analysis of microvascular networks is essential for accurate evaluation of the distribution of physical forces, substrate delivery and tissue clearance of waste, as well as sex differences and consequences of intracranial networks remodeling under physiological and pathological conditions. This will create knowledge enabling a better understanding of the pathogenesis of vascular impairments under estrogen and sleep deprivation, identify common molecular mechanisms and the efficacy and effectiveness of different therapeutic treatments. Without the ability to construct total structural and functional blood/lymphatic vascular network maps from studies limited to individual tissue component parts, it is little wonder that translation from the molecular and cellular levels to the whole organ and system levels is deficient and hinders translational progress towards a comprehensive understanding of the pathophysiology associated with a range of neurological disorders. Detailed analysis of structural relationships of both blood and lymphatic circulation in the brain system will have a direct impact on our general understanding of vascular function in brain/meningeal communication, and the cause and resolution of numerous diseases resulting from intracranial vascular disorders including impact of sex hormone (estrogen) deprivation, sleep deprivation, migraines, stroke, multiple sclerosis, dural arterio-venous fistulae, intradural hygroma and hematoma, spontaneous cerebral spinal fluid leaks, and intradural aneurysms that can lead to the development of neurological and cognitive impairment, including Alzheimer's. Quantitative description of blood and lymphatic vessel network structures using image analytics and machine learning algorithms distributed as software tools will have broad applications to quantification of other thin complex curvilinear anatomical structures (i.e. nerves, neuronal circuits, neurons, and neuroglia). The new software for blood and vessel network measurement will enable translation of fundamental pathophysiological knowledge gained from this proposal towards the development and assessment of the effectiveness of treatments and therapeutic interventions to enhance health, lengthen life, and reduce illness and disability associated with a range of neurological disorders.",Quantitative analysis of estrogen and sleep deprivation-induced blood and lymphatic vascular remodeling in the brain system,9712424,R01NS110915,"['Acute', 'Algorithmic Analysis', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Aneurysm', 'Animals', 'Arteriovenous fistula', 'Blood', 'Blood Circulation', 'Blood Vessels', 'Blood capillaries', 'Brain', 'Brain Mapping', 'Cardiovascular system', 'Cephalic', 'Cerebrospinal Fluid', 'Cerebrovascular Disorders', 'Chronic', 'Chronic Insomnia', 'Communication', 'Complex', 'Computer software', 'Cystic Lymphangioma', 'Detection', 'Development', 'Disease', 'Dura Mater', 'Dural Arteriovenous Fistulas', 'Effectiveness', 'Elements', 'Estrogens', 'Evaluation', 'Female', 'Functional disorder', 'Geometry', 'Gonadal Steroid Hormones', 'Health', 'Hematoma', 'Heterogeneity', 'Hybrids', 'Image', 'Image Analysis', 'Imaging Techniques', 'Impaired cognition', 'Impairment', 'Individual', 'Knowledge', 'Lead', 'Life', 'Link', 'Lymphatic', 'Lymphatic vessel', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Meningeal', 'Metabolism', 'Methods', 'Microscopy', 'Migraine', 'Modeling', 'Molecular', 'Morphology', 'Mosaicism', 'Multiple Sclerosis', 'Mus', 'Nerve', 'Neuraxis', 'Neuroglia', 'Neurologic', 'Neurons', 'Optical Coherence Tomography', 'Parietal', 'Pathogenesis', 'Pathologic', 'Pathway interactions', 'Physiological', 'Positioning Attribute', 'Resolution', 'Route', 'Sex Differences', 'Shapes', 'Site', 'Sleep', 'Sleep Deprivation', 'Sleep Disorders', 'Sleep disturbances', 'Software Tools', 'Stroke', 'Structure', 'Subdural Hematoma', 'Subdural Hygroma', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Intervention', 'Thinness', 'Tissues', 'Translations', 'Treatment Effectiveness', 'Vascular Diseases', 'Vascular remodeling', 'Vascular resistance', 'Venous', 'associated symptom', 'base', 'body system', 'cerebral microvasculature', 'cerebrovascular', 'clinically relevant', 'confocal imaging', 'deep learning', 'deprivation', 'disability', 'geometric structure', 'in vivo', 'lymphatic circulation', 'machine learning algorithm', 'male', 'microleakage', 'nervous system disorder', 'neuronal circuitry', 'noninvasive diagnosis', 'novel', 'response', 'sex', 'sleep pattern', 'solute', 'stem', 'submicron', 'tool', 'wasting']",NINDS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2019,540520,-0.15417923895033037
"Quantitative analysis of estrogen and sleep deprivation-induced blood and lymphatic vascular remodeling in the brain system Abstract Numerous small vessels making up the central nervous system blood and lymphatic vascular networks are heterogeneous and region-specific dynamic structures, whose segments, position, shape and function can change in response to physiological and pathophysiological conditions. To date it has not been possible to integrate blood and lymphatic vascular elements and their microenvironment to achieve a holistic quantitative characterization of the combined brain and meningeal tissue-scale vascular networks, its structure and function in normal and disease states. This application proposes to develop microscopy- based high-throughput image analysis techniques for automated extraction of blood and lymphatic vascular networks enabling quantitative morphodynamic characterization of cerebrovascular microenvironment changes in two intracranial compartments – the brain and dura mater. The study will focus on new algorithms for precise region-specific microvessel registration, mosaicing, segmentation, fusion and colocalization for constructing large tissue scale spatially aligned dual blood/lymphatic vascular network structural maps in the animals of both sexes, as well as characterization of heterogeneities of microvascular networks, including blood and lymphatic vasculature, under estrogen and sleep deprivation (the conditions relevant to multiple cerebrovascular disorders) compared to physiological settings. In other words, advanced microscopy-based techniques will be used to image blood and lymphatic vessels at sub-micron resolution in dura mater and the brain, and then cutting-edge deep machine learning imaging analysis methods will be employed to segment and quantify these vessels, their geometry, vessel wall structure, functionality, and interrelationship. Detailed structural analysis of microvascular networks is essential for accurate evaluation of the distribution of physical forces, substrate delivery and tissue clearance of waste, as well as sex differences and consequences of intracranial networks remodeling under physiological and pathological conditions. This will create knowledge enabling a better understanding of the pathogenesis of vascular impairments under estrogen and sleep deprivation, identify common molecular mechanisms and the efficacy and effectiveness of different therapeutic treatments. Without the ability to construct total structural and functional blood/lymphatic vascular network maps from studies limited to individual tissue component parts, it is little wonder that translation from the molecular and cellular levels to the whole organ and system levels is deficient and hinders translational progress towards a comprehensive understanding of the pathophysiology associated with a range of neurological disorders. Detailed analysis of structural relationships of both blood and lymphatic circulation in the brain system will have a direct impact on our general understanding of vascular function in brain/meningeal communication, and the cause and resolution of numerous diseases resulting from intracranial vascular disorders including impact of sex hormone (estrogen) deprivation, sleep deprivation, migraines, stroke, multiple sclerosis, dural arterio-venous fistulae, intradural hygroma and hematoma, spontaneous cerebral spinal fluid leaks, and intradural aneurysms that can lead to the development of neurological and cognitive impairment, including Alzheimer's. Quantitative description of blood and lymphatic vessel network structures using image analytics and machine learning algorithms distributed as software tools will have broad applications to quantification of other thin complex curvilinear anatomical structures (i.e. nerves, neuronal circuits, neurons, and neuroglia). The new software for blood and vessel network measurement will enable translation of fundamental pathophysiological knowledge gained from this proposal towards the development and assessment of the effectiveness of treatments and therapeutic interventions to enhance health, lengthen life, and reduce illness and disability associated with a range of neurological disorders.",Quantitative analysis of estrogen and sleep deprivation-induced blood and lymphatic vascular remodeling in the brain system,9914136,R01NS110915,"['Acute', 'Algorithmic Analysis', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Aneurysm', 'Animals', 'Arteriovenous fistula', 'Blood', 'Blood Circulation', 'Blood Vessels', 'Blood capillaries', 'Brain', 'Brain Mapping', 'Cardiovascular system', 'Cephalic', 'Cerebrospinal Fluid', 'Cerebrovascular Disorders', 'Chronic', 'Chronic Insomnia', 'Communication', 'Complex', 'Computer software', 'Cystic Lymphangioma', 'Detection', 'Development', 'Disease', 'Dura Mater', 'Dural Arteriovenous Fistulas', 'Effectiveness', 'Elements', 'Estrogens', 'Evaluation', 'Female', 'Functional disorder', 'Geometry', 'Gonadal Steroid Hormones', 'Health', 'Hematoma', 'Heterogeneity', 'Hybrids', 'Image', 'Image Analysis', 'Imaging Techniques', 'Impaired cognition', 'Impairment', 'Individual', 'Knowledge', 'Lead', 'Life', 'Link', 'Lymphatic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Meningeal', 'Metabolism', 'Methods', 'Microscopy', 'Migraine', 'Modeling', 'Molecular', 'Morphology', 'Mosaicism', 'Multiple Sclerosis', 'Mus', 'Nerve', 'Neuraxis', 'Neuroglia', 'Neurologic', 'Neurons', 'Optical Coherence Tomography', 'Parietal', 'Pathogenesis', 'Pathologic', 'Pathway interactions', 'Physiological', 'Positioning Attribute', 'Resolution', 'Route', 'Sex Differences', 'Shapes', 'Site', 'Sleep', 'Sleep Deprivation', 'Sleep Disorders', 'Sleep disturbances', 'Software Tools', 'Stroke', 'Structure', 'Subdural Hematoma', 'Subdural Hygroma', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Intervention', 'Thinness', 'Tissues', 'Translations', 'Treatment Effectiveness', 'Vascular Diseases', 'Vascular remodeling', 'Vascular resistance', 'Venous', 'associated symptom', 'base', 'body system', 'cerebral microvasculature', 'cerebrovascular', 'clinically relevant', 'confocal imaging', 'deep learning', 'deprivation', 'disability', 'geometric structure', 'in vivo', 'lymphatic circulation', 'lymphatic vasculature', 'lymphatic vessel', 'machine learning algorithm', 'male', 'microleakage', 'nervous system disorder', 'neuronal circuitry', 'noninvasive diagnosis', 'novel', 'response', 'sex', 'sleep pattern', 'solute', 'stem', 'submicron', 'tool', 'wasting']",NINDS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2020,540520,-0.15417923895033037
"Determination of structure, dynamics and energetics of enzyme reactions ﻿    DESCRIPTION (provided by applicant): The determination of enzyme mechanisms is a central topic in the study of biomolecular systems because they are involved in most processes in living organisms. The development of new experimental and computational biophysics methods that allow new and ever more detailed views of these processes is of fundamental importance not just from the basic science point of view, but also due to the wide range of applications of the methods and the knowledge derived from them.  The goal of the proposal is to create a ""molecular movie"" where the position, movement and energy of every atom in the system followed over the course of the reaction. This will be achieved by pursuing two Specific Aims: (i) Time-Resolved Laue Crystallography of HMG-CoA Reductase (HMGR) and (ii) Simulation of the Reaction Pathway of HMGR. The application of the methodology will allow access to a level of detailed knowledge about enzyme chemistry that was not attainable up to now.  The proposed approach relies on the emerging convergence of the timescales accessible by time- resolved crystallography and computational methods. We will use our recently developed photocaged cofactors to generate structural ""snapshots"" with a time resolution of 1-100 µs by Laue crystallography. The ensembles of intermediate states generated by these snapshots will be deconvoluted using singular value decomposition (SVD) and connected using long timescale molecular dynamics (MD) simulations to provide structural, dynamic, and energetic insights into the complete reaction pathway. Polarizable and non- polarizable transition state force fields (TSFF) will be generated by the quantum-guided molecular mechanics (Q2MM). The use of TSFFs is 102-104 times faster than the widely used QM/MM methods, thus allowing extensive sampling, and treats the entire system at a consistent level, thus preventing the well- known problems resulting from the QM/MM interface region. Iterative cycles of crystal structure -> MD simulation -> Markov State ensemble generation -> SVD analysis of Laue data -> new time resolved structures will be used to study a complex reaction pathway, which can be broken down into smaller steps to facilitate both experimental and computational approaches.  This combined methodology will be applied to the case of Pseudomonas mevalonii HMGR, which has a complex reaction mechanism involving three chemical steps, six large-scale conformational changes and two cofactor exchange steps. HMGR is of broad biomedical interest because it is the target of the widely used statins and a potential target for antibacterial treatments by new classes of antibiotics, but the methodology developed in this proposal is in principle applicable to a wide range of systems. To promote the use of the experimental and computational innovations introduced, all tool compounds and computational codes to be developed will be made available to the broader scientific community. PUBLIC HEALTH RELEVANCE: The detailed study of enzyme mechanisms is a cornerstone of biophysical chemistry that, while basic in nature, has had a major impact on human health including the development of new mechanism- based drugs for a range of diseases and an understanding of the mechanism of action for existing drugs that allows the design of combination therapies. The combination of Laue crystallography and long-scale MD simulations will allow simultaneous studies of structure, dynamics and energetic studies with unprecedented detail. The application to HMG-CoA reductase, arguably the single most important drug target in western industrialized countries, will demonstrate the applicability of the methodology to an enzyme of very high mechanistic complexity. 1","Determination of structure, dynamics and energetics of enzyme reactions",9517960,R01GM111645,"['Anti-Bacterial Agents', 'Antibiotics', 'Basic Science', 'Biochemistry', 'Biological', 'Biophysics', 'Chemicals', 'Chemistry', 'Cholesterol', 'Code', 'Combined Modality Therapy', 'Communities', 'Complex', 'Computing Methodologies', 'Crystallization', 'Crystallography', 'Data', 'Developed Countries', 'Development', 'Disease', 'Drug Targeting', 'Enzymes', 'Evolution', 'Free Energy', 'Generations', 'Goals', 'Health', 'Human', 'Hydroxymethylglutaryl-CoA reductase', 'Knowledge', 'Life', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Conformation', 'Molecular Structure', 'Movement', 'Nature', 'Noise', 'Organism', 'Pathway interactions', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Proteins', 'Pseudomonas', 'Reaction', 'Resolution', 'Roentgen Rays', 'Role', 'Running', 'Sampling', 'Science', 'Signal Transduction', 'Structure', 'System', 'Systems Analysis', 'Time', 'Validation', 'Weight', 'Work', 'base', 'biophysical chemistry', 'biophysical techniques', 'cofactor', 'computer studies', 'design', 'electron density', 'electronic structure', 'enzyme mechanism', 'improved', 'innovation', 'insight', 'interest', 'irradiation', 'millisecond', 'molecular dynamics', 'molecular mechanics', 'molecular scale', 'movie', 'new therapeutic target', 'prevent', 'public health relevance', 'quantum', 'simulation', 'tool']",NIGMS,UNIVERSITY OF NOTRE DAME,R01,2018,296099,-0.016888871882996866
"Determination of structure, dynamics and energetics of enzyme reactions ﻿    DESCRIPTION (provided by applicant): The determination of enzyme mechanisms is a central topic in the study of biomolecular systems because they are involved in most processes in living organisms. The development of new experimental and computational biophysics methods that allow new and ever more detailed views of these processes is of fundamental importance not just from the basic science point of view, but also due to the wide range of applications of the methods and the knowledge derived from them.  The goal of the proposal is to create a ""molecular movie"" where the position, movement and energy of every atom in the system followed over the course of the reaction. This will be achieved by pursuing two Specific Aims: (i) Time-Resolved Laue Crystallography of HMG-CoA Reductase (HMGR) and (ii) Simulation of the Reaction Pathway of HMGR. The application of the methodology will allow access to a level of detailed knowledge about enzyme chemistry that was not attainable up to now.  The proposed approach relies on the emerging convergence of the timescales accessible by time- resolved crystallography and computational methods. We will use our recently developed photocaged cofactors to generate structural ""snapshots"" with a time resolution of 1-100 µs by Laue crystallography. The ensembles of intermediate states generated by these snapshots will be deconvoluted using singular value decomposition (SVD) and connected using long timescale molecular dynamics (MD) simulations to provide structural, dynamic, and energetic insights into the complete reaction pathway. Polarizable and non- polarizable transition state force fields (TSFF) will be generated by the quantum-guided molecular mechanics (Q2MM). The use of TSFFs is 102-104 times faster than the widely used QM/MM methods, thus allowing extensive sampling, and treats the entire system at a consistent level, thus preventing the well- known problems resulting from the QM/MM interface region. Iterative cycles of crystal structure -> MD simulation -> Markov State ensemble generation -> SVD analysis of Laue data -> new time resolved structures will be used to study a complex reaction pathway, which can be broken down into smaller steps to facilitate both experimental and computational approaches.  This combined methodology will be applied to the case of Pseudomonas mevalonii HMGR, which has a complex reaction mechanism involving three chemical steps, six large-scale conformational changes and two cofactor exchange steps. HMGR is of broad biomedical interest because it is the target of the widely used statins and a potential target for antibacterial treatments by new classes of antibiotics, but the methodology developed in this proposal is in principle applicable to a wide range of systems. To promote the use of the experimental and computational innovations introduced, all tool compounds and computational codes to be developed will be made available to the broader scientific community. PUBLIC HEALTH RELEVANCE: The detailed study of enzyme mechanisms is a cornerstone of biophysical chemistry that, while basic in nature, has had a major impact on human health including the development of new mechanism- based drugs for a range of diseases and an understanding of the mechanism of action for existing drugs that allows the design of combination therapies. The combination of Laue crystallography and long-scale MD simulations will allow simultaneous studies of structure, dynamics and energetic studies with unprecedented detail. The application to HMG-CoA reductase, arguably the single most important drug target in western industrialized countries, will demonstrate the applicability of the methodology to an enzyme of very high mechanistic complexity. 1","Determination of structure, dynamics and energetics of enzyme reactions",9278009,R01GM111645,"['Anti-Bacterial Agents', 'Antibiotics', 'Basic Science', 'Biochemistry', 'Biological', 'Biophysics', 'Chemicals', 'Chemistry', 'Cholesterol', 'Code', 'Combined Modality Therapy', 'Communities', 'Complex', 'Computing Methodologies', 'Crystallization', 'Crystallography', 'Data', 'Developed Countries', 'Development', 'Disease', 'Drug Targeting', 'Enzymes', 'Evolution', 'Free Energy', 'Generations', 'Goals', 'Health', 'Human', 'Hydroxymethylglutaryl-CoA reductase', 'Knowledge', 'Life', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Conformation', 'Molecular Structure', 'Movement', 'Nature', 'Noise', 'Organism', 'Pathway interactions', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Proteins', 'Pseudomonas', 'Reaction', 'Resolution', 'Roentgen Rays', 'Role', 'Running', 'Sampling', 'Science', 'Signal Transduction', 'Structure', 'System', 'Systems Analysis', 'Time', 'Validation', 'Weight', 'Work', 'base', 'biophysical chemistry', 'biophysical techniques', 'cofactor', 'computer studies', 'design', 'electron density', 'electronic structure', 'enzyme mechanism', 'improved', 'innovation', 'insight', 'interest', 'irradiation', 'millisecond', 'molecular dynamics', 'molecular mechanics', 'molecular scale', 'movie', 'new therapeutic target', 'prevent', 'public health relevance', 'quantum', 'simulation', 'tool']",NIGMS,UNIVERSITY OF NOTRE DAME,R01,2017,296223,-0.016888871882996866
"Determination of structure, dynamics and energetics of enzyme reactions ﻿    DESCRIPTION (provided by applicant): The determination of enzyme mechanisms is a central topic in the study of biomolecular systems because they are involved in most processes in living organisms. The development of new experimental and computational biophysics methods that allow new and ever more detailed views of these processes is of fundamental importance not just from the basic science point of view, but also due to the wide range of applications of the methods and the knowledge derived from them.  The goal of the proposal is to create a ""molecular movie"" where the position, movement and energy of every atom in the system followed over the course of the reaction. This will be achieved by pursuing two Specific Aims: (i) Time-Resolved Laue Crystallography of HMG-CoA Reductase (HMGR) and (ii) Simulation of the Reaction Pathway of HMGR. The application of the methodology will allow access to a level of detailed knowledge about enzyme chemistry that was not attainable up to now.  The proposed approach relies on the emerging convergence of the timescales accessible by time- resolved crystallography and computational methods. We will use our recently developed photocaged cofactors to generate structural ""snapshots"" with a time resolution of 1-100 µs by Laue crystallography. The ensembles of intermediate states generated by these snapshots will be deconvoluted using singular value decomposition (SVD) and connected using long timescale molecular dynamics (MD) simulations to provide structural, dynamic, and energetic insights into the complete reaction pathway. Polarizable and non- polarizable transition state force fields (TSFF) will be generated by the quantum-guided molecular mechanics (Q2MM). The use of TSFFs is 102-104 times faster than the widely used QM/MM methods, thus allowing extensive sampling, and treats the entire system at a consistent level, thus preventing the well- known problems resulting from the QM/MM interface region. Iterative cycles of crystal structure -> MD simulation -> Markov State ensemble generation -> SVD analysis of Laue data -> new time resolved structures will be used to study a complex reaction pathway, which can be broken down into smaller steps to facilitate both experimental and computational approaches.  This combined methodology will be applied to the case of Pseudomonas mevalonii HMGR, which has a complex reaction mechanism involving three chemical steps, six large-scale conformational changes and two cofactor exchange steps. HMGR is of broad biomedical interest because it is the target of the widely used statins and a potential target for antibacterial treatments by new classes of antibiotics, but the methodology developed in this proposal is in principle applicable to a wide range of systems. To promote the use of the experimental and computational innovations introduced, all tool compounds and computational codes to be developed will be made available to the broader scientific community. PUBLIC HEALTH RELEVANCE: The detailed study of enzyme mechanisms is a cornerstone of biophysical chemistry that, while basic in nature, has had a major impact on human health including the development of new mechanism- based drugs for a range of diseases and an understanding of the mechanism of action for existing drugs that allows the design of combination therapies. The combination of Laue crystallography and long-scale MD simulations will allow simultaneous studies of structure, dynamics and energetic studies with unprecedented detail. The application to HMG-CoA reductase, arguably the single most important drug target in western industrialized countries, will demonstrate the applicability of the methodology to an enzyme of very high mechanistic complexity. 1","Determination of structure, dynamics and energetics of enzyme reactions",9100786,R01GM111645,"['Anti-Bacterial Agents', 'Antibiotics', 'Basic Science', 'Biochemistry', 'Biological', 'Biophysics', 'Chemicals', 'Chemistry', 'Cholesterol', 'Code', 'Combined Modality Therapy', 'Communities', 'Complex', 'Computing Methodologies', 'Crystallography', 'Data', 'Developed Countries', 'Development', 'Disease', 'Drug Targeting', 'Enzymes', 'Evolution', 'Free Energy', 'Generations', 'Goals', 'Health', 'Human', 'Hydroxymethylglutaryl-CoA reductase', 'Knowledge', 'Life', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Movement', 'Nature', 'Noise', 'Organism', 'Pathway interactions', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Proteins', 'Pseudomonas', 'Reaction', 'Resolution', 'Roentgen Rays', 'Role', 'Running', 'Sampling', 'Science', 'Signal Transduction', 'Structure', 'System', 'Systems Analysis', 'Time', 'Validation', 'Weight', 'Work', 'base', 'biophysical chemistry', 'cofactor', 'computer studies', 'design', 'electron density', 'electronic structure', 'enzyme mechanism', 'improved', 'innovation', 'insight', 'interest', 'irradiation', 'millisecond', 'molecular dynamics', 'molecular mechanics', 'movie', 'prevent', 'quantum', 'simulation', 'tool']",NIGMS,UNIVERSITY OF NOTRE DAME,R01,2016,296349,-0.016888871882996866
"Determination of structure, dynamics and energetics of enzyme reactions ﻿    DESCRIPTION (provided by applicant): The determination of enzyme mechanisms is a central topic in the study of biomolecular systems because they are involved in most processes in living organisms. The development of new experimental and computational biophysics methods that allow new and ever more detailed views of these processes is of fundamental importance not just from the basic science point of view, but also due to the wide range of applications of the methods and the knowledge derived from them.  The goal of the proposal is to create a ""molecular movie"" where the position, movement and energy of every atom in the system followed over the course of the reaction. This will be achieved by pursuing two Specific Aims: (i) Time-Resolved Laue Crystallography of HMG-CoA Reductase (HMGR) and (ii) Simulation of the Reaction Pathway of HMGR. The application of the methodology will allow access to a level of detailed knowledge about enzyme chemistry that was not attainable up to now.  The proposed approach relies on the emerging convergence of the timescales accessible by time- resolved crystallography and computational methods. We will use our recently developed photocaged cofactors to generate structural ""snapshots"" with a time resolution of 1-100 µs by Laue crystallography. The ensembles of intermediate states generated by these snapshots will be deconvoluted using singular value decomposition (SVD) and connected using long timescale molecular dynamics (MD) simulations to provide structural, dynamic, and energetic insights into the complete reaction pathway. Polarizable and non- polarizable transition state force fields (TSFF) will be generated by the quantum-guided molecular mechanics (Q2MM). The use of TSFFs is 102-104 times faster than the widely used QM/MM methods, thus allowing extensive sampling, and treats the entire system at a consistent level, thus preventing the well- known problems resulting from the QM/MM interface region. Iterative cycles of crystal structure -> MD simulation -> Markov State ensemble generation -> SVD analysis of Laue data -> new time resolved structures will be used to study a complex reaction pathway, which can be broken down into smaller steps to facilitate both experimental and computational approaches.  This combined methodology will be applied to the case of Pseudomonas mevalonii HMGR, which has a complex reaction mechanism involving three chemical steps, six large-scale conformational changes and two cofactor exchange steps. HMGR is of broad biomedical interest because it is the target of the widely used statins and a potential target for antibacterial treatments by new classes of antibiotics, but the methodology developed in this proposal is in principle applicable to a wide range of systems. To promote the use of the experimental and computational innovations introduced, all tool compounds and computational codes to be developed will be made available to the broader scientific community.         PUBLIC HEALTH RELEVANCE: The detailed study of enzyme mechanisms is a cornerstone of biophysical chemistry that, while basic in nature, has had a major impact on human health including the development of new mechanism- based drugs for a range of diseases and an understanding of the mechanism of action for existing drugs that allows the design of combination therapies. The combination of Laue crystallography and long-scale MD simulations will allow simultaneous studies of structure, dynamics and energetic studies with unprecedented detail. The application to HMG-CoA reductase, arguably the single most important drug target in western industrialized countries, will demonstrate the applicability of the methodology to an enzyme of very high mechanistic complexity. 1            ","Determination of structure, dynamics and energetics of enzyme reactions",8888788,R01GM111645,"['Anti-Bacterial Agents', 'Antibiotics', 'Basic Science', 'Biochemistry', 'Biological', 'Biophysics', 'Chemicals', 'Chemistry', 'Cholesterol', 'Code', 'Combined Modality Therapy', 'Communities', 'Complex', 'Computing Methodologies', 'Crystallography', 'Data', 'Developed Countries', 'Development', 'Disease', 'Drug Targeting', 'Enzymes', 'Evolution', 'Free Energy', 'Generations', 'Goals', 'Health', 'Human', 'Hydroxymethylglutaryl-CoA reductase', 'Knowledge', 'Life', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Movement', 'Nature', 'Noise', 'Organism', 'Pathway interactions', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Proteins', 'Pseudomonas', 'Reaction', 'Resolution', 'Roentgen Rays', 'Role', 'Running', 'Sampling', 'Science', 'Signal Transduction', 'Structure', 'System', 'Systems Analysis', 'Time', 'Validation', 'Weight', 'Work', 'base', 'biophysical chemistry', 'cofactor', 'computer studies', 'design', 'electron density', 'electronic structure', 'enzyme mechanism', 'improved', 'innovation', 'insight', 'interest', 'irradiation', 'millisecond', 'molecular dynamics', 'molecular mechanics', 'movie', 'prevent', 'public health relevance', 'quantum', 'simulation', 'tool']",NIGMS,UNIVERSITY OF NOTRE DAME,R01,2015,309473,-0.016888871882996866
"Development of advanced voltammetric method for basal neurotransmitter level measurement PROJECT SUMMARY We propose to develop and optimize an advanced neurochemical recording technique that would be able to measure relatively rapid physiologically representative second-to-second changes in basal concentrations of specific neurochemicals, such as dopamine, in the brains of awake behaving animals. Microdialysis, a commonly used in vivo sampling technique, is able to measure changes that occur in basal levels. However, in practice the sampling timescale is significantly limited to minute-to-minute changes and it suffers from poor spatial resolution and induces significant tissue damage. As well, conventional in vivo electrochemical recording techniques, such as fast-scan cyclic voltammetry, are intrinsically limited to measuring phasic (stimulation-induced) changes in neurochemical concentrations and not changes in basal concentrations. The proposed electrochemical technique we call Multiple Cyclic Square Wave Voltammetry (M-CSWV) will enable second-to-second measurements of basal extracellular levels of neurochemicals with exceptional spatial resolution, sensitivity, specificity, and minimal tissue disturbance. This proposal leverages our unique expertise in neuroscience, electrochemistry, software development, and engineering to develop and validate this novel neurochemical recording technology for broad use in basic neuroscience research, clinical brain neuromodulation, and a variety of electrochemical applications. Our initial animal studies will guide and inform the application of our investigational technique for use by the general neuroscience and medical community. Our proposal seeks to (1) establish M-CSWV as a reliable research tool that is capable of identifying and quantifying basal dopamine extracellular levels in vivo with unsurpassed sensitivity and selectivity; and (2) validate the use of M-CSWV for in vivo chronic selective measurement of basal dopamine concentrations and application in an animal model of drug-induced neurochemical sensitization. PROJECT NARRATIVE Neurochemicals in the brain, such as dopamine, transmit information between neurons to process input and produce normal behavior, but that occasionally when their levels are disrupted lead to neurologic and psychiatric disorders. We have developed a novel neurochemical recording method called Multi-Cyclic Square Wave Voltammetry that for the first time measures basal neurochemical concentrations in real-time in the brain with single-second time resolution and unprecedented chemical selectivity. Furthermore, we propose to standardize this novel technique for use in neuroscience research directed to understanding the neurochemical basis of neuropsychiatric diseases by demonstrating its capability to quantify basal levels in a well-known animal model of drug-induced neurochemical and behavioral sensitization.",Development of advanced voltammetric method for basal neurotransmitter level measurement,9994394,R01NS112176,"['Acute', 'Adsorption', 'Advanced Development', 'Amphetamines', 'Animal Model', 'Animals', 'Behavior', 'Behavioral', 'Biological', 'Brain', 'Cells', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Communities', 'Computer software', 'Corpus striatum structure', 'Data', 'Data Analyses', 'Dialysis procedure', 'Dimensions', 'Disease', 'Dopamine', 'Dopaminergic Agents', 'Drug Modelings', 'Electrochemistry', 'Engineering', 'Equilibrium', 'Future', 'Goals', 'Implant', 'In Vitro', 'Injections', 'Investigation', 'Lead', 'Link', 'Literature', 'Measurement', 'Measures', 'Medical', 'Mental disorders', 'Methods', 'Microdialysis', 'Microelectrodes', 'Modeling', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Neurotransmitters', 'Periodicity', 'Pharmaceutical Preparations', 'Pharmacology', 'Pharmacotherapy', 'Phase', 'Physiological', 'Principal Component Analysis', 'Process', 'Publishing', 'Rattus', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Sensitivity and Specificity', 'Slice', 'Spectrum Analysis', 'Standardization', 'Stimulus', 'Surface', 'Synapses', 'Synaptic plasticity', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Validation', 'Variant', 'awake', 'base', 'behavioral sensitization', 'carbon fiber', 'electric impedance', 'extracellular', 'in vivo', 'kinetic model', 'method development', 'nervous system disorder', 'neurochemistry', 'neuropsychiatric disorder', 'neuroregulation', 'neurotransmission', 'noradrenergic', 'novel', 'phase change', 'receptor', 'response', 'software development', 'temporal measurement', 'tool', 'two-dimensional']",NINDS,MAYO CLINIC ROCHESTER,R01,2020,334425,-0.00451264588695903
"Development of advanced voltammetric method for basal neurotransmitter level measurement PROJECT SUMMARY We propose to develop and optimize an advanced neurochemical recording technique that would be able to measure relatively rapid physiologically representative second-to-second changes in basal concentrations of specific neurochemicals, such as dopamine, in the brains of awake behaving animals. Microdialysis, a commonly used in vivo sampling technique, is able to measure changes that occur in basal levels. However, in practice the sampling timescale is significantly limited to minute-to-minute changes and it suffers from poor spatial resolution and induces significant tissue damage. As well, conventional in vivo electrochemical recording techniques, such as fast-scan cyclic voltammetry, are intrinsically limited to measuring phasic (stimulation-induced) changes in neurochemical concentrations and not changes in basal concentrations. The proposed electrochemical technique we call Multiple Cyclic Square Wave Voltammetry (M-CSWV) will enable second-to-second measurements of basal extracellular levels of neurochemicals with exceptional spatial resolution, sensitivity, specificity, and minimal tissue disturbance. This proposal leverages our unique expertise in neuroscience, electrochemistry, software development, and engineering to develop and validate this novel neurochemical recording technology for broad use in basic neuroscience research, clinical brain neuromodulation, and a variety of electrochemical applications. Our initial animal studies will guide and inform the application of our investigational technique for use by the general neuroscience and medical community. Our proposal seeks to (1) establish M-CSWV as a reliable research tool that is capable of identifying and quantifying basal dopamine extracellular levels in vivo with unsurpassed sensitivity and selectivity; and (2) validate the use of M-CSWV for in vivo chronic selective measurement of basal dopamine concentrations and application in an animal model of drug-induced neurochemical sensitization. PROJECT NARRATIVE Neurochemicals in the brain, such as dopamine, transmit information between neurons to process input and produce normal behavior, but that occasionally when their levels are disrupted lead to neurologic and psychiatric disorders. We have developed a novel neurochemical recording method called Multi-Cyclic Square Wave Voltammetry that for the first time measures basal neurochemical concentrations in real-time in the brain with single-second time resolution and unprecedented chemical selectivity. Furthermore, we propose to standardize this novel technique for use in neuroscience research directed to understanding the neurochemical basis of neuropsychiatric diseases by demonstrating its capability to quantify basal levels in a well-known animal model of drug-induced neurochemical and behavioral sensitization.",Development of advanced voltammetric method for basal neurotransmitter level measurement,9796267,R01NS112176,"['Acute', 'Adsorption', 'Advanced Development', 'Amphetamines', 'Animal Model', 'Animals', 'Behavior', 'Behavioral', 'Biological', 'Brain', 'Cells', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Communities', 'Computer software', 'Corpus striatum structure', 'Data', 'Data Analyses', 'Dialysis procedure', 'Dimensions', 'Disease', 'Dopamine', 'Dopaminergic Agents', 'Drug Modelings', 'Electrochemistry', 'Engineering', 'Equilibrium', 'Future', 'Goals', 'Implant', 'In Vitro', 'Injections', 'Investigation', 'Kinetics', 'Lead', 'Link', 'Literature', 'Measurement', 'Measures', 'Medical', 'Mental disorders', 'Methods', 'Microdialysis', 'Microelectrodes', 'Modeling', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Neurotransmitters', 'Periodicity', 'Pharmaceutical Preparations', 'Pharmacology', 'Pharmacotherapy', 'Phase', 'Physiological', 'Principal Component Analysis', 'Process', 'Publishing', 'Rattus', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Sensitivity and Specificity', 'Slice', 'Spectrum Analysis', 'Standardization', 'Stimulus', 'Surface', 'Synapses', 'Synaptic plasticity', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Validation', 'Variant', 'awake', 'base', 'behavioral sensitization', 'carbon fiber', 'electric impedance', 'extracellular', 'in vivo', 'method development', 'nervous system disorder', 'neurochemistry', 'neuropsychiatric disorder', 'neuroregulation', 'neurotransmission', 'noradrenergic', 'novel', 'phase change', 'receptor', 'response', 'software development', 'temporal measurement', 'tool', 'two-dimensional']",NINDS,MAYO CLINIC ROCHESTER,R01,2019,349175,-0.00451264588695903
"Quantitative MR Imaging of Vascular Factors in Parkinsons Disease Abstract Vascular health has been shown to be an important factor in the development of neurodegenerative diseases like Alzheimer’s and Parkinson’s diseases. Hence, the ability to measure reliably and quantitatively early hemodynamic changes in the aging brain can be a powerful tool for diagnosing, studying, and developing treatments. Arterial Spin Labeling (ASL) magnetic resonance imaging can yield quantitative perfusion images without the use of contrast agents. We propose that combining new ASL techniques, such as Velocity Selective Inversion (VSI) labeling pulses and magnetic resonance fingerprinting (MRF) with deep learning regression methods will allow quantification of multiple hemodynamic parameters beyond perfusion, thus providing a much more nuanced picture of the state of the vasculature. We also expect that the new technique will offer dramatic improvements in SNR, specificity and sensitivity of ASL, and that the proposed techniques will have many other applications in research and in the clinic. We propose to use these techniques to fill the knowledge gap regarding the relationship between vascular changes and Parkinson’s disease and its symptoms, particularly fatigue, whose pathogenesis is not well understood. If we are successful in this application, future work will use the hemodynamic parameters of interest as biomarkers to assess risk of neurodegeneration, determine therapeutic targets, and guide in the development of new therapies. Public Health Statement Vascular health is an important factor in the development of neurodegenerative diseases like Alzheimer’s and Parkinson’s diseases We propose to develop a method that can produce quantifiable images of multiple blood flow related parameters with a single scan and without the use of contrast injections. We will use this method to gain a better understanding of the relationship between Parkinson’s disease and cerebral blood flow, and expect that our method will have multiple applications beside Parkinson’s disease.",Quantitative MR Imaging of Vascular Factors in Parkinsons Disease,9968566,R01NS112233,"['Address', 'Agreement', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arteries', 'Biological', 'Biological Markers', 'Blood Vessels', 'Blood Volume', 'Blood flow', 'Bolus Infusion', 'Brain', 'Cerebrovascular Circulation', 'Classification', 'Clinic', 'Clinical', 'Contrast Media', 'Development', 'Diagnosis', 'Disease', 'Ensure', 'Etiology', 'Fatigue', 'Fingerprint', 'Future', 'Health', 'Human Volunteers', 'Hybrids', 'Image', 'Injections', 'Knowledge', 'Label', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Movement', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Neuronal Injury', 'Noise', 'Parkinson Disease', 'Pathogenesis', 'Pathologic', 'Patients', 'Perfusion', 'Physiologic pulse', 'Public Health', 'Reproducibility', 'Research', 'Risk', 'Scanning', 'Sensitivity and Specificity', 'Severities', 'Signal Transduction', 'Spatial Distribution', 'Spin Labels', 'Symptoms', 'Techniques', 'Testing', 'Time', 'Tissues', 'Translating', 'Validation', 'Weight', 'Work', 'aging brain', 'deep learning', 'feeding', 'hemodynamics', 'imaging biomarker', 'insight', 'interest', 'machine learning algorithm', 'neural network', 'non-invasive imaging', 'novel therapeutics', 'patient stratification', 'perfusion imaging', 'relating to nervous system', 'success', 'support vector machine', 'therapeutic target', 'tool', 'vascular factor', 'vector', 'white matter']",NINDS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,348762,-0.01957951944338784
"In vivo Imaging of Neuroactivity in the Deep Forward Scattering Regime Using Speckle Identification and Demixing (SPID) Microscopy PROJECT SUMMARY Optical imaging of neuronal activity in the mammalian brain at depth and at high spatial and temporal resolution remains a key challenge in neuroscience. This is because tissue scattering eliminates directional information carried by photons, with a characteristic length scale of hundreds of microns. As a result, the remaining unscattered, or “ballistic” component of light decays exponentially with depth. Since all optical microscopy modalities rely—either in the excitation or the emission—on this ballistic component of light for image formation, the limitations imposed by scattering limit the maximally obtainable imaging depths to ~1.5 mm in the rodent brain. Hereby, we propose a systematic approach for exploring and developing a new platform for in vivo deep tissue calcium imaging that actively exploits the scattering properties of the tissue as a resource to localize, demix and extract neuronal activity traces in the millimeter depth range in the highly scattering rodent brain. Our approach utilizes the fact that the scattering process results in an accumulation of different relative phases between scattered light components, leading to the formation of deterministic but complex interference patterns known as speckles. Emission from different locations of the sample results in speckles with different intensity distributions. Thus, these patterns carry complex information about both the location of the emitter and the properties of the scattering medium. Our current results show that the obtainable contrast for these speckle patterns should be sufficient to identify, localize and demix them even for extended objects and with partially coherent light. We will design and build an array of hardware and computational tools that will allow us to systematically explore the limits within which such demixing and neuronal signal extraction can be achieved in vivo. We will use these insights and develop an optical platform for volumetric calcium imaging in the mouse brain in the deep forward-scattering regime, as well as for transcranial calcium imaging in the mouse brain. Our approach will synergistically combine the expertise on the physics of light propagation in disordered media in the Gigan Lab with the experience in machine learning, computational imaging and in vivo high-speed volumetric neuronal recording in the Vaziri Lab. Our method has the potential of opening up a new paradigm for neuronal imagining and signal extraction in the multiple scattering regime by enabling millimeter-range calcium imaging in the highly scattering rodent brain. PROJECT NARRATIVE Optical recording of neuronal activity in the mammalian brain at depth with high spatial resolution remains, due to the highly scattering properties of brain tissue, a key challenge in neuroscience. This is because all microscopy modalities rely on the non-scattered ballistic component of light—which is exponentially attenuated with tissue depth—either in the excitation or the emission for image formation. We will overcome this issue by developing a deep-tissue calcium imaging platform that exploits the scattering properties of the tissue to localize, demix and extract neuronal activity traces in the millimeter range in the highly scattering rodent brain.",In vivo Imaging of Neuroactivity in the Deep Forward Scattering Regime Using Speckle Identification and Demixing (SPID) Microscopy,9828367,RF1NS113251,"['Algorithms', 'Attenuated', 'Ballistics', 'Brain', 'Calcium', 'Characteristics', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Detection', 'Development', 'Disease', 'Image', 'In Vitro', 'Joints', 'Left', 'Length', 'Light', 'Location', 'Machine Learning', 'Methods', 'Microscope', 'Microscopy', 'Modality', 'Motion', 'Mus', 'Nature', 'Neurons', 'Neurosciences', 'Optics', 'Pattern', 'Phase', 'Photons', 'Physics', 'Process', 'Property', 'Resolution', 'Resources', 'Rest', 'Rodent', 'Sampling', 'Scheme', 'Signal Transduction', 'Snakes', 'Speed', 'Techniques', 'Technology', 'Tissue imaging', 'Tissues', 'attenuation', 'base', 'brain tissue', 'computerized tools', 'cranium', 'design', 'experience', 'experimental study', 'image reconstruction', 'imaging platform', 'imaging system', 'in vivo', 'in vivo calcium imaging', 'in vivo imaging', 'insight', 'light scattering', 'method development', 'millimeter', 'neuroimaging', 'neuronal cell body', 'neurotransmission', 'optical imaging', 'practical application', 'temporal measurement', 'two photon microscopy']",NINDS,ROCKEFELLER UNIVERSITY,RF1,2019,1549615,-0.03743900503581708
"Construction of a high-resolution human tractography atlas and its related toolbox PROJECT SUMMARY Mapping the human connectome and exploring its characteristics is one of the largest endeavors in the neuroscience field, but a detailed tractography atlas that provides the 3D trajectories in a standard space has yet to be constructed and validated. A tractography atlas can provide neuroanatomical insight into the structural organization of the human brain and allow for modeling, simulation, and confirmation of cortical connections to facilitate the new development of treatment and intervention for brain diseases. In this study, we propose to construct a high spatial and angular resolution tractography atlas using a large sample of the Human Connectome Project (HCP) diffusion MRI data, averaging them into a template for fiber tracking, validating the tracks by post-mortem Klingler microdissection on 100 cadavers under a neurosurgery microscope digitized using high resolution 3D scanners, and building a deep learning toolbox that allows for automatic track recognition in individuals. This study will construct the most detailed and accurate tractography of human connectome and provide a novel toolbox for future HCP data analysis. PROJECT NARRATIVE The proposed research will construct an atlas of human brain fiber pathways and a related toolbox for track- specific analysis, aiming to understand the structural characteristics of brain connections in healthy individuals and provide track-specific analysis for brain imaging data.",Construction of a high-resolution human tractography atlas and its related toolbox,9771640,R56MH113634,"['Algorithms', 'Aphasia', 'Architecture', 'Atlases', 'Autopsy', 'Base of the Brain', 'Brain', 'Brain Diseases', 'Brain Stem', 'Brain imaging', 'Brain scan', 'Cadaver', 'Cerebellum', 'Characteristics', 'Complement', 'Cranial Nerves', 'Data', 'Data Analyses', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Fiber', 'Future', 'Human', 'Individual', 'Intervention', 'Label', 'Machine Learning', 'Manuals', 'Maps', 'Memory', 'Microdissection', 'Microscope', 'Neuroanatomy', 'Neurosciences', 'Neurosurgeon', 'Parkinson Disease', 'Pathway interactions', 'Patients', 'Perception', 'Performance', 'Population', 'Records', 'Research', 'Resolution', 'Route', 'Sampling', 'Scanning', 'Source', 'Structure', 'Surface', 'System', 'Thinking', 'Tissues', 'Training', 'Validation', 'Variant', 'base', 'brain abnormalities', 'brain research', 'clinical application', 'connectome', 'deep learning', 'deep neural network', 'digital', 'experience', 'human data', 'human subject', 'insight', 'models and simulation', 'neuropsychiatric disorder', 'neurosurgery', 'novel', 'therapy development', 'tool', 'tractography', 'virtual', 'white matter', 'young adult']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R56,2018,452250,0.0014632656327176884
"Next-generation Monte Carlo eXtreme Light Transport Simulation Platform Project Summary/Abstract Abstract: The rapid evolution of the field of biophotonics has produced numerous emerging techniques for combatting diseases and addressing urgent human health challenges, offering safe, non-invasive, and portable light-based diagnostic and therapeutic methods, and attracting exponentially growing attention over the past decade. Rigorous, fast, versatile and publicly available computational tools have played pivotal roles in the success of these novel approaches, leading to breakthroughs in new instrumentation designs and extensive explorations of complex biological systems such as human brains. The Monte Carlo eXtreme (MCX, http://mcx.space) light transport simulation platform developed by our team has become one of the most widely disseminated biophotonics modeling platforms, known for its high accuracy, high speed and versatility, as attested to by its over 27,000 downloads and nearly 1,000 citations from a large (2,400+ registered users) world-wide user community. Over the past years, we have also been pushing the boundaries in cutting-edge Monte Carlo (MC) photon simulation algorithms by exploring modern GPU architectures, advanced anatomical modeling methods and systematic software optimizations. In this proposed project, we will build upon the strong momentum created in the initial funding period, and strive to further advance the state-of-the-art of GPU-accelerated MC light transport modeling with strong support from the world’s leading GPU manufacturers and experts, further expanding our platform to address a number of emerging challenges in biomedical optics applications. Specifically, we will further explore emerging GPU architecture and resources, such as ray- tracing cores, half- and mixed-precision hardware, and portable programming models, to further accelerate the MC modeling speed. We will also develop hybrid shape/mesh-based MC algorithms to dramatically advance the capability in simulating extremely complex yet realistic anatomical structures, such as porous tissues in the lung, dense vessel networks in the brain, and multi-scaled tissue domains. In parallel, we aim to make a break- through in applying deep-learning-based image denoising techniques to equivalently accelerate MC simulations by 2 to 3 orders of magnitudes, as suggested in our preliminary studies. In the continuation of this project, we strive to create a dynamic and community-engaging simulation environment by extending our software to allow users to create, share, browse, and reuse pre-configured simulations, avoiding redundant works in re-creating complex simulations and facilitating reproducible research. In addition, we will expand our well-received user training programs and widely disseminate our open-source tools via major Linux distributions and container images. At the end of this continued funding period, we will provide the community with a significantly accelerated, widely-available and well-supported biophotonics modeling platform that can handle multi-scaled tissue optical modeling ranging from microscopic to macroscopic domains. Project Narrative The Monte Carlo eXtreme (MCX) light transport modeling platform has quadrupled its user community and paper citation numbers during the initial funding period. Building upon this strong momentum, we aim to further explore computational acceleration enabled by emerging GPU architectures and resources, and spearhead novel Monte Carlo (MC) algorithms to address the emerging needs of a broad biophotonics research community. We also dedicate our efforts to the further dissemination, training and usability enhancement of our software, and provide timely support to our large (>2,400 registered users) and active (>300 mailing list subscribers) user community.",Next-generation Monte Carlo eXtreme Light Transport Simulation Platform,10052188,R01GM114365,"['Acceleration', 'Address', 'Adopted', 'Algorithms', 'Anatomic Models', 'Anatomy', 'Architecture', 'Attention', 'Benchmarking', 'Biophotonics', 'Brain', 'Communities', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnostic', 'Disease', 'Documentation', 'Educational workshop', 'Environment', 'Evolution', 'Funding', 'Future Generations', 'Health', 'Human', 'Hybrids', 'Image', 'Industry', 'Letters', 'Libraries', 'Light', 'Linux', 'Lung', 'Manufacturer Name', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modernization', 'Monte Carlo Method', 'Motivation', 'Online Systems', 'Optics', 'Output', 'Paper', 'Performance', 'Photons', 'Play', 'Readability', 'Reproducibility', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Shapes', 'Speed', 'Techniques', 'Therapeutic', 'Time', 'Tissues', 'Tracer', 'Training', 'Training Programs', 'Training Support', 'United States National Institutes of Health', 'Work', 'base', 'combat', 'complex biological systems', 'computerized tools', 'cost', 'data standards', 'deep learning', 'denoising', 'design', 'flexibility', 'graphical user interface', 'improved', 'instrumentation', 'interoperability', 'next generation', 'novel', 'novel strategies', 'open data', 'open source', 'portability', 'rapid growth', 'simulation', 'simulation environment', 'software development', 'success', 'tool', 'usability']",NIGMS,NORTHEASTERN UNIVERSITY,R01,2020,347094,-0.011990618305425903
"In vivo Characterization of Stents using Intravascular OCT Imaging     DESCRIPTION (provided by applicant): Every year, 100s of thousands of patients in the US are treated with intravascular stents. Although the technology has advanced and drug eluting metal stents hinder restenosis, there remains significant room for improvement. Stent parameters include drug choice, bioresorbable versus metal, mechanical design, coatings to stimulate cell coverage, etc. To optimize designs, sensitive, in vivo assessments are needed for preclinical and clinical studies. Intravascular OCT (iOCT) alone provides the resolution and contrast s necessary for in vivo interrogation of vascular healing; stent deployment issues such as malposition; and assessment of stent strut tissue coverage. The Cardiovascular Imaging Core Laboratory at CWRU analyzes iOCT images as a service to numerous clinical and preclinical trials from around the world. An analyst takes many hours to analyze manually a single stent, greatly limiting the size and number of studies. Despite training and quality assurance measures, inter-analyst variability limits the ability to determine changes between stent types. We will develop highly automated software to greatly speed analysis, improve reproducibility, increase accuracy, etc. Careful evaluations/validations will be performed using our database of >1500 manually analyzed stents, and new phantom and pig studies. With the successful completion of this research and development, we will deliver well-validated, highly automated software, which will enable routine use of iOCT for sensitive evaluation of emerging stent technologies, thereby providing greatly improved treatments of cardiovascular disease. In addition, fast, robust software will contribute to clinical usage of iOCT for assessment of stent deployment and healing of a stented vessel.         PUBLIC HEALTH RELEVANCE: We will develop methods for improved in vivo assessment of intravascular stents, the treatment of choice for 100s of thousands of patients, suffering from ischemic heart disease, in the US every year. Our methods will enable optimization of stent technologies for improved treatment of vascular disease.            ",In vivo Characterization of Stents using Intravascular OCT Imaging,8529140,R01HL114406,"['Algorithms', 'Ally', 'Area', 'Arteries', 'Back', 'Biological Markers', 'Blood Vessels', 'Cardiac', 'Cardiovascular Diseases', 'Catheters', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Code', 'Color', 'Computer software', 'Data', 'Databases', 'Dependence', 'Detection', 'Development', 'Devices', 'Evaluation', 'Family suidae', 'Feedback', 'Fibrin', 'Fracture', 'Goals', 'Graph', 'Healed', 'Histocompatibility Testing', 'Hour', 'Hyperplasia', 'Image', 'Image Analysis', 'Imagery', 'Industry', 'International', 'Laboratories', 'Licensing', 'Life', 'Machine Learning', 'Manuals', 'Manufacturer Name', 'Measurement', 'Measures', 'Mechanics', 'Metals', 'Methods', 'Modeling', 'Myocardial Ischemia', 'Needs Assessment', 'Optics', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Polymers', 'Process', 'Property', 'Reporting', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Shadowing (Histology)', 'Shapes', 'Simulate', 'Site', 'Speed', 'Statistical Data Interpretation', 'Stents', 'Techniques', 'Technology', 'Testing', 'Thick', 'Thrombosis', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Universities', 'Validation', 'Vascular Diseases', 'arm', 'base', 'cardiovascular imaging', 'cost', 'design', 'follow-up', 'healing', 'imaging modality', 'implantation', 'improved', 'in vivo', 'innovation', 'meetings', 'novel', 'preclinical evaluation', 'preclinical study', 'public health relevance', 'quality assurance', 'research and development', 'research clinical testing', 'restenosis', 'tool']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,2013,412883,-0.0323130631230227
"In vivo Characterization of Stents using Intravascular OCT Imaging DESCRIPTION (provided by applicant): Every year, 100s of thousands of patients in the US are treated with intravascular stents. Although the technology has advanced and drug eluting metal stents hinder restenosis, there remains significant room for improvement. Stent parameters include drug choice, bioresorbable versus metal, mechanical design, coatings to stimulate cell coverage, etc. To optimize designs, sensitive, in vivo assessments are needed for preclinical and clinical studies. Intravascular OCT (iOCT) alone provides the resolution and contrast s necessary for in vivo interrogation of vascular healing; stent deployment issues such as malposition; and assessment of stent strut tissue coverage. The Cardiovascular Imaging Core Laboratory at CWRU analyzes iOCT images as a service to numerous clinical and preclinical trials from around the world. An analyst takes many hours to analyze manually a single stent, greatly limiting the size and number of studies. Despite training and quality assurance measures, inter-analyst variability limits the ability to determine changes between stent types. We will develop highly automated software to greatly speed analysis, improve reproducibility, increase accuracy, etc. Careful evaluations/validations will be performed using our database of >1500 manually analyzed stents, and new phantom and pig studies. With the successful completion of this research and development, we will deliver well-validated, highly automated software, which will enable routine use of iOCT for sensitive evaluation of emerging stent technologies, thereby providing greatly improved treatments of cardiovascular disease. In addition, fast, robust software will contribute to clinical usage of iOCT for assessment of stent deployment and healing of a stented vessel. PUBLIC HEALTH RELEVANCE: We will develop methods for improved in vivo assessment of intravascular stents, the treatment of choice for 100s of thousands of patients, suffering from ischemic heart disease, in the US every year. Our methods will enable optimization of stent technologies for improved treatment of vascular disease.",In vivo Characterization of Stents using Intravascular OCT Imaging,8885879,R01HL114406,"['Algorithms', 'Ally', 'Area', 'Arteries', 'Back', 'Biological Markers', 'Blood Vessels', 'Cardiac', 'Cardiovascular Diseases', 'Catheters', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Code', 'Color', 'Computer software', 'Data', 'Databases', 'Dependence', 'Detection', 'Development', 'Devices', 'Evaluation', 'Family suidae', 'Feedback', 'Fibrin', 'Fracture', 'Geometry', 'Goals', 'Graph', 'Healed', 'Health', 'Histocompatibility Testing', 'Hour', 'Hyperplasia', 'Image', 'Image Analysis', 'Imagery', 'Industry', 'International', 'Laboratories', 'Licensing', 'Life', 'Machine Learning', 'Manuals', 'Manufacturer Name', 'Measurement', 'Measures', 'Mechanics', 'Metals', 'Methods', 'Modeling', 'Myocardial Ischemia', 'Needs Assessment', 'Optics', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Polymers', 'Process', 'Property', 'Reporting', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Shadowing (Histology)', 'Shapes', 'Site', 'Speed', 'Statistical Data Interpretation', 'Stents', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Universities', 'Validation', 'Vascular Diseases', 'arm', 'base', 'cardiovascular imaging', 'cost', 'design', 'follow-up', 'healing', 'imaging modality', 'implantation', 'improved', 'in vivo', 'innovation', 'meetings', 'novel', 'preclinical evaluation', 'preclinical study', 'quality assurance', 'research and development', 'research clinical testing', 'restenosis', 'stent thrombosis', 'tool']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,2015,457216,-0.0323130631230227
"In vivo Characterization of Stents using Intravascular OCT Imaging     DESCRIPTION (provided by applicant): Every year, 100s of thousands of patients in the US are treated with intravascular stents. Although the technology has advanced and drug eluting metal stents hinder restenosis, there remains significant room for improvement. Stent parameters include drug choice, bioresorbable versus metal, mechanical design, coatings to stimulate cell coverage, etc. To optimize designs, sensitive, in vivo assessments are needed for preclinical and clinical studies. Intravascular OCT (iOCT) alone provides the resolution and contrast s necessary for in vivo interrogation of vascular healing; stent deployment issues such as malposition; and assessment of stent strut tissue coverage. The Cardiovascular Imaging Core Laboratory at CWRU analyzes iOCT images as a service to numerous clinical and preclinical trials from around the world. An analyst takes many hours to analyze manually a single stent, greatly limiting the size and number of studies. Despite training and quality assurance measures, inter-analyst variability limits the ability to determine changes between stent types. We will develop highly automated software to greatly speed analysis, improve reproducibility, increase accuracy, etc. Careful evaluations/validations will be performed using our database of >1500 manually analyzed stents, and new phantom and pig studies. With the successful completion of this research and development, we will deliver well-validated, highly automated software, which will enable routine use of iOCT for sensitive evaluation of emerging stent technologies, thereby providing greatly improved treatments of cardiovascular disease. In addition, fast, robust software will contribute to clinical usage of iOCT for assessment of stent deployment and healing of a stented vessel.         PUBLIC HEALTH RELEVANCE: We will develop methods for improved in vivo assessment of intravascular stents, the treatment of choice for 100s of thousands of patients, suffering from ischemic heart disease, in the US every year. Our methods will enable optimization of stent technologies for improved treatment of vascular disease.",In vivo Characterization of Stents using Intravascular OCT Imaging,8724992,R01HL114406,"['Algorithms', 'Ally', 'Area', 'Arteries', 'Back', 'Biological Markers', 'Blood Vessels', 'Cardiac', 'Cardiovascular Diseases', 'Catheters', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Code', 'Color', 'Computer software', 'Data', 'Databases', 'Dependence', 'Detection', 'Development', 'Devices', 'Evaluation', 'Family suidae', 'Feedback', 'Fibrin', 'Fracture', 'Geometry', 'Goals', 'Graph', 'Healed', 'Histocompatibility Testing', 'Hour', 'Hyperplasia', 'Image', 'Image Analysis', 'Imagery', 'Industry', 'International', 'Laboratories', 'Licensing', 'Life', 'Machine Learning', 'Manuals', 'Manufacturer Name', 'Measurement', 'Measures', 'Mechanics', 'Metals', 'Methods', 'Modeling', 'Myocardial Ischemia', 'Needs Assessment', 'Optics', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Polymers', 'Process', 'Property', 'Reporting', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Shadowing (Histology)', 'Shapes', 'Simulate', 'Site', 'Speed', 'Statistical Data Interpretation', 'Stents', 'Techniques', 'Technology', 'Testing', 'Thick', 'Thrombosis', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Universities', 'Validation', 'Vascular Diseases', 'arm', 'base', 'cardiovascular imaging', 'cost', 'design', 'follow-up', 'healing', 'imaging modality', 'implantation', 'improved', 'in vivo', 'innovation', 'meetings', 'novel', 'preclinical evaluation', 'preclinical study', 'public health relevance', 'quality assurance', 'research and development', 'research clinical testing', 'restenosis', 'tool']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,2014,402534,-0.0323130631230227
"In vivo Characterization of Stents using Intravascular OCT Imaging DESCRIPTION (provided by applicant): Every year, 100s of thousands of patients in the US are treated with intravascular stents. Although the technology has advanced and drug eluting metal stents hinder restenosis, there remains significant room for improvement. Stent parameters include drug choice, bioresorbable versus metal, mechanical design, coatings to stimulate cell coverage, etc. To optimize designs, sensitive, in vivo assessments are needed for preclinical and clinical studies. Intravascular OCT (iOCT) alone provides the resolution and contrast s necessary for in vivo interrogation of vascular healing; stent deployment issues such as malposition; and assessment of stent strut tissue coverage. The Cardiovascular Imaging Core Laboratory at CWRU analyzes iOCT images as a service to numerous clinical and preclinical trials from around the world. An analyst takes many hours to analyze manually a single stent, greatly limiting the size and number of studies. Despite training and quality assurance measures, inter-analyst variability limits the ability to determine changes between stent types. We will develop highly automated software to greatly speed analysis, improve reproducibility, increase accuracy, etc. Careful evaluations/validations will be performed using our database of >1500 manually analyzed stents, and new phantom and pig studies. With the successful completion of this research and development, we will deliver well-validated, highly automated software, which will enable routine use of iOCT for sensitive evaluation of emerging stent technologies, thereby providing greatly improved treatments of cardiovascular disease. In addition, fast, robust software will contribute to clinical usage of iOCT for assessment of stent deployment and healing of a stented vessel. PUBLIC HEALTH RELEVANCE: We will develop methods for improved in vivo assessment of intravascular stents, the treatment of choice for 100s of thousands of patients, suffering from ischemic heart disease, in the US every year. Our methods will enable optimization of stent technologies for improved treatment of vascular disease.",In vivo Characterization of Stents using Intravascular OCT Imaging,9097737,R01HL114406,"['Algorithms', 'Ally', 'Area', 'Arteries', 'Back', 'Blood Vessels', 'Cardiac', 'Cardiovascular Diseases', 'Catheters', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Management', 'Clinical Research', 'Clinical Trials', 'Code', 'Color', 'Computer software', 'Data', 'Databases', 'Dependence', 'Detection', 'Development', 'Devices', 'Evaluation', 'Family suidae', 'Feedback', 'Fibrin', 'Fracture', 'Geometry', 'Goals', 'Graph', 'Healed', 'Health', 'Histocompatibility Testing', 'Hour', 'Hyperplasia', 'Image', 'Image Analysis', 'Imagery', 'Industry', 'International', 'Laboratories', 'Licensing', 'Life', 'Machine Learning', 'Manuals', 'Manufacturer Name', 'Measurement', 'Measures', 'Mechanics', 'Metals', 'Methods', 'Modeling', 'Myocardial Ischemia', 'Needs Assessment', 'Optics', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Polymers', 'Process', 'Property', 'Reporting', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Shapes', 'Site', 'Speed', 'Statistical Data Interpretation', 'Stents', 'Surrogate Markers', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Universities', 'Validation', 'Vascular Diseases', 'arm', 'base', 'cardiovascular imaging', 'cost', 'design', 'follow-up', 'healing', 'imaging modality', 'implantation', 'improved', 'in vivo', 'innovation', 'meetings', 'novel', 'preclinical evaluation', 'preclinical study', 'preclinical trial', 'quality assurance', 'research and development', 'research clinical testing', 'restenosis', 'stent thrombosis', 'tool', 'treatment choice']",NHLBI,CASE WESTERN RESERVE UNIVERSITY,R01,2016,447440,-0.0323130631230227
"A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive Project Summary The Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative promotes the development and application of technologies to describe the temporal and spatial dynamics of cell types and neural circuits in the brain. The Principal Investigator, senior personnel and staff of this project have diverse expertise required to marshall data across the BRAIN Initiative consortium, including experience in data collection from multiple institutions, large-scale quality control and analysis processing capability, familiarity with NIH policy and public archive deposition strategies. To promote smooth interactions across a large research consortium, we will develop the Neuroscience Multi-Omic Archive (NeMO Archive), a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects. We will utilize a federated model for data storage such that the physical location of data can be distributed between the NeMO local file system, public repositories, and a cloud-based storage system (e.g. Amazon S3). We will leverage this capability and distribute BRAIN Initiative data between our local filesystem and the cloud. The Nemo Archive will be a data resource consistent with the principles advanced by research community members who are launching resources in next generation NIH data ecosystem. These practices include FAIR Principles, documentation of APIs, data-indexing systems, workflow sharing, use of shareable software pipelines and storage on cloud-based systems. The information incorporating into the NeMO archive will, in part, enable understanding of 1) genomic regions associated with brain abnormalities and disease; 2) transcription factor binding sites and other regulatory elements; 3) transcription activity; 4) levels of cytosine modification; and 5) histone modification profiles and chromatin accessibility. It will enable users to answer diverse questions of relevance to brain research, such as identifying diagnostic candidates, predicting prognosis, selecting treatments, and testing hypotheses. It will also provide the basic knowledge to guide the development and execution of predictive and machine learning algorithms in the future.   Project Narrative The Neuroscience Multi-Omic Archive (NeMO Archive) is a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects.",A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive,9989180,R24MH114788,"['Archives', 'Atlases', 'BRAIN initiative', 'Binding Sites', 'Bioconductor', 'Brain', 'Brain Diseases', 'Chromatin', 'Communities', 'Computer software', 'Cytosine', 'Data', 'Data Collection', 'Data Coordinating Center', 'Data Management Resources', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Docking', 'Documentation', 'Elements', 'Ensure', 'Familiarity', 'Future', 'Generations', 'Genetic Transcription', 'Genomic Segment', 'Individual', 'Institution', 'Internet', 'Knowledge', 'Location', 'Metadata', 'Modeling', 'Modification', 'Multiomic Data', 'Neurosciences', 'Patients', 'Personnel Staffing', 'Phenotype', 'Policies', 'Principal Investigator', 'Procedures', 'Process', 'Quality Control', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Running', 'Services', 'Site', 'Standardization', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Work', 'analysis pipeline', 'base', 'brain abnormalities', 'brain research', 'cell type', 'cloud based', 'cloud storage', 'complex R', 'computerized data processing', 'data archive', 'data centers', 'data ecosystem', 'data ingestion', 'data integration', 'data pipeline', 'data resource', 'data standards', 'data submission portal', 'data visualization', 'data warehouse', 'database structure', 'experience', 'experimental study', 'histone modification', 'indexing', 'machine learning algorithm', 'member', 'multiple data types', 'multiple omics', 'neural circuit', 'next generation', 'online resource', 'operation', 'outcome forecast', 'programs', 'public repository', 'query tools', 'repository', 'tool', 'transcription factor', 'web site', 'working group']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2020,1263611,-0.021332896702555778
"A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive Project Summary The Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative promotes the development and application of technologies to describe the temporal and spatial dynamics of cell types and neural circuits in the brain. The Principal Investigator, senior personnel and staff of this project have diverse expertise required to marshall data across the BRAIN Initiative consortium, including experience in data collection from multiple institutions, large-scale quality control and analysis processing capability, familiarity with NIH policy and public archive deposition strategies. To promote smooth interactions across a large research consortium, we will develop the Neuroscience Multi-Omic Archive (NeMO Archive), a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects. We will utilize a federated model for data storage such that the physical location of data can be distributed between the NeMO local file system, public repositories, and a cloud-based storage system (e.g. Amazon S3). We will leverage this capability and distribute BRAIN Initiative data between our local filesystem and the cloud. The Nemo Archive will be a data resource consistent with the principles advanced by research community members who are launching resources in next generation NIH data ecosystem. These practices include FAIR Principles, documentation of APIs, data-indexing systems, workflow sharing, use of shareable software pipelines and storage on cloud-based systems. The information incorporating into the NeMO archive will, in part, enable understanding of 1) genomic regions associated with brain abnormalities and disease; 2) transcription factor binding sites and other regulatory elements; 3) transcription activity; 4) levels of cytosine modification; and 5) histone modification profiles and chromatin accessibility. It will enable users to answer diverse questions of relevance to brain research, such as identifying diagnostic candidates, predicting prognosis, selecting treatments, and testing hypotheses. It will also provide the basic knowledge to guide the development and execution of predictive and machine learning algorithms in the future. Project Narrative The Neuroscience Multi-Omic Archive (NeMO Archive) is a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects.",A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive,9999822,R24MH114788,"['Archives', 'Atlases', 'Binding Sites', 'Bioconductor', 'Brain', 'Brain Diseases', 'Chromatin', 'Communities', 'Computer software', 'Cytosine', 'Data', 'Data Collection', 'Data Coordinating Center', 'Data Management Resources', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Docking', 'Documentation', 'Ecosystem', 'Elements', 'Ensure', 'Familiarity', 'Future', 'Generations', 'Genetic Transcription', 'Genomic Segment', 'Imagery', 'Individual', 'Ingestion', 'Institution', 'Internet', 'Knowledge', 'Location', 'Metadata', 'Modeling', 'Modification', 'Multiomic Data', 'Neurosciences', 'Patients', 'Personnel Staffing', 'Phenotype', 'Policies', 'Principal Investigator', 'Procedures', 'Process', 'Quality Control', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Running', 'Services', 'Site', 'Standardization', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization software', 'Work', 'analysis pipeline', 'base', 'brain abnormalities', 'brain research', 'cell type', 'cloud based', 'cloud storage', 'complex R', 'computerized data processing', 'data archive', 'data integration', 'data management', 'data pipeline', 'data resource', 'data submission', 'data visualization', 'data warehouse', 'database structure', 'experience', 'experimental study', 'histone modification', 'indexing', 'innovative neurotechnologies', 'machine learning algorithm', 'member', 'multiple omics', 'neural circuit', 'next generation', 'online resource', 'operation', 'outcome forecast', 'programs', 'repository', 'tool', 'transcription factor', 'web site', 'working group']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2019,102318,-0.021332896702555778
"A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive Project Summary The Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative promotes the development and application of technologies to describe the temporal and spatial dynamics of cell types and neural circuits in the brain. The Principal Investigator, senior personnel and staff of this project have diverse expertise required to marshall data across the BRAIN Initiative consortium, including experience in data collection from multiple institutions, large-scale quality control and analysis processing capability, familiarity with NIH policy and public archive deposition strategies. To promote smooth interactions across a large research consortium, we will develop the Neuroscience Multi-Omic Archive (NeMO Archive), a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects. We will utilize a federated model for data storage such that the physical location of data can be distributed between the NeMO local file system, public repositories, and a cloud-based storage system (e.g. Amazon S3). We will leverage this capability and distribute BRAIN Initiative data between our local filesystem and the cloud. The Nemo Archive will be a data resource consistent with the principles advanced by research community members who are launching resources in next generation NIH data ecosystem. These practices include FAIR Principles, documentation of APIs, data-indexing systems, workflow sharing, use of shareable software pipelines and storage on cloud-based systems. The information incorporating into the NeMO archive will, in part, enable understanding of 1) genomic regions associated with brain abnormalities and disease; 2) transcription factor binding sites and other regulatory elements; 3) transcription activity; 4) levels of cytosine modification; and 5) histone modification profiles and chromatin accessibility. It will enable users to answer diverse questions of relevance to brain research, such as identifying diagnostic candidates, predicting prognosis, selecting treatments, and testing hypotheses. It will also provide the basic knowledge to guide the development and execution of predictive and machine learning algorithms in the future.   Project Narrative The Neuroscience Multi-Omic Archive (NeMO Archive) is a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects.",A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive,9748608,R24MH114788,"['Archives', 'Atlases', 'Binding Sites', 'Bioconductor', 'Brain', 'Brain Diseases', 'Chromatin', 'Communities', 'Computer software', 'Cytosine', 'Data', 'Data Collection', 'Data Coordinating Center', 'Data Management Resources', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Docking', 'Documentation', 'Ecosystem', 'Elements', 'Ensure', 'Familiarity', 'Future', 'Generations', 'Genetic Transcription', 'Genomic Segment', 'Imagery', 'Individual', 'Ingestion', 'Institution', 'Internet', 'Knowledge', 'Location', 'Metadata', 'Modeling', 'Modification', 'Multiomic Data', 'Neurosciences', 'Patients', 'Personnel Staffing', 'Phenotype', 'Policies', 'Principal Investigator', 'Procedures', 'Process', 'Quality Control', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Running', 'Services', 'Site', 'Standardization', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization software', 'Work', 'analysis pipeline', 'base', 'brain abnormalities', 'brain research', 'cell type', 'cloud based', 'cloud storage', 'complex R', 'computerized data processing', 'data archive', 'data integration', 'data management', 'data pipeline', 'data resource', 'data submission', 'data visualization', 'data warehouse', 'database structure', 'experience', 'experimental study', 'histone modification', 'indexing', 'innovative neurotechnologies', 'machine learning algorithm', 'member', 'multiple omics', 'neural circuit', 'next generation', 'online resource', 'operation', 'outcome forecast', 'programs', 'repository', 'tool', 'transcription factor', 'web site', 'working group']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2019,1263611,-0.021332896702555778
"A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive Project Summary The Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative promotes the development and application of technologies to describe the temporal and spatial dynamics of cell types and neural circuits in the brain. The Principal Investigator, senior personnel and staff of this project have diverse expertise required to marshall data across the BRAIN Initiative consortium, including experience in data collection from multiple institutions, large-scale quality control and analysis processing capability, familiarity with NIH policy and public archive deposition strategies. To promote smooth interactions across a large research consortium, we will develop the Neuroscience Multi-Omic Archive (NeMO Archive), a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects. We will utilize a federated model for data storage such that the physical location of data can be distributed between the NeMO local file system, public repositories, and a cloud-based storage system (e.g. Amazon S3). We will leverage this capability and distribute BRAIN Initiative data between our local filesystem and the cloud. The Nemo Archive will be a data resource consistent with the principles advanced by research community members who are launching resources in next generation NIH data ecosystem. These practices include FAIR Principles, documentation of APIs, data-indexing systems, workflow sharing, use of shareable software pipelines and storage on cloud-based systems. The information incorporating into the NeMO archive will, in part, enable understanding of 1) genomic regions associated with brain abnormalities and disease; 2) transcription factor binding sites and other regulatory elements; 3) transcription activity; 4) levels of cytosine modification; and 5) histone modification profiles and chromatin accessibility. It will enable users to answer diverse questions of relevance to brain research, such as identifying diagnostic candidates, predicting prognosis, selecting treatments, and testing hypotheses. It will also provide the basic knowledge to guide the development and execution of predictive and machine learning algorithms in the future.   Project Narrative The Neuroscience Multi-Omic Archive (NeMO Archive) is a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects.",A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive,9565669,R24MH114788,"['Algorithms', 'Archives', 'Atlases', 'Binding Sites', 'Bioconductor', 'Brain', 'Brain Diseases', 'Chromatin', 'Communities', 'Computer software', 'Cytosine', 'Data', 'Data Collection', 'Data Coordinating Center', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Docking', 'Documentation', 'Ecosystem', 'Elements', 'Ensure', 'Familiarity', 'Future', 'Generations', 'Genetic Transcription', 'Genomic Segment', 'Imagery', 'Individual', 'Ingestion', 'Institution', 'Internet', 'Knowledge', 'Location', 'Machine Learning', 'Metadata', 'Modeling', 'Modification', 'Neurosciences', 'Patients', 'Personnel Staffing', 'Phenotype', 'Policies', 'Principal Investigator', 'Procedures', 'Process', 'Quality Control', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Running', 'Services', 'Site', 'Standardization', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'brain abnormalities', 'brain research', 'cell type', 'cloud based', 'cloud storage', 'complex R', 'computerized data processing', 'data archive', 'data integration', 'data management', 'data resource', 'data submission', 'data visualization', 'data warehouse', 'database structure', 'experience', 'experimental study', 'histone modification', 'indexing', 'innovative neurotechnologies', 'member', 'multiple omics', 'neural circuit', 'next generation', 'online resource', 'operation', 'outcome forecast', 'programs', 'repository', 'tool', 'transcription factor', 'web site', 'working group']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2018,1293101,-0.021332896702555778
"A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive Project Summary The Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative promotes the development and application of technologies to describe the temporal and spatial dynamics of cell types and neural circuits in the brain. The Principal Investigator, senior personnel and staff of this project have diverse expertise required to marshall data across the BRAIN Initiative consortium, including experience in data collection from multiple institutions, large-scale quality control and analysis processing capability, familiarity with NIH policy and public archive deposition strategies. To promote smooth interactions across a large research consortium, we will develop the Neuroscience Multi-Omic Archive (NeMO Archive), a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects. We will utilize a federated model for data storage such that the physical location of data can be distributed between the NeMO local file system, public repositories, and a cloud-based storage system (e.g. Amazon S3). We will leverage this capability and distribute BRAIN Initiative data between our local filesystem and the cloud. The Nemo Archive will be a data resource consistent with the principles advanced by research community members who are launching resources in next generation NIH data ecosystem. These practices include FAIR Principles, documentation of APIs, data-indexing systems, workflow sharing, use of shareable software pipelines and storage on cloud-based systems. The information incorporating into the NeMO archive will, in part, enable understanding of 1) genomic regions associated with brain abnormalities and disease; 2) transcription factor binding sites and other regulatory elements; 3) transcription activity; 4) levels of cytosine modification; and 5) histone modification profiles and chromatin accessibility. It will enable users to answer diverse questions of relevance to brain research, such as identifying diagnostic candidates, predicting prognosis, selecting treatments, and testing hypotheses. It will also provide the basic knowledge to guide the development and execution of predictive and machine learning algorithms in the future.   Project Narrative The Neuroscience Multi-Omic Archive (NeMO Archive) is a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects.",A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive,9413774,R24MH114788,"['Algorithms', 'Archives', 'Atlases', 'Binding Sites', 'Bioconductor', 'Brain', 'Brain Diseases', 'Chromatin', 'Communities', 'Computer software', 'Cytosine', 'Data', 'Data Collection', 'Data Coordinating Center', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Docking', 'Documentation', 'Ecosystem', 'Elements', 'Ensure', 'Familiarity', 'Future', 'Generations', 'Genetic Transcription', 'Genomic Segment', 'Imagery', 'Individual', 'Ingestion', 'Institution', 'Internet', 'Knowledge', 'Location', 'Machine Learning', 'Metadata', 'Modification', 'Neurosciences', 'Patients', 'Personnel Staffing', 'Phenotype', 'Policies', 'Principal Investigator', 'Procedures', 'Process', 'Quality Control', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Running', 'Services', 'Site', 'Standardization', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'brain abnormalities', 'brain research', 'cell type', 'cloud based', 'cloud storage', 'complex R', 'computerized data processing', 'data archive', 'data integration', 'data management', 'data modeling', 'data resource', 'data visualization', 'database structure', 'experience', 'experimental study', 'histone modification', 'indexing', 'innovative neurotechnologies', 'member', 'neural circuit', 'next generation', 'online resource', 'operation', 'outcome forecast', 'programs', 'repository', 'tool', 'transcription factor', 'web site', 'working group']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2017,1247018,-0.021332896702555778
"SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy Project Abstract Advances in imaging have had a profound effect on our ability to generate high-resolution measurements of the brain’s structure. One of the major hurdles in processing modern neuroimaging datasets designed to produce large-scale maps of the connections and the organization of the brain lies in the sheer size of these data. For instance, electron microscopic (EM) images of a cubic millimeter of cortex occupies roughly 3 PBon disk, and lower resolution emerging X-ray microtomography (XRM) data can exceed 10 TB for a single mouse brain. When dealing with datasets of this size, the application of even simple algorithms becomes difficult. The size of datasets also exacerbates the considerable challenges for dissemination, reproducibility, and collaboration across laboratories. Addressing these challenges requires a new approach that leverages state-of-the-art computer science technology while remaining conscientious of the underlying bioinformatics. We propose Scalable Analytics for Brain Exploration Research (SABER), a user-friendly and portable framework that automates the retrieval, extraction, and analysis of large-scale imagery data to facilitate neuroscientific analyses. SABER aims to improve the reliability and reproducibility of neuroimagery research by providing a common substrate upon which algorithms may be developed. Leveraging SABER’s containers — a standardized packaging for software — this substrate can then be trivially transferred to other machines by the same researcher or by other teams aiming to reproduce or adapt the prior work, making sharing workflows and extracting knowledge commonplace. Using SABER will ensure that the analysis runs identically, regardless of by whom or where the workflow is executed. Because developing and deploying these analysis solutions for large image volumes are acute barriers to developing consistently reproducible workflows, SABER will further the neuroscientific analysis community by simplifying the workflow-development and workflow-execution steps. To demonstrate this, we plan to distribute two community-vetted, optimized workflows to convert large-scale EM and XRM volumetric imagery into maps of neuronal connectivity. Many neurological diseases are characterized by their impact on the density of cells and vessels, neuron death, connectivity, or other factors that are visible with imaging technologies. SABER will provide a framework for producing reproducible estimates of cell counts, vasculature density, and connectomes, thus enabling increased understanding of the impact of disease on the neuroanatomy of many brains. This work will enable the development of tools that can both be applied to massive data and shared amongst many scientists, which will in turn accelerate progress and neuroscientific discovery. Project Narrative: Our Johns Hopkins University Applied Physics Laboratory team leverages prior neuroscience analysis experience to present SABER: Scalable Analytics for Brain Exploration Research — a portable, easy-to-install framework that enables large-scale neuroanatomical data processing by providing a scaffold upon which highly-reproducible bioinformatics protocols may be built. To support emerging efforts to understand the biological basis of disease, we demonstrate turn-key pipelines to translate multi-terabyte electron microscopy and X-ray microtomography data volumes into maps of neuronal connectivity.",SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy,9733348,R24MH114799,"['Acute', 'Address', 'Algorithms', 'Artificial Arm', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Brain', 'Cell Count', 'Cell Density', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Discovery', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Ensure', 'Environment', 'Evaluation', 'Grant', 'Human Resources', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Infrastructure', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Microscopic', 'Modality', 'Modernization', 'Mus', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Neurons', 'Neurosciences', 'Optics', 'Physics', 'Pluto', 'Process', 'Protocols documentation', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Retrieval', 'Roentgen Rays', 'Running', 'Science', 'Scientist', 'Source', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translating', 'Traumatic Brain Injury', 'Universities', 'Work', 'brain research', 'brain tissue', 'computer science', 'computerized data processing', 'connectome', 'data access', 'data archive', 'data management', 'data sharing', 'density', 'design', 'experience', 'experimental study', 'high resolution imaging', 'improved', 'innovative neurotechnologies', 'microscopic imaging', 'millimeter', 'nervous system disorder', 'neuroimaging', 'neuron loss', 'novel', 'novel strategies', 'portability', 'relating to nervous system', 'scaffold', 'software development', 'terabyte', 'tool', 'tool development', 'user-friendly', 'virtual technology']",NIMH,JOHNS HOPKINS UNIVERSITY,R24,2019,379357,-0.035852988445325064
"SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy Project Abstract Advances in imaging have had a profound effect on our ability to generate high-resolution measurements of the brain’s structure. One of the major hurdles in processing modern neuroimaging datasets designed to produce large-scale maps of the connections and the organization of the brain lies in the sheer size of these data. For instance, electron microscopic (EM) images of a cubic millimeter of cortex occupies roughly 3 PBon disk, and lower resolution emerging X-ray microtomography (XRM) data can exceed 10 TB for a single mouse brain. When dealing with datasets of this size, the application of even simple algorithms becomes difficult. The size of datasets also exacerbates the considerable challenges for dissemination, reproducibility, and collaboration across laboratories. Addressing these challenges requires a new approach that leverages state-of-the-art computer science technology while remaining conscientious of the underlying bioinformatics. We propose Scalable Analytics for Brain Exploration Research (SABER), a user-friendly and portable framework that automates the retrieval, extraction, and analysis of large-scale imagery data to facilitate neuroscientific analyses. SABER aims to improve the reliability and reproducibility of neuroimagery research by providing a common substrate upon which algorithms may be developed. Leveraging SABER’s containers — a standardized packaging for software — this substrate can then be trivially transferred to other machines by the same researcher or by other teams aiming to reproduce or adapt the prior work, making sharing workflows and extracting knowledge commonplace. Using SABER will ensure that the analysis runs identically, regardless of by whom or where the workflow is executed. Because developing and deploying these analysis solutions for large image volumes are acute barriers to developing consistently reproducible workflows, SABER will further the neuroscientific analysis community by simplifying the workflow-development and workflow-execution steps. To demonstrate this, we plan to distribute two community-vetted, optimized workflows to convert large-scale EM and XRM volumetric imagery into maps of neuronal connectivity. Many neurological diseases are characterized by their impact on the density of cells and vessels, neuron death, connectivity, or other factors that are visible with imaging technologies. SABER will provide a framework for producing reproducible estimates of cell counts, vasculature density, and connectomes, thus enabling increased understanding of the impact of disease on the neuroanatomy of many brains. This work will enable the development of tools that can both be applied to massive data and shared amongst many scientists, which will in turn accelerate progress and neuroscientific discovery. Project Narrative: Our Johns Hopkins University Applied Physics Laboratory team leverages prior neuroscience analysis experience to present SABER: Scalable Analytics for Brain Exploration Research — a portable, easy-to-install framework that enables large-scale neuroanatomical data processing by providing a scaffold upon which highly-reproducible bioinformatics protocols may be built. To support emerging efforts to understand the biological basis of disease, we demonstrate turn-key pipelines to translate multi-terabyte electron microscopy and X-ray microtomography data volumes into maps of neuronal connectivity.",SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy,9568023,R24MH114799,"['Acute', 'Address', 'Algorithms', 'Artificial Arm', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Brain', 'Cell Count', 'Cell Density', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Discovery', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Ensure', 'Environment', 'Evaluation', 'Grant', 'Human Resources', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Microscopic', 'Modality', 'Modernization', 'Mus', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Neurons', 'Neurosciences', 'Optics', 'Physics', 'Pluto', 'Process', 'Protocols documentation', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Retrieval', 'Roentgen Rays', 'Running', 'Science', 'Scientist', 'Source', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translating', 'Traumatic Brain Injury', 'Universities', 'Work', 'brain research', 'brain tissue', 'computer science', 'computerized data processing', 'connectome', 'data access', 'data archive', 'data management', 'density', 'design', 'experience', 'experimental study', 'high resolution imaging', 'improved', 'innovative neurotechnologies', 'microscopic imaging', 'millimeter', 'nervous system disorder', 'neuroimaging', 'neuron loss', 'novel', 'novel strategies', 'portability', 'relating to nervous system', 'scaffold', 'software development', 'terabyte', 'tool', 'tool development', 'user-friendly', 'virtual']",NIMH,JOHNS HOPKINS UNIVERSITY,R24,2018,386960,-0.035852988445325064
"SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy Project Abstract Advances in imaging have had a profound effect on our ability to generate high-resolution measurements of the brain’s structure. One of the major hurdles in processing modern neuroimaging datasets designed to produce large-scale maps of the connections and the organization of the brain lies in the sheer size of these data. For instance, electron microscopic (EM) images of a cubic millimeter of cortex occupies roughly 3 PBon disk, and lower resolution emerging X-ray microtomography (XRM) data can exceed 10 TB for a single mouse brain. When dealing with datasets of this size, the application of even simple algorithms becomes difficult. The size of datasets also exacerbates the considerable challenges for dissemination, reproducibility, and collaboration across laboratories. Addressing these challenges requires a new approach that leverages state-of-the-art computer science technology while remaining conscientious of the underlying bioinformatics. We propose Scalable Analytics for Brain Exploration Research (SABER), a user-friendly and portable framework that automates the retrieval, extraction, and analysis of large-scale imagery data to facilitate neuroscientific analyses. SABER aims to improve the reliability and reproducibility of neuroimagery research by providing a common substrate upon which algorithms may be developed. Leveraging SABER’s containers — a standardized packaging for software — this substrate can then be trivially transferred to other machines by the same researcher or by other teams aiming to reproduce or adapt the prior work, making sharing workflows and extracting knowledge commonplace. Using SABER will ensure that the analysis runs identically, regardless of by whom or where the workflow is executed. Because developing and deploying these analysis solutions for large image volumes are acute barriers to developing consistently reproducible workflows, SABER will further the neuroscientific analysis community by simplifying the workflow-development and workflow-execution steps. To demonstrate this, we plan to distribute two community-vetted, optimized workflows to convert large-scale EM and XRM volumetric imagery into maps of neuronal connectivity. Many neurological diseases are characterized by their impact on the density of cells and vessels, neuron death, connectivity, or other factors that are visible with imaging technologies. SABER will provide a framework for producing reproducible estimates of cell counts, vasculature density, and connectomes, thus enabling increased understanding of the impact of disease on the neuroanatomy of many brains. This work will enable the development of tools that can both be applied to massive data and shared amongst many scientists, which will in turn accelerate progress and neuroscientific discovery. Project Narrative: Our Johns Hopkins University Applied Physics Laboratory team leverages prior neuroscience analysis experience to present SABER: Scalable Analytics for Brain Exploration Research — a portable, easy-to-install framework that enables large-scale neuroanatomical data processing by providing a scaffold upon which highly-reproducible bioinformatics protocols may be built. To support emerging efforts to understand the biological basis of disease, we demonstrate turn-key pipelines to translate multi-terabyte electron microscopy and X-ray microtomography data volumes into maps of neuronal connectivity.",SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy,9414126,R24MH114799,"['Acute', 'Address', 'Algorithms', 'Artificial Arm', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Brain', 'Cell Count', 'Cell Density', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Discovery', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Ensure', 'Environment', 'Evaluation', 'Grant', 'Human Resources', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Microscopic', 'Modality', 'Modernization', 'Mus', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Neurons', 'Neurosciences', 'Optics', 'Physics', 'Pluto', 'Process', 'Protocols documentation', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Retrieval', 'Roentgen Rays', 'Running', 'Science', 'Scientist', 'Source', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Tissue imaging', 'Tissues', 'Training', 'Translating', 'Traumatic Brain Injury', 'Universities', 'Work', 'brain research', 'brain tissue', 'computer science', 'computerized data processing', 'connectome', 'data access', 'data archive', 'data management', 'density', 'design', 'experience', 'experimental study', 'high resolution imaging', 'improved', 'innovative neurotechnologies', 'microscopic imaging', 'millimeter', 'nervous system disorder', 'neuroimaging', 'neuron loss', 'novel', 'novel strategies', 'portability', 'relating to nervous system', 'scaffold', 'software development', 'terabyte', 'tool', 'tool development', 'user-friendly', 'virtual']",NIMH,JOHNS HOPKINS UNIVERSITY,R24,2017,395542,-0.035852988445325064
"Risk Assessment of Cerebral Aneurysm Growth with 4D flow MRI PROJECT SUMMARY The goal of this project is to determine the contribution of hemodynamic factors to risk assessment of unruptured intracranial aneurysms (UIAs) and calculate these factors from enhanced in vivo 4D flow MRI data. Even though most UIAs are stable, the majority of UIA patients are offered interventional treatment due to the grave risk presented if an aneurysm ruptures. Previous studies indicated that in addition to clinical (e.g., age, sex, comorbidities) and morphological (e.g., location and size) factors, UIA progression is affected by local blood flow dynamics. Hemodynamic factors associated with UIA growth can be obtained from computational and experimental models or from 4D flow MRI measurements; however, each approach has limitations. The previous NIH-funded project focused on developing image-based computational methods for predicting postoperative flow following interventions. The goal of this renewal is to use the developed framework to improve risk stratification of UIAs using image-based flow analysis. The proposed project will develop multi-parametric predictive models that combine clinical and morphological factors with hemodynamic factors calculated from augmented 4D flow MRI data. The UIA growth predicted by different models will be compared to outcomes observed in longitudinal imaging studies. The aims of the proposed project are, therefore, to: (1) determine the probability of UIA growth by utilizing morphological and clinical factors together with hemodynamic factors obtained from computational and experimental flow models by a) performing statistical analysis based on morphological and clinical factors obtained from longitudinal imaging, and b) extending statistical model by including hemodynamic factors computed from patient-specific models; (2) Enhance 4D flow MRI data by a) determining 4D flow reproducibility and variability with in vitro studies, and b) applying advanced data augmentation methods to improve the accuracy of calculated hemodynamic factors affecting aneurysm growth; (3) determine the probability of UIA growth based on multi-parametric analysis utilizing hemodynamic factors calculated from enhanced 4D flow MRI. Successful completion of the project will resolve the controversy regarding how hemodynamic factors affect aneurysm growth and establish 4D flow MRI as a diagnostic tool for UIA risk stratification. This collaborative project engages the cardiovascular engineering group at Purdue University and neurosurgeons, neuroradiologists and MRI physicists at Northwestern University, University of California San Francisco and Barrow Neurological Institute. This cross-disciplinary team will bring together experts in neurovascular surgeries, MRI velocimetry, patient-specific flow computations, experimental fluid mechanics and statistical analysis. Retrospective and prospective UIAs data obtained from these superb clinical centers will be used in this study. The outstanding engineering resources available at Purdue and world-class imaging resources at Northwestern, UC San Francisco and Barrow, as well as existing the data sharing agreements between these institutions and ongoing collaborations between the PIs, will ensure the project's success. PROJECT NARRATIVE The majority of brain aneurysms are treated, despite the fact that most of them have very low risk or rupture. Studies indicated that brain aneurysm risk factors include a range of clinical (e.g., patient’s age, sex, family history) and anatomical (e.g., aneurysm location, size and shape) parameters, as well as local blood flow dynamics. The proposed studies will determine the specific contribution of blood flow variables for improving the risk assessment of brain aneurysms and examine whether these variables can be reliably calculated from flow velocities measured with phase-contrast magnetic resonance imaging.",Risk Assessment of Cerebral Aneurysm Growth with 4D flow MRI,10052679,R01HL115267,"['4D MRI', 'Affect', 'Age', 'Agreement', 'Anatomy', 'Aneurysm', 'Angiography', 'Artificial Intelligence', 'Blood flow', 'Brain Aneurysms', 'California', 'Cardiovascular system', 'Cerebral Aneurysm', 'Cerebrovascular Circulation', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Computer Models', 'Computing Methodologies', 'Data', 'Databases', 'Deposition', 'Diagnostic', 'Engineering', 'Ensure', 'Experimental Models', 'Family', 'Funding', 'Future', 'Goals', 'Growth', 'Guidelines', 'Hypertension', 'Image', 'In Vitro', 'Institutes', 'Institution', 'Intervention', 'Intracranial Aneurysm', 'Liquid substance', 'Location', 'Magnetic Resonance Imaging', 'Measurement', 'Measures', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Morphology', 'Multiparametric Analysis', 'Neurologic', 'Neurosurgeon', 'Noise', 'Operative Surgical Procedures', 'Outcome', 'Patients', 'Phase', 'Physics', 'Postoperative Period', 'Probability', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk', 'Risk Assessment', 'Risk Factors', 'Risk stratification', 'Role', 'Rupture', 'Ruptured Aneurysm', 'San Francisco', 'Shapes', 'Site', 'Statistical Data Interpretation', 'Statistical Models', 'Techniques', 'Thrombus', 'Time', 'United States National Institutes of Health', 'Universities', 'Velocimetries', 'Work', 'base', 'clinical center', 'clinical risk', 'comorbidity', 'data sharing', 'enhancing factor', 'follow-up', 'hemodynamics', 'imaging study', 'improved', 'in vivo', 'indexing', 'mortality', 'neurovascular', 'novel strategies', 'particle', 'predictive modeling', 'prospective', 'residence', 'serial imaging', 'sex', 'shear stress', 'success', 'tool']",NHLBI,PURDUE UNIVERSITY,R01,2020,707635,0.02099507716973401
"Spatial Genomics for in situ single cell expression analysis in the brain Summary Identifying the spatial organization of tissues at cellular resolution from single cell gene expression profiles is essential to understanding neurological systems. We have developed a spatial genomics approach that allows in situ 3D multiplexed imaging of many genes in single cells called sequential Fluorescence in situ hybridization (seqFISH). This technology can profile transcriptional states of single cells directly in their native tissue context with up to 249 genes multiplexed with single molecule sensitivity on each gene. We have demonstrated over 15,000 cells profiled in mouse brain slices. This SBIR project will be focused on the design, production and optimization of an instrument that allows hundreds of genes to be multiplexed and imaged in single cells within their native tissue context. The resulting machine will be commercially launched and targeted to imaging or sequencing cores at research institutions. We will design the hardware, code the control software, and build the prototype instrument. We will engineer the hardware component including automated fluidics and multiple camera imaging system with a parallel effort to develop software controls as well as integrated analysis tools. In phase II, we will beta-test the instrument, generate probe sets for gene panels targeting different brain samples, and receive valuable feedback from users and optimize our instrument design. Narrative A major challenge of the BRAIN initiative and international Human Cell Atlas project is to identifying distinct cell populations in the brain within their native spatial environment. Addressing this challenge is essential not only to fundamental biological questions of understanding how different cell types interact to form neural circuits, but also essential in investigating mechanisms of human diseases where small subpopulations of cells, such as microglial, play pivotal roles. We have developed an in situ 3D multiplexed imaging method called sequential Fluorescence in situ hybridization (seqFISH), that can profile transcriptional states of single cells directly in a mouse coronal section with up to 249 genes multiplexed in the hippocampus and the cortex (Shah et al., Neuron 2016, Frieda et al., Nature 2016). Delivering this technology as a robust platform that can be used by neuroscientists would enable breakthrough discoveries and treatment options. To make this technology available for a broad range of users and customers, this phase I SBIR project will be focused on the design, production and optimization of an instrument called seqFISH100 and the parallel development of the control software to operate the seqFISH100.",Spatial Genomics for in situ single cell expression analysis in the brain,9730606,R43MH115538,"['3-Dimensional', 'Address', 'Adoption', 'Algorithms', 'Atlases', 'Automation', 'BRAIN initiative', 'Basic Science', 'Biological', 'Brain', 'Cells', 'Clinical Pathways', 'Clinical Research', 'Code', 'Computer software', 'Contracts', 'Custom', 'DNA Sequencing Facility', 'Development', 'Drops', 'Educational workshop', 'Effector Cell', 'Engineering', 'Environment', 'Expression Profiling', 'Feedback', 'Fluorescent in Situ Hybridization', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Genomic approach', 'Genomics', 'Hippocampus (Brain)', 'Human', 'Hypothalamic structure', 'Image', 'In Situ', 'Individual', 'Institutes', 'Institution', 'International', 'Intervention', 'Liquid substance', 'Machine Learning', 'Manufacturer Name', 'Measurement', 'Messenger RNA', 'Methods', 'Microscope', 'Molecular', 'Mus', 'Nature', 'Neuroglia', 'Neurologic', 'Neurons', 'Neurosciences', 'Pattern', 'Phase', 'Play', 'Population', 'Process', 'Production', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Services', 'Signal Transduction', 'Site', 'Slice', 'Small Business Innovation Research Grant', 'Speed', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Visualization software', 'Work', 'base', 'cell type', 'data visualization', 'design', 'experimental study', 'fluorescence imaging', 'genome wide association study', 'human disease', 'imaging modality', 'imaging system', 'instrument', 'interest', 'multiplexed\xa0imaging', 'neural circuit', 'operation', 'programs', 'prototype', 'scale up', 'single molecule', 'software development', 'tool', 'touchscreen', 'user-friendly']",NIMH,"SPATIAL GENOMICS, INC.",R43,2019,265936,-0.04871207558253981
"Spatial Genomics for in situ single cell expression analysis in the brain Summary Identifying the spatial organization of tissues at cellular resolution from single cell gene expression profiles is essential to understanding neurological systems. We have developed a spatial genomics approach that allows in situ 3D multiplexed imaging of many genes in single cells called sequential Fluorescence in situ hybridization (seqFISH). This technology can profile transcriptional states of single cells directly in their native tissue context with up to 249 genes multiplexed with single molecule sensitivity on each gene. We have demonstrated over 15,000 cells profiled in mouse brain slices. This SBIR project will be focused on the design, production and optimization of an instrument that allows hundreds of genes to be multiplexed and imaged in single cells within their native tissue context. The resulting machine will be commercially launched and targeted to imaging or sequencing cores at research institutions. We will design the hardware, code the control software, and build the prototype instrument. We will engineer the hardware component including automated fluidics and multiple camera imaging system with a parallel effort to develop software controls as well as integrated analysis tools. In phase II, we will beta-test the instrument, generate probe sets for gene panels targeting different brain samples, and receive valuable feedback from users and optimize our instrument design. Narrative A major challenge of the BRAIN initiative and international Human Cell Atlas project is to identifying distinct cell populations in the brain within their native spatial environment. Addressing this challenge is essential not only to fundamental biological questions of understanding how different cell types interact to form neural circuits, but also essential in investigating mechanisms of human diseases where small subpopulations of cells, such as microglial, play pivotal roles. We have developed an in situ 3D multiplexed imaging method called sequential Fluorescence in situ hybridization (seqFISH), that can profile transcriptional states of single cells directly in a mouse coronal section with up to 249 genes multiplexed in the hippocampus and the cortex (Shah et al., Neuron 2016, Frieda et al., Nature 2016). Delivering this technology as a robust platform that can be used by neuroscientists would enable breakthrough discoveries and treatment options. To make this technology available for a broad range of users and customers, this phase I SBIR project will be focused on the design, production and optimization of an instrument called seqFISH100 and the parallel development of the control software to operate the seqFISH100.",Spatial Genomics for in situ single cell expression analysis in the brain,9748985,R43MH115538,"['Address', 'Adoption', 'Algorithms', 'Atlases', 'Automation', 'BRAIN initiative', 'Basic Science', 'Biological', 'Brain', 'Cells', 'Clinical Pathways', 'Clinical Research', 'Code', 'Computer software', 'Contracts', 'Custom', 'DNA Sequencing Facility', 'Development', 'Drops', 'Educational workshop', 'Effector Cell', 'Engineering', 'Environment', 'Expression Profiling', 'Feedback', 'Fluorescent in Situ Hybridization', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Genomic approach', 'Genomics', 'Hippocampus (Brain)', 'Human', 'Hypothalamic structure', 'Image', 'In Situ', 'Individual', 'Institutes', 'Institution', 'International', 'Intervention', 'Liquid substance', 'Machine Learning', 'Manufacturer Name', 'Measurement', 'Messenger RNA', 'Methods', 'Microscope', 'Molecular', 'Mus', 'Nature', 'Neuroglia', 'Neurologic', 'Neurons', 'Neurosciences', 'Pattern', 'Phase', 'Play', 'Population', 'Process', 'Production', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Services', 'Signal Transduction', 'Site', 'Slice', 'Small Business Innovation Research Grant', 'Speed', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Visualization software', 'Work', 'base', 'cell type', 'data visualization', 'design', 'experimental study', 'fluorescence imaging', 'genome wide association study', 'human disease', 'imaging modality', 'imaging system', 'instrument', 'interest', 'neural circuit', 'operation', 'programs', 'prototype', 'scale up', 'single molecule', 'software development', 'tool', 'touchscreen', 'user-friendly']",NIMH,"SPATIAL GENOMICS, INC.",R43,2018,30000,-0.04871207558253981
"Spatial Genomics for in situ single cell expression analysis in the brain Summary Identifying the spatial organization of tissues at cellular resolution from single cell gene expression profiles is essential to understanding neurological systems. We have developed a spatial genomics approach that allows in situ 3D multiplexed imaging of many genes in single cells called sequential Fluorescence in situ hybridization (seqFISH). This technology can profile transcriptional states of single cells directly in their native tissue context with up to 249 genes multiplexed with single molecule sensitivity on each gene. We have demonstrated over 15,000 cells profiled in mouse brain slices. This SBIR project will be focused on the design, production and optimization of an instrument that allows hundreds of genes to be multiplexed and imaged in single cells within their native tissue context. The resulting machine will be commercially launched and targeted to imaging or sequencing cores at research institutions. We will design the hardware, code the control software, and build the prototype instrument. We will engineer the hardware component including automated fluidics and multiple camera imaging system with a parallel effort to develop software controls as well as integrated analysis tools. In phase II, we will beta-test the instrument, generate probe sets for gene panels targeting different brain samples, and receive valuable feedback from users and optimize our instrument design. Narrative A major challenge of the BRAIN initiative and international Human Cell Atlas project is to identifying distinct cell populations in the brain within their native spatial environment. Addressing this challenge is essential not only to fundamental biological questions of understanding how different cell types interact to form neural circuits, but also essential in investigating mechanisms of human diseases where small subpopulations of cells, such as microglial, play pivotal roles. We have developed an in situ 3D multiplexed imaging method called sequential Fluorescence in situ hybridization (seqFISH), that can profile transcriptional states of single cells directly in a mouse coronal section with up to 249 genes multiplexed in the hippocampus and the cortex (Shah et al., Neuron 2016, Frieda et al., Nature 2016). Delivering this technology as a robust platform that can be used by neuroscientists would enable breakthrough discoveries and treatment options. To make this technology available for a broad range of users and customers, this phase I SBIR project will be focused on the design, production and optimization of an instrument called seqFISH100 and the parallel development of the control software to operate the seqFISH100.",Spatial Genomics for in situ single cell expression analysis in the brain,9559516,R43MH115538,"['Address', 'Adoption', 'Algorithms', 'Atlases', 'Automation', 'BRAIN initiative', 'Basic Science', 'Biological', 'Brain', 'Cells', 'Clinical Pathways', 'Clinical Research', 'Code', 'Computer software', 'Contracts', 'Custom', 'DNA Sequencing Facility', 'Development', 'Drops', 'Educational workshop', 'Effector Cell', 'Engineering', 'Environment', 'Expression Profiling', 'Feedback', 'Fluorescent in Situ Hybridization', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Genomic approach', 'Genomics', 'Hippocampus (Brain)', 'Human', 'Hypothalamic structure', 'Image', 'In Situ', 'Individual', 'Institutes', 'Institution', 'International', 'Intervention', 'Liquid substance', 'Machine Learning', 'Manufacturer Name', 'Measurement', 'Messenger RNA', 'Methods', 'Microscope', 'Molecular', 'Mus', 'Nature', 'Neuroglia', 'Neurologic', 'Neurons', 'Neurosciences', 'Pattern', 'Phase', 'Play', 'Population', 'Process', 'Production', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Sampling', 'Services', 'Signal Transduction', 'Site', 'Slice', 'Small Business Innovation Research Grant', 'Speed', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Visualization software', 'Work', 'base', 'cell type', 'data visualization', 'design', 'experimental study', 'fluorescence imaging', 'genome wide association study', 'human disease', 'imaging modality', 'imaging system', 'instrument', 'interest', 'neural circuit', 'operation', 'programs', 'prototype', 'scale up', 'single molecule', 'software development', 'tool', 'touchscreen', 'user-friendly']",NIMH,"SPATIAL GENOMICS, INC.",R43,2018,885468,-0.04871207558253981
"Functional Cardiovascular 4D MRI in Congenital Heart Disease SUMMARY / ABSTRACT Congenital heart disease (CHD) is the most common birth defect, affecting 1.2% of all live births. Imaging plays a major role in managing CHD but remains challenging for evaluating complex cardiac and vascular abnormalities across a wide range of age and habitus. To address these limitations, the PIs have developed cardiovascular 4D flow MRI which can measure complex 3D blood flow in-vivo, a task difficult or impossible to obtain with other imaging strategies. Recent efforts have focused on two forms of CHD: 1) bicuspid aortic valve (BAV) which is the most common form of CHD, and 2) single ventricle physiology (SVP), one of the most severe forms of CHD. Our 4D flow MRI studies have successfully identified new hemodynamic biomarkers to better characterize CHD. We were the first to establish a physiologic link between aberrant 3D blood flow, elevated wall shear stress (WSS), aortopathy phenotype, and aortic wall tissue degeneration on histopathology in patients with BAV. In patients with SVP, our findings demonstrated relationships between surgical correction strategies and flow distribution to the lungs, a known factor implicated in SVP outcome. We have achieved successful clinical translation at Northwestern, where 4D flow MRI is now used as a clinical tool in diagnostic MRI exams for patients with CHD and aortic disease. Over the past four years, the PIs have assembled one of the largest 4D MRI databases with over 2500 patient exams. For this renewal application, we identified a need to increase the dynamic range of 4D MRI flow sensitivity to account for data complexity (3D + time) and the wide age range in CHD by a combination of dual-venc flow encoding, compressed sensing, and SSFP imaging. Second, three is a need for longitudinal studies to identify predictors of BAV and SVP outcome. Third, making these unique but complex 4D MRI data sets and analysis tools more widely available to the greater research community is challenging. In addition, no automated methods currently exist for advanced processing such as atlas based analysis across large cohorts. Analysis is thus time consuming and requires manual interactions (e.g. 3D vessel segmentation) which limits reproducibility and translation. To address this need, an established Northwestern data archival and pipeline processing resource based on remote high-performance computing clusters (NUNDA) will be utilized for standardized data archival, sharing, and pipeline processing of 4D MRI data. This platform will provide the unique opportunity to utilize annotated data available in the 4D MRI database (>1300 BAV, SVP, and control 4D MRI data analyzed in the initial funding cycle) for application of machine learning concepts to establish (semi-)automated 4D MRI analysis workflows in NUNDA. Thus, the renewal application for this study aims to 1) develop a rapid (15 min) non-contrast 4D MRI for clinical translation, 2) leverage the existing large 4D MRI database to identify 4D MRI metrics predictive of long-term (> 5 years) CHD patient outcome, and 3) establish a remote NUNDA platform for 4D MRI data sharing and automated analysis across large cohorts. PROJECT NARRATIVE Our goal is to develop non-contrast 4D MRI, a new diagnostic test to achieve an improved assessment for the most common and one of the most severe forms of congenital heart disease: bicuspid aortic valve and single ventricle physiology. We will leverage an existing large 4D MRI database to allow for long-term 5-8-year follow-up to establish new measures for improved outcome prediction and therapy management for patients with bicuspid aortic valve and single ventricle physiology. A comprehensive data archive will be established to allow for the dissemination of the 4D MRI data, analysis tools, and study results to the greater research community.",Functional Cardiovascular 4D MRI in Congenital Heart Disease,9903426,R01HL115828,"['3-Dimensional', '4D MRI', 'Acceleration', 'Address', 'Adult', 'Affect', 'Age', 'Anatomy', 'Aortic Diseases', 'Archives', 'Atlases', 'Biological Markers', 'Blood flow', 'Cardiac Output', 'Cardiovascular system', 'Child', 'Clinical', 'Common Ventricle', 'Communities', 'Complex', 'Congenital Abnormality', 'Consumption', 'Contrast Media', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnostic', 'Diagnostic tests', 'Dimensions', 'Disease Progression', 'Exposure to', 'Funding', 'General Anesthesia', 'Goals', 'Growth', 'Heart Abnormalities', 'Heart Rate', 'High Performance Computing', 'Histopathology', 'Hospitals', 'Image', 'Infant', 'Link', 'Live Birth', 'Longitudinal Studies', 'Lung', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Operative Surgical Procedures', 'Outcome', 'Outcome Study', 'Oxygen', 'Patient-Focused Outcomes', 'Patients', 'Phenotype', 'Physiological', 'Physiology', 'Play', 'Population', 'Process', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Role', 'Secure', 'Sex Differences', 'Testing', 'Time', 'Translations', 'adverse outcome', 'analysis pipeline', 'aortic valve', 'automated analysis', 'base', 'bicuspid aortic valve', 'clinical translation', 'cluster computing', 'cohort', 'congenital heart disorder', 'data analysis pipeline', 'data archive', 'data sharing', 'data standards', 'design', 'exercise capacity', 'flexibility', 'follow-up', 'hemodynamics', 'improved', 'improved outcome', 'in vivo', 'novel', 'novel diagnostics', 'outcome prediction', 'patient population', 'pediatric patients', 'prevent', 'shear stress', 'spatiotemporal', 'tissue degeneration', 'tool', 'vascular abnormality']",NHLBI,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2020,747084,-0.009637893949438526
"Functional Cardiovascular 4D MRI in Congenital Heart Disease SUMMARY / ABSTRACT Congenital heart disease (CHD) is the most common birth defect, affecting 1.2% of all live births. Imaging plays a major role in managing CHD but remains challenging for evaluating complex cardiac and vascular abnormalities across a wide range of age and habitus. To address these limitations, the PIs have developed cardiovascular 4D flow MRI which can measure complex 3D blood flow in-vivo, a task difficult or impossible to obtain with other imaging strategies. Recent efforts have focused on two forms of CHD: 1) bicuspid aortic valve (BAV) which is the most common form of CHD, and 2) single ventricle physiology (SVP), one of the most severe forms of CHD. Our 4D flow MRI studies have successfully identified new hemodynamic biomarkers to better characterize CHD. We were the first to establish a physiologic link between aberrant 3D blood flow, elevated wall shear stress (WSS), aortopathy phenotype, and aortic wall tissue degeneration on histopathology in patients with BAV. In patients with SVP, our findings demonstrated relationships between surgical correction strategies and flow distribution to the lungs, a known factor implicated in SVP outcome. We have achieved successful clinical translation at Northwestern, where 4D flow MRI is now used as a clinical tool in diagnostic MRI exams for patients with CHD and aortic disease. Over the past four years, the PIs have assembled one of the largest 4D MRI databases with over 2500 patient exams. For this renewal application, we identified a need to increase the dynamic range of 4D MRI flow sensitivity to account for data complexity (3D + time) and the wide age range in CHD by a combination of dual-venc flow encoding, compressed sensing, and SSFP imaging. Second, three is a need for longitudinal studies to identify predictors of BAV and SVP outcome. Third, making these unique but complex 4D MRI data sets and analysis tools more widely available to the greater research community is challenging. In addition, no automated methods currently exist for advanced processing such as atlas based analysis across large cohorts. Analysis is thus time consuming and requires manual interactions (e.g. 3D vessel segmentation) which limits reproducibility and translation. To address this need, an established Northwestern data archival and pipeline processing resource based on remote high-performance computing clusters (NUNDA) will be utilized for standardized data archival, sharing, and pipeline processing of 4D MRI data. This platform will provide the unique opportunity to utilize annotated data available in the 4D MRI database (>1300 BAV, SVP, and control 4D MRI data analyzed in the initial funding cycle) for application of machine learning concepts to establish (semi-)automated 4D MRI analysis workflows in NUNDA. Thus, the renewal application for this study aims to 1) develop a rapid (15 min) non-contrast 4D MRI for clinical translation, 2) leverage the existing large 4D MRI database to identify 4D MRI metrics predictive of long-term (> 5 years) CHD patient outcome, and 3) establish a remote NUNDA platform for 4D MRI data sharing and automated analysis across large cohorts. PROJECT NARRATIVE Our goal is to develop non-contrast 4D MRI, a new diagnostic test to achieve an improved assessment for the most common and one of the most severe forms of congenital heart disease: bicuspid aortic valve and single ventricle physiology. We will leverage an existing large 4D MRI database to allow for long-term 5-8-year follow-up to establish new measures for improved outcome prediction and therapy management for patients with bicuspid aortic valve and single ventricle physiology. A comprehensive data archive will be established to allow for the dissemination of the 4D MRI data, analysis tools, and study results to the greater research community.",Functional Cardiovascular 4D MRI in Congenital Heart Disease,9663985,R01HL115828,"['3-Dimensional', '4D MRI', 'Acceleration', 'Address', 'Adult', 'Affect', 'Age', 'Anatomy', 'Aortic Diseases', 'Archives', 'Atlases', 'Biological Markers', 'Blood flow', 'Cardiac Output', 'Cardiovascular system', 'Child', 'Clinical', 'Common Ventricle', 'Communities', 'Complex', 'Congenital Abnormality', 'Consumption', 'Contrast Media', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnostic', 'Diagnostic tests', 'Dimensions', 'Disease Progression', 'Exposure to', 'Funding', 'General Anesthesia', 'Goals', 'Growth', 'Heart Abnormalities', 'Heart Rate', 'High Performance Computing', 'Histopathology', 'Hospitals', 'Image', 'Infant', 'Link', 'Live Birth', 'Longitudinal Studies', 'Lung', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Operative Surgical Procedures', 'Outcome', 'Outcome Study', 'Oxygen', 'Patient-Focused Outcomes', 'Patients', 'Phenotype', 'Physiological', 'Physiology', 'Play', 'Population', 'Process', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Role', 'Secure', 'Sex Differences', 'Standardization', 'Testing', 'Time', 'Translations', 'adverse outcome', 'analysis pipeline', 'aortic valve', 'automated analysis', 'base', 'bicuspid aortic valve', 'clinical translation', 'cluster computing', 'cohort', 'congenital heart disorder', 'data archive', 'data pipeline', 'data sharing', 'design', 'exercise capacity', 'flexibility', 'follow-up', 'hemodynamics', 'improved', 'improved outcome', 'in vivo', 'novel', 'novel diagnostics', 'outcome prediction', 'patient population', 'pediatric patients', 'prevent', 'shear stress', 'spatiotemporal', 'tissue degeneration', 'tool', 'vascular abnormality']",NHLBI,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,740572,-0.009637893949438526
"Functional Cardiovascular 4D MRI in Congenital Heart Disease SUMMARY / ABSTRACT Congenital heart disease (CHD) is the most common birth defect, affecting 1.2% of all live births. Imaging plays a major role in managing CHD but remains challenging for evaluating complex cardiac and vascular abnormalities across a wide range of age and habitus. To address these limitations, the PIs have developed cardiovascular 4D flow MRI which can measure complex 3D blood flow in-vivo, a task difficult or impossible to obtain with other imaging strategies. Recent efforts have focused on two forms of CHD: 1) bicuspid aortic valve (BAV) which is the most common form of CHD, and 2) single ventricle physiology (SVP), one of the most severe forms of CHD. Our 4D flow MRI studies have successfully identified new hemodynamic biomarkers to better characterize CHD. We were the first to establish a physiologic link between aberrant 3D blood flow, elevated wall shear stress (WSS), aortopathy phenotype, and aortic wall tissue degeneration on histopathology in patients with BAV. In patients with SVP, our findings demonstrated relationships between surgical correction strategies and flow distribution to the lungs, a known factor implicated in SVP outcome. We have achieved successful clinical translation at Northwestern, where 4D flow MRI is now used as a clinical tool in diagnostic MRI exams for patients with CHD and aortic disease. Over the past four years, the PIs have assembled one of the largest 4D MRI databases with over 2500 patient exams. For this renewal application, we identified a need to increase the dynamic range of 4D MRI flow sensitivity to account for data complexity (3D + time) and the wide age range in CHD by a combination of dual-venc flow encoding, compressed sensing, and SSFP imaging. Second, three is a need for longitudinal studies to identify predictors of BAV and SVP outcome. Third, making these unique but complex 4D MRI data sets and analysis tools more widely available to the greater research community is challenging. In addition, no automated methods currently exist for advanced processing such as atlas based analysis across large cohorts. Analysis is thus time consuming and requires manual interactions (e.g. 3D vessel segmentation) which limits reproducibility and translation. To address this need, an established Northwestern data archival and pipeline processing resource based on remote high-performance computing clusters (NUNDA) will be utilized for standardized data archival, sharing, and pipeline processing of 4D MRI data. This platform will provide the unique opportunity to utilize annotated data available in the 4D MRI database (>1300 BAV, SVP, and control 4D MRI data analyzed in the initial funding cycle) for application of machine learning concepts to establish (semi-)automated 4D MRI analysis workflows in NUNDA. Thus, the renewal application for this study aims to 1) develop a rapid (15 min) non-contrast 4D MRI for clinical translation, 2) leverage the existing large 4D MRI database to identify 4D MRI metrics predictive of long-term (> 5 years) CHD patient outcome, and 3) establish a remote NUNDA platform for 4D MRI data sharing and automated analysis across large cohorts. PROJECT NARRATIVE Our goal is to develop non-contrast 4D MRI, a new diagnostic test to achieve an improved assessment for the most common and one of the most severe forms of congenital heart disease: bicuspid aortic valve and single ventricle physiology. We will leverage an existing large 4D MRI database to allow for long-term 5-8-year follow-up to establish new measures for improved outcome prediction and therapy management for patients with bicuspid aortic valve and single ventricle physiology. A comprehensive data archive will be established to allow for the dissemination of the 4D MRI data, analysis tools, and study results to the greater research community.",Functional Cardiovascular 4D MRI in Congenital Heart Disease,9515521,R01HL115828,"['4D MRI', 'Acceleration', 'Address', 'Adult', 'Affect', 'Age', 'Anatomy', 'Aortic Diseases', 'Archives', 'Atlases', 'Biological Markers', 'Blood flow', 'Cardiac Output', 'Cardiovascular system', 'Child', 'Clinical', 'Common Ventricle', 'Communities', 'Complex', 'Congenital Abnormality', 'Contrast Media', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnostic', 'Diagnostic tests', 'Dimensions', 'Disease Progression', 'Exposure to', 'Funding', 'General Anesthesia', 'Goals', 'Growth', 'Heart Abnormalities', 'Heart Rate', 'High Performance Computing', 'Histopathology', 'Hospitals', 'Image', 'Infant', 'Link', 'Live Birth', 'Longitudinal Studies', 'Lung', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Operative Surgical Procedures', 'Outcome', 'Outcome Study', 'Oxygen', 'Patient-Focused Outcomes', 'Patients', 'Phenotype', 'Physiological', 'Physiology', 'Play', 'Population', 'Process', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Role', 'Secure', 'Standardization', 'Testing', 'Time', 'Translations', 'adverse outcome', 'aortic valve', 'base', 'bicuspid aortic valve', 'clinical translation', 'cluster computing', 'cohort', 'congenital heart disorder', 'data archive', 'data sharing', 'design', 'exercise capacity', 'flexibility', 'follow-up', 'hemodynamics', 'improved', 'improved outcome', 'in vivo', 'novel', 'novel diagnostics', 'outcome prediction', 'patient population', 'pediatric patients', 'prevent', 'sex', 'shear stress', 'spatiotemporal', 'tissue degeneration', 'tool', 'vascular abnormality']",NHLBI,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2018,704597,-0.009637893949438526
"Toward Automated Spike Sorting via Ground Truth Neural Recordings PROJECT​ ​SUMMARY Scaling extracellular electrophysiology to higher channel counts is hindered by the burden of data handling,storage, and especially preprocessing, e.g. spike sorting ​[1]​. The burden of spike sorting can in principle be reduced through a combination of high-density multielectrode array (probe) technology and algorithm optimization to yield a spike sorting method that is both highly accurate and fully automated [2–4]​. With a known-good spike sorting method in hand, the algorithm can be baked into the data stream as early as possible to allow for automatic data sorting and a massive reduction in data rate to downstream storage and processing. However, it takes an investment of considerable resources to implement this sort of large-scale real-time processing, and great confidence to throw away raw data and keep​ ​only​ ​processed​ ​data. Accuracy and automation of spike sorting increases with the spatial density of recording sites ​[5–7]​. Neural activity recorded from high-density probes can serve as a data corpus for testing the accuracy of spike sorting algorithms. However, to quantify spike sorting performance for comparison between algorithms, the ground truth spiking activity of neurons captured in the data corpus must be measured, such as by simultaneously recording via patch-clamp pipette or some other recording modality ​[8–10]​. Unfortunately, because ground truth recordings are so challenging to perform, they remain too rare to allow for this sort of analysis in a large-scale, meaningful way ​[11]​. Until this need is met, spike sorting development lacks a compass, and cutting-edge techniques such as supervised machine learning which require large amounts of labelled data remain out-of-reach ​[12]​. ​Accordingly, we propose a series of multimodal neural recordings combining multielectrode array and patch pipette techniques to generate​ ​a​ ​corpus​ ​of​ ​ground​ ​truth​ ​data​ ​for​ ​validation​ ​of​ ​spike​ ​sorting​ ​algorithms. PROJECT​ ​NARRATIVE Electrophysiological​ ​recording​ ​systems​ ​allow​ ​direct​ ​observation​ ​of​ ​neural​ ​activity​ ​in​ ​animal subjects.​ ​This​ ​facilitates​ ​the​ ​study​ ​of​ ​crucial​ ​neuroscientific​ ​topics​ ​such​ ​as​ ​development, learning​ ​and​ ​memory,​ ​and​ ​cognition,​ ​as​ ​well​ ​as​ ​brain​ ​diseases​ ​such​ ​as​ ​Alzheimer’s,​ ​epilepsy, Parkinson’s,​ ​and​ ​depression.​ ​LeafLabs’​ ​tools​ ​for​ ​characterizing​ ​and​ ​analyzing​ ​high-channel count​ ​electrophysiology​ ​recordings​ ​will​ ​allow​ ​researchers​ ​to​ ​more​ ​easily​ ​collect​ ​and​ ​interpret neural​ ​data​ ​at​ ​a​ ​large​ ​scale.",Toward Automated Spike Sorting via Ground Truth Neural Recordings,9558957,R41MH116752,"['Algorithm Design', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Animals', 'Automation', 'Brain', 'Brain Diseases', 'Cells', 'Cellular Morphology', 'Chronic', 'Cognition', 'Data', 'Development', 'Dyes', 'Electrophysiology (science)', 'Epilepsy', 'Failure', 'Future', 'Geometry', 'Hand', 'Head', 'Individual', 'Investments', 'Label', 'Learning', 'Location', 'Machine Learning', 'Measures', 'Memory', 'Mental Depression', 'Methods', 'Microelectrodes', 'Microscope', 'Modality', 'Morphology', 'Mus', 'Neurons', 'Outcome', 'Output', 'Parkinson Disease', 'Performance', 'Population', 'Process', 'Property', 'Quantitative Evaluations', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Series', 'Silicon', 'Site', 'Sorting - Cell Movement', 'Stream', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Validation', 'Visual Cortex', 'Width', 'Work', 'awake', 'base', 'cell assembly', 'data sharing', 'density', 'design', 'extracellular', 'image guided', 'information processing', 'multi-electrode arrays', 'multimodality', 'patch clamp', 'relating to nervous system', 'tool', 'two-photon']",NIMH,"LEAFLABS, LLC",R41,2018,438789,-0.024605227119413542
"Towards integrated 3D reconstruction of whole human brains at subcellular resolution Project Summary A detailed understanding of the anatomical and molecular architectures of brain cells and their brain-wide organization is essential for interrogating human brain function and dysfunction. Extensive efforts have been made toward mapping brain cells through various lenses, which have established invaluable databases yielding new insights. However, integrative extraction of the multimodal properties of various cell-types brain-wide within the same brain, crucial to elucidating complex intercellular relationships, remains nearly impossible. We have developed high-throughput, cost-effective technology platforms to create a fully integrated three-dimensional (3D) human brain cell atlas by simultaneously mapping high-dimensional features (e.g., spatial, molecular, morphological, and microenvironment information) of all cells acquired from the same whole brain. The proposed work will establish the most comprehensive 3D human brain map to date, with unprecedented resolution and completeness. We envision that this atlas will facilitate the integration of a broad range of studies and allow the research community to interrogate human brain structure and function at multiple levels. In Aim 1, we will apply a novel technology to transform whole human brain tissue into indestructible hydrogel–tissue hybrids that allow highly multiplexed molecular labeling and subcellular-resolution volume imaging. In Aim 2, we will apply scalable labeling and imaging technologies to map the brain-wide 3D distribution of various cell-type and structural markers at subcellular resolution within the same brain. Our chemical engineering–based approach to this aim will enable cost-effective, lossless 3D labeling of the entire human brain at lower cost as traditional subsampling approaches. True volume labeling and subcellular-resolution imaging will allow us to extract fine morphological and connectivity information from labeled cells and reconstruct the microenvironment of all cells. In Aim 3, we will use a host of rapid and highly automated algorithms to perform unbiased, integrative high- dimensional phenotyping of all cells based on their spatial location, molecular expression, morphology, and microenvironment. In Aim 4, we will perform super-resolution phenotyping of cells in a selected brain region from the same sample used in Aim 3 to map inter-areal axonal connectivity at single-fiber resolution and to characterize chemical synapses. This integrative approach will likely unveil unique cell-types and brain regions, a crucial step toward a better understanding of brain function. The complete 3D dataset will be linked to magnetic resonance and diffusion spectrum images and existing reference atlases to facilitate the integration of a wide breadth of study at multiple levels and to make the data publicly accessible for mining and analysis. A detailed picture of the human brain’s complex intermolecular, intercellular, and interareal interactions is of paramount importance for understanding its function and dysfunction. However, the immense complexity of the human brain and the absence of enabling technologies have hither-to made it impossible to interrogate these interactions brain-wide. Here we propose to use a host of new technology platforms to create a fully integrated whole human brain cell atlas with unprecedented levels of resolution and completeness.",Towards integrated 3D reconstruction of whole human brains at subcellular resolution,9935968,U01MH117072,"['3-Dimensional', 'Algorithms', 'Anatomy', 'Antibodies', 'Architecture', 'Atlases', 'Axon', 'Brain', 'Brain Mapping', 'Brain Stem', 'Brain region', 'Cell Nucleus', 'Cells', 'Cerebral hemisphere', 'Chemical Engineering', 'Chemical Synapse', 'Communities', 'Complex', 'Custom', 'Cytoplasm', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Diffusion', 'Disease', 'Dyes', 'Fiber', 'Formalin', 'Functional disorder', 'Goals', 'Histologic', 'Human', 'Hybrids', 'Hydrogels', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Label', 'Left', 'Libraries', 'Link', 'Location', 'MRI Scans', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Maps', 'Methods', 'Microscope', 'Mining', 'Molecular', 'Molecular Profiling', 'Morphology', 'Mus', 'Optics', 'Pattern', 'Permeability', 'Phenotype', 'Process', 'Property', 'Proteins', 'Proteome', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Slice', 'Stains', 'Structure', 'Synapses', 'Techniques', 'Technology', 'Thick', 'Tissues', 'Work', 'antibody libraries', 'automated algorithm', 'base', 'brain cell', 'brain tissue', 'cell type', 'cost', 'cost effective', 'deep learning', 'high dimensionality', 'imaging biomarker', 'insight', 'lens', 'macromolecule', 'molecular phenotype', 'multidisciplinary', 'multimodal data', 'multimodality', 'new technology', 'novel', 'novel therapeutics', 'preservation', 'reconstruction', 'spectrograph', 'two-photon']",NIMH,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,U01,2020,1563930,-0.022418746541225734
"Towards integrated 3D reconstruction of whole human brains at subcellular resolution Project Summary A detailed understanding of the anatomical and molecular architectures of brain cells and their brain-wide organization is essential for interrogating human brain function and dysfunction. Extensive efforts have been made toward mapping brain cells through various lenses, which have established invaluable databases yielding new insights. However, integrative extraction of the multimodal properties of various cell-types brain-wide within the same brain, crucial to elucidating complex intercellular relationships, remains nearly impossible. We have developed high-throughput, cost-effective technology platforms to create a fully integrated three-dimensional (3D) human brain cell atlas by simultaneously mapping high-dimensional features (e.g., spatial, molecular, morphological, and microenvironment information) of all cells acquired from the same whole brain. The proposed work will establish the most comprehensive 3D human brain map to date, with unprecedented resolution and completeness. We envision that this atlas will facilitate the integration of a broad range of studies and allow the research community to interrogate human brain structure and function at multiple levels. In Aim 1, we will apply a novel technology to transform whole human brain tissue into indestructible hydrogel–tissue hybrids that allow highly multiplexed molecular labeling and subcellular-resolution volume imaging. In Aim 2, we will apply scalable labeling and imaging technologies to map the brain-wide 3D distribution of various cell-type and structural markers at subcellular resolution within the same brain. Our chemical engineering–based approach to this aim will enable cost-effective, lossless 3D labeling of the entire human brain at lower cost as traditional subsampling approaches. True volume labeling and subcellular-resolution imaging will allow us to extract fine morphological and connectivity information from labeled cells and reconstruct the microenvironment of all cells. In Aim 3, we will use a host of rapid and highly automated algorithms to perform unbiased, integrative high- dimensional phenotyping of all cells based on their spatial location, molecular expression, morphology, and microenvironment. In Aim 4, we will perform super-resolution phenotyping of cells in a selected brain region from the same sample used in Aim 3 to map inter-areal axonal connectivity at single-fiber resolution and to characterize chemical synapses. This integrative approach will likely unveil unique cell-types and brain regions, a crucial step toward a better understanding of brain function. The complete 3D dataset will be linked to magnetic resonance and diffusion spectrum images and existing reference atlases to facilitate the integration of a wide breadth of study at multiple levels and to make the data publicly accessible for mining and analysis. A detailed picture of the human brain’s complex intermolecular, intercellular, and interareal interactions is of paramount importance for understanding its function and dysfunction. However, the immense complexity of the human brain and the absence of enabling technologies have hither-to made it impossible to interrogate these interactions brain-wide. Here we propose to use a host of new technology platforms to create a fully integrated whole human brain cell atlas with unprecedented levels of resolution and completeness.",Towards integrated 3D reconstruction of whole human brains at subcellular resolution,9768578,U01MH117072,"['3-Dimensional', 'Algorithms', 'Anatomy', 'Antibodies', 'Architecture', 'Atlases', 'Axon', 'Brain', 'Brain Mapping', 'Brain Stem', 'Brain region', 'Cell Nucleus', 'Cells', 'Cerebral hemisphere', 'Chemical Engineering', 'Chemical Synapse', 'Communities', 'Complex', 'Custom', 'Cytoplasm', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Diffusion', 'Dimensions', 'Disease', 'Dyes', 'Fiber', 'Formalin', 'Functional disorder', 'Goals', 'Histologic', 'Human', 'Hybrids', 'Hydrogels', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Label', 'Left', 'Libraries', 'Link', 'Location', 'MRI Scans', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Maps', 'Methods', 'Microscope', 'Mining', 'Molecular', 'Molecular Profiling', 'Morphology', 'Mus', 'Optics', 'Pattern', 'Permeability', 'Phenotype', 'Process', 'Property', 'Proteins', 'Proteome', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Slice', 'Stains', 'Structure', 'Synapses', 'Techniques', 'Technology', 'Thick', 'Tissues', 'Work', 'antibody libraries', 'base', 'brain cell', 'brain tissue', 'cell type', 'cost', 'cost effective', 'deep learning', 'high dimensionality', 'imaging biomarker', 'insight', 'lens', 'macromolecule', 'molecular phenotype', 'multidisciplinary', 'multimodality', 'new technology', 'novel', 'novel therapeutics', 'preservation', 'reconstruction', 'spectrograph', 'two-photon']",NIMH,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,U01,2019,2003389,-0.022418746541225734
"Towards integrated 3D reconstruction of whole human brains at subcellular resolution Project Summary A detailed understanding of the anatomical and molecular architectures of brain cells and their brain-wide organization is essential for interrogating human brain function and dysfunction. Extensive efforts have been made toward mapping brain cells through various lenses, which have established invaluable databases yielding new insights. However, integrative extraction of the multimodal properties of various cell-types brain-wide within the same brain, crucial to elucidating complex intercellular relationships, remains nearly impossible. We have developed high-throughput, cost-effective technology platforms to create a fully integrated three-dimensional (3D) human brain cell atlas by simultaneously mapping high-dimensional features (e.g., spatial, molecular, morphological, and microenvironment information) of all cells acquired from the same whole brain. The proposed work will establish the most comprehensive 3D human brain map to date, with unprecedented resolution and completeness. We envision that this atlas will facilitate the integration of a broad range of studies and allow the research community to interrogate human brain structure and function at multiple levels. In Aim 1, we will apply a novel technology to transform whole human brain tissue into indestructible hydrogel–tissue hybrids that allow highly multiplexed molecular labeling and subcellular-resolution volume imaging. In Aim 2, we will apply scalable labeling and imaging technologies to map the brain-wide 3D distribution of various cell-type and structural markers at subcellular resolution within the same brain. Our chemical engineering–based approach to this aim will enable cost-effective, lossless 3D labeling of the entire human brain at lower cost as traditional subsampling approaches. True volume labeling and subcellular-resolution imaging will allow us to extract fine morphological and connectivity information from labeled cells and reconstruct the microenvironment of all cells. In Aim 3, we will use a host of rapid and highly automated algorithms to perform unbiased, integrative high- dimensional phenotyping of all cells based on their spatial location, molecular expression, morphology, and microenvironment. In Aim 4, we will perform super-resolution phenotyping of cells in a selected brain region from the same sample used in Aim 3 to map inter-areal axonal connectivity at single-fiber resolution and to characterize chemical synapses. This integrative approach will likely unveil unique cell-types and brain regions, a crucial step toward a better understanding of brain function. The complete 3D dataset will be linked to magnetic resonance and diffusion spectrum images and existing reference atlases to facilitate the integration of a wide breadth of study at multiple levels and to make the data publicly accessible for mining and analysis. A detailed picture of the human brain’s complex intermolecular, intercellular, and interareal interactions is of paramount importance for understanding its function and dysfunction. However, the immense complexity of the human brain and the absence of enabling technologies have hither-to made it impossible to interrogate these interactions brain-wide. Here we propose to use a host of new technology platforms to create a fully integrated whole human brain cell atlas with unprecedented levels of resolution and completeness.",Towards integrated 3D reconstruction of whole human brains at subcellular resolution,9584926,U01MH117072,"['Algorithms', 'Anatomy', 'Antibodies', 'Architecture', 'Atlases', 'Axon', 'Brain', 'Brain Mapping', 'Brain Stem', 'Brain region', 'Cell Nucleus', 'Cells', 'Cerebral hemisphere', 'Chemical Engineering', 'Chemical Synapse', 'Communities', 'Complex', 'Custom', 'Cytoplasm', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Diffusion', 'Dimensions', 'Disease', 'Dyes', 'Fiber', 'Formalin', 'Functional disorder', 'Goals', 'Histologic', 'Human', 'Hybrids', 'Hydrogels', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Label', 'Left', 'Libraries', 'Link', 'Location', 'MRI Scans', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Maps', 'Methods', 'Microscope', 'Mining', 'Molecular', 'Molecular Profiling', 'Morphology', 'Mus', 'Optics', 'Pattern', 'Permeability', 'Phenotype', 'Process', 'Property', 'Proteins', 'Proteome', 'Research', 'Resolution', 'Sampling', 'Scanning', 'Slice', 'Stains', 'Structure', 'Synapses', 'Techniques', 'Technology', 'Thick', 'Tissues', 'Work', 'antibody libraries', 'base', 'brain cell', 'brain tissue', 'cell type', 'cost', 'cost effective', 'deep learning', 'high dimensionality', 'imaging biomarker', 'insight', 'lens', 'macromolecule', 'molecular phenotype', 'multidisciplinary', 'multimodality', 'new technology', 'novel', 'novel therapeutics', 'reconstruction', 'spectrograph', 'two-photon']",NIMH,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,U01,2018,1882491,-0.022418746541225734
"RAVE: A New Open Software Tool for Analysis and Visualization of Electrocorticography Data Project Summary/Abstract  A fast-growing technique in human neuroscience is electrocorticography (ECOG), the only technique that allows the activity of small population of neurons in the human brain to be directly recorded. We use the term ECOG to refer to the entire range of invasive recording techniques (from subdural strips and grids to penetrating electrodes) that share the common attribute of recording neural activity from the human brain with high spatial and temporal resolution. While this ability has resulted in many high-impact advances in understanding fundamental mechanisms of brain function in health and disease, it generates staggering amounts of data as a single patient can be implanted with hundreds of electrodes, each sampled thousands of times a second for hours or even days. The difficulty of exploring these vast datasets is the rate-limiting step in using them to improve human health. We propose to overcome this obstacle by creating an easy-to-use, powerful platform designed from the ground up for the unique properties of ECOG. We dub this software tool RAVE (“R Analysis and Visualization of Electrocorticography data”).  The first goal of Aim 1 is to release RAVE 1.0 to the entire ECOG community by month 6 of the first funding period. This will maximize transformative impact by putting the new tools in the hands of users as quickly as possible, facilitating rapid adoption. The design philosophy of RAVE is driven by three imperatives. The first is to keep users ""close to the data"" so that users may make discoveries about the brain without being misled by artifacts. The second imperative is rigorous statistical methodology. The final imperative is ""play well with others"". As described in Aim 2, our approach will make it easy to seamlessly incorporate new and existing analysis tools written in Matlab, C++, Python or R into RAVE, giving users the best of both worlds: advanced but easy-to-use visualization of results from ECOG experiments, whether they are analyzed with the off-the- shelf tools routines provided with RAVE or novel tools developed by others. Project Narrative  A fast-growing technique in human neuroscience is electrocorticography (ECOG), the only technique that allow the activity of small population of neurons in the human brain to be directly recorded with high spatial and temporal resolution. ECOG generates staggering amounts of data, and the rate-limiting step in generating new insights about the human brain is the difficulty in exploring this vast quantity of data. We propose to remove this obstacle by creating an easy-to-use, powerful platform designed from the ground up for the analysis and visualization of ECOG data, known as RAVE (“R Analysis and Visualization of Electrocorticography data”).",RAVE: A New Open Software Tool for Analysis and Visualization of Electrocorticography Data,9933092,R24MH117529,"['Adoption', 'Algorithms', 'Amalgam', 'Brain', 'Code', 'Communication', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Electrocorticogram', 'Electrodes', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Grant', 'Health', 'Hour', 'Human', 'Human Activities', 'Implant', 'Implanted Electrodes', 'Laboratories', 'Language', 'Least-Squares Analysis', 'Letters', 'Literature', 'Medicine', 'Methodology', 'Morphologic artifacts', 'Neurons', 'Neurosciences', 'Nightmare', 'Paper', 'Patients', 'Philosophy', 'Play', 'Plug-in', 'Population', 'Proliferating', 'Property', 'Pythons', 'Recording of previous events', 'Research', 'Research Design', 'Research Personnel', 'Rest', 'Sampling', 'Seeds', 'Software Design', 'Software Tools', 'Techniques', 'Time', 'United States National Institutes of Health', 'Variant', 'Visit', 'Visualization', 'application programming interface', 'base', 'college', 'computer science', 'design', 'experience', 'experimental study', 'graphical user interface', 'improved', 'insight', 'interoperability', 'novel', 'open source', 'programs', 'relating to nervous system', 'statistical and machine learning', 'statistics', 'temporal measurement', 'tool', 'wiki']",NIMH,BAYLOR COLLEGE OF MEDICINE,R24,2020,236620,-0.030756743617379782
"RAVE: A New Open Software Tool for Analysis and Visualization of Electrocorticography Data Project Summary/Abstract  A fast-growing technique in human neuroscience is electrocorticography (ECOG), the only technique that allows the activity of small population of neurons in the human brain to be directly recorded. We use the term ECOG to refer to the entire range of invasive recording techniques (from subdural strips and grids to penetrating electrodes) that share the common attribute of recording neural activity from the human brain with high spatial and temporal resolution. While this ability has resulted in many high-impact advances in understanding fundamental mechanisms of brain function in health and disease, it generates staggering amounts of data as a single patient can be implanted with hundreds of electrodes, each sampled thousands of times a second for hours or even days. The difficulty of exploring these vast datasets is the rate-limiting step in using them to improve human health. We propose to overcome this obstacle by creating an easy-to-use, powerful platform designed from the ground up for the unique properties of ECOG. We dub this software tool RAVE (“R Analysis and Visualization of Electrocorticography data”).  The first goal of Aim 1 is to release RAVE 1.0 to the entire ECOG community by month 6 of the first funding period. This will maximize transformative impact by putting the new tools in the hands of users as quickly as possible, facilitating rapid adoption. The design philosophy of RAVE is driven by three imperatives. The first is to keep users ""close to the data"" so that users may make discoveries about the brain without being misled by artifacts. The second imperative is rigorous statistical methodology. The final imperative is ""play well with others"". As described in Aim 2, our approach will make it easy to seamlessly incorporate new and existing analysis tools written in Matlab, C++, Python or R into RAVE, giving users the best of both worlds: advanced but easy-to-use visualization of results from ECOG experiments, whether they are analyzed with the off-the- shelf tools routines provided with RAVE or novel tools developed by others. Project Narrative  A fast-growing technique in human neuroscience is electrocorticography (ECOG), the only technique that allow the activity of small population of neurons in the human brain to be directly recorded with high spatial and temporal resolution. ECOG generates staggering amounts of data, and the rate-limiting step in generating new insights about the human brain is the difficulty in exploring this vast quantity of data. We propose to remove this obstacle by creating an easy-to-use, powerful platform designed from the ground up for the analysis and visualization of ECOG data, known as RAVE (“R Analysis and Visualization of Electrocorticography data”).",RAVE: A New Open Software Tool for Analysis and Visualization of Electrocorticography Data,9766391,R24MH117529,"['Adoption', 'Algorithms', 'Amalgam', 'Brain', 'Code', 'Communication', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Electrocorticogram', 'Electrodes', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Grant', 'Health', 'Hour', 'Human', 'Human Activities', 'Imagery', 'Implant', 'Implanted Electrodes', 'Laboratories', 'Language', 'Least-Squares Analysis', 'Letters', 'Literature', 'Machine Learning', 'Medicine', 'Methodology', 'Morphologic artifacts', 'Neurons', 'Neurosciences', 'Nightmare', 'Paper', 'Patients', 'Philosophy', 'Play', 'Plug-in', 'Population', 'Proliferating', 'Property', 'Pythons', 'Recording of previous events', 'Research', 'Research Design', 'Research Personnel', 'Rest', 'Sampling', 'Seeds', 'Software Design', 'Software Tools', 'Techniques', 'Time', 'United States National Institutes of Health', 'Variant', 'Visit', 'application programming interface', 'base', 'college', 'computer science', 'design', 'experience', 'experimental study', 'graphical user interface', 'improved', 'insight', 'interoperability', 'novel', 'open source', 'programs', 'relating to nervous system', 'statistics', 'temporal measurement', 'tool', 'wiki']",NIMH,BAYLOR COLLEGE OF MEDICINE,R24,2019,236620,-0.030756743617379782
"RAVE: A New Open Software Tool for Analysis and Visualization of Electrocorticography Data Project Summary/Abstract  A fast-growing technique in human neuroscience is electrocorticography (ECOG), the only technique that allows the activity of small population of neurons in the human brain to be directly recorded. We use the term ECOG to refer to the entire range of invasive recording techniques (from subdural strips and grids to penetrating electrodes) that share the common attribute of recording neural activity from the human brain with high spatial and temporal resolution. While this ability has resulted in many high-impact advances in understanding fundamental mechanisms of brain function in health and disease, it generates staggering amounts of data as a single patient can be implanted with hundreds of electrodes, each sampled thousands of times a second for hours or even days. The difficulty of exploring these vast datasets is the rate-limiting step in using them to improve human health. We propose to overcome this obstacle by creating an easy-to-use, powerful platform designed from the ground up for the unique properties of ECOG. We dub this software tool RAVE (“R Analysis and Visualization of Electrocorticography data”).  The first goal of Aim 1 is to release RAVE 1.0 to the entire ECOG community by month 6 of the first funding period. This will maximize transformative impact by putting the new tools in the hands of users as quickly as possible, facilitating rapid adoption. The design philosophy of RAVE is driven by three imperatives. The first is to keep users ""close to the data"" so that users may make discoveries about the brain without being misled by artifacts. The second imperative is rigorous statistical methodology. The final imperative is ""play well with others"". As described in Aim 2, our approach will make it easy to seamlessly incorporate new and existing analysis tools written in Matlab, C++, Python or R into RAVE, giving users the best of both worlds: advanced but easy-to-use visualization of results from ECOG experiments, whether they are analyzed with the off-the- shelf tools routines provided with RAVE or novel tools developed by others. Project Narrative  A fast-growing technique in human neuroscience is electrocorticography (ECOG), the only technique that allow the activity of small population of neurons in the human brain to be directly recorded with high spatial and temporal resolution. ECOG generates staggering amounts of data, and the rate-limiting step in generating new insights about the human brain is the difficulty in exploring this vast quantity of data. We propose to remove this obstacle by creating an easy-to-use, powerful platform designed from the ground up for the analysis and visualization of ECOG data, known as RAVE (“R Analysis and Visualization of Electrocorticography data”).",RAVE: A New Open Software Tool for Analysis and Visualization of Electrocorticography Data,9592408,R24MH117529,"['Adoption', 'Algorithms', 'Amalgam', 'Brain', 'Code', 'Communication', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Electrocorticogram', 'Electrodes', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Grant', 'Health', 'Hour', 'Human', 'Human Activities', 'Imagery', 'Implant', 'Implanted Electrodes', 'Laboratories', 'Language', 'Least-Squares Analysis', 'Letters', 'Literature', 'Machine Learning', 'Medicine', 'Methodology', 'Morphologic artifacts', 'Neurons', 'Neurosciences', 'Nightmare', 'Paper', 'Patients', 'Philosophy', 'Play', 'Plug-in', 'Population', 'Proliferating', 'Property', 'Pythons', 'Recording of previous events', 'Research', 'Research Design', 'Research Personnel', 'Rest', 'Sampling', 'Seeds', 'Software Design', 'Software Tools', 'Techniques', 'Time', 'United States National Institutes of Health', 'Variant', 'Visit', 'application programming interface', 'base', 'college', 'computer science', 'design', 'experience', 'experimental study', 'graphical user interface', 'improved', 'insight', 'interoperability', 'novel', 'open source', 'programs', 'relating to nervous system', 'statistics', 'temporal measurement', 'tool', 'wiki']",NIMH,BAYLOR COLLEGE OF MEDICINE,R24,2018,244545,-0.030756743617379782
"PsyNeuLink:  A Block Modeling Environment for Cognitive Neuroscience and Computational Psychiatry Project Summary Paralleling the growth of neuroscience research, there has been an explosion in the development of computationally explicit models of the functions of core brain subsystems. Unfortunately, however, there has not been a commensurate development of the tools needed to share, validate, and compare such models, or integrate them into models of system-level function. Such sharing, evaluation, and integration are necessary if computational modeling efforts are to be useful not only in generating reliable and accurate accounts of how brain subsystems operate, but also of how they interact to give rise to higher cognitive functions, and how disruptions of such interactions may give rise to disturbances of mental function observed in psychiatric and neurological disorders. This proposal seeks to meet this need by developing PsyNeuLink: an open source, Python-based software environment that makes it easy to create new models, import and/or re-implement existing ones, integrate these within a single software environment that will facilitate head-to-head comparison of comparable models, the assembly of complementary models into system-level models, and serve as a common repository for the documentation and dissemination of such models for both research and didactic purposes (i.e., publication, education, etc.). These goals will be pursued under two Specific Aims: 1) Extend the scope of modeling efforts that PsyNeuLink can accommodate by: i) enhancing its application programmer interface (API) used to add new components and interfaces to statistical analysis tools and other modeling environments (such as PyTorch, Emergent and ACT-R; ii) enriching its Library by adding PsyNeuLink implementations of influential models of neural subsystems; and iii) developing a publicly available workbook of simulation exercises as both an introduction to PsyNeuLink and for use in Cognitive Neuroscience and Computational Psychiatry curricula. 2) Accelerate PsyNeuLink by developing a custom compiler that preserves its simplicity and flexibility, while dramatically increasing its speed, to make it suitable for simulation of large and complex system-level models, and for parameter estimation, model fitting, and model comparison. This project will exploit the power and accelerating use of Python, and modern just-in-time compilation methods to develop a tool designed specifically for the needs of systems-level Cognitive Neuroscience and Computational Psychiatry. This promises to open up new opportunities for research at the systems-level — a level of analysis that is crucial both for understanding how human mental function emerges from the interplay among neural subsystems, and how disturbances of individual neural subsystems impact this interplay, disruptions of which are almost certainly a critical factor in neurologic and psychiatric disorders. Project Narrative Paralleling the growth of neuroscience research has been an explosion in the development of computationally explicit models of the functions of core brain subsystems. Unfortunately, however, there has not been a commensurate development of the tools needed to share, validate, and compare such models, or integrate them into models of system-level function. This proposed project seeks to address this need by developing a standard software platform for the construction, documentation, sharing, and integration of computational models of brain function, that promises to accelerate the study of how system-level interactions give rise to mental function and, critically, the kinds of disruptions of such system-level interactions produced by disturbances of individual subsystems — disruptions that are sure to be a complex but critical factor in neurological and psychiatric disorders.",PsyNeuLink:  A Block Modeling Environment for Cognitive Neuroscience and Computational Psychiatry,9976610,R21MH117548,"['Acceleration', 'Address', 'Architecture', 'Attention', 'Basal Ganglia', 'Biological', 'Biological Models', 'Brain', 'Complement', 'Complex', 'Computer Models', 'Computer software', 'Custom', 'Data', 'Development', 'Documentation', 'Education', 'Educational Curriculum', 'Environment', 'Episodic memory', 'Evaluation', 'Exercise', 'Explosion', 'Foundations', 'Goals', 'Grain', 'Growth', 'Hippocampus (Brain)', 'Human', 'Individual', 'Influentials', 'Libraries', 'Literature', 'Maintenance', 'Manuals', 'Mental disorders', 'Methods', 'Modeling', 'Modernization', 'Neurosciences Research', 'Perceptual learning', 'Play', 'Prefrontal Cortex', 'Procedures', 'Psychiatry', 'Publications', 'Publishing', 'Pythons', 'Research', 'Role', 'Seeds', 'Short-Term Memory', 'Speed', 'Statistical Data Interpretation', 'System', 'Time', 'Writing', 'application programming interface', 'base', 'cognitive function', 'cognitive neuroscience', 'cognitive process', 'deep learning', 'deep neural network', 'design', 'flexibility', 'head-to-head comparison', 'improved', 'learning network', 'memory encoding', 'memory retrieval', 'mental function', 'nervous system disorder', 'neural model', 'open source', 'parallelization', 'preservation', 'programs', 'relating to nervous system', 'repository', 'simulation', 'tool', 'tool development']",NIMH,PRINCETON UNIVERSITY,R21,2020,197545,-0.06875246004759648
"PsyNeuLink:  A Block Modeling Environment for Cognitive Neuroscience and Computational Psychiatry Project Summary Paralleling the growth of neuroscience research, there has been an explosion in the development of computationally explicit models of the functions of core brain subsystems. Unfortunately, however, there has not been a commensurate development of the tools needed to share, validate, and compare such models, or integrate them into models of system-level function. Such sharing, evaluation, and integration are necessary if computational modeling efforts are to be useful not only in generating reliable and accurate accounts of how brain subsystems operate, but also of how they interact to give rise to higher cognitive functions, and how disruptions of such interactions may give rise to disturbances of mental function observed in psychiatric and neurological disorders. This proposal seeks to meet this need by developing PsyNeuLink: an open source, Python-based software environment that makes it easy to create new models, import and/or re-implement existing ones, integrate these within a single software environment that will facilitate head-to-head comparison of comparable models, the assembly of complementary models into system-level models, and serve as a common repository for the documentation and dissemination of such models for both research and didactic purposes (i.e., publication, education, etc.). These goals will be pursued under two Specific Aims: 1) Extend the scope of modeling efforts that PsyNeuLink can accommodate by: i) enhancing its application programmer interface (API) used to add new components and interfaces to statistical analysis tools and other modeling environments (such as PyTorch, Emergent and ACT-R; ii) enriching its Library by adding PsyNeuLink implementations of influential models of neural subsystems; and iii) developing a publicly available workbook of simulation exercises as both an introduction to PsyNeuLink and for use in Cognitive Neuroscience and Computational Psychiatry curricula. 2) Accelerate PsyNeuLink by developing a custom compiler that preserves its simplicity and flexibility, while dramatically increasing its speed, to make it suitable for simulation of large and complex system-level models, and for parameter estimation, model fitting, and model comparison. This project will exploit the power and accelerating use of Python, and modern just-in-time compilation methods to develop a tool designed specifically for the needs of systems-level Cognitive Neuroscience and Computational Psychiatry. This promises to open up new opportunities for research at the systems-level — a level of analysis that is crucial both for understanding how human mental function emerges from the interplay among neural subsystems, and how disturbances of individual neural subsystems impact this interplay, disruptions of which are almost certainly a critical factor in neurologic and psychiatric disorders. Project Narrative Paralleling the growth of neuroscience research has been an explosion in the development of computationally explicit models of the functions of core brain subsystems. Unfortunately, however, there has not been a commensurate development of the tools needed to share, validate, and compare such models, or integrate them into models of system-level function. This proposed project seeks to address this need by developing a standard software platform for the construction, documentation, sharing, and integration of computational models of brain function, that promises to accelerate the study of how system-level interactions give rise to mental function and, critically, the kinds of disruptions of such system-level interactions produced by disturbances of individual subsystems — disruptions that are sure to be a complex but critical factor in neurological and psychiatric disorders.",PsyNeuLink:  A Block Modeling Environment for Cognitive Neuroscience and Computational Psychiatry,9824928,R21MH117548,"['Acceleration', 'Address', 'Architecture', 'Attention', 'Basal Ganglia', 'Biological', 'Biological Models', 'Brain', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Custom', 'Data', 'Development', 'Documentation', 'Education', 'Educational Curriculum', 'Environment', 'Episodic memory', 'Evaluation', 'Exercise', 'Explosion', 'Foundations', 'Goals', 'Grain', 'Growth', 'Hippocampus (Brain)', 'Human', 'Individual', 'Influentials', 'Libraries', 'Literature', 'Maintenance', 'Manuals', 'Mental disorders', 'Methods', 'Modeling', 'Modernization', 'Neurosciences Research', 'Perceptual learning', 'Play', 'Prefrontal Cortex', 'Procedures', 'Psychiatry', 'Publications', 'Publishing', 'Pythons', 'Research', 'Role', 'Seeds', 'Short-Term Memory', 'Speed', 'Statistical Data Interpretation', 'System', 'Time', 'Writing', 'base', 'cognitive function', 'cognitive neuroscience', 'cognitive process', 'deep learning', 'deep neural network', 'design', 'flexibility', 'head-to-head comparison', 'improved', 'learning network', 'memory encoding', 'memory retrieval', 'mental function', 'nervous system disorder', 'neural model', 'open source', 'parallelization', 'preservation', 'programs', 'relating to nervous system', 'repository', 'simulation', 'tool', 'tool development']",NIMH,PRINCETON UNIVERSITY,R21,2019,253798,-0.06875246004759648
"Mechanisms of attentional control: Structure and dynamics from simultaneous EEG-fMRI and machine learning PROJECT SUMMARY/ABSTRACT Selective attention is an essential cognitive ability that permits us to effectively process and act upon relevant information while ignoring distracting events. A network involving frontal and parietal cortex for top-down attentional control, referred to as the Dorsal Attention Network (DAN), is active during both spatial and non- spatial (feature-based) attention. However, we know very little about the fine structure of attentional control activity in the DAN, how this structure changes to represent different to-be-attended stimulus features, how the connectivity within the DAN, and between the DAN and sensory cortex shifts when attending different features, or how these top-down processes and their influence in sensory cortex unfold over time. This gap in our knowledge is a critical problem for our models and theories of attention, and because attentional deficits are involved in a wide variety of neuropsychiatric disorders including autism, attention deficit disorder, dementia, and schizophrenia. The working model guiding this research is that top-down attentional control, based on different to-be-attended stimulus attributes, is guided by a smaller-scale neural fine structure within the DAN and prefrontal cortex that makes specific connections with specialized areas of visual cortex coding the attended attributes. Moreover, the time course of activity within the DAN in relation to that in sensory cortex follows a top-down cascading model, being earliest in frontal, then parietal cortex, and finally sensory cortex for preparatory, voluntary, attentional control. To identify the functional networks for attentional control for different forms of attention, and to define their time courses, this project uses innovative simultaneous recording of electroencephalographic (EEG) and functional magnetic resonance imaging (fMRI) data. Advanced signal processing and modeling, including multivariate pattern analysis (MVPA), graph theoretic connectivity analysis, and Granger causality analysis will be used to reveal the fine functional anatomy and time course of attentional control and selection. The project includes three experiments that vary the to-be-attended stimulus attributes from spatial location to stimulus features (color and motion), and pursues three aims. Aim 1 is to reveal the fine structure of top-down preparatory attentional control for different to-be-attended stimulus features. Aim 2 is to elucidate the specific connectivity between fine structures for preparatory attentional control in the DAN and their target sensory structures in sensory cortex. Aim 3 is to reveal the time course of top-down attentional control for different to-be-attended stimulus attributes. PROJECT NARRATIVE The capacity to focus attention is at the core of human mental functioning, and therefore, elucidating the neural bases of attention remains a central challenge for neuroscience, representing an essential aim in translational efforts to ameliorate attentional deficits in a wide variety of neuropsychiatric disorders including autism, attention deficit disorder, dementia, and schizophrenia.",Mechanisms of attentional control: Structure and dynamics from simultaneous EEG-fMRI and machine learning,9878145,R01MH117991,"['Anatomy', 'Attention', 'Attention Deficit Disorder', 'Attentional deficit', 'Brain', 'Code', 'Color', 'Cues', 'Data', 'Dementia', 'Discrimination', 'Dorsal', 'Etiology', 'Event', 'Face', 'Functional Magnetic Resonance Imaging', 'Graph', 'Human', 'Inferior', 'Knowledge', 'Location', 'Machine Learning', 'Modeling', 'Motion', 'Neurosciences', 'Parietal Lobe', 'Pattern', 'Perception', 'Physiological', 'Prefrontal Cortex', 'Process', 'Property', 'Research', 'Schizophrenia', 'Sensory', 'Specific qualifier value', 'Specificity', 'Stimulus', 'Structure', 'Testing', 'Time', 'Visual', 'Visual Cortex', 'Work', 'attentional control', 'autism spectrum disorder', 'base', 'cognitive ability', 'data integration', 'experimental study', 'extrastriate visual cortex', 'frontal lobe', 'indexing', 'innovation', 'mental function', 'neuropsychiatric disorder', 'predictive modeling', 'relating to nervous system', 'selective attention', 'sensory cortex', 'signal processing', 'theories', 'tool']",NIMH,UNIVERSITY OF CALIFORNIA AT DAVIS,R01,2020,530307,-0.06343708439222229
"Mechanisms of attentional control: Structure and dynamics from simultaneous EEG-fMRI and machine learning PROJECT SUMMARY/ABSTRACT Selective attention is an essential cognitive ability that permits us to effectively process and act upon relevant information while ignoring distracting events. A network involving frontal and parietal cortex for top-down attentional control, referred to as the Dorsal Attention Network (DAN), is active during both spatial and non- spatial (feature-based) attention. However, we know very little about the fine structure of attentional control activity in the DAN, how this structure changes to represent different to-be-attended stimulus features, how the connectivity within the DAN, and between the DAN and sensory cortex shifts when attending different features, or how these top-down processes and their influence in sensory cortex unfold over time. This gap in our knowledge is a critical problem for our models and theories of attention, and because attentional deficits are involved in a wide variety of neuropsychiatric disorders including autism, attention deficit disorder, dementia, and schizophrenia. The working model guiding this research is that top-down attentional control, based on different to-be-attended stimulus attributes, is guided by a smaller-scale neural fine structure within the DAN and prefrontal cortex that makes specific connections with specialized areas of visual cortex coding the attended attributes. Moreover, the time course of activity within the DAN in relation to that in sensory cortex follows a top-down cascading model, being earliest in frontal, then parietal cortex, and finally sensory cortex for preparatory, voluntary, attentional control. To identify the functional networks for attentional control for different forms of attention, and to define their time courses, this project uses innovative simultaneous recording of electroencephalographic (EEG) and functional magnetic resonance imaging (fMRI) data. Advanced signal processing and modeling, including multivariate pattern analysis (MVPA), graph theoretic connectivity analysis, and Granger causality analysis will be used to reveal the fine functional anatomy and time course of attentional control and selection. The project includes three experiments that vary the to-be-attended stimulus attributes from spatial location to stimulus features (color and motion), and pursues three aims. Aim 1 is to reveal the fine structure of top-down preparatory attentional control for different to-be-attended stimulus features. Aim 2 is to elucidate the specific connectivity between fine structures for preparatory attentional control in the DAN and their target sensory structures in sensory cortex. Aim 3 is to reveal the time course of top-down attentional control for different to-be-attended stimulus attributes. PROJECT NARRATIVE The capacity to focus attention is at the core of human mental functioning, and therefore, elucidating the neural bases of attention remains a central challenge for neuroscience, representing an essential aim in translational efforts to ameliorate attentional deficits in a wide variety of neuropsychiatric disorders including autism, attention deficit disorder, dementia, and schizophrenia.",Mechanisms of attentional control: Structure and dynamics from simultaneous EEG-fMRI and machine learning,9718308,R01MH117991,"['Anatomy', 'Attention', 'Attention Deficit Disorder', 'Attentional deficit', 'Brain', 'Code', 'Color', 'Cues', 'Data', 'Dementia', 'Discrimination', 'Dorsal', 'Etiology', 'Event', 'Face', 'Functional Magnetic Resonance Imaging', 'Graph', 'Human', 'Inferior', 'Knowledge', 'Location', 'Machine Learning', 'Modeling', 'Motion', 'Neurosciences', 'Parietal Lobe', 'Pattern', 'Perception', 'Physiological', 'Prefrontal Cortex', 'Process', 'Property', 'Research', 'Schizophrenia', 'Sensory', 'Specific qualifier value', 'Specificity', 'Stimulus', 'Structure', 'Testing', 'Time', 'Visual', 'Visual Cortex', 'Work', 'attentional control', 'autism spectrum disorder', 'base', 'cognitive ability', 'data integration', 'experimental study', 'extrastriate visual cortex', 'frontal lobe', 'indexing', 'innovation', 'mental function', 'neuropsychiatric disorder', 'predictive modeling', 'relating to nervous system', 'selective attention', 'sensory cortex', 'signal processing', 'theories', 'tool']",NIMH,UNIVERSITY OF CALIFORNIA AT DAVIS,R01,2019,530307,-0.06343708439222229
"Mechanisms of attentional control: Structure and dynamics from simultaneous EEG-fMRI and machine learning PROJECT SUMMARY/ABSTRACT Selective attention is an essential cognitive ability that permits us to effectively process and act upon relevant information while ignoring distracting events. A network involving frontal and parietal cortex for top-down attentional control, referred to as the Dorsal Attention Network (DAN), is active during both spatial and non- spatial (feature-based) attention. However, we know very little about the fine structure of attentional control activity in the DAN, how this structure changes to represent different to-be-attended stimulus features, how the connectivity within the DAN, and between the DAN and sensory cortex shifts when attending different features, or how these top-down processes and their influence in sensory cortex unfold over time. This gap in our knowledge is a critical problem for our models and theories of attention, and because attentional deficits are involved in a wide variety of neuropsychiatric disorders including autism, attention deficit disorder, dementia, and schizophrenia. The working model guiding this research is that top-down attentional control, based on different to-be-attended stimulus attributes, is guided by a smaller-scale neural fine structure within the DAN and prefrontal cortex that makes specific connections with specialized areas of visual cortex coding the attended attributes. Moreover, the time course of activity within the DAN in relation to that in sensory cortex follows a top-down cascading model, being earliest in frontal, then parietal cortex, and finally sensory cortex for preparatory, voluntary, attentional control. To identify the functional networks for attentional control for different forms of attention, and to define their time courses, this project uses innovative simultaneous recording of electroencephalographic (EEG) and functional magnetic resonance imaging (fMRI) data. Advanced signal processing and modeling, including multivariate pattern analysis (MVPA), graph theoretic connectivity analysis, and Granger causality analysis will be used to reveal the fine functional anatomy and time course of attentional control and selection. The project includes three experiments that vary the to-be-attended stimulus attributes from spatial location to stimulus features (color and motion), and pursues three aims. Aim 1 is to reveal the fine structure of top-down preparatory attentional control for different to-be-attended stimulus features. Aim 2 is to elucidate the specific connectivity between fine structures for preparatory attentional control in the DAN and their target sensory structures in sensory cortex. Aim 3 is to reveal the time course of top-down attentional control for different to-be-attended stimulus attributes. PROJECT NARRATIVE The capacity to focus attention is at the core of human mental functioning, and therefore, elucidating the neural bases of attention remains a central challenge for neuroscience, representing an essential aim in translational efforts to ameliorate attentional deficits in a wide variety of neuropsychiatric disorders including autism, attention deficit disorder, dementia, and schizophrenia.",Mechanisms of attentional control: Structure and dynamics from simultaneous EEG-fMRI and machine learning,9616747,R01MH117991,"['Anatomy', 'Attention', 'Attention Deficit Disorder', 'Attentional deficit', 'Autistic Disorder', 'Brain', 'Code', 'Color', 'Cues', 'Data', 'Dementia', 'Discrimination', 'Dorsal', 'Etiology', 'Event', 'Face', 'Functional Magnetic Resonance Imaging', 'Graph', 'Human', 'Inferior', 'Knowledge', 'Location', 'Machine Learning', 'Modeling', 'Motion', 'Neurosciences', 'Parietal Lobe', 'Pattern', 'Perception', 'Physiological', 'Prefrontal Cortex', 'Process', 'Property', 'Research', 'Schizophrenia', 'Sensory', 'Specific qualifier value', 'Specificity', 'Stimulus', 'Structure', 'Testing', 'Time', 'Visual', 'Visual Cortex', 'Work', 'attentional control', 'base', 'cognitive ability', 'data integration', 'experimental study', 'extrastriate visual cortex', 'frontal lobe', 'indexing', 'innovation', 'mental function', 'neuropsychiatric disorder', 'predictive modeling', 'relating to nervous system', 'selective attention', 'sensory cortex', 'signal processing', 'theories', 'tool']",NIMH,UNIVERSITY OF CALIFORNIA AT DAVIS,R01,2018,567754,-0.06343708439222229
"High-throughput high-content single cell analysis by multichannel stimulatedRaman flow cytometry ﻿    DESCRIPTION (provided by applicant): Flow cytometry is one of the most important tools for high-throughput single cell analysis. Fluorescent labeling acts as the primary approach for cellular analysis in flow cytometry. Nevertheless, fluorescent tags are not applicable to all cases especially small molecules (e.g. metabolites) for which labeling may significantly perturb their properties. Raman spectroscopic signals arising from inherent molecular vibrations provide a key approach to detect specific molecules inside cells and to differentiate cellular state. Raman-based microfluidic devices have been reported. However, the very small cross section of spontaneous Raman scattering results in low Raman signal level and consequently long data acquisition time, which is not compatible with the high- speed flow condition. The long-term goal of the proposed project is to establish a high-throughput high-content single cell analysis platform using molecular fingerprint vibrations as contrast. The specific objective of current application is to develop a vibrational spectroscopic cytometer based on the stimulated Raman scattering (SRS) process. Several recent advances in the Ji-Xin Cheng (PI) lab, including the highly sensitive femtosecond SRS imaging, lock-in free SRS signal detection and a tuned amplifier array for multiplex SRS imaging, pave the foundation for the planned instrumentation. The PI has assembled an interdisciplinary team for the proposed study. Dr. J. Paul Robinson (co-PI) is a leader in development and applications of fluorescence-based flow cytometer and he will bring expertise to the design of fluidics and multichannel detection systems. Dr. Bartek Rajwa (co-PI) will provide expertise for spectroscopic cytometry data analysis and machine learning. The team will design and construct a SRS flow cytometer by multichannel detection of dispersed SRS signal (Aim 1), construct a tandem system able to collect SRS and fluorescence data (Aim 2), develop spectral un-mixing and machine-learning analysis tools able to combine the information obtained from SRS spectra and labeled biomarkers for functional classification of cells (Aim 3), and validate the capability of SRS flow cytometer for label-free detection of single-cell metabolism (Aim 4). With a speed of analyzing thousands of cells per second, SRS flow cytometer will enable high-throughput analysis of single-cell chemical content which is beyond the reach by fluorescence-based flow cytometer. PUBLIC HEALTH RELEVANCE: We propose to build a high-throughput single cell analysis platform through multiplex stimulated Raman scattering detection of single flowing cells at microsecond time scale. Having fluorescence and stimulated Raman scattering detection in tandem, our platform is capable of discovering new metabolic signatures of cell subpopulations (e.g. cancer stem cells sorted through fluorescent markers). Such discovery could potentially lead to new development of disease-specific treatment strategies.",High-throughput high-content single cell analysis by multichannel stimulatedRaman flow cytometry,9692022,R01GM118471,"['Amplifiers', 'Biological Markers', 'Cells', 'Cellular Metabolic Process', 'Characteristics', 'Chemicals', 'Classification', 'Cytometry', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Energy Transfer', 'Event', 'Flow Cytometry', 'Fluorescence', 'Foundations', 'Geometry', 'Glucose', 'Goals', 'Image', 'Label', 'Lead', 'Light', 'Liquid substance', 'Machine Learning', 'Malignant neoplasm of ovary', 'Mammalian Cell', 'Measurement', 'Measures', 'Metabolic', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nanostructures', 'Optics', 'Phase', 'Polymers', 'Principal Component Analysis', 'Process', 'Property', 'Quartz', 'Reporting', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Stream', 'Surface', 'System', 'Techniques', 'Time', 'Tube', 'base', 'biological systems', 'cancer cell', 'cancer stem cell', 'data acquisition', 'design', 'design and construction', 'detector', 'electric impedance', 'high throughput analysis', 'instrumentation', 'light scattering', 'novel strategies', 'particle', 'photomultiplier', 'public health relevance', 'single cell analysis', 'small molecule', 'spectroscopic data', 'tool', 'treatment strategy', 'vibration']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2019,388535,-0.0408126189387815
"High-throughput high-content single cell analysis by multichannel stimulatedRaman flow cytometry ﻿    DESCRIPTION (provided by applicant): Flow cytometry is one of the most important tools for high-throughput single cell analysis. Fluorescent labeling acts as the primary approach for cellular analysis in flow cytometry. Nevertheless, fluorescent tags are not applicable to all cases especially small molecules (e.g. metabolites) for which labeling may significantly perturb their properties. Raman spectroscopic signals arising from inherent molecular vibrations provide a key approach to detect specific molecules inside cells and to differentiate cellular state. Raman-based microfluidic devices have been reported. However, the very small cross section of spontaneous Raman scattering results in low Raman signal level and consequently long data acquisition time, which is not compatible with the high- speed flow condition. The long-term goal of the proposed project is to establish a high-throughput high-content single cell analysis platform using molecular fingerprint vibrations as contrast. The specific objective of current application is to develop a vibrational spectroscopic cytometer based on the stimulated Raman scattering (SRS) process. Several recent advances in the Ji-Xin Cheng (PI) lab, including the highly sensitive femtosecond SRS imaging, lock-in free SRS signal detection and a tuned amplifier array for multiplex SRS imaging, pave the foundation for the planned instrumentation. The PI has assembled an interdisciplinary team for the proposed study. Dr. J. Paul Robinson (co-PI) is a leader in development and applications of fluorescence-based flow cytometer and he will bring expertise to the design of fluidics and multichannel detection systems. Dr. Bartek Rajwa (co-PI) will provide expertise for spectroscopic cytometry data analysis and machine learning. The team will design and construct a SRS flow cytometer by multichannel detection of dispersed SRS signal (Aim 1), construct a tandem system able to collect SRS and fluorescence data (Aim 2), develop spectral un-mixing and machine-learning analysis tools able to combine the information obtained from SRS spectra and labeled biomarkers for functional classification of cells (Aim 3), and validate the capability of SRS flow cytometer for label-free detection of single-cell metabolism (Aim 4). With a speed of analyzing thousands of cells per second, SRS flow cytometer will enable high-throughput analysis of single-cell chemical content which is beyond the reach by fluorescence-based flow cytometer. PUBLIC HEALTH RELEVANCE: We propose to build a high-throughput single cell analysis platform through multiplex stimulated Raman scattering detection of single flowing cells at microsecond time scale. Having fluorescence and stimulated Raman scattering detection in tandem, our platform is capable of discovering new metabolic signatures of cell subpopulations (e.g. cancer stem cells sorted through fluorescent markers). Such discovery could potentially lead to new development of disease-specific treatment strategies.",High-throughput high-content single cell analysis by multichannel stimulatedRaman flow cytometry,9525343,R01GM118471,"['Amplifiers', 'Biological Markers', 'Cells', 'Cellular Metabolic Process', 'Characteristics', 'Chemicals', 'Classification', 'Cytometry', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Energy Transfer', 'Event', 'Flow Cytometry', 'Fluorescence', 'Foundations', 'Geometry', 'Glucose', 'Goals', 'Image', 'Label', 'Lead', 'Light', 'Liquid substance', 'Machine Learning', 'Malignant neoplasm of ovary', 'Mammalian Cell', 'Measurement', 'Measures', 'Metabolic', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nanostructures', 'Optics', 'Phase', 'Polymers', 'Principal Component Analysis', 'Process', 'Property', 'Quartz', 'Reporting', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Stream', 'Surface', 'System', 'Techniques', 'Time', 'Tube', 'base', 'biological systems', 'cancer cell', 'cancer stem cell', 'data acquisition', 'design', 'design and construction', 'detector', 'electric impedance', 'high throughput analysis', 'instrumentation', 'light scattering', 'novel strategies', 'particle', 'photomultiplier', 'public health relevance', 'single cell analysis', 'small molecule', 'spectroscopic data', 'tool', 'treatment strategy', 'vibration']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2018,388379,-0.0408126189387815
"High-throughput high-content single cell analysis by multichannel stimulatedRaman flow cytometry ﻿    DESCRIPTION (provided by applicant): Flow cytometry is one of the most important tools for high-throughput single cell analysis. Fluorescent labeling acts as the primary approach for cellular analysis in flow cytometry. Nevertheless, fluorescent tags are not applicable to all cases especially small molecules (e.g. metabolites) for which labeling may significantly perturb their properties. Raman spectroscopic signals arising from inherent molecular vibrations provide a key approach to detect specific molecules inside cells and to differentiate cellular state. Raman-based microfluidic devices have been reported. However, the very small cross section of spontaneous Raman scattering results in low Raman signal level and consequently long data acquisition time, which is not compatible with the high- speed flow condition. The long-term goal of the proposed project is to establish a high-throughput high-content single cell analysis platform using molecular fingerprint vibrations as contrast. The specific objective of current application is to develop a vibrational spectroscopic cytometer based on the stimulated Raman scattering (SRS) process. Several recent advances in the Ji-Xin Cheng (PI) lab, including the highly sensitive femtosecond SRS imaging, lock-in free SRS signal detection and a tuned amplifier array for multiplex SRS imaging, pave the foundation for the planned instrumentation. The PI has assembled an interdisciplinary team for the proposed study. Dr. J. Paul Robinson (co-PI) is a leader in development and applications of fluorescence-based flow cytometer and he will bring expertise to the design of fluidics and multichannel detection systems. Dr. Bartek Rajwa (co-PI) will provide expertise for spectroscopic cytometry data analysis and machine learning. The team will design and construct a SRS flow cytometer by multichannel detection of dispersed SRS signal (Aim 1), construct a tandem system able to collect SRS and fluorescence data (Aim 2), develop spectral un-mixing and machine-learning analysis tools able to combine the information obtained from SRS spectra and labeled biomarkers for functional classification of cells (Aim 3), and validate the capability of SRS flow cytometer for label-free detection of single-cell metabolism (Aim 4). With a speed of analyzing thousands of cells per second, SRS flow cytometer will enable high-throughput analysis of single-cell chemical content which is beyond the reach by fluorescence-based flow cytometer. PUBLIC HEALTH RELEVANCE: We propose to build a high-throughput single cell analysis platform through multiplex stimulated Raman scattering detection of single flowing cells at microsecond time scale. Having fluorescence and stimulated Raman scattering detection in tandem, our platform is capable of discovering new metabolic signatures of cell subpopulations (e.g. cancer stem cells sorted through fluorescent markers). Such discovery could potentially lead to new development of disease-specific treatment strategies.",High-throughput high-content single cell analysis by multichannel stimulatedRaman flow cytometry,9320713,R01GM118471,"['Amplifiers', 'Biological Markers', 'Cells', 'Cellular Metabolic Process', 'Characteristics', 'Chemicals', 'Classification', 'Cytometry', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Energy Transfer', 'Event', 'Flow Cytometry', 'Fluorescence', 'Foundations', 'Geometry', 'Glucose', 'Goals', 'Image', 'Label', 'Lead', 'Light', 'Liquid substance', 'Machine Learning', 'Malignant neoplasm of ovary', 'Mammalian Cell', 'Measurement', 'Measures', 'Metabolic', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nanostructures', 'Optics', 'Phase', 'Polymers', 'Principal Component Analysis', 'Process', 'Property', 'Quartz', 'Reporting', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Stream', 'Surface', 'System', 'Techniques', 'Time', 'Tube', 'base', 'biological systems', 'cancer cell', 'cancer stem cell', 'data acquisition', 'design', 'design and construction', 'detector', 'electric impedance', 'high throughput analysis', 'instrumentation', 'light scattering', 'novel strategies', 'particle', 'photomultiplier', 'public health relevance', 'single cell analysis', 'small molecule', 'spectroscopic data', 'tool', 'treatment strategy', 'vibration']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2017,403725,-0.0408126189387815
"High-throughput high-content single cell analysis by multichannel stimulatedRaman flow cytometry ﻿    DESCRIPTION (provided by applicant): Flow cytometry is one of the most important tools for high-throughput single cell analysis. Fluorescent labeling acts as the primary approach for cellular analysis in flow cytometry. Nevertheless, fluorescent tags are not applicable to all cases especially small molecules (e.g. metabolites) for which labeling may significantly perturb their properties. Raman spectroscopic signals arising from inherent molecular vibrations provide a key approach to detect specific molecules inside cells and to differentiate cellular state. Raman-based microfluidic devices have been reported. However, the very small cross section of spontaneous Raman scattering results in low Raman signal level and consequently long data acquisition time, which is not compatible with the high- speed flow condition. The long-term goal of the proposed project is to establish a high-throughput high-content single cell analysis platform using molecular fingerprint vibrations as contrast. The specific objective of current application is to develop a vibrational spectroscopic cytometer based on the stimulated Raman scattering (SRS) process. Several recent advances in the Ji-Xin Cheng (PI) lab, including the highly sensitive femtosecond SRS imaging, lock-in free SRS signal detection and a tuned amplifier array for multiplex SRS imaging, pave the foundation for the planned instrumentation. The PI has assembled an interdisciplinary team for the proposed study. Dr. J. Paul Robinson (co-PI) is a leader in development and applications of fluorescence-based flow cytometer and he will bring expertise to the design of fluidics and multichannel detection systems. Dr. Bartek Rajwa (co-PI) will provide expertise for spectroscopic cytometry data analysis and machine learning. The team will design and construct a SRS flow cytometer by multichannel detection of dispersed SRS signal (Aim 1), construct a tandem system able to collect SRS and fluorescence data (Aim 2), develop spectral un-mixing and machine-learning analysis tools able to combine the information obtained from SRS spectra and labeled biomarkers for functional classification of cells (Aim 3), and validate the capability of SRS flow cytometer for label-free detection of single-cell metabolism (Aim 4). With a speed of analyzing thousands of cells per second, SRS flow cytometer will enable high-throughput analysis of single-cell chemical content which is beyond the reach by fluorescence-based flow cytometer.         PUBLIC HEALTH RELEVANCE: We propose to build a high-throughput single cell analysis platform through multiplex stimulated Raman scattering detection of single flowing cells at microsecond time scale. Having fluorescence and stimulated Raman scattering detection in tandem, our platform is capable of discovering new metabolic signatures of cell subpopulations (e.g. cancer stem cells sorted through fluorescent markers). Such discovery could potentially lead to new development of disease-specific treatment strategies.        ",High-throughput high-content single cell analysis by multichannel stimulatedRaman flow cytometry,9080637,R01GM118471,"['Amplifiers', 'Biological Markers', 'Cell Separation', 'Cells', 'Cellular Metabolic Process', 'Characteristics', 'Chemicals', 'Classification', 'Cytometry', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Energy Transfer', 'Event', 'Flow Cytometry', 'Fluorescence', 'Foundations', 'Geometry', 'Glucose', 'Goals', 'Image', 'Label', 'Lead', 'Light', 'Machine Learning', 'Malignant neoplasm of ovary', 'Mammalian Cell', 'Measurement', 'Measures', 'Metabolic', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nanostructures', 'Optics', 'Phase', 'Polymers', 'Population', 'Principal Component Analysis', 'Process', 'Property', 'Quartz', 'Reporting', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Stream', 'Surface', 'System', 'Techniques', 'Time', 'Tube', 'base', 'biological systems', 'cancer cell', 'cancer stem cell', 'data acquisition', 'design', 'design and construction', 'detector', 'electric impedance', 'high throughput analysis', 'instrumentation', 'light scattering', 'novel strategies', 'particle', 'photomultiplier', 'public health relevance', 'single cell analysis', 'small molecule', 'spectroscopic imaging', 'tool', 'treatment strategy', 'vibration']",NIGMS,PURDUE UNIVERSITY,R01,2016,428677,-0.0408126189387815
"Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data This project aims to develop NeuroManager™, an innovative neuroinformatics platform for advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data. A core technology that we will develop in NeuroManager will be Image Content Analysis for Retrieval Using Semantics (ICARUS), a novel, intelligent neuroimage curation system that will enable image retrieval based on visual appearance or by semantic concept. ICARUS will use machine learning applied to content-based image retrieval - (CBIR) to build and refine models that summarize microscopic and macroscopic image appearance and automatically assign semantic concepts to neuroimages. Neuroscience research generates extensive, multifaceted data that is considerably under-utilized because access to original raw data is typically maintained by the source lab. On the other hand, there are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results. Unfortunately, none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. To solve this problem, NeuroManager will include the following distinct, significant innovations: (i) versatility for handling two-dimensional (2D) and three-dimensional neuroimaging data sets from animal models and humans; (ii) functionality to share complex datasets that extends secure, privacy-controlled paradigms from institutional, laboratory-based and even public domains; (iii) flexibility to implement NeuroManager within an institute’s IT infrastructure, or on most cloud-based virtualized environments including Azure, Google Cloud Services and Amazon Web Services; (iv) and most importantly, the ICARUS technology for CBIR in neuroimaging data sets. The benefit of NeuroManager for the neuroscience research community, pharmacological and biotechnological R&D, and society in general will be to foster collaboration between scientists and institutions, promoting innovation through combined expertise in an interdisciplinary atmosphere. This will open new horizons for better understanding the neuropathology associated with several human neuropsychiatric and neurological conditions at various levels (i.e., macroscopically, microscopically, subcellularly and functionally), ultimately leading to an improved basis for developing novel treatment and prevention strategies for complex brain diseases. In Phase I we will prove feasibility of this novel technology by developing prototype software that will perform CBIR on 2D whole slide images of coronal sections of entire mouse brains from ongoing research projects of our collaborators. Work in Phase II will focus on developing the commercial software product that will include all of the innovations mentioned above. A competing technology with comparable functionality, addressing the full breadth of needs for modern neuroscience research, is currently not available commercially or otherwise. There are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results; however none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. This project commercializes an innovative software for sophisticated advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data, including a novel, intelligent neuroimage curation system that will enable content-based neuroscience image search powered by machine learning, thereby opening new horizons in neuroscience research collaborations. This system will allow researchers to make new discoveries based on new studies that are currently not feasible, ultimately providing the basis for developing novel treatments to prevent and fight complex brain diseases.",Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data,9989186,R44MH118815,"['3-Dimensional', 'Address', 'Amygdaloid structure', 'Animal Model', 'Appearance', 'Archives', 'Biotechnology', 'Brain', 'Brain Diseases', 'Brain imaging', 'Chicago', 'Cloud Service', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Aggregation', 'Data Files', 'Data Provenance', 'Data Set', 'Data Sources', 'Digital Imaging and Communications in Medicine', 'Dimensions', 'Fostering', 'Human', 'Image', 'Information Systems', 'Infrastructure', 'Institutes', 'Institution', 'Intelligence', 'Laboratories', 'Machine Learning', 'Manuals', 'Microscopic', 'Modeling', 'Modernization', 'Mus', 'National Institute of Mental Health', 'Neurologic', 'Neurosciences', 'Neurosciences Research', 'New York', 'Notification', 'Pharmacology', 'Phase', 'Prevention strategy', 'Privacy', 'Problem Solving', 'Production', 'Public Domains', 'Records', 'Regenerative Medicine', 'Reproducibility', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Retrieval', 'Schools', 'Scientist', 'Secure', 'Semantics', 'Societies', 'Source', 'System', 'Technology', 'Testing', 'Universities', 'Validation', 'Visual', 'Work', 'application programming interface', 'base', 'cloud based', 'collaborative environment', 'data access', 'data format', 'data sharing', 'data warehouse', 'fighting', 'flexibility', 'hands-on learning', 'improved', 'innovation', 'interest', 'neuroimaging', 'neuroinformatics', 'neuropathology', 'neuropsychiatry', 'new technology', 'novel', 'prevent', 'prototype', 'research and development', 'stem cells', 'treatment strategy', 'two-dimensional', 'usability', 'virtual environment', 'web services', 'whole slide imaging']",NIMH,"MICROBRIGHTFIELD, LLC",R44,2020,749716,-0.0308415441951564
"Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data This project aims to develop NeuroManager™, an innovative neuroinformatics platform for advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data. A core technology that we will develop in NeuroManager will be Image Content Analysis for Retrieval Using Semantics (ICARUS), a novel, intelligent neuroimage curation system that will enable image retrieval based on visual appearance or by semantic concept. ICARUS will use machine learning applied to content-based image retrieval - (CBIR) to build and refine models that summarize microscopic and macroscopic image appearance and automatically assign semantic concepts to neuroimages. Neuroscience research generates extensive, multifaceted data that is considerably under-utilized because access to original raw data is typically maintained by the source lab. On the other hand, there are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results. Unfortunately, none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. To solve this problem, NeuroManager will include the following distinct, significant innovations: (i) versatility for handling two-dimensional (2D) and three-dimensional neuroimaging data sets from animal models and humans; (ii) functionality to share complex datasets that extends secure, privacy-controlled paradigms from institutional, laboratory-based and even public domains; (iii) flexibility to implement NeuroManager within an institute’s IT infrastructure, or on most cloud-based virtualized environments including Azure, Google Cloud Services and Amazon Web Services; (iv) and most importantly, the ICARUS technology for CBIR in neuroimaging data sets. The benefit of NeuroManager for the neuroscience research community, pharmacological and biotechnological R&D, and society in general will be to foster collaboration between scientists and institutions, promoting innovation through combined expertise in an interdisciplinary atmosphere. This will open new horizons for better understanding the neuropathology associated with several human neuropsychiatric and neurological conditions at various levels (i.e., macroscopically, microscopically, subcellularly and functionally), ultimately leading to an improved basis for developing novel treatment and prevention strategies for complex brain diseases. In Phase I we will prove feasibility of this novel technology by developing prototype software that will perform CBIR on 2D whole slide images of coronal sections of entire mouse brains from ongoing research projects of our collaborators. Work in Phase II will focus on developing the commercial software product that will include all of the innovations mentioned above. A competing technology with comparable functionality, addressing the full breadth of needs for modern neuroscience research, is currently not available commercially or otherwise. There are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results; however none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. This project commercializes an innovative software for sophisticated advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data, including a novel, intelligent neuroimage curation system that will enable content-based neuroscience image search powered by machine learning, thereby opening new horizons in neuroscience research collaborations. This system will allow researchers to make new discoveries based on new studies that are currently not feasible, ultimately providing the basis for developing novel treatments to prevent and fight complex brain diseases.",Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data,9797689,R44MH118815,"['Address', 'Amygdaloid structure', 'Animal Model', 'Appearance', 'Archives', 'Biotechnology', 'Brain', 'Brain Diseases', 'Brain imaging', 'Chicago', 'Cloud Service', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Aggregation', 'Data Files', 'Data Provenance', 'Data Set', 'Data Sources', 'Digital Imaging and Communications in Medicine', 'Dimensions', 'Fostering', 'Human', 'Image', 'Information Systems', 'Infrastructure', 'Institutes', 'Institution', 'Intelligence', 'Laboratories', 'Machine Learning', 'Manuals', 'Microscopic', 'Modeling', 'Modernization', 'Mus', 'National Institute of Mental Health', 'Neurologic', 'Neurosciences', 'Neurosciences Research', 'New York', 'Notification', 'Pharmacology', 'Phase', 'Prevention strategy', 'Privacy', 'Problem Solving', 'Production', 'Public Domains', 'Records', 'Regenerative Medicine', 'Reproducibility', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Retrieval', 'Schools', 'Scientist', 'Secure', 'Semantics', 'Societies', 'Source', 'Stem cells', 'System', 'Technology', 'Testing', 'Universities', 'Validation', 'Visual', 'Work', 'application programming interface', 'base', 'cloud based', 'collaborative environment', 'data access', 'data format', 'data sharing', 'data warehouse', 'fighting', 'flexibility', 'hands-on learning', 'improved', 'innovation', 'interest', 'neuroimaging', 'neuroinformatics', 'neuropathology', 'neuropsychiatry', 'new technology', 'novel', 'prevent', 'prototype', 'research and development', 'treatment strategy', 'two-dimensional', 'usability', 'virtual reality', 'web services', 'whole slide imaging']",NIMH,"MICROBRIGHTFIELD, LLC",R44,2019,748584,-0.0308415441951564
"Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data This project aims to develop NeuroManager™, an innovative neuroinformatics platform for advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data. A core technology that we will develop in NeuroManager will be Image Content Analysis for Retrieval Using Semantics (ICARUS), a novel, intelligent neuroimage curation system that will enable image retrieval based on visual appearance or by semantic concept. ICARUS will use machine learning applied to content-based image retrieval (CBIR) to build and refine models that summarize microscopic and macroscopic image appearance and automatically assign semantic concepts to neuroimages. Neuroscience research generates extensive, multifaceted data that is considerably under-utilized because access to original raw data is typically maintained by the source lab. On the other hand, there are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results. Unfortunately, none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. To solve this problem, NeuroManager will include the following distinct, significant innovations: (i) versatility for handling two-dimensional (2D) and three-dimensional neuroimaging data sets from animal models and humans; (ii) functionality to share complex datasets that extends secure, privacy-controlled paradigms from institutional, laboratory-based and even public domains; (iii) flexibility to implement NeuroManager within an institute’s IT infrastructure, or on most cloud-based virtualized environments including Azure, Google Cloud Services and Amazon Web Services; (iv) and most importantly, the ICARUS technology for CBIR in neuroimaging data sets. The benefit of NeuroManager for the neuroscience research community, pharmacological and biotechnological R&D, and society in general will be to foster collaboration between scientists and institutions, promoting innovation through combined expertise in an interdisciplinary atmosphere. This will open new horizons for better understanding the neuropathology associated with several human neuropsychiatric and neurological conditions at various levels (i.e., macroscopically, microscopically, subcellularly and functionally), ultimately leading to an improved basis for developing novel treatment and prevention strategies for complex brain diseases. In Phase I we will prove feasibility of this novel technology by developing prototype software that will perform CBIR on 2D whole slide images of coronal sections of entire mouse brains from ongoing research projects of our collaborators. Work in Phase II will focus on developing the commercial software product that will include all of the innovations mentioned above. A competing technology with comparable functionality, addressing the full breadth of needs for modern neuroscience research, is currently not available commercially or otherwise. Narrative There are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results; however none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. This project commercializes an innovative software for sophisticated advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data, including a novel, intelligent neuroimage curation system that will enable content-based neuroscience image search powered by machine learning, thereby opening new horizons in neuroscience research collaborations. This system will allow researchers to make new discoveries based on new studies that are currently not feasible, ultimately providing the basis for developing novel treatments to prevent and fight complex brain diseases.",Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data,9680657,R44MH118815,"['Address', 'Amygdaloid structure', 'Animal Model', 'Appearance', 'Archives', 'Biotechnology', 'Brain', 'Brain Diseases', 'Brain imaging', 'Chicago', 'Cloud Service', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Aggregation', 'Data Files', 'Data Provenance', 'Data Set', 'Data Sources', 'Dimensions', 'Fostering', 'Human', 'Image', 'Information Systems', 'Institutes', 'Institution', 'Laboratories', 'Machine Learning', 'Manuals', 'Microscopic', 'Modeling', 'Modernization', 'Mus', 'National Institute of Mental Health', 'Neurologic', 'Neurosciences', 'Neurosciences Research', 'New York', 'Notification', 'Pharmacology', 'Phase', 'Prevention strategy', 'Privacy', 'Problem Solving', 'Production', 'Public Domains', 'Records', 'Regenerative Medicine', 'Reproducibility', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Retrieval', 'Schools', 'Scientist', 'Secure', 'Semantics', 'Societies', 'Source', 'Stem cells', 'System', 'Technology', 'Testing', 'Universities', 'Validation', 'Visual', 'Work', 'application programming interface', 'base', 'cloud based', 'data access', 'data format', 'data sharing', 'data warehouse', 'fighting', 'flexibility', 'hands-on learning', 'improved', 'innovation', 'interest', 'neuroimaging', 'neuroinformatics', 'neuropathology', 'neuropsychiatry', 'new technology', 'novel', 'prevent', 'professional atmosphere', 'prototype', 'research and development', 'treatment strategy', 'two-dimensional', 'usability', 'virtual reality', 'web services', 'whole slide imaging']",NIMH,"MICROBRIGHTFIELD, LLC",R44,2018,449918,-0.0308415441951564
"Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets Project Summary  Morphometric analysis is a primary algorithmic tool to discover disease and drug related effects on brain anatomy. Neurological degeneration and disease manifest in subtle and varied changes in brain anatomy that can be non-local in nature and effect amounts of white and gray matter as well as relative positioning and shapes of local brain anatomy. State-of-the-art morphometry methods focus on local matter distribution or on shape variations of apriori selected anatomies but have difficulty in detecting global or regional deterioration of matter; an important effect in many neurodegenerative processes. The proposal team recently developed a morphometric analysis based on unbalanced optimal transport, called UTM, that promises to be capable to discover local and global alteration of matter without the need to apriori select an anatomical region of interest.  The goal of this proposal is to develop the UTM technology into a software tool for automated high-throughput screening of large neurological image data sets. A more sensitive automated morphometric analysis tool will help researchers to discover neurological effects related to disease and lead to more efficient screening for drug related effects. Project Narrative  Describing anatomical differences in neurological image data set is a key technology to non-invasively discover the effects of disease processes or drug treatments on brain anatomy. Current morphometric analysis focus on local matter composition and on the shape of a priori defined regions of interest. The goal of this proposal is to extend the capabilities of image based morphometric analysis to be able to discover regionally varying deterioration and alteration of matter without the need for fine-grained segmentations and a priori definitions of regions of interest.",Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets,10115288,R41MH118845,"['Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Brain', 'Calibration', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Computer software', 'Data Set', 'Databases', 'Dementia', 'Deterioration', 'Development', 'Diffuse', 'Disease', 'Drug Screening', 'Early Diagnosis', 'Foundations', 'Goals', 'Grain', 'Image', 'Image Analysis', 'Internet', 'Lead', 'Location', 'Machine Learning', 'Medical Imaging', 'Methodology', 'Methods', 'Modality', 'Nature', 'Nerve Degeneration', 'Neurologic', 'Neurologic Effect', 'Online Systems', 'Outcome', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Population Study', 'Positioning Attribute', 'Positron-Emission Tomography', 'Process', 'Research', 'Research Personnel', 'Services', 'Shapes', 'Software Tools', 'Structure', 'Technology', 'Temporal Lobe', 'Testing', 'Validation', 'Variant', 'Visualization', 'autism spectrum disorder', 'base', 'clinical Diagnosis', 'experience', 'frontal lobe', 'gray matter', 'high throughput screening', 'image processing', 'image registration', 'imaging capabilities', 'improved', 'interest', 'machine learning method', 'morphometry', 'nervous system disorder', 'predict clinical outcome', 'predictive modeling', 'programs', 'research and development', 'shape analysis', 'software development', 'task analysis', 'tool', 'web services', 'white matter']",NIMH,"KITWARE, INC.",R41,2020,99860,-0.027092731348949867
"Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets Project Summary  Morphometric analysis is a primary algorithmic tool to discover disease and drug related effects on brain anatomy. Neurological degeneration and disease manifest in subtle and varied changes in brain anatomy that can be non-local in nature and effect amounts of white and gray matter as well as relative positioning and shapes of local brain anatomy. State-of-the-art morphometry methods focus on local matter distribution or on shape variations of apriori selected anatomies but have difficulty in detecting global or regional deterioration of matter; an important effect in many neurodegenerative processes. The proposal team recently developed a morphometric analysis based on unbalanced optimal transport, called UTM, that promises to be capable to discover local and global alteration of matter without the need to apriori select an anatomical region of interest.  The goal of this proposal is to develop the UTM technology into a software tool for automated high-throughput screening of large neurological image data sets. ​A more sensitive automated morphometric analysis tool will help researchers to discover neurological effects related to disease and lead to more efficient screening for drug related effects. Project Narrative  Describing anatomical differences in neurological image data set is a key technology to non-invasively discover the effects of disease processes or drug treatments on brain anatomy. Current morphometric analysis focus on local matter composition and on the shape of a priori defined regions of interest. The goal of this proposal is to extend the capabilities of image based morphometric analysis to be able to discover regionally varying deterioration and alteration of matter without the need for fine-grained segmentations and a priori definitions of regions of interest.",Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets,9787575,R41MH118845,"['Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Brain', 'Calibration', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Computer software', 'Data Set', 'Databases', 'Dementia', 'Deterioration', 'Development', 'Diffuse', 'Disease', 'Drug Screening', 'Early Diagnosis', 'Foundations', 'Goals', 'Grain', 'Image', 'Image Analysis', 'Imagery', 'Internet', 'Lead', 'Location', 'Machine Learning', 'Medical Imaging', 'Methodology', 'Methods', 'Modality', 'Nature', 'Nerve Degeneration', 'Neurologic', 'Neurologic Effect', 'Online Systems', 'Outcome', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Population Study', 'Positioning Attribute', 'Positron-Emission Tomography', 'Process', 'Research', 'Research Personnel', 'Services', 'Shapes', 'Software Tools', 'Structure', 'Technology', 'Temporal Lobe', 'Testing', 'Validation', 'Variant', 'autism spectrum disorder', 'base', 'clinical Diagnosis', 'experience', 'frontal lobe', 'gray matter', 'high throughput screening', 'image processing', 'image registration', 'imaging capabilities', 'improved', 'interest', 'learning strategy', 'morphometry', 'nervous system disorder', 'predict clinical outcome', 'predictive modeling', 'programs', 'research and development', 'shape analysis', 'software development', 'task analysis', 'tool', 'web services', 'white matter']",NIMH,"KITWARE, INC.",R41,2019,291536,-0.027092731348949867
"Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets Project Summary  Morphometric analysis is a primary algorithmic tool to discover disease and drug related effects on brain anatomy. Neurological degeneration and disease manifest in subtle and varied changes in brain anatomy that can be non-local in nature and effect amounts of white and gray matter as well as relative positioning and shapes of local brain anatomy. State-of-the-art morphometry methods focus on local matter distribution or on shape variations of apriori selected anatomies but have difficulty in detecting global or regional deterioration of matter; an important effect in many neurodegenerative processes. The proposal team recently developed a morphometric analysis based on unbalanced optimal transport, called UTM, that promises to be capable to discover local and global alteration of matter without the need to apriori select an anatomical region of interest.  The goal of this proposal is to develop the UTM technology into a software tool for automated high-throughput screening of large neurological image data sets. ​A more sensitive automated morphometric analysis tool will help researchers to discover neurological effects related to disease and lead to more efficient screening for drug related effects. Project Narrative  Describing anatomical differences in neurological image data set is a key technology to non-invasively discover the effects of disease processes or drug treatments on brain anatomy. Current morphometric analysis focus on local matter composition and on the shape of a priori defined regions of interest. The goal of this proposal is to extend the capabilities of image based morphometric analysis to be able to discover regionally varying deterioration and alteration of matter without the need for fine-grained segmentations and a priori definitions of regions of interest.",Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets,9679722,R41MH118845,"['Algorithmic Software', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Autistic Disorder', 'Brain', 'Calibration', 'Clinical', 'Clinical Research', 'Cluster Analysis', 'Computer software', 'Data Set', 'Databases', 'Dementia', 'Deterioration', 'Development', 'Diffuse', 'Disease', 'Drug Screening', 'Early Diagnosis', 'Foundations', 'Goals', 'Grain', 'Image', 'Image Analysis', 'Imagery', 'Internet', 'Lead', 'Location', 'Machine Learning', 'Medical Imaging', 'Methodology', 'Methods', 'Modality', 'Nature', 'Nerve Degeneration', 'Neurologic', 'Neurologic Effect', 'Online Systems', 'Outcome', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Population Study', 'Positioning Attribute', 'Positron-Emission Tomography', 'Process', 'Research', 'Research Personnel', 'Services', 'Shapes', 'Software Tools', 'Technology', 'Temporal Lobe', 'Testing', 'Validation', 'Variant', 'base', 'clinical Diagnosis', 'experience', 'frontal lobe', 'gray matter', 'high throughput screening', 'image processing', 'image registration', 'imaging capabilities', 'improved', 'interest', 'learning strategy', 'morphometry', 'nervous system disorder', 'predict clinical outcome', 'predictive modeling', 'programs', 'research and development', 'shape analysis', 'software development', 'task analysis', 'tool', 'web services', 'white matter']",NIMH,"KITWARE, INC.",R41,2018,303226,-0.027092731348949867
"Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets Project Summary Morphometric analysis is a primary algorithmic tool to discover disease and drug related effects on brain anatomy. Neurological degeneration and disease manifest in subtle and varied changes in brain anatomy that can be non-local in nature and affect amounts of white and gray matter as well as relative positioning and shapes of local brain anatomy. State-of-the-art morphometry methods focus on local matter distribution or on shape variations of apriori selected anatomies but have difficulty in detecting global or regional deterioration of matter; an important effect in many neurodegenerative processes. The proposal team recently developed a morphometric analysis based on unbalanced optimal transport, called UTM, that promises to be capable of discovering local and global alteration of matter without the need to apriori select an anatomical region of interest. The goal of this proposal is to develop the UTM technology into a software tool for automated high-throughput screening of large neurological image data sets. A more sensitive automated morphometric analysis tool will help researchers to discover neurological effects related to disease and lead to more efficient screening for drug related effects. Project Narrative  Describing anatomical differences in neurological image datasets is a key technology to non-invasively discover the effects of disease processes or drug treatments on brain anatomy. Current morphometric analysis focuses on local matter composition and on the shape of a priori defined regions of interest. The goal of this proposal is to extend the capabilities of image based morphometric analysis to be able to discover regionally varying deterioration and alteration of matter without the need for fine-grained segmentations and a priori definitions of regions of interest.",Enhanced Software Tools for Detecting Anatomical Differences in Image Data Sets,10139715,R42MH118845,"['Affect', 'Algorithmic Software', 'Algorithms', 'Anatomy', 'Blood flow', 'Brain', 'Clinical', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Data Set', 'Databases', 'Detection', 'Deterioration', 'Diffuse', 'Disease', 'Drug Screening', 'Goals', 'Grain', 'HIV', 'Image', 'Image Analysis', 'Internet', 'Joints', 'Label', 'Lead', 'Machine Learning', 'Measures', 'Medical Imaging', 'Metabolic', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Nature', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Neurologic', 'Neurologic Effect', 'Neurosurgeon', 'Online Systems', 'Outcome', 'Performance', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Population Analysis', 'Population Study', 'Positioning Attribute', 'Process', 'Questionnaires', 'Research', 'Research Personnel', 'Services', 'Shapes', 'Software Tools', 'Software Validation', 'Source', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Variant', 'Visualization', 'Washington', 'analysis pipeline', 'base', 'data access', 'data infrastructure', 'deep learning', 'experience', 'gray matter', 'high throughput screening', 'image registration', 'imaging capabilities', 'improved', 'insight', 'interest', 'metabolic rate', 'morphometry', 'nervous system disorder', 'neurodegenerative dementia', 'novel', 'programs', 'prototype', 'regional difference', 'research and development', 'shape analysis', 'software development', 'software infrastructure', 'task analysis', 'tool', 'usability', 'web app', 'web services', 'white matter']",NIMH,"KITWARE, INC.",R42,2020,456359,-0.030650095771422317
"CRCNS:  Neural Basis of Planning Humans and other animals can choose their actions using multiple learning algorithms and decision­ making strategies. For example, habitual behaviors adapted to a stable environment can be selected using so-called model-free reinforcement learning algorithms, in which the value of each action is incrementally updated according to the amount of unexpected reward. The underlying neural mechanisms for this type of reinforcement learning have been intensively studied. By contrast, how the brain utilizes the animal's knowledge of its environment to plan sequential actions using a model-based reinforcement learning algorithm remains unexplored. In this application, PIs with complementary expertise will investigate how different subdivisions of the primate prefrontal cortex contribute to the evaluation and arbitration of different learning algorithms during strategic planning in primates, using a sequential game referred to as ""4-in-a­ row"". Previous studies have revealed that with training, humans improve their competence in this game by gradually switching away from a model-free reinforcement learning towards a model-based reinforcement learning in the form of a tree search. In the first set of experiments, we will train non-human primates to play the 4-in-a-row game against a computer opponent. We predict that the complexity of the strategic planning and the opponent's move violating the animal's expectation will be reflected in the speed of animal's action and pupil diameters. Next, we will test how the medial and lateral aspects of prefrontal cortex contribute to the evaluation and selection of different learning algorithms during strategic interaction between the animal and computer opponent. We hypothesize that the lateral prefrontal cortex is involved in computing the integrated values of alternative actions originating from multiple sources and guiding the animal's choice, whereas the medial prefrontal cortex might be more involved in monitoring and resolving the discrepancies of actions favored by different learning algorithms. The results from these experiments will expand our knowledge of the neural mechanisms for complex strategic planning and unify various approaches to study naturalistic behaviors. By taking advantage of recent advances in machine learning and decision neuroscience, proposed studies will elucidate how multiple learning algorithms are simultaneously implemented and coordinated via specific patterns of activity in the prefrontal cortex. The results from these studies will transform the behavioral and analytical paradigms used to study high-order planning and their neural underpinnings in humans and animals. Strategic planning is commonly impaired in many psychiatric illnesses, including schizophrenia and depression, but their underlying neural mechanisms remain poorly understood. The proposed studies will elucidate the role of prefrontal cortex in implementing and arbitrating between multiple learning algorithms used for strategic planning.",CRCNS:  Neural Basis of Planning,9929660,R01MH118925,"['Algorithms', 'Animal Structures', 'Animals', 'Arbitration', 'Behavior', 'Behavioral', 'Brain', 'Caliber', 'Competence', 'Complex', 'Computers', 'Decision Making', 'Decision Trees', 'Economics', 'Environment', 'Evaluation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Human', 'Impairment', 'Individual', 'Knowledge', 'Lateral', 'Learning', 'Macaca mulatta', 'Machine Learning', 'Medial', 'Mental Depression', 'Mental disorders', 'Modeling', 'Monitor', 'Monkeys', 'Neurons', 'Neurosciences', 'Outcome', 'Participant', 'Pattern', 'Physiological', 'Play', 'Positioning Attribute', 'Prefrontal Cortex', 'Primates', 'Probability', 'Psyche structure', 'Psychological reinforcement', 'Pupil', 'Research', 'Rewards', 'Role', 'Schizophrenia', 'Series', 'Signal Transduction', 'Speed', 'Strategic Planning', 'Testing', 'Training', 'Trees', 'Update', 'base', 'expectation', 'experimental study', 'fictional works', 'improved', 'learning algorithm', 'neuromechanism', 'neurophysiology', 'nonhuman primate', 'predictive modeling', 'relating to nervous system', 'response', 'simulation', 'source guides']",NIMH,JOHNS HOPKINS UNIVERSITY,R01,2020,387421,-0.01962348897943657
"CRCNS:  Neural Basis of Planning Humans and other animals can choose their actions using multiple learning algorithms and decision­ making strategies. For example, habitual behaviors adapted to a stable environment can be selected using so-called model-free reinforcement learning algorithms, in which the value of each action is incrementally updated according to the amount of unexpected reward. The underlying neural mechanisms for this type of reinforcement learning have been intensively studied. By contrast, how the brain utilizes the animal's knowledge of its environment to plan sequential actions using a model-based reinforcement learning algorithm remains unexplored. In this application, PIs with complementary expertise will investigate how different subdivisions of the primate prefrontal cortex contribute to the evaluation and arbitration of different learning algorithms during strategic planning in primates, using a sequential game referred to as ""4-in-a­ row"". Previous studies have revealed that with training, humans improve their competence in this game by gradually switching away from a model-free reinforcement learning towards a model-based reinforcement learning in the form of a tree search. In the first set of experiments, we will train non-human primates to play the 4-in-a-row game against a computer opponent. We predict that the complexity of the strategic planning and the opponent's move violating the animal's expectation will be reflected in the speed of animal's action and pupil diameters. Next, we will test how the medial and lateral aspects of prefrontal cortex contribute to the evaluation and selection of different learning algorithms during strategic interaction between the animal and computer opponent. We hypothesize that the lateral prefrontal cortex is involved in computing the integrated values of alternative actions originating from multiple sources and guiding the animal's choice, whereas the medial prefrontal cortex might be more involved in monitoring and resolving the discrepancies of actions favored by different learning algorithms. The results from these experiments will expand our knowledge of the neural mechanisms for complex strategic planning and unify various approaches to study naturalistic behaviors. By taking advantage of recent advances in machine learning and decision neuroscience, proposed studies will elucidate how multiple learning algorithms are simultaneously implemented and coordinated via specific patterns of activity in the prefrontal cortex. The results from these studies will transform the behavioral and analytical paradigms used to study high-order planning and their neural underpinnings in humans and animals. Strategic planning is commonly impaired in many psychiatric illnesses, including schizophrenia and depression, but their underlying neural mechanisms remain poorly understood. The proposed studies will elucidate the role of prefrontal cortex in implementing and arbitrating between multiple learning algorithms used for strategic planning.",CRCNS:  Neural Basis of Planning,9952966,R01MH118925,"['Algorithms', 'Animal Structures', 'Animals', 'Arbitration', 'Behavior', 'Behavioral', 'Brain', 'Caliber', 'Competence', 'Complex', 'Computers', 'Decision Making', 'Decision Trees', 'Economics', 'Environment', 'Evaluation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Human', 'Impairment', 'Individual', 'Knowledge', 'Lateral', 'Learning', 'Macaca mulatta', 'Machine Learning', 'Medial', 'Mental Depression', 'Mental disorders', 'Modeling', 'Monitor', 'Monkeys', 'Neurons', 'Neurosciences', 'Outcome', 'Participant', 'Pattern', 'Physiological', 'Play', 'Positioning Attribute', 'Prefrontal Cortex', 'Primates', 'Probability', 'Psyche structure', 'Psychological reinforcement', 'Pupil', 'Research', 'Rewards', 'Role', 'Schizophrenia', 'Series', 'Signal Transduction', 'Speed', 'Strategic Planning', 'Testing', 'Training', 'Trees', 'Update', 'base', 'expectation', 'experimental study', 'fictional works', 'improved', 'learning algorithm', 'neuromechanism', 'neurophysiology', 'nonhuman primate', 'predictive modeling', 'relating to nervous system', 'response', 'simulation', 'source guides']",NIMH,JOHNS HOPKINS UNIVERSITY,R01,2019,281463,-0.01962348897943657
"CRCNS: Neural Basis of Planning Humans and other animals can choose their actions using multiple learning algorithms and decision­ making strategies. For example, habitual behaviors adapted to a stable environment can be selected using so-called model-free reinforcement learning algorithms, in which the value of each action is incrementally updated according to the amount of unexpected reward. The underlying neural mechanisms for this type of reinforcement learning have been intensively studied. By contrast, how the brain utilizes the animal's knowledge of its environment to plan sequential actions using a model-based reinforcement learning algorithm remains unexplored. In this application, PIs with complementary expertise will investigate how different subdivisions of the primate prefrontal cortex contribute to the evaluation and arbitration of different learning algorithms during strategic planning in primates, using a sequential game referred to as ""4-in-a­ row"". Previous studies have revealed that with training, humans improve their competence in this game by gradually switching away from a model-free reinforcement learning towards a model-based reinforcement learning in the form of a tree search. In the first set of experiments, we will train non-human primates to play the 4-in-a-row game against a computer opponent. We predict that the complexity of the strategic planning and the opponent's move violating the animal's expectation will be reflected in the speed of animal's action and pupil diameters. Next, we will test how the medial and lateral aspects of prefrontal cortex contribute to the evaluation and selection of different learning algorithms during strategic interaction between the animal and computer opponent. We hypothesize that the lateral prefrontal cortex is involved in computing the integrated values of alternative actions originating from multiple sources and guiding the animal's choice, whereas the medial prefrontal cortex might be more involved in monitoring and resolving the discrepancies of actions favored by different learning algorithms. The results from these experiments will expand our knowledge of the neural mechanisms for complex strategic planning and unify various approaches to study naturalistic behaviors. By taking advantage of recent advances in machine learning and decision neuroscience, proposed studies will elucidate how multiple learning algorithms are simultaneously implemented and coordinated via specific patterns of activity in the prefrontal cortex. The results from these studies will transform the behavioral and analytical paradigms used to study high-order planning and their neural underpinnings in humans and animals. Strategic planning is commonly impaired in many psychiatric illnesses, including schizophrenia and depression, but their underlying neural mechanisms remain poorly understood. The proposed studies will elucidate the role of prefrontal cortex in implementing and arbitrating between multiple learning algorithms used for strategic planning.",CRCNS: Neural Basis of Planning,9762221,R01MH118925,"['Algorithms', 'Animal Structures', 'Animals', 'Arbitration', 'Behavior', 'Behavioral', 'Brain', 'Caliber', 'Competence', 'Complex', 'Computers', 'Decision Making', 'Decision Trees', 'Economics', 'Environment', 'Evaluation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Human', 'Impairment', 'Individual', 'Knowledge', 'Lateral', 'Learning', 'Macaca mulatta', 'Machine Learning', 'Medial', 'Mental Depression', 'Mental disorders', 'Modeling', 'Monitor', 'Monkeys', 'Neurons', 'Neurosciences', 'Outcome', 'Participant', 'Pattern', 'Physiological', 'Play', 'Positioning Attribute', 'Prefrontal Cortex', 'Primates', 'Probability', 'Psyche structure', 'Psychological reinforcement', 'Pupil', 'Research', 'Rewards', 'Role', 'Schizophrenia', 'Series', 'Signal Transduction', 'Speed', 'Strategic Planning', 'Testing', 'Training', 'Trees', 'Update', 'base', 'expectation', 'experimental study', 'fictional works', 'improved', 'learning algorithm', 'neuromechanism', 'neurophysiology', 'nonhuman primate', 'predictive modeling', 'relating to nervous system', 'response', 'simulation', 'source guides']",NIMH,YALE UNIVERSITY,R01,2019,94319,-0.01962348897943657
"CRCNS: Neural Basis of Planning Humans and other animals can choose their actions using multiple learning algorithms and decision­ making strategies. For example, habitual behaviors adapted to a stable environment can be selected using so-called model-free reinforcement learning algorithms, in which the value of each action is incrementally updated according to the amount of unexpected reward. The underlying neural mechanisms for this type of reinforcement learning have been intensively studied. By contrast, how the brain utilizes the animal's knowledge of its environment to plan sequential actions using a model-based reinforcement learning algorithm remains unexplored. In this application, PIs with complementary expertise will investigate how different subdivisions of the primate prefrontal cortex contribute to the evaluation and arbitration of different learning algorithms during strategic planning in primates, using a sequential game referred to as ""4-in-a­ row"". Previous studies have revealed that with training, humans improve their competence in this game by gradually switching away from a model-free reinforcement learning towards a model-based reinforcement learning in the form of a tree search. In the first set of experiments, we will train non-human primates to play the 4-in-a-row game against a computer opponent. We predict that the complexity of the strategic planning and the opponent's move violating the animal's expectation will be reflected in the speed of animal's action and pupil diameters. Next, we will test how the medial and lateral aspects of prefrontal cortex contribute to the evaluation and selection of different learning algorithms during strategic interaction between the animal and computer opponent. We hypothesize that the lateral prefrontal cortex is involved in computing the integrated values of alternative actions originating from multiple sources and guiding the animal's choice, whereas the medial prefrontal cortex might be more involved in monitoring and resolving the discrepancies of actions favored by different learning algorithms. The results from these experiments will expand our knowledge of the neural mechanisms for complex strategic planning and unify various approaches to study naturalistic behaviors. By taking advantage of recent advances in machine learning and decision neuroscience, proposed studies will elucidate how multiple learning algorithms are simultaneously implemented and coordinated via specific patterns of activity in the prefrontal cortex. The results from these studies will transform the behavioral and analytical paradigms used to study high-order planning and their neural underpinnings in humans and animals. Strategic planning is commonly impaired in many psychiatric illnesses, including schizophrenia and depression, but their underlying neural mechanisms remain poorly understood. The proposed studies will elucidate the role of prefrontal cortex in implementing and arbitrating between multiple learning algorithms used for strategic planning.",CRCNS: Neural Basis of Planning,9692133,R01MH118925,"['Algorithms', 'Animals', 'Arbitration', 'Behavior', 'Behavioral', 'Brain', 'Caliber', 'Competence', 'Complex', 'Computers', 'Decision Making', 'Decision Trees', 'Economics', 'Environment', 'Evaluation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Human', 'Impairment', 'Individual', 'Knowledge', 'Lateral', 'Learning', 'Macaca mulatta', 'Machine Learning', 'Medial', 'Mental Depression', 'Mental disorders', 'Modeling', 'Monitor', 'Monkeys', 'Neurons', 'Neurosciences', 'Outcome', 'Participant', 'Pattern', 'Physiological', 'Play', 'Positioning Attribute', 'Prefrontal Cortex', 'Primates', 'Probability', 'Psyche structure', 'Psychological reinforcement', 'Pupil', 'Research', 'Rewards', 'Role', 'Schizophrenia', 'Series', 'Signal Transduction', 'Speed', 'Strategic Planning', 'Testing', 'Training', 'Trees', 'Update', 'base', 'expectation', 'experimental study', 'fictional works', 'improved', 'neuromechanism', 'neurophysiology', 'nonhuman primate', 'predictive modeling', 'relating to nervous system', 'response', 'simulation', 'source guides']",NIMH,YALE UNIVERSITY,R01,2018,429555,-0.01962348897943657
"Quantitative MRI of Multiple Sclerosis - Resubmission - 1 Project Summary  In this project, we propose a novel T1 and T2 quantification method that generates quantitative T1 or T2 maps from weighted MR images. Magnetic resonance imaging (MRI) is commonly used as a tool to diagnose Multiple Sclerosis (MS) and track lesional changes over time. Because MRI has various contrasts that display different information about the underlying tissue microstructure and physiology, it can potentially be used as a tool to predict MS disease progression and even disability. However, there is no known measure derived from MR images of MS that correlates well with clinical disability as described by the Expanded Disability Status Score (EDSS). Previous efforts to correlate MRI features and EDSS have included calculating total lesion load on T1- and T2-weighted images, measuring the variations in the magnetic transfer ration of normal-appearing brain tissues, and calculating cerebral atrophy, each with a varying level of success. Yet, there has been little study of the evolution of relaxation times of the lesions over time and how it relates to disability. Because changes in the T1 (spin-lattice) and T2 (spin-spin) relaxation times of a tissue can reflect pathological changes in that tissue over time, quantitative T1 and T2 maps derived from MR images may be more indicative of microscopic changes that manifest as disability in MS patients.  The specific aims of this proposal are: (1) develop and validate novel T1 and T2 quantification method on spin-echo MR images, (2) extend the novel quantification method to common MS imaging sequences, and (3) apply the novel quantification method to MS datasets to predict EDSS using machine learning. Aim 1 will involve the validation of the quantification pipeline on both T1- and T2-weighted spin-echo MR images in vivo, resulting in a range of acceptable parameters for the novel quantification method. Aim 2 will extend the quantification pipeline to include commonly used and more complicated MS imaging sequences, again resulting in a range of acceptable parameters for the quantification method. Aim 3 will use the quantification pipeline to compare machine learning algorithms with and without quantification to determine the added value of quantification in the imaging of MS. Additionally, Aim 3 will result in a predictive machine learning model utilizing multiple imaging contrasts for the prediction of disability in MS. These results will provide a more thorough understanding of the role of MR quantification in the evaluation of neurological diseases, such as MS, and will offer a scientific foundation to extend the use of MR quantification as a potential imaging biomarker for other diseases and pathologies. Project Narrative The proposed research aims to develop and validate a T1 and T2 quantification method using internal reference values derived from T1- or T2-weighted MR images. This method will be applied to weighted images of patients who have been diagnosed Multiple Sclerosis and input into various classification algorithms to determine which method is most predictive of worsening clinical disability as described by the Expanded Disability Status Score. By doing this, we will determine the impact of this novel quantification method as a tool for both the analysis of patients with Multiple Sclerosis as well as a tool for the normalization of big MR datasets before being input into machine learning algorithms.",Quantitative MRI of Multiple Sclerosis - Resubmission - 1,10154293,F31NS118930,"['Affect', 'Age-Years', 'Attenuated', 'Axon', 'Brain', 'Cerebrospinal Fluid', 'Clinical', 'Coupled', 'Data', 'Data Set', 'Demyelinations', 'Dependence', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Ensure', 'Equation', 'Evaluation', 'Evolution', 'Fatty acid glycerol esters', 'Foundations', 'Frequencies', 'Image', 'Inflammation', 'Investigation', 'Learning', 'Lesion', 'Liquid substance', 'Literature', 'Longitudinal Studies', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetism', 'Maps', 'Measures', 'Medical', 'Methods', 'Microscopic', 'Modeling', 'Morbidity - disease rate', 'Multiple Sclerosis', 'Neuraxis', 'Pathologic', 'Pathology', 'Patient imaging', 'Patients', 'Physiology', 'Population', 'Prediction of Response to Therapy', 'Predictive Value', 'Process', 'ROC Curve', 'Recovery', 'Reference Values', 'Relaxation', 'Research', 'Role', 'Scanning', 'Signal Transduction', 'T2 weighted imaging', 'Techniques', 'Time', 'Tissues', 'United States', 'Validation', 'Variant', 'brain tissue', 'cerebral atrophy', 'classification algorithm', 'contrast enhanced', 'contrast imaging', 'data harmonization', 'digital', 'disability', 'gray matter', 'healthy volunteer', 'imaging biomarker', 'improved', 'in vivo', 'in vivo imaging', 'longitudinal dataset', 'machine learning algorithm', 'multiple sclerosis patient', 'nervous system disorder', 'novel', 'predictive modeling', 'prospective', 'simulation', 'subcutaneous', 'success', 'tool', 'treatment response', 'white matter']",NINDS,UNIVERSITY OF CHICAGO,F31,2020,45520,-0.022592088785170945
"Peripheral Blood Exosome Lipids as Biomarkers of Disease Activity in Crohn’s Disease Abstract Management of inflammatory bowel disease (IBD) patients requires knowledge of disease status to inform treatment decisions. Commonly used biomarkers such as clinical disease activity indices, C reactive protein (CRP) and fecal calprotectin achieve suboptimal sensitivity and specificity for intestinal inflammation1. Data provided indicate that peripheral blood exosome (PBE) lipid composition distinguishes active IBD from normal patients (Fig. 1). We contend that PBE lipid compositions will provide clinicians with a highly sensitive and specific biomarker to assess disease activity without invasive testing. The need to identify subtle disease derives from the consensus conclusion that “deep remission” should be the endpoint of therapy. Thus, monitoring of patients with intent to suppress subclinical inflammation has emerged as a treatment goal3. Exosomes are lipid-encased, subcellular (60-80 nm) structures released into the peripheral blood at from intestinal epithelial cells (IEC), activated leukocytes (e.g. neutrophils, macrophages, dendritic cells, etc.) and mesenchymal cells during inflammation3. We used an ultrahigh resolution Orbitrap mass spectrometer to analyze lipid compositions of PBEs from patients (>25/group). Principal component analysis (PCA) of PBE lipid intensities showed a wide separation of datapoints between active IBD and normal controls and a heatmap analysis of differential abundance revealed high within-group correlations and low between-group correlations suggesting that distinct lipids can be resolved that correlate with disease activity. In this two year project, we propose to collect plasma from 1) moderate-severe, 2) mildly-active and 3) quiescent Crohn’s disease (CD) patients as well as inflammatory (C. difficile-infected) and 4) normal controls. Using CD allows us to test whether PBE composition detects levels of mild (in many cases, subclinical) disease activity as this identifies patients not in deep remission, a goal of medical therapy. In Aim 1, PBE lipid-based classifiers will be developed to discriminate mildly-active from severely-active CD and controls. In Aim 2, PBE lipids will be examined longitudinally within individual patients before and after therapy to identify lipids correlate with clinical response. Such information will also be passed to Aim 1 to develop more robust classifiers. Testing patterns of PBE lipids in responsive and refractory patients is intended to improve the robustness of the classifiers. Studies in Aim 1 and 2 are greatly enhanced by the inclusion of a second clinical site (Baylor University) to provide samples to validate (or not) data from University of Kentucky. The long-term goal will be to justify studies to identify a “exosomal lipid signature” panel that provide a novel set of biomarkers for discriminating levels of CD disease activity and informing treatment decisions. We post that the creation of a CD exosome lipid biomarker test will obviate the need for follow-up endoscopy in the majority of patients. Project Narrative This proposal is designed to test the novel idea that the lipid composition of peripheral blood exosomes (PBE) provides useful biomarkers of disease activity in Crohn’s Disease (CD). Our research is relevant to public health because it will provide new means for assessing disease activity in CD. This information will greatly facilitate the speed of discovery of new therapies as well as improve clinical assessment of CD patients.",Peripheral Blood Exosome Lipids as Biomarkers of Disease Activity in Crohn’s Disease,9767782,R21DK118954,"['Address', 'Biological Markers', 'C-reactive protein', 'Cells', 'Charge', 'Clinical', 'Clinical assessments', 'Clostridium difficile', 'Consensus', 'Control Groups', 'Crohn&apos', 's disease', 'Data', 'Data Set', 'Dendritic Cells', 'Development', 'Diagnostic', 'Disease', 'Disease remission', 'Endoscopy', 'Epithelial Cells', 'Goals', 'Image', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Intestines', 'Kentucky', 'Knowledge', 'Leukocyte L1 Antigen Complex', 'Leukocytes', 'Link', 'Lipids', 'Longitudinal Studies', 'Mass Spectrum Analysis', 'Medical', 'Mesenchymal', 'Newly Diagnosed', 'Pathology', 'Patient Monitoring', 'Patients', 'Pattern', 'Plasma', 'Principal Component Analysis', 'Process', 'Public Health', 'Refractory', 'Research', 'Resolution', 'Sampling', 'Sensitivity and Specificity', 'Speed', 'Structure', 'Testing', 'Tissues', 'Universities', 'Work', 'base', 'clinical research site', 'design', 'disorder control', 'exosome', 'follow-up', 'improved', 'indexing', 'individual patient', 'macrophage', 'mass spectrometer', 'neutrophil', 'novel', 'novel therapeutics', 'peripheral blood', 'response', 'sample collection', 'specific biomarkers', 'tool']",NIDDK,UNIVERSITY OF KENTUCKY,R21,2019,192662,-0.03921152064363259
"Peripheral Blood Exosome Lipids as Biomarkers of Disease Activity in Crohn’s Disease Abstract Management of inflammatory bowel disease (IBD) patients requires knowledge of disease status to inform treatment decisions. Commonly used biomarkers such as clinical disease activity indices, C reactive protein (CRP) and fecal calprotectin achieve suboptimal sensitivity and specificity for intestinal inflammation1. Data provided indicate that peripheral blood exosome (PBE) lipid composition distinguishes active IBD from normal patients (Fig. 1). We contend that PBE lipid compositions will provide clinicians with a highly sensitive and specific biomarker to assess disease activity without invasive testing. The need to identify subtle disease derives from the consensus conclusion that “deep remission” should be the endpoint of therapy. Thus, monitoring of patients with intent to suppress subclinical inflammation has emerged as a treatment goal3. Exosomes are lipid-encased, subcellular (60-80 nm) structures released into the peripheral blood at from intestinal epithelial cells (IEC), activated leukocytes (e.g. neutrophils, macrophages, dendritic cells, etc.) and mesenchymal cells during inflammation3. We used an ultrahigh resolution Orbitrap mass spectrometer to analyze lipid compositions of PBEs from patients (>25/group). Principal component analysis (PCA) of PBE lipid intensities showed a wide separation of datapoints between active IBD and normal controls and a heatmap analysis of differential abundance revealed high within-group correlations and low between-group correlations suggesting that distinct lipids can be resolved that correlate with disease activity. In this two year project, we propose to collect plasma from 1) moderate-severe, 2) mildly-active and 3) quiescent Crohn’s disease (CD) patients as well as inflammatory (C. difficile-infected) and 4) normal controls. Using CD allows us to test whether PBE composition detects levels of mild (in many cases, subclinical) disease activity as this identifies patients not in deep remission, a goal of medical therapy. In Aim 1, PBE lipid-based classifiers will be developed to discriminate mildly-active from severely-active CD and controls. In Aim 2, PBE lipids will be examined longitudinally within individual patients before and after therapy to identify lipids correlate with clinical response. Such information will also be passed to Aim 1 to develop more robust classifiers. Testing patterns of PBE lipids in responsive and refractory patients is intended to improve the robustness of the classifiers. Studies in Aim 1 and 2 are greatly enhanced by the inclusion of a second clinical site (Baylor University) to provide samples to validate (or not) data from University of Kentucky. The long-term goal will be to justify studies to identify a “exosomal lipid signature” panel that provide a novel set of biomarkers for discriminating levels of CD disease activity and informing treatment decisions. We post that the creation of a CD exosome lipid biomarker test will obviate the need for follow-up endoscopy in the majority of patients. Project Narrative This proposal is designed to test the novel idea that the lipid composition of peripheral blood exosomes (PBE) provides useful biomarkers of disease activity in Crohn’s Disease (CD). Our research is relevant to public health because it will provide new means for assessing disease activity in CD. This information will greatly facilitate the speed of discovery of new therapies as well as improve clinical assessment of CD patients.",Peripheral Blood Exosome Lipids as Biomarkers of Disease Activity in Crohn’s Disease,9616667,R21DK118954,"['Address', 'Biological Markers', 'C-reactive protein', 'Cells', 'Charge', 'Clinical', 'Clinical assessments', 'Clostridium difficile', 'Consensus', 'Control Groups', 'Crohn&apos', 's disease', 'Data', 'Data Set', 'Dendritic Cells', 'Development', 'Diagnostic', 'Disease', 'Disease remission', 'Endoscopy', 'Epithelial Cells', 'Goals', 'Image', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Intestines', 'Kentucky', 'Knowledge', 'Leukocyte L1 Antigen Complex', 'Leukocytes', 'Link', 'Lipids', 'Longitudinal Studies', 'Mass Spectrum Analysis', 'Medical', 'Mesenchymal', 'Newly Diagnosed', 'Pathology', 'Patient Monitoring', 'Patients', 'Pattern', 'Plasma', 'Principal Component Analysis', 'Process', 'Public Health', 'Refractory', 'Research', 'Resolution', 'Sampling', 'Sensitivity and Specificity', 'Speed', 'Structure', 'Testing', 'Tissues', 'Universities', 'Work', 'base', 'clinical research site', 'design', 'disorder control', 'exosome', 'follow-up', 'improved', 'indexing', 'individual patient', 'macrophage', 'mass spectrometer', 'neutrophil', 'novel', 'novel therapeutics', 'peripheral blood', 'response', 'sample collection', 'specific biomarkers', 'tool']",NIDDK,UNIVERSITY OF KENTUCKY,R21,2018,238816,-0.03921152064363259
"Segmenting the subarachnoid space and dura from clinical MRI Project Summary/Abstract The subarachnoid space (SAS) and dura are critical to brain health, but the vast majority of neuroimaging studies have been concerned solely with the gray matter and white matter of the brain itself. The dura is a thin, tough membrane about 1 mm thick that lies just inside the skull protecting the brain as well as the blood vessels and cerebrospinal fluid (CSF). The paired dural membranes separate to create the dural sinuses, which contain venous blood that drains blood as well as CSF from the brain. The SAS is a variable-thickness space lying just inside the dura and outside the brain and contains CSF, blood vessels, and arachnoid trabeculae, which loosely connect the arachnoid and pia matter. Both structures are thought to provide mechanical protection for the brain and are therefore important in modeling impacts that may lead to traumatic brain injury. As well, these structures are critical for their roles in facilitating adequate blood and CSF flow, which are both critical to brain health. Magnetic resonance imaging (MRI) is the key modality for imaging the brain, and one of the first steps in a conventional neuroimage pipeline is to remove materials outside the brain, including the SAS and dura (in a step called “skull-stripping” or “brain isolation”). This grant aims to radically alter this practice and to recover this conventionally-ignored extra-axial material by providing methods to segment both the SAS and dura and to characterize their geometries in health and disease. Specifically, we will: 1) Develop and carry out a detailed manual delineation protocol for labeling the SAS, the dural sinuses, and the epidural surface using multi-modal images; 2) Develop an automated algorithm to segment the subarachnoid space, dural sinuses, and dura from conventional T1-weighted and T2-weighted MRI; 3) Carry out pilot studies on subjects with normal pressure hydrocephalus and multiple sclerosis as well as normally aging adults. The software implementing these methods will be made freely available to the neuroscience community which will enable a host of new studies involving the quantification of this extra-axial anatomy across many neurological and neurodegenerative diseases. Project Narrative The subarachnoid space and dura surround and protect the brain within the skull; they also provide critical pathways for blood and cerebrospinal fluid, which are critical to proper brain function. This project will develop methods to segment and characterize the subarachnoid space and dura, and thereby characterize the full intracranial volume, from standard clinical-quality magnetic resonance images. The developed tools will enable studies of the distribution of cerebrospinal fluid spaces and intracranial volume around the brain which have never been possible in large populations to date.",Segmenting the subarachnoid space and dura from clinical MRI,10128985,R21NS120286,"['Address', 'Adult', 'Age', 'Anatomy', 'Appearance', 'Arachnoid Membrane', 'Arachnoid mater', 'Blood', 'Blood Vessels', 'Brain', 'Brain Diseases', 'Brain imaging', 'Cephalic', 'Cerebellum', 'Cerebrospinal Fluid', 'Cerebrum', 'Clinical', 'Communities', 'Computer software', 'Critical Pathways', 'Data', 'Data Set', 'Disease', 'Dura Mater', 'Elderly', 'Evaluation', 'Geometry', 'Goals', 'Grant', 'Health', 'Intracranial Hypertension', 'Knowledge', 'Label', 'Lead', 'Location', 'Longitudinal Studies', 'Magnetic Resonance Imaging', 'Manuals', 'Mechanics', 'Membrane', 'Meningeal', 'Meninges', 'Methods', 'Modeling', 'Multimodal Imaging', 'Multiple Sclerosis', 'Neurodegenerative Disorders', 'Neurosciences', 'Normal Pressure Hydrocephalus', 'Patients', 'Performance', 'Pilot Projects', 'Population', 'Positioning Attribute', 'Process', 'Protocols documentation', 'Reproducibility', 'Resolution', 'Role', 'Scanning', 'Sinus', 'Structure', 'Subarachnoid Space', 'Surface', 'Testing', 'Thick', 'Thinness', 'Traumatic Brain Injury', 'Variant', 'Venous', 'X-Ray Computed Tomography', 'automated algorithm', 'bone', 'brain health', 'cerebrospinal fluid flow', 'convolutional neural network', 'cranium', 'digital', 'gray matter', 'image processing', 'imaging modality', 'nervous system disorder', 'neuroimaging', 'normal aging', 'novel', 'tool', 'white matter']",NINDS,JOHNS HOPKINS UNIVERSITY,R21,2020,453891,-0.012407398588057979
"CRCNS: Neural computations for continuous control in virtual reality foraging Neuroscience has been able to gain major insights by relating measurements of neural activity to the  brain’s sensory inputs and motor outputs. Yet most neural activity supports computations and cognitive functions (‘thoughts’) that are not directly measurable by the experimenter. The investigators for the  present proposal invented a novel method to model an animal's thoughts by combining eXplainable  Artificial Intelligence (XAI) cognitive models for naturalistic tasks with measurements of the animal’s  sensory inputs and behavioral outputs. This model, called Inverse Rational Control (IRC), infers the internal model assumptions under which an animal's actions would be optimal. It then provides estimates of time series of subjective beliefs about the world that are consistent with this internal model. These estimates provide targets for a dimensionality reduction framework that assesses task-relevant  computational dynamics within neural population activity. The investigators propose to use these analysis  tools to find neural representations and transformations that implement these cognitive processes. They will apply this to a complex, naturalistic task that they developed: catching fireflies in virtual reality. The  monkeys they successfully trained to perform this task demonstrably weigh uncertainty, develop  predictions and long-term strategies, and apply nonlinear dynamics — all computations that are  fundamental for brain function. The investigators propose first to apply their method to analyze existing behavioral data and neural recordings collected in a simple version of this task with a single target firefly.  They will then collect new data on a multi-firefly version of the task, which incentivizes animals to make and implement longer-term plans. To analyze this data, the investigators will generalize their approach to  allow them to learn which compressed representations are selected by the animal as the foundation for  their strategies. These results will be used to form predictions about neural computations that will be tested using the electrophysiological data collected from multiple brain regions during this project. The  results of this study will explain the computations required to perform a complex, strategic navigation task  in the presence of uncertainty, and will demonstrate a new paradigm for understanding naturalistic brain computations. RELEVANCE (See instructions):  This project will uncover the neural basis of cognitive processes in the primate brain that underlie spatial  navigation, strategic planning, and behavioral control. It will demonstrate how a powerful new paradigm  for understanding complex, natural brain computations can apply to a wide variety of tasks, to explain  either adaptive or pathologically structured behavior. This will provide crucial guidance for understanding  and improving disrupted human cognitive function. n/a",CRCNS: Neural computations for continuous control in virtual reality foraging,10146521,R01NS120407,"['Animals', 'Area', 'Artificial Intelligence', 'Behavior', 'Behavior Control', 'Behavioral', 'Belief', 'Brain', 'Brain region', 'Cognition', 'Cognitive', 'Complex', 'Data', 'Dimensions', 'Electrophysiology (science)', 'Environment', 'Fireflies', 'Foundations', 'Goals', 'Human', 'Incentives', 'Instruction', 'Juice', 'Learning', 'Location', 'Macaca', 'Measurable', 'Measurement', 'Memory', 'Methods', 'Modeling', 'Monkeys', 'Motor output', 'Neurons', 'Neurosciences', 'Nonlinear Dynamics', 'Output', 'Parietal', 'Parietal Lobe', 'Pathologic', 'Perception', 'Population', 'Prefrontal Cortex', 'Primates', 'Process', 'Research', 'Research Personnel', 'Research Proposals', 'Rewards', 'Sensory', 'Series', 'Strategic Planning', 'Structure', 'Testing', 'Thinking', 'Time', 'Training', 'Uncertainty', 'Utah', 'cognitive function', 'cognitive process', 'design', 'improved', 'insight', 'neurophysiology', 'novel', 'preference', 'relating to nervous system', 'sensory input', 'theories', 'tool', 'virtual reality', 'way finding']",NINDS,BAYLOR COLLEGE OF MEDICINE,R01,2020,423814,-0.05360614282153155
"Quantitative Ultrasound Imaging Biomarkers of Crohn's Disease PROJECT SUMMARY Crohn's Disease (CD), a chronic inflammatory bowel condition, affects over 1 million Americans and costs billions of dollars annually. Frequent and accurate evaluation of bowel inflammation and fibrosis is critical to guide therapy. Ultrasound is safe, cost-effective, and widely available, thus provides an attractive alternative to contrast enhanced CT/MRI for frequent follow-ups. Here we propose a multi-parameter ultrasound imaging approach combining B-mode, VesselQuest (a new microvessel imaging method with much higher sensitivity than conventional Doppler), and Comb-push Ultrasound Shearwave Elastography (CUSE) for comprehensive evaluation of CD inflammation and fibrosis burden. Specific Aim 1: Technical Advancement. Synthetic Transmit Aperture imaging with coded virtual sources will be used to improve the resolution and penetration of the high definition version VesselQuestHD. Random Singular Value Decomposition (SVD) and randomized spatial down-sampling will be used to accelerate SVD for realtime VesselQuestRT imaging. We will improve CUSE with marching push beams for shear wave generation, and harmonic imaging for shear wave detection. Pilot patient tests will be conducted to optimize these technologies and pave the way for clinical studies below. Specific Aim 2: Comparison with MRI. We will study 100 CD patients to investigate efficacy of this new technology for disease evaluation and treatment outcome prediction. Patients starting a new medical therapy will have contrast enhanced MRI (used as reference standard) at baseline and 6-months post therapy, and ultrasound at baseline, 4-weeks, and 6-months. The correlation of ultrasound parameters with MRI scores at baseline and 6-months will be assessed. The efficacy of ultrasound for differentiating mild vs. severe disease will be assessed by ROC (Receiver Operating Characteristic) analysis. In addition, ROC analysis will be used to assess whether ultrasound parameters at baseline or 4-weeks can predict treatment response at 6-months. Specific Aim 3: Reproducibility Study. Two sonographers will repeatedly scan 45 CD patients. Intraclass correlation coefficients will be used to evaluate the inter-sonographer agreement for ultrasound parameters. The within patient variance component from the model will provide an estimate of the inter-sonographer variance, which represents a lower bound for the minimum detectable difference for longitudinal follow-ups. Specific Aim 4: Comparison with Surgical Pathology. We will study 50 CD patients to investigate the efficacy of this new technology for evaluating fibrosis, using surgical pathology as the reference standard. The association of ultrasound parameters with fibrosis category obtained from pathology will be assessed using Spearman rank correlation. In addition, ROC analysis will be used to assess whether individual or combined ultrasound parameters can distinguish between none-to-moderate versus severe fibrosis. Successful completion of this project will lead to a novel technology for frequent follow-ups of Crohn's disease. PROJECT NARRATIVE Crohn's Disease (CD) is a chronic inflammatory bowel condition that affects over 1 million Americans, costing billions of dollars every year. Frequent imaging follow-ups is critical for guiding therapy. In this study, we will develop and test a new ultrasound technology for accurate, safe, cost-effective, and frequent evaluation of Crohn's disease to guide treatment adjustments.",Quantitative Ultrasound Imaging Biomarkers of Crohn's Disease,9888380,R01DK120559,"['Affect', 'Agreement', 'American', 'Analysis of Variance', 'Anatomy', 'Area', 'Biological Markers', 'Categories', 'Chronic', 'Clinical', 'Clinical Research', 'Code', 'Comb animal structure', 'Crohn&apos', 's disease', 'Decision Making', 'Detection', 'Disease', 'Distal part of ileum', 'Doppler Ultrasound', 'Endoscopy', 'Evaluation', 'Excision', 'Fibrosis', 'Generations', 'Image', 'Imaging technology', 'Individual', 'Inflammation', 'Inflammatory', 'Intestines', 'Ionizing radiation', 'Length', 'Magnetic Resonance Imaging', 'Measurement', 'Medical', 'Modeling', 'Modification', 'Operative Surgical Procedures', 'Outcome', 'Pathology', 'Patient Schedules', 'Patients', 'Penetration', 'Prediction of Response to Therapy', 'ROC Curve', 'Randomized', 'Reference Standards', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Source', 'Speed', 'Surgical Pathology', 'Technology', 'Testing', 'Thick', 'Time', 'Treatment outcome', 'Ultrasonography', 'angiogenesis', 'contrast enhanced', 'cost', 'cost effective', 'disorder control', 'elastography', 'imaging approach', 'imaging biomarker', 'imaging modality', 'improved', 'multimodality', 'new technology', 'novel', 'outcome prediction', 'quantitative ultrasound', 'recruit', 'responders and non-responders', 'virtual']",NIDDK,MAYO CLINIC ROCHESTER,R01,2020,427438,-0.03027940671940273
"Quantitative Ultrasound Imaging Biomarkers of Crohn's Disease PROJECT SUMMARY Crohn's Disease (CD), a chronic inflammatory bowel condition, affects over 1 million Americans and costs billions of dollars annually. Frequent and accurate evaluation of bowel inflammation and fibrosis is critical to guide therapy. Ultrasound is safe, cost-effective, and widely available, thus provides an attractive alternative to contrast enhanced CT/MRI for frequent follow-ups. Here we propose a multi-parameter ultrasound imaging approach combining B-mode, VesselQuest (a new microvessel imaging method with much higher sensitivity than conventional Doppler), and Comb-push Ultrasound Shearwave Elastography (CUSE) for comprehensive evaluation of CD inflammation and fibrosis burden. Specific Aim 1: Technical Advancement. Synthetic Transmit Aperture imaging with coded virtual sources will be used to improve the resolution and penetration of the high definition version VesselQuestHD. Random Singular Value Decomposition (SVD) and randomized spatial down-sampling will be used to accelerate SVD for realtime VesselQuestRT imaging. We will improve CUSE with marching push beams for shear wave generation, and harmonic imaging for shear wave detection. Pilot patient tests will be conducted to optimize these technologies and pave the way for clinical studies below. Specific Aim 2: Comparison with MRI. We will study 100 CD patients to investigate efficacy of this new technology for disease evaluation and treatment outcome prediction. Patients starting a new medical therapy will have contrast enhanced MRI (used as reference standard) at baseline and 6-months post therapy, and ultrasound at baseline, 4-weeks, and 6-months. The correlation of ultrasound parameters with MRI scores at baseline and 6-months will be assessed. The efficacy of ultrasound for differentiating mild vs. severe disease will be assessed by ROC (Receiver Operating Characteristic) analysis. In addition, ROC analysis will be used to assess whether ultrasound parameters at baseline or 4-weeks can predict treatment response at 6-months. Specific Aim 3: Reproducibility Study. Two sonographers will repeatedly scan 45 CD patients. Intraclass correlation coefficients will be used to evaluate the inter-sonographer agreement for ultrasound parameters. The within patient variance component from the model will provide an estimate of the inter-sonographer variance, which represents a lower bound for the minimum detectable difference for longitudinal follow-ups. Specific Aim 4: Comparison with Surgical Pathology. We will study 50 CD patients to investigate the efficacy of this new technology for evaluating fibrosis, using surgical pathology as the reference standard. The association of ultrasound parameters with fibrosis category obtained from pathology will be assessed using Spearman rank correlation. In addition, ROC analysis will be used to assess whether individual or combined ultrasound parameters can distinguish between none-to-moderate versus severe fibrosis. Successful completion of this project will lead to a novel technology for frequent follow-ups of Crohn's disease. PROJECT NARRATIVE Crohn's Disease (CD) is a chronic inflammatory bowel condition that affects over 1 million Americans, costing billions of dollars every year. Frequent imaging follow-ups is critical for guiding therapy. In this study, we will develop and test a new ultrasound technology for accurate, safe, cost-effective, and frequent evaluation of Crohn's disease to guide treatment adjustments.",Quantitative Ultrasound Imaging Biomarkers of Crohn's Disease,9707399,R01DK120559,"['Affect', 'Agreement', 'American', 'Analysis of Variance', 'Anatomy', 'Area', 'Biological Markers', 'Categories', 'Chronic', 'Clinical', 'Clinical Research', 'Code', 'Comb animal structure', 'Crohn&apos', 's disease', 'Decision Making', 'Detection', 'Disease', 'Distal part of ileum', 'Doppler Ultrasound', 'Endoscopy', 'Evaluation', 'Excision', 'Fibrosis', 'Generations', 'Image', 'Imaging technology', 'Individual', 'Inflammation', 'Inflammatory', 'Intestines', 'Ionizing radiation', 'Length', 'Magnetic Resonance Imaging', 'Measurement', 'Medical', 'Modeling', 'Modification', 'Operative Surgical Procedures', 'Outcome', 'Pathology', 'Patient Schedules', 'Patients', 'Penetration', 'Prediction of Response to Therapy', 'Randomized', 'Receiver Operating Characteristics', 'Reference Standards', 'Reproducibility', 'Resolution', 'Sampling', 'Scanning', 'Source', 'Speed', 'Surgical Pathology', 'Technology', 'Testing', 'Thick', 'Time', 'Treatment outcome', 'Ultrasonography', 'angiogenesis', 'contrast enhanced', 'cost', 'cost effective', 'disorder control', 'elastography', 'imaging approach', 'imaging biomarker', 'imaging modality', 'improved', 'multimodality', 'new technology', 'novel', 'outcome prediction', 'quantitative ultrasound', 'recruit', 'responders and non-responders', 'virtual']",NIDDK,MAYO CLINIC ROCHESTER,R01,2019,365845,-0.03027940671940273
"Motion Sequencing for Neuropsychiatric Drug Development SUMMARY Neuropsychiatric disorders afflict more than 20% of the global population, resulting enormous personal and societal burdens, including trillions of dollars in total costs. Despite its prevalence and widespread impact, the development of drugs to treat neuropsychiatric diseases significantly lags behind other disease areas. This gap is due primarily to the fact that so few candidate drugs ever make it to the clinic; the average lag time for those that do make it is 13 years, further exacerbating the problem. The success rate for psychiatric drugs is historically low, even as financial investments in the area rise. Developing safe and effective drugs for neuropsychiatric disorders is inherently difficult, as it relies on characterizing the behavioral phenotypes of animal models. Current approaches to this are low-throughput, unreliable, expensive, and minimally informative. Most methods attempt to reduce complex behaviors that depend upon many neural circuits into one or a few quantifiable metrics, which are then used to predict how the candidates will impact the even more complex human nervous system. Syllable Life Sciences was founded on the vision of improving the way we measure and interpret changes in the behavior in the lab to improve pre-clinical drug development. To address this challenge, we have developed a behavioral analysis platform called Motion Sequencing (MoSeq). MoSeq combines machine vision and unsupervised machine learning techniques to objectively identify a set of stereotyped three-dimensional behavioral motifs (rears, turns, head-bobs, runs, pauses, etc.) that encapsulates all the spontaneous actions of mice within a particular experiment. In addition to revealing which motif (termed a “behavioral syllable”) is expressed at each moment, MoSeq identifies the statistics that govern how syllables transition from one to anther over time (“behavioral grammar”). Using MoSeq, therefore, we can comprehensively and quantitatively profile rodent behavior. Our previous work demonstrates that MoSeq directly reflects ongoing brain activity in psychiatry- relevant brain circuits, and that it may significantly outperform more standard methods of phenotyping drug effects in mice. In this SBIR Phase I project, we propose to extend the capabilities of MoSeq and explicitly demonstrate its translational value. Specifically, in Aim 1 we will expand the purview of MoSeq to include neuropsychiatry-relevant circuits; in Aim 2 we will build a behavioral space that describes relationships among drugs spanning the current psychopharmacopeia; and in Aim 3 we will demonstrate the clinical utility of MoSeq by using it to predict clinical trial outcomes. This project will lay essential groundwork for revolutionizing the preclinical pipeline for neuro- and psychotherapeutics. NARRATIVE Syllable Life Sciences is committed to addressing key preclinical bottlenecks that hinder neuropsychiatric drug development. Key to this is to develop more efficient and accurate ways to identify and measure drug impact on behavior in model systems. Motion Sequencing (MoSeq) integrates state-of-the-art 3D cameras and novel machine learning techniques to track changes in rodent behavior in response to drug, and it does so with unprecedented levels of accuracy and temporal resolution. The current proposal details plans to expand the capabilities of MoSeq and explicitly demonstrate its translational value for psychiatric drug development.",Motion Sequencing for Neuropsychiatric Drug Development,9981839,R43MH121178,"['3-Dimensional', 'Address', 'Adoption', 'Algorithms', 'Animal Behavior', 'Animal Model', 'Antidepressive Agents', 'Anxiety', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biological Assay', 'Biological Models', 'Biological Sciences', 'Biotechnology', 'Brain', 'Categories', 'Chemicals', 'Chemistry', 'Clinic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Collaborations', 'Complex', 'Data', 'Data Set', 'Development', 'Disease', 'Drug usage', 'Encapsulated', 'Failure', 'Genomics', 'Head', 'Human', 'Investments', 'Locomotion', 'Machine Learning', 'Maps', 'Measures', 'Mental disorders', 'Methods', 'Motion', 'Motivation', 'Mus', 'Nervous system structure', 'Neurobiology', 'Neurologic', 'Outcome', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Population', 'Preclinical Drug Development', 'Prevalence', 'Psychiatry', 'Psychotropic Drugs', 'Resolution', 'Rodent', 'Running', 'Senior Scientist', 'Short-Term Memory', 'Small Business Innovation Research Grant', 'Stereotyping', 'Techniques', 'Time', 'Translations', 'Vision', 'Water', 'Work', 'base', 'behavior test', 'behavioral phenotyping', 'brain machine interface', 'clinical efficacy', 'clinical predictors', 'computational chemistry', 'cost', 'drug candidate', 'drug development', 'experience', 'experimental study', 'forced swim test', 'improved', 'machine vision', 'neural circuit', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'pre-clinical', 'preclinical development', 'response', 'social', 'statistics', 'structural biology', 'success', 'temporal measurement', 'therapeutic candidate', 'tool', 'unsupervised learning']",NIMH,"SYLLABLE LIFE SCIENCES, INC.",R43,2020,432586,-0.05586219555512447
"Motion Sequencing for Neuropsychiatric Drug Development SUMMARY Neuropsychiatric disorders afflict more than 20% of the global population, resulting enormous personal and societal burdens, including trillions of dollars in total costs. Despite its prevalence and widespread impact, the development of drugs to treat neuropsychiatric diseases significantly lags behind other disease areas. This gap is due primarily to the fact that so few candidate drugs ever make it to the clinic; the average lag time for those that do make it is 13 years, further exacerbating the problem. The success rate for psychiatric drugs is historically low, even as financial investments in the area rise. Developing safe and effective drugs for neuropsychiatric disorders is inherently difficult, as it relies on characterizing the behavioral phenotypes of animal models. Current approaches to this are low-throughput, unreliable, expensive, and minimally informative. Most methods attempt to reduce complex behaviors that depend upon many neural circuits into one or a few quantifiable metrics, which are then used to predict how the candidates will impact the even more complex human nervous system. Syllable Life Sciences was founded on the vision of improving the way we measure and interpret changes in the behavior in the lab to improve pre-clinical drug development. To address this challenge, we have developed a behavioral analysis platform called Motion Sequencing (MoSeq). MoSeq combines machine vision and unsupervised machine learning techniques to objectively identify a set of stereotyped three-dimensional behavioral motifs (rears, turns, head-bobs, runs, pauses, etc.) that encapsulates all the spontaneous actions of mice within a particular experiment. In addition to revealing which motif (termed a “behavioral syllable”) is expressed at each moment, MoSeq identifies the statistics that govern how syllables transition from one to anther over time (“behavioral grammar”). Using MoSeq, therefore, we can comprehensively and quantitatively profile rodent behavior. Our previous work demonstrates that MoSeq directly reflects ongoing brain activity in psychiatry- relevant brain circuits, and that it may significantly outperform more standard methods of phenotyping drug effects in mice. In this SBIR Phase I project, we propose to extend the capabilities of MoSeq and explicitly demonstrate its translational value. Specifically, in Aim 1 we will expand the purview of MoSeq to include neuropsychiatry-relevant circuits; in Aim 2 we will build a behavioral space that describes relationships among drugs spanning the current psychopharmacopeia; and in Aim 3 we will demonstrate the clinical utility of MoSeq by using it to predict clinical trial outcomes. This project will lay essential groundwork for revolutionizing the preclinical pipeline for neuro- and psychotherapeutics. NARRATIVE Syllable Life Sciences is committed to addressing key preclinical bottlenecks that hinder neuropsychiatric drug development. Key to this is to develop more efficient and accurate ways to identify and measure drug impact on behavior in model systems. Motion Sequencing (MoSeq) integrates state-of-the-art 3D cameras and novel machine learning techniques to track changes in rodent behavior in response to drug, and it does so with unprecedented levels of accuracy and temporal resolution. The current proposal details plans to expand the capabilities of MoSeq and explicitly demonstrate its translational value for psychiatric drug development.",Motion Sequencing for Neuropsychiatric Drug Development,9847002,R43MH121178,"['3-Dimensional', 'Address', 'Adoption', 'Algorithms', 'Animal Behavior', 'Animal Model', 'Antidepressive Agents', 'Anxiety', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biological Assay', 'Biological Models', 'Biological Sciences', 'Biotechnology', 'Brain', 'Categories', 'Chemicals', 'Chemistry', 'Clinic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Collaborations', 'Complex', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Drug usage', 'Encapsulated', 'Failure', 'Genomics', 'Head', 'Human', 'Investments', 'Locomotion', 'Machine Learning', 'Maps', 'Measures', 'Mental disorders', 'Methods', 'Motion', 'Motivation', 'Mus', 'Nervous system structure', 'Neurobiology', 'Neurologic', 'Outcome', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Population', 'Preclinical Drug Development', 'Prevalence', 'Psychiatry', 'Psychotropic Drugs', 'Resolution', 'Rodent', 'Running', 'Senior Scientist', 'Short-Term Memory', 'Small Business Innovation Research Grant', 'Stereotyping', 'Techniques', 'Time', 'Translations', 'Vision', 'Water', 'Work', 'base', 'behavior test', 'brain machine interface', 'clinical efficacy', 'clinical predictors', 'computational chemistry', 'cost', 'drug candidate', 'drug development', 'experience', 'experimental study', 'forced swim test', 'improved', 'machine vision', 'neural circuit', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'pre-clinical', 'preclinical development', 'response', 'social', 'statistics', 'structural biology', 'success', 'temporal measurement', 'therapeutic candidate', 'tool', 'unsupervised learning']",NIMH,"SYLLABLE LIFE SCIENCES, INC.",R43,2019,447370,-0.05586219555512447
"Direct sub-second measurement of neuromodulator signaling during risky decision-making PROJECT SUMMARY Much work in decision neuroscience has been predicated on the hypothesis that dopaminergic signaling plays a critical role in value representations and their updating, and recent work has linked abnormalities in such value representations to specific features of psychiatric illness (e.g., anhedonia in major depression). However, despite the fundamental role that risk plays in evaluating choice options as well as the role sensitivity to risk plays in psychiatric illness, including anxiety disorders at one extreme and health risk behaviors like substance use at another other, an understanding of the neurobiology of risk remains elusive. Indirect evidence from pharmacological studies have suggested both serotoninergic and noradrengergic signaling may play roles in representations of risk; however, direct measurement of serotonin or norepinephrine signaling during risky choice has yet to be examined in humans. Recent advances by MPI Montague’s group allow the unprecedented ability to track neuromodulator responses with high temporal resolution and chemical specificity. Specifically, MPI Montague’s team is able to directly and simultaneously measure dopamine and serotonin responses in awake humans with the temporal resolution (~ 1 ms) required to examine the relationship of neuromodulator release with decision-making processes. For signal identification and extraction, the recording method uses machine-learning algorithms (elastic net regression) combined with electrochemistry using only off-the shelf hardware and software. The product of this ‘elastic net electrochemistry’ is recordings of in vivo neuromodulator fluctuations at sub-second resolution. This application merges the decision neuroscience expertise of MPI King-Casas with these advances of MPI Montague to directly examine serotonergic, noradrenergic, and dopaminergic functioning during risky choice. To achieve this goal, we will record neuromodulator responses in participants with medication-resistant epilepsy who already have intracranial depth electrodes in place for phase-II monitoring. Depth electrodes will be implanted by our neurosurgery colleagues at Virginia Tech’s medical affiliate Carilion Clinic (Carilion Clinic PI: Witcher). During recording, participants will perform i) a risk elicitation task (Holt & Laury type task) and ii) a reward learning task (multi-arm bandit task) that have been shown by our group and others both to reliably evoke neural responses associated with risk and representations as they are monitored in a standard (i.e., non-surgical) hospital suite. Depth recordings will be made using a standard montage that includes multiple contacts along the dorsal-rostral axis of the medial prefrontal cortex. PROJECT NARRATIVE Despite the fundamental role that risk plays in evaluating choice options as well as the role sensitivity to risk plays in psychiatric illness, including anxiety disorders at one extreme and health risk behaviors like substance use at another other, an understanding of the neurobiology of risk remains elusive. Here, we take advantage of a new technology to directly measure serotonin, norepinephrine and dopamine on a sub-second basis to investigate the neurocomputational role of these neurotransmitters in risky choice.",Direct sub-second measurement of neuromodulator signaling during risky decision-making,9961146,R01MH122948,"['Alcohol or Other Drugs use', 'Anhedonia', 'Anxiety Disorders', 'Behavioral', 'Chemicals', 'Clinic', 'Computer software', 'Decision Making', 'Dopamine', 'Dorsal', 'Electrochemistry', 'Electrodes', 'Epilepsy', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Hospitals', 'Human', 'Implant', 'Implanted Electrodes', 'Learning', 'Link', 'Major Depressive Disorder', 'Measurement', 'Measures', 'Medial', 'Medical', 'Mental disorders', 'Methods', 'Monitor', 'Morale', 'Neurobiology', 'Neuromodulator', 'Neurosciences', 'Neurotransmitters', 'Norepinephrine', 'Outcome', 'Participant', 'Pharmaceutical Preparations', 'Pharmacology Study', 'Phase', 'Play', 'Prefrontal Cortex', 'Process', 'Resistance', 'Resolution', 'Rewards', 'Risk', 'Risk Behaviors', 'Role', 'Serotonin', 'Signal Transduction', 'Specificity', 'Testing', 'Update', 'Virginia', 'Work', 'arm', 'awake', 'base', 'experience', 'in vivo', 'machine learning algorithm', 'neurosurgery', 'new technology', 'noradrenergic', 'relating to nervous system', 'response', 'temporal measurement']",NIMH,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2020,717227,-0.057330038168073326
"Protein structure determination from low-resolution experimental data Project Abstract Determination of a protein’s three-dimensional structure is of critical importance in biology, providing insights to biological mechanisms and important targets for drug design. While high- resolution X-ray diffraction data provides an atomic view of cellular components, for many interesting and biologically relevant complexes, it may only be possible to obtain low-resolution structural information. Both cryo-electron microscopy and X-ray crystallography, when applied to large, flexible molecular machines, often produce data of 3-6 Å resolution. Extracting detailed atomic information from this data, critical in understanding function, the effects of mutation, or in designing drugs is impossible due to the low number of observations and the large conformational space proteins may adopt. I propose to develop computational methods for extracting high-resolution atomic models from this low-resolution data, bridging the “resolution gap” with computational methods. My proposed research develops and extends our labs’ methods for automatically inferring atomic accuracy models, from these “near-atomic” resolution sources of experimental data. We develop novel conformational sampling methods, guided by experimental data, to infer atomic information both in cases where homologous high-resolution data is available, and where it is not. Additionally, we propose development of methods for estimating model uncertainty; these are critical in understanding to what degree structural conclusions may be made from a particular dataset. Finally, in pushing the resolution limit further, we develop general tools for biomolecular forcefield optimization. These machine-learning tools will allow development of a next-generation forcefield, critical in extending the resolution limit of data from which we can infer atomic details. The overall goal of the proposed research is robust and accessible methods to determine protein structures to atomic accuracy from only sparse experimental data. Combined, the three aims in this proposal will lead to dramatic improvements in our ability to infer atomic interactions from sparse experimental data. This will lead to determination of structures that will reveal key insights into how biomedically important protein complexes perform their function and what goes wrong in human disease. Project Narrative This project develops computational tools for accurately determining protein structure from low- resolution experimental data. The proposed work will lead to dramatic improvements in our ability to model dynamic structures from sparse data. Obtaining accurate structures from this data will reveal insights into how biomedically important protein complexes perform their function, and what goes wrong in human disease.",Protein structure determination from low-resolution experimental data,9988448,R01GM123089,"['Address', 'Adopted', 'Biological', 'Biology', 'Complex', 'Computing Methodologies', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Development', 'Drug Design', 'Drug Targeting', 'Goals', 'Homologous Gene', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Molecular Conformation', 'Molecular Machines', 'Mutation', 'Phase', 'Positioning Attribute', 'Proteins', 'Refit', 'Research', 'Resolution', 'Roentgen Rays', 'Sampling', 'Source', 'Structure', 'System', 'Testing', 'Torsion', 'Uncertainty', 'Validation', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'atomic data', 'atomic interactions', 'computerized tools', 'exhaustion', 'flexibility', 'heterogenous data', 'human disease', 'improved', 'insight', 'method development', 'model building', 'next generation', 'novel', 'protein complex', 'protein structure', 'reconstruction', 'structured data', 'three dimensional structure', 'tool']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,279050,0.005543841707383359
"Protein structure determination from low-resolution experimental data Project Abstract Determination of a protein’s three-dimensional structure is of critical importance in biology, providing insights to biological mechanisms and important targets for drug design. While high- resolution X-ray diffraction data provides an atomic view of cellular components, for many interesting and biologically relevant complexes, it may only be possible to obtain low-resolution structural information. Both cryo-electron microscopy and X-ray crystallography, when applied to large, flexible molecular machines, often produce data of 3-6 Å resolution. Extracting detailed atomic information from this data, critical in understanding function, the effects of mutation, or in designing drugs is impossible due to the low number of observations and the large conformational space proteins may adopt. I propose to develop computational methods for extracting high-resolution atomic models from this low-resolution data, bridging the “resolution gap” with computational methods. My proposed research develops and extends our labs’ methods for automatically inferring atomic accuracy models, from these “near-atomic” resolution sources of experimental data. We develop novel conformational sampling methods, guided by experimental data, to infer atomic information both in cases where homologous high-resolution data is available, and where it is not. Additionally, we propose development of methods for estimating model uncertainty; these are critical in understanding to what degree structural conclusions may be made from a particular dataset. Finally, in pushing the resolution limit further, we develop general tools for biomolecular forcefield optimization. These machine-learning tools will allow development of a next-generation forcefield, critical in extending the resolution limit of data from which we can infer atomic details. The overall goal of the proposed research is robust and accessible methods to determine protein structures to atomic accuracy from only sparse experimental data. Combined, the three aims in this proposal will lead to dramatic improvements in our ability to infer atomic interactions from sparse experimental data. This will lead to determination of structures that will reveal key insights into how biomedically important protein complexes perform their function and what goes wrong in human disease. Project Narrative This project develops computational tools for accurately determining protein structure from low- resolution experimental data. The proposed work will lead to dramatic improvements in our ability to model dynamic structures from sparse data. Obtaining accurate structures from this data will reveal insights into how biomedically important protein complexes perform their function, and what goes wrong in human disease.",Protein structure determination from low-resolution experimental data,9768492,R01GM123089,"['Address', 'Adopted', 'Biological', 'Biology', 'Complex', 'Computing Methodologies', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Development', 'Drug Design', 'Drug Targeting', 'Goals', 'Homologous Gene', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Molecular Conformation', 'Molecular Machines', 'Mutation', 'Phase', 'Positioning Attribute', 'Proteins', 'Refit', 'Research', 'Resolution', 'Roentgen Rays', 'Sampling', 'Source', 'Structural Protein', 'Structure', 'System', 'Testing', 'Torsion', 'Uncertainty', 'Validation', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'atomic data', 'atomic interactions', 'computerized tools', 'exhaustion', 'flexibility', 'human disease', 'improved', 'insight', 'method development', 'model building', 'next generation', 'novel', 'protein complex', 'protein structure', 'reconstruction', 'three dimensional structure', 'tool']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,279675,0.005543841707383359
"Protein structure determination from low-resolution experimental data Project Abstract Determination of a protein’s three-dimensional structure is of critical importance in biology, providing insights to biological mechanisms and important targets for drug design. While high- resolution X-ray diffraction data provides an atomic view of cellular components, for many interesting and biologically relevant complexes, it may only be possible to obtain low-resolution structural information. Both cryo-electron microscopy and X-ray crystallography, when applied to large, flexible molecular machines, often produce data of 3-6 Å resolution. Extracting detailed atomic information from this data, critical in understanding function, the effects of mutation, or in designing drugs is impossible due to the low number of observations and the large conformational space proteins may adopt. I propose to develop computational methods for extracting high-resolution atomic models from this low-resolution data, bridging the “resolution gap” with computational methods. My proposed research develops and extends our labs’ methods for automatically inferring atomic accuracy models, from these “near-atomic” resolution sources of experimental data. We develop novel conformational sampling methods, guided by experimental data, to infer atomic information both in cases where homologous high-resolution data is available, and where it is not. Additionally, we propose development of methods for estimating model uncertainty; these are critical in understanding to what degree structural conclusions may be made from a particular dataset. Finally, in pushing the resolution limit further, we develop general tools for biomolecular forcefield optimization. These machine-learning tools will allow development of a next-generation forcefield, critical in extending the resolution limit of data from which we can infer atomic details. The overall goal of the proposed research is robust and accessible methods to determine protein structures to atomic accuracy from only sparse experimental data. Combined, the three aims in this proposal will lead to dramatic improvements in our ability to infer atomic interactions from sparse experimental data. This will lead to determination of structures that will reveal key insights into how biomedically important protein complexes perform their function and what goes wrong in human disease. Project Narrative This project develops computational tools for accurately determining protein structure from low- resolution experimental data. The proposed work will lead to dramatic improvements in our ability to model dynamic structures from sparse data. Obtaining accurate structures from this data will reveal insights into how biomedically important protein complexes perform their function, and what goes wrong in human disease.",Protein structure determination from low-resolution experimental data,9536048,R01GM123089,"['Address', 'Adopted', 'Biological', 'Biology', 'Complex', 'Computing Methodologies', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Development', 'Drug Design', 'Drug Targeting', 'Goals', 'Homologous Gene', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Molecular Conformation', 'Molecular Machines', 'Mutation', 'Phase', 'Positioning Attribute', 'Proteins', 'Refit', 'Research', 'Resolution', 'Roentgen Rays', 'Sampling', 'Source', 'Structure', 'System', 'Testing', 'Torsion', 'Uncertainty', 'Validation', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'atomic data', 'atomic interactions', 'computerized tools', 'exhaustion', 'flexibility', 'human disease', 'improved', 'insight', 'method development', 'model building', 'next generation', 'novel', 'protein complex', 'protein structure', 'reconstruction', 'three dimensional structure', 'tool']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,280284,0.005543841707383359
"Protein structure determination from low-resolution experimental data Project Abstract Determination of a protein’s three-dimensional structure is of critical importance in biology, providing insights to biological mechanisms and important targets for drug design. While high- resolution X-ray diffraction data provides an atomic view of cellular components, for many interesting and biologically relevant complexes, it may only be possible to obtain low-resolution structural information. Both cryo-electron microscopy and X-ray crystallography, when applied to large, flexible molecular machines, often produce data of 3-6 Å resolution. Extracting detailed atomic information from this data, critical in understanding function, the effects of mutation, or in designing drugs is impossible due to the low number of observations and the large conformational space proteins may adopt. I propose to develop computational methods for extracting high-resolution atomic models from this low-resolution data, bridging the “resolution gap” with computational methods. My proposed research develops and extends our labs’ methods for automatically inferring atomic accuracy models, from these “near-atomic” resolution sources of experimental data. We develop novel conformational sampling methods, guided by experimental data, to infer atomic information both in cases where homologous high-resolution data is available, and where it is not. Additionally, we propose development of methods for estimating model uncertainty; these are critical in understanding to what degree structural conclusions may be made from a particular dataset. Finally, in pushing the resolution limit further, we develop general tools for biomolecular forcefield optimization. These machine-learning tools will allow development of a next-generation forcefield, critical in extending the resolution limit of data from which we can infer atomic details. The overall goal of the proposed research is robust and accessible methods to determine protein structures to atomic accuracy from only sparse experimental data. Combined, the three aims in this proposal will lead to dramatic improvements in our ability to infer atomic interactions from sparse experimental data. This will lead to determination of structures that will reveal key insights into how biomedically important protein complexes perform their function and what goes wrong in human disease. Project Narrative This project develops computational tools for accurately determining protein structure from low- resolution experimental data. The proposed work will lead to dramatic improvements in our ability to model dynamic structures from sparse data. Obtaining accurate structures from this data will reveal insights into how biomedically important protein complexes perform their function, and what goes wrong in human disease.",Protein structure determination from low-resolution experimental data,9287589,R01GM123089,"['Address', 'Adopted', 'Biological', 'Biology', 'Complex', 'Computing Methodologies', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Development', 'Drug Design', 'Drug Targeting', 'Goals', 'Homologous Gene', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Molecular Conformation', 'Molecular Machines', 'Mutation', 'Phase', 'Positioning Attribute', 'Proteins', 'Refit', 'Research', 'Resolution', 'Roentgen Rays', 'Sampling', 'Source', 'Structure', 'System', 'Testing', 'Torsion', 'Uncertainty', 'Validation', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'X-Ray Diffraction', 'atomic data', 'atomic interactions', 'computerized tools', 'exhaustion', 'flexibility', 'human disease', 'improved', 'insight', 'method development', 'model building', 'next generation', 'novel', 'protein complex', 'protein structure', 'reconstruction', 'three dimensional structure', 'tool']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2017,280183,0.005543841707383359
"Open-source software for multi-scale mapping of the human brain Project Summary (maximum 30 lines) The BRAIN initiative seeks to develop and apply technologies in order to understand of how brain cells interact in both time and space to give rise to brain function. A key deliverable in BRAIN is a systematic census of neuronal and glial cell types, which is a prerequisite to understand how these cells interact and change in healthy aging and in disease. Moreover, such a census will provide a common reference cell taxonomy, which is crucial to harmonize studies at different sites and achieve the goals of BRAIN.  A necessary companion of the census is a reference coordinate system, which enables us to understand the spatio-anatomical context in which cells interact, as well as their connectivity. Building such a coordinate system requires advanced spatial alignment (registration) tools, since virtually every lab technique used in microscopic brain cell phenotyping – particularly in human brain – requires blocking and/or sectioning of samples, hence distorting the structure of tissue. Due to the difficulty of providing support for datasets and acquisition setups different to the original, most publicly available techniques to recover the lost tissue structure (“3D reconstruction”) rely on very simple techniques, such as vanilla pairwise registration of neighboring sections. Moreover, conventional reconstruction methods are notoriously slow, and no available method is designed to 3D reconstruct whole human brains.  In this interdisciplinary project, which lies at the nexus of computer science, MRI physics, histology, optical imaging, anatomy and statistics, we propose to extend, robustify, test, distribute and support our recently developed, state-or-the-art techniques that will enable the constructions of a coordinate system capable of representing multi-scale maps of human brain anatomy and function. This includes algorithms and software for: image analysis of ex vivo MRI; construction of laminar models of the human cerebral cortex; 3D reconstruction of microscopic images and alignment to the laminar models; surface based analysis of microscopy data on the laminar structure; and alignment of ex vivo and in vivo images to accurately transfer information from microscopy to MRI studies of the living brain, in health and in disease.  The tools we propose to build and disseminate will combine modern deep learning techniques with principled Bayesian inference, and have the potential to deliver accurate registration at the macroscopic, mesoscopic, and microscopic level, with high throughput delivered using cutting-edge machine learning algorithms. Effective dissemination of these tools, along with companion test data, will be achieved through our widespread package FreeSurfer. The distributed tools will not only enable the construction of a cell census with rich spatial information at human brain scale (including a novel laminar model), but will also have a tremendous impact in other areas of neuroimaging, including overarching goals of BRAIN such as: linking cellular-level activity to functional MRI, atlas building, or connecting axonal anatomy to diffusion MRI. Project narrative (maximum 3 sentences) In this project, we seek resources to develop, integrate, distribute and support a set of tools for the automated spatial mapping of in vivo (MRI, PET, etc…) and ex vivo (microscopy) imaging modalities, which can be used to enhance the BRAIN cell census with a coordinate system for representing maps of human brain function and anatomy at multiple scales – including laminar models of the cerebral cortex that have been long desired by the neuroimaging community. The tools will include algorithms for segmentation of ex vivo MRI; laminar modeling; registration of microscopic images to MRI; and analysis of histological data on spherical coordinate systems of the cortex. These tools will not only enable the scientific community to include spatial information into the cell census, but will also have a dramatic impact on other neuroimaging projects related to the BRAIN initiative in terms of their ability to share and compare data in a unified coordinate system and use the results of BRAIN research to make inferences in studies of living human beings.",Open-source software for multi-scale mapping of the human brain,10008355,RF1MH123195,"['3-Dimensional', 'Affect', 'Algorithmic Software', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Atlases', 'Axon', 'BRAIN initiative', 'Bayesian Analysis', 'Brain', 'Brain Diseases', 'Brain Stem', 'Cells', 'Censuses', 'Cerebellum', 'Cerebral cortex', 'Cerebrum', 'Communities', 'Companions', 'Computer software', 'Data', 'Data Set', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Documentation', 'Engineering', 'Ensure', 'Environment', 'Fiber', 'Freezing', 'Functional Magnetic Resonance Imaging', 'Future', 'Gap Junctions', 'Genus Vanilla', 'Goals', 'Health', 'Hippocampus (Brain)', 'Histologic', 'Histology', 'Human', 'Image', 'Image Analysis', 'Institutes', 'Iron', 'Licensing', 'Link', 'Location', 'Magnetic Resonance Imaging', 'Maintenance', 'Maps', 'Methods', 'Microscopic', 'Microscopy', 'Modality', 'Modeling', 'Modernization', 'Molecular', 'Neuroglia', 'Neurons', 'Neurosciences', 'Noise', 'Phenotype', 'Physics', 'Positron-Emission Tomography', 'Property', 'Protocols documentation', 'Protons', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Secure', 'Signal Transduction', 'Site', 'Stains', 'Structure', 'Surface', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Tissues', 'Visualization', 'Visualization software', 'Work', 'base', 'brain cell', 'brain research', 'cell type', 'cloud based', 'computer science', 'contrast imaging', 'data sharing', 'data tools', 'deep learning', 'design', 'healthy aging', 'heterogenous data', 'human data', 'image reconstruction', 'imaging modality', 'improved', 'in vivo', 'in vivo imaging', 'machine learning algorithm', 'microscopic imaging', 'morphometry', 'multidisciplinary', 'neuroimaging', 'novel', 'open source', 'optical imaging', 'reconstruction', 'sample fixation', 'segmentation algorithm', 'skills', 'statistics', 'stem', 'tool', 'virtual']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,RF1,2020,1438262,-0.006435601654923538
"Mapping human brain perivascular space in lifespan using human connectome project data PROJECT SUMMARY Perivascular spaces are a critical component of the glia-lymphatic circuit, facilitating the clearance of soluble waste. The role of perivascular spaces and changes in the brain’s clearance system in normal development, aging, and cognition is not fully understood, mainly due to lack of neuroimaging capabilities. However, noninvasive in vivo mapping of the perivascular space fluid with high accuracy and reliability is now made possible with our recent analytical developments, using human connectome project (HCP) data. The objective of this project is to map structural and diffusion features of perivascular space fluid across lifespan. PVS features include PVS presence (e.g., count and volume fraction) and diffusion (e.g., diffusivity and anisotropy). These features will be extracted regionally and globally across the brain. Structural MRI will provide information regarding localization and extent of the PVS and diffusion MRI will be used to investigate biophysical properties of the PVS fluid and surrounding tissue. The central hypothesis is that the perivascular space fluid increases across lifespan. We also hypothesize that individual differences exist in perivascular spaces as a function of demographic, general health and lifestyle health choices, such as body mass index, blood pressure, tobacco use and sleep quality. The central hypothesis will be tested by characterizing the normative map of the perivascular space fluid across the lifespan and in relation to various demographic, cognitive measures, and health factors. We will also examine whether subjects neuro-behavioral performances can be predicted by perivascular space features. We will pursue these aims by applying innovative MRI-based computational techniques that we recently developed and optimized on HCP data. We will also use Adolescent Brain Cognitive Development (ABCD) Studies to build first normative template of PVS in neurodevelopment. Together, our findings will ultimately allow for a better understanding of the human brain clearance system, and our shared perivascular space mapping workflow can provide a resource for researchers to study a wide range of neurological conditions. PROJECT NARRATIVE The proposed research is relevant to public health because it is expected to fill our gap knowledge regarding the role of the clearance system in brain health and cognition and its neuroimaging signature. We will identify and provide access to quantitative morphological and diffusion features of brain glia-lymphatic network to the larger scientific community to help facilitate our understanding of what role the brain clearance system has across lifespan. The analytical workflow and the analyzed data can also be used to study a wide range of neurological conditions.",Mapping human brain perivascular space in lifespan using human connectome project data,10012731,RF1MH123223,"['3-Dimensional', 'Adolescent', 'Affect', 'Age', 'Aging', 'Anisotropy', 'BRAIN initiative', 'Biological', 'Blood Pressure', 'Body mass index', 'Brain', 'Brain region', 'Caliber', 'Cerebrospinal Fluid', 'Cognition', 'Cognitive', 'Communities', 'Computational Technique', 'Data', 'Data Analyses', 'Data Set', 'Deposition', 'Development', 'Diffuse', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Drug Delivery Systems', 'Early Diagnosis', 'Foundations', 'Future', 'Gatekeeping', 'Goals', 'Health', 'Heterogeneity', 'Human', 'Image', 'Immune system', 'Individual Differences', 'Intercellular Fluid', 'Knowledge', 'Life', 'Life Style', 'Light', 'Liquid substance', 'Longevity', 'Lymphatic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measures', 'Modeling', 'Morphology', 'National Institute of Mental Health', 'Neuroglia', 'Neurologic', 'Outcome', 'Participant', 'Pathologic', 'Pathway interactions', 'Pediatric cohort', 'Performance', 'Physiology', 'Process', 'Property', 'Public Health', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Robin bird', 'Role', 'Sample Size', 'Sampling', 'Short-Term Memory', 'Sleep', 'Smoking', 'Statistical Data Interpretation', 'Structure', 'System', 'T2 weighted imaging', 'Techniques', 'Testing', 'Tissues', 'Tobacco use', 'Variant', 'Visual', 'Walking', 'Work', 'age related', 'base', 'biophysical properties', 'brain health', 'cardiovascular health', 'cognitive development', 'cognitive function', 'cognitive performance', 'cohort', 'connectome', 'data archive', 'glymphatic system', 'human imaging', 'in vivo', 'innovation', 'insight', 'interest', 'lifestyle factors', 'neurobehavioral', 'neurodevelopment', 'neuroimaging', 'neurovascular unit', 'novel', 'sex', 'sleep quality', 'tool', 'wasting']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,RF1,2020,1336500,-0.030823023247699668
"Computational Design of Crystal Lattice Interactions to Determine Recalcitrant Protein Structures PROJECT SUMMARY Proteins are macromolecules that carry out innumerable biological functions ranging from intracellular signaling to immune response. The specific function of a protein is determined by its three-dimensional structure. Therefore, an essential step towards understanding protein function is elucidating protein structure. However, the premier method for structure determination, X-ray crystallography, can be limited by many potential factors including the quality of the protein crystal. The proposed research will investigate the effects of protein– protein interactions in the crystal lattice on crystal quality and resolution and develop a design approach to stabilize lattice interactions, which should result in higher-quality crystals.  First, the molecular determinants of high-quality protein crystal structures will be identified by (1) curating a set of representative structures from the Protein Data Bank, (2) extracting relevant features, such as interaction energy, buried solvent accessible surface area, interfacial packing quality, and residue usage, and (3) analyzing the features’ relationship to crystal resolution (a proxy for quality). Based on these findings, a design strategy and score function for ranking designs within the Rosetta framework will be developed.  Second, the design strategy will be applied to SNase, a model protein that is easy to purify and well- behaved in crystallization and diffraction experiments. The designed proteins will be crystallized and the crystal structures will be solved, testing for improved resolution. Analysis of the resultant crystal structures will drive development of the design strategy.  Third, the design strategy will be applied on a bacterial gyrase, an antibiotic-target protein for which there are several drug-bound structures at low resolution, lacking sufficient detail to reveal key antibiotic– gyrase interactions. Preliminary data suggests that the designed gyrase mutants will yield high-resolution structures, permitting a better understanding of antibiotic–gyrase interactions, with implications for drug design.  Should the method be successful, it will be immediately applicable to ~26,000 structures in the Protein Data Bank, and countless structures that have not been published due to lack of resolution. Re-engineered, high-resolution structures of these proteins could yield structural data on molecular interactions pertinent to disease, drug development, and basic understanding of protein function. PROJECT NARRATIVE This project will develop methods to improve low-resolution crystal structures of proteins to high-resolution sufficient (1) to resolve key mechanistic features of enzyme function and disease, and (2) to design or screen drug molecules. In addition to method development, we will pursue crystal structure of M. tuberculosis DNA gyrase in complex with relevant antibiotic drugs.",Computational Design of Crystal Lattice Interactions to Determine Recalcitrant Protein Structures,9644449,F31GM123616,"['Address', 'Affect', 'Algorithm Design', 'Anti-Bacterial Agents', 'Antibiotics', 'Area', 'Basic Science', 'Benchmarking', 'Binding', 'Biochemical', 'Biological', 'Biological Process', 'Complex', 'Crystallization', 'Crystallography', 'DNA', 'DNA Gyrase', 'Data', 'Development', 'Discipline', 'Disease', 'Drug Design', 'Drug Screening', 'Engineering', 'Enzymes', 'Free Energy', 'Glean', 'Goals', 'Immune response', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Mycobacterium tuberculosis', 'Pharmaceutical Preparations', 'Pharmacology', 'Protein Engineering', 'Proteins', 'Proxy', 'Publishing', 'Research', 'Resolution', 'Side', 'Signal Transduction', 'Solvents', 'Staphylococcus aureus', 'Structure', 'Surface', 'System', 'Testing', 'X-Ray Crystallography', 'base', 'data warehouse', 'design', 'drug development', 'experimental study', 'functional gain', 'improved', 'insight', 'interfacial', 'macromolecule', 'method development', 'mutant', 'novel', 'nuclease', 'protein function', 'protein protein interaction', 'protein structure', 'research and development', 'structural biology', 'three dimensional structure', 'tool']",NIGMS,JOHNS HOPKINS UNIVERSITY,F31,2018,37308,-0.020540483232427915
"Artificial neural networks for high performance, fully automated particle tracking analysis even at low signal-to-noise regimes Abstract: Particle tracking (PT) is a powerful biophysical tool for elucidating molecular interactions, transport phenomena and rheological properties in complex biological environments. Unfortunately, PT remains a niche tool in life and physical sciences with a limited user base, in large part due to significant time and technical constraints in extracting accurate time-variant positional data from recorded movies. These constraints are exacerbated in experiments with low signal-to-noise ratios or substantial heterogeneity, as frequently encountered with nanoparticles and pathogens in biological fluids. Currently available software that attempts to automate the movie analysis process rely almost exclusively on assigning static image filters based on specific intensity, pixel size and signal-to-noise ratio thresholds. Unfortunately, when applied to actual experimental data with substantial spatial and temporal heterogeneity, the current software generally produces substantial numbers of false positives (i.e. tracking artifacts) or false negatives (i.e. missing actual traces), and frequently both. Frequent user intervention is thus required to ensure accurate tracking even when using sophisticated tracking software, markedly reducing experimental throughput and resulting in substantial user- to-user variations in analyzed data. The time required for accurate particle tracking analysis makes PT experiments exceedingly expensive compared to other commonly used experimental techniques in life sciences. These same tracking analysis limitations have effectively precluded investigators from undertaking more sophisticated 3D PT, even though the microscopy capability to obtain such movies is readily available and critical scientific insights can be gained from 3D PT. To circumvent the challenges with currently available particle tracking software, we have developed a new approach for particle identification and tracking, based on machine learning and convolutional neural networks (CNN). CNN is a type of feed-forward artificial neural network designed to process information in a layered network of connections that mimics the organization of real neural networks in the mammalian retina and visual cortex. Unlike most CNN imaging models that are trained to make predictions on static images, we have trained our CNN to input adjacent frames so that each prediction includes information from the past and future, thus effectively performing convolutions in both space and time to infer particle locations. Similar principles of image analysis are now being harnessed by developers of autonomous vehicle technologies to distinguish the motions of different objects on the road. We have applied our CNN tracking algorithm to a wide range of 2D movies capturing dynamic motions of nanoparticles, viruses and highly motile bacteria, achieving at least 30-fold time savings with virtually no need for human intervention while maintaining robust tracking performance (i.e. low false positive and low false negative rates). In this STTR proposal, we seek to focus on further optimization and testing of our neural network tracking platform for 2D PT, including the use of cloud computing (Aim 1), and extending our neural network tracker to enable accurate 3D PT (Aim 2). Our vision is to popularize PT as a research tool among researchers by minimizing the time and labor costs associated with PT analysis. Narrative Particle tracking is a powerful biophysical tool in life and physical sciences, but unfortunately its application has been strongly limited by inefficiencies in accurately extracting particle traces from raw movies. Unlike conventional particle tracking methods, we have combined artificial intelligence and machine learning to create a computational neural network that can recognize objects in much the same way as the human eye, and which consistently provided superior and truly automated tracking performance compared to current alternatives. This STTR will establish the feasibility of using our computational neural network for robust 2D and 3D particle tracking analysis.","Artificial neural networks for high performance, fully automated particle tracking analysis even at low signal-to-noise regimes",9347679,R41GM123897,"['Adopted', 'Advanced Development', 'Algorithms', 'Artificial Intelligence', 'Automobile Driving', 'Bacteria', 'Binding', 'Biological', 'Biological Neural Networks', 'Biological Sciences', 'Classification', 'Cloud Computing', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diffuse', 'Ensure', 'Environment', 'Eye', 'Future', 'Gaussian model', 'Generations', 'Goals', 'Heterogeneity', 'Human', 'Image', 'Image Analysis', 'Intervention', 'Knowledge', 'Laboratories', 'Lead', 'Life', 'Link', 'Liquid substance', 'Location', 'Machine Learning', 'Manuals', 'Measurement', 'Methods', 'Microscopy', 'Modeling', 'Morphologic artifacts', 'Motion', 'Noise', 'Performance', 'Phase', 'Photobleaching', 'Process', 'Property', 'Radial', 'Research', 'Research Personnel', 'Retina', 'Savings', 'Scientist', 'Signal Transduction', 'Small Business Technology Transfer Research', 'Software Tools', 'Spottings', 'Students', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Variant', 'Virus', 'Vision', 'Visual Cortex', 'Work', 'base', 'biophysical tools', 'cell motility', 'cloud based', 'cost', 'design', 'experimental study', 'feeding', 'field study', 'graduate student', 'improved', 'insight', 'interest', 'macromolecule', 'movie', 'nanoparticle', 'novel strategies', 'particle', 'pathogen', 'physical science', 'response', 'spatiotemporal', 'submicron', 'terabyte', 'tool', 'virtual']",NIGMS,"AI TRACKING SOLUTIONS, LLC",R41,2017,224997,-0.053031490571568904
"Calculation of Percent Body Fat by Analyzing Virtual Body Models ﻿    DESCRIPTION (provided by applicant):  Excess body fat is a key underlying factor in the development of numerous chronic diseases, including type II diabetes, heart disease, stroke, and cancer. The AMA recently declared that obesity, itself, is a disease. Most epidemiologic studies utilize Body Mass Index (BMI) to classify people as underweight, normal, overweight, or obese because it is a convenient and simple method that has been shown to correlate with disease risk. Since the majority of the health risks associated with obesity are more directly linked to an overabundance of body fat than weight, measuring body fat is essential for more precise guidelines. However, accurate methods of assessing body fat are expensive, inconvenient, and require immobile equipment. Consequently, the AMA has called for more cost effective and convenient methods to assess body composition to assist doctors in their assessment and treatment. Virtual modeling of humans in particular has provided ways to scan and analyze the body and its motion. Supervised Machine Learning (SML), a sub-field of artificial intelligence, has made great progress in taking measured data to infer new relationships. It is our belief that virtual modeling and SML can provide the techniques necessary to conveniently and accurately calculate the percentage of body fat (%BF) and to provide new tools in treating obesity based on body shapes. The project will develop a system that uses commercially available depth cameras such as the Microsoft Kinect(r) to capture the surface of the human body. This will be accomplished by developing a new algorithm to perform deformable registration of several RGB-Depth views of the body. A new algorithm that uses SML will be developed to calculate percentage body fat using the surface data. The system will be trained and validated by collecting data from a number of subjects. The surface captured will be used to explore the role of visual body representation in motivation and adherence. The developed systems can be implemented in clinical or personal settings and be utilized as a public health research tool and deployed widely given the low-cost of the hardware required. In addition to the immediate impact that the system will have on managing obesity, the project will have a broad impact on a number of areas. A large database of such shapes captured over time may lead to ways to predict how an individual's body shape will change given a particular intervention. Certain medical conditions that result in body shape change, such as those involving lymphatic circulations, may be diagnosed and tracked more easily. Growth patterns of children may be tracked by change of body shapes. Further research can be conducted to determine the effect of body shape on %BF using data mining techniques. PUBLIC HEALTH RELEVANCE: The project will develop a new method to capture the 3D surface and shape of a human body and a new method to use these data to calculate percent body fat. By making these tools widely available and economical, the proposed approach has the potential for major contributions in the assessment and treatment of obesity.",Calculation of Percent Body Fat by Analyzing Virtual Body Models,9099872,R21HL124443,"['Adherence', 'Age', 'Air', 'Algorithms', 'Area', 'Artificial Intelligence', 'Behavior Therapy', 'Belief', 'Body Composition', 'Body Size', 'Body Surface', 'Body Weight decreased', 'Body fat', 'Body mass index', 'Child', 'Chronic Disease', 'Client', 'Clinical', 'Clinical Research', 'Data', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Epidemiologic Studies', 'Equipment', 'Fatty acid glycerol esters', 'Future', 'Goals', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Heart Diseases', 'Human', 'Human body', 'Imagery', 'Incentives', 'Individual', 'Intervention', 'Lead', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Motivation', 'Muscle', 'Non-Insulin-Dependent Diabetes Mellitus', 'Obesity', 'Outcome', 'Overweight', 'Participant', 'Patients', 'Pattern', 'Perception', 'Plethysmography', 'Public Health', 'Reporting', 'Research', 'Risk', 'Role', 'Scanning', 'Self Perception', 'Series', 'Shapes', 'Stroke', 'Surface', 'System', 'Techniques', 'Technology', 'Time', 'Training', 'Underweight', 'Variant', 'Visual', 'Water', 'Weight', 'Weights and Measures', 'base', 'body density', 'body volume', 'cost', 'cost effective', 'data mining', 'density', 'disorder risk', 'lymphatic circulation', 'novel', 'novel strategies', 'obesity treatment', 'preference', 'public health research', 'reconstruction', 'sex', 'study population', 'tool', 'virtual', 'weight loss intervention']",NHLBI,GEORGE WASHINGTON UNIVERSITY,R21,2016,194340,-0.01172920546407223
"Calculation of Percent Body Fat by Analyzing Virtual Body Models ﻿    DESCRIPTION (provided by applicant):  Excess body fat is a key underlying factor in the development of numerous chronic diseases, including type II diabetes, heart disease, stroke, and cancer. The AMA recently declared that obesity, itself, is a disease. Most epidemiologic studies utilize Body Mass Index (BMI) to classify people as underweight, normal, overweight, or obese because it is a convenient and simple method that has been shown to correlate with disease risk. Since the majority of the health risks associated with obesity are more directly linked to an overabundance of body fat than weight, measuring body fat is essential for more precise guidelines. However, accurate methods of assessing body fat are expensive, inconvenient, and require immobile equipment. Consequently, the AMA has called for more cost effective and convenient methods to assess body composition to assist doctors in their assessment and treatment. Virtual modeling of humans in particular has provided ways to scan and analyze the body and its motion. Supervised Machine Learning (SML), a sub-field of artificial intelligence, has made great progress in taking measured data to infer new relationships. It is our belief that virtual modeling and SML can provide the techniques necessary to conveniently and accurately calculate the percentage of body fat (%BF) and to provide new tools in treating obesity based on body shapes. The project will develop a system that uses commercially available depth cameras such as the Microsoft Kinect(r) to capture the surface of the human body. This will be accomplished by developing a new algorithm to perform deformable registration of several RGB-Depth views of the body. A new algorithm that uses SML will be developed to calculate percentage body fat using the surface data. The system will be trained and validated by collecting data from a number of subjects. The surface captured will be used to explore the role of visual body representation in motivation and adherence. The developed systems can be implemented in clinical or personal settings and be utilized as a public health research tool and deployed widely given the low-cost of the hardware required. In addition to the immediate impact that the system will have on managing obesity, the project will have a broad impact on a number of areas. A large database of such shapes captured over time may lead to ways to predict how an individual's body shape will change given a particular intervention. Certain medical conditions that result in body shape change, such as those involving lymphatic circulations, may be diagnosed and tracked more easily. Growth patterns of children may be tracked by change of body shapes. Further research can be conducted to determine the effect of body shape on %BF using data mining techniques.         PUBLIC HEALTH RELEVANCE: The project will develop a new method to capture the 3D surface and shape of a human body and a new method to use these data to calculate percent body fat. By making these tools widely available and economical, the proposed approach has the potential for major contributions in the assessment and treatment of obesity.        ",Calculation of Percent Body Fat by Analyzing Virtual Body Models,8970310,R21HL124443,"['Adherence', 'Age', 'Air', 'Algorithms', 'Area', 'Artificial Intelligence', 'Behavior Therapy', 'Belief', 'Body Composition', 'Body Size', 'Body Surface', 'Body Weight decreased', 'Body fat', 'Body mass index', 'Child', 'Chronic Disease', 'Client', 'Clinical', 'Clinical Research', 'Data', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Epidemiologic Studies', 'Equipment', 'Fatty acid glycerol esters', 'Future', 'Goals', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Heart Diseases', 'Human', 'Human body', 'Imagery', 'Incentives', 'Individual', 'Intervention', 'Lead', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Monitor', 'Motion', 'Motivation', 'Muscle', 'Non-Insulin-Dependent Diabetes Mellitus', 'Obesity', 'Outcome', 'Overweight', 'Participant', 'Patients', 'Pattern', 'Perception', 'Plethysmography', 'Population Study', 'Public Health', 'Reporting', 'Research', 'Risk', 'Role', 'Scanning', 'Self Perception', 'Series', 'Shapes', 'Stroke', 'Surface', 'System', 'Techniques', 'Technology', 'Time', 'Training', 'Underweight', 'Variant', 'Visual', 'Water', 'Weight', 'Weights and Measures', 'base', 'body density', 'body volume', 'cost', 'cost effective', 'data mining', 'density', 'disorder risk', 'lymphatic circulation', 'novel', 'novel strategies', 'obesity treatment', 'preference', 'public health relevance', 'public health research', 'reconstruction', 'sex', 'tool', 'virtual', 'weight loss intervention']",NHLBI,GEORGE WASHINGTON UNIVERSITY,R21,2015,227680,-0.01172920546407223
"Combined Topological and Machine Learning Tools for Neuroscience Two major recent advances have raised the possibility of fundamental breakthroughs in both basic and clinical neuroscience: the development of new tools to probe the nervous system with single-cell resolution as well as brain-wide scope, and breakthroughs in machine learning methods for handling complex data. Yet there remain crucial barriers to progress: while data acquisition tools are now broadly within the grasp of neuroscience researchers, the same cannot be said about data analytical tools that can tackle the complexities of the new data sets being gathered. In addition, the highly training-data dependent, black-box Artificial Neural Network (ANN) methods that have shown rapid growth in the technological domain, are not well-suited to scientific data analysis, where transparency and understanding is more important than black-box performance measures. This proposal brings together a cross-disciplinary team of leading neuroscience and computer science researchers to develop and deploy a critical set of data analytical tools for the neuroscience community. The tools will be useful for data already gathered in major group efforts in the US Brain Initiative, and also for new data sets being acquired using the tools developed in the Initiative.  Extraction of the projection morphologies of individual neurons, and the classification and analysis of neuronal cell types is a central goal of the Brain Initiative. Because data from various sources are often analyzed with custom algorithms, scaling up existing approaches for use across large datasets and multiple data types has been a challenge. Instead researchers need a comprehensive, flexible mathematical framework that can be applied to a wide variety of data, including both static and dynamic measures. We propose to achieve this goal by combining Topological Data Analysis (TDA) methods with Deep Net based machine learning methods. Such a combined approach retains the flexibility of data-driven ANN methods while at the same time brings in conceptually well-grounded methods from TDA that are still able to address the complexities of brain-wide data sets with single-cell resolution. Aim 1 of the proposal will use these methods to automate tasks in neuroanatomy previously requiring intensive human expert effort. Aim 2 will apply the methods to single cell omics data sets. Aim 3 will deploy the tools developed to the Brain Initiative Cell Census Network and the neuroscience community. Recent years have seen many advances in experimental tools for probing brains in unprecedented ways, with single cell resolution, and as a result both individual investigators and large consortia are generating brain-wide single-cell data at an unprecedented scale. To derive full benefit from these data sets, researchers need theoretical and computational tools to analyze, visualize and derive knowledge from the data. In the proposed work, theoretically principled tools from Topological Data Analysis, in particular Discrete Morse Theory, are combined with artificial neural networks for Machine Learning, to provide a computational and analytical framework to deal with the complexity of the large-scale, high-dimensional data sets and derive the full benefits for basic as well as clinical neuroscience research.",Combined Topological and Machine Learning Tools for Neuroscience,10123310,RF1MH125317,"['Address', 'Algorithms', 'Area', 'Atlases', 'Axon', 'BRAIN initiative', 'Biological', 'Brain', 'Brain imaging', 'Categories', 'Cell Nucleus', 'Cells', 'Censuses', 'Classification', 'Clinical', 'Communities', 'Computer Analysis', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Dendrites', 'Development', 'Educational workshop', 'Epigenetic Process', 'Goals', 'Human', 'Image', 'Individual', 'Injections', 'Instruction', 'Knowledge', 'Letters', 'Light', 'Machine Learning', 'Mathematics', 'Measures', 'Methods', 'Molecular', 'Morphology', 'Nervous system structure', 'Neuroanatomy', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Performance', 'Pythons', 'Research Personnel', 'Resolution', 'Semantics', 'Skeleton', 'Software Tools', 'Source', 'Structure', 'Time', 'Tracer', 'Training', 'Trees', 'Visual', 'Work', 'analytical tool', 'artificial neural network', 'base', 'brain cell', 'cell type', 'complex data ', 'computer science', 'computerized tools', 'data acquisition', 'data centers', 'flexibility', 'grasp', 'high dimensionality', 'informatics tool', 'large datasets', 'machine learning method', 'mathematical methods', 'microscopic imaging', 'multidimensional data', 'multiple data types', 'neuronal cell body', 'rapid growth', 'repository', 'scale up', 'theories', 'tool', 'transcriptome sequencing', 'transcriptomics']",NIMH,COLD SPRING HARBOR LABORATORY,RF1,2020,2048230,-0.003650790783419577
"MELD: accelerating MD modeling of proteins using Bayesian inference PROJECT SUMMARY This proposal is to develop MELD, a computational Bayesian accelerator that “melds” together molecular dynamics simulations with external knowledge. It is novel in harnessing information that has not been usable before – because it is too sparse, noisy, ambiguous, combinatoric, or too corrupted for traditional approaches. In contrast to the high-certainty restraints traditionally used in MD simulations, MELD leverages a much broader range of real-world high-uncertainty restraints. The first specific aim is to incorporate such information in protein structure determination, in several collaboration projects with experimentalists who perform solution x-ray scattering, ESR, and high-throughput alanine scanning structures of peptide protein complexes. The second aim is to also harness information about processes, trajectories, and dynamic routes to speed the identification of protein states. MELD promises to extend physics-based simulations for determining larger protein structures, for folding larger proteins, for binding more flexible ligands, and for exploring larger mechanistic actions, than current MD simulation methods can handle. PROJECT NARRATIVE Biomedical research and pharmaceutics depend on detailed understanding of the structures and motions of proteins. Molecular dynamics simulations provide the most detailed descriptions possible, however they cannot yet describe average to large sized protein structures or motions within a reasonable time frame. We propose to develop a new physics-based computational accelerator for Molecular Dynamics, called MELD, which incorporates many types of relevant external information that was too vague and difficult to compute to have been practically useful before.",MELD: accelerating MD modeling of proteins using Bayesian inference,9848587,R01GM125813,"['Affinity', 'Alanine', 'Automobile Driving', 'Bayesian Analysis', 'Bayesian Method', 'Binding', 'Binding Proteins', 'Bioinformatics', 'Biomedical Research', 'Collaborations', 'Combinatorics', 'Complex', 'Computers', 'Crystallization', 'Data', 'Event', 'Goals', 'Homology Modeling', 'Knowledge', 'Label', 'Laws', 'Learning', 'Ligands', 'Metagenomics', 'Methods', 'Modeling', 'Molecular Computations', 'Molecular Conformation', 'Motion', 'Pharmacy (field)', 'Physics', 'Process', 'Proteins', 'Psychological reinforcement', 'Resolution', 'Roentgen Rays', 'Route', 'Sampling', 'Scanning', 'Source', 'Speed', 'Structure', 'Supercomputing', 'System', 'TP53 gene', 'Testing', 'Time', 'Uncertainty', 'Virginia', 'base', 'blind', 'deep learning', 'experimental study', 'flexibility', 'improved', 'insight', 'molecular dynamics', 'novel', 'peptide structure', 'protein complex', 'protein structure', 'protein structure prediction', 'restraint', 'scale up', 'simulation']",NIGMS,STATE UNIVERSITY NEW YORK STONY BROOK,R01,2020,317007,-0.028688878177256603
"MELD: accelerating MD modeling of proteins using Bayesian inference PROJECT SUMMARY This proposal is to develop MELD, a computational Bayesian accelerator that “melds” together molecular dynamics simulations with external knowledge. It is novel in harnessing information that has not been usable before – because it is too sparse, noisy, ambiguous, combinatoric, or too corrupted for traditional approaches. In contrast to the high-certainty restraints traditionally used in MD simulations, MELD leverages a much broader range of real-world high-uncertainty restraints. The first specific aim is to incorporate such information in protein structure determination, in several collaboration projects with experimentalists who perform solution x-ray scattering, ESR, and high-throughput alanine scanning structures of peptide protein complexes. The second aim is to also harness information about processes, trajectories, and dynamic routes to speed the identification of protein states. MELD promises to extend physics-based simulations for determining larger protein structures, for folding larger proteins, for binding more flexible ligands, and for exploring larger mechanistic actions, than current MD simulation methods can handle. PROJECT NARRATIVE Biomedical research and pharmaceutics depend on detailed understanding of the structures and motions of proteins. Molecular dynamics simulations provide the most detailed descriptions possible, however they cannot yet describe average to large sized protein structures or motions within a reasonable time frame. We propose to develop a new physics-based computational accelerator for Molecular Dynamics, called MELD, which incorporates many types of relevant external information that was too vague and difficult to compute to have been practically useful before.",MELD: accelerating MD modeling of proteins using Bayesian inference,9618886,R01GM125813,"['Affinity', 'Alanine', 'Automobile Driving', 'Bayesian Analysis', 'Bayesian Method', 'Binding', 'Binding Proteins', 'Bioinformatics', 'Biomedical Research', 'Collaborations', 'Combinatorics', 'Complex', 'Computers', 'Crystallization', 'Data', 'Event', 'Goals', 'Homology Modeling', 'Knowledge', 'Label', 'Laws', 'Learning', 'Ligands', 'Metagenomics', 'Methods', 'Modeling', 'Molecular Computations', 'Molecular Conformation', 'Motion', 'Pharmacy (field)', 'Physics', 'Process', 'Proteins', 'Psychological reinforcement', 'Resolution', 'Roentgen Rays', 'Route', 'Sampling', 'Scanning', 'Source', 'Speed', 'Structural Protein', 'Structure', 'Supercomputing', 'System', 'TP53 gene', 'Testing', 'Time', 'Uncertainty', 'Virginia', 'base', 'blind', 'deep learning', 'experimental study', 'flexibility', 'improved', 'insight', 'molecular dynamics', 'novel', 'peptide structure', 'protein complex', 'protein structure', 'protein structure prediction', 'restraint', 'scale up', 'simulation']",NIGMS,STATE UNIVERSITY NEW YORK STONY BROOK,R01,2019,317007,-0.028688878177256603
"MELD: accelerating MD modeling of proteins using Bayesian inference PROJECT SUMMARY This proposal is to develop MELD, a computational Bayesian accelerator that “melds” together molecular dynamics simulations with external knowledge. It is novel in harnessing information that has not been usable before – because it is too sparse, noisy, ambiguous, combinatoric, or too corrupted for traditional approaches. In contrast to the high-certainty restraints traditionally used in MD simulations, MELD leverages a much broader range of real-world high-uncertainty restraints. The first specific aim is to incorporate such information in protein structure determination, in several collaboration projects with experimentalists who perform solution x-ray scattering, ESR, and high-throughput alanine scanning structures of peptide protein complexes. The second aim is to also harness information about processes, trajectories, and dynamic routes to speed the identification of protein states. MELD promises to extend physics-based simulations for determining larger protein structures, for folding larger proteins, for binding more flexible ligands, and for exploring larger mechanistic actions, than current MD simulation methods can handle. PROJECT NARRATIVE Biomedical research and pharmaceutics depend on detailed understanding of the structures and motions of proteins. Molecular dynamics simulations provide the most detailed descriptions possible, however they cannot yet describe average to large sized protein structures or motions within a reasonable time frame. We propose to develop a new physics-based computational accelerator for Molecular Dynamics, called MELD, which incorporates many types of relevant external information that was too vague and difficult to compute to have been practically useful before.",MELD: accelerating MD modeling of proteins using Bayesian inference,9422220,R01GM125813,"['Affinity', 'Alanine', 'Automobile Driving', 'Bayesian Analysis', 'Bayesian Method', 'Binding', 'Binding Proteins', 'Bioinformatics', 'Biomedical Research', 'Collaborations', 'Combinatorics', 'Complex', 'Computers', 'Crystallization', 'Data', 'Event', 'Goals', 'Homology Modeling', 'Knowledge', 'Label', 'Laws', 'Learning', 'Ligands', 'Metagenomics', 'Methods', 'Modeling', 'Molecular Computations', 'Molecular Conformation', 'Motion', 'Pharmacy (field)', 'Physics', 'Process', 'Proteins', 'Psychological reinforcement', 'Resolution', 'Roentgen Rays', 'Route', 'Sampling', 'Scanning', 'Source', 'Speed', 'Structure', 'Supercomputing', 'System', 'TP53 gene', 'Testing', 'Time', 'Uncertainty', 'Virginia', 'base', 'blind', 'computer based statistical methods', 'deep learning', 'experimental study', 'flexibility', 'improved', 'insight', 'molecular dynamics', 'novel', 'peptide structure', 'protein complex', 'protein structure', 'protein structure prediction', 'restraint', 'scale up', 'simulation']",NIGMS,STATE UNIVERSITY NEW YORK STONY BROOK,R01,2018,316515,-0.028688878177256603
"Electrical connectomics: an innovative approach for dissecting brain mechanisms underlying behavioral states Project Summary: Understanding the mechanisms by which the brain encodes behavior presents a major challenge for developing effective therapies with which to treat neurological and psychiatric disorders. The development of brain-wide measurements of neural connectivity in mammalian models holds great potential for overcoming this challenge. Here we propose an innovative approach for collecting and integrating such data across an unprecedented number of interconnected brain regions for use in elucidating the mechanisms by which sensory processing is altered in disease. A number of neurological and psychiatric disorders are triggered or exacerbated by sensory stimuli, yet little is understood about the brain connectivity underlying such sensory hyper/hypo- sensitization. Sensory processing plays a major role in the pathology of: autism spectrum disorders (ASD), schizophrenia, fibromyalgia, attention deficit hyperactivity disorder (ADHD), sensory perception disorders (SPDs), and migraine. Migraine in particular represents a compelling model of sensory hypersensitization, as the response to sensory stimuli is clear, dose- dependent, and measurable. Using state-of-the-art, multi-site in vivo recordings in a well- characterized migraine model, coupled with machine learning, we will develop network-wide electrical maps of the sensory hypersensitivity that underlies migraine. These networks will be validated for their roles in migraine using multiple behavioral assays and migraine-related pharmacological manipulations. We will additionally dissect the mechanisms underlying the sensory hypersensitivity brain state in a mouse model of migraine using optogenetic circuit manipulations as well as single-cell RNA-Seq, with the aim of identifying the contributions of specific circuits, cells, and molecules to this state. This approach is expected to substantially facilitate the use of neural oscillation-based brain networks in biomedical research, as well as provide: 1) a tool for rapid identification of a sensory hypersensitive brain state that can be tested for mechanisms shared across disorders, 2) a map of features of electrical brain networks, which serve as strong hypotheses regarding the routes whereby sensory hypersensitivity brain networks are regulated, and 3) insight into the contributions of specific cell types and molecules to the hypersensitive brain state. Collectively, this study is expected to provide insights into the etiology of migraine and other sensory hypersensitivity disorders that will be critical to developing brain network-based therapies for these diseases. RELEVANCE TO PUBLIC HEALTH: This proposal seeks to harness a powerful approach for mapping electrical activity across the brain and to use this information to dissect the biological mechanisms underlying sensory hypersensitivity, a key aspect of many neurological and psychiatric disorders. The data collected from this study will provide a comprehensive map of electrical activity in sensory hypersensitivity in a preclinical migraine model; migraine is the most prevalent neurological disorder in the general population. These maps will then be used to identify novel brain-network based targets for the development of new therapeutics, establishing a unique approach that could be applied broadly to many brain disorders for which effective therapies remain elusive.",Electrical connectomics: an innovative approach for dissecting brain mechanisms underlying behavioral states,10001808,DP2MH126377,"['Attention deficit hyperactivity disorder', 'Base of the Brain', 'Behavior', 'Behavioral', 'Behavioral Assay', 'Biological', 'Biomedical Research', 'Brain', 'Brain Diseases', 'Brain region', 'Cells', 'Coupled', 'Data', 'Development', 'Disease', 'Dose', 'Etiology', 'Fibromyalgia', 'General Population', 'Hypersensitivity', 'Machine Learning', 'Maps', 'Measurable', 'Measurement', 'Mental disorders', 'Migraine', 'Modeling', 'Network-based', 'Pathology', 'Perceptual Disorders', 'Pharmacology', 'Play', 'Public Health', 'Role', 'Route', 'Schizophrenia', 'Sensory', 'Site', 'Testing', 'autism spectrum disorder', 'cell type', 'effective therapy', 'hypersensitivity desensitization', 'in vivo', 'innovation', 'insight', 'mouse model', 'nervous system disorder', 'novel', 'novel therapeutics', 'optogenetics', 'pre-clinical', 'relating to nervous system', 'response', 'sensory stimulus', 'single-cell RNA sequencing', 'tool']",NIMH,UNIVERSITY OF IOWA,DP2,2020,2317500,-0.07018300539209571
"SPOTs: Optical Technologies for Instantly Quantifying Multicellular Response Profiles PROJECT SUMMARY/ABSTRACT Human organ systems require temporally and spatially coordinated multicellular actions at a macroscale to actuate, sustain, or terminate dedicated and vital functions. Cells that comprise discrete or distributed physiologic systems that fail to respond to appropriate stimuli with coordination may cause significant morbidity and often mortality. Collective and coordinated physiologic activities typically involve millions to billions of cells that may span large physical distances. Technologies for quantifying the electrical, chemical, and mechanical coupling in these multicellular systems are critically important to understanding the underlying mechanisms of disease and develop therapeutic approaches. However, no technology currently exists to quantify rapid mechanical cell responses to transmitted distal perturbations for all cells within a collection of cells. This multi- PI proposal (Chiou (contact PI) and Teitell) aims to develop a new platform imaging technology called SPOT (single pixel optical technology) for concurrent and direct measurements of cellular traction forces over a 1.0 x 1.0 cm2 field of view (FOV) with cellular spatial resolution, and a 1,000 frames/sec temporal resolution. SPOT provides a 4-order of magnitude larger FOV than conventional traction force microscopy. Cardiomyocytes (CMs) are the test bed here because of a high potential for impact in cardiovascular disease, the leading cause of mortality in the Western World. We will demonstrate the ability for SPOT to determine quantitative indices of abnormalities for human CM contraction and relaxation in healthy and diseased states. We will establish proof of concept studies in SPOT screens for small molecules that augment or affect CM contraction in desmoplakin deficient states. We will build a platform that integrates SPOT for direct contraction force measurements and Optical Mapping for electrical property measurements for sheets of CMs. This will enable, for the first time, studies of temporal and spatial electromechanical coupling behaviors for sheets of CMs at single cell resolution. We will distinguish different subtypes of CMs, their distributions, their interactions, and their phenotypic responses under external perturbations. And we will apply this platform to investigate the structural and electromechanical coupling properties of hESC-derived CMs by integrating quantitative biomass and stiffness data measured using non-invasive live cell interferometry (LCI). Changes in biomass and cell stiffness are druggable biophysical parameters with correlates to mechanical contraction/relaxation cycles of CMs. In addition to detailed studies of CMs that have the potential to impact the number one killer of US citizens, SPOT applications should have utility and provide new insights in additional settings that require cell or tissue traction-force generation. Such settings could include models in a dish for wound healing, cancer cell metastasis, or models of diseases that affect cell and tissue structural integrity, such as connective tissue disorders Ehlers-Danlos or Marfan syndromes. PROJECT NARATIVE Our proposal is exclusively technology development but portends public health relevance because we will invent a way to quantify previously undiscoverable interactions and mechanical responses to external and internal perturbations in interconnected biological systems, as occurs in physiologic and pathologic states. We will develop, test and fine-tune a new technology platform called SPOT (Single Pixel Optical Technology) to extract mechanical responses at cellular resolution in a very wide field, in real-time, concurrently for all cells in a sheet to enable studies and potentially new-age therapeutics that are currently impossible.",SPOTs: Optical Technologies for Instantly Quantifying Multicellular Response Profiles,9972477,R01GM127985,"['Address', 'Affect', 'Age', 'Area', 'Arrhythmogenic Right Ventricular Dysplasia', 'Beds', 'Behavior', 'Biomass', 'Cardiac', 'Cardiac Myocytes', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Collection', 'Color', 'Connective Tissue Diseases', 'Coupling', 'Data', 'Defect', 'Development', 'Disease', 'Disease model', 'Distal', 'Drug Screening', 'Electrophysiology (science)', 'Fibroblasts', 'Future', 'Generations', 'Genes', 'Genetic Diseases', 'Giant Cells', 'Goals', 'Heart Atrium', 'Human', 'Imaging technology', 'Interferometry', 'Left', 'Machine Learning', 'Marfan Syndrome', 'Measurement', 'Measures', 'Mechanics', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Neoplasm Metastasis', 'Optics', 'Pathologic', 'Pharmacology Study', 'Phenotype', 'Physiological', 'Population', 'Process', 'Property', 'Relaxation', 'Reporting', 'Resolution', 'Series', 'Spottings', 'Stimulus', 'Structure', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Traction', 'Traction Force Microscopy', 'Transplantation', 'Ventricular', 'Western World', 'biological systems', 'biophysical properties', 'body system', 'cancer cell', 'design', 'desmoplakin', 'electrical measurement', 'electrical property', 'human embryonic stem cell', 'human pluripotent stem cell', 'imaging platform', 'improved', 'indexing', 'insight', 'instrument', 'mechanical properties', 'mortality', 'new technology', 'patch clamp', 'prospective', 'public health relevance', 'regenerative', 'response', 'screening', 'small molecule', 'technology development', 'temporal measurement', 'tool', 'wound healing']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,530374,-0.018570585767882702
"Development of label-free computational flow cytometry for high-throughput micro-organism classification The purpose of flow cytometers is to enable the classification of cells or organisms at high throughput. Label-free optical flow cytometers not based on fluorescence are generally based on scattering. The most common of these compares the amount of forward (FS) versus side (SS) scattering. Such two-parameter information permits rudimentary classification based on size or granularity, but it misses more subtle features that can be critical in defining organism identity. Nevertheless, FS/SS flow cytometry remains popular, largely because of its simplicity and capacity for high throughput.  We propose to develop a label-free computational flow cytometer that preserves much of the simplicity and high-throughput capacity of FS/SS flow cytometry, but provides significantly enhanced information. Instead of characterizing organisms based on scattering direction (as does FS/SS flow cytometry), we will characterize based on scattering patterns. We will insert a reconfigurable diffractive element in the imaging optics of a flow cytometer to route user-defined basis patterns to independent detectors. The basis patterns will be optimally matched to specific sample features. The respective weights of these basis patterns will serve as signatures to identify organisms of interest. The basis patterns themselves will be determined by machine learning algorithms. Both the device and the learning algorithms will be developed from scratch.  We anticipate that our flow cytometer will be able to operate at flow rates on the order of meters per second, commensurate with state-of-the-art FS/SS flow cytometers, while providing significantly more information for improved classification capacity. While our technique should be advantageous for any label-free flow cytometry application requiring high throughput, we will test it here by demonstrating high-throughput classification of microbial communities. NARRATIVE Our goal is to improve the information extraction capacity of label-free flow cytometers, while maintaining high throughput capacity. As such, our device should have a broad range of applications.",Development of label-free computational flow cytometry for high-throughput micro-organism classification,9510096,R21GM128020,"['Address', 'Algorithms', 'Awareness', 'Bioinformatics', 'Biological', 'Biology', 'Categories', 'Cells', 'Classification', 'Communities', 'Custom', 'Detection', 'Development', 'Devices', 'Elements', 'Flow Cytometry', 'Fluorescence', 'Goals', 'Image', 'Image Compression', 'Label', 'Learning', 'Light', 'Machine Learning', 'Measurement', 'Microbe', 'Modernization', 'Optics', 'Organism', 'Pattern', 'Performance', 'Pupil', 'Resolution', 'Route', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Techniques', 'Testing', 'Traction', 'Validation', 'Weight', 'base', 'cellular imaging', 'cost', 'cost effective', 'design', 'detector', 'improved', 'interest', 'meter', 'microbial community', 'microorganism', 'microorganism classification', 'optical imaging', 'prototype', 'recruit']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R21,2018,239527,-0.0340888433496371
"Development of label-free computational flow cytometry for high-throughput micro-organism classification The purpose of flow cytometers is to enable the classification of cells or organisms at high throughput. Label-free optical flow cytometers not based on fluorescence are generally based on scattering. The most common of these compares the amount of forward (FS) versus side (SS) scattering. Such two-parameter information permits rudimentary classification based on size or granularity, but it misses more subtle features that can be critical in defining organism identity. Nevertheless, FS/SS flow cytometry remains popular, largely because of its simplicity and capacity for high throughput.  We propose to develop a label-free computational flow cytometer that preserves much of the simplicity and high-throughput capacity of FS/SS flow cytometry, but provides significantly enhanced information. Instead of characterizing organisms based on scattering direction (as does FS/SS flow cytometry), we will characterize based on scattering patterns. We will insert a reconfigurable diffractive element in the imaging optics of a flow cytometer to route user-defined basis patterns to independent detectors. The basis patterns will be optimally matched to specific sample features. The respective weights of these basis patterns will serve as signatures to identify organisms of interest. The basis patterns themselves will be determined by machine learning algorithms. Both the device and the learning algorithms will be developed from scratch.  We anticipate that our flow cytometer will be able to operate at flow rates on the order of meters per second, commensurate with state-of-the-art FS/SS flow cytometers, while providing significantly more information for improved classification capacity. While our technique should be advantageous for any label-free flow cytometry application requiring high throughput, we will test it here by demonstrating high-throughput classification of microbial communities. NARRATIVE Our goal is to improve the information extraction capacity of label-free flow cytometers, while maintaining high throughput capacity. As such, our device should have a broad range of applications.",Development of label-free computational flow cytometry for high-throughput micro-organism classification,9702053,R21GM128020,"['Address', 'Awareness', 'Bioinformatics', 'Biological', 'Biology', 'Categories', 'Cells', 'Classification', 'Communities', 'Custom', 'Detection', 'Development', 'Devices', 'Elements', 'Flow Cytometry', 'Fluorescence', 'Goals', 'Image', 'Image Compression', 'Label', 'Light', 'Machine Learning', 'Measurement', 'Microbe', 'Modernization', 'Optics', 'Organism', 'Pattern', 'Performance', 'Pupil', 'Resolution', 'Route', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Techniques', 'Testing', 'Traction', 'Validation', 'Weight', 'base', 'cellular imaging', 'cost', 'cost effective', 'design', 'detector', 'improved', 'interest', 'learning algorithm', 'machine learning algorithm', 'meter', 'microbial community', 'microorganism', 'microorganism classification', 'optical imaging', 'preservation', 'prototype', 'recruit']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R21,2019,206250,-0.0340888433496371
"VISUALIZATION OF SUBCELLULAR DYNAMICS IN MULTICELLULAR ORGANISMS Abstract Contemporary fluorescence microscopy connects our understanding of molecular events from biochemistry and structural biology with their activities in living cells. Lattice light sheet microscopy (LLSM) has made it possible for us to track phenomena such as endocytic vesicle assembly or lipid kinase recruitment across an entire cell with high resolution, in both space and time, and with nearly single-molecule sensitivity. Development during the past year of lattice light sheet microscopy with adaptive optics (AO-LLSM) has overcome the optical limitations that have so far restricted most studies to individual cells in culture, allowing us to achieve comparable resolution and sensitivity in the complex optical environment of an intact, living, multicellular organism. It promises to bridge the gap between cells and organisms, through high sensitivity, volumetric imaging, with diffraction-limited resolution, of living tissues and developing embryos. We propose a research program in three overlapping stages: implementation of AO-LLSM (in collaboration with its developer), development of the new kinds of visualization and analysis software required by the scale and complexity of the datasets, and use of AO-LLSM to solve a problem in vertebrate development. To meet the computational challenges of analyzing the 4D data sets (from low signal-to-noise, the often non- punctate characteristics of the objects being studied, the temporally varying spatial complexity of the data, and the size of the data sets), we will develop new approaches using deep learning and related algorithms, with consultation from experts. As a paradigm application, we will study the consequences of Notch signaling and the related membrane-traffic and protein translocation events for cell differentiation in zebrafish early neurogenesis. AO-LLSM will for the first time allow us to relate molecular signaling events occurring on a timescale of seconds at cell interfaces to the ultimate fate of daughter cells many hours later. We therefore expect that in the course of resolving some long- standing issues in cell fate determination, we will develop microscopy approaches and computer visualization tools that are widely applicable to a range of model systems and biological questions. Narrative We will implement and apply a novel, live-cell imaging strategy (Lattice Light-Sheet Microscopy with Adaptive Optics: AO-LLSM) that spans, with diffraction-limited resolution, a range from molecules to tissues and from seconds to hours. We will use this new technology to study cell differentiation in the embryonic zebrafish brain, concentrating on how Notch-mediated signaling exerts long-range control over neuronal development. Obtaining accurate and comprehensive models of the underlying biology will require that we devise new and generalizable ways to display and analyze complex data sets, while overcoming the computational challenges posed by the low SNR of the imaging regime, the time-varying spatial complexity of the data, and the size of the data sets.",VISUALIZATION OF SUBCELLULAR DYNAMICS IN MULTICELLULAR ORGANISMS,9872183,R35GM130386,"['Algorithms', 'Biochemistry', 'Biological', 'Biological Models', 'Biology', 'Brain', 'Cell Differentiation process', 'Cells', 'Characteristics', 'Collaborations', 'Complex', 'Computer software', 'Computers', 'Consultations', 'Data', 'Data Set', 'Development', 'Embryo', 'Endocytic Vesicle', 'Environment', 'Event', 'Fluorescence Microscopy', 'Hour', 'Image', 'Individual', 'Light', 'Lipids', 'Mediating', 'Membrane Protein Traffic', 'Microscopy', 'Modeling', 'Molecular', 'Noise', 'Optics', 'Organism', 'Phosphotransferases', 'Protein translocation', 'Research', 'Resolution', 'Signal Transduction', 'Time', 'Tissues', 'Visualization', 'Visualization software', 'Zebrafish', 'adaptive optics', 'complex data ', 'daughter cell', 'deep learning', 'live cell imaging', 'neurogenesis', 'neuron development', 'new technology', 'notch protein', 'novel', 'novel strategies', 'programs', 'recruit', 'single molecule', 'structural biology']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R35,2020,442500,0.00261031815216402
"VISUALIZATION OF SUBCELLULAR DYNAMICS IN MULTICELLULAR ORGANISMS Abstract Contemporary fluorescence microscopy connects our understanding of molecular events from biochemistry and structural biology with their activities in living cells. Lattice light sheet microscopy (LLSM) has made it possible for us to track phenomena such as endocytic vesicle assembly or lipid kinase recruitment across an entire cell with high resolution, in both space and time, and with nearly single-molecule sensitivity. Development during the past year of lattice light sheet microscopy with adaptive optics (AO-LLSM) has overcome the optical limitations that have so far restricted most studies to individual cells in culture, allowing us to achieve comparable resolution and sensitivity in the complex optical environment of an intact, living, multicellular organism. It promises to bridge the gap between cells and organisms, through high sensitivity, volumetric imaging, with diffraction-limited resolution, of living tissues and developing embryos. We propose a research program in three overlapping stages: implementation of AO-LLSM (in collaboration with its developer), development of the new kinds of visualization and analysis software required by the scale and complexity of the datasets, and use of AO-LLSM to solve a problem in vertebrate development. To meet the computational challenges of analyzing the 4D data sets (from low signal-to-noise, the often non- punctate characteristics of the objects being studied, the temporally varying spatial complexity of the data, and the size of the data sets), we will develop new approaches using deep learning and related algorithms, with consultation from experts. As a paradigm application, we will study the consequences of Notch signaling and the related membrane-traffic and protein translocation events for cell differentiation in zebrafish early neurogenesis. AO-LLSM will for the first time allow us to relate molecular signaling events occurring on a timescale of seconds at cell interfaces to the ultimate fate of daughter cells many hours later. We therefore expect that in the course of resolving some long- standing issues in cell fate determination, we will develop microscopy approaches and computer visualization tools that are widely applicable to a range of model systems and biological questions. Narrative We will implement and apply a novel, live-cell imaging strategy (Lattice Light-Sheet Microscopy with Adaptive Optics: AO-LLSM) that spans, with diffraction-limited resolution, a range from molecules to tissues and from seconds to hours. We will use this new technology to study cell differentiation in the embryonic zebrafish brain, concentrating on how Notch-mediated signaling exerts long-range control over neuronal development. Obtaining accurate and comprehensive models of the underlying biology will require that we devise new and generalizable ways to display and analyze complex data sets, while overcoming the computational challenges posed by the low SNR of the imaging regime, the time-varying spatial complexity of the data, and the size of the data sets.",VISUALIZATION OF SUBCELLULAR DYNAMICS IN MULTICELLULAR ORGANISMS,9946046,R35GM130386,"['Algorithms', 'Biochemistry', 'Biological', 'Biological Models', 'Biology', 'Brain', 'Cell Differentiation process', 'Cells', 'Characteristics', 'Collaborations', 'Complex', 'Computer software', 'Computers', 'Consultations', 'Data', 'Data Set', 'Development', 'Embryo', 'Endocytic Vesicle', 'Environment', 'Event', 'Fluorescence Microscopy', 'Hour', 'Image', 'Imagery', 'Individual', 'Light', 'Lipids', 'Mediating', 'Membrane Protein Traffic', 'Microscopy', 'Modeling', 'Molecular', 'Noise', 'Optics', 'Organism', 'Phosphotransferases', 'Protein translocation', 'Research', 'Resolution', 'Signal Transduction', 'Time', 'Tissues', 'Visualization software', 'Zebrafish', 'adaptive optics', 'daughter cell', 'deep learning', 'live cell imaging', 'neurogenesis', 'neuron development', 'new technology', 'notch protein', 'novel', 'novel strategies', 'programs', 'recruit', 'single molecule', 'structural biology']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R35,2019,233580,0.00261031815216402
"VISUALIZATION OF SUBCELLULAR DYNAMICS IN MULTICELLULAR ORGANISMS Abstract Contemporary fluorescence microscopy connects our understanding of molecular events from biochemistry and structural biology with their activities in living cells. Lattice light sheet microscopy (LLSM) has made it possible for us to track phenomena such as endocytic vesicle assembly or lipid kinase recruitment across an entire cell with high resolution, in both space and time, and with nearly single-molecule sensitivity. Development during the past year of lattice light sheet microscopy with adaptive optics (AO-LLSM) has overcome the optical limitations that have so far restricted most studies to individual cells in culture, allowing us to achieve comparable resolution and sensitivity in the complex optical environment of an intact, living, multicellular organism. It promises to bridge the gap between cells and organisms, through high sensitivity, volumetric imaging, with diffraction-limited resolution, of living tissues and developing embryos. We propose a research program in three overlapping stages: implementation of AO-LLSM (in collaboration with its developer), development of the new kinds of visualization and analysis software required by the scale and complexity of the datasets, and use of AO-LLSM to solve a problem in vertebrate development. To meet the computational challenges of analyzing the 4D data sets (from low signal-to-noise, the often non- punctate characteristics of the objects being studied, the temporally varying spatial complexity of the data, and the size of the data sets), we will develop new approaches using deep learning and related algorithms, with consultation from experts. As a paradigm application, we will study the consequences of Notch signaling and the related membrane-traffic and protein translocation events for cell differentiation in zebrafish early neurogenesis. AO-LLSM will for the first time allow us to relate molecular signaling events occurring on a timescale of seconds at cell interfaces to the ultimate fate of daughter cells many hours later. We therefore expect that in the course of resolving some long- standing issues in cell fate determination, we will develop microscopy approaches and computer visualization tools that are widely applicable to a range of model systems and biological questions. Narrative We will implement and apply a novel, live-cell imaging strategy (Lattice Light-Sheet Microscopy with Adaptive Optics: AO-LLSM) that spans, with diffraction-limited resolution, a range from molecules to tissues and from seconds to hours. We will use this new technology to study cell differentiation in the embryonic zebrafish brain, concentrating on how Notch-mediated signaling exerts long-range control over neuronal development. Obtaining accurate and comprehensive models of the underlying biology will require that we devise new and generalizable ways to display and analyze complex data sets, while overcoming the computational challenges posed by the low SNR of the imaging regime, the time-varying spatial complexity of the data, and the size of the data sets.",VISUALIZATION OF SUBCELLULAR DYNAMICS IN MULTICELLULAR ORGANISMS,9626714,R35GM130386,"['Algorithms', 'Biochemistry', 'Biological', 'Biological Models', 'Biology', 'Brain', 'Cell Differentiation process', 'Cells', 'Characteristics', 'Collaborations', 'Complex', 'Computer software', 'Computers', 'Consultations', 'Data', 'Data Set', 'Development', 'Embryo', 'Endocytic Vesicle', 'Environment', 'Event', 'Fluorescence Microscopy', 'Hour', 'Image', 'Imagery', 'Individual', 'Light', 'Lipids', 'Mediating', 'Membrane Protein Traffic', 'Microscopy', 'Modeling', 'Molecular', 'Noise', 'Optics', 'Organism', 'Phosphotransferases', 'Protein translocation', 'Research', 'Resolution', 'Signal Transduction', 'Time', 'Tissues', 'Visualization software', 'Zebrafish', 'adaptive optics', 'daughter cell', 'deep learning', 'live cell imaging', 'neurogenesis', 'neuron development', 'new technology', 'notch protein', 'novel', 'novel strategies', 'programs', 'recruit', 'single molecule', 'structural biology']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R35,2019,442500,0.00261031815216402
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,10000112,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'ZIKA', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'data infrastructure', 'data modeling', 'data streams', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'heterogenous data', 'improved', 'innovation', 'mathematical methods', 'multiple data sources', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2020,365601,-0.03258702106159733
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9789907,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Infrastructure', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2019,366616,-0.03258702106159733
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9639469,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Research Infrastructure', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computer infrastructure', 'digital', 'disease transmission', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'predictive modeling', 'social', 'social media', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector', 'vector control']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2018,407175,-0.03258702106159733
"High-Throughput Computing for a Multi-Plan Framework in Radiotherapy  PROJECT SUMMARY/ABSTRACT Computerized planning for radiation delivery via either external beam radiation therapy (EBRT) or intensity- modulated radiation therapy (IMRT) from linear accelerators is a complex process involving a large amount of input data and vast numbers of decision variables. Such large-scale combinatorial optimization problems are typically intractable for conventional approaches such as the direct application of the best available commercial algorithms, and thus specialized methods that take advantage of problem structure are required. Radiation treatment planning (RTP) problems are further complicated by the fact that they are multi-objective, that is, the RTP optimization process must take into account a trade-off between the competing goals of delivering appropriate doses to the tumor and avoiding the delivery of harmful radiation to nearby healthy organs. The goal of this proposal is to harness distributive computing via the Condor system for High Throughput Computing (HTC) within an RTP environment. The specific aims for this proposal are: 1) To develop a Nested Partitions (NP) framework that guides a global search process for optimal IMRT delivery parameters using HTC. 2) To develop parallel HTC-based linear programming (LP) methods to efficiently solve the dose optimization problem in IMRT for each given set of beam angles or beam apertures. (3) To exploit a high-throughput computing (HTC) environment and the developed NP/LP/segmentation framework to efficiently generate multiple plans for each given patient case. (4) To couple this multi-plan framework with a decision support system (DSS) that includes planning surface models, a graphical-user-interface (GUI) and machine learning tools to prediction OAR complication in order to aid in the ranking and selection of the generated treatment plans. This proposal requires a multi-disciplinary approach that is best conducted within the framework of the Innovations in Biomedical Computational Science and Technology program announcement. It brings together an interdisciplinary team of investigators with expertise in medical physics, mathematical programming, industrial engineering and clinical radiation oncology that is crucial to the development of the proposed multi- plan framework using HTC in radiation therapy.  PROJECT NARRATIVE The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.",High-Throughput Computing for a Multi-Plan Framework in Radiotherapy,8271284,R01CA130814,"['Accounting', 'Algorithms', 'Behavior', 'Clinical Engineering', 'Collection', 'Complex', 'Complication', 'Computational Science', 'Data', 'Decision Support Systems', 'Dependence', 'Development', 'Dose', 'Engineering', 'Environment', 'External Beam Radiation Therapy', 'Generations', 'Genetic Programming', 'Goals', 'Intensity-Modulated Radiotherapy', 'Knowledge', 'Lead', 'Linear Accelerator Radiotherapy Systems', 'Linear Programming', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Modeling', 'Monte Carlo Method', 'NIH Program Announcements', 'Organ', 'Patients', 'Physicians', 'Physics', 'Process', 'Property', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Relative (related person)', 'Research Personnel', 'Risk', 'Sampling', 'Shapes', 'Simulate', 'Solutions', 'Speed', 'Structure', 'Surface', 'System', 'Technology', 'Time', 'Toxic effect', 'base', 'cluster computing', 'combinatorial', 'computer science', 'computerized', 'computing resources', 'direct application', 'graphical user interface', 'heuristics', 'improved', 'innovation', 'insight', 'novel', 'novel strategies', 'predictive modeling', 'process optimization', 'programs', 'research clinical testing', 'tool', 'treatment planning', 'tumor']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2012,297329,-0.0031406557614466917
"High-Throughput Computing for a Multi-Plan Framework in Radiotherapy    DESCRIPTION (provided by applicant):    Computerized planning for radiation delivery via either external beam radiation therapy (EBRT) or intensity- modulated radiation therapy (IMRT) from linear accelerators is a complex process involving a large amount of input data and vast numbers of decision variables. Such large-scale combinatorial optimization problems are typically intractable for conventional approaches such as the direct application of the best available commercial algorithms, and thus specialized methods that take advantage of problem structure are required. Radiation treatment planning (RTP) problems are further complicated by the fact that they are multi-objective, that is, the RTP optimization process must take into account a trade-off between the competing goals of delivering appropriate doses to the tumor and avoiding the delivery of harmful radiation to nearby healthy organs. The goal of this proposal is to harness distributive computing via the Condor system for High Throughput Computing (HTC) within an RTP environment. The specific aims for this proposal are: 1) To develop a Nested Partitions (NP) framework that guides a global search process for optimal IMRT delivery parameters using HTC. 2) To develop parallel HTC-based linear programming (LP) methods to efficiently solve the dose optimization problem in IMRT for each given set of beam angles or beam apertures. (3) To exploit a high-throughput computing (HTC) environment and the developed NP/LP/segmentation framework to efficiently generate multiple plans for each given patient case. (4) To couple this multi-plan framework with a decision support system (DSS) that includes planning surface models, a graphical-user-interface (GUI) and machine learning tools to prediction OAR complication in order to aid in the ranking and selection of the generated treatment plans. This proposal requires a multi-disciplinary approach that is best conducted within the framework of the Innovations in Biomedical Computational Science and Technology program announcement. It brings together an interdisciplinary team of investigators with expertise in medical physics, mathematical programming, industrial engineering and clinical radiation oncology that is crucial to the development of the proposed multi- plan framework using HTC in radiation therapy. PUBLIC HEALTH RELEVANCE: The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.           PROJECT NARRATIVE The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.",High-Throughput Computing for a Multi-Plan Framework in Radiotherapy,8077861,R01CA130814,"['Accounting', 'Algorithms', 'Behavior', 'Clinical Engineering', 'Collection', 'Complex', 'Complication', 'Computational Science', 'Data', 'Decision Support Systems', 'Dependence', 'Development', 'Dose', 'Engineering', 'Environment', 'External Beam Radiation Therapy', 'Generations', 'Genetic Programming', 'Goals', 'Health', 'Intensity-Modulated Radiotherapy', 'Knowledge', 'Lead', 'Linear Accelerator Radiotherapy Systems', 'Linear Programming', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Modeling', 'Monte Carlo Method', 'NIH Program Announcements', 'Organ', 'Patients', 'Physicians', 'Physics', 'Process', 'Property', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Relative (related person)', 'Research Personnel', 'Risk', 'Sampling', 'Shapes', 'Simulate', 'Solutions', 'Speed', 'Structure', 'Surface', 'System', 'Technology', 'Time', 'Toxic effect', 'base', 'cluster computing', 'combinatorial', 'computer science', 'computerized', 'computing resources', 'direct application', 'graphical user interface', 'heuristics', 'improved', 'innovation', 'insight', 'novel', 'novel strategies', 'predictive modeling', 'process optimization', 'programs', 'research clinical testing', 'tool', 'treatment planning', 'tumor']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2011,297451,0.0011572680481050878
"High-Throughput Computing for a Multi-Plan Framework in Radiotherapy    DESCRIPTION (provided by applicant):    Computerized planning for radiation delivery via either external beam radiation therapy (EBRT) or intensity- modulated radiation therapy (IMRT) from linear accelerators is a complex process involving a large amount of input data and vast numbers of decision variables. Such large-scale combinatorial optimization problems are typically intractable for conventional approaches such as the direct application of the best available commercial algorithms, and thus specialized methods that take advantage of problem structure are required. Radiation treatment planning (RTP) problems are further complicated by the fact that they are multi-objective, that is, the RTP optimization process must take into account a trade-off between the competing goals of delivering appropriate doses to the tumor and avoiding the delivery of harmful radiation to nearby healthy organs. The goal of this proposal is to harness distributive computing via the Condor system for High Throughput Computing (HTC) within an RTP environment. The specific aims for this proposal are: 1) To develop a Nested Partitions (NP) framework that guides a global search process for optimal IMRT delivery parameters using HTC. 2) To develop parallel HTC-based linear programming (LP) methods to efficiently solve the dose optimization problem in IMRT for each given set of beam angles or beam apertures. (3) To exploit a high-throughput computing (HTC) environment and the developed NP/LP/segmentation framework to efficiently generate multiple plans for each given patient case. (4) To couple this multi-plan framework with a decision support system (DSS) that includes planning surface models, a graphical-user-interface (GUI) and machine learning tools to prediction OAR complication in order to aid in the ranking and selection of the generated treatment plans. This proposal requires a multi-disciplinary approach that is best conducted within the framework of the Innovations in Biomedical Computational Science and Technology program announcement. It brings together an interdisciplinary team of investigators with expertise in medical physics, mathematical programming, industrial engineering and clinical radiation oncology that is crucial to the development of the proposed multi- plan framework using HTC in radiation therapy. PUBLIC HEALTH RELEVANCE: The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.           PROJECT NARRATIVE The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.",High-Throughput Computing for a Multi-Plan Framework in Radiotherapy,7845650,R01CA130814,"['Accounting', 'Algorithms', 'Behavior', 'Clinical Engineering', 'Collection', 'Complex', 'Complication', 'Computational Science', 'Data', 'Decision Support Systems', 'Dependence', 'Development', 'Dose', 'Engineering', 'Environment', 'External Beam Radiation Therapy', 'Generations', 'Genetic Programming', 'Goals', 'Intensity-Modulated Radiotherapy', 'Knowledge', 'Lead', 'Linear Accelerator Radiotherapy Systems', 'Linear Programming', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Modeling', 'Monte Carlo Method', 'NIH Program Announcements', 'Organ', 'Patients', 'Physicians', 'Physics', 'Process', 'Property', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Relative (related person)', 'Research Personnel', 'Risk', 'Sampling', 'Shapes', 'Simulate', 'Solutions', 'Speed', 'Structure', 'Surface', 'System', 'Technology', 'Time', 'Toxic effect', 'base', 'cluster computing', 'combinatorial', 'computer science', 'computerized', 'computing resources', 'direct application', 'graphical user interface', 'heuristics', 'improved', 'innovation', 'insight', 'novel', 'novel strategies', 'predictive modeling', 'process optimization', 'programs', 'public health relevance', 'research clinical testing', 'tool', 'treatment planning', 'tumor']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2010,306826,0.0011572680481050878
"High-Throughput Computing for a Multi-Plan Framework in Radiotherapy    DESCRIPTION (provided by applicant):    Computerized planning for radiation delivery via either external beam radiation therapy (EBRT) or intensity- modulated radiation therapy (IMRT) from linear accelerators is a complex process involving a large amount of input data and vast numbers of decision variables. Such large-scale combinatorial optimization problems are typically intractable for conventional approaches such as the direct application of the best available commercial algorithms, and thus specialized methods that take advantage of problem structure are required. Radiation treatment planning (RTP) problems are further complicated by the fact that they are multi-objective, that is, the RTP optimization process must take into account a trade-off between the competing goals of delivering appropriate doses to the tumor and avoiding the delivery of harmful radiation to nearby healthy organs. The goal of this proposal is to harness distributive computing via the Condor system for High Throughput Computing (HTC) within an RTP environment. The specific aims for this proposal are: 1) To develop a Nested Partitions (NP) framework that guides a global search process for optimal IMRT delivery parameters using HTC. 2) To develop parallel HTC-based linear programming (LP) methods to efficiently solve the dose optimization problem in IMRT for each given set of beam angles or beam apertures. (3) To exploit a high-throughput computing (HTC) environment and the developed NP/LP/segmentation framework to efficiently generate multiple plans for each given patient case. (4) To couple this multi-plan framework with a decision support system (DSS) that includes planning surface models, a graphical-user-interface (GUI) and machine learning tools to prediction OAR complication in order to aid in the ranking and selection of the generated treatment plans. This proposal requires a multi-disciplinary approach that is best conducted within the framework of the Innovations in Biomedical Computational Science and Technology program announcement. It brings together an interdisciplinary team of investigators with expertise in medical physics, mathematical programming, industrial engineering and clinical radiation oncology that is crucial to the development of the proposed multi- plan framework using HTC in radiation therapy. PUBLIC HEALTH RELEVANCE: The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.           PROJECT NARRATIVE The goal of this proposal is to develop a multi-dimensional platform for sophisticated treatment planning of radiation delivery. It will develop novel algorithms that will enable generation of superior treatment plans with the added advantage of increasing the speed of treatment planning. Further, it will allow physicians to know beforehand the quality of the treatment plan relative to the multiple treatment objectives and be able to determine the treatment complication scenario beforehand.",High-Throughput Computing for a Multi-Plan Framework in Radiotherapy,7736445,R01CA130814,"['Accounting', 'Algorithms', 'Behavior', 'Clinical Engineering', 'Collection', 'Complex', 'Complication', 'Computational Science', 'Data', 'Decision Support Systems', 'Dependence', 'Development', 'Dose', 'Engineering', 'Environment', 'External Beam Radiation Therapy', 'Generations', 'Genetic Programming', 'Goals', 'Intensity-Modulated Radiotherapy', 'Knowledge', 'Lead', 'Linear Accelerator Radiotherapy Systems', 'Linear Programming', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Modeling', 'Monte Carlo Method', 'NIH Program Announcements', 'Organ', 'Patients', 'Physicians', 'Physics', 'Process', 'Property', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Relative (related person)', 'Research Personnel', 'Risk', 'Sampling', 'Shapes', 'Simulate', 'Solutions', 'Speed', 'Structure', 'Surface', 'System', 'Technology', 'Time', 'Toxic effect', 'base', 'cluster computing', 'combinatorial', 'computer science', 'computerized', 'computing resources', 'direct application', 'graphical user interface', 'heuristics', 'improved', 'innovation', 'insight', 'novel', 'novel strategies', 'predictive modeling', 'process optimization', 'programs', 'public health relevance', 'research clinical testing', 'tool', 'treatment planning', 'tumor']",NCI,UNIVERSITY OF MARYLAND BALTIMORE,R01,2009,317367,0.0011572680481050878
"Computerized Visualization and Prediction of Coronary Artery Ischemia ﻿    DESCRIPTION (provided by applicant):  Coronary artery disease (CAD) is the principal cause of morbidity and mortality. In the US, CAD affects >16 million adults and accounts for >1/3 deaths annually. Recently, computed tomography (CT) has emerged as a non-invasive option for imaging coronary arteries and myocardial perfusion. Nevertheless, diagnosis of ischemic CAD based on CT alone is less robust due to the lack of lesion-specific physiologic information. Computational fluid dynamics (CFD) has been applied to image-based modeling of patient-specific geometry from CT data, which now allows for non-invasive calculation of coronary pressure, flow and shear stress, and thus lesion-specific evaluation of coronary ischemia in higher diagnostic accuracy than the strategy based on CT alone. There is a pressing need to incorporate CT imaging, image-based modeling, and CFD into clinical practice for better diagnosis of coronary ischemia. However, progress has been thwarted by three major challenges: (1) lack of integrated tools to visualize immense data to assist diagnosis, (2) inabiliy to quantify and select salient anatomic and physiologic features for optimal prediction of ischemia, and (3) lack of comprehensive clinically relevant evaluation. In this proposal, we will develop and evaluate a novel computerized system to improve visual and predictive assessment of coronary ischemia from CT imaging and CFD. To accomplish this goal: (1) We will create tools to visualize coronary anatomy, physiology and myocardial perfusion for routine diagnosis assistance; The evaluation will be performed by comparing the diagnostic performance of two experienced cardiologists using our visualization tool and the conventional workstation using invasive ground truths; (2) We will develop methods to automatically quantify and select salient anatomic and physiologic features for maximizing predictive accuracy using machine learning. The evaluation will be assessed through cross-validation on a large existing database with respect to invasive ground truths. If successful, our developments will provide a new computerized system to assist the diagnosis of coronary ischemia by visualizing the totality of anatomic and physiologic findings over current unassisted approaches, and predict ischemia by machine learning methods that are superior to current heuristic techniques, and ultimately accelerate the translation of diagnostic performance gain into routine clinical practice. PUBLIC HEALTH RELEVANCE:  Coronary artery disease (CAD) is the principal cause of morbidity and mortality. Based on computed tomography and computational fluid dynamics, our proposed work will create and evaluate a new computerized system to improve the diagnosis of ischemic CAD by developing advanced visualization and prediction tools.",Computerized Visualization and Prediction of Coronary Artery Ischemia,9264013,R21HL132277,"['Accounting', 'Adult', 'Affect', 'Anatomy', 'Arterial Fatty Streak', 'Arteries', 'Biomechanics', 'Biomedical Engineering', 'Blood flow', 'Cardiology', 'Cardiovascular Diagnostic Techniques', 'Cardiovascular system', 'Cessation of life', 'Characteristics', 'Clinical', 'Clinical Research', 'Complex', 'Computer-Assisted Diagnosis', 'Computing Methodologies', 'Coronary', 'Coronary Arteriosclerosis', 'Coronary Artery Ischemia', 'Coronary Stenosis', 'Coronary artery', 'Data', 'Databases', 'Decision Making', 'Defect', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Echocardiography', 'Evaluation', 'Foundations', 'Functional Imaging', 'Future', 'Geometry', 'Goals', 'Hospitalization', 'Image', 'Imagery', 'Imaging Techniques', 'Individual', 'Ischemia', 'Lesion', 'Liquid substance', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Methods', 'Modality', 'Modeling', 'Morbidity - disease rate', 'Morphology', 'Motion', 'Multicenter Trials', 'Myocardial perfusion', 'Patients', 'Performance', 'Physiological', 'Physiology', 'Radionuclide Imaging', 'Reproducibility', 'Rest', 'Severities', 'Stenosis', 'Stress', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Three-Dimensional Image', 'Time', 'Translations', 'Validation', 'Visual', 'Visualization software', 'Work', 'X-Ray Computed Tomography', 'base', 'burden of illness', 'cardiovascular imaging', 'cardiovascular visualization', 'clinical practice', 'clinically relevant', 'computer science', 'computerized', 'design', 'diagnostic accuracy', 'experience', 'heuristics', 'imaging Segmentation', 'imaging modality', 'improved', 'learning strategy', 'mortality', 'novel', 'nuclear imaging', 'pressure', 'prognostic value', 'prospective', 'public health relevance', 'shear stress', 'simulation', 'success', 'targeted treatment', 'tool']",NHLBI,WEILL MEDICAL COLL OF CORNELL UNIV,R21,2017,211875,-0.02240892518782962
"Computerized Visualization and Prediction of Coronary Artery Ischemia ﻿    DESCRIPTION (provided by applicant):  Coronary artery disease (CAD) is the principal cause of morbidity and mortality. In the US, CAD affects >16 million adults and accounts for >1/3 deaths annually. Recently, computed tomography (CT) has emerged as a non-invasive option for imaging coronary arteries and myocardial perfusion. Nevertheless, diagnosis of ischemic CAD based on CT alone is less robust due to the lack of lesion-specific physiologic information. Computational fluid dynamics (CFD) has been applied to image-based modeling of patient-specific geometry from CT data, which now allows for non-invasive calculation of coronary pressure, flow and shear stress, and thus lesion-specific evaluation of coronary ischemia in higher diagnostic accuracy than the strategy based on CT alone. There is a pressing need to incorporate CT imaging, image-based modeling, and CFD into clinical practice for better diagnosis of coronary ischemia. However, progress has been thwarted by three major challenges: (1) lack of integrated tools to visualize immense data to assist diagnosis, (2) inabiliy to quantify and select salient anatomic and physiologic features for optimal prediction of ischemia, and (3) lack of comprehensive clinically relevant evaluation. In this proposal, we will develop and evaluate a novel computerized system to improve visual and predictive assessment of coronary ischemia from CT imaging and CFD. To accomplish this goal: (1) We will create tools to visualize coronary anatomy, physiology and myocardial perfusion for routine diagnosis assistance; The evaluation will be performed by comparing the diagnostic performance of two experienced cardiologists using our visualization tool and the conventional workstation using invasive ground truths; (2) We will develop methods to automatically quantify and select salient anatomic and physiologic features for maximizing predictive accuracy using machine learning. The evaluation will be assessed through cross-validation on a large existing database with respect to invasive ground truths. If successful, our developments will provide a new computerized system to assist the diagnosis of coronary ischemia by visualizing the totality of anatomic and physiologic findings over current unassisted approaches, and predict ischemia by machine learning methods that are superior to current heuristic techniques, and ultimately accelerate the translation of diagnostic performance gain into routine clinical practice.         PUBLIC HEALTH RELEVANCE:  Coronary artery disease (CAD) is the principal cause of morbidity and mortality. Based on computed tomography and computational fluid dynamics, our proposed work will create and evaluate a new computerized system to improve the diagnosis of ischemic CAD by developing advanced visualization and prediction tools.        ",Computerized Visualization and Prediction of Coronary Artery Ischemia,9093071,R21HL132277,"['Accounting', 'Adult', 'Affect', 'Anatomy', 'Arterial Fatty Streak', 'Arteries', 'Biomechanics', 'Biomedical Engineering', 'Blood flow', 'Cardiology', 'Cardiovascular Diseases', 'Cardiovascular system', 'Cessation of life', 'Characteristics', 'Clinical', 'Clinical Research', 'Complex', 'Computer Simulation', 'Computer-Assisted Diagnosis', 'Computing Methodologies', 'Coronary', 'Coronary Arteriosclerosis', 'Coronary Artery Ischemia', 'Coronary Stenosis', 'Coronary artery', 'Data', 'Databases', 'Decision Making', 'Defect', 'Development', 'Diagnosis', 'Diagnostic', 'Echocardiography', 'Evaluation', 'Foundations', 'Functional Imaging', 'Future', 'Geometry', 'Goals', 'Hospitalization', 'Image', 'Imagery', 'Imaging Techniques', 'Individual', 'Ischemia', 'Lesion', 'Liquid substance', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Methods', 'Modality', 'Modeling', 'Morbidity - disease rate', 'Motion', 'Multicenter Trials', 'Myocardial perfusion', 'Nuclear', 'Patients', 'Performance', 'Physiological', 'Physiology', 'Radionuclide Imaging', 'Reproducibility', 'Rest', 'Severities', 'Stenosis', 'Stress', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Three-Dimensional Image', 'Time', 'Translations', 'Validation', 'Visual', 'Work', 'X-Ray Computed Tomography', 'base', 'burden of illness', 'cardiovascular imaging', 'cardiovascular visualization', 'clinical practice', 'clinically relevant', 'computer science', 'computerized', 'design', 'diagnostic accuracy', 'experience', 'heuristics', 'imaging modality', 'improved', 'learning strategy', 'mortality', 'novel', 'pressure', 'prognostic value', 'prospective', 'public health relevance', 'shear stress', 'simulation', 'success', 'targeted treatment', 'tool']",NHLBI,WEILL MEDICAL COLL OF CORNELL UNIV,R21,2016,254250,-0.02240892518782962
"Open Data-driven Infrastructure for Building Biomolecular Force Field for Predictive Biophysics and Drug Design PROJECT SUMMARY/ABSTRACT Molecular simulation is a powerful tool to predict the properties of biomolecules, interpret biophysical experiments, and design small molecules or biomolecules with therapeutic utility. However, a number of obstacles have impeded the development of quantitative, cloud-scale research workﬂows involving biomolecular simulation. Two main ob- stacles are the insufﬁcient accuracy of current atomistic models for biomolecules and small molecule therapeutics and the lack of interoperability in simulation toolchains used in both academic and industrial biomolecular research. Our original R01, “Open Data-driven Infrastructure for Building Biomolecular Force Fields for Predictive Bio- physics and Drug Design,” seeks to solve the ﬁrst problem. It helps fund our effort, the Open Force Field Initiative (https://openforceﬁeld.org) to develop open, extensible, and shared software and data infrastructure, implementing statistically robust methods of parameterizing force ﬁelds and choosing new force ﬁelds in a statistically sound manner. This work is designed to create not just a new generation of force ﬁelds, but an open technology to continue advancing force ﬁeld science. However, even with improved molecular models, putting together complete workﬂows of biomolecular simulations involves interfacing substantial numbers of different tools. However the majority of the existing molecular simulation workﬂows are mutually incompatible, with differing representations of the molecular models. The Open Force Field Initiative effort already includes the development of molecular data structures that we can ex- port into existing molecular simulation tools. We propose to extend the existing scope of our R01 to create an extensible common molecular simulation representation and translators to and from this representation. Such a set of tools will immediately make it signiﬁcantly easier to combine the disparate workﬂows developed for different sets of molecular simulation tools. Researchers will be able to set up and build the biophysical simulations using their usual tools, but run and analyze them with currently incompatible tools, enabling better matching of computational resources and methods to problems. It will help avoid trapping in a single software framework, and enable combinations of functionalities previously impossible without substantial developer time and effort. We will (Aim 1) work with partners to generalize our modular, extensible object model for representing parameterized biomolecular systems in a manner that accommodates the force ﬁeld terms currently supported by most popular biomolecular simulation packages. We will engineer it to be extensible to advanced interaction forms, such as polarizability and other multibody terms, and machine learning models for intermolecular forces. We will (Aim 2): enable easy conversion between components of molecular simulation workﬂows by allowing other molecular simulation packages to easily store their representations in this data model, developing converters that can import/export this object model to multiple popular ﬁle formats, focusing initially on OpenMM, AMBER, CHARMM, and GROMACS. We will demonstrate the utility of this interface in cloud-ready workﬂows. PROJECT NARRATIVE Scientists use computer simulations of proteins, DNA, and RNA, at atomic detail, to learn how these molecules of life carry out their functions and to design new medications. We aim to greatly increase the utility of all of these simulations by improving the accuracy of the formulas they use to compute the forces acting between atoms. This supplement will make it much easier for molecular simulation workﬂows to interoperate with each other in large-scale workﬂows.",Open Data-driven Infrastructure for Building Biomolecular Force Field for Predictive Biophysics and Drug Design,10166314,R01GM132386,"['Affinity', 'Binding', 'Biophysics', 'COVID-19', 'Collaborations', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'DNA', 'Development', 'Drug Design', 'Ecosystem', 'Engineering', 'Funding', 'Generations', 'Human', 'Individual', 'Industrialization', 'Infrastructure', 'Language', 'Learning', 'Libraries', 'Life', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Motion', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Problem Solving', 'Property', 'Proteins', 'Pythons', 'RNA', 'Readability', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Science', 'Scientist', 'Software Framework', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Work', 'Writing', 'biomaterial interface', 'computing resources', 'data infrastructure', 'data modeling', 'design', 'experimental study', 'file format', 'improved', 'interoperability', 'molecular modeling', 'open data', 'simulation', 'small molecule', 'small molecule therapeutics', 'software infrastructure', 'sound', 'structured data', 'tool']",NIGMS,UNIVERSITY OF COLORADO,R01,2020,225000,-0.028796529163875834
"Multiscale ab initio QM/MM and machine learning methods for accelerated free energy simulations Q-Chem is a state-of-the-art commercial computational quantum chemistry program that has aided about 60,000 users in their modeling of molecular processes in a wide range of disciplines, including biology, chemistry, and materials science. In this proposal, we seek to significantly reduce the computational time (now around 500,000 CPU hours) required to obtain accurate free energy profiles of enzymatic reactions. Specifically, we propose to use a multiple time step (MTS) simulation method, where a low-level (and less accurate) quantum chemistry method is used to propagate the system (i.e. move all atoms) at each time step (usually 0.5 or 1 fs), and then a high-level (i.e. more accurate and expensive) quantum chemistry method is used to correct the force on the atoms at longer time intervals. In this way, the simulation can be performed at the high-level energy surface in a fraction of time, compared with simulations performed only using the high-level quantum chemical method. In the Phase I proposal, our goal is to allow the high-level force update only once every 40—50 fs by identifying appropriate lower-level theories (Aim 1) and incorporating machine-learning techniques (Aim 2). This will accelerate accurate free energy simulations by 20—25 fold, reducing the overall computer time to around 25,000 CPU hours. Thus, our new MTS simulation method will make it feasible to routinely perform computational studies on enzymatic reaction mechanism. The addition of these new tools will also further strengthen Q-Chem's position as a global leader in the molecular modeling software market, making our program the most efficient and reliable computational quantum chemistry package for simulating large, complex chemical/biological systems. In this project, we seek to significantly reduce the computational time (ca. 500,000 CPU hours) required to obtain accurate free energy profiles of enzymatic reactions to ca. 25,000 CPU Hours. Building upon sophisticated quantum mechanics, this can lead to reliable and quick predictions of enzyme activities.",Multiscale ab initio QM/MM and machine learning methods for accelerated free energy simulations,9778517,R43GM133270,"['Acceleration', 'Accounting', 'Adopted', 'Back', 'Biochemical', 'Biochemical Reaction', 'Biology', 'Biomedical Research', 'Chemicals', 'Chemistry', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Computers', 'Development', 'Discipline', 'Enzymes', 'Foundations', 'Free Energy', 'Goals', 'Hour', 'Hybrids', 'Lead', 'Machine Learning', 'Maps', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular Conformation', 'Molecular Machines', 'Pathway interactions', 'Performance', 'Phase', 'Positioning Attribute', 'Potential Energy', 'Process', 'Protein Conformation', 'Proteins', 'Quantum Mechanics', 'Reaction', 'Recipe', 'Research', 'Research Personnel', 'Sampling', 'Scheme', 'Solvents', 'Surface', 'System', 'Techniques', 'Time', 'Update', 'biological systems', 'computer studies', 'cost', 'density', 'enzyme activity', 'enzyme model', 'improved', 'innovation', 'learning strategy', 'materials science', 'molecular mechanics', 'molecular modeling', 'programs', 'quantum', 'quantum chemistry', 'quantum computing', 'simulation', 'theories', 'time interval', 'tool']",NIGMS,"Q-CHEM, INC.",R43,2019,132011,-0.01835050744791087
"Unraveling subcellular heterogeneity of molecular coordination by machine learning PROJECT SUMMARY/ABSTRACT Recent advances in fluorescence microscopy allow researchers to acquire an unprecedented amount of live cell image data at high spatial and temporal resolutions. However, these images pose a significant challenge for data analyses due to massive subcellular heterogeneity. Although conventional computer vision algorithms have facilitated automatic image analysis, traditional ensemble-averaging of subcellular heterogeneity could lead to the loss of critical mechanistic details. Given the current rapid growth of cell biological data from new technological development, it is nearly impossible to keep up with the data generation if we solely rely on human intelligence for algorithm development and data analysis. Recently, machine learning (ML) is making tremendous progress and has shown that computers can outperform humans in the analysis of complex high dimensional datasets. Conventional ML application in cell biology, however, is usually limited to fixed cells or low spatial resolution setting (single cell resolution), which is limited in analyzing dynamic subcellular information. To fill this voids, we have been developing an ML framework for fluorescence live cell image analyses at the subcellular level. In our previous study, we established the method to deconvolve the subcellular heterogeneity of lamellipodial protrusion from live cell imaging, which identified distinct subcellular protrusion phenotypes with differential drug susceptibility. Thus, our goal is to advance this ML framework and address technical and cell biological challenges in the live cell analysis. The overall goal of our research is two- fold: i) advancing a new ML framework for cell biological research (technological development) and ii) applying our ML framework to integrate mechanobiology and metabolism in cell protrusion (targeted cell biological study). First, we will advance our ML framework for the deconvolution of subcellular heterogeneity of protrusion and molecular coordination in live cells. This method will integrate time-series modeling and ML to deconvolve subcellular molecular coordination. Second, we will develop deep learning based high-throughput fluorescence live cell imaging. This will include microscope automation, resolution enhancement, and data synthesis, which will build up the massive dataset for ML. Third, we will apply our ML framework to study the mechanosensitivity of subcellular bioenergetic status in cell protrusion. We will evaluate how AMPK reacts to mechanical forces and controls the subcellular organization of actin assembly and mitochondria to promote energy-demanding protrusion phenotypes. Our ML framework will bring unprecedented analytical power to cell biology by analyzing a large numbers of individual cells at the high spatial resolution and automatically extracting a multitude of subcellular phenotypes. This framework can be applied to various areas of cell biology such as cytoskeleton, membrane remodeling, and membrane-bound organelles. PROJECT NARRATIVE We propose to develop a novel machine learning framework for automated, large-scale analyses of single cells at the subcellular level. By integrating time-series modeling and machine learning, this new system will enable us to identify hidden phenotypes and molecular coordination from live cell movies. We will apply the developed technology to study the interplay between mechanical forces and metabolism in cell protrusion.",Unraveling subcellular heterogeneity of molecular coordination by machine learning,9797909,R35GM133725,"['Actins', 'Address', 'Algorithms', 'Area', 'Automation', 'Bioenergetics', 'Biological', 'Cells', 'Cellular biology', 'Complex', 'Computer Vision Systems', 'Computers', 'Cytoskeleton', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Fluorescence', 'Fluorescence Microscopy', 'Generations', 'Goals', 'Heterogeneity', 'Human', 'Image', 'Image Analysis', 'Individual', 'Intelligence', 'Lead', 'Machine Learning', 'Membrane', 'Metabolism', 'Methods', 'Microscope', 'Mitochondria', 'Modeling', 'Molecular', 'Organelles', 'Pharmaceutical Preparations', 'Phenotype', 'Predisposition', 'Research', 'Research Personnel', 'Resolution', 'Series', 'System', 'Technology', 'Time', 'base', 'biological research', 'cell growth', 'deep learning', 'high dimensionality', 'live cell imaging', 'mechanical force', 'movie', 'novel', 'rapid growth', 'single cell analysis', 'temporal measurement']",NIGMS,WORCESTER POLYTECHNIC INSTITUTE,R35,2019,232688,0.007839912110006427
"Building protein structure models for intermediate resolution cryo-electron microscopy maps Project Summary Cryo-electron microscopy (cryo-EM) is an emerging technique in structural biology, which is capable of determining three-dimensional (3D) structures of biological macromolecules. Compared to conventional structural biology techniques, such as X-ray crystallography and NMR, a major advantage of cryo-EM is its ability to solve large macromolecular assemblies. Moreover, recent technical breakthroughs in cryo-EM have enabled determination of 3D structures at nearly atomic-level resolutions. Cryo-EM will undoubtedly become a method of central importance in structural biology in the next decade. With the rapid accumulation of cryo-EM structure data, it has become crucial to develop computational methods that can effectively build and extract 3D structures of biological macromolecules from EM maps. The goal of this project is to develop computational methods for modeling both global and local structures and for interpreting 3D structures embedded in EM maps of around 4 Å to medium-resolution. Recently, we have developed a new de novo protein structure modeling method, MAINMAST, which can model protein structures from an EM density map without using existing template or fragment structures on the map. Based on the successful development of MAINMAST, we further extend the capability of MAINMAST toward more accurate modeling and for multiple-chain modeling. In addition, we will also develop novel modeling methods for medium-resolution EM maps, which combine a coarse-grained protein structure modeling technique, methods in protein structure prediction, and a low- resolution image processing approach with deep learning, a state-of-the-art powerful machine learning method. The proposed project capitalizes on the tremendous efforts and progress made in structural determination with cryo-EM by developing computational tools that allow researchers to perform efficient and reliable structure analyses for 3D EM density maps. The project will greatly facilitate investigation into the molecular mechanisms of macromolecule function by providing an efficient means of 3D structure modeling. Narrative Cryo-electron microscopy is an emerging technique for determining the three-dimensional structure of biological macromolecules. We propose to develop computational methods that can accurately and effectively model and interpret structures of biomolecules embedded in electron microscopy density maps and thereby facilitate the understanding of molecular mechanisms of protein function and disease.",Building protein structure models for intermediate resolution cryo-electron microscopy maps,9971996,R01GM133840,"['3-Dimensional', 'Algorithms', 'Amino Acids', 'Area', 'Biological', 'Cells', 'Chimera organism', 'Code', 'Communication', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Cryoelectron Microscopy', 'DNA', 'Detection', 'Development', 'Disease', 'Electron Microscopy', 'Goals', 'Grain', 'Human', 'Intervention', 'Investigation', 'Knowledge', 'Ligands', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Conformation', 'Multiprotein Complexes', 'Nature', 'Positioning Attribute', 'Preparation', 'Protein Region', 'Proteins', 'Publishing', 'RNA', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Side', 'Source Code', 'Structural Models', 'Structure', 'Techniques', 'Three-Dimensional Image', 'Three-dimensional analysis', 'Visualization software', 'X-Ray Crystallography', 'base', 'computerized tools', 'data acquisition', 'deep learning', 'density', 'graphical user interface', 'image processing', 'improved', 'machine learning method', 'macromolecular assembly', 'macromolecule', 'model building', 'novel', 'programs', 'protein function', 'protein structure', 'protein structure prediction', 'repository', 'software development', 'structural biology', 'structured data', 'three dimensional structure', 'tool']",NIGMS,PURDUE UNIVERSITY,R01,2020,305459,0.005499439650225654
"Novel machine learning approaches for improving structural discrimination in cryo-electron tomography Project Summary Cellular cryo-electron tomography (Cryo-ET) has made possible the observation of cellular organelles and macromolecular complexes at nanometer resolution with native conformations. The rapid increasing amount of Cryo-ET data available however brings along some major challenges for analysis which we will timely ad- dress in this proposal. We will design novel data-driven machine learning algorithms for improving structural discrimination and resolution. In particular, we have the following speciﬁc aims: (1) We will develop a novel Autoencoder and Iterative region Matching (AIM) algorithm for marker-free alignment of image tilt-series to re- construct tomograms with improved resolution; (2) We will develop a saliency-based auto-picking algorithm for better detecting macromolecular complexes, and combine it with an innovative 2D-to-3D framework to further improve structure detection accuracy; (3) We will design an end-to-end convolutional model for pose-invariant clustering of subtomograms. This model will produce an initial clustering which will be reﬁned by a new subto- mogram averaging algorithm that automatically down-weights subtomograms of noise and little contribution; (4) We will perform experimental evaluations by using previously reported bacterial secretion systems and mito- chondrial ultrastructures datasets to improve the ﬁnal resolution. Implementing algorithms in Aims 1-3, we will develop a user-friendly open-source graphical user interface -tom to directly beneﬁt the scientiﬁc community.  -tom will be systematically compared with existing software including IMOD, EMAN2, and Relion on simulated and benchmark datasets. To facilitate distribution, -tom will be integrated into existing software platforms Sci- pion and TomoMiner. Our data-driven algorithms and software not only will facilitate and accelerate the future use of Cryo-ET, but also can be readily used on analyzing the existing large amounts of Cryo-ET data to im- prove our understanding of the structure, function, and spatial organization of macromolecular complexes in situ. Project Narrative This project will create a system of machine learning algorithms to accelerate and facilitate the use and re-use of the rapidly accumulating Cryo-ET datasets. For easy use, we will develop an open-source GUI -Tom (to be disseminated into the Scipion and TomoMiner software platforms) that streamlines the new approaches from the initial tomogram reconstruction step to the ﬁnal subtomogram averaging step. We will validate the performance of our system by applying it on published Cryo-ET datasets and monitor the improvement of the ﬁnal results.",Novel machine learning approaches for improving structural discrimination in cryo-electron tomography,9973462,R01GM134020,"['3-Dimensional', 'Algorithmic Software', 'Algorithms', 'Back', 'Benchmarking', 'Biological Process', 'Cells', 'Communities', 'Computer Analysis', 'Computer software', 'Cryo-electron tomography', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Discrimination', 'Evaluation', 'Future', 'Gaussian model', 'Group Structure', 'Hour', 'Image', 'In Situ', 'Knowledge', 'Laplacian', 'Literature', 'Machine Learning', 'Macromolecular Complexes', 'Manuals', 'Methods', 'Mitochondria', 'Modeling', 'Molecular Conformation', 'Monitor', 'Neurophysiology - biologic function', 'Noise', 'Organelles', 'Performance', 'Process', 'Publishing', 'Reporting', 'Resolution', 'Series', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Tomogram', 'Weight', 'Work', 'autoencoder', 'automated algorithm', 'base', 'deep learning', 'design', 'falls', 'feature detection', 'graphical user interface', 'improved', 'innovation', 'insight', 'machine learning algorithm', 'nano', 'nanometer resolution', 'novel', 'novel strategies', 'open source', 'particle', 'pi-Mesons', 'programs', 'reconstruction', 'success', 'user-friendly']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2020,342970,0.0156924863256192
"Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence Abstract Enzyme functionality is a critical component of all life systems. Whereas advances in experimental methodology have enabled a better understanding of factors that control enzyme function, critical components of the reaction space such as highly unstable intermediates and transition states are best accessed for evaluation through computational simulations. Similarly, computational methodology continues to provide a key resource for probing excited-state processes such as bioluminescence. Combined ab initio quantum mechanical molecular mechanical (ai-QM/MM) simulations are, in principle, the preferred choice in the modeling of both processes. But ai-QM/MM modeling of enzymatic reactions is now severely limited by its computational cost, where a direct ai-QM/MM free energy simulation of an enzymatic reaction can take 500,000 or more CPU hours. Meanwhile, ai-QM/MM modeling of firefly bioluminescence is also hindered by the computational accuracy, where it has yet to produce quantitatively correct predictions for the bioluminescence spectral shift with site-directed mutagenesis. The goal of this proposal is to accelerate ai-QM/MM simulations of enzymatic reaction free energy and to improve the quality of ai-QM/MM-simulated bioluminescence spectra, so that ai-QM/MM simulations can be routinely performed by experimental groups. This will be achieved via a) using a lower-level (semi-empirical QM/MM) Hamiltonian for sampling; b) an enhancement to the similarity between the two Hamiltonians by calibrating the low-level Hamiltonian using the reaction pathway force matching approach, in conjunction with several other methods. The expected outcomes of this collaborative effort include: a) advanced methodologies for accelerated reaction free energy simulations and accurate bioluminescence spectra predictions, which will be released through multiple software platforms; b) a fundamental understanding of reactions such as Kemp elimination and polymerase-eta catalyzed DNA replication; c) a deeper insight into the role of macromolecular environment in the modulation of enzyme catalytic activities or bioluminescence wavelengths, which can further enhance our capability of designing new enzymes and bioluminescence probes. Narrative This project aims to develop quantum-mechanics-based computational methods to more quickly model enzymatic reactions and more accurately model bioluminescence spectra. It will lead to reliable and efficient computational tools for use by the general scientific community. It will facilitate the probe of enzymatic reaction mechanisms and the computer-aided design of new bioluminescence probes.",Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence,10021018,R01GM135392,"['Adopted', 'Biochemical Reaction', 'Bioluminescence', 'Calibration', 'Communities', 'Computer Simulation', 'Computer software', 'Computer-Aided Design', 'Computing Methodologies', 'DNA biosynthesis', 'DNA-Directed DNA Polymerase', 'Electrostatics', 'Environment', 'Enzymes', 'Evaluation', 'Fireflies', 'Free Energy', 'Freedom', 'Generations', 'Goals', 'Hour', 'Ions', 'Life', 'Machine Learning', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Multienzyme Complexes', 'Outcome', 'Pathway interactions', 'Polymerase', 'Process', 'Protocols documentation', 'Quantum Mechanics', 'Reaction', 'Resources', 'Role', 'Sampling', 'Site-Directed Mutagenesis', 'System', 'Temperature', 'Thermodynamics', 'Time', 'base', 'computerized tools', 'cost', 'design', 'experimental group', 'improved', 'innovation', 'insight', 'multi-scale modeling', 'mutant', 'quantum', 'simulation', 'theories']",NIGMS,UNIVERSITY OF OKLAHOMA NORMAN,R01,2020,255238,0.0024344358088020054
"Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence Abstract Enzyme functionality is a critical component of all life systems. Whereas advances in experimental methodology have enabled a better understanding of factors that control enzyme function, critical components of the reaction space such as highly unstable intermediates and transition states are best accessed for evaluation through computational simulations. Similarly, computational methodology continues to provide a key resource for probing excited-state processes such as bioluminescence. Combined ab initio quantum mechanical molecular mechanical (ai-QM/MM) simulations are, in principle, the preferred choice in the modeling of both processes. But ai-QM/MM modeling of enzymatic reactions is now severely limited by its computational cost, where a direct ai-QM/MM free energy simulation of an enzymatic reaction can take 500,000 or more CPU hours. Meanwhile, ai-QM/MM modeling of firefly bioluminescence is also hindered by the computational accuracy, where it has yet to produce quantitatively correct predictions for the bioluminescence spectral shift with site-directed mutagenesis. The goal of this proposal is to accelerate ai-QM/MM simulations of enzymatic reaction free energy and to improve the quality of ai-QM/MM-simulated bioluminescence spectra, so that ai-QM/MM simulations can be routinely performed by experimental groups. This will be achieved via a) using a lower-level (semi-empirical QM/MM) Hamiltonian for sampling; b) an enhancement to the similarity between the two Hamiltonians by calibrating the low-level Hamiltonian using the reaction pathway force matching approach, in conjunction with several other methods. The expected outcomes of this collaborative effort include: a) advanced methodologies for accelerated reaction free energy simulations and accurate bioluminescence spectra predictions, which will be released through multiple software platforms; b) a fundamental understanding of reactions such as Kemp elimination and polymerase-eta catalyzed DNA replication; c) a deeper insight into the role of macromolecular environment in the modulation of enzyme catalytic activities or bioluminescence wavelengths, which can further enhance our capability of designing new enzymes and bioluminescence probes. Narrative This project aims to develop quantum-mechanics-based computational methods to more quickly model enzymatic reactions and more accurately model bioluminescence spectra. It will lead to reliable and efficient computational tools for use by the general scientific community. It will facilitate the probe of enzymatic reaction mechanisms and the computer-aided design of new bioluminescence probes.",Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence,9864664,R01GM135392,"['Adopted', 'Biochemical Reaction', 'Bioluminescence', 'Calibration', 'Communities', 'Computer Simulation', 'Computer software', 'Computer-Aided Design', 'Computing Methodologies', 'DNA biosynthesis', 'DNA-Directed DNA Polymerase', 'Electrostatics', 'Environment', 'Enzymes', 'Evaluation', 'Fireflies', 'Free Energy', 'Freedom', 'Generations', 'Goals', 'Hour', 'Ions', 'Life', 'Machine Learning', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Multienzyme Complexes', 'Outcome', 'Pathway interactions', 'Polymerase', 'Process', 'Protocols documentation', 'Quantum Mechanics', 'Reaction', 'Resources', 'Role', 'Sampling', 'Site-Directed Mutagenesis', 'System', 'Temperature', 'Thermodynamics', 'Time', 'base', 'computerized tools', 'cost', 'design', 'experimental group', 'improved', 'innovation', 'insight', 'multi-scale modeling', 'mutant', 'quantum', 'simulation', 'theories']",NIGMS,UNIVERSITY OF OKLAHOMA NORMAN,R01,2019,269849,0.0024344358088020054
"3-D Imaging Flow Cytometry PROJECT SUMMARY This project aims to develop and test two innovative platforms and related software for 3-D imaging flow cytometry of fluorescent or absorbing (stained) samples. These systems will allow 3-D structural and functional imaging of many single cells at a subcellular resolution and at a scale that used to be available only in flow cytometry or recently in 2-D imaging. Thereby, the proposed methods have the potential to fundamentally change the ways cultured cells, patient-derived samples, and small experimental organisms are studied. Automated classification based on the 3-D features will enable the diagnosis of hematologic disorders at single-cell precision. Existing 3-D microscopy methods can provide the same information at higher resolution; however, by relying on a scanning mechanism they cannot be applied to suspending cells, especially in a flow configuration, which is essential for high-speed interrogation. Snapshot 3-D microscopy techniques have been developed to address this challenge, but they have insufficient spatial resolution for single-cell imaging and suffer from long data processing time. We overcome these limitations by combining two novel snapshot techniques developed by the PI with the most rigorous optical imaging theories and cutting-edge component technologies. We will use an array of lenslets, which simultaneously records many projection images corresponding with different viewing angles. The use of pupil phase masks, designed using wavefront coding and a theory of 3-D high-numerical-aperture optical imaging, will increase the resolution of each projection image to the theoretical limit given by the objective-lens numerical aperture. The target resolution is 0.5 µm, which is comparable to existing 2-D imaging flow cytometry systems. The target imaging throughputs based on current component technologies are 120 volumes/sec for fluorescence imaging and 700 volumes/sec for absorption imaging, which are higher than 100 volumes/sec of cutting-edge 3-D optical microscopy for stationary specimens. The vast amount of data acquired by these 3-D imaging systems imposes a serious challenge to data processing. The developed systems record true projection images, which obviate iterative deconvolution process, thereby allowing much faster tomographic reconstruction than in existing snapshot techniques. Using general-purpose graphics processing units and optical diffraction tomography, which includes the diffraction of light by subcellular organelles, our tomographic reconstruction algorithm will be faster yet more accurate than existing approaches. Further, we will explore the feasibility of applying a deep convolutional neural network to the images acquired by the developed systems for accurate single-cell classification based on 3-D features. PROJECT NARRATIVE This project aims to develop 3-D imaging flow cytometry platforms for fluorescent or absorbing (stained) cells at high resolution and high throughput in a flow configuration. The developed systems will be built upon two novel snapshot 3-D techniques developed by the PI in combination with most rigorous 3-D optical imaging theories and cutting-edge component technologies. The proposed methods have the potential to fundamentally change the ways that biological specimens are examined by allowing 3-D structural and functional imaging of suspending cells at a scale that used to be available only in flow cytometry or 2-D imaging.",3-D Imaging Flow Cytometry,10023268,R21GM135848,"['3-Dimensional', 'Address', 'Adopted', 'Algorithms', 'Biological', 'Blood', 'Blood specimen', 'Cells', 'Classification', 'Code', 'Computer software', 'Cultured Cells', 'Data', 'Diagnosis', 'Dimensions', 'Flow Cytometry', 'Functional Imaging', 'Geometry', 'Hematological Disease', 'Holography', 'Image', 'Laboratory Organism', 'Leukocytes', 'Lifting', 'Lighting', 'Masks', 'Methods', 'Microscope', 'Microscopy', 'Optical Tomography', 'Optics', 'Organelles', 'Patients', 'Phase', 'Process', 'Pupil', 'Records', 'Resolution', 'Sampling', 'Scanning', 'Specimen', 'Speed', 'Stains', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Image', 'Three-Dimensional Imaging', 'Time', 'Work', 'absorption', 'base', 'cellular imaging', 'commercialization', 'computerized data processing', 'convolutional neural network', 'deep learning', 'design', 'diffraction of light', 'digital', 'feature extraction', 'fluorescence imaging', 'imaging system', 'innovation', 'lens', 'novel', 'optical imaging', 'reconstruction', 'software development', 'targeted imaging', 'theories', 'three dimensional structure', 'tomography']",NIGMS,UNIVERSITY OF WISCONSIN MILWAUKEE,R21,2020,155784,0.020498033808340244
"3-D Imaging Flow Cytometry PROJECT SUMMARY This project aims to develop and test two innovative platforms and related software for 3-D imaging flow cytometry of fluorescent or absorbing (stained) samples. These systems will allow 3-D structural and functional imaging of many single cells at a subcellular resolution and at a scale that used to be available only in flow cytometry or recently in 2-D imaging. Thereby, the proposed methods have the potential to fundamentally change the ways cultured cells, patient-derived samples, and small experimental organisms are studied. Automated classification based on the 3-D features will enable the diagnosis of hematologic disorders at single-cell precision. Existing 3-D microscopy methods can provide the same information at higher resolution; however, by relying on a scanning mechanism they cannot be applied to suspending cells, especially in a flow configuration, which is essential for high-speed interrogation. Snapshot 3-D microscopy techniques have been developed to address this challenge, but they have insufficient spatial resolution for single-cell imaging and suffer from long data processing time. We overcome these limitations by combining two novel snapshot techniques developed by the PI with the most rigorous optical imaging theories and cutting-edge component technologies. We will use an array of lenslets, which simultaneously records many projection images corresponding with different viewing angles. The use of pupil phase masks, designed using wavefront coding and a theory of 3-D high-numerical-aperture optical imaging, will increase the resolution of each projection image to the theoretical limit given by the objective-lens numerical aperture. The target resolution is 0.5 µm, which is comparable to existing 2-D imaging flow cytometry systems. The target imaging throughputs based on current component technologies are 120 volumes/sec for fluorescence imaging and 700 volumes/sec for absorption imaging, which are higher than 100 volumes/sec of cutting-edge 3-D optical microscopy for stationary specimens. The vast amount of data acquired by these 3-D imaging systems imposes a serious challenge to data processing. The developed systems record true projection images, which obviate iterative deconvolution process, thereby allowing much faster tomographic reconstruction than in existing snapshot techniques. Using general-purpose graphics processing units and optical diffraction tomography, which includes the diffraction of light by subcellular organelles, our tomographic reconstruction algorithm will be faster yet more accurate than existing approaches. Further, we will explore the feasibility of applying a deep convolutional neural network to the images acquired by the developed systems for accurate single-cell classification based on 3-D features. PROJECT NARRATIVE This project aims to develop 3-D imaging flow cytometry platforms for fluorescent or absorbing (stained) cells at high resolution and high throughput in a flow configuration. The developed systems will be built upon two novel snapshot 3-D techniques developed by the PI in combination with most rigorous 3-D optical imaging theories and cutting-edge component technologies. The proposed methods have the potential to fundamentally change the ways that biological specimens are examined by allowing 3-D structural and functional imaging of suspending cells at a scale that used to be available only in flow cytometry or 2-D imaging.",3-D Imaging Flow Cytometry,9877321,R21GM135848,"['3-Dimensional', 'Address', 'Adopted', 'Algorithms', 'Biological', 'Blood', 'Blood specimen', 'Cells', 'Classification', 'Code', 'Computer software', 'Cultured Cells', 'Data', 'Diagnosis', 'Dimensions', 'Flow Cytometry', 'Functional Imaging', 'Geometry', 'Hematological Disease', 'Holography', 'Image', 'Laboratory Organism', 'Leukocytes', 'Lifting', 'Lighting', 'Masks', 'Methods', 'Microscope', 'Microscopy', 'Optical Tomography', 'Optics', 'Organelles', 'Patients', 'Phase', 'Process', 'Pupil', 'Records', 'Resolution', 'Sampling', 'Scanning', 'Specimen', 'Speed', 'Stains', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Three-Dimensional Image', 'Three-Dimensional Imaging', 'Time', 'Work', 'absorption', 'base', 'cellular imaging', 'commercialization', 'computerized data processing', 'convolutional neural network', 'deep learning', 'design', 'diffraction of light', 'digital', 'fluorescence imaging', 'imaging system', 'innovation', 'lens', 'novel', 'optical imaging', 'reconstruction', 'software development', 'targeted imaging', 'theories', 'three dimensional structure', 'tomography']",NIGMS,UNIVERSITY OF WISCONSIN MILWAUKEE,R21,2019,195110,0.020498033808340244
"Dissecting the mechanism of cell migration at the systems level Project Summary/Abstract Cell migration is required for many important physiological and pathological processes such as embryonic development, wound healing, and cancerous invasion. As a process that involves concerted action of multiple ensembles of molecules over the length of the entire cell, cell migration cannot be understood using conventional molecular approaches alone without considering sensing, actuation, and control at the whole cell level. This project seeks to approach migrating cells in a top-down manner as an integrated mechanochemical system. Based on observations that likely represent the manifestation of a complex network of molecular interactions, we may deduct how the underlying machine operates. The project will be facilitated by the development of new technologies, including 3D printing of polyacrylamide hydrogels and machine learning for cell tracking, traction force microscopy, and super resolution imaging. We will address three important aspects. First, we will ask how cells initiate migration through a process known as symmetry breaking, which causes a symmetrically spreading cell to initiate directional migration. We will examine various anisotropic properties of the substrate as potential symmetry breaking cues. In addition, the function of filopodia as possible sensors for symmetry breaking will be studied with imaging and pharmacological approaches. Second, we will address several poorly understood aspects of 2D and 3D cell migration. By following migrating cells over a long distance at a high magnification, we expect to place the newly discovered process of contact following in the context of cell collectives. To understand how cell shape control, cell-cell interaction, and cell migration respond to 3D environment, we will use 3D printed polyacrylamide to create model systems and systematically vary geometrical and mechanical parameters. We will then extend the experiments to decellularized lung scaffolds, which have been used for tissue engineering, to determine how migration characteristics in 3D is related to the promotion of tissue formation. Another overlooked area we will examine is the function of the tail in defining cell polarity and mediating contact following. Third, we will seek mechanistic understanding of cellular responses to cyclic stretching, which occurs in various tissues. A novel imaging approach will allow us to determine the responses during the stretching and relaxation phase respectively. A combination of experimentation and computer modeling is planned to explain why epithelial cells respond to static stretching along the direction of forces but perpendicularly in response to cyclic stretching. We will also test the hypothesis that responses to cyclic stretching can cause cell intercalation, a fundamentally important process in embryonic morphogenesis. We expect our results to complement studies at the molecular level and bring paradigm shifting insights into cell migration for both basic cell biology and repair of tissue functions. Project Narrative This project seeks to fill important gaps of knowledge in cell migration, which is essential for many physiological and pathological processes such as embryonic development, wound healing, and cancerous invasion. We will use a top-down approach that treats migrating cells as an integrated mechanical system, and we are developing technologies such as 3D printing and artificial intelligence for fabricating experimental environments, following cell movements, and imaging interactions in 3D scaffolds. Knowledge in how migrating cells interact with the environment and with each other is expected to further our abilities to counter diseases and restore body functions.",Dissecting the mechanism of cell migration at the systems level,9931843,R35GM136345,"['3-Dimensional', '3D Print', 'Address', 'Area', 'Artificial Intelligence', 'Binding', 'Biological Models', 'Cancerous', 'Cell Communication', 'Cell Polarity', 'Cell Shape', 'Cells', 'Cellular biology', 'Characteristics', 'Complement', 'Complex', 'Computer Models', 'Cues', 'Deductibles', 'Development', 'Disease', 'Embryo', 'Embryonic Development', 'Environment', 'Epithelial Cells', 'Filopodia', 'Image', 'Knowledge', 'Length', 'Lung', 'Machine Learning', 'Mechanics', 'Mediating', 'Molecular', 'Morphogenesis', 'Pathologic Processes', 'Periodicity', 'Pharmacology', 'Phase', 'Physiological Processes', 'Process', 'Property', 'Relaxation', 'Resolution', 'Stretching', 'System', 'Tail', 'Technology', 'Testing', 'Tissue Engineering', 'Tissues', 'Traction Force Microscopy', 'base', 'cell motility', 'experimental study', 'imaging approach', 'insight', 'intercalation', 'migration', 'new technology', 'novel', 'polyacrylamide', 'polyacrylamide hydrogels', 'response', 'scaffold', 'sensor', 'tissue repair', 'wound healing']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R35,2020,275685,-0.03132128257337434
"NMR Fingerprinting: Leveraging optimal control pulse design, tailored isotope labeling, and machine learning to study intractable proteins Project summary Nuclear magnetic resonance (NMR) spectroscopy is essential for the study structure, dynamics and function of proteins in near-native conditions. NMR studies have vital implications for therapeutic development. However, as the number of amino acids in the protein increases, NMR signals decay (relax) faster, yielding lower sensitivity and resolution, while the spectrum becomes more crowded. In these cases it is challenging to match observed signals to specific nuclei in the protein (called `resonance assignment') in order to meaningfully interpret NMR data. The overarching goal of our research is to push the boundaries of NMR enabling valuable insight about the dynamics and functions of currently intractable proteins. The objective of this project is to design an NMR platform consisting of coordinated, next-generation biochemical, biophysical, mathematical, and computational techniques. Our platform is built around an original approach to NMR spectroscopy in which new information about the local environment of each nucleus is encoded in the shape and pattern of its NMR signal. The rationale is that these patterns are a `fingerprint' – an intricate and unique signature that encodes key information about which atom is responsible for each resonance peak in the NMR spectrum. We will design and realize fingerprint patterns using two innovative approaches: 1) biochemically, by selectively introducing NMR-active isotopes into carefully chosen positions in the protein samples, and biophysically, and 2) by using specialized radiofrequency pulses to accurately control the quantum interactions that determine the NMR spectrum. The resulting fingerprints will be decoded using established algorithmic structures from machine learning, notably artificial neural networks. This will facilitate automated analyses that are accessible to non-NMR specialists. Our approach to spectroscopy holds promise in the study of therapeutically important proteins expressed in eukaryotic expression systems (e.g. G-protein coupled receptors and glycosylated proteins). Current NMR data from such proteins shows clear dynamics and interactions with other proteins, but cannot yet be properly interpreted because of the difficulty of relating each NMR peak to an amino acid in the protein sequence. Our platform will deliver two significant outcomes: 1) NMR resonance assignment for meaningful analyses of previously intractable systems. 2) Enable non-NMR specialists, to easily proceed from expressing their protein sample to using NMR to study dynamics and interactions via assigned spectra. This will have a positive impact on protein science and medical research. To support our mission we have assembled a team of leading experts to test our platform with their own protein systems. Project Narrative Nuclear magnetic resonance (NMR) spectroscopy is used to study the structure, function, interactions, and dynamics of proteins, with important implications for therapeutic development. However, interpreting NMR information is extremely challenging and labor intensive, especially for large proteins, which produce many overlapping, weak NMR signals. We are developing next-generation methods in which key information is more clearly visible and accessible in the NMR data, using a highly coordinated platform of emerging biochemical, biophysical, mathematical, and computational techniques.","NMR Fingerprinting: Leveraging optimal control pulse design, tailored isotope labeling, and machine learning to study intractable proteins",9942654,R01GM136859,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acids', 'Biochemical', 'Biological Models', 'Biophysics', 'COSY', 'Cell Nucleus', 'Chemicals', 'Communities', 'Complement', 'Computational Technique', 'Computer software', 'Coupling', 'Crowding', 'Cryoelectron Microscopy', 'Data', 'Drug Design', 'Environment', 'Fingerprint', 'Frequencies', 'G-Protein-Coupled Receptors', 'Goals', 'HMQC', 'Hour', 'Investigation', 'Isotope Labeling', 'Isotopes', 'Label', 'Machine Learning', 'Mainstreaming', 'Mathematics', 'Measurement', 'Measures', 'Medical Research', 'Methods', 'Mission', 'Modernization', 'Molecular Weight', 'NMR Spectroscopy', 'Nuclear', 'Nuclear Magnetic Resonance', 'Outcome', 'Pattern', 'Physiologic pulse', 'Positioning Attribute', 'Protein Dynamics', 'Proteins', 'Pyruvate', 'Relaxation', 'Research', 'Research Proposals', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Shapes', 'Side', 'Signal Transduction', 'Specialist', 'Spectrum Analysis', 'Structural Protein', 'Structure', 'System', 'TOCSY', 'Testing', 'Textbooks', 'Therapeutic', 'Therapeutic Studies', 'Time', 'Training', 'Vertebral column', 'X-Ray Crystallography', 'artificial neural network', 'automated analysis', 'base', 'control theory', 'cost', 'design', 'experimental study', 'innovation', 'insight', 'instrument', 'methyl group', 'next generation', 'nonlinear regression', 'novel', 'novel strategies', 'nuclear power', 'protein function', 'quantum', 'radio frequency', 'reconstruction', 'therapeutic development']",NIGMS,DANA-FARBER CANCER INST,R01,2020,495502,-0.03902278409444609
"Investigating how mechanical connectivity yields developmental robustness ABSTRACT  It is essential for the fate of an organism that key morphogenetic processes occur reproducibly even under tissue damage or environmental perturbations. While much is known about how genetic redundancy and regulation achieves robust development, less is understood about how a tissue mechanically ensures reproducible shape change when perturbed. This project uncovers how populations of physically interacting cells mechanically respond to challenging conditions and modify their collective behavior to still sculpt the correct final shape.  One way for cells to coordinate tissue-scale forces and movements is through direct mechanical connections. In fact, many developing tissues exhibit supracellular networks of actomyosin connections that link hundreds of cells. A large roadblock has been with the challenges of imaging and quantifying subcellular protein at the tissue scale. I adapted a topological smoothing algorithm originally used to trace high-noise filamentous structure of galaxies in the Universe to data to trace high-noise filamentous myosin structure in confocal images. This allowed for the first quantification of a supracellular myosin network across an entire tissue over developmental time. Subsequent analysis adopting techniques from network theory allowed me to identify that the robust folding of the Drosophila fruit fly embryo during ventral furrow formation is mechanically ensured by patterns in the supracellular network spanning its ventral cells.  This newly discovered importance of supracellular networks in coordinating robust shape change highlights the need for a comprehensive understanding of how supracellular networks form, and how their patterns impact the function and robustness of a population of cells. Deciphering robustness at the tissue-level, where the displacement and fate of hundreds of cells must be considered, requires techniques at the interface of cell and developmental biology, biophysics and computer science. The proposed project will take a highly interdisciplinary approach to identify how supracellular network patterns are controlled molecularly, at the cell level, and via tissue constraints. As well, how heterogeneity in tissue-level patterns impacts morphogenetic robustness will be addressed. Together this comprehensive study of the structure and function of supracellular networks will represent a new way to interpret mechanical robustness across diverse developing tissues. As well, a generalized description of mechanical robustness has the potential to uncover new paths to predict and control tissue malformation, which would represent a significant advance for both developmental biology and fetal medicine. PROJECT NARRATIVE The robust establishment of correct shape is essential for proper tissue function. Tissue shape change is a mechanical process that necessitates the coordinated force generation and motion of thousands of cells. Identifying how physically interacting cells mechanically respond to challenging conditions and modify their behavior to still sculpt the correct final shape will shed light onto many congenital disorders that result from morphogenetic dysregulation.",Investigating how mechanical connectivity yields developmental robustness,9950519,K99GM136915,"['Actomyosin', 'Address', 'Adopted', 'Affect', 'Algorithms', 'Architecture', 'Behavior', 'Biophysics', 'Cell Culture Techniques', 'Cell Size', 'Cells', 'Cellular biology', 'Congenital Abnormality', 'Congenital Disorders', 'Data', 'Development', 'Developmental Biology', 'Disease', 'Drosophila genus', 'Early Diagnosis', 'Embryo', 'Engineering', 'Ensure', 'Exhibits', 'Galaxy', 'Generations', 'Genetic', 'Heterogeneity', 'Image', 'In Vitro', 'Light', 'Link', 'Location', 'Machine Learning', 'Maternal-fetal medicine', 'Mechanics', 'Molecular', 'Morphogenesis', 'Morphology', 'Motion', 'Movement', 'Myosin ATPase', 'Neural Tube Defects', 'Noise', 'Organism', 'Pathway Analysis', 'Pattern', 'Population', 'Process', 'Property', 'Proteins', 'Regulation', 'Reproducibility', 'Shapes', 'Stimulus', 'Structure', 'Techniques', 'Testing', 'Time', 'Tissues', 'Transportation', 'Work', 'cohesion', 'computer science', 'confocal imaging', 'congenital heart disorder', 'fetal', 'fetal medicine', 'fly', 'in vivo', 'interdisciplinary approach', 'malformation', 'mechanical force', 'mechanical properties', 'novel', 'optogenetics', 'theories', 'tissue-level behavior', 'transmission process']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,K99,2020,98836,-0.013768408500043378
"Software for OCT Analysis of Vascular Stents Software for OCT Analysis of Vascular Stents PI: Ronny Shalev, PhD, Dyad Medical Summary Dyad Medical, Inc. will create intravascular OCT (IVOCT) software for clinical, live time determination of stent apposition (OCTivat-live, the live time OCT image visualization and analysis tool) and for offline analysis of stent implantation (OCTivat-stent). Every year, 100s of thousands of patients in the US are treated with intra- vascular stents creating an opportunity for both solutions. Although advancements such as drug eluting metal stents hinder restenosis, there remains significant room for improvement. Stent design parameters include drug, material (bioresorbable vs metal), polymer composition, coatings to stimulate cell coverage, etc. To opti- mize designs, sensitive, in vivo assessments are needed for preclinical and clinical evaluations. Intravascular OCT (IVOCT) is the lone imaging modality with the resolution and contrast to meet this challenge. The Core Lab at CWRU is the premiere site in the world for manual analysis of IVOCT image data. A cardiologist analyst takes 6-16 hrs to analyze manually a single stent, and despite training and quality assurance measures, inter- analyst variability can limit the power to determine changes between stent designs. Building upon work at CWRU, we will develop advanced, highly automated software to greatly speed analysis, improve reproducibil- ity, increase accuracy, and harmonize analysis. Software will reduce costs by decreasing manual labor, and with improved reproducibility, possibly enable the use of historical data, eliminating cost of a control arm. Re- garding live time analysis, rather than manually reviewing >500 images in a pullback, with fast software, it will be possible to present the number and location of malapposed struts in 3D, providing instant feedback to phy- sicians on the need for additional dilatation with a larger balloon or higher pressure. In addition, we will auto- matically determine stent and vessel area along the length of the pullback, allowing us to compute stent ex- pansion and eccentricity, quantitative measures related to successful stent deployment, the most important de- terminant of outcome. IVOCT could also play a role at patient follow up. If a stent is well covered, then long- term anti-platelet therapy might be unnecessary, minimizing bleeding risk. If a stent has many uncovered struts, a therapeutic might prevent stent thrombosis or stimulate healing. Project Narrative: We will develop methods for improved in vivo assessment of intravascular stents, the treatment of choice for 100s of thousands of patients, suffering from ischemic heart disease, in the US every year. Our methods will enable optimization of stent technologies and improved deployment for improved treatment of vascular dis- ease.",Software for OCT Analysis of Vascular Stents,9407267,R43HL137500,"['Agreement', 'Algorithms', 'Area', 'Blinded', 'Blood Vessels', 'Cells', 'Classification', 'Clinical', 'Clinical Trials', 'Code', 'Computer software', 'Data', 'Detection', 'Devices', 'Dilatation - action', 'Doctor of Philosophy', 'Feedback', 'Follow-Up Studies', 'Heart', 'Hemorrhage', 'Hour', 'Image', 'Image Analysis', 'Implant', 'Institutes', 'International', 'Ions', 'Laboratories', 'Length', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Metals', 'Methods', 'Myocardial Ischemia', 'Needs Assessment', 'Outcome', 'Pathology', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Play', 'Polymers', 'Process', 'Reproducibility', 'Research Personnel', 'Resolution', 'Risk', 'Role', 'Services', 'Site', 'Speed', 'Stents', 'Surrogate Markers', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Variant', 'Vascular Diseases', 'Work', 'arm', 'cardiovascular imaging', 'cost', 'design', 'follow-up', 'healing', 'image visualization', 'imaging modality', 'implantation', 'improved', 'in vivo', 'personalized diagnostics', 'preclinical evaluation', 'pressure', 'prevent', 'prototype', 'quality assurance', 'research clinical testing', 'restenosis', 'statistics', 'stent thrombosis', 'tool', 'treatment choice']",NHLBI,"DYAD MEDICAL, INC.",R43,2017,224744,-0.02870133192159061
"Super-multiplexed fluorescence nanoscopy for imaging-based proteomics PROJECT SUMMARY In situ immunofluorescence imaging is a powerful method to study the locations, expression levels and structures of proteins in cells and tissues. In particular, multiplexed imaging reveals the interaction networks of proteins, which allows us to understand the underlying mechanisms of many diseases. However, it has been challenging to perform multiplexed immunofluorescence imaging due to its extremely time-consuming process, high cost and lack of signal amplification. The limited spatial resolution achievable with confocal microscopy often fails to reveal complex spatial organization and to determine localizations of proteins. Here we propose super-multiplexed immunofluorescence nanoscopy that is capable of imaging more than twenty different proteins in 24 hours with nanoscale resolution. We will employ DNA-barcoded secondary nanobodies that are monovalent, open-source and designed for quantitative labeling. Repeated introduction and washing of fluorescent DNA imagers will generate highly multiplexed images. Moreover, we will develop unprecedentedly fast stimulated emission depletion (STED) microscopy that employs a parallelized line array of doughnut beams. It will feature a large imaging area and excellent optical sectioning capability. Photon reassignment, hyperspectral imaging and deep-learning will further facilitate rapid super-resolution-based protein profiling. Our new biochemical and optical tools will play crucial roles in diverse biomedical areas including brain proteomics and cancer profiling. PROJECT NARRATIVE We propose to develop highly multiplexed immunofluorescence super-resolution imaging tools. Our approach is fast, low cost and readily accessible, which will facilitate nanoscale imaging-based proteomics in cells and tissues.",Super-multiplexed fluorescence nanoscopy for imaging-based proteomics,10028050,R35GM138039,"['Area', 'Bar Codes', 'Biochemical', 'Brain', 'Cells', 'Complex', 'Confocal Microscopy', 'Consumption', 'DNA', 'Disease', 'Fluorescence', 'Hour', 'Image', 'Imaging Device', 'Immunofluorescence Immunologic', 'In Situ', 'Label', 'Location', 'Malignant Neoplasms', 'Methods', 'Microscopy', 'Nanoscopy', 'Optics', 'Photons', 'Play', 'Process', 'Proteins', 'Proteomics', 'Resolution', 'Role', 'Signal Transduction', 'Structural Protein', 'Time', 'Tissues', 'base', 'cost', 'deep learning', 'design', 'imager', 'multiplexed imaging', 'nanobodies', 'nanoscale', 'open source', 'protein profiling', 'tool']",NIGMS,UNIVERSITY OF CENTRAL FLORIDA,R35,2020,267626,-0.009185041422170153
"Imaging Molecular Level Details of Collagen Fibers by VSFG Microscopy In this proposed project, we plan to fill the knowledge gap of the relationships between microscopic self-assembled structures, collagen-molecule interactions and macroscopic fiber morphologies of type-I collagen, the primary component of most human tissues and a commonly used biomaterial for tissue engineering. By investigating collagen-water and collagen-protein interactions in in vitro systems that mimic basic aspects of physiologically relevant three- dimensional fibrillar tissue architectures, we aim to fill knowledge gaps in fundamental collagen research. We will achieve this goal by developing a hyperspectral imaging technique – vibrational sum frequency generation (VSFG) microscopy – at high repetition rates (400 kHz) and apply it to collagen. The long-term vision is to develop new biophysics methods to reveal molecular-level structures and interactions for pericellular space research and other complex biological environments, and eventually applying it to study various pericellular environment related diseases. In order to correlate spectral features to microscopic and macroscopic structures of type I collagen, we plan to apply machine-learning techniques to analyze our data and extract spectral signatures of collagen’s micro/macrostructures. We will two major scientific focuses: (A) understanding molecular signatures of microscopic self-assembly fibrils structures and its relationship to the macroscopic morphology (plan 1 and 2); and (B) investigating molecular level collagen-molecule interactions (plan 3 and 4). Specific plans include:  1. Obtaining hyperspectral VSFG images of collagen tissues to study their morphology in a  label free and non-invasive manner  2. Establishing molecular spectral signatures of self-assembled collagen fibril structures  3. Understanding collagen-water interaction in first solvation layer of collagen fibers.  4. Imaging spatial locations of chemicals and peptides that interact with collagens. If successful, the significance is that a label free, vibrational mode specific imaging technique specific for pericellular space will be available, which can reveal molecular level insights of collagen structures and its interactions with surrounding molecules, pertinent to fibrosis and cell— pericellular space interaction related diseases. This proposed project contributes to the scope of NIGMS by developing new technology to reveal fundamental molecular-level principle, mechanism and signatures related to morphology of collagen I at both micro- and macroscopic scales, and collagen-molecule interactions, laying foundations for biophysical/biochemical principles for future biomedical applications related to collagens. This proposed development of vibrational sum frequency generation microscopy, in the short term, will spatially resolve collagen tissues with chemical structure and molecular interaction information in a complicated environment. Machine learning and simulation approaches will be employed to build a data base to convert hyperspectral images of collagen into a spatial map with microscopic structures and molecular interaction information. In the long term, the fundamental biochemical knowledge learned from this development will lay foundations for rationally design biomedical approaches to monitor and control pericellular spaces and its interaction with cells, and further advance treatment to diseases related to it.",Imaging Molecular Level Details of Collagen Fibers by VSFG Microscopy,10028946,R35GM138092,"['3-Dimensional', 'Architecture', 'Binding', 'Biochemical', 'Biocompatible Materials', 'Biological', 'Biophysics', 'Cells', 'Chemical Structure', 'Chemicals', 'Collagen', 'Collagen Fiber', 'Collagen Fibril', 'Collagen Type I', 'Complex', 'Data', 'Databases', 'Development', 'Disease', 'Environment', 'Fiber', 'Fibrosis', 'Foundations', 'Frequencies', 'Future', 'Generations', 'Goals', 'Image', 'Imaging Techniques', 'In Vitro', 'Knowledge', 'Label', 'Location', 'Machine Learning', 'Maps', 'Microscopic', 'Microscopy', 'Molecular', 'Molecular Profiling', 'Monitor', 'Morphology', 'National Institute of General Medical Sciences', 'Peptides', 'Physiological', 'Proteins', 'Research', 'Structure', 'Sum', 'System', 'Techniques', 'Tissue Engineering', 'Tissues', 'Vision', 'Water', 'biophysical techniques', 'design', 'human tissue', 'image reconstruction', 'insight', 'molecular imaging', 'new technology', 'self assembly', 'simulation', 'vibration']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R35,2020,393958,-0.045833841052652785
"Quantification of myocardial blood flow using Dynamic PET/CTA fused imagery to determine physiological significance of specific coronary lesions Project Summary One of every 6 deaths in the USA in 2015 was caused by coronary artery disease (CAD). Traditionally, primarily anatomic considerations have been used to diagnose CAD. Fractional flow reserve (FFR), a physiological index of blood-flow reduction caused by coronary stenosis, has been shown by the FAME trials as a better predictor of clinical outcomes from coronary revascularization than that based on anatomy alone. PET-derived absolute myocardial blood flow (MBF), flow reserve (MFR) and relative flow reserve (RFR) have been shown to add clinical value in detecting CAD and risk assessment. Currently, PET measurements of MBF, MFR and RFR are not lesion specific, calculated either globally for the entire left ventricle (LV), or regionally to pre-defined vascular or segmental territories. This approach is limited by the intermixing of normal flow from normal regions with abnormal flow from abnormal regions thus reducing the measured degree of flow-impairment, diagnostic performance and culpable lesion location. We and others have shown that the variability alone of vessel pathway between patients leads to 18% misdiagnosis rate. We propose to develop algorithms to non-invasively measure MBF, MFR and RFR across specific coronary lesions for the entire coronary tree at least as accurately as those measured invasively during cardiac catheterization using fused coronary anatomy data obtained from CT coronary angiography (CTA) with dynamic PET (dPET) flow physiologic data. We hypothesize that our novel 3D fusion dPET/CTA approach will accurately and non- invasively predict lesion-specific severity as defined by invasive coronary angiography (ICA) FFR obtained with flow-wire/pressure-wire approaches. We anticipate that our dPET/CTA approach will be significantly more accurate than other existing non-invasive approaches. Exploiting our achievements in algorithm development, we will pursue our specific aims of 1) automating CTA myocardial border and vessel segmentation, 2) automating dPET/CTA 3D fusion to localize myocardial volumes of interest (VOIs) on dPET studies corresponding to the anatomical path of coronary vessels from CTA, and 3) calculating MBF and related flow parameters along coronary vessels using clinically accepted PET flow methods. Our dPET/CTA method will result in the following game-changing paradigm: 1) eliminate unnecessary ICAs in patients with no significant lesions, 2) avoid stenting physiologically insignificant lesions, 3) guide the PCI process to the location of significant lesions, 4) provide a flow-color-coded 3D roadmap of the entire coronary tree to guide bypass surgery, and 5) use less radiation and lower cost. Project Narrative  The aim of this work is to develop software tools to fuse coronary anatomy data obtained from CT coronary angiography with dynamic PET data (combination of anatomic and physiologic information) to noninvasively measure absolute myocardial blood flow, flow reserve and relative flow reserve across specific coronary lesions. These tools should reduce or eliminate unnecessary catheterizations and stenting and thus reduce patient risk, lower radiation exposure, and reduce the healthcare costs associated with unnecessary costly invasive treatments.",Quantification of myocardial blood flow using Dynamic PET/CTA fused imagery to determine physiological significance of specific coronary lesions,9578310,R01HL143350,"['Achievement', 'Algorithms', 'Anatomy', 'Automation', 'Blood Vessels', 'Blood flow', 'Bypass', 'Cardiac', 'Cardiac Catheterization Procedures', 'Caring', 'Catheterization', 'Cessation of life', 'Clinical', 'Code', 'Color', 'Computers', 'Coronary', 'Coronary Angiography', 'Coronary Arteriosclerosis', 'Coronary Stenosis', 'Coronary Vessels', 'Data', 'Databases', 'Decision Making', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Drops', 'Evaluation', 'Generic Drugs', 'Goals', 'Health Care Costs', 'Image', 'Imagery', 'Impairment', 'Left', 'Left ventricular structure', 'Lesion', 'Location', 'Manuals', 'Masks', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modality', 'Morphologic artifacts', 'Myocardial', 'Myocardial perfusion', 'Myocardium', 'Non-Invasive Lesion', 'Operative Surgical Procedures', 'Pathway interactions', 'Patient Selection', 'Patient risk', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Perfusion', 'Physicians', 'Physiological', 'Positron-Emission Tomography', 'Principal Component Analysis', 'Procedures', 'Process', 'Radiation', 'Radiation exposure', 'Risk Assessment', 'Sampling', 'Selection for Treatments', 'Severities', 'Shapes', 'Software Tools', 'Stents', 'Surface', 'Thick', 'Three-Dimensional Image', 'Time', 'Trees', 'Unnecessary Procedures', 'Variant', 'Work', 'base', 'clinical application', 'coronary lesion', 'cost', 'experience', 'image processing', 'improved', 'improved outcome', 'indexing', 'innovation', 'interest', 'novel', 'patient variability', 'perfusion imaging', 'predict clinical outcome', 'pressure', 'prevent', 'software development', 'standard measure', 'tool']",NHLBI,EMORY UNIVERSITY,R01,2018,643795,-0.03177501989129297
"Quantification of myocardial blood flow using Dynamic PET/CTA fused imagery to determine physiological significance of specific coronary lesions Project Summary One of every 6 deaths in the USA in 2015 was caused by coronary artery disease (CAD). Traditionally, primarily anatomic considerations have been used to diagnose CAD. Fractional flow reserve (FFR), a physiological index of blood-flow reduction caused by coronary stenosis, has been shown by the FAME trials as a better predictor of clinical outcomes from coronary revascularization than that based on anatomy alone. PET-derived absolute myocardial blood flow (MBF), flow reserve (MFR) and relative flow reserve (RFR) have been shown to add clinical value in detecting CAD and risk assessment. Currently, PET measurements of MBF, MFR and RFR are not lesion specific, calculated either globally for the entire left ventricle (LV), or regionally to pre-defined vascular or segmental territories. This approach is limited by the intermixing of normal flow from normal regions with abnormal flow from abnormal regions thus reducing the measured degree of flow-impairment, diagnostic performance and culpable lesion location. We and others have shown that the variability alone of vessel pathway between patients leads to 18% misdiagnosis rate. We propose to develop algorithms to non-invasively measure MBF, MFR and RFR across specific coronary lesions for the entire coronary tree at least as accurately as those measured invasively during cardiac catheterization using fused coronary anatomy data obtained from CT coronary angiography (CTA) with dynamic PET (dPET) flow physiologic data. We hypothesize that our novel 3D fusion dPET/CTA approach will accurately and non- invasively predict lesion-specific severity as defined by invasive coronary angiography (ICA) FFR obtained with flow-wire/pressure-wire approaches. We anticipate that our dPET/CTA approach will be significantly more accurate than other existing non-invasive approaches. Exploiting our achievements in algorithm development, we will pursue our specific aims of 1) automating CTA myocardial border and vessel segmentation, 2) automating dPET/CTA 3D fusion to localize myocardial volumes of interest (VOIs) on dPET studies corresponding to the anatomical path of coronary vessels from CTA, and 3) calculating MBF and related flow parameters along coronary vessels using clinically accepted PET flow methods. Our dPET/CTA method will result in the following game-changing paradigm: 1) eliminate unnecessary ICAs in patients with no significant lesions, 2) avoid stenting physiologically insignificant lesions, 3) guide the PCI process to the location of significant lesions, 4) provide a flow-color-coded 3D roadmap of the entire coronary tree to guide bypass surgery, and 5) use less radiation and lower cost. Project Narrative  The aim of this work is to develop software tools to fuse coronary anatomy data obtained from CT coronary angiography with dynamic PET data (combination of anatomic and physiologic information) to noninvasively measure absolute myocardial blood flow, flow reserve and relative flow reserve across specific coronary lesions. These tools should reduce or eliminate unnecessary catheterizations and stenting and thus reduce patient risk, lower radiation exposure, and reduce the healthcare costs associated with unnecessary costly invasive treatments.",Quantification of myocardial blood flow using Dynamic PET/CTA fused imagery to determine physiological significance of specific coronary lesions,9980994,R01HL143350,"['3-Dimensional', 'Achievement', 'Algorithms', 'Anatomy', 'Automation', 'Blood Vessels', 'Blood flow', 'Bypass', 'Cardiac', 'Cardiac Catheterization Procedures', 'Caring', 'Catheterization', 'Cessation of life', 'Clinical', 'Code', 'Color', 'Computing Methodologies', 'Consumption', 'Coronary', 'Coronary Angiography', 'Coronary Arteriosclerosis', 'Coronary Stenosis', 'Coronary Vessels', 'Data', 'Databases', 'Decision Making', 'Detection', 'Diagnosis', 'Diagnostic', 'Drops', 'Evaluation', 'Goals', 'Health Care Costs', 'Image', 'Imagery', 'Impairment', 'Left', 'Left ventricular structure', 'Lesion', 'Location', 'Manuals', 'Masks', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Morphologic artifacts', 'Myocardial', 'Myocardial perfusion', 'Myocardium', 'Non-Invasive Lesion', 'Operative Surgical Procedures', 'Pathway interactions', 'Patient Selection', 'Patient risk', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Perfusion', 'Physicians', 'Physiological', 'Positron-Emission Tomography', 'Principal Component Analysis', 'Procedures', 'Process', 'Radiation', 'Radiation exposure', 'Risk Assessment', 'Sampling', 'Selection for Treatments', 'Severities', 'Shapes', 'Software Tools', 'Stents', 'Surface', 'Thick', 'Three-Dimensional Image', 'Time', 'Trees', 'Unnecessary Procedures', 'Variant', 'Work', 'algorithm development', 'base', 'clinical application', 'coronary lesion', 'cost', 'experience', 'image processing', 'improved', 'improved outcome', 'indexing', 'innovation', 'interest', 'multimodality', 'novel', 'patient variability', 'perfusion imaging', 'predict clinical outcome', 'pressure', 'prevent', 'software development', 'standard measure', 'tool']",NHLBI,EMORY UNIVERSITY,R01,2020,620253,-0.03177501989129297
"Quantification of myocardial blood flow using Dynamic PET/CTA fused imagery to determine physiological significance of specific coronary lesions Project Summary One of every 6 deaths in the USA in 2015 was caused by coronary artery disease (CAD). Traditionally, primarily anatomic considerations have been used to diagnose CAD. Fractional flow reserve (FFR), a physiological index of blood-flow reduction caused by coronary stenosis, has been shown by the FAME trials as a better predictor of clinical outcomes from coronary revascularization than that based on anatomy alone. PET-derived absolute myocardial blood flow (MBF), flow reserve (MFR) and relative flow reserve (RFR) have been shown to add clinical value in detecting CAD and risk assessment. Currently, PET measurements of MBF, MFR and RFR are not lesion specific, calculated either globally for the entire left ventricle (LV), or regionally to pre-defined vascular or segmental territories. This approach is limited by the intermixing of normal flow from normal regions with abnormal flow from abnormal regions thus reducing the measured degree of flow-impairment, diagnostic performance and culpable lesion location. We and others have shown that the variability alone of vessel pathway between patients leads to 18% misdiagnosis rate. We propose to develop algorithms to non-invasively measure MBF, MFR and RFR across specific coronary lesions for the entire coronary tree at least as accurately as those measured invasively during cardiac catheterization using fused coronary anatomy data obtained from CT coronary angiography (CTA) with dynamic PET (dPET) flow physiologic data. We hypothesize that our novel 3D fusion dPET/CTA approach will accurately and non- invasively predict lesion-specific severity as defined by invasive coronary angiography (ICA) FFR obtained with flow-wire/pressure-wire approaches. We anticipate that our dPET/CTA approach will be significantly more accurate than other existing non-invasive approaches. Exploiting our achievements in algorithm development, we will pursue our specific aims of 1) automating CTA myocardial border and vessel segmentation, 2) automating dPET/CTA 3D fusion to localize myocardial volumes of interest (VOIs) on dPET studies corresponding to the anatomical path of coronary vessels from CTA, and 3) calculating MBF and related flow parameters along coronary vessels using clinically accepted PET flow methods. Our dPET/CTA method will result in the following game-changing paradigm: 1) eliminate unnecessary ICAs in patients with no significant lesions, 2) avoid stenting physiologically insignificant lesions, 3) guide the PCI process to the location of significant lesions, 4) provide a flow-color-coded 3D roadmap of the entire coronary tree to guide bypass surgery, and 5) use less radiation and lower cost. Project Narrative  The aim of this work is to develop software tools to fuse coronary anatomy data obtained from CT coronary angiography with dynamic PET data (combination of anatomic and physiologic information) to noninvasively measure absolute myocardial blood flow, flow reserve and relative flow reserve across specific coronary lesions. These tools should reduce or eliminate unnecessary catheterizations and stenting and thus reduce patient risk, lower radiation exposure, and reduce the healthcare costs associated with unnecessary costly invasive treatments.",Quantification of myocardial blood flow using Dynamic PET/CTA fused imagery to determine physiological significance of specific coronary lesions,9755481,R01HL143350,"['3-Dimensional', 'Achievement', 'Algorithms', 'Anatomy', 'Automation', 'Blood Vessels', 'Blood flow', 'Bypass', 'Cardiac', 'Cardiac Catheterization Procedures', 'Caring', 'Catheterization', 'Cessation of life', 'Clinical', 'Code', 'Color', 'Computing Methodologies', 'Consumption', 'Coronary', 'Coronary Angiography', 'Coronary Arteriosclerosis', 'Coronary Stenosis', 'Coronary Vessels', 'Data', 'Databases', 'Decision Making', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Drops', 'Evaluation', 'Goals', 'Health Care Costs', 'Image', 'Imagery', 'Impairment', 'Left', 'Left ventricular structure', 'Lesion', 'Location', 'Manuals', 'Masks', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Morphologic artifacts', 'Myocardial', 'Myocardial perfusion', 'Myocardium', 'Non-Invasive Lesion', 'Operative Surgical Procedures', 'Pathway interactions', 'Patient Selection', 'Patient risk', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Perfusion', 'Physicians', 'Physiological', 'Positron-Emission Tomography', 'Principal Component Analysis', 'Procedures', 'Process', 'Radiation', 'Radiation exposure', 'Risk Assessment', 'Sampling', 'Selection for Treatments', 'Severities', 'Shapes', 'Software Tools', 'Stents', 'Surface', 'Thick', 'Three-Dimensional Image', 'Time', 'Trees', 'Unnecessary Procedures', 'Variant', 'Work', 'base', 'clinical application', 'coronary lesion', 'cost', 'experience', 'image processing', 'improved', 'improved outcome', 'indexing', 'innovation', 'interest', 'multimodality', 'novel', 'patient variability', 'perfusion imaging', 'predict clinical outcome', 'pressure', 'prevent', 'software development', 'standard measure', 'tool']",NHLBI,EMORY UNIVERSITY,R01,2019,632046,-0.03177501989129297
"System-independent quantitative cardiac CT perfusion System-independent quantitative cardiac CT perfusion Summary BioInVision, Inc. and Case Western Reserve University researchers will develop software for quantitative anal- ysis of cardiac CT perfusion (CCTP), creating an important tool for evaluation of cardiovascular disease. With this product, cardiologists will be able to identify functional flow deficits in coronary artery territories. When one combines functional myocardial blood flow (MBF) with coronary anatomy from computed tomography angi- ography (CTA), it provides needed information on the physiologic significance of a stenosis. The CTA+CCTP combo could provide an ideal gateway exam for deciding whether to send a patient for percutaneous invasive coronary angiography and potential intervention (e.g., stenting). In addition, if flow is low and no stenosis is present, it will suggest microvascular disease, a very prevalent ailment of growing concern, especially among women and in diabetes. CT compares favorably to all other non-invasive cardiovascular imaging techniques (SPECT, PET, and MRI). It is available in many settings, including emergency departments. It provides both MBF and reliable coronary anatomy, not available in any other single modality. It has excellent resolution ena- bling detection of endocardial perfusion deficit, thought to be an early disease indicator that is impossible to assess with SPECT. CT is cheaper and has higher patient throughput than MRI or PET. With inclusion of MBF, CT would have an excellent opportunity to disrupt the diagnostic pathway leading to percutaneous intervention, a pathway now dominated by SPECT myocardial imaging, which includes zero information about coronary anatomy. To achieve reliable, accurate CT MBF measurements, we will invoke innovations to reduce beam hardening and to make reliable estimates of flow. Currently, CT perfusion is done on different CT machines with manufacturers’ proprietary software, using algorithms that can give erroneous MBFs. Applicable to any commercial scanner; our solution would harmonize measurements across acquisition systems providing trust- worthy, standardized measurements to clinicians, thereby improving management of cardiovascular patients. Narrative We will develop software to enable reliable evaluation of blood flow in heart tissue using CT imaging. With suc- cess, our project could lead to an improved gateway examination that could reduce unnecessary invasive cor- onary angiography, thereby reducing costs, patient discomfort, patient risk, and possibly unnecessary interven- tional therapies.",System-independent quantitative cardiac CT perfusion,9622204,R41HL144271,"['Accident and Emergency department', 'Affect', 'Algorithms', 'Anatomy', 'Angiography', 'Attention', 'Benchmarking', 'Blood Vessels', 'Blood flow', 'Calibration', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular system', 'Cause of Death', 'Chest Pain', 'Clinical', 'Clinical Data', 'Computer software', 'Confidence Intervals', 'Coronary', 'Coronary Angiography', 'Coronary artery', 'Cost Savings', 'Coupled', 'Data', 'Detection', 'Development', 'Diabetes Mellitus', 'Diagnostic', 'Disease', 'Dose', 'Electrocardiogram', 'Evaluation', 'Family suidae', 'Heart', 'Image', 'Imaging Techniques', 'Inferior', 'Intervention', 'Iodine', 'Lead', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manufacturer Name', 'Measurement', 'Methods', 'Microvascular Dysfunction', 'Modality', 'Modeling', 'Morphologic artifacts', 'Myocardial', 'Myocardium', 'Obesity', 'Pathway interactions', 'Patient risk', 'Patients', 'Perfusion', 'Persons', 'Phase', 'Physiological', 'Physiology', 'Positron-Emission Tomography', 'Pre-Clinical Model', 'Prevalence', 'Quantitative Evaluations', 'Research', 'Research Personnel', 'Resolution', 'Source', 'Standardization', 'Stenosis', 'Stents', 'System', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Tissues', 'Trust', 'Universities', 'Woman', 'X-Ray Computed Tomography', 'blood flow measurement', 'cardiovascular imaging', 'cost', 'design', 'detector', 'digital', 'imaging system', 'improved', 'innovation', 'noninvasive diagnosis', 'perfusion imaging', 'pre-clinical', 'prototype', 'research and development', 'single photon emission computed tomography', 'software development', 'success', 'tool', 'virtual']",NHLBI,"BIOINVISION, INC.",R41,2018,224240,-0.05812519395409244
"In situ transcriptome profiling in single cells Summary We have recently developed intron seqFISH (sequential Fluorescence in situ hybridization) to multiplex 10,421 genes directly in single cells. We showed that the 10,421 gene nascent transcriptome profile can identify cell types as well as capture the trajectory of the cells. We further demonstrated that we can perform mRNA seqFISH as well as immunostaining in the same cells following the 10,421 gene intron seqFISH measurement. We propose to develop this technology as a potential alternative approach to single cell RNAseq for the HuBMAP to characterize cell types directly in situ in tissues. In particular, we will adept in situ amplification methods such as hybridization chain reaction (HCR) to intron seqFISH. We had previously shown that mRNA seqFISH with HCR amplification performs exceptionally in tissues in overcoming autofluorescence background and enable robust decoding seqFISH barcodes. We will validate the integrated intron and mRNA seqFISH protocol in the mouse hippocampus in the UG3 phase of the project. Also in UG3 phase, we will develop computational tools to integrate intron seqFISH data with mRNA seqFISH as well as single cell RNAseq data. In the UH3 phase, we will translate the technology to human tissues, with a focus on human mammary tissues provided by Dr. Seewaldt at City of Hope. We will also work with the tissue mapping centers in the HuBMAP program to accelerate the translation of this technology to many tissue types. In the UH3 phase, we will generate million cell spatial atlas of human tissues containing intron profiles, mRNA profiles and protein abundances in each single cell. We will further develop computational tools to analyze for spatial enrichment of genes in the tissue and generate a pseudotime of developmental trajectories using the nascent transcriptome data. Taken together, we will develop a high throughput in situ imaging based platform to characterize cell types and future trajectories of cells using intron and mRNA seqFISH technologies. Narrative Transcriptome profiling in situ at the single cell level has transformative potential for understanding healthy versus diseased tissues in the human body. We propose an integrative approach profiling the nascent transcriptome as well as hundreds of mature transcripts in single cells in tissues. We will generate million cell spatial atlas for the mouse hippocampus as well as the human mammary tissue. We will gain unprecedented insight into the developmental of neurons in the brain as well as the duct cells in the breast. At the same time we will develop the computational tools to analyze the spatial genomics data.",In situ transcriptome profiling in single cells,9791198,UG3HL145609,"['Algorithms', 'Animals', 'Atlases', 'Biology', 'Biopsy Specimen', 'Brain', 'Breast', 'Cell Differentiation process', 'Cells', 'Cities', 'Data', 'Data Quality', 'Data Set', 'Development', 'Disease', 'Ductal Epithelial Cell', 'Epigenetic Process', 'Epithelium', 'Expression Profiling', 'Fluorescent in Situ Hybridization', 'Future', 'Gene Expression', 'Genes', 'Goals', 'Hippocampus (Brain)', 'Human', 'Human body', 'Image', 'Image Analysis', 'Immunofluorescence Immunologic', 'In Situ', 'Introns', 'Letters', 'Machine Learning', 'Mammary Gland Parenchyma', 'Mammary gland', 'Maps', 'Measurement', 'Messenger RNA', 'Methods', 'Mus', 'Natural regeneration', 'Neuroglia', 'Neurons', 'Nuclear Structure', 'Pattern', 'Phase', 'Play', 'Positioning Attribute', 'Process', 'Proteins', 'Protocols documentation', 'Publications', 'Publishing', 'Radial', 'Reaction', 'Role', 'Signal Transduction', 'Slice', 'Speed', 'Stains', 'Standardization', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Sample', 'Tissue imaging', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Validation', 'Weight', 'Work', 'base', 'cell type', 'combinatorial', 'computerized tools', 'dentate gyrus', 'genomic data', 'granule cell', 'human tissue', 'imaging system', 'in situ imaging', 'insight', 'internal control', 'mammary epithelium', 'markov model', 'programs', 'sample fixation', 'scale up', 'single molecule', 'tool', 'transcriptome', 'transcriptome sequencing']",NHLBI,CALIFORNIA INSTITUTE OF TECHNOLOGY,UG3,2019,375000,-0.04940091815530782
"In situ transcriptome profiling in single cells Summary We have recently developed intron seqFISH (sequential Fluorescence in situ hybridization) to multiplex 10,421 genes directly in single cells. We showed that the 10,421 gene nascent transcriptome profile can identify cell types as well as capture the trajectory of the cells. We further demonstrated that we can perform mRNA seqFISH as well as immunostaining in the same cells following the 10,421 gene intron seqFISH measurement. We propose to develop this technology as a potential alternative approach to single cell RNAseq for the HuBMAP to characterize cell types directly in situ in tissues. In particular, we will adept in situ amplification methods such as hybridization chain reaction (HCR) to intron seqFISH. We had previously shown that mRNA seqFISH with HCR amplification performs exceptionally in tissues in overcoming autofluorescence background and enable robust decoding seqFISH barcodes. We will validate the integrated intron and mRNA seqFISH protocol in the mouse hippocampus in the UG3 phase of the project. Also in UG3 phase, we will develop computational tools to integrate intron seqFISH data with mRNA seqFISH as well as single cell RNAseq data. In the UH3 phase, we will translate the technology to human tissues, with a focus on human mammary tissues provided by Dr. Seewaldt at City of Hope. We will also work with the tissue mapping centers in the HuBMAP program to accelerate the translation of this technology to many tissue types. In the UH3 phase, we will generate million cell spatial atlas of human tissues containing intron profiles, mRNA profiles and protein abundances in each single cell. We will further develop computational tools to analyze for spatial enrichment of genes in the tissue and generate a pseudotime of developmental trajectories using the nascent transcriptome data. Taken together, we will develop a high throughput in situ imaging based platform to characterize cell types and future trajectories of cells using intron and mRNA seqFISH technologies. Narrative Transcriptome profiling in situ at the single cell level has transformative potential for understanding healthy versus diseased tissues in the human body. We propose an integrative approach profiling the nascent transcriptome as well as hundreds of mature transcripts in single cells in tissues. We will generate million cell spatial atlas for the mouse hippocampus as well as the human mammary tissue. We will gain unprecedented insight into the developmental of neurons in the brain as well as the duct cells in the breast. At the same time we will develop the computational tools to analyze the spatial genomics data.",In situ transcriptome profiling in single cells,9660361,UG3HL145609,"['Algorithms', 'Animals', 'Atlases', 'Biology', 'Biopsy Specimen', 'Brain', 'Breast', 'Cell Differentiation process', 'Cells', 'Cities', 'Data', 'Data Quality', 'Data Set', 'Development', 'Disease', 'Ductal Epithelial Cell', 'Epigenetic Process', 'Epithelium', 'Expression Profiling', 'Fluorescent in Situ Hybridization', 'Future', 'Gene Expression', 'Genes', 'Goals', 'Hippocampus (Brain)', 'Human', 'Human body', 'Image', 'Image Analysis', 'Immunofluorescence Immunologic', 'In Situ', 'Introns', 'Letters', 'Machine Learning', 'Mammary Gland Parenchyma', 'Mammary gland', 'Maps', 'Measurement', 'Messenger RNA', 'Methods', 'Mus', 'Natural regeneration', 'Neuroglia', 'Neurons', 'Nuclear Structure', 'Pattern', 'Phase', 'Play', 'Positioning Attribute', 'Process', 'Proteins', 'Protocols documentation', 'Publications', 'Publishing', 'Radial', 'Reaction', 'Role', 'Signal Transduction', 'Slice', 'Speed', 'Stains', 'Standardization', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Sample', 'Tissue imaging', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Validation', 'Weight', 'Work', 'base', 'cell type', 'combinatorial', 'computerized tools', 'dentate gyrus', 'genomic data', 'granule cell', 'human tissue', 'imaging system', 'in situ imaging', 'insight', 'internal control', 'mammary epithelium', 'markov model', 'programs', 'sample fixation', 'scale up', 'single molecule', 'tool', 'transcriptome', 'transcriptome sequencing']",NHLBI,CALIFORNIA INSTITUTE OF TECHNOLOGY,UG3,2018,375000,-0.04940091815530782
"Mechanisms of mechano-chemical rupture of blood clots and thrombi Mechanisms of mechano-chemical rupture of blood clots and thrombi Prashant K. Purohit, John L. Bassani, Valeri Barsegov and John W. Weisel The goal of this proposal is to explore and understand the fracture toughness of blood clots and thrombi, thus providing a mechanistic basis for life-threatening thrombotic embolization. A combination of experiments, theoretical modeling and computer simulations will reveal how mechanical stresses (due to blood flow) in synergy with enzymatic lysis induce structural damage from the molecular to continuum scales and affect the propensity of a clot to embolize. The specific aims of this proposal are: (1) Measure and model fracture toughness of fibrin gels in quasi-static conditions, (2) Investigate rate dependent dissipative effects on toughness of fibrin gels, and (3) Study the effects of blood cells, prothrombotic blood composition, and fibrinolysis on rupture of blood clots. In Specific Aim (SA) 1, we will measure toughness of fibrin clots and provide a structural basis for rupture at the micron and nanometer scales. In SA2, we will delve into the thermodynamics and rate-dependence of the fracture of fibrin gels, including fluid flow through pores and fluid drag on fibrin fibers to capture how energy dissipation increases toughness. In the translational SA3, we will investigate toughness of physiologically relevant clots with effects of platelets, red blood cells, and neutrophils in the absence and presence of the physiological fibrinolytic activator (tPA). We will also study the rupture of clots made from the blood of venous thromboembolism patients to explore the effects of (pro)thrombotic alterations of blood composition on clot mechanical stability. Our preliminary studies show that i) the toughness of cross-linked fibrin gels is in the range of those for synthetic hydrogels, ii) the addition of tPA to a crack tip reduces the loads for crack growth, iii) fibers are aligned and broken along the tensile direction at the crack tip, and iv) crack propagation results from the rupture of covalent and non-covalent bonds. We also developed v) dynamic force spectroscopy in silico to mechanically test fibrin fibers and fibrin networks using pulling simulations and vi) atomic stress approach to map the stress-strain fields using the output from simulations. We will use continuum and finite element models of swellable biopolymer hydrogels, and statistical mechanical models for the forced unfolding of fibrin molecules. We will employ multiscale computational modeling based on Molecular Dynamics simulations of atomic structures of fibrin fibers, and Langevin simulations of fibrin networks accelerated on Graphics Processing Units. The proposed experiments cover the whole gamut of macroscopic tensile tests, shear rheometry, electron microscopy and confocal microscopy to visualize and quantitate the structural alterations of ruptured blood clots. Our experiments and modeling will help us to understand the mechanisms of thrombotic embolization and will address the clinically important question: why is there a strong association between clot structure/mechanical properties and cardiovascular diseases? The new knowledge will also help to design new hydrogel-based biomaterials that are currently at the forefront of research in mechanics, materials science and bioengineering. Project Narrative The research objective of this proposal is to measure, model and predict the mechanisms of mechano-chemical rupture of blood clots and thrombi at the molecular and continuum length scales. Our experiments and modeling will help to understand the mechanisms of embolization and will address the clinically important question: why is there a strong correlation between clot structure/mechanical properties and cardiovascular disease? The new knowledge will also help to design new hydrogel-based biomaterials that are currently at the forefront of research in mechanics, materials science and bioengineering.",Mechanisms of mechano-chemical rupture of blood clots and thrombi,9970812,R01HL148227,"['Address', 'Affect', 'Biocompatible Materials', 'Biological', 'Biomedical Engineering', 'Biopolymers', 'Blood', 'Blood Cells', 'Blood Platelets', 'Blood coagulation', 'Blood flow', 'Cardiovascular Diseases', 'Cause of Death', 'Chemicals', 'Clinical', 'Clinical Medicine', 'Coagulation Process', 'Complex', 'Computer Models', 'Computer Simulation', 'Confocal Microscopy', 'Cytolysis', 'Dependence', 'Diagnosis', 'Disease', 'Electron Microscopy', 'Elements', 'Enzymes', 'Erythrocytes', 'Evolution', 'Fiber', 'Fibrin', 'Fibrinogen', 'Fibrinolysis', 'Fracture', 'Frustration', 'Gel', 'Glean', 'Goals', 'Growth', 'Hydrogels', 'Knowledge', 'Laws', 'Length', 'Life', 'Link', 'Liquid substance', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Mechanical Stress', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Molecular Structure', 'Output', 'Patients', 'Physicians', 'Physiological', 'Plasma', 'Predisposition', 'Prevention', 'Process', 'Property', 'Prophylactic treatment', 'Proteins', 'Research', 'Research Proposals', 'Resistance', 'Resources', 'Rupture', 'Specimen', 'Spectrum Analysis', 'Stress', 'Structural Models', 'Structural defect', 'Structure', 'Testing', 'Theoretical Studies', 'Theoretical model', 'Therapeutic Embolization', 'Thermodynamics', 'Thick', 'Thrombin', 'Thromboembolism', 'Thrombosis', 'Thrombus', 'Traction', 'Work', 'base', 'crosslink', 'density', 'design', 'disability', 'experimental study', 'fiber cell', 'fluid flow', 'in silico', 'in vivo', 'insight', 'instrumentation', 'interdisciplinary approach', 'materials science', 'mechanical properties', 'models and simulation', 'molecular dynamics', 'molecular scale', 'multi-scale modeling', 'nanoscale', 'neutrophil', 'novel strategies', 'predictive modeling', 'prevent', 'response', 'simulation', 'synergism', 'theories', 'tool', 'venous thromboembolism', 'viscoelasticity']",NHLBI,UNIVERSITY OF PENNSYLVANIA,R01,2020,656886,-0.010999634155899517
"Ultrasonic perfusion imaging of peripheral vascular disease Project Summary Our 4-yr project aims to develop a new ultrasonic Doppler method named Higher-Order Perfusion Estimation (HOPE) imaging. Applications enabled by this technology are routine assessments of microvascular disease progression in diabetic patients with symptoms of peripheral artery disease (PAD). Technical advances include new ultrasonic echo sampling and filtering techniques that significantly increase the sensitivity and specificity of standard sonographic instruments to spatially disorganized patterns of red-blood-cell movement without the addition of contrast media. Sensitivity to slowly perfusing blood is increased by transmitting a sparse regular sequence of Doppler pulses over long durations (1-10s). To counter a concomitant increase in clutter-signal power in perfusing-blood frequency channels, spatiotemporal echo acquisitions are first rearranged into 3-D data arrays and a 3-D singular-value decomposition (3D-SVD) clutter filter is formed for each experiment. Our approach now successfully separates weak perfusing-blood echoes from other echo-signal sources in the peripheral vasculature because of the typically narrow eigen-bandwidth of clutter echoes in peripheral muscle. Preliminary results using echo simulation, microchannel flow phantoms, and preclinical models of mouse ischemic hindlimb and melanoma lesions demonstrate that our method is very well designed for monitoring steady peripheral microvascular flow patterns using commercial ultrasonic instruments (with software updates). Four aims are proposed to demonstrate the utility of HOPE imaging (both power and color-flow) for measuring blood flow and perfusion. Aim 1 expands preliminary studies in mouse models to evaluate perfusion measurement sensitivity at 12-24 MHz in diabetic animals, and to uncover mechanisms of the angiogenic response of tissues to sudden ischemia. Aim 2 continues development of HOPE imaging by improving perfusing- blood echo sensitivity and clutter-filter performance under more general imaging conditions (viz., broad eigen- bandwidth clutter and 3-D spatially varying perfusion). Aim 3 focuses on a progressively ischemic pig model using 5-12 MHz HOPE imaging, MR angiography, and radioactive microsphere techniques to calibrate and compare HOPE imaging results with standard clinical approaches. Aim 4 is a 4-yr, 150-patient study designed to evaluate the diagnostic performance of HOPE imaging at assessing PAD. The overall project aims to develop existing ultrasonic instruments into highly-effect tools for evaluating microvascular changes leading to common disabilities and major cardiovascular events. The three in vivo studies proposed in this plan are designed to develop and evaluate HOPE imaging specifically for PAD diagnosis, staging, and therapeutic monitoring. Project Narrative The project leverages a new understanding of the information contained within Doppler ultrasound echo signals to create a more effective diagnostic tool for imaging microvascular changes associated with peripheral arterial diseases. Success in this project brings effective new methods to existing instruments enabling medical professionals to track peripheral microvascular decline as well as treatment responses. This safe, low-cost method can also be applied to patients frequently helping them directly observe the effects of lifestyle decisions.",Ultrasonic perfusion imaging of peripheral vascular disease,9987730,R01HL148664,"['3-Dimensional', 'Anatomy', 'Angiography', 'Animals', 'Ankle', 'Blood', 'Blood Vessels', 'Blood flow', 'Cardiovascular system', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Color', 'Complex', 'Computer software', 'Contrast Media', 'Data', 'Development', 'Diabetic mouse', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease Progression', 'Doppler Ultrasound', 'Erythrocytes', 'Etiology', 'Evaluation', 'Event', 'Exercise Therapy', 'Failure', 'Family suidae', 'Frequencies', 'Gene Expression', 'Goals', 'Hindlimb', 'Human', 'Image', 'Imaging Device', 'Imaging Techniques', 'Injectable', 'Ischemia', 'Legal patent', 'Lesion', 'Life Style', 'Link', 'Measurement', 'Measures', 'Medical', 'Methods', 'Microspheres', 'Microvascular Dysfunction', 'Modeling', 'Monitor', 'Mus', 'Muscle', 'Names', 'Nitric Oxide', 'Operative Surgical Procedures', 'Patient Monitoring', 'Patients', 'Pattern', 'Performance', 'Perfusion', 'Peripheral', 'Peripheral Vascular Diseases', 'Peripheral arterial disease', 'Physiologic pulse', 'Pre-Clinical Model', 'Production', 'Property', 'Radioactive', 'Recovery', 'Research', 'Research Design', 'Resolution', 'Risk', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Source', 'Staging', 'Surgeon', 'Symptoms', 'Techniques', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Translating', 'Translations', 'Ultrasonics', 'Update', 'Work', 'angiogenesis', 'base', 'blood perfusion', 'cardiovascular health', 'cell motility', 'claudication', 'cohort', 'cost', 'design', 'diabetic', 'diabetic patient', 'disability', 'disease diagnosis', 'experimental study', 'imaging approach', 'improved', 'in vivo', 'in vivo imaging', 'indexing', 'instrument', 'melanoma', 'mouse model', 'multimodality', 'muscle metabolism', 'novel', 'nuclear imaging', 'perfusion imaging', 'phantom model', 'quantitative imaging', 'research study', 'response', 'simulation', 'spatiotemporal', 'success', 'tool', 'treatment response']",NHLBI,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,R01,2020,571833,-0.03763574596958588
"Ultrasonic perfusion imaging of peripheral vascular disease Project Summary Our 4-yr project aims to develop a new ultrasonic Doppler method named Higher-Order Perfusion Estimation (HOPE) imaging. Applications enabled by this technology are routine assessments of microvascular disease progression in diabetic patients with symptoms of peripheral artery disease (PAD). Technical advances include new ultrasonic echo sampling and filtering techniques that significantly increase the sensitivity and specificity of standard sonographic instruments to spatially disorganized patterns of red-blood-cell movement without the addition of contrast media. Sensitivity to slowly perfusing blood is increased by transmitting a sparse regular sequence of Doppler pulses over long durations (1-10s). To counter a concomitant increase in clutter-signal power in perfusing-blood frequency channels, spatiotemporal echo acquisitions are first rearranged into 3-D data arrays and a 3-D singular-value decomposition (3D-SVD) clutter filter is formed for each experiment. Our approach now successfully separates weak perfusing-blood echoes from other echo-signal sources in the peripheral vasculature because of the typically narrow eigen-bandwidth of clutter echoes in peripheral muscle. Preliminary results using echo simulation, microchannel flow phantoms, and preclinical models of mouse ischemic hindlimb and melanoma lesions demonstrate that our method is very well designed for monitoring steady peripheral microvascular flow patterns using commercial ultrasonic instruments (with software updates). Four aims are proposed to demonstrate the utility of HOPE imaging (both power and color-flow) for measuring blood flow and perfusion. Aim 1 expands preliminary studies in mouse models to evaluate perfusion measurement sensitivity at 12-24 MHz in diabetic animals, and to uncover mechanisms of the angiogenic response of tissues to sudden ischemia. Aim 2 continues development of HOPE imaging by improving perfusing- blood echo sensitivity and clutter-filter performance under more general imaging conditions (viz., broad eigen- bandwidth clutter and 3-D spatially varying perfusion). Aim 3 focuses on a progressively ischemic pig model using 5-12 MHz HOPE imaging, MR angiography, and radioactive microsphere techniques to calibrate and compare HOPE imaging results with standard clinical approaches. Aim 4 is a 4-yr, 150-patient study designed to evaluate the diagnostic performance of HOPE imaging at assessing PAD. The overall project aims to develop existing ultrasonic instruments into highly-effect tools for evaluating microvascular changes leading to common disabilities and major cardiovascular events. The three in vivo studies proposed in this plan are designed to develop and evaluate HOPE imaging specifically for PAD diagnosis, staging, and therapeutic monitoring. Project Narrative The project leverages a new understanding of the information contained within Doppler ultrasound echo signals to create a more effective diagnostic tool for imaging microvascular changes associated with peripheral arterial diseases. Success in this project brings effective new methods to existing instruments enabling medical professionals to track peripheral microvascular decline as well as treatment responses. This safe, low-cost method can also be applied to patients frequently helping them directly observe the effects of lifestyle decisions.",Ultrasonic perfusion imaging of peripheral vascular disease,9801057,R01HL148664,"['3-Dimensional', 'Anatomy', 'Angiography', 'Animals', 'Ankle', 'Blood', 'Blood Vessels', 'Blood flow', 'Cardiovascular system', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Color', 'Complex', 'Computer software', 'Contrast Media', 'Data', 'Development', 'Diabetic mouse', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease Progression', 'Doppler Ultrasound', 'Erythrocytes', 'Etiology', 'Evaluation', 'Event', 'Exercise Therapy', 'Failure', 'Family suidae', 'Frequencies', 'Gene Expression', 'Goals', 'Hindlimb', 'Human', 'Image', 'Imaging Device', 'Imaging Techniques', 'Injectable', 'Ischemia', 'Legal patent', 'Lesion', 'Life Style', 'Link', 'Measurement', 'Measures', 'Medical', 'Methods', 'Microspheres', 'Microvascular Dysfunction', 'Modeling', 'Monitor', 'Mus', 'Muscle', 'Names', 'Nitric Oxide', 'Operative Surgical Procedures', 'Patient Monitoring', 'Patients', 'Pattern', 'Performance', 'Perfusion', 'Peripheral', 'Peripheral Vascular Diseases', 'Peripheral arterial disease', 'Physiologic pulse', 'Pre-Clinical Model', 'Production', 'Property', 'Radioactive', 'Recovery', 'Research', 'Research Design', 'Resolution', 'Risk', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Source', 'Staging', 'Surgeon', 'Symptoms', 'Techniques', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Translating', 'Translations', 'Ultrasonics', 'Update', 'Work', 'angiogenesis', 'base', 'blood perfusion', 'cardiovascular health', 'cell motility', 'claudication', 'cohort', 'cost', 'design', 'diabetic', 'diabetic patient', 'disability', 'disease diagnosis', 'experimental study', 'imaging approach', 'improved', 'in vivo', 'in vivo imaging', 'indexing', 'instrument', 'melanoma', 'mouse model', 'multimodality', 'muscle metabolism', 'novel', 'nuclear imaging', 'off-patent', 'perfusion imaging', 'phantom model', 'quantitative imaging', 'research study', 'response', 'simulation', 'spatiotemporal', 'success', 'tool', 'treatment response']",NHLBI,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,R01,2019,610167,-0.03763574596958588
"Reduced-Order Constrained Optimization for Rapid IMRT and VMAT Treatment Planning    DESCRIPTION (provided by applicant): Intensity-modulated radiotherapy (IMRT) has revolutionized the treatment of cancers in the last decade, since it can tightly conform and escalate radiation dose to a tumor while simultaneously protecting nearby radiation- sensitive normal tissues, resulting in better local control and fewer post-treatment complications than previous techniques. However, the process of obtaining a clinically acceptable IMRT plan for a difficult site is still extremely slow, requiring many hours of a busy expert's time in a manual trial-and-error loop of parameter adjustment. The goal of this project is to drastically reduce the amount of time to obtain a clinically acceptable IMRT plan using a new automated method that directly applies constrained optimization in a computationally tractable and clinically meaningful way. The hypothesis is that clinical treatment planning times using this technique will be reduced from several hours to a matter of minutes.  The new approach, called ROCO (Reduced-Order Constrained Optimization) translates well-established concepts from optimization and machine learning theory to the novel application of IMRT planning, exploiting the speed and ease of unconstrained optimizations and introducing a dimensionality reduction step that makes true constrained optimization tractable. The Specific Aims of the proposal are to (1) apply Reduced-Order Con- strained Optimization to IMRT planning for non-small cell lung cancers and nasopharynx cancers, where the planning process is highly time-consuming; (2) develop and extend the Reduced-Order Constrained Optimization paradigm to a promising IMRT variant called Volumetric Modulated Arc Therapy (VMAT) for the prostate site, which is currently nearly clinically intractable to plan; and (3) integrate the new tools into the clinical IMRT planning process at Memorial Sloan-Kettering Cancer Center, using a powered study to verify the hypothesis that the proposed method significantly improves planning speed. The experiments will be designed in consultation with an expert clinical treatment planner and biostatistician, and carefully validated using anonymized data from approximately 50 patients for each site.  The main benefit of the proposed approach is to drastically reduce planning times, which is critical if IMRT and VMAT are to reach their full potential in clinical application. In a busy clinic, long planning times place a severe stress on available resources, and can result in treatment delays, acceptance of sub-optimal plans or - in the worst case - errors due to time pressure. In the longer term, the proposed approach will provide deeper insight into the critical elements of the dose optimization problem, significantly reduce the trial-and-error effort characteristic of current IMRT planning, and reduce subjectivity in treatment plan selection.       PUBLIC HEALTH RELEVANCE: Intensity-modulated radiation therapy (IMRT) is an extremely promising cancer treatment, but currently requires many hours of an expert treatment planner's time to obtain a plan that meets all the clinical constraints specified by the physician. This proposal describes a new, automatic method for treatment plan optimization that is very fast, requiring only minutes to produce a plan that directly meets all the physician's constraints, potentially saving much time for a busy clinic.         ",Reduced-Order Constrained Optimization for Rapid IMRT and VMAT Treatment Planning,8444578,R01CA148876,"['Acute', 'Address', 'Aftercare', 'Algorithms', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Treatment', 'Code', 'Collaborations', 'Complex', 'Computer software', 'Computers', 'Consultations', 'Data', 'Development', 'Dose', 'Dose-Limiting', 'Drug Formulations', 'Effectiveness', 'Elements', 'Goals', 'Head and Neck Neoplasms', 'Head and neck structure', 'Hour', 'Housing', 'Human', 'Intensity-Modulated Radiotherapy', 'Journals', 'Linear Accelerator Radiotherapy Systems', 'Lung', 'Machine Learning', 'Malignant neoplasm of nasopharynx', 'Manuals', 'Measures', 'Medical', 'Memorial Sloan-Kettering Cancer Center', 'Methods', 'Nasopharynx', 'Non-Small-Cell Lung Carcinoma', 'Normal tissue morphology', 'Organ', 'Paper', 'Parotid Gland', 'Patients', 'Phase', 'Physicians', 'Physics', 'Process', 'Prostate', 'Quality of life', 'Radiation', 'Radiation therapy', 'Resources', 'Risk', 'Sampling', 'Site', 'Solutions', 'Specific qualifier value', 'Speed', 'Staging', 'Stress', 'Sum', 'System', 'Techniques', 'Technology', 'Time', 'To specify', 'Toxic effect', 'Translating', 'Variant', 'Weight', 'Xerostomia', 'base', 'cancer therapy', 'clinical application', 'clinical practice', 'design', 'experience', 'image processing', 'improved', 'insight', 'meetings', 'novel', 'novel strategies', 'pressure', 'process optimization', 'public health relevance', 'rectal', 'research study', 'symposium', 'theories', 'time use', 'tool', 'treatment planning', 'tumor']",NCI,RENSSELAER POLYTECHNIC INSTITUTE,R01,2013,334665,-0.031950438686886594
"Reduced-Order Constrained Optimization for Rapid IMRT and VMAT Treatment Planning    DESCRIPTION (provided by applicant): Intensity-modulated radiotherapy (IMRT) has revolutionized the treatment of cancers in the last decade, since it can tightly conform and escalate radiation dose to a tumor while simultaneously protecting nearby radiation- sensitive normal tissues, resulting in better local control and fewer post-treatment complications than previous techniques. However, the process of obtaining a clinically acceptable IMRT plan for a difficult site is still extremely slow, requiring many hours of a busy expert's time in a manual trial-and-error loop of parameter adjustment. The goal of this project is to drastically reduce the amount of time to obtain a clinically acceptable IMRT plan using a new automated method that directly applies constrained optimization in a computationally tractable and clinically meaningful way. The hypothesis is that clinical treatment planning times using this technique will be reduced from several hours to a matter of minutes.  The new approach, called ROCO (Reduced-Order Constrained Optimization) translates well-established concepts from optimization and machine learning theory to the novel application of IMRT planning, exploiting the speed and ease of unconstrained optimizations and introducing a dimensionality reduction step that makes true constrained optimization tractable. The Specific Aims of the proposal are to (1) apply Reduced-Order Con- strained Optimization to IMRT planning for non-small cell lung cancers and nasopharynx cancers, where the planning process is highly time-consuming; (2) develop and extend the Reduced-Order Constrained Optimization paradigm to a promising IMRT variant called Volumetric Modulated Arc Therapy (VMAT) for the prostate site, which is currently nearly clinically intractable to plan; and (3) integrate the new tools into the clinical IMRT planning process at Memorial Sloan-Kettering Cancer Center, using a powered study to verify the hypothesis that the proposed method significantly improves planning speed. The experiments will be designed in consultation with an expert clinical treatment planner and biostatistician, and carefully validated using anonymized data from approximately 50 patients for each site.  The main benefit of the proposed approach is to drastically reduce planning times, which is critical if IMRT and VMAT are to reach their full potential in clinical application. In a busy clinic, long planning times place a severe stress on available resources, and can result in treatment delays, acceptance of sub-optimal plans or - in the worst case - errors due to time pressure. In the longer term, the proposed approach will provide deeper insight into the critical elements of the dose optimization problem, significantly reduce the trial-and-error effort characteristic of current IMRT planning, and reduce subjectivity in treatment plan selection.      PUBLIC HEALTH RELEVANCE: Intensity-modulated radiation therapy (IMRT) is an extremely promising cancer treatment, but currently requires many hours of an expert treatment planner's time to obtain a plan that meets all the clinical constraints specified by the physician. This proposal describes a new, automatic method for treatment plan optimization that is very fast, requiring only minutes to produce a plan that directly meets all the physician's constraints, potentially saving much time for a busy clinic.           Project Narrative Intensity-modulated radiation therapy (IMRT) is an extremely promising cancer treatment, but currently requires many hours of an expert treatment planner's time to obtain a plan that meets all the clinical constraints specified by the physician. This proposal describes a new, automatic method for treatment plan optimization that is very fast, requiring only minutes to produce a plan that directly meets all the physician's constraints, potentially saving much time for a busy clinic.",Reduced-Order Constrained Optimization for Rapid IMRT and VMAT Treatment Planning,8234172,R01CA148876,"['Acute', 'Address', 'Aftercare', 'Algorithms', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Treatment', 'Code', 'Collaborations', 'Complex', 'Computer software', 'Computers', 'Consultations', 'Data', 'Development', 'Dose', 'Dose-Limiting', 'Drug Formulations', 'Effectiveness', 'Elements', 'Goals', 'Head and Neck Neoplasms', 'Head and neck structure', 'Hour', 'Housing', 'Human', 'Intensity-Modulated Radiotherapy', 'Journals', 'Linear Accelerator Radiotherapy Systems', 'Lung', 'Machine Learning', 'Malignant neoplasm of nasopharynx', 'Manuals', 'Measures', 'Medical', 'Memorial Sloan-Kettering Cancer Center', 'Methods', 'Nasopharynx', 'Non-Small-Cell Lung Carcinoma', 'Normal tissue morphology', 'Organ', 'Paper', 'Parotid Gland', 'Patients', 'Phase', 'Physicians', 'Physics', 'Process', 'Prostate', 'Quality of life', 'Radiation', 'Radiation therapy', 'Resources', 'Risk', 'Sampling', 'Site', 'Solutions', 'Specific qualifier value', 'Speed', 'Staging', 'Stress', 'Sum', 'System', 'Techniques', 'Technology', 'Time', 'To specify', 'Toxic effect', 'Translating', 'Variant', 'Weight', 'Xerostomia', 'base', 'cancer therapy', 'clinical application', 'clinical practice', 'design', 'experience', 'image processing', 'improved', 'insight', 'meetings', 'novel', 'novel strategies', 'pressure', 'process optimization', 'public health relevance', 'rectal', 'research study', 'symposium', 'theories', 'time use', 'tool', 'treatment planning', 'tumor']",NCI,RENSSELAER POLYTECHNIC INSTITUTE,R01,2012,383930,-0.0280870229598926
"Reduced-Order Constrained Optimization for Rapid IMRT and VMAT Treatment Planning    DESCRIPTION (provided by applicant): Intensity-modulated radiotherapy (IMRT) has revolutionized the treatment of cancers in the last decade, since it can tightly conform and escalate radiation dose to a tumor while simultaneously protecting nearby radiation- sensitive normal tissues, resulting in better local control and fewer post-treatment complications than previous techniques. However, the process of obtaining a clinically acceptable IMRT plan for a difficult site is still extremely slow, requiring many hours of a busy expert's time in a manual trial-and-error loop of parameter adjustment. The goal of this project is to drastically reduce the amount of time to obtain a clinically acceptable IMRT plan using a new automated method that directly applies constrained optimization in a computationally tractable and clinically meaningful way. The hypothesis is that clinical treatment planning times using this technique will be reduced from several hours to a matter of minutes.  The new approach, called ROCO (Reduced-Order Constrained Optimization) translates well-established concepts from optimization and machine learning theory to the novel application of IMRT planning, exploiting the speed and ease of unconstrained optimizations and introducing a dimensionality reduction step that makes true constrained optimization tractable. The Specific Aims of the proposal are to (1) apply Reduced-Order Con- strained Optimization to IMRT planning for non-small cell lung cancers and nasopharynx cancers, where the planning process is highly time-consuming; (2) develop and extend the Reduced-Order Constrained Optimization paradigm to a promising IMRT variant called Volumetric Modulated Arc Therapy (VMAT) for the prostate site, which is currently nearly clinically intractable to plan; and (3) integrate the new tools into the clinical IMRT planning process at Memorial Sloan-Kettering Cancer Center, using a powered study to verify the hypothesis that the proposed method significantly improves planning speed. The experiments will be designed in consultation with an expert clinical treatment planner and biostatistician, and carefully validated using anonymized data from approximately 50 patients for each site.  The main benefit of the proposed approach is to drastically reduce planning times, which is critical if IMRT and VMAT are to reach their full potential in clinical application. In a busy clinic, long planning times place a severe stress on available resources, and can result in treatment delays, acceptance of sub-optimal plans or - in the worst case - errors due to time pressure. In the longer term, the proposed approach will provide deeper insight into the critical elements of the dose optimization problem, significantly reduce the trial-and-error effort characteristic of current IMRT planning, and reduce subjectivity in treatment plan selection.      PUBLIC HEALTH RELEVANCE: Intensity-modulated radiation therapy (IMRT) is an extremely promising cancer treatment, but currently requires many hours of an expert treatment planner's time to obtain a plan that meets all the clinical constraints specified by the physician. This proposal describes a new, automatic method for treatment plan optimization that is very fast, requiring only minutes to produce a plan that directly meets all the physician's constraints, potentially saving much time for a busy clinic.           Project Narrative Intensity-modulated radiation therapy (IMRT) is an extremely promising cancer treatment, but currently requires many hours of an expert treatment planner's time to obtain a plan that meets all the clinical constraints specified by the physician. This proposal describes a new, automatic method for treatment plan optimization that is very fast, requiring only minutes to produce a plan that directly meets all the physician's constraints, potentially saving much time for a busy clinic.",Reduced-Order Constrained Optimization for Rapid IMRT and VMAT Treatment Planning,8066714,R01CA148876,"['Acute', 'Address', 'Aftercare', 'Algorithms', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Treatment', 'Code', 'Collaborations', 'Complex', 'Computer software', 'Computers', 'Consultations', 'Data', 'Development', 'Dose', 'Dose-Limiting', 'Drug Formulations', 'Effectiveness', 'Elements', 'Goals', 'Head and Neck Neoplasms', 'Head and neck structure', 'Hour', 'Housing', 'Human', 'Intensity-Modulated Radiotherapy', 'Journals', 'Linear Accelerator Radiotherapy Systems', 'Lung', 'Machine Learning', 'Malignant neoplasm of nasopharynx', 'Manuals', 'Measures', 'Medical', 'Memorial Sloan-Kettering Cancer Center', 'Methods', 'Nasopharynx', 'Non-Small-Cell Lung Carcinoma', 'Normal tissue morphology', 'Organ', 'Paper', 'Parotid Gland', 'Patients', 'Phase', 'Physicians', 'Physics', 'Process', 'Prostate', 'Quality of life', 'Radiation', 'Radiation therapy', 'Resources', 'Risk', 'Sampling', 'Site', 'Solutions', 'Specific qualifier value', 'Speed', 'Staging', 'Stress', 'Sum', 'System', 'Techniques', 'Technology', 'Time', 'To specify', 'Toxic effect', 'Translating', 'Variant', 'Weight', 'Xerostomia', 'base', 'cancer therapy', 'clinical application', 'clinical practice', 'design', 'experience', 'image processing', 'improved', 'insight', 'meetings', 'novel', 'novel strategies', 'pressure', 'process optimization', 'public health relevance', 'rectal', 'research study', 'symposium', 'theories', 'time use', 'tool', 'treatment planning', 'tumor']",NCI,RENSSELAER POLYTECHNIC INSTITUTE,R01,2011,250094,-0.0280870229598926
"Reduced-Order Constrained Optimization for Rapid IMRT and VMAT Treatment Planning    DESCRIPTION (provided by applicant): Intensity-modulated radiotherapy (IMRT) has revolutionized the treatment of cancers in the last decade, since it can tightly conform and escalate radiation dose to a tumor while simultaneously protecting nearby radiation- sensitive normal tissues, resulting in better local control and fewer post-treatment complications than previous techniques. However, the process of obtaining a clinically acceptable IMRT plan for a difficult site is still extremely slow, requiring many hours of a busy expert's time in a manual trial-and-error loop of parameter adjustment. The goal of this project is to drastically reduce the amount of time to obtain a clinically acceptable IMRT plan using a new automated method that directly applies constrained optimization in a computationally tractable and clinically meaningful way. The hypothesis is that clinical treatment planning times using this technique will be reduced from several hours to a matter of minutes.  The new approach, called ROCO (Reduced-Order Constrained Optimization) translates well-established concepts from optimization and machine learning theory to the novel application of IMRT planning, exploiting the speed and ease of unconstrained optimizations and introducing a dimensionality reduction step that makes true constrained optimization tractable. The Specific Aims of the proposal are to (1) apply Reduced-Order Con- strained Optimization to IMRT planning for non-small cell lung cancers and nasopharynx cancers, where the planning process is highly time-consuming; (2) develop and extend the Reduced-Order Constrained Optimization paradigm to a promising IMRT variant called Volumetric Modulated Arc Therapy (VMAT) for the prostate site, which is currently nearly clinically intractable to plan; and (3) integrate the new tools into the clinical IMRT planning process at Memorial Sloan-Kettering Cancer Center, using a powered study to verify the hypothesis that the proposed method significantly improves planning speed. The experiments will be designed in consultation with an expert clinical treatment planner and biostatistician, and carefully validated using anonymized data from approximately 50 patients for each site.  The main benefit of the proposed approach is to drastically reduce planning times, which is critical if IMRT and VMAT are to reach their full potential in clinical application. In a busy clinic, long planning times place a severe stress on available resources, and can result in treatment delays, acceptance of sub-optimal plans or - in the worst case - errors due to time pressure. In the longer term, the proposed approach will provide deeper insight into the critical elements of the dose optimization problem, significantly reduce the trial-and-error effort characteristic of current IMRT planning, and reduce subjectivity in treatment plan selection.      PUBLIC HEALTH RELEVANCE: Intensity-modulated radiation therapy (IMRT) is an extremely promising cancer treatment, but currently requires many hours of an expert treatment planner's time to obtain a plan that meets all the clinical constraints specified by the physician. This proposal describes a new, automatic method for treatment plan optimization that is very fast, requiring only minutes to produce a plan that directly meets all the physician's constraints, potentially saving much time for a busy clinic.           Project Narrative Intensity-modulated radiation therapy (IMRT) is an extremely promising cancer treatment, but currently requires many hours of an expert treatment planner's time to obtain a plan that meets all the clinical constraints specified by the physician. This proposal describes a new, automatic method for treatment plan optimization that is very fast, requiring only minutes to produce a plan that directly meets all the physician's constraints, potentially saving much time for a busy clinic.",Reduced-Order Constrained Optimization for Rapid IMRT and VMAT Treatment Planning,7862840,R01CA148876,"['Acute', 'Address', 'Aftercare', 'Algorithms', 'Cancer Center', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Treatment', 'Code', 'Collaborations', 'Complex', 'Computer software', 'Computers', 'Consultations', 'Data', 'Development', 'Dose', 'Dose-Limiting', 'Drug Formulations', 'Effectiveness', 'Elements', 'Goals', 'Head and Neck Neoplasms', 'Head and neck structure', 'Hour', 'Housing', 'Human', 'Intensity-Modulated Radiotherapy', 'Journals', 'Linear Accelerator Radiotherapy Systems', 'Lung', 'Machine Learning', 'Malignant neoplasm of nasopharynx', 'Manuals', 'Measures', 'Medical', 'Memorial Sloan-Kettering Cancer Center', 'Methods', 'Nasopharynx', 'Non-Small-Cell Lung Carcinoma', 'Normal tissue morphology', 'Organ', 'Paper', 'Parotid Gland', 'Patients', 'Phase', 'Physicians', 'Physics', 'Process', 'Prostate', 'Quality of life', 'Radiation', 'Radiation therapy', 'Resources', 'Risk', 'Sampling', 'Site', 'Solutions', 'Specific qualifier value', 'Speed', 'Staging', 'Stress', 'Sum', 'System', 'Techniques', 'Technology', 'Time', 'To specify', 'Toxic effect', 'Translating', 'Variant', 'Weight', 'Xerostomia', 'base', 'cancer therapy', 'clinical application', 'clinical practice', 'design', 'experience', 'image processing', 'improved', 'insight', 'meetings', 'novel', 'novel strategies', 'pressure', 'process optimization', 'public health relevance', 'rectal', 'research study', 'symposium', 'theories', 'time use', 'tool', 'treatment planning', 'tumor']",NCI,RENSSELAER POLYTECHNIC INSTITUTE,R01,2010,297596,-0.0280870229598926
"Pro-inflammatory activation of human macrophages regulated by lncRNAs Project Summary Macrophage activation promotes major inflammatory disorders, including arterial diseases. Its underlying mechanisms, however, remain obscure. The present study will establish a systems approach, involving computational prediction analyses, multi-omics, network science, and in vitro and in vivo validation, to discover long noncoding RNA (lncRNA)-mediated mechanisms for pro-inflammatory activation of macrophages and arterial disease. In Specific Aim 1, we will involve omics studies of human macrophages to identify lncRNAs and their interacting proteins and develop computational analyses to predict human lncRNAs that regulate macrophage activation. Specific Aim 2 will examine the functionality of candidate lncRNAs in macrophage activation in vitro and in vivo. The findings from the study will help to identify new mechanisms for macrophage activation and may provide molecular bases for new therapies. Project Narrative Inflammation plays a key role in coronary artery disease and other major vascular diseases, global health threats. Even with potent risk modifiers, e.g., statins, many patients still suffer vascular events. Long noncoding RNAs (lncRNAs) regulate various biological processes. We aim to discover lncRNAs that promotes vascular inflammation. The potential outcomes will offer new targets for much needed therapies for vascular diseases.",Pro-inflammatory activation of human macrophages regulated by lncRNAs,9973174,R01HL149302,"['Address', 'Biological', 'Biological Process', 'Biology', 'Blood', 'Blood Vessels', 'Cells', 'Communities', 'Complex', 'Computational Biology', 'Computer Analysis', 'Coronary Arteriosclerosis', 'Data', 'Development', 'Discipline', 'Disease', 'Drug usage', 'Endotoxemia', 'Event', 'Gene Expression Profiling', 'Goals', 'Hematopoietic Stem Cell Transplantation', 'Heterogeneity', 'Human', 'In Vitro', 'Inflammation', 'Inflammatory', 'Laboratories', 'Lesion', 'Leukocytes', 'Life', 'Link', 'Machine Learning', 'Macrophage Activation', 'Mechanics', 'Mediating', 'Methods', 'Molecular', 'Myocardial', 'NF-kappa B', 'Network-based', 'Outcome', 'Pathway Analysis', 'Patients', 'Plasma', 'Play', 'Protein Analysis', 'Proteins', 'Proteomics', 'RNA', 'Reporting', 'Residual state', 'Risk', 'Risk Factors', 'Role', 'Science', 'Signal Transduction', 'Small Interfering RNA', 'Splenocyte', 'System', 'Systems Biology', 'Tissues', 'Untranslated RNA', 'Validation', 'Vascular Diseases', 'arterial lesion', 'base', 'cytokine', 'experimental study', 'femoral artery', 'gain of function', 'global health', 'human disease', 'humanized mouse', 'in vivo', 'injured', 'loss of function', 'macrophage', 'modifiable risk', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel therapeutics', 'overexpression', 'protein protein interaction', 'single cell analysis', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'vascular inflammation']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2020,726322,-0.08993259245393931
"Pro-inflammatory activation of human macrophages regulated by lncRNAs Project Summary Macrophage activation promotes major inflammatory disorders, including arterial diseases. Its underlying mechanisms, however, remain obscure. The present study will establish a systems approach, involving computational prediction analyses, multi-omics, network science, and in vitro and in vivo validation, to discover long noncoding RNA (lncRNA)-mediated mechanisms for pro-inflammatory activation of macrophages and arterial disease. In Specific Aim 1, we will involve omics studies of human macrophages to identify lncRNAs and their interacting proteins and develop computational analyses to predict human lncRNAs that regulate macrophage activation. Specific Aim 2 will examine the functionality of candidate lncRNAs in macrophage activation in vitro and in vivo. The findings from the study will help to identify new mechanisms for macrophage activation and may provide molecular bases for new therapies. Project Narrative Inflammation plays a key role in coronary artery disease and other major vascular diseases, global health threats. Even with potent risk modifiers, e.g., statins, many patients still suffer vascular events. Long noncoding RNAs (lncRNAs) regulate various biological processes. We aim to discover lncRNAs that promotes vascular inflammation. The potential outcomes will offer new targets for much needed therapies for vascular diseases.",Pro-inflammatory activation of human macrophages regulated by lncRNAs,9840015,R01HL149302,"['Address', 'Biological', 'Biological Process', 'Biology', 'Blood', 'Blood Vessels', 'Cells', 'Communities', 'Complex', 'Computational Biology', 'Computer Analysis', 'Coronary Arteriosclerosis', 'Data', 'Development', 'Discipline', 'Disease', 'Drug usage', 'Endotoxemia', 'Event', 'Gene Expression Profiling', 'Goals', 'Hematopoietic Stem Cell Transplantation', 'Heterogeneity', 'Human', 'In Vitro', 'Inflammation', 'Inflammatory', 'Laboratories', 'Lesion', 'Leukocytes', 'Life', 'Link', 'Machine Learning', 'Macrophage Activation', 'Mechanics', 'Mediating', 'Methods', 'Molecular', 'Myocardial', 'NF-kappa B', 'Network-based', 'Outcome', 'Pathway Analysis', 'Patients', 'Plasma', 'Play', 'Protein Analysis', 'Proteins', 'Proteomics', 'RNA', 'Reporting', 'Residual state', 'Risk', 'Risk Factors', 'Role', 'Science', 'Signal Transduction', 'Small Interfering RNA', 'Splenocyte', 'System', 'Systems Biology', 'Tissues', 'Untranslated RNA', 'Validation', 'Vascular Diseases', 'arterial lesion', 'base', 'cytokine', 'experimental study', 'femoral artery', 'gain of function', 'global health', 'human disease', 'humanized mouse', 'in vivo', 'injured', 'loss of function', 'macrophage', 'modifiable risk', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel therapeutics', 'overexpression', 'protein protein interaction', 'single cell analysis', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'vascular inflammation']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2019,767374,-0.08993259245393931
"MRI based phosphocreatine mapping method to assess patients with peripheral arterial disease. Peripheral arterial disease (PAD) is caused by atherosclerosis, the buildup of plaque that can obstruct blood flow in the arteries to the lower extremities. The current assessment of patients with PAD targets the anatomic or hemodynamic burden of atherosclerotic plaque stenosis with measurement of ankle-brachial index (ABI), and several imaging other techniques. However, anatomic and hemodynamic indices do not always correlate with the functional limitations and disability that PAD patients experience, and prior work suggests that the PAD population would benefit from more specific functional tissue tests. We hypothesize that metabolic maps of phosphocreatine (PCr) measures, reflecting severe skeletal muscle (SM) ischemia or downstream mitochondrial changes, may fill that gap. PCr is the most abundant high- energy phosphate present in muscle. Energy metabolism and PCr play a vital role in cellular homeostasis, but there currently are no routine diagnostic tests to noninvasively quantify or map the distribution of PCr in patients with PAD.  Phosphorus (31P) magnetic resonance spectroscopy (MRS) is arguably the gold standard for the noninvasive assessment of SM mitochondrial function and high-energy phosphate content. However, due to the relatively low MR detection sensitivity and the requirement for unique hardware, 31P MRS is not used in routine clinical applications. Chemical exchange saturation transfer (CEST) MRI has emerged as a novel, high-sensitivity technique that may overcome several of the limitations of 31P MRS. However, CEST MRI is still under development and one major impediment for more widespread application is limited specificity for a particular metabolite due to spectral overlap of CEST signal from other metabolites and proteins and as well as the background signal from semi-solid macromolecules and direct saturation of water Our long-term goal is to develop clinically translatable CEST methods to extract and quantity PCr concentrations in skeletal muscle that provides a sensitive MRI approach to assess SM metabolism. If successful, this new technique should provide a completely new and sensitive method for detecting PCr in calf muscle and may play a pivotal role for the evaluation of regional musle pathophysiology change in many musculoskeletal diseases.  We recently developed two new CEST techniques, dubbed as polynomial and Lorentzian line-shape fitting (PLOF) method and artificial neural network based CEST quantification method (ANNCEST) that are able to detect PCr signal with high sensitivity and specificity. We will develop and optimize the PLOF and ANNCEST methods for PCr mapping through one novel animal model and in-magnet plantar flexion exercise for human leg. The optimized CEST MRI methods will be applied on PAD patients to validate that PCr dynamic curve is correlated with the severity of the PAD. Upon the successful completion of this proposal, we anticipate developing the first rapid, high-resolution skeletal muscle energetic functional exercise test. Our overall goal is to develop and optimize novel, sensitive and potentially widely available MRI techniques to characterize the regional distribution and kinetics of phosphocreatine, which reflects severe skeletal muscle ischemia or downstream mitochondrial changes in peripheral arterial disease. The technique is achieved through a novel MRI contrast mechanism, chemical exchange saturation transfer, and will be developed and applied on both animal model with low phosphocreatine concentration and patients with peripheral arterial disease. The new method can be translated to other clinical applications and metabolic diseases such as aging, myopathies, diabetes, obesity, heart failure where skeletal muscle abnormalities may contribute to exercise intolerance and fatigue.",MRI based phosphocreatine mapping method to assess patients with peripheral arterial disease.,10049670,R01HL149742,"['Address', 'Aging', 'Anatomy', 'Animal Model', 'Ankle', 'Arterial Fatty Streak', 'Arteries', 'Atherosclerosis', 'Blood flow', 'Brain', 'Chemicals', 'Clinical', 'Creatine', 'Data', 'Detection', 'Development', 'Diabetes Mellitus', 'Energy Metabolism', 'Evaluation', 'Exercise', 'Exercise Test', 'Fatigue', 'Functional disorder', 'Funding', 'Glutamates', 'Glycogen', 'Glycosaminoglycans', 'Goals', 'Gold', 'Guanidinoacetate N-Methyltransferase', 'Heart failure', 'Homeostasis', 'Human', 'Image', 'Imaging Techniques', 'Ischemia', 'Kinetics', 'Knowledge', 'Leg', 'Lower Extremity', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Maps', 'Measurement', 'Measures', 'Metabolic', 'Metabolic Diseases', 'Metabolism', 'Methods', 'Mitochondria', 'Mus', 'Muscle', 'Muscle Mitochondria', 'Musculoskeletal Diseases', 'Myopathy', 'Network-based', 'Obesity', 'Patients', 'Peripheral arterial disease', 'Phosphocreatine', 'Phosphorus', 'Play', 'Polynomial Models', 'Population', 'Proteins', 'Recovery', 'Reproducibility', 'Resolution', 'Rodent', 'Role', 'Routine Diagnostic Tests', 'Scanning', 'Sensitivity and Specificity', 'Severities', 'Shapes', 'Signal Transduction', 'Skeletal Muscle', 'Solid', 'Specificity', 'Stenosis', 'Stress', 'Techniques', 'Testing', 'Time', 'Tissues', 'Translating', 'Water', 'Work', 'anatomic imaging', 'artificial neural network', 'base', 'clinical application', 'clinical practice', 'clinically translatable', 'disability', 'exercise intolerance', 'experience', 'hemodynamics', 'imaging platform', 'indexing', 'inorganic phosphate', 'macromolecule', 'magnetic field', 'mouse model', 'novel', 'radio frequency', 'rapid technique', 'skeletal muscle metabolism', 'spectroscopic imaging', 'success', 'validation studies']",NHLBI,HUGO W. MOSER RES INST KENNEDY KRIEGER,R01,2020,477894,-0.045044347217928486
"Decision support for dose prescription in radiation treatment planning    DESCRIPTION (provided by applicant):  Recent advances in radiation therapy [1], such as Intensity Modulated Radiotherapy (IMRT) and Image-Guided Radiotherapy (IGRT), offer the ability to maximize tumor control while reducing the risk of radiation-induced damage to adjacent normal tissue. Typically, radiation therapy involves three phases: (1) prescription - where radiation oncologists (physicians) specify the dose constraints for targets and organs at risk (OAR); (2) planning - where treatment planners (physicists, dosimetrists) determine the treatment parameters to achieve the prescribed dose constraints; and (3) treatment - where therapists carry out the plan to treat the patients. In current practice, radiation oncologists typically draw on a variety of sources for dose prescription, including the 1991 ""Emami"" paper [8] on normal tissue tolerance, updated guidance from QUANTEC, other data in journals and texts, and their personal experiences. While these provide a general understanding of the dependence of normal tissue complication on dose distribution or the upper limits of the organ tolerance in populations of patients, their application to an individual patient is less certain and precise. Application of data and guidelines that are available in the literature is further complicated by the fact that this information is available only as narrative texts, tables and charts that are difficult to quantitatively integrate into clinical practice. Furthermore, the existing guidelines do not consider patient specific information regarding the ideal dose distribution achievable at individual treatments [9]. Radiation oncologists are frequently forced to make difficult prescription decisions by synthesizing available population level guidelines, personal experience, and their understanding of the specific patient needs on an ad hoc basis. Our overarching goal is to improve outcome by providing evidence-based decision support for radiation oncologists, planners, and therapists in every phase of the treatment process. In this project we propose to develop practical and clinically useful decision support tools to help radiation oncologists prescribe patient- specific optimal dose constraints. The specific aims are (1) Provide radiation oncologists with reliable predictions of patient-specific dose distributions achievable for the patient's anatomy and tumor volume; and (2) Provide radiation oncologists with intuitive tools that integrate patient-specific dose predictions with population-based dose guidelines to support prescription decision making. We believe the technologies developed in this project will not only improve the quality of radiotherapy prescriptions but also reduce planning time with optimal dose constraints and improve clinical outcomes.         In this project we propose to develop practical and clinically useful decision support tools to help radiation oncologists prescribe patient-specific optimal dose constraints. The technologies developed in this project will not only improve the quality of radiotherapy prescriptions but also reduce planning time with optimal dose constraints and improve clinical outcomes.         ",Decision support for dose prescription in radiation treatment planning,8507627,R21CA161389,"['Anatomy', 'Charts and Tables', 'Clinic', 'Clinical', 'Cognitive', 'Complication', 'Data', 'Databases', 'Decision Making', 'Dependence', 'Dose', 'Goals', 'Guidelines', 'Image', 'Imagery', 'Individual', 'Intensity-Modulated Radiotherapy', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Methods', 'Modeling', 'Normal tissue morphology', 'Organ', 'Outcome', 'Paper', 'Patients', 'Phase', 'Physicians', 'Population', 'Process', 'Radiation', 'Radiation Oncologist', 'Radiation induced damage', 'Radiation therapy', 'Research', 'Risk', 'Source', 'Specific qualifier value', 'Technology', 'Text', 'Time', 'Toxic effect', 'Tumor Volume', 'Update', 'Work', 'base', 'clinical practice', 'evidence base', 'experience', 'improved', 'patient population', 'population based', 'tool', 'treatment planning', 'tumor']",NCI,UNIVERSITY OF NORTH CAROLINA CHARLOTTE,R21,2013,158328,-0.022839031465232147
"Decision support for dose prescription in radiation treatment planning    DESCRIPTION (provided by applicant):  Recent advances in radiation therapy [1], such as Intensity Modulated Radiotherapy (IMRT) and Image-Guided Radiotherapy (IGRT), offer the ability to maximize tumor control while reducing the risk of radiation-induced damage to adjacent normal tissue. Typically, radiation therapy involves three phases: (1) prescription - where radiation oncologists (physicians) specify the dose constraints for targets and organs at risk (OAR); (2) planning - where treatment planners (physicists, dosimetrists) determine the treatment parameters to achieve the prescribed dose constraints; and (3) treatment - where therapists carry out the plan to treat the patients. In current practice, radiation oncologists typically draw on a variety of sources for dose prescription, including the 1991 ""Emami"" paper [8] on normal tissue tolerance, updated guidance from QUANTEC, other data in journals and texts, and their personal experiences. While these provide a general understanding of the dependence of normal tissue complication on dose distribution or the upper limits of the organ tolerance in populations of patients, their application to an individual patient is less certain and precise. Application of data and guidelines that are available in the literature is further complicated by the fact that this information is available only as narrative texts, tables and charts that are difficult to quantitatively integrate into clinical practice. Furthermore, the existing guidelines do not consider patient specific information regarding the ideal dose distribution achievable at individual treatments [9]. Radiation oncologists are frequently forced to make difficult prescription decisions by synthesizing available population level guidelines, personal experience, and their understanding of the specific patient needs on an ad hoc basis. Our overarching goal is to improve outcome by providing evidence-based decision support for radiation oncologists, planners, and therapists in every phase of the treatment process. In this project we propose to develop practical and clinically useful decision support tools to help radiation oncologists prescribe patient- specific optimal dose constraints. The specific aims are (1) Provide radiation oncologists with reliable predictions of patient-specific dose distributions achievable for the patient's anatomy and tumor volume; and (2) Provide radiation oncologists with intuitive tools that integrate patient-specific dose predictions with population-based dose guidelines to support prescription decision making. We believe the technologies developed in this project will not only improve the quality of radiotherapy prescriptions but also reduce planning time with optimal dose constraints and improve clinical outcomes.      PUBLIC HEALTH RELEVANCE:  In this project we propose to develop practical and clinically useful decision support tools to help radiation oncologists prescribe patient-specific optimal dose constraints. The technologies developed in this project will not only improve the quality of radiotherapy prescriptions but also reduce planning time with optimal dose constraints and improve clinical outcomes.            In this project we propose to develop practical and clinically useful decision support tools to help radiation oncologists prescribe patient-specific optimal dose constraints. The technologies developed in this project will not only improve the quality of radiotherapy prescriptions but also reduce planning time with optimal dose constraints and improve clinical outcomes.         ",Decision support for dose prescription in radiation treatment planning,8242945,R21CA161389,"['Anatomy', 'Charts and Tables', 'Clinic', 'Clinical', 'Cognitive', 'Complication', 'Data', 'Databases', 'Decision Making', 'Dependence', 'Dose', 'Goals', 'Guidelines', 'Image', 'Imagery', 'Individual', 'Intensity-Modulated Radiotherapy', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Methods', 'Modeling', 'Normal tissue morphology', 'Organ', 'Outcome', 'Paper', 'Patients', 'Phase', 'Physicians', 'Population', 'Process', 'Radiation', 'Radiation Oncologist', 'Radiation induced damage', 'Radiation therapy', 'Research', 'Risk', 'Source', 'Specific qualifier value', 'Technology', 'Text', 'Time', 'Toxic effect', 'Tumor Volume', 'Update', 'Work', 'base', 'clinical practice', 'evidence base', 'experience', 'improved', 'patient population', 'population based', 'tool', 'treatment planning', 'tumor']",NCI,WAKE FOREST UNIVERSITY HEALTH SCIENCES,R21,2012,272,-0.018810701168122525
"MRI-Based Radiation Therapy Treatment Planning ﻿    DESCRIPTION (provided by applicant): CT is currently the gold standard in radiation therapy treatment planning. MRI provides a number of advantages over CT, including improved accuracy of target delineation, reduced radiation exposure, and simplified clinical workflow. There are two major technical hurdles that are impeding the clinical adoption of MRI-based radiation treatment planning: (1) geometric distortion, and (2) lack of electron density information. The goal of this project is to develop novel image analysis and computational tools to enable MRI-based radiation treatment planning. We hypothesize that accurate patient geometry and electron density information can be derived from MRI if the appropriate MR image acquisition, reconstruction, and analysis methods are applied. In Aim 1, we will improve the geometric accuracy of MRI by minimizing system-level and patient- specific distortions. To maintain sufficient system-level accuracy, we will perform comprehensive machine- specific calibrations and ongoing quality assurance procedures. To correct patient-induced distortions, we will develop novel computational tools to derive a detailed magnetic field distortion map based on physical principles, which is used to correct susceptibility-induced spatial distortions. In Aim 2, we will develop a unifying Bayesian method for quantitative electron density mapping, by combining the complementary intensity and geometry information. By utilizing multiple patient atlases and panoramic, multi-parametric MRI with differential contrast, we will apply machine learning techniques to encode the information given by intensity and geometry into two conditional probability density functions. These will be combined into one unifying posterior probability density function, which provides the optimal electron density on a continuous scale. In Aim 3, we will clinically evaluate the geometric and dosimetric accuracy of MRI for treatment planning in terms of 3 primary end points: (1) organ contours, (2) patient setup based on reference images, and (3) 3D dose distributions (both photon and proton), using CT as the ground truth. These evaluations will be conducted through patient studies at multiple disease sites, including brain, head and neck, and prostate. Success of the project will afford distortion-free MRI with reliable, quantitative electron density information. This will pave the way for MRI-based radiation treatment planning, leading to an improved accuracy in the overall radiation therapy process. It will streamline the treatment workflow for the MRI-guided radiation delivery systems under active development. With minimal modification, the proposed techniques can be applied to MR-based PET attenuation correction in PET/MR imaging. More broadly, the unifying Bayesian formalism can be used to improve current imaging biomarkers by integrating a wide variety of disparate information including anatomical and functional imaging such as perfusion/diffusion-weighted imaging and MR spectroscopic imaging. It will facilitate the incorporation of multimodality MRI into the entire process of cancer management: diagnosis, staging, radiation treatment planning, and treatment response assessment. PUBLIC HEALTH RELEVANCE:  The goal of this project is to develop novel image analysis and computational tools to enable MRI-based radiation therapy treatment planning. Success of the project will overcome the major technical hurdles in the use of MRI in radiation therapy and will afford distortion-free MRI with reliable, quantitative electron density information. It will pve the way for MRI-based radiation treatment planning, leading to an improved accuracy in the overall radiation therapy process.",MRI-Based Radiation Therapy Treatment Planning,9843490,R01CA193730,"['3-Dimensional', 'Adopted', 'Adoption', 'Anatomy', 'Atlases', 'Bayesian Method', 'Bayesian Modeling', 'Brain', 'Calibration', 'Clinical', 'Data', 'Development', 'Diagnosis', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Dose', 'Evaluation', 'Functional Imaging', 'Geometry', 'Goals', 'Gold', 'Head and neck structure', 'Image', 'Image Analysis', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Methods', 'Modeling', 'Modification', 'Organ', 'Patients', 'Perfusion', 'Photons', 'Positron-Emission Tomography', 'Predisposition', 'Probability', 'Procedures', 'Process', 'Prostate', 'Protons', 'Radiation Dose Unit', 'Radiation exposure', 'Radiation therapy', 'Site', 'Source', 'Staging', 'System', 'Techniques', 'anatomic imaging', 'attenuation', 'base', 'computerized tools', 'cost', 'density', 'electron density', 'image guided', 'imaging biomarker', 'imaging modality', 'improved', 'magnetic field', 'multimodality', 'novel', 'primary endpoint', 'public health relevance', 'quality assurance', 'radiation delivery', 'reconstruction', 'spectroscopic imaging', 'success', 'tool', 'treatment planning', 'treatment response']",NCI,STANFORD UNIVERSITY,R01,2020,354198,-0.012098916577924822
"MRI-Based Radiation Therapy Treatment Planning ﻿    DESCRIPTION (provided by applicant): CT is currently the gold standard in radiation therapy treatment planning. MRI provides a number of advantages over CT, including improved accuracy of target delineation, reduced radiation exposure, and simplified clinical workflow. There are two major technical hurdles that are impeding the clinical adoption of MRI-based radiation treatment planning: (1) geometric distortion, and (2) lack of electron density information. The goal of this project is to develop novel image analysis and computational tools to enable MRI-based radiation treatment planning. We hypothesize that accurate patient geometry and electron density information can be derived from MRI if the appropriate MR image acquisition, reconstruction, and analysis methods are applied. In Aim 1, we will improve the geometric accuracy of MRI by minimizing system-level and patient- specific distortions. To maintain sufficient system-level accuracy, we will perform comprehensive machine- specific calibrations and ongoing quality assurance procedures. To correct patient-induced distortions, we will develop novel computational tools to derive a detailed magnetic field distortion map based on physical principles, which is used to correct susceptibility-induced spatial distortions. In Aim 2, we will develop a unifying Bayesian method for quantitative electron density mapping, by combining the complementary intensity and geometry information. By utilizing multiple patient atlases and panoramic, multi-parametric MRI with differential contrast, we will apply machine learning techniques to encode the information given by intensity and geometry into two conditional probability density functions. These will be combined into one unifying posterior probability density function, which provides the optimal electron density on a continuous scale. In Aim 3, we will clinically evaluate the geometric and dosimetric accuracy of MRI for treatment planning in terms of 3 primary end points: (1) organ contours, (2) patient setup based on reference images, and (3) 3D dose distributions (both photon and proton), using CT as the ground truth. These evaluations will be conducted through patient studies at multiple disease sites, including brain, head and neck, and prostate. Success of the project will afford distortion-free MRI with reliable, quantitative electron density information. This will pave the way for MRI-based radiation treatment planning, leading to an improved accuracy in the overall radiation therapy process. It will streamline the treatment workflow for the MRI-guided radiation delivery systems under active development. With minimal modification, the proposed techniques can be applied to MR-based PET attenuation correction in PET/MR imaging. More broadly, the unifying Bayesian formalism can be used to improve current imaging biomarkers by integrating a wide variety of disparate information including anatomical and functional imaging such as perfusion/diffusion-weighted imaging and MR spectroscopic imaging. It will facilitate the incorporation of multimodality MRI into the entire process of cancer management: diagnosis, staging, radiation treatment planning, and treatment response assessment. PUBLIC HEALTH RELEVANCE:  The goal of this project is to develop novel image analysis and computational tools to enable MRI-based radiation therapy treatment planning. Success of the project will overcome the major technical hurdles in the use of MRI in radiation therapy and will afford distortion-free MRI with reliable, quantitative electron density information. It will pve the way for MRI-based radiation treatment planning, leading to an improved accuracy in the overall radiation therapy process.",MRI-Based Radiation Therapy Treatment Planning,9624738,R01CA193730,"['3-Dimensional', 'Adopted', 'Adoption', 'Anatomy', 'Atlases', 'Bayesian Method', 'Bayesian Modeling', 'Brain', 'Calibration', 'Clinical', 'Data', 'Development', 'Diagnosis', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Dose', 'Evaluation', 'Functional Imaging', 'Geometry', 'Goals', 'Gold', 'Head and neck structure', 'Image', 'Image Analysis', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Methods', 'Modeling', 'Modification', 'Organ', 'Patients', 'Perfusion', 'Photons', 'Positron-Emission Tomography', 'Predisposition', 'Probability', 'Procedures', 'Process', 'Prostate', 'Protons', 'Radiation Dose Unit', 'Radiation exposure', 'Radiation therapy', 'Site', 'Source', 'Staging', 'System', 'Techniques', 'anatomic imaging', 'attenuation', 'base', 'computerized tools', 'cost', 'density', 'electron density', 'image guided', 'imaging biomarker', 'imaging modality', 'improved', 'magnetic field', 'multimodality', 'novel', 'primary endpoint', 'public health relevance', 'quality assurance', 'radiation delivery', 'reconstruction', 'spectroscopic imaging', 'success', 'tool', 'treatment planning', 'treatment response']",NCI,STANFORD UNIVERSITY,R01,2019,343572,-0.012098916577924822
"MRI-Based Radiation Therapy Treatment Planning ﻿    DESCRIPTION (provided by applicant): CT is currently the gold standard in radiation therapy treatment planning. MRI provides a number of advantages over CT, including improved accuracy of target delineation, reduced radiation exposure, and simplified clinical workflow. There are two major technical hurdles that are impeding the clinical adoption of MRI-based radiation treatment planning: (1) geometric distortion, and (2) lack of electron density information. The goal of this project is to develop novel image analysis and computational tools to enable MRI-based radiation treatment planning. We hypothesize that accurate patient geometry and electron density information can be derived from MRI if the appropriate MR image acquisition, reconstruction, and analysis methods are applied. In Aim 1, we will improve the geometric accuracy of MRI by minimizing system-level and patient- specific distortions. To maintain sufficient system-level accuracy, we will perform comprehensive machine- specific calibrations and ongoing quality assurance procedures. To correct patient-induced distortions, we will develop novel computational tools to derive a detailed magnetic field distortion map based on physical principles, which is used to correct susceptibility-induced spatial distortions. In Aim 2, we will develop a unifying Bayesian method for quantitative electron density mapping, by combining the complementary intensity and geometry information. By utilizing multiple patient atlases and panoramic, multi-parametric MRI with differential contrast, we will apply machine learning techniques to encode the information given by intensity and geometry into two conditional probability density functions. These will be combined into one unifying posterior probability density function, which provides the optimal electron density on a continuous scale. In Aim 3, we will clinically evaluate the geometric and dosimetric accuracy of MRI for treatment planning in terms of 3 primary end points: (1) organ contours, (2) patient setup based on reference images, and (3) 3D dose distributions (both photon and proton), using CT as the ground truth. These evaluations will be conducted through patient studies at multiple disease sites, including brain, head and neck, and prostate. Success of the project will afford distortion-free MRI with reliable, quantitative electron density information. This will pave the way for MRI-based radiation treatment planning, leading to an improved accuracy in the overall radiation therapy process. It will streamline the treatment workflow for the MRI-guided radiation delivery systems under active development. With minimal modification, the proposed techniques can be applied to MR-based PET attenuation correction in PET/MR imaging. More broadly, the unifying Bayesian formalism can be used to improve current imaging biomarkers by integrating a wide variety of disparate information including anatomical and functional imaging such as perfusion/diffusion-weighted imaging and MR spectroscopic imaging. It will facilitate the incorporation of multimodality MRI into the entire process of cancer management: diagnosis, staging, radiation treatment planning, and treatment response assessment. PUBLIC HEALTH RELEVANCE:  The goal of this project is to develop novel image analysis and computational tools to enable MRI-based radiation therapy treatment planning. Success of the project will overcome the major technical hurdles in the use of MRI in radiation therapy and will afford distortion-free MRI with reliable, quantitative electron density information. It will pve the way for MRI-based radiation treatment planning, leading to an improved accuracy in the overall radiation therapy process.",MRI-Based Radiation Therapy Treatment Planning,9414991,R01CA193730,"['Adopted', 'Adoption', 'Anatomy', 'Atlases', 'Bayesian Method', 'Bayesian Modeling', 'Brain', 'Calibration', 'Clinical', 'Data', 'Development', 'Diagnosis', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Dose', 'Evaluation', 'Functional Imaging', 'Geometry', 'Goals', 'Gold', 'Head and neck structure', 'Image', 'Image Analysis', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Methods', 'Modality', 'Modeling', 'Modification', 'Organ', 'Patients', 'Perfusion', 'Photons', 'Positron-Emission Tomography', 'Predisposition', 'Probability', 'Procedures', 'Process', 'Prostate', 'Protons', 'Radiation', 'Radiation exposure', 'Radiation therapy', 'Site', 'Source', 'Staging', 'System', 'Techniques', 'anatomic imaging', 'attenuation', 'base', 'computerized tools', 'cost', 'density', 'electron density', 'image guided', 'imaging biomarker', 'imaging modality', 'improved', 'magnetic field', 'multimodality', 'novel', 'primary endpoint', 'public health relevance', 'quality assurance', 'reconstruction', 'spectroscopic imaging', 'success', 'tool', 'treatment planning', 'treatment response']",NCI,STANFORD UNIVERSITY,R01,2018,354198,-0.012098916577924822
"MRI-Based Radiation Therapy Treatment Planning ﻿    DESCRIPTION (provided by applicant): CT is currently the gold standard in radiation therapy treatment planning. MRI provides a number of advantages over CT, including improved accuracy of target delineation, reduced radiation exposure, and simplified clinical workflow. There are two major technical hurdles that are impeding the clinical adoption of MRI-based radiation treatment planning: (1) geometric distortion, and (2) lack of electron density information. The goal of this project is to develop novel image analysis and computational tools to enable MRI-based radiation treatment planning. We hypothesize that accurate patient geometry and electron density information can be derived from MRI if the appropriate MR image acquisition, reconstruction, and analysis methods are applied. In Aim 1, we will improve the geometric accuracy of MRI by minimizing system-level and patient- specific distortions. To maintain sufficient system-level accuracy, we will perform comprehensive machine- specific calibrations and ongoing quality assurance procedures. To correct patient-induced distortions, we will develop novel computational tools to derive a detailed magnetic field distortion map based on physical principles, which is used to correct susceptibility-induced spatial distortions. In Aim 2, we will develop a unifying Bayesian method for quantitative electron density mapping, by combining the complementary intensity and geometry information. By utilizing multiple patient atlases and panoramic, multi-parametric MRI with differential contrast, we will apply machine learning techniques to encode the information given by intensity and geometry into two conditional probability density functions. These will be combined into one unifying posterior probability density function, which provides the optimal electron density on a continuous scale. In Aim 3, we will clinically evaluate the geometric and dosimetric accuracy of MRI for treatment planning in terms of 3 primary end points: (1) organ contours, (2) patient setup based on reference images, and (3) 3D dose distributions (both photon and proton), using CT as the ground truth. These evaluations will be conducted through patient studies at multiple disease sites, including brain, head and neck, and prostate. Success of the project will afford distortion-free MRI with reliable, quantitative electron density information. This will pave the way for MRI-based radiation treatment planning, leading to an improved accuracy in the overall radiation therapy process. It will streamline the treatment workflow for the MRI-guided radiation delivery systems under active development. With minimal modification, the proposed techniques can be applied to MR-based PET attenuation correction in PET/MR imaging. More broadly, the unifying Bayesian formalism can be used to improve current imaging biomarkers by integrating a wide variety of disparate information including anatomical and functional imaging such as perfusion/diffusion-weighted imaging and MR spectroscopic imaging. It will facilitate the incorporation of multimodality MRI into the entire process of cancer management: diagnosis, staging, radiation treatment planning, and treatment response assessment. PUBLIC HEALTH RELEVANCE:  The goal of this project is to develop novel image analysis and computational tools to enable MRI-based radiation therapy treatment planning. Success of the project will overcome the major technical hurdles in the use of MRI in radiation therapy and will afford distortion-free MRI with reliable, quantitative electron density information. It will pve the way for MRI-based radiation treatment planning, leading to an improved accuracy in the overall radiation therapy process.",MRI-Based Radiation Therapy Treatment Planning,9197624,R01CA193730,"['Adopted', 'Adoption', 'Anatomy', 'Atlases', 'Bayesian Method', 'Bayesian Modeling', 'Brain', 'Calibration', 'Clinical', 'Data', 'Development', 'Diagnosis', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Dose', 'Evaluation', 'Functional Imaging', 'Geometry', 'Goals', 'Gold', 'Head and neck structure', 'Image', 'Image Analysis', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Methods', 'Modality', 'Modeling', 'Modification', 'Organ', 'Patients', 'Perfusion', 'Photons', 'Positron-Emission Tomography', 'Predisposition', 'Probability', 'Procedures', 'Process', 'Prostate', 'Protons', 'Radiation', 'Radiation exposure', 'Radiation therapy', 'Site', 'Source', 'Staging', 'System', 'Techniques', 'anatomic imaging', 'attenuation', 'base', 'computerized tools', 'cost', 'density', 'electron density', 'image guided', 'imaging biomarker', 'imaging modality', 'improved', 'magnetic field', 'multimodality', 'novel', 'public health relevance', 'quality assurance', 'reconstruction', 'spectroscopic imaging', 'success', 'tool', 'treatment planning', 'treatment response']",NCI,STANFORD UNIVERSITY,R01,2017,359444,-0.012098916577924822
"MRI-Based Radiation Therapy Treatment Planning ﻿    DESCRIPTION (provided by applicant): CT is currently the gold standard in radiation therapy treatment planning. MRI provides a number of advantages over CT, including improved accuracy of target delineation, reduced radiation exposure, and simplified clinical workflow. There are two major technical hurdles that are impeding the clinical adoption of MRI-based radiation treatment planning: (1) geometric distortion, and (2) lack of electron density information. The goal of this project is to develop novel image analysis and computational tools to enable MRI-based radiation treatment planning. We hypothesize that accurate patient geometry and electron density information can be derived from MRI if the appropriate MR image acquisition, reconstruction, and analysis methods are applied. In Aim 1, we will improve the geometric accuracy of MRI by minimizing system-level and patient- specific distortions. To maintain sufficient system-level accuracy, we will perform comprehensive machine- specific calibrations and ongoing quality assurance procedures. To correct patient-induced distortions, we will develop novel computational tools to derive a detailed magnetic field distortion map based on physical principles, which is used to correct susceptibility-induced spatial distortions. In Aim 2, we will develop a unifying Bayesian method for quantitative electron density mapping, by combining the complementary intensity and geometry information. By utilizing multiple patient atlases and panoramic, multi-parametric MRI with differential contrast, we will apply machine learning techniques to encode the information given by intensity and geometry into two conditional probability density functions. These will be combined into one unifying posterior probability density function, which provides the optimal electron density on a continuous scale. In Aim 3, we will clinically evaluate the geometric and dosimetric accuracy of MRI for treatment planning in terms of 3 primary end points: (1) organ contours, (2) patient setup based on reference images, and (3) 3D dose distributions (both photon and proton), using CT as the ground truth. These evaluations will be conducted through patient studies at multiple disease sites, including brain, head and neck, and prostate. Success of the project will afford distortion-free MRI with reliable, quantitative electron density information. This will pave the way for MRI-based radiation treatment planning, leading to an improved accuracy in the overall radiation therapy process. It will streamline the treatment workflow for the MRI-guided radiation delivery systems under active development. With minimal modification, the proposed techniques can be applied to MR-based PET attenuation correction in PET/MR imaging. More broadly, the unifying Bayesian formalism can be used to improve current imaging biomarkers by integrating a wide variety of disparate information including anatomical and functional imaging such as perfusion/diffusion-weighted imaging and MR spectroscopic imaging. It will facilitate the incorporation of multimodality MRI into the entire process of cancer management: diagnosis, staging, radiation treatment planning, and treatment response assessment.         PUBLIC HEALTH RELEVANCE:  The goal of this project is to develop novel image analysis and computational tools to enable MRI-based radiation therapy treatment planning. Success of the project will overcome the major technical hurdles in the use of MRI in radiation therapy and will afford distortion-free MRI with reliable, quantitative electron density information. It will pve the way for MRI-based radiation treatment planning, leading to an improved accuracy in the overall radiation therapy process.            ",MRI-Based Radiation Therapy Treatment Planning,9026075,R01CA193730,"['Adopted', 'Adoption', 'Atlases', 'Bayesian Method', 'Bayesian Modeling', 'Brain', 'Calibration', 'Clinical', 'Data', 'Development', 'Diagnosis', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Dose', 'Evaluation', 'Functional Imaging', 'Geometry', 'Goals', 'Gold', 'Head and neck structure', 'Image', 'Image Analysis', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Methods', 'Modality', 'Modeling', 'Modification', 'Organ', 'Patients', 'Perfusion', 'Photons', 'Positron-Emission Tomography', 'Predisposition', 'Probability', 'Procedures', 'Process', 'Prostate', 'Protons', 'Radiation', 'Radiation therapy', 'Site', 'Source', 'Staging', 'System', 'Techniques', 'attenuation', 'base', 'computerized tools', 'cost', 'density', 'electron density', 'image guided', 'imaging biomarker', 'imaging modality', 'improved', 'magnetic field', 'multimodality', 'novel', 'public health relevance', 'quality assurance', 'reconstruction', 'spectroscopic imaging', 'success', 'tool', 'treatment planning', 'treatment response']",NCI,STANFORD UNIVERSITY,R01,2016,362066,-0.012098916577924822
"Rad-path-omic tools for rectal cancer treatment evaluation PROJECT SUMMARY: Of the estimated 43,030 patients who will be newly diagnosed with rectal cancer in 2018, a majority will receive neoadjuvant chemoradiation (NAC) to reduce tumor burden. All patients ultimately undergo an aggressive excision of the rectum, of which 25% exhibit complete pathologic response (pCR, i.e. disease- free after NAC) on the post-surgical specimen. These patients have therefore been subjected to an unnecessary, morbid procedure resulting in quality of life issues, in the absence of any definitive, non-invasive biomarkers for NAC response in vivo. While multi-parametric MRI is utilized to pre-operatively assess tumor response and regression to NAC, expert interpretation is confounded and variable due to overlapping appearance of benign treatment effects (e.g. fibrosis, ulceration) and residual tumor.  Recently, more quantitative characterization of lesions has been enabled via radiomics, involving high- throughput, computerized extraction of textural or kinetic attributes from imaging. Radiomic maps of the tumor environment can depict presence of different tissue types based on their structural and functional characteristics, visualized as regions of “low” and ‘high” feature expression. In fact, the post-NAC tumor environment on the excised rectal tissue specimen has been shown to reflect a variety and organization in different pathologic tissue types, also linked to patient prognosis and outcome. However, existing radiomic approaches only attempt to characterize the overall heterogeneity in a tissue region, as they lack “ground truth” to quantify tissue types and their organization on post-NAC MRI. A more comprehensive and accurate predictor for pCR based off multi- parametric MRI could thus be constructed by (a) quantifying the density and arrangement of structural and functional attributes on post-NAC rectal MRIs, and (b) optimizing radiomic descriptors against pathologically validated information of post-NAC tissue types on MRI, via spatial correlation with pathology.  In this proposal, I will develop novel radiomic tools in conjunction with spatially co-localized “ground truth” pathology to build a predictor for identifying rectal cancer patients exhibiting pCR via post-NAC MRI. Aim 1 will involve developing and evaluating a novel radiomic descriptor to quantify spatial organization of morphologic (via structural MRI) and physiologic (via contrast enhancement functional MRI) heterogeneity of the post-NAC lesion environment. Aim 2 will focus on optimizing this radiomic organization descriptor to capture distinctive tissue organization associated with pCR, via spatial mapping of post-surgical pathology information onto pre-operative MRI. My novel descriptor will be evaluated and validated via a machine learning predictor to identify patients exhibiting pCR using a discovery and a hold-out validation cohort, acquired from 2 different institutions; and compared with clinical markers of response. My project will build on promising preliminary results for my radiomic organization descriptor as well as a radiology-pathology co-registration framework, to result in a clinically reliable and impactful radiomics-based tool which could enable personalized management of rectal cancer patients. PROJECT NARRATIVE: This F31 proposal focuses on novel computational imaging tools for accurate identification and assessment of pathologic complete response to neoadjuvant chemoradiation via routine multi- parametric MRI. A comprehensive, quantitative assessment of post-chemoradiation tissue types and their tissue organization on imaging could enable (a) non-invasive identification of complete responders to avoid unneeded and highly morbid surgeries, and (b) more personalized management and follow-up for non-responders to NAC.",Rad-path-omic tools for rectal cancer treatment evaluation,9916627,F31CA216935,"['Appearance', 'Benign', 'Biological Markers', 'Cancer Patient', 'Characteristics', 'Clinical', 'Clinical Markers', 'Clinical Protocols', 'Colorectal Surgery', 'Colostomy Procedure', 'Complement', 'Computer Analysis', 'Confounding Factors (Epidemiology)', 'Data', 'Descriptor', 'Diagnosis', 'Diagnostic Neoplasm Staging', 'Diagnostic radiologic examination', 'Disease', 'Doctor of Philosophy', 'Environment', 'Evaluation', 'Excision', 'Exhibits', 'Extravasation', 'Fibrosis', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Heterogeneity', 'Image', 'Imaging Device', 'In complete remission', 'Institution', 'Intervention', 'Kinetics', 'Lesion', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Mentors', 'Methodology', 'Morphology', 'Neoadjuvant Therapy', 'Newly Diagnosed', 'Oncology', 'Operative Surgical Procedures', 'Outcome', 'Paper', 'Pathologic', 'Pathology', 'Patients', 'Peer Review', 'Perfusion', 'Physiological', 'Procedures', 'Publishing', 'Quality of life', 'Radiology Specialty', 'Rectal Cancer', 'Rectum', 'Residual Tumors', 'Shapes', 'Site', 'Specimen', 'Sphincter', 'Structure', 'Surgical Pathology', 'Texture', 'Tissues', 'Tumor Burden', 'Tumor Markers', 'Ulcer', 'Validation', 'base', 'cancer therapy', 'chemoradiation', 'cohort', 'computerized', 'contrast enhanced', 'deep learning', 'density', 'design', 'digital pathology', 'disease diagnosis', 'follow-up', 'imaging biomarker', 'in vivo', 'learning classifier', 'multidisciplinary', 'novel', 'outcome forecast', 'patient response', 'personalized management', 'quantitative imaging', 'radiologist', 'radiomics', 'rectal', 'response', 'response biomarker', 'success', 'symposium', 'tool', 'treatment effect', 'treatment response', 'tumor', 'uptake']",NCI,CASE WESTERN RESERVE UNIVERSITY,F31,2020,26372,-0.08599022585473896
"Rad-path-omic tools for rectal cancer treatment evaluation PROJECT SUMMARY: Of the estimated 43,030 patients who will be newly diagnosed with rectal cancer in 2018, a majority will receive neoadjuvant chemoradiation (NAC) to reduce tumor burden. All patients ultimately undergo an aggressive excision of the rectum, of which 25% exhibit complete pathologic response (pCR, i.e. disease- free after NAC) on the post-surgical specimen. These patients have therefore been subjected to an unnecessary, morbid procedure resulting in quality of life issues, in the absence of any definitive, non-invasive biomarkers for NAC response in vivo. While multi-parametric MRI is utilized to pre-operatively assess tumor response and regression to NAC, expert interpretation is confounded and variable due to overlapping appearance of benign treatment effects (e.g. fibrosis, ulceration) and residual tumor.  Recently, more quantitative characterization of lesions has been enabled via radiomics, involving high- throughput, computerized extraction of textural or kinetic attributes from imaging. Radiomic maps of the tumor environment can depict presence of different tissue types based on their structural and functional characteristics, visualized as regions of “low” and ‘high” feature expression. In fact, the post-NAC tumor environment on the excised rectal tissue specimen has been shown to reflect a variety and organization in different pathologic tissue types, also linked to patient prognosis and outcome. However, existing radiomic approaches only attempt to characterize the overall heterogeneity in a tissue region, as they lack “ground truth” to quantify tissue types and their organization on post-NAC MRI. A more comprehensive and accurate predictor for pCR based off multi- parametric MRI could thus be constructed by (a) quantifying the density and arrangement of structural and functional attributes on post-NAC rectal MRIs, and (b) optimizing radiomic descriptors against pathologically validated information of post-NAC tissue types on MRI, via spatial correlation with pathology.  In this proposal, I will develop novel radiomic tools in conjunction with spatially co-localized “ground truth” pathology to build a predictor for identifying rectal cancer patients exhibiting pCR via post-NAC MRI. Aim 1 will involve developing and evaluating a novel radiomic descriptor to quantify spatial organization of morphologic (via structural MRI) and physiologic (via contrast enhancement functional MRI) heterogeneity of the post-NAC lesion environment. Aim 2 will focus on optimizing this radiomic organization descriptor to capture distinctive tissue organization associated with pCR, via spatial mapping of post-surgical pathology information onto pre-operative MRI. My novel descriptor will be evaluated and validated via a machine learning predictor to identify patients exhibiting pCR using a discovery and a hold-out validation cohort, acquired from 2 different institutions; and compared with clinical markers of response. My project will build on promising preliminary results for my radiomic organization descriptor as well as a radiology-pathology co-registration framework, to result in a clinically reliable and impactful radiomics-based tool which could enable personalized management of rectal cancer patients. PROJECT NARRATIVE: This F31 proposal focuses on novel computational imaging tools for accurate identification and assessment of pathologic complete response to neoadjuvant chemoradiation via routine multi- parametric MRI. A comprehensive, quantitative assessment of post-chemoradiation tissue types and their tissue organization on imaging could enable (a) non-invasive identification of complete responders to avoid unneeded and highly morbid surgeries, and (b) more personalized management and follow-up for non-responders to NAC.",Rad-path-omic tools for rectal cancer treatment evaluation,9683190,F31CA216935,"['Appearance', 'Benign', 'Biological Markers', 'Cancer Patient', 'Characteristics', 'Clinical', 'Clinical Markers', 'Clinical Protocols', 'Colorectal Surgery', 'Colostomy Procedure', 'Complement', 'Computer Analysis', 'Confounding Factors (Epidemiology)', 'Data', 'Descriptor', 'Diagnosis', 'Diagnostic Neoplasm Staging', 'Diagnostic radiologic examination', 'Disease', 'Doctor of Philosophy', 'Environment', 'Evaluation', 'Excision', 'Exhibits', 'Extravasation', 'Fibrosis', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Heterogeneity', 'Image', 'Imaging Device', 'In complete remission', 'Institution', 'Intervention', 'Kinetics', 'Lesion', 'Link', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Mentors', 'Methodology', 'Morphology', 'Neoadjuvant Therapy', 'Newly Diagnosed', 'Operative Surgical Procedures', 'Outcome', 'Paper', 'Pathologic', 'Pathology', 'Patients', 'Peer Review', 'Perfusion', 'Physiological', 'Procedures', 'Publishing', 'Quality of life', 'Radiology Specialty', 'Rectal Cancer', 'Rectum', 'Residual Tumors', 'Shapes', 'Site', 'Specimen', 'Sphincter', 'Structure', 'Surgical Pathology', 'Texture', 'Tissues', 'Tumor Burden', 'Tumor Markers', 'Ulcer', 'Validation', 'base', 'cancer therapy', 'chemoradiation', 'cohort', 'computerized', 'contrast enhanced', 'deep learning', 'density', 'design', 'digital pathology', 'disease diagnosis', 'follow-up', 'imaging biomarker', 'in vivo', 'multidisciplinary', 'novel', 'oncology', 'outcome forecast', 'patient response', 'personalized management', 'quantitative imaging', 'radiologist', 'radiomics', 'rectal', 'response', 'response biomarker', 'success', 'symposium', 'tool', 'treatment effect', 'treatment response', 'tumor', 'uptake']",NCI,CASE WESTERN RESERVE UNIVERSITY,F31,2019,45016,-0.08599022585473896
"Development and Dissemination of Clinical CEST MRI Acquisition and Analysis Methods for Cancer Imaging Applications This research plan will continue our development of Chemical Exchange Saturation Transfer (CEST) MRI acquisition and analysis methods for imaging patients with cancer. AcidoCEST MRI with an exogenous contrast agent is an innovative variation of this technique that measures extracellular pH in solid tumors. We have also developed an exceptionally innovative method that acquires endogenous CEST MR images with multiple powers, which can measure the chemical exchange rate of endogenous proteins that can assess relative differences in pH. We will improve our CEST MRI acquisition methods by accelerating the imaging speed, reducing and eliminating complications due to patient motion, eliminating complications caused by fat signal, and expanding to 3D imaging methods. We will also improve image analysis methods that are required for CEST contrast from endogenous proteins and exogenous contrast agents.  Our research has strong impact because acidoCEST MRI can track changes in tumor acidosis in response to chemotherapy and chemoradiation therapy in patients who have breast cancer and head & neck cancer. Endogenous CEST MRI can improve the diagnoses of brain tumor recurrence vs. pseudoprogression, and evaluations of lung cancer vs. lung infection. We will perform clinical studies with our endogenous and exogenous CEST MRI methods to image patients with brain, breast, lung, and head & neck cancers.  To amplify the impact of our research, we will develop versions of our CEST MRI acquisition methods for the many versions of the 13 Siemens hardware platforms and software operating systems at the MD Anderson Cancer Center. We will also develop user-friendly CEST analysis methods for many researchers at MD Anderson. We will leverage our unique research environment and expertise with intra-institutional dissemination to provide inter-institutional dissemination, by sharing share these acquisition and analysis tools with other CEST MRI researchers. We will collaborate with NIST to provide the first CEST MRI phantom that can standardize the development and implementation of intra- and inter-institutional CEST MRI methods.  Our team of outstanding investigators includes experts in CEST saturation methods, CEST MR image analysis, clinical contrast agents, rapid MRI acquisition methods, high field 7T MRI, clinical radiology, biostatistics, histopathology and oncology. Based on our expertise and years of productivity in clinical CEST MRI, we have developed an exceptional research approach. As the world's largest cancer center and a health destination, the MD Anderson Cancer Center has the patient population that provides very strong institutional support for our clinical studies. NARRATIVE We will develop new acquisition and analysis methods that assess tumor acidosis with CEST MRI, using exogenous CEST agents and endogenous sources of CEST contrast. We will apply our new methods to evaluate tumor acidosis in patients with breast, head & neck, lung, and brain cancers. We will disseminate our CEST MRI acquisition and analysis methods to other CEST MRI researchers, along with a NIST-approved standard for clinical CEST MRI studies.",Development and Dissemination of Clinical CEST MRI Acquisition and Analysis Methods for Cancer Imaging Applications,10017170,R01CA231513,"['3-Dimensional', 'Acidosis', 'Address', 'Biometry', 'Brain', 'Brain Neoplasms', 'Breast', 'Breast Cancer Patient', 'Cancer Center', 'Chemicals', 'Clinical', 'Clinical Research', 'Clinical/Radiologic', 'Communities', 'Computer software', 'Contrast Media', 'Custom', 'Destinations', 'Development', 'Diagnosis', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Fatty acid glycerol esters', 'Fingerprint', 'Glioblastoma', 'Goals', 'Head Cancer', 'Head and Neck Cancer', 'Health', 'Histopathology', 'Image', 'Image Analysis', 'Implant', 'Kidney', 'Lead', 'Lesion', 'Lung', 'Lung infections', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Maps', 'Measures', 'Methods', 'Motion', 'Neck Cancer', 'Non-Malignant', 'Oncology', 'Operating System', 'Pathway interactions', 'Patient imaging', 'Patients', 'Pattern', 'Productivity', 'Proteins', 'Protocols documentation', 'Radiation therapy', 'Randomized', 'Recurrence', 'Research', 'Research Personnel', 'Signal Transduction', 'Solid Neoplasm', 'Source', 'Speed', 'Standardization', 'Techniques', 'Testing', 'Three-Dimensional Imaging', 'Variant', 'base', 'cancer imaging', 'chemical standard', 'chemoradiation', 'chemotherapy', 'clinical imaging', 'clinical translation', 'design', 'extracellular', 'frontier', 'head and neck cancer patient', 'imaging modality', 'improved', 'in vivo', 'innovation', 'inter-institutional', 'interest', 'lung imaging', 'machine learning method', 'malignant breast neoplasm', 'ovarian neoplasm', 'patient population', 'preclinical study', 'prospective', 'respiratory', 'response', 'tool', 'treatment response', 'tumor', 'user-friendly']",NCI,UNIVERSITY OF TX MD ANDERSON CAN CTR,R01,2020,657412,-0.05641352272324683
"Development and Dissemination of Clinical CEST MRI Acquisition and Analysis Methods for Cancer Imaging Applications This research plan will continue our development of Chemical Exchange Saturation Transfer (CEST) MRI acquisition and analysis methods for imaging patients with cancer. AcidoCEST MRI with an exogenous contrast agent is an innovative variation of this technique that measures extracellular pH in solid tumors. We have also developed an exceptionally innovative method that acquires endogenous CEST MR images with multiple powers, which can measure the chemical exchange rate of endogenous proteins that can assess relative differences in pH. We will improve our CEST MRI acquisition methods by accelerating the imaging speed, reducing and eliminating complications due to patient motion, eliminating complications caused by fat signal, and expanding to 3D imaging methods. We will also improve image analysis methods that are required for CEST contrast from endogenous proteins and exogenous contrast agents.  Our research has strong impact because acidoCEST MRI can track changes in tumor acidosis in response to chemotherapy and chemoradiation therapy in patients who have breast cancer and head & neck cancer. Endogenous CEST MRI can improve the diagnoses of brain tumor recurrence vs. pseudoprogression, and evaluations of lung cancer vs. lung infection. We will perform clinical studies with our endogenous and exogenous CEST MRI methods to image patients with brain, breast, lung, and head & neck cancers.  To amplify the impact of our research, we will develop versions of our CEST MRI acquisition methods for the many versions of the 13 Siemens hardware platforms and software operating systems at the MD Anderson Cancer Center. We will also develop user-friendly CEST analysis methods for many researchers at MD Anderson. We will leverage our unique research environment and expertise with intra-institutional dissemination to provide inter-institutional dissemination, by sharing share these acquisition and analysis tools with other CEST MRI researchers. We will collaborate with NIST to provide the first CEST MRI phantom that can standardize the development and implementation of intra- and inter-institutional CEST MRI methods.  Our team of outstanding investigators includes experts in CEST saturation methods, CEST MR image analysis, clinical contrast agents, rapid MRI acquisition methods, high field 7T MRI, clinical radiology, biostatistics, histopathology and oncology. Based on our expertise and years of productivity in clinical CEST MRI, we have developed an exceptional research approach. As the world's largest cancer center and a health destination, the MD Anderson Cancer Center has the patient population that provides very strong institutional support for our clinical studies. NARRATIVE We will develop new acquisition and analysis methods that assess tumor acidosis with CEST MRI, using exogenous CEST agents and endogenous sources of CEST contrast. We will apply our new methods to evaluate tumor acidosis in patients with breast, head & neck, lung, and brain cancers. We will disseminate our CEST MRI acquisition and analysis methods to other CEST MRI researchers, along with a NIST-approved standard for clinical CEST MRI studies.",Development and Dissemination of Clinical CEST MRI Acquisition and Analysis Methods for Cancer Imaging Applications,9671020,R01CA231513,"['3-Dimensional', 'Acidosis', 'Address', 'Biometry', 'Brain', 'Brain Neoplasms', 'Breast', 'Breast Cancer Patient', 'Cancer Center', 'Chemicals', 'Clinical', 'Clinical Research', 'Clinical/Radiologic', 'Communities', 'Computer software', 'Contrast Media', 'Custom', 'Destinations', 'Development', 'Diagnosis', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Fatty acid glycerol esters', 'Fingerprint', 'Glioblastoma', 'Goals', 'Head Cancer', 'Head and Neck Cancer', 'Health', 'Histopathology', 'Image', 'Image Analysis', 'Implant', 'Kidney', 'Lead', 'Lesion', 'Lung', 'Lung infections', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Maps', 'Measures', 'Methods', 'Motion', 'Neck Cancer', 'Non-Malignant', 'Operating System', 'Pathway interactions', 'Patient imaging', 'Patients', 'Pattern', 'Productivity', 'Proteins', 'Protocols documentation', 'Radiation therapy', 'Randomized', 'Recurrence', 'Research', 'Research Personnel', 'Signal Transduction', 'Solid Neoplasm', 'Source', 'Speed', 'Standardization', 'Techniques', 'Testing', 'Three-Dimensional Imaging', 'Variant', 'base', 'cancer imaging', 'chemical standard', 'chemoradiation', 'chemotherapy', 'clinical imaging', 'clinical translation', 'design', 'extracellular', 'frontier', 'head and neck cancer patient', 'imaging modality', 'improved', 'in vivo', 'innovation', 'inter-institutional', 'interest', 'learning strategy', 'lung imaging', 'malignant breast neoplasm', 'oncology', 'ovarian neoplasm', 'patient population', 'preclinical study', 'prospective', 'respiratory', 'response', 'tool', 'treatment response', 'tumor', 'user-friendly']",NCI,UNIVERSITY OF TX MD ANDERSON CAN CTR,R01,2019,652625,-0.05641352272324683
"New instrument and methods for fast, diagnostic-quality histology of un-embedded bone marrow and lymph node specimens Project Summary Over 600,000 bone marrow biopsies are performed every year in the United States, while hundreds of thousands more lymph node biospies are performed. Histological evaluation of these biopsies is a critical component of care for hematologic diseases including leukemia, lymphoma, myelodysplastic syndrome, myeloproliferative disease and non-neoplastic conditions such as viral infections and autoimmune conditions.  We have developed a platform that we consider a paradigm shift in the histologic examination of tissues. It is based on a new chemical process, imaging, and image processing approach that we have dubbed Clearing Histology with MultiPhoton microscopy (CHiMP).The CHiMP technology enables visual analysis of entire intact, un-embedded and uncut specimens within a short time-frame and with a resolution that is amenable to primary diagnosis. The significant clinical benefits include: 1) potential for same-day diagnosis, 2) labor and cost savings, 3) access to 3D perspective, 4) increased visual data from same specimen, 5) complete tissue preservation for ancillary studies such as DNA analysis and 6) inherent benefits of digital data such as reduced risk of loss, ready remote review by experts, and amenability to machine learning tools. Hematopoietic tissue evaluation would similarly benefit from these advantages, but unfortunately the systems developed thus far lack the resolution typically needed for visual examination of hematopoietic tissues.  For this Phase I SBIR proposal, an objective is to develop customized optics to improve the resolution of our current microscopes and thereby enable use in the specialized field of hematology. Commercially available objective lenses that are compatible with our immersion medium are either limited to numerical apertures (NA) that are less than one, affecting resolution and image quality, or have insufficient working distances for imaging past the coverslip and surface roughness to obtain complete sections. We will design and test a custom objective lens with high NA and long working distance, suited for our proprietary reagents. Integrating such a lens into our microscope will also require the design of a custom scan lens, custom beam conditioning optics, and a custom polygon scanner.  An associated goal is to develop a novel approach to preparing bone marrow aspiration specimens that will make them amenable to imaging with CHiMP, potentially reducing the need for core biopsies by permitting unambiguous morphologic categorization of cell subtypes in their architectural context, without the routine need for immunohistochemistry, and while preserving nucleic acids for molecular/genetic evaluation. Project Narrative Applikate Technologies has developed a powerful platform for histological evaluation of tissue called Clearing Histology with MultiPhoton Microscopy, or CHiMP. This platform has many advantages over traditional approaches, including same-day turn-around, reduced labor costs, preservation of tissue for DNA analysis, and direct-to-digital imaging for ease of consultation with remote experts. This proposal seeks to develop custom optics to enable very-high-resolution imaging of bone marrow and lymph node samples that are critical for diagnosing diseases such as leukemia and lymphoma.","New instrument and methods for fast, diagnostic-quality histology of un-embedded bone marrow and lymph node specimens",9677952,R43CA235890,"['3-Dimensional', 'Address', 'Adoption', 'Affect', 'Ancillary Study', 'Antigens', 'Architecture', 'Aspirate substance', 'Autoimmune Diseases', 'Autoimmune Process', 'Biopsy', 'Blinded', 'Bone Marrow', 'Bone Marrow Aspiration', 'Bone marrow biopsy', 'Caring', 'Cells', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Data', 'Communicable Diseases', 'Consultations', 'Consumption', 'Core Biopsy', 'Cost Savings', 'Custom', 'DNA', 'DNA analysis', 'Data', 'Decalcification', 'Diagnosis', 'Diagnostic', 'Disease', 'Dysmyelopoietic Syndromes', 'Evaluation', 'Fibrosis', 'Goals', 'Hematological Disease', 'Hematology', 'Hematopathology', 'Histologic', 'Histology', 'Image', 'Imagery', 'Immersion Investigative Technique', 'Immunohistochemistry', 'Iron', 'Lateral', 'Lymph', 'Machine Learning', 'Marrow', 'Measurement', 'Measures', 'Methods', 'Microscope', 'Microscopy', 'Molecular Genetics', 'Morphologic artifacts', 'Morphology', 'Myeloproliferative disease', 'Nucleic Acids', 'Optics', 'Pan Genus', 'Pathologist', 'Pathology', 'Phase', 'Preparation', 'Process', 'Protocols documentation', 'RNA', 'Reagent', 'Recovery', 'Resolution', 'Risk', 'Sampling', 'Scanning', 'Slice', 'Slide', 'Small Business Innovation Research Grant', 'Specimen', 'Speed', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissue Preservation', 'Tissue Sample', 'Tissue Stains', 'Tissues', 'Training', 'United States', 'Virus Diseases', 'Visual', 'Waxes', 'Work', 'base', 'bone imaging', 'conditioning', 'cost', 'design', 'digital', 'digital imaging', 'disease diagnosis', 'hematopoietic tissue', 'high resolution imaging', 'image processing', 'improved', 'instrument', 'lens', 'leukemia/lymphoma', 'lymph nodes', 'medical specialties', 'multiphoton microscopy', 'nanoparticle', 'novel', 'novel strategies', 'particle', 'pre-clinical', 'preservation', 'second harmonic', 'tool', 'whole slide imaging']",NCI,"APPLIKATE TECHNOLOGIES, LLC",R43,2019,224713,-0.0799490471877511
"Mitigating High Grade Radiation-Induced Lymphopenia through Pretreatment Autologous Lymphocyte Infusion PROJECT SUMMARY Radiation induced lymphopenia (RIL) is a common radiation-related toxicity that has been recognized for over a century but often ignored as clinically inconsequential. However, accumulating evidence has demonstrated strong association of high grade RIL (seen in 30-50% of patients) with poor prognosis. The pervasive role of radiotherapy in the curative management of solid tumors supports the need to develop mitigating strategies, particularly for patients with a high risk of developing grade 4 (G4) RIL. We have compelling evidence from both clinical and preclinical work that severe RIL impacts cancer control and therapy effectiveness, and methods to reduce RIL may improve treatment outcomes. To further develop these approaches for clinical translation, we have proposed 2 specific aims. In aim 1, we will build on our initial prediction model for G4 RIL and leverage our large database of esophageal cancer patients who have completed chemoradiation (CRT) to develop a better predictive model for G4 RIL so that we can rapidly and efficiently identify the highest risk patients for mitigating strategies. In aim 2, we will determine the feasibility and safety of raising the baseline lymphocyte levels by autologous lymphocyte infusion (ALI) prior to initiating CRT. Fundamentally, this research will allow us to develop the necessary computational tool capable of properly identifying patients at risk for developing severe RIL, and complete a small feasibility and safety study of using ALI as a way to raise the baseline pre-treatment lymphocyte levels so that the probability of developing G4 RIL could be possibly curtailed. By targeting the at-risk patients to receive RIL mitigating strategies, we will hopefully be able to improve the cancer outcomes of standard cancer therapies, and build on current innovative strategies of immunotherapy and radiation combinations. PROJECT NARRATIVE Radiation therapy, a key pillar in the management of cancers, causes a common yet ignored side effect of radiation induced lymphopenia (RIL). Severe RIL has been linked to poor outcomes of patients, presumably due to reduced immune surveillance thereby increasing disease recurrence after radiation therapy. We propose developing a prediction model to help identify who are most at risk for developing severe RIL, and conduct a pilot study to help reduce the impact of RIL so that clinical outcomes for these at-risk patients will improve in the future.",Mitigating High Grade Radiation-Induced Lymphopenia through Pretreatment Autologous Lymphocyte Infusion,9807793,R21CA240881,"['Applications Grants', 'Bayesian Prediction', 'Blood Component Removal', 'CD8B1 gene', 'Cancer Control', 'Cancer Patient', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Combination immunotherapy', 'Complication', 'Data', 'Databases', 'Diagnosis', 'Disease', 'Distant', 'Effectiveness', 'Enrollment', 'Feasibility Studies', 'Future', 'Grant', 'Immunologic Surveillance', 'Immunotherapy', 'Incidence', 'Inferior', 'Infusion procedures', 'Link', 'Low Dose Radiation', 'Lymphocyte', 'Lymphocyte Count', 'Lymphopenia', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Methods', 'Modeling', 'Nomograms', 'Normal tissue morphology', 'Outcome', 'Patient-Focused Outcomes', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Pilot Projects', 'Play', 'Probability', 'Proliferating', 'Radiation', 'Radiation Dose Unit', 'Radiation therapy', 'Recording of previous events', 'Recurrence', 'Risk', 'Risk Factors', 'Role', 'Safety', 'Solid Neoplasm', 'Statistical Models', 'Stem cell transplant', 'Time', 'Toxic effect', 'Training', 'Translating', 'Treatment outcome', 'Tumor Antigens', 'Tumor Escape', 'Tumor-Infiltrating Lymphocytes', 'Validation', 'Work', 'autologous lymphocytes', 'base', 'cancer radiation therapy', 'cancer therapy', 'chemoradiation', 'clinical translation', 'cohort', 'computerized tools', 'disorder control', 'fundamental research', 'high risk', 'high risk population', 'improved', 'innovation', 'outcome forecast', 'personalized approach', 'pre-clinical', 'predictive modeling', 'prognostic value', 'radiation effect', 'response', 'safety and feasibility', 'safety study', 'side effect', 'tool', 'tumor']",NCI,UNIVERSITY OF TX MD ANDERSON CAN CTR,R21,2019,211410,-0.026235839474825096
"Fluorescence tomography plugin unit for spatial monitoring of T cell migration Immunotherapy (IMT) is a cancer treatment that harnesses activated T cells to induce a targeted immune response against the cancer. Preclinical IMT research has been limited because there are no effective methods to longitudinally image T cell biodistribution in mouse models, including ovarian cancer. This is an urgent unmet need because the only viable methods to monitor activated T cells, the immune marker most associated with antitumor response and prognosis, immunohistochemistry and FACs, are both terminal and ex vivo. Fluorescence imaging (FLi) offers many advantages for monitoring T cell migration, including the relatively long photostability of fluorescent ligands, ease of use, and its low cost. However, FLi does not provide 3D spatial maps of fluorescent reporters due to diffuse light propagation in animal tissue. In addition, fluorescence intensities on the tissue surface also depend on the animal’s size, pose, and shape and, hence, limit quantification and reproducibility. Last, there is no anatomical reference that also provides a template for automated organ delineation along with T cell biodistribution analysis. Therefore, InVivo Analytics seeks funding to develop InVivoFLUOR, an automated data analysis tool for 3D fluorescence tomography (FLt) of mouse models. InVivoFLUOR is comprised of: a Body Conforming Animal Mold (BCAM) for multi-source transillumination FLt and spatial registration of the animal’s geometry and pose; an Organ Probability Map (OPM) for providing an organ template; and a cloud-based FLt algorithm. We will demonstrate its feasibility on an IMT example for determining the spatial biodistribution of fluorescence-labeled T cells and, in combination with bioluminescence imaging (BLi), will compare localization of ovarian cancer cells. In Aim 1 we will confirm the ability to quantitatively determine fluorescent targets inside a small animal. The spatial distribution of known fluorescent targets will be reconstructed and compared to ex vivo data. In Aim 2 we will confirm the ability to determine the in vivo T cell biodistribution at tumor sites. The fluorescence-labeled T cell distribution will be calculated and coregistered to the anatomy based on the OPM and to disseminated ovarian tumors. The ability to instantaneously quantify the T cell distribution in the same animal longitudinally, as opposed to sacrificing a different animal at every time point for T cell counting via FACS or histology, neither of which can identify sites where the activated T cells may be “hiding”, has an impact on the development of and outcome of new IMTs with high accuracy. InVivoFLUOR will enable cross-platform data comparison and analysis, eliminate operator-dependent variability, increase data reproducibility, and will facilitate the translation of new therapeutics. The successful completion of the proposed project will help to commercialize InVivoFLUOR and will find immediate application in the pharmaceutical industry for rapid development of novel IMTs. InVivo Analytics seeks funding for demonstrating the feasibility for mapping the 3D spatial biodistribution of fluorescent T cells in a small animal model of ovarian cancer. Analysis of T cell migration currently lacks the ability to obtain 3D spatial maps of T cells in the same animal in longitudinal studies. Therefore, InVivo Analytics will develop a 3D fluorescence tomography and data analysis tool, which can quantify T cell migration in the living animal, thereby allowing accurate monitoring of novel immunotherapies.",Fluorescence tomography plugin unit for spatial monitoring of T cell migration,9846612,R43CA243827,"['3-Dimensional', 'Algorithms', 'Anatomy', 'Animal Model', 'Animals', 'Antitumor Response', 'Atlases', 'Attenuated', 'Biodistribution', 'Bioluminescence', 'Biotechnology', 'Cell Count', 'Cells', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Development', 'Diffuse', 'Drug Industry', 'Fluorescence', 'Funding', 'Geometry', 'Goals', 'Growth', 'Histology', 'Human Resources', 'Image', 'Image Analysis', 'Immune Targeting', 'Immune response', 'Immunohistochemistry', 'Immunologic Markers', 'Immunotherapy', 'Institution', 'Knowledge', 'Label', 'Ligands', 'Light', 'Location', 'Longitudinal Studies', 'Malignant Neoplasms', 'Malignant neoplasm of ovary', 'Maps', 'Methods', 'Molds', 'Monitor', 'Mus', 'Optics', 'Organ', 'Outcome', 'Positron-Emission Tomography', 'Preclinical Testing', 'Probability', 'Property', 'Radioisotopes', 'Radiolabeled', 'Reporter', 'Reproducibility', 'Research', 'Research Personnel', 'Services', 'Shapes', 'Site', 'Source', 'Spatial Distribution', 'Surface', 'T-Lymphocyte', 'Three-Dimensional Imaging', 'Time', 'Tissues', 'Training', 'Transillumination', 'Translations', 'Treatment outcome', 'animal tissue', 'base', 'bioluminescence imaging', 'cancer cell', 'cancer therapy', 'cell motility', 'cloud based', 'commercial application', 'cost', 'digital', 'fluorescence imaging', 'fluorophore', 'imaging system', 'improved', 'in vivo', 'machine learning algorithm', 'mouse model', 'novel', 'novel therapeutics', 'optical imaging', 'outcome forecast', 'ovarian neoplasm', 'pre-clinical', 'serial imaging', 'tomography', 'tool', 'tumor']",NCI,"IN VIVO ANALYTICS, INC.",R43,2019,222435,-0.033168627030579434
"Functional Cancer Cell Maps FUNCTIONAL CANCER CELL MAPS SUMMARY I am currently an Academic Program Officer in Prof. Trey Ideker’s lab at UC San Diego. My title reflects the varied roles I play in the Ideker Lab, both scientifically and administratively, as I also serve as the Assistant Director of the Cancer Cell Map Initiative (CCMI) and the San Diego Center for Systems Biology (SDCSB). I am involved in a wide range of research projects, both within the Ideker Lab and across the various Centers. Central though to many of these efforts is the role I play supervising a number of projects using the CRISPR/Cas9­based approach to map genetic interactions in cancer cells. These functional maps can be used to identify protein complexes and pathways in cancer cells and to reveal genetic dependencies that might be therapeutically tractable. These studies will also provide us with the necessary training data to build “visible” AIs, machine learning models that not only make accurate predictions but also provide mechanistic insights. FUNCTIONAL CANCER CELL MAPS NARRATIVE Many cancers in adults are caused by mutations acquired over time. Research in both the Ideker Lab and the Cancer Cell Map Initiative seek to understand how these mutations alter the function of proteins leading to cancer using a variety of biochemical, genetic and computational approaches. We are particularly interested in understanding how combinations of mutated genes can disrupt normal cell physiology as knowing about these mechanisms can help us identify new drug targets or biomarkers.",Functional Cancer Cell Maps,10016231,R50CA243885,"['Adult', 'Biochemical Genetics', 'Biological Markers', 'CRISPR/Cas technology', 'Cell physiology', 'Data', 'Dependence', 'Genes', 'Genetic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Modeling', 'Mutate', 'Mutation', 'Normal Cell', 'Pathway interactions', 'Play', 'Research', 'Research Project Grants', 'Role', 'Supervision', 'Systems Biology', 'Therapeutic', 'Time', 'Training', 'academic program', 'base', 'cancer cell', 'insight', 'interest', 'new therapeutic target', 'protein complex', 'protein function']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R50,2020,126700,-0.07104085193912536
"Functional Cancer Cell Maps FUNCTIONAL CANCER CELL MAPS SUMMARY I am currently an Academic Program Officer in Prof. Trey Ideker’s lab at UC San Diego. My title reflects the varied roles I play in the Ideker Lab, both scientifically and administratively, as I also serve as the Assistant Director of the Cancer Cell Map Initiative (CCMI) and the San Diego Center for Systems Biology (SDCSB). I am involved in a wide range of research projects, both within the Ideker Lab and across the various Centers. Central though to many of these efforts is the role I play supervising a number of projects using the CRISPR/Cas9­based approach to map genetic interactions in cancer cells. These functional maps can be used to identify protein complexes and pathways in cancer cells and to reveal genetic dependencies that might be therapeutically tractable. These studies will also provide us with the necessary training data to build “visible” AIs, machine learning models that not only make accurate predictions but also provide mechanistic insights. FUNCTIONAL CANCER CELL MAPS NARRATIVE Many cancers in adults are caused by mutations acquired over time. Research in both the Ideker Lab and the Cancer Cell Map Initiative seek to understand how these mutations alter the function of proteins leading to cancer using a variety of biochemical, genetic and computational approaches. We are particularly interested in understanding how combinations of mutated genes can disrupt normal cell physiology as knowing about these mechanisms can help us identify new drug targets or biomarkers.",Functional Cancer Cell Maps,9849989,R50CA243885,"['Adult', 'Biochemical Genetics', 'Biological Markers', 'CRISPR/Cas technology', 'Cell physiology', 'Data', 'Dependence', 'Genes', 'Genetic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Modeling', 'Mutate', 'Mutation', 'Normal Cell', 'Pathway interactions', 'Play', 'Research', 'Research Project Grants', 'Role', 'Supervision', 'Systems Biology', 'Therapeutic', 'Time', 'Training', 'academic program', 'base', 'cancer cell', 'insight', 'interest', 'new therapeutic target', 'protein complex', 'protein function']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R50,2019,126633,-0.07104085193912536
"Rapid motion-robust quantitative DCE-MRI for the assessment of gynecologic cancers PROJECT SUMMARY  Gynecologic cancers are some of the most lethal diseases affecting women. Globally, one woman dies of cervical cancer every two minutes. MRI is increasingly used in the evaluation of gynecologic and many other cancers. Beyond its established use for cancer staging, there has long been an interest in the use of MRI-derived quantitative metrics to gain insights into the tumor microenvironment. Parametric maps obtained from quantification of dynamic contrast enhanced (DCE) MRI data can be used to study tumor vascularity and identify tumors that are better perfused and oxygenated and thus more sensitive to some treatments such as chemotherapy and radiation. However, the relative slow imaging speed and motion sensitivity of current MRI technology results in non-reliable and non-reproducible quantification of DCE-MRI data, which restricts its application in clinical practice.  Our group is a world leader in development of rapid motion-resistant DCE-MRI techniques, in particular using combinations of radial imaging and compressed sensing. We developed the technique called GRASP, which was conceived as an academic-industrial partnership and has now been successfully translated into standard clinical practice. Though powerful, the first generation of GRASP has limitations. First, radial imaging is robust to motion, but not free of motion, which usually results in blurring. Second, GRASP uses a very simple sparsifying transform for compressed sensing, which can introduce issues with quantification. Third, GRASP was not originally developed for pharmacokinetic analysis and misses important ingredients such as integration of AIF estimation and T1 mapping. Fourth, image reconstruction time is still very long – in the order of several minutes.  We have developed new advances to circumvent these limitations and offer a new DCE-MRI technique with increased speed, motion-resistance and personalized AIF estimation and T1 mapping for pharmacokinetic analysis. Following the PAR-18-009 guidelines, our main goal is to form an academic-industrial partnership between Memorial Sloan Kettering Cancer Center and General Electric Healthcare to translate these new developments in quantitative DCE-MRI for use in patients with gynecologic and other type of cancers. Specific Aims are as follows: 1. Develop and implement a fast motion-resistant quantitative DCE-MRI technique that goes beyond GRASP  to offer increased speed and resistance to motion; dynamic T1 mapping; and personalized and automated  pharmacokinetic analysis 2. Evaluate the repeatability, reproducibility and preliminary tumor response assessment of the fast motion-  robust quantitative DCE-MRI technique (“DCE-new”) and compare DCE-new to standard of care DCE-MRI  (“DCE-standard”) in patients with gynecologic cancer 3. Develop and evaluate fast image reconstruction algorithms based on deep learning PROJECT NARRATIVE This project aims to establish an academic-industrial partnership between Memorial Sloan Kettering Cancer Center and General Electric Healthcare to develop and disseminate advances in dynamic contrast-enhanced (DCE) MRI for use in cancer patients. The new developments, which include radial imaging, compressed sensing, and deep learning, will deliver rapid motion-resistant DCE-MRI with high spatial and temporal resolution for more accurate and reproducible quantification of MRI-derived metrics. The new technology to be disseminated as a prototype on GE scanners will promote the use of quantitative DCE-MRI biomarkers in clinical practice, a long-desired goal.",Rapid motion-robust quantitative DCE-MRI for the assessment of gynecologic cancers,10052888,R01CA244532,"['Affect', 'Aftercare', 'Algorithms', 'Automation', 'Blood Vessels', 'Breathing', 'Cancer Patient', 'Chemotherapy and/or radiation', 'Clinical', 'Complex', 'Data', 'Data Set', 'Development', 'Diagnostic Neoplasm Staging', 'Dimensions', 'Discipline of obstetrics', 'Disease', 'Drug Kinetics', 'Early treatment', 'Environment', 'Evaluation', 'Generations', 'Goals', 'Guidelines', 'Gynecologic', 'Gynecology', 'Healthcare', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging Techniques', 'Imaging technology', 'International', 'Learning', 'Licensing', 'Magnetic Resonance Imaging', 'Malignant Female Reproductive System Neoplasm', 'Malignant Neoplasms', 'Malignant neoplasm of cervix uteri', 'Manufacturer Name', 'Maps', 'Memorial Sloan-Kettering Cancer Center', 'Methodology', 'Modeling', 'Monitor', 'Morphology', 'Motion', 'New York', 'Patients', 'Prediction of Response to Therapy', 'Qualitative Evaluations', 'Radial', 'Relapse', 'Reproducibility', 'Resistance', 'Speed', 'T2 weighted imaging', 'Techniques', 'Time', 'Training', 'Translating', 'Translations', 'Treatment Side Effects', 'Tumor stage', 'Universities', 'Woman', 'advanced disease', 'anticancer research', 'base', 'cancer imaging', 'cancer type', 'chemoradiation', 'clinical practice', 'contrast enhanced', 'convolutional neural network', 'deep learning', 'experience', 'image reconstruction', 'improved', 'improved outcome', 'individualized medicine', 'industry partner', 'insight', 'interest', 'magnetic resonance imaging biomarker', 'motion sensitivity', 'new technology', 'novel', 'population based', 'prototype', 'reconstruction', 'response', 'standard of care', 'success', 'temporal measurement', 'tool', 'treatment response', 'tumor', 'tumor microenvironment']",NCI,SLOAN-KETTERING INST CAN RESEARCH,R01,2020,535197,-0.0829362001632292
"A robust platform for multiplexed, subcellular proteomic imaging in human tissue Project Summary Multiplexed Ion Beam Imaging by Time of Flight (MIBI-TOF) uses secondary ion mass spectrometry and metal conjugated primary antibodies to simultaneously visualize dozens of proteins at subcellular resolution in a single tissue section. This technology is back compatible with archival formalin fixed, paraffin embedded tissue (FFPE) and has been used in peer-reviewed work to simultaneously visualize and quantify 36 proteins in retrospective human tissue cohorts. In line with the stated goals of the HuBMAP consortium to develop both “High-sensitivity, high-resolution imaging techniques that can rapidly provide spectral data over large areas of tissue” and “Quantitative imaging analysis tools, including automated 3D image segmentation, feature extraction, and image annotation,” the work outlined here will create a standardized, high throughput, and user-friendly workflow for using MIBI-TOF in basic and translational research to gain insight into how single cell phenotype and tissue structure are functionally-linked in health and disease. To achieve this, we will validate 100 FFPE antibodies and optimize ready-to-use multiplexed staining panels in lyophilized format that will permit storage for at least two years. Protocols and reagents for multiplexed signal amplification of protein and mRNA targets will be further refined, while next generation instrumentation will increase sample throughput to permit full tissue section imaging of up to 40 proteins in 1 hour. Standardized reagents and more robust instrumentation will be accompanied by an automated computational pipeline that utilizes a standard set of segmentation markers and machine learning to accurately identify nuclei and cell borders in any non-neural human tissue. This data will be used to cluster single cell events into functionally distinct populations according to morphology, protein expression, and histological distribution. The reagents and computational pipeline proposed here synergize with existing HuBMAP-funded platforms and could be readily generalized to virtually any high dimensional imaging modality. Thus, this work will not only provide a practical, back compatible imaging platform for high throughput multiplexed imaging, but will also accelerate development of other complimentary imaging technologies as well. Project Narrative Multiplexed ion beam imaging by time of flight (MIBI-TOF) is a new technology for visualizing dozens of proteins in standard clinical tissue biopsies at high resolution. The work outlined here will create a standardized, high throughput, and user-friendly workflow for using MIBI-TOF in basic and translational research to gain insight into how single cell phenotype and tissue structure are functionally-linked in health and disease.","A robust platform for multiplexed, subcellular proteomic imaging in human tissue",9894465,UH3CA246633,"['Allergic', 'Antibodies', 'Archives', 'Area', 'Atlases', 'Back', 'Basic Science', 'Biopsy', 'Cell Nucleus', 'Cells', 'Clinical', 'Cloud Computing', 'Communities', 'Data', 'Data Set', 'Decidua', 'Development', 'Disease', 'Equipment', 'Event', 'Extramural Activities', 'Feedback', 'First Pregnancy Trimester', 'Formalin', 'Foundations', 'Freeze Drying', 'Funding', 'Goals', 'Granuloma', 'Health', 'Hippocampus (Brain)', 'Histologic', 'Hour', 'Human', 'Image', 'Image Analysis', 'Imaging Techniques', 'Imaging technology', 'Immune', 'Immunosuppression', 'Individual', 'Institutes', 'Ions', 'Letters', 'Link', 'Machine Learning', 'Medical center', 'Messenger RNA', 'Metals', 'Morphology', 'Multiplexed Ion Beam Imaging', 'Noninfiltrating Intraductal Carcinoma', 'Optics', 'Organ', 'Paraffin Embedding', 'Pathology', 'Peer Review', 'Phenotype', 'Population', 'Proteins', 'Proteomics', 'Protocols documentation', 'Pulmonary Tuberculosis', 'Readiness', 'Reagent', 'Reproducibility', 'Resolution', 'Resources', 'Sampling', 'Scanning', 'Signal Transduction', 'Site', 'Spectrometry, Mass, Secondary Ion', 'Stains', 'Standardization', 'Structure', 'Technology', 'Three-Dimensional Image', 'Time', 'Tissue Embedding', 'Tissues', 'Translational Research', 'United States National Institutes of Health', 'Work', 'cancer immunotherapy', 'cohort', 'computational platform', 'computerized tools', 'design', 'graphical user interface', 'high dimensionality', 'high resolution imaging', 'human imaging', 'human tissue', 'imaging Segmentation', 'imaging modality', 'imaging platform', 'insight', 'instrumentation', 'ion source', 'learning strategy', 'multiplexed\xa0imaging', 'new technology', 'next generation', 'programs', 'protein expression', 'quantitative imaging', 'reagent standardization', 'technology validation', 'tool', 'tumor microenvironment', 'user-friendly', 'virtual']",NCI,STANFORD UNIVERSITY,UH3,2019,590000,-0.04749325182294323
"A robust platform for multiplexed, subcellular proteomic imaging in human tissue Project Summary Multiplexed Ion Beam Imaging by Time of Flight (MIBI-TOF) uses secondary ion mass spectrometry and metal conjugated primary antibodies to simultaneously visualize dozens of proteins at subcellular resolution in a single tissue section. This technology is back compatible with archival formalin fixed, paraffin embedded tissue (FFPE) and has been used in peer-reviewed work to simultaneously visualize and quantify 36 proteins in retrospective human tissue cohorts. In line with the stated goals of the HuBMAP consortium to develop both “High-sensitivity, high-resolution imaging techniques that can rapidly provide spectral data over large areas of tissue” and “Quantitative imaging analysis tools, including automated 3D image segmentation, feature extraction, and image annotation,” the work outlined here will create a standardized, high throughput, and user-friendly workflow for using MIBI-TOF in basic and translational research to gain insight into how single cell phenotype and tissue structure are functionally-linked in health and disease. To achieve this, we will validate 100 FFPE antibodies and optimize ready-to-use multiplexed staining panels in lyophilized format that will permit storage for at least two years. Protocols and reagents for multiplexed signal amplification of protein and mRNA targets will be further refined, while next generation instrumentation will increase sample throughput to permit full tissue section imaging of up to 40 proteins in 1 hour. Standardized reagents and more robust instrumentation will be accompanied by an automated computational pipeline that utilizes a standard set of segmentation markers and machine learning to accurately identify nuclei and cell borders in any non-neural human tissue. This data will be used to cluster single cell events into functionally distinct populations according to morphology, protein expression, and histological distribution. The reagents and computational pipeline proposed here synergize with existing HuBMAP-funded platforms and could be readily generalized to virtually any high dimensional imaging modality. Thus, this work will not only provide a practical, back compatible imaging platform for high throughput multiplexed imaging, but will also accelerate development of other complimentary imaging technologies as well. Project Narrative Multiplexed ion beam imaging by time of flight (MIBI-TOF) is a new technology for visualizing dozens of proteins in standard clinical tissue biopsies at high resolution. The work outlined here will create a standardized, high throughput, and user-friendly workflow for using MIBI-TOF in basic and translational research to gain insight into how single cell phenotype and tissue structure are functionally-linked in health and disease.","A robust platform for multiplexed, subcellular proteomic imaging in human tissue",10231018,UH3CA246633,"['Allergic', 'Antibodies', 'Archives', 'Area', 'Back', 'Basic Science', 'Biopsy', 'Cell Nucleus', 'Cells', 'Clinical', 'Cloud Computing', 'Communities', 'Data', 'Data Set', 'Decidua', 'Development', 'Disease', 'Equipment', 'Event', 'Extramural Activities', 'Feedback', 'First Pregnancy Trimester', 'Formalin', 'Foundations', 'Freeze Drying', 'Funding', 'Goals', 'Granuloma', 'Health', 'Hippocampus (Brain)', 'Histologic', 'Hour', 'Human', 'Human BioMolecular Atlas Program', 'Image', 'Imaging Techniques', 'Imaging technology', 'Immune', 'Immunosuppression', 'Individual', 'Institutes', 'Ions', 'Letters', 'Link', 'Machine Learning', 'Medical center', 'Messenger RNA', 'Metals', 'Morphology', 'Multiplexed Ion Beam Imaging', 'Noninfiltrating Intraductal Carcinoma', 'Optics', 'Organ', 'Paraffin Embedding', 'Pathology', 'Peer Review', 'Phenotype', 'Population', 'Proteins', 'Proteomics', 'Protocols documentation', 'Pulmonary Tuberculosis', 'Readiness', 'Reagent', 'Reproducibility', 'Resolution', 'Resources', 'Sampling', 'Scanning', 'Signal Transduction', 'Site', 'Spectrometry, Mass, Secondary Ion', 'Stains', 'Standardization', 'Structure', 'Technology', 'Three-Dimensional Image', 'Time', 'Tissue Embedding', 'Tissues', 'Translational Research', 'United States National Institutes of Health', 'Work', 'cancer immunotherapy', 'cohort', 'computational pipelines', 'computational platform', 'computerized tools', 'design', 'feature extraction', 'graphical user interface', 'high dimensionality', 'high resolution imaging', 'human imaging', 'human tissue', 'imaging Segmentation', 'imaging modality', 'imaging platform', 'insight', 'instrumentation', 'ion source', 'machine learning method', 'multiplexed imaging', 'new technology', 'next generation', 'protein expression', 'quantitative imaging', 'reagent standardization', 'technology validation', 'tool', 'tumor microenvironment', 'user-friendly', 'virtual']",NCI,STANFORD UNIVERSITY,UH3,2020,700000,-0.04749325182294323
"Machine learning accelerated on-line adaptive replanning Abstract. The overall goal of this proposal is to develop and test a novel machine learning (ML) accelerated On-Line Adaptive Replanning (MOLAR) solution for magnetic resonance imaging (MRI) guided radiation therapy (RT) (MRgRT). During the multi-fraction RT process, the location, shape and size of tumors and normal organs vary significantly between the fractions. These interfraction variations are among the major factors that can limit the accuracy of RT targeting. The current standard practice of image-guided RT (IGRT), developed to address the interfraction variations based on cone-beam CT (CBCT), can only correct for translational errors, and thus does not fully account for interfraction changes. To address this issue, researchers recently introduced online adaptive replanning (OLAR) that generates a new plan based on the anatomy of the day and delivers the plan for the fraction. Currently, two main obstacles affect the success of OLAR: (1) the anatomy of the day cannot be delineated accurately based on CBCT, and (2) the time required to perform OLAR is long enough to render it impractical. One way to improve the delineation accuracy is to use MRI versus CT. MRI-guided OLAR is currently being introduced into the clinics to substantially improve RT targeting. However, the bottleneck is still the impractical length of time required to segment the anatomy of the day, which can exceed 30 minutes. Furthermore, available synthetic CT (sCT) generation methods are slow or inaccurate for MRI-guided OLAR. There is no method available to quickly and objective determine when OLAR is necessary. To address these issues, we plan to develop novel techniques in the MOLAR solution. We hypothesize that the MRI-based MOLAR solution will fully account for interfraction changes, thereby substantially improving tumor targeting during RT delivery and the effectiveness of RT. Specifically, we aim to (1) develop practical ML-based solutions to quickly determine the necessity of OLAR and to rapidly generate accurate synthetic CTs; (2) develop ML-based techniques to substantially accelerate segmentation for OLAR using a progressive three-step process; and (3) verify clinical practicality and effectiveness of MOLAR by retrospectively and prospectively applying the MOLAR on MRI sets to test its speed and effectiveness in accounting for interfraction variations. We will develop this novel MOLAR solution by forging unique collaborations between clinical physicists, radiation oncologists and industry developers via an established academic-industry partnership. The successful completion of this project will enable clinicians to routinely practice “image-plan-treat”, which is the optimal solution for MRgRT. This new paradigm will fully account for interfraction variations, improve tumor targeting, reduce normal tissue toxicity, and ultimately encourage clinicians to revise the current doses and/or dose fractionations to increase therapeutic gain, enhance patient quality of life, and/or substantially save on healthcare costs. Our proposed strategy represents a drastic departure from current practice. We firmly believe that this strategy is the future of RT delivery. Project Narrative: This R01 application proposes to develop and test a novel machine learning accelerated online adaptive replanning (MOLAR) solution for magnetic resonance imaging (MRI) guided adaptive radiation therapy through a unique academic and industry partnership. The MOLAR solution aims to fully account for interfraction variations, thereby substantially improving the accuracy and effectiveness of radiation therapy (RT) for cancer. This solution will enable clinicians to routinely practice “image-plan-treat”, a drastic departure from current practice and representing the future of RT delivery.",Machine learning accelerated on-line adaptive replanning,9941621,R01CA247960,"['3-Dimensional', 'Accounting', 'Address', 'Adoption', 'Affect', 'Air', 'Anatomy', 'Clinic', 'Clinical', 'Collaborations', 'Dose Fractionation', 'Effectiveness', 'Electron Transport', 'Future', 'Generations', 'Goals', 'Health Care Costs', 'Image', 'Industry', 'Learning', 'Length', 'Location', 'Lung', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant neoplasm of pancreas', 'Maps', 'Methodology', 'Methods', 'Modality', 'Normal tissue morphology', 'Organ', 'Patients', 'Physiology', 'Process', 'Quality of life', 'Radiation Oncologist', 'Radiation therapy', 'Research Personnel', 'Shapes', 'Site', 'Speed', 'Surface', 'Techniques', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Toxic effect', 'Variant', 'automated segmentation', 'base', 'bone', 'cancer radiation therapy', 'cone-beam computed tomography', 'convolutional neural network', 'electron density', 'forging', 'image guided', 'image guided radiation therapy', 'imaging modality', 'improved', 'industry partner', 'innovation', 'large datasets', 'neural network algorithm', 'novel', 'pancreatic cancer patients', 'prospective', 'prospective test', 'quantitative imaging', 'routine practice', 'soft tissue', 'success', 'targeted treatment', 'tool', 'treatment response', 'tumor']",NCI,MEDICAL COLLEGE OF WISCONSIN,R01,2020,517299,-0.041493493102865354
"Automatic Organ Segmentation Tool for Radiation Treatment Planning of Cancers ABSTRACT As early detection and better treatment have increased cancer patient survival rates, the importance of protecting normal organs during radiation treatment is drawing more attention, which is critical in reducing long term toxicity of cancers. To avoid excessively high radiation doses to organs-at-risk (OARs), OARs need to be correctly segmented from simulation computed tomography (CT) scans during radiation treatment planning to get an accurate dose distribution. Despite tremendous effort in developing semi- or fully-automatic segmentation solutions, current automated segmentation software, mostly using the atlas-based methods, has not yet reached the level of accuracy and robustness required for clinical usage. Therefore, in current practice, significant manual efforts are still required in the OAR segmentation process. Manual contouring suffers from inter- and intra-observer variability, as well as institutional variability where different sites adopt distinct contouring atlases and labeling criteria, thus leading to inaccuracy and variability in OAR segmentation. When OARs are very close to the treatment target, segmentation errors as small as a few millimeters can have a statistically significant impact on dosimetry distribution and outcome. In addition, it is also costly and time consuming as it can take 1-2 hours of a clinicians’ time to segment major thoracic organs due to the large number of axial slices required. In summary, an accurate and fast process for segmenting OARs in treatment planning using CT scans is needed for improving patient outcomes and reducing the cost of radiation therapy of cancers. In recent years, the rapid development of deep learning methods has revolutionized many computer-vision areas and the adoption of deep learning in medical applications has shown great success. Based on a deep-learning-based algorithm we developed that achieved better-than-human performance and ranked 1st in 2017 American Association of Physicist in Medicine Thoracic Auto-segmentation Challenge, an automatic OAR segmentation product will be developed in this project with the three aims: 1) further improve the performance and robustness of OAR segmentation algorithms, focusing on addressing the heterogeneity issue of different clinical environments; 2) further enrich the functionalities and enhance usability of the cloud- based software product; and 3) perform clinical validation study on the algorithm performance and software usability at collaborating sites. With this product, the segmentation accuracy can be improved, leading to more robust treatment plans in protecting normal organs and improved long term patient outcome. The time and cost of radiation treatment planning can be greatly reduced, contributing to a more affordable cancer treatment and reduced healthcare burden. NARRATIVE As early detection and better treatment have increased cancer patient survival rates, the importance of protecting normal organs during radiation treatment is drawing more attention. To avoid excessively high radiation doses to such organs-at-risk (OARs), they are required to be correctly segmented from simulation computed tomography (CT) scans. A deep-learning-based automatic OAR segmentation product developed in this project can improve the segmentation accuracy and reduce the time and cost of radiation treatment planning as compared with the current manual process, leading to improved long term patient outcome and reduced cancer treatment cost.",Automatic Organ Segmentation Tool for Radiation Treatment Planning of Cancers,10081752,R44CA254844,"['3-Dimensional', 'Address', 'Adopted', 'Adoption', 'Algorithms', 'American', 'Area', 'Artificial Intelligence', 'Atlases', 'Attention', 'Body Regions', 'Body part', 'Cancer Patient', 'Chest', 'Clinical', 'Clinical Research', 'Computer Vision Systems', 'Computer software', 'Consumption', 'Data', 'Development', 'Digital Imaging and Communications in Medicine', 'Dose', 'Early Diagnosis', 'Environment', 'Healthcare', 'Heterogeneity', 'Hour', 'Human', 'Image', 'Intraobserver Variability', 'Label', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medical', 'Medicine', 'Methods', 'Modality', 'Modeling', 'Online Systems', 'Organ', 'Outcome', 'Patient-Focused Outcomes', 'Performance', 'Phase', 'Process', 'Protocols documentation', 'Radiation Dose Unit', 'Radiation therapy', 'Risk', 'Scanning', 'Site', 'Slice', 'Survival Rate', 'Techniques', 'Testing', 'Time', 'Toxic effect', 'Treatment Cost', 'Update', 'X-Ray Computed Tomography', 'algorithm development', 'automated segmentation', 'base', 'cancer radiation therapy', 'cancer therapy', 'clinical heterogeneity', 'cloud based', 'commercialization', 'convolutional neural network', 'cost', 'deep learning', 'deep learning algorithm', 'dosimetry', 'healthcare community', 'imaging modality', 'improved', 'innovation', 'learning strategy', 'life-long learning', 'millimeter', 'novel', 'phase 1 study', 'prototype', 'satisfaction', 'segmentation algorithm', 'simulation', 'software development', 'success', 'tool', 'treatment planning', 'usability', 'user-friendly', 'validation studies']",NCI,"CARINA MEDICAL, LLC",R44,2020,1000000,-0.007824641554914625
"In situ transcriptome profiling in single cells Summary We have recently developed intron seqFISH (sequential Fluorescence in situ hybridization) to multiplex 10,421 genes directly in single cells. We showed that the 10,421 gene nascent transcriptome profile can identify cell types as well as capture the trajectory of the cells. We further demonstrated that we can perform mRNA seqFISH as well as immunostaining in the same cells following the 10,421 gene intron seqFISH measurement. We propose to develop this technology as a potential alternative approach to single cell RNAseq for the HuBMAP to characterize cell types directly in situ in tissues. In particular, we will adept in situ amplification methods such as hybridization chain reaction (HCR) to intron seqFISH. We had previously shown that mRNA seqFISH with HCR amplification performs exceptionally in tissues in overcoming autofluorescence background and enable robust decoding seqFISH barcodes. We will validate the integrated intron and mRNA seqFISH protocol in the mouse hippocampus in the UG3 phase of the project. Also in UG3 phase, we will develop computational tools to integrate intron seqFISH data with mRNA seqFISH as well as single cell RNAseq data. In the UH3 phase, we will translate the technology to human tissues, with a focus on human mammary tissues provided by Dr. Seewaldt at City of Hope. We will also work with the tissue mapping centers in the HuBMAP program to accelerate the translation of this technology to many tissue types. In the UH3 phase, we will generate million cell spatial atlas of human tissues containing intron profiles, mRNA profiles and protein abundances in each single cell. We will further develop computational tools to analyze for spatial enrichment of genes in the tissue and generate a pseudotime of developmental trajectories using the nascent transcriptome data. Taken together, we will develop a high throughput in situ imaging based platform to characterize cell types and future trajectories of cells using intron and mRNA seqFISH technologies. Narrative Transcriptome profiling in situ at the single cell level has transformative potential for understanding healthy versus diseased tissues in the human body. We propose an integrative approach profiling the nascent transcriptome as well as hundreds of mature transcripts in single cells in tissues. We will generate million cell spatial atlas for the mouse hippocampus as well as the human mammary tissue. We will gain unprecedented insight into the developmental of neurons in the brain as well as the duct cells in the breast. At the same time we will develop the computational tools to analyze the spatial genomics data.",In situ transcriptome profiling in single cells,10026445,UH3CA255134,"['Algorithms', 'Animals', 'Atlases', 'Bar Codes', 'Biology', 'Biopsy Specimen', 'Brain', 'Breast', 'Cell Differentiation process', 'Cells', 'Cities', 'Data', 'Data Set', 'Development', 'Disease', 'Ductal Epithelial Cell', 'Epigenetic Process', 'Epithelial', 'Epithelial-Stromal Communication', 'Epithelium', 'Expression Profiling', 'Fluorescent in Situ Hybridization', 'Future', 'Gene Expression', 'Genes', 'Goals', 'Hippocampus (Brain)', 'Human', 'Human BioMolecular Atlas Program', 'Human body', 'Image', 'Image Analysis', 'Immunofluorescence Immunologic', 'In Situ', 'Introns', 'Letters', 'Machine Learning', 'Mammary Gland Parenchyma', 'Mammary gland', 'Maps', 'Measurement', 'Messenger RNA', 'Methods', 'Mus', 'Natural regeneration', 'Neuroglia', 'Neurons', 'Nuclear Structure', 'Pattern', 'Phase', 'Play', 'Positioning Attribute', 'Process', 'Proteins', 'Protocols documentation', 'Publications', 'Publishing', 'Radial', 'Reaction', 'Role', 'Signal Transduction', 'Slice', 'Speed', 'Stains', 'Standardization', 'System', 'Techniques', 'Technology', 'Time', 'Tissue Sample', 'Tissue imaging', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Validation', 'Weight', 'Work', 'base', 'cell type', 'combinatorial', 'computerized tools', 'dentate gyrus', 'genomic data', 'granule cell', 'human tissue', 'imaging system', 'in situ imaging', 'insight', 'internal control', 'mammary epithelium', 'markov model', 'programs', 'sample fixation', 'scale up', 'single molecule', 'single-cell RNA sequencing', 'tool', 'transcriptome']",NCI,CALIFORNIA INSTITUTE OF TECHNOLOGY,UH3,2020,400000,-0.04940091815530782
"SBIR Phase I Topic 402: Artificial Intelligence-Aided Imaging for Cancer Prevention, Diagnosis, and Monitoring Image-based evaluation of lymph nodes is an essential step in cancer diagnosis, treatment and monitoring. Current clinical practice mostly uses qualitative or semi-quantitative measures in evaluation and thus suffers from inaccuracy due to intra- and inter-observer variability and increased human efforts. This becomes a more serious issue in head and neck cancers due to the large number of clinically relevant lymph nodes. In this project an AI-based automatic segmentation software will be developed for quantitative cervical lymph node evaluation to increase the accuracy and reduce the cost. However, there are a few challenges in developing and deploying such a software due to different clinical practices such as usage of different modalities (MRI and/or CT) and complex clinical workflow. To address these challenges, a novel AI algorithm that can handle the variability in imaging modalities and support incremental learning using site-specific data to enhance its robustness will be developed; a private-cloud-based software framework with high usability will then be developed to incorporate this algorithm and provide advanced visualization and reporting for clinical usage. This software will have high impact on all stages of patient care for head and neck cancers and can be further extended to other cancers. n/a","SBIR Phase I Topic 402: Artificial Intelligence-Aided Imaging for Cancer Prevention, Diagnosis, and Monitoring",10269836,5N91020C00048,"['Address', 'Algorithms', 'Artificial Intelligence', 'Cervical lymph node group', 'Clinical', 'Complex', 'Computer software', 'Data', 'Detection', 'Diagnosis', 'Evaluation', 'Head and Neck Cancer', 'Human', 'Image', 'Interobserver Variability', 'Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Measures', 'Modality', 'Monitor', 'Patient Care', 'Performance', 'Phase', 'Privatization', 'Reporting', 'Site', 'Small Business Innovation Research Grant', 'Software Framework', 'Visualization', 'automated segmentation', 'base', 'cancer diagnosis', 'cancer imaging', 'cancer prevention', 'clinical practice', 'clinically relevant', 'cloud based', 'cost', 'imaging modality', 'lymph nodes', 'novel', 'segmentation algorithm', 'usability']",NCI,"CARINA MEDICAL, LLC",N43,2020,400000,-0.01175529210065164
"MAPPING AND END USER INTERFACE FOR TOPIC MODELS OF NIH GRANTS In the fall of 2007, NINDS initiated a review of funded grants as part of a general strategic planning effort. The Institute was directed by a member of the NINDS advisory council to a web-based technique for analyzing the ~14,000 Annual Meeting abstracts of the Society for Neuroscience (http://scimaps.org/maps/neurovis/new/). This technique used a publically available topic modeling algorithm known as Latent Dirichlet Allocation to determine the topic distribution of individual SfN abstracts, and used a simulated annealing graphical layout algorithm known as VxINSITE (later incorporated into DrL - SAND2008-2936J: Sandia National Laboratories) to map the topical relationships between abstracts into a Google Maps application interface, in which users can select sets of abstracts from a two-dimensional color-coded map and access topical information regarding these abstracts via an underlying database. The team that developed this framework, Gully Burns at University of Southern California (USC), David Newman at University of California Irvine (UCI), and Bruce Herr at Indiana University (IU), agreed to apply it to a set of NIH grants in order to test its potential utility for NIH staff and extramural scientists. Subsequently the project was expanded to model and map the entire 2007 set of NIH grants, with a resulting color-coded map and topic database. The results of this work can be viewed at http://scimaps.org/maps/ninds/ and http://scimaps.org/maps/nih/2007/. For the upcoming project period, the Institute has enlisted the services of the Information Extraction and Synthesis Laboratory (IESL) at the University of Massachusetts to perform topic modeling of NIH grants. The basic grants data plus the results of the topic model analysis will be transferred to Chalklabs, which will integrate it into an interface for Topic Model Browsing and for mapping using the DrL algorithm. n/a",MAPPING AND END USER INTERFACE FOR TOPIC MODELS OF NIH GRANTS,8356039,71201100725P,"['Algorithms', 'Burn injury', 'California', 'Categories', 'Code', 'Color', 'Data', 'Databases', 'Extramural Activities', 'Funding', 'Grant', 'Indiana', 'Individual', 'Information Services', 'Institutes', 'Laboratories', 'Logic', 'Maintenance', 'Maps', 'Massachusetts', 'Modeling', 'Modification', 'NIH Program Announcements', 'National Institute of Neurological Disorders and Stroke', 'Neurosciences', 'Online Systems', 'Scientist', 'Simulate', 'Societies', 'Specific qualifier value', 'Stimulus', 'Strategic Planning', 'Study Section', 'Techniques', 'Testing', 'United States National Institutes of Health', 'Universities', 'Update', 'Work', 'abstracting', 'falls', 'meeting abstracts', 'member', 'tool', 'two-dimensional']",NINDS,"CHALKLABS, LLC",N02,2011,100000,-0.029130766070417514
"CHALKLABS LLC [10-011442] In the fall of 2007, NINDS initiated a review of funded grants as part of a general strategic planning effort. The Institute was directed by a member of the NINDS advisory council to a web-based technique for analyzing the ~14,000 Annual Meeting abstracts of the Society for Neuroscience (http://scimaps.org/maps/neurovis/new/). This technique used a publically available topic modeling algorithm known as Latent Dirichlet Allocation to determine the topic distribution of individual SfN abstracts, and used a simulated annealing graphical layout algorithm known as VxINSITE (later incorporated into DrL - SAND2008-2936J: Sandia National Laboratories) to map the topical relationships between abstracts into a Google Maps application interface, in which users can select sets of abstracts from a two-dimensional color-coded map and access topical information regarding these abstracts via an underlying database. The team that developed this framework, Gully Burns at University of Southern California (USC), David Newman at University of California Irvine (UCI), and Bruce Herr at Indiana University (IU), agreed to apply it to a set of NIH grants in order to test its potential utility for NIH staff and extramural scientists. Subsequently the project was expanded to model and map the entire 2007 set of NIH grants, with a resulting color-coded map and topic database. The results of this work can be viewed at http://scimaps.org/maps/ninds/ and http://scimaps.org/maps/nih/2007/. For the upcoming project period, the Institute has enlisted the services of the Information Extraction and Synthesis Laboratory (IESL) at the University of Massachusetts to perform topic modeling of NIH grants. The basic grants data plus the results of the topic model analysis will be transferred to Chalklabs, which will integrate it into an interface for Topic Model Browsing and for mapping using the DrL algorithm. n/a",CHALKLABS LLC [10-011442],8164240,71201000701P,[' '],NINDS,"CHALKLABS, LLC",N02,2010,200000,-0.021047411805062255
"MAPPING AND END USER INTERFACE FOR TOPIC MODELS OF NIH GRANTS In the fall of 2007, NINDS initiated a review of funded grants as part of a general strategic planning effort. The Institute was directed by a member of the NINDS advisory council to a web-based technique for analyzing the ~14,000 Annual Meeting abstracts of the Society for Neuroscience (http://scimaps.org/maps/neurovis/new/). This technique used a publically available topic modeling algorithm known as Latent Dirichlet Allocation to determine the topic distribution of individual SfN abstracts, and used a simulated annealing graphical layout algorithm known as VxINSITE (later incorporated into DrL - SAND2008-2936J: Sandia National Laboratories) to map the topical relationships between abstracts into a Google Maps application interface, in which users can select sets of abstracts from a two-dimensional color-coded map and access topical information regarding these abstracts via an underlying database. The team that developed this framework, Gully Burns at University of Southern California (USC), David Newman at University of California Irvine (UCI), and Bruce Herr at Indiana University (IU), agreed to apply it to a set of NIH grants in order to test its potential utility for NIH staff and extramural scientists. Subsequently the project was expanded to model and map the entire 2007 set of NIH grants, with a resulting color-coded map and topic database. The results of this work can be viewed at http://scimaps.org/maps/ninds/ and http://scimaps.org/maps/nih/2007/. For the upcoming project period, the Institute has enlisted the services of the Information Extraction and Synthesis Laboratory (IESL) at the University of Massachusetts to perform topic modeling of NIH grants. The basic grants data plus the results of the topic model analysis will be transferred to Chalklabs, which will integrate it into an interface for Topic Model Browsing and for mapping using the DrL algorithm. n/a",MAPPING AND END USER INTERFACE FOR TOPIC MODELS OF NIH GRANTS,7977349,71200900695P,[' '],NINDS,"CHALKLABS, LLC",N02,2009,115000,-0.029130766070417514

text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6717704,P01EB000216,"['automated medical record system', 'health care facility information system', 'radiology', 'telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2004,1723576,0.02510484479981226
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6682850,P01EB000216,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2003,1645392,0.02510484479981226
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6512665,P01EB000216,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2002,2120168,0.02510484479981226
"INFORMATION AND THE TRANSFORMATION OF MOLECULAR BIOLOGY This project leads to book tentatively entitled Code, Program, Text: Information and the Transformation of Molecular Biology.  It focuses on the years 1953-1973, when researches unravelled salient mechanisms of DNA function, representing it as information transfer: a genetic code in which DNA messages are transcribed and then translated into proteins. These works have elucidated key features of living systems: genetic transmission, mutation, and regulation.  They have also altered our basic concepts of nature, organisms, health, disease and behavior, and through genetic engineering technologies, have endowed scientists with unprecedented power over life.  The aim of this project is to reconstruct a critical history of the genetic code, posing the questions: how did scientists come to view organisms and molecules as information storage and retrieval systems; by which processes did life come to be conceptualized as a text written in a natural language; what have been the cognitive/social consequences of this transformation.  The work examines conceptual models, linguistic tools, and representational strategies that produced a discourse of heredity grounded in terms such as code, program, tape, message, reading- frame, language, information and text; terms that were absent in life science before the 1950s.  The study is framed within an historically specific knowledge/power nexus linked by epistemic and technological objectives, discursive and nondiscursive practices.  It draws on the history of biology, cybernetics, communications technologies, information theory, digital systems, and the militarization of Western (especially American) culture during World War II and Cold War.  Thus this study hopes to show that the information-based transformation of molecular biology is part of a broader cultural and discursive trend nested in postwar political and technoscientific reconfiguration.  n/a",INFORMATION AND THE TRANSFORMATION OF MOLECULAR BIOLOGY,2209134,R01HG000901,"['DNA', ' biochemical evolution', ' biological information processing', ' books', ' genetics', ' health science research', ' history of life science', ' molecular biology', ' nucleic acid sequence', ' systematic biology', ' technology /technique']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,1994,199751,0.048445561873306994
"High Performance Text Mining for Translator We propose to build a knowledge provider that will seek out, integrate and provide AIready, BioLink-compatible models via high-performance text-mining of the biomedical literature. Problems with Translator’s current mining of the biomedical literature that we intend to solve include: (1) weaknesses in framework extensibility and benchmarking that make integrating and validating new text-mining approaches difficult; (2) problematic licensing of software, terminologies and other resources that do not adequately support FAIR (and TLC) best practices; (3) processing only PubMed titles and abstracts, not full text publications; (4) Translator’s use of older NLP technology with relatively poor performance; (5) lack of a mechanism for community feedback regarding errors and other problems; (6) lack of continuous updates to add knowledge from new publications; (7) output knowledge representation that is simplistic and vague, failing to reflect the richness of what is expressed in scientific documents. n/a",High Performance Text Mining for Translator,10053507,OT2TR003422,"['Benchmarking', 'Communities', 'Computer software', 'Feedback', 'Knowledge', 'Licensing', 'Literature', 'Mining', 'Modeling', 'Output', 'Performance', 'Provider', 'PubMed', 'Publications', 'Resources', 'Technology', 'Terminology', 'Text', 'Update', 'information organization', 'text searching']",NCATS,UNIVERSITY OF COLORADO DENVER,OT2,2020,735622,0.06494913187484201
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8515555,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2012,290837,-0.01915986944087652
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8034342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2011,332732,-0.01915986944087652
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,7772342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2010,326303,-0.01915986944087652
"Textpresso: information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso: information retrieval and extraction system for biological literature,7583249,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Body of uterus', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2009,320000,-0.01915986944087652
"INDEXING AND RETRIEVING INFORMATION The goal of this proposal is to develop an expert system for archiving medical information.  The main feature of the system will be the ability to automatically index medical text with keywords.  Input to the system will consist of reports from medical charts, abstracts from the medical literature, and descriptive passages from teaching collections.  The system will retrieve information through requests formulated in natural language, or through Boolean combinations of keywords.   The knowledge base for the expert system will be a semantic network generated from thesauri of medical terms.  The expert system shell; the natural language and Boolean retrieval routines and utilities for thesaurus construction have already been developed. Partial thesauri containing an average of 3,000 terms and 15,000 links have been constructed for neuropathology, neuroradiology, and psychiatry.  These thesauri have been successfully tested in pilot studies of automated indexing and natural language retrieval.  During the course of this project we will complete these thesauri and compare the performance of the full autoindexing system to human indexers.  If these tests are successful, we will extend thesaurus construction to neurology and neurosurgery developing a merged thesaurus covering the clinical neurosciences.  The expert system will then be integrated into a comprehensive medical information system under development at the University of Pittsburgh.  n/a",INDEXING AND RETRIEVING INFORMATION,3373894,R01LM004635,"['automated data processing', ' indexing', ' information retrieval', ' information system analysis']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,1990,159491,0.04598253372878391
"INDEXING AND RETRIEVING INFORMATION The goal of this proposal is to develop an expert system for archiving medical information.  The main feature of the system will be the ability to automatically index medical text with keywords.  Input to the system will consist of reports from medical charts, abstracts from the medical literature, and descriptive passages from teaching collections.  The system will retrieve information through requests formulated in natural language, or through Boolean combinations of keywords.   The knowledge base for the expert system will be a semantic network generated from thesauri of medical terms.  The expert system shell; the natural language and Boolean retrieval routines and utilities for thesaurus construction have already been developed. Partial thesauri containing an average of 3,000 terms and 15,000 links have been constructed for neuropathology, neuroradiology, and psychiatry.  These thesauri have been successfully tested in pilot studies of automated indexing and natural language retrieval.  During the course of this project we will complete these thesauri and compare the performance of the full autoindexing system to human indexers.  If these tests are successful, we will extend thesaurus construction to neurology and neurosurgery developing a merged thesaurus covering the clinical neurosciences.  The expert system will then be integrated into a comprehensive medical information system under development at the University of Pittsburgh.  n/a",INDEXING AND RETRIEVING INFORMATION,3373893,R01LM004635,"['automated data processing', ' indexing', ' information retrieval', ' information system analysis']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,1989,167639,0.04598253372878391
"INDEXING AND RETRIEVING INFORMATION The goal of this proposal is to develop an expert system for archiving medical information.  The main feature of the system will be the ability to automatically index medical text with keywords.  Input to the system will consist of reports from medical charts, abstracts from the medical literature, and descriptive passages from teaching collections.  The system will retrieve information through requests formulated in natural language, or through Boolean combinations of keywords.   The knowledge base for the expert system will be a semantic network generated from thesauri of medical terms.  The expert system shell; the natural language and Boolean retrieval routines and utilities for thesaurus construction have already been developed. Partial thesauri containing an average of 3,000 terms and 15,000 links have been constructed for neuropathology, neuroradiology, and psychiatry.  These thesauri have been successfully tested in pilot studies of automated indexing and natural language retrieval.  During the course of this project we will complete these thesauri and compare the performance of the full autoindexing system to human indexers.  If these tests are successful, we will extend thesaurus construction to neurology and neurosurgery developing a merged thesaurus covering the clinical neurosciences.  The expert system will then be integrated into a comprehensive medical information system under development at the University of Pittsburgh.  n/a",INDEXING AND RETRIEVING INFORMATION,3373892,R01LM004635,"['automated data processing', ' indexing', ' information retrieval', ' information system analysis']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,1988,179102,0.04598253372878391
"SAPHIRE--A CONCEPT-BASED APPROACH TO INFO RETRIEVAL The major goal of this project is to develop and evaluate innovative approaches to information retrieval (IR) in the biomedical domain. Building on the initial work done by the Principal Investigator (PI) with the SAPHIRE Project and taking advantage of the efforts of the Unified Medical Language Systems (UMLS) Project, we aim to design and test new methods for automated indexing and retrieval.  The underlying thesis of the SAPHIRE approach to IR is movement of information representation from the level of terms to that of concepts.  Terms, such as MeSH entries, are surface string representations of underlying concepts.  A problem with their use in representing concepts is that they cannot account for the different ways a concept may be expressed in medical texts or information system queries.  SAPHIRE is a first step in the direction of concept-based IR, and we plan to investigate several enhancements to this approach.  The major goal will be achieved with six separate but interrelated tasks:  1. Develop methodology for evaluation of IR systems in laboratory and clinical settings.  2. Assess the utility of computational linguistic approaches to concept discovery in text using constrained natural language processing and knowledge base construction.  3. Refine strategies for automated indexing of a wide variety of textual material, including abstracts, full text of articles, textbooks, and hypertext.  4. Explore different user interfaces, aiming to allow optimal retrieval for both novice and expert users.  5. Assess the use of semantic relationships between concepts in indexing and retrieval.  6. Integrate the SAPHIRE approach with other programs, such as the CODEX system and Explorer-2, and scale up to large text collections.  In the course of the project we will create an IR system that will help meet the information needs of busy health care providers.  Such a system should have a diverse variety of content available as well as quality indexing to represent content accurately.  It should also feature retrieval capability that is fast and easy to use.  In this grant, we propose to iteratively build an IR system that utilizes concept-based probabilistic indexing and retrieval, and evaluate it each step along the way in laboratory as well as real world settings.  n/a",SAPHIRE--A CONCEPT-BASED APPROACH TO INFO RETRIEVAL,2237758,R29LM005307,"['artificial intelligence', ' health care facility information system', ' indexing', ' information retrieval', ' information system analysis', ' information systems', ' language']",NLM,OREGON HEALTH AND SCIENCE UNIVERSITY,R29,1994,98036,0.05297308930283798
"SAPHIRE:  A CONCEPT-BASED APPROACH TO INFO RETRIEVAL The major goal of this project is to develop and evaluate innovative approaches to information retrieval (IR) in the biomedical domain. Building on the initial work done by the Principal Investigator (PI) with the SAPHIRE Project and taking advantage of the efforts of the Unified Medical Language Systems (UMLS) Project, we aim to design and test new methods for automated indexing and retrieval.  The underlying thesis of the SAPHIRE approach to IR is movement of information representation from the level of terms to that of concepts.  Terms, such as MeSH entries, are surface string representations of underlying concepts.  A problem with their use in representing concepts is that they cannot account for the different ways a concept may be expressed in medical texts or information system queries.  SAPHIRE is a first step in the direction of concept-based IR, and we plan to investigate several enhancements to this approach.  The major goal will be achieved with six separate but interrelated tasks:  1. Develop methodology for evaluation of IR systems in laboratory and clinical settings.  2. Assess the utility of computational linguistic approaches to concept discovery in text using constrained natural language processing and knowledge base construction.  3. Refine strategies for automated indexing of a wide variety of textual material, including abstracts, full text of articles, textbooks, and hypertext.  4. Explore different user interfaces, aiming to allow optimal retrieval for both novice and expert users.  5. Assess the use of semantic relationships between concepts in indexing and retrieval.  6. Integrate the SAPHIRE approach with other programs, such as the CODEX system and Explorer-2, and scale up to large text collections.  In the course of the project we will create an IR system that will help meet the information needs of busy health care providers.  Such a system should have a diverse variety of content available as well as quality indexing to represent content accurately.  It should also feature retrieval capability that is fast and easy to use.  In this grant, we propose to iteratively build an IR system that utilizes concept-based probabilistic indexing and retrieval, and evaluate it each step along the way in laboratory as well as real world settings.  n/a",SAPHIRE:  A CONCEPT-BASED APPROACH TO INFO RETRIEVAL,3474527,R29LM005307,"['artificial intelligence', ' health care facility information system', ' indexing', ' information retrieval', ' information system analysis', ' information systems', ' language']",NLM,OREGON HEALTH AND SCIENCE UNIVERSITY,R29,1993,91942,0.05297308930283798
"SAPHIRE--A CONCEPT-BASED APPROACH TO INFO RETRIEVAL The major goal of this project is to develop and evaluate innovative approaches to information retrieval (IR) in the biomedical domain. Building on the initial work done by the Principal Investigator (PI) with the SAPHIRE Project and taking advantage of the efforts of the Unified Medical Language Systems (UMLS) Project, we aim to design and test new methods for automated indexing and retrieval.  The underlying thesis of the SAPHIRE approach to IR is movement of information representation from the level of terms to that of concepts.  Terms, such as MeSH entries, are surface string representations of underlying concepts.  A problem with their use in representing concepts is that they cannot account for the different ways a concept may be expressed in medical texts or information system queries.  SAPHIRE is a first step in the direction of concept-based IR, and we plan to investigate several enhancements to this approach.  The major goal will be achieved with six separate but interrelated tasks:  1. Develop methodology for evaluation of IR systems in laboratory and clinical settings.  2. Assess the utility of computational linguistic approaches to concept discovery in text using constrained natural language processing and knowledge base construction.  3. Refine strategies for automated indexing of a wide variety of textual material, including abstracts, full text of articles, textbooks, and hypertext.  4. Explore different user interfaces, aiming to allow optimal retrieval for both novice and expert users.  5. Assess the use of semantic relationships between concepts in indexing and retrieval.  6. Integrate the SAPHIRE approach with other programs, such as the CODEX system and Explorer-2, and scale up to large text collections.  In the course of the project we will create an IR system that will help meet the information needs of busy health care providers.  Such a system should have a diverse variety of content available as well as quality indexing to represent content accurately.  It should also feature retrieval capability that is fast and easy to use.  In this grant, we propose to iteratively build an IR system that utilizes concept-based probabilistic indexing and retrieval, and evaluate it each step along the way in laboratory as well as real world settings.  n/a",SAPHIRE--A CONCEPT-BASED APPROACH TO INFO RETRIEVAL,3474526,R29LM005307,"['artificial intelligence', ' health care facility information system', ' indexing', ' information retrieval', ' information system analysis', ' information systems', ' language']",NLM,OREGON HEALTH AND SCIENCE UNIVERSITY,R29,1992,94952,0.05297308930283798
"SAPHIRE:  A CONCEPT-BASED APPROACH TO INFO RETRIEVAL The major goal of this project is to develop and evaluate innovative approaches to information retrieval (IR) in the biomedical domain. Building on the initial work done by the Principal Investigator (PI) with the SAPHIRE Project and taking advantage of the efforts of the Unified Medical Language Systems (UMLS) Project, we aim to design and test new methods for automated indexing and retrieval.  The underlying thesis of the SAPHIRE approach to IR is movement of information representation from the level of terms to that of concepts.  Terms, such as MeSH entries, are surface string representations of underlying concepts.  A problem with their use in representing concepts is that they cannot account for the different ways a concept may be expressed in medical texts or information system queries.  SAPHIRE is a first step in the direction of concept-based IR, and we plan to investigate several enhancements to this approach.  The major goal will be achieved with six separate but interrelated tasks:  1. Develop methodology for evaluation of IR systems in laboratory and clinical settings.  2. Assess the utility of computational linguistic approaches to concept discovery in text using constrained natural language processing and knowledge base construction.  3. Refine strategies for automated indexing of a wide variety of textual material, including abstracts, full text of articles, textbooks, and hypertext.  4. Explore different user interfaces, aiming to allow optimal retrieval for both novice and expert users.  5. Assess the use of semantic relationships between concepts in indexing and retrieval.  6. Integrate the SAPHIRE approach with other programs, such as the CODEX system and Explorer-2, and scale up to large text collections.  In the course of the project we will create an IR system that will help meet the information needs of busy health care providers.  Such a system should have a diverse variety of content available as well as quality indexing to represent content accurately.  It should also feature retrieval capability that is fast and easy to use.  In this grant, we propose to iteratively build an IR system that utilizes concept-based probabilistic indexing and retrieval, and evaluate it each step along the way in laboratory as well as real world settings.  n/a",SAPHIRE:  A CONCEPT-BASED APPROACH TO INFO RETRIEVAL,3474525,R29LM005307,"['artificial intelligence', ' health care facility information system', ' indexing', ' information retrieval', ' information system analysis', ' information systems', ' language']",NLM,OREGON HEALTH AND SCIENCE UNIVERSITY,R29,1991,91537,0.05297308930283798
"DEVELOPMENT OF A SEMANTIC PARSER FOR MEDICAL TEXT This project seeks to develop a natural language understanding system specifically aimed at extracting relevant clinical facts from medical reports.  The system is based largely on a semantic parsing technique that stresses the use of medical knowledge encoded in four forms.  These forms include: a hierarchy of terms embedded in a general purpose medical data dictionary; a semantic network designed to capture knowledge concerning the relative locations of different anatomic sites; a collection of frames specifying allowable combinations of terms.  These frames also have a hierarchial organization designed to help the parser find an appropriate format for the recognition and storage of a complex medical fact; a transformational grammar attached to the hierarchy of frames which can propose the different ways a medical fact, as indicated by the combined terms in a frame, might be expressed; a causal network developed specifically to allow disambiguation of the many incompletely expressed facts that can be found in a medical report.  Both a lexicon expressing the different words known to the system and a thesaurus expressing all meaningful phrases expected in the reporting domain will also be built.  A system that uses this information to parse medical text will be constructed and evaluated.  The domains tested will be the reports of chest x-rays and admitting history and physical examination for patients with pulmonary and/or cardiac diseases.  The evaluation will determine whether relevant medical facts presented in the reports are captured and stored by the natural language parser in an integrated, general purpose medical data base.  The goal of this project is to further techniques that allow the encoding of medical information captured as free text into a form appropriate for research, quality, assurance, and direct clinical decision support.  n/a",DEVELOPMENT OF A SEMANTIC PARSER FOR MEDICAL TEXT,2237761,R01LM005323,"['abstracting', ' artificial intelligence', ' computer program /software', ' health care facility information system', ' information system analysis', ' respiratory imaging /visualization', ' semantics']",NLM,LDS HOSPITAL,R01,1993,158368,0.06081125718994117
"DEVELOPMENT OF A SEMANTIC PARSER FOR MEDICAL TEXT This project seeks to develop a natural language understanding system specifically aimed at extracting relevant clinical facts from medical reports.  The system is based largely on a semantic parsing technique that stresses the use of medical knowledge encoded in four forms.  These forms include: a hierarchy of terms embedded in a general purpose medical data dictionary; a semantic network designed to capture knowledge concerning the relative locations of different anatomic sites; a collection of frames specifying allowable combinations of terms.  These frames also have a hierarchial organization designed to help the parser find an appropriate format for the recognition and storage of a complex medical fact; a transformational grammar attached to the hierarchy of frames which can propose the different ways a medical fact, as indicated by the combined terms in a frame, might be expressed; a causal network developed specifically to allow disambiguation of the many incompletely expressed facts that can be found in a medical report.  Both a lexicon expressing the different words known to the system and a thesaurus expressing all meaningful phrases expected in the reporting domain will also be built.  A system that uses this information to parse medical text will be constructed and evaluated.  The domains tested will be the reports of chest x-rays and admitting history and physical examination for patients with pulmonary and/or cardiac diseases.  The evaluation will determine whether relevant medical facts presented in the reports are captured and stored by the natural language parser in an integrated, general purpose medical data base.  The goal of this project is to further techniques that allow the encoding of medical information captured as free text into a form appropriate for research, quality, assurance, and direct clinical decision support.  n/a",DEVELOPMENT OF A SEMANTIC PARSER FOR MEDICAL TEXT,3374328,R01LM005323,"['abstracting', ' artificial intelligence', ' health care facility information system', ' information system analysis', ' respiratory imaging /visualization', ' semantics']",NLM,LDS HOSPITAL,R01,1992,148634,0.06081125718994117
"DEVELOPMENT OF A SEMANTIC PARSER FOR MEDICAL TEXT This project seeks to develop a natural language understanding system specifically aimed at extracting relevant clinical facts from medical reports.  The system is based largely on a semantic parsing technique that stresses the use of medical knowledge encoded in four forms.  These forms include: a hierarchy of terms embedded in a general purpose medical data dictionary; a semantic network designed to capture knowledge concerning the relative locations of different anatomic sites; a collection of frames specifying allowable combinations of terms.  These frames also have a hierarchial organization designed to help the parser find an appropriate format for the recognition and storage of a complex medical fact; a transformational grammar attached to the hierarchy of frames which can propose the different ways a medical fact, as indicated by the combined terms in a frame, might be expressed; a causal network developed specifically to allow disambiguation of the many incompletely expressed facts that can be found in a medical report.  Both a lexicon expressing the different words known to the system and a thesaurus expressing all meaningful phrases expected in the reporting domain will also be built.  A system that uses this information to parse medical text will be constructed and evaluated.  The domains tested will be the reports of chest x-rays and admitting history and physical examination for patients with pulmonary and/or cardiac diseases.  The evaluation will determine whether relevant medical facts presented in the reports are captured and stored by the natural language parser in an integrated, general purpose medical data base.  The goal of this project is to further techniques that allow the encoding of medical information captured as free text into a form appropriate for research, quality, assurance, and direct clinical decision support.  n/a",DEVELOPMENT OF A SEMANTIC PARSER FOR MEDICAL TEXT,3374327,R01LM005323,"['abstracting', ' artificial intelligence', ' health care facility information system', ' information system analysis', ' respiratory imaging /visualization', ' semantics']",NLM,LDS HOSPITAL,R01,1991,143397,0.06081125718994117
"LATENT SEMANTIC INDEXING--PATIENT DATA RETRIEVAL Most medical knowledge and patient record data are represented as natural language.  Record-based clinical research in the areas of outcome analysis, epidemiology, and health services research are dependent upon the organization of patient data into analyzable categories.  Thus classifying patient events (diagnoses, procedures, or findings) is critical for the conduct of research based on the patient record.  Any progress in computer assisted medical text classification would directly contribute to the efficient conduct of clinical research deriving from patient data.  This proposal seeks to bring state of the art information retrieval techniques to bear on the problem of computer classification of clinical phrases about patients.  Success in this effort will make possible patient record-based research that includes text descriptions, a practice presently too costly or tedious to conduct widely in most medical centers.  We outline experimental variations on lexicon based word and phrase mapping into canonical form using the CLARIT system from Carnegie Mellon University.  This work will include synonym mapping, phrase recognition, and the assignment of term weights for  information matrix construction.  We have evaluated a modification of the Latent Semantic Indexing (LSI) information retrieval technique to exploit the rich structure of the UMLS Metathesaurus.  We propose refinements on our preliminary work, which constitute testable strategies for incorporating several weighting options, multidimensional structures, and ancillary information resources such as the complete ICD-9-CM.  Because this task is dependent on the computationally demanding singular value decomposition (SVD) to create principal components for statistical mapping, we include a consortium agreement with the University of Minnesota to address algorithmic variations suited to our sparse information matrix structure.  This aspect of our proposal will make the initial solution of SVD practical, removing its present dependence on supercomputers.  However, application of our proposed techniques, once a solution is computed, can be undertaken on personal computers.  Our proposal promises to improve computer-assisted classification of medical text by using the structured knowledge sources of the UMLS and its contributing nosologies in an application of LSI.  This research minimizes dependence on hand built semantic networks, focusing on statistical decomposition of existing classification structures, enriched by lexicon based preprocessing of medical text sources.  These techniques apply equally to classifying patient records and processing natural language inquiries of these databases, thereby broadening the scope and opportunity for research based on clinical records.  n/a",LATENT SEMANTIC INDEXING--PATIENT DATA RETRIEVAL,2237806,R01LM005416,"['automated medical record system', ' computer assisted medical decision making', ' computer program /software', ' indexing', ' information retrieval', ' semantics', ' vocabulary development for information system']",NLM,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",R01,1994,115842,0.02825128010310496
"LATENT SEMANTIC INDEXING IN SUPPORT OF PATIENT DATA RETR Most medical knowledge and patient record data are represented as natural language.  Record-based clinical research in the areas of outcome analysis, epidemiology, and health services research are dependent upon the organization of patient data into analyzable categories.  Thus classifying patient events (diagnoses, procedures, or findings) is critical for the conduct of research based on the patient record.  Any progress in computer assisted medical text classification would directly contribute to the efficient conduct of clinical research deriving from patient data.  This proposal seeks to bring state of the art information retrieval techniques to bear on the problem of computer classification of clinical phrases about patients.  Success in this effort will make possible patient record-based research that includes text descriptions, a practice presently too costly or tedious to conduct widely in most medical centers.  We outline experimental variations on lexicon based word and phrase mapping into canonical form using the CLARIT system from Carnegie Mellon University.  This work will include synonym mapping, phrase recognition, and the assignment of term weights for  information matrix construction.  We have evaluated a modification of the Latent Semantic Indexing (LSI) information retrieval technique to exploit the rich structure of the UMLS Metathesaurus.  We propose refinements on our preliminary work, which constitute testable strategies for incorporating several weighting options, multidimensional structures, and ancillary information resources such as the complete ICD-9-CM.  Because this task is dependent on the computationally demanding singular value decomposition (SVD) to create principal components for statistical mapping, we include a consortium agreement with the University of Minnesota to address algorithmic variations suited to our sparse information matrix structure.  This aspect of our proposal will make the initial solution of SVD practical, removing its present dependence on supercomputers.  However, application of our proposed techniques, once a solution is computed, can be undertaken on personal computers.  Our proposal promises to improve computer-assisted classification of medical text by using the structured knowledge sources of the UMLS and its contributing nosologies in an application of LSI.  This research minimizes dependence on hand built semantic networks, focusing on statistical decomposition of existing classification structures, enriched by lexicon based preprocessing of medical text sources.  These techniques apply equally to classifying patient records and processing natural language inquiries of these databases, thereby broadening the scope and opportunity for research based on clinical records.  n/a",LATENT SEMANTIC INDEXING IN SUPPORT OF PATIENT DATA RETR,3374374,R01LM005416,"['automated medical record system', ' computer assisted medical decision making', ' computer program /software', ' indexing', ' information retrieval', ' semantics', ' vocabulary development for information system']",NLM,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",R01,1993,111387,0.02825128010310496
"LATENT SEMANTIC INDEXING IN SUPPORT OF PATIENT DATA RETR Most medical knowledge and patient record data are represented as natural language.  Record-based clinical research in the areas of outcome analysis, epidemiology, and health services research are dependent upon the organization of patient data into analyzable categories.  Thus classifying patient events (diagnoses, procedures, or findings) is critical for the conduct of research based on the patient record.  Any progress in computer assisted medical text classification would directly contribute to the efficient conduct of clinical research deriving from patient data.  This proposal seeks to bring state of the art information retrieval techniques to bear on the problem of computer classification of clinical phrases about patients.  Success in this effort will make possible patient record-based research that includes text descriptions, a practice presently too costly or tedious to conduct widely in most medical centers.  We outline experimental variations on lexicon based word and phrase mapping into canonical form using the CLARIT system from Carnegie Mellon University.  This work will include synonym mapping, phrase recognition, and the assignment of term weights for  information matrix construction.  We have evaluated a modification of the Latent Semantic Indexing (LSI) information retrieval technique to exploit the rich structure of the UMLS Metathesaurus.  We propose refinements on our preliminary work, which constitute testable strategies for incorporating several weighting options, multidimensional structures, and ancillary information resources such as the complete ICD-9-CM.  Because this task is dependent on the computationally demanding singular value decomposition (SVD) to create principal components for statistical mapping, we include a consortium agreement with the University of Minnesota to address algorithmic variations suited to our sparse information matrix structure.  This aspect of our proposal will make the initial solution of SVD practical, removing its present dependence on supercomputers.  However, application of our proposed techniques, once a solution is computed, can be undertaken on personal computers.  Our proposal promises to improve computer-assisted classification of medical text by using the structured knowledge sources of the UMLS and its contributing nosologies in an application of LSI.  This research minimizes dependence on hand built semantic networks, focusing on statistical decomposition of existing classification structures, enriched by lexicon based preprocessing of medical text sources.  These techniques apply equally to classifying patient records and processing natural language inquiries of these databases, thereby broadening the scope and opportunity for research based on clinical records.  n/a",LATENT SEMANTIC INDEXING IN SUPPORT OF PATIENT DATA RETR,3374373,R01LM005416,"['automated medical record system', ' computer assisted medical decision making', ' indexing', ' information retrieval', ' semantics', ' vocabulary development for information system']",NLM,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",R01,1992,140691,0.02825128010310496
"Text Mining for High-fidelity Curation and Discovery of Gene-drug-phenotype Relationships ﻿    DESCRIPTION (provided by applicant): The rate at which new drugs are being introduced to market is decreasing, with grave implications for human health. Knowledge about the molecular mechanisms relevant to drug response is critical, but is collected in myriad individual experiments. As a result, the published literature contains rich information about how drugs and genes/gene products interact to produce phenotypes at the molecular, cellular and organismal levels, but this textual data requires substantial additional processing. As a result, there are efforts to manually curate the literature, and extract relationships between three key entities: genes/gene products, drugs and phenotypes-with the goal of representing the information in structured, computable formats. Although automated text mining may ultimately replace expert human curators, its best current role is to triage the literature and bring potentially important information to the attention of human curators. Recent advances in computational natural language processing (NLP) generally, and within our laboratory specifically, offer an opportunity to extract relationships between key entities with high accuracy. In particular, we have prototyped methods that take a relatively small set of examples of a relationship of interest (e.g. examples of gene-drug pairs in which the gene product metabolizes the drug) and then and other pairs that share a similar relationship. These methods can be applied to any relationship between our three key entity types. Thus, we propose an ambitious plan to (1) gather large corpora of biomedical text and extend existing lexicons for these entities, (2) build a database of all sentences/paragraphs relating these entities to one another, (3) create methods for accurately extracting semantically precise relationships from all pairs of entity types, and (4) validate these extracted relationships using both available gold standard data experimental sources and expert curator evaluation. In addition to directly supporting curation, our methods and extractions will be made available as general purpose resources for understanding drug action. Project Narrative Published biological text contains an abundance of valuable information about how drugs work at the molecular, cellular and organism levels and how they affect patients. However, it is difﬁcult to gather all relevant information to support discovery of new drugs and optimal use of existing drugs. This proposal outlines a plan to develop methods for extracting drug-related information from huge collections of text, and using it to improve drug discovery and use.",Text Mining for High-fidelity Curation and Discovery of Gene-drug-phenotype Relationships,9521542,R01LM005652,"['Affect', 'Algorithms', 'Attention', 'Basic Science', 'Biological', 'Collection', 'Communities', 'Computing Methodologies', 'Data', 'Data Sources', 'Databases', 'Decision Making', 'Dependence', 'Drug effect disorder', 'Drug usage', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Goals', 'Gold', 'Health', 'Human', 'Individual', 'Investigation', 'Knowledge', 'Laboratories', 'Learning', 'Linguistics', 'Literature', 'Manuals', 'Metabolism', 'Methods', 'Molecular', 'Natural Language Processing', 'Organism', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Proteins', 'Publishing', 'Reporting', 'Research', 'Resources', 'Role', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Text', 'Time', 'Triage', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'computer science', 'drug discovery', 'experience', 'experimental study', 'gene discovery', 'gene product', 'improved', 'interest', 'knowledge base', 'novel therapeutics', 'prototype', 'response', 'text searching', 'tool']",NLM,STANFORD UNIVERSITY,R01,2018,341913,0.12218775996109484
"Text Mining for High-fidelity Curation and Discovery of Gene-drug-phenotype Relationships Project Summary The rate at which new drugs are being introduced to market is decreasing, with grave implications for human health. Knowledge about the molecular mechanisms relevant to drug response is critical, but is collected in myriad individual experiments. As a result, the published literature contains rich information about how drugs and genes/gene products interact to produce phenotypes at the molecular, cellular and organismal levels, but this textual data requires substantial additional processing. As a result, there are efforts to manually curate the literature, and extract relationships between three key entities: genes/gene products, drugs and phenotypes—with the goal of representing the information in structured, computable formats. Although automated text mining may ultimately replace expert human curators, its best current role is to triage the literature and bring potentially important information to the attention of human curators. Recent advances in computational natural language processing (NLP) generally, and within our laboratory speciﬁcally, offer an opportunity to extract relationships between key entities with high accuracy. In particular, we have prototyped methods that take a relatively small set of examples of a relationship of interest (e.g. examples of gene-drug pairs in which the gene product metabolizes the drug) and then ﬁnd other pairs that share a similar relationship. These methods can be applied to any relationship between our three key entity types. Thus, we propose an ambitious plan to (1) gather large corpora of biomedical text and extend existing lexicons for these entities, (2) build a database of all sentences/paragraphs relating these entities to one another, (3) create methods for accurately extracting semantically precise relationships from all pairs of entity types, and (4) validate these extracted relationships using both available gold standard data experimental sources and expert curator evaluation. In addition to directly supporting curation, our methods and extractions will be made available as general purpose resources for understanding drug action. Project Narrative Published biological text contains an abundance of valuable information about how drugs work at the molecular, cellular and organism levels and how they affect patients. However, it is difﬁcult to gather all relevant information to support discovery of new drugs and optimal use of existing drugs. This proposal outlines a plan to develop methods for extracting drug-related information from huge collections of text, and using it to improve drug discovery and use.",Text Mining for High-fidelity Curation and Discovery of Gene-drug-phenotype Relationships,9755730,R01LM005652,"['Affect', 'Algorithms', 'Attention', 'Basic Science', 'Biological', 'Collection', 'Communities', 'Computing Methodologies', 'Data', 'Data Sources', 'Databases', 'Decision Making', 'Dependence', 'Drug effect disorder', 'Drug usage', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Goals', 'Gold', 'Health', 'Human', 'Individual', 'Investigation', 'Knowledge', 'Laboratories', 'Learning', 'Linguistics', 'Literature', 'Manuals', 'Metabolism', 'Methods', 'Molecular', 'Natural Language Processing', 'Organism', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Proteins', 'Publishing', 'Reporting', 'Research', 'Resources', 'Role', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Text', 'Time', 'Triage', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'computer science', 'drug discovery', 'experience', 'experimental study', 'gene discovery', 'gene product', 'improved', 'interest', 'knowledge base', 'novel therapeutics', 'prototype', 'response', 'text searching', 'tool']",NLM,STANFORD UNIVERSITY,R01,2018,353976,0.12218775996109484
"Text Mining for High-fidelity Curation and Discovery of Gene-drug-phenotype Relationships ﻿    DESCRIPTION (provided by applicant): The rate at which new drugs are being introduced to market is decreasing, with grave implications for human health. Knowledge about the molecular mechanisms relevant to drug response is critical, but is collected in myriad individual experiments. As a result, the published literature contains rich information about how drugs and genes/gene products interact to produce phenotypes at the molecular, cellular and organismal levels, but this textual data requires substantial additional processing. As a result, there are efforts to manually curate the literature, and extract relationships between three key entities: genes/gene products, drugs and phenotypes-with the goal of representing the information in structured, computable formats. Although automated text mining may ultimately replace expert human curators, its best current role is to triage the literature and bring potentially important information to the attention of human curators. Recent advances in computational natural language processing (NLP) generally, and within our laboratory specifically, offer an opportunity to extract relationships between key entities with high accuracy. In particular, we have prototyped methods that take a relatively small set of examples of a relationship of interest (e.g. examples of gene-drug pairs in which the gene product metabolizes the drug) and then and other pairs that share a similar relationship. These methods can be applied to any relationship between our three key entity types. Thus, we propose an ambitious plan to (1) gather large corpora of biomedical text and extend existing lexicons for these entities, (2) build a database of all sentences/paragraphs relating these entities to one another, (3) create methods for accurately extracting semantically precise relationships from all pairs of entity types, and (4) validate these extracted relationships using both available gold standard data experimental sources and expert curator evaluation. In addition to directly supporting curation, our methods and extractions will be made available as general purpose resources for understanding drug action. Project Narrative Published biological text contains an abundance of valuable information about how drugs work at the molecular, cellular and organism levels and how they affect patients. However, it is difﬁcult to gather all relevant information to support discovery of new drugs and optimal use of existing drugs. This proposal outlines a plan to develop methods for extracting drug-related information from huge collections of text, and using it to improve drug discovery and use.",Text Mining for High-fidelity Curation and Discovery of Gene-drug-phenotype Relationships,9306199,R01LM005652,"['Affect', 'Algorithms', 'Attention', 'Basic Science', 'Biological', 'Collection', 'Communities', 'Computing Methodologies', 'Data', 'Data Sources', 'Databases', 'Decision Making', 'Dependency', 'Drug effect disorder', 'Drug usage', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Goals', 'Gold', 'Health', 'Human', 'Individual', 'Investigation', 'Knowledge', 'Laboratories', 'Learning', 'Linguistics', 'Literature', 'Manuals', 'Metabolism', 'Methods', 'Molecular', 'Natural Language Processing', 'Organism', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Proteins', 'Publishing', 'Reporting', 'Research', 'Resources', 'Role', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Text', 'Time', 'Triage', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'computer science', 'drug discovery', 'experience', 'experimental study', 'gene discovery', 'gene product', 'improved', 'interest', 'knowledge base', 'novel therapeutics', 'prototype', 'response', 'text searching', 'tool']",NLM,STANFORD UNIVERSITY,R01,2017,331290,0.12218775996109484
"Text Mining for High-fidelity Curation and Discovery of Gene-drug-phenotype Relationships ﻿    DESCRIPTION (provided by applicant): The rate at which new drugs are being introduced to market is decreasing, with grave implications for human health. Knowledge about the molecular mechanisms relevant to drug response is critical, but is collected in myriad individual experiments. As a result, the published literature contains rich information about how drugs and genes/gene products interact to produce phenotypes at the molecular, cellular and organismal levels, but this textual data requires substantial additional processing. As a result, there are efforts to manually curate the literature, and extract relationships between three key entities: genes/gene products, drugs and phenotypes-with the goal of representing the information in structured, computable formats. Although automated text mining may ultimately replace expert human curators, its best current role is to triage the literature and bring potentially important information to the attention of human curators. Recent advances in computational natural language processing (NLP) generally, and within our laboratory specifically, offer an opportunity to extract relationships between key entities with high accuracy. In particular, we have prototyped methods that take a relatively small set of examples of a relationship of interest (e.g. examples of gene-drug pairs in which the gene product metabolizes the drug) and then and other pairs that share a similar relationship. These methods can be applied to any relationship between our three key entity types. Thus, we propose an ambitious plan to (1) gather large corpora of biomedical text and extend existing lexicons for these entities, (2) build a database of all sentences/paragraphs relating these entities to one another, (3) create methods for accurately extracting semantically precise relationships from all pairs of entity types, and (4) validate these extracted relationships using both available gold standard data experimental sources and expert curator evaluation. In addition to directly supporting curation, our methods and extractions will be made available as general purpose resources for understanding drug action. Project Narrative Published biological text contains an abundance of valuable information about how drugs work at the molecular, cellular and organism levels and how they affect patients. However, it is difﬁcult to gather all relevant information to support discovery of new drugs and optimal use of existing drugs. This proposal outlines a plan to develop methods for extracting drug-related information from huge collections of text, and using it to improve drug discovery and use.",Text Mining for High-fidelity Curation and Discovery of Gene-drug-phenotype Relationships,9119636,R01LM005652,"['Affect', 'Algorithms', 'Attention', 'Basic Science', 'Biological', 'Collection', 'Communities', 'Computing Methodologies', 'Data', 'Data Sources', 'Databases', 'Decision Making', 'Dependency', 'Drug effect disorder', 'Drug usage', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Goals', 'Gold', 'Health', 'Human', 'Individual', 'Investigation', 'Knowledge', 'Laboratories', 'Learning', 'Linguistics', 'Literature', 'Marketing', 'Metabolism', 'Methods', 'Molecular', 'Natural Language Processing', 'Organism', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Process', 'Proteins', 'Publishing', 'Reporting', 'Research', 'Resources', 'Role', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Text', 'Time', 'Triage', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'computer science', 'drug discovery', 'experience', 'gene discovery', 'gene product', 'improved', 'interest', 'knowledge base', 'novel therapeutics', 'research study', 'response', 'text searching', 'tool']",NLM,STANFORD UNIVERSITY,R01,2016,340748,0.12218775996109484
"Text Mining for High-fidelity Curation and Discovery of Gene-drug-phenotype Relationships ﻿    DESCRIPTION (provided by applicant): The rate at which new drugs are being introduced to market is decreasing, with grave implications for human health. Knowledge about the molecular mechanisms relevant to drug response is critical, but is collected in myriad individual experiments. As a result, the published literature contains rich information about how drugs and genes/gene products interact to produce phenotypes at the molecular, cellular and organismal levels, but this textual data requires substantial additional processing. As a result, there are efforts to manually curate the literature, and extract relationships between three key entities: genes/gene products, drugs and phenotypes-with the goal of representing the information in structured, computable formats. Although automated text mining may ultimately replace expert human curators, its best current role is to triage the literature and bring potentially important information to the attention of human curators. Recent advances in computational natural language processing (NLP) generally, and within our laboratory specifically, offer an opportunity to extract relationships between key entities with high accuracy. In particular, we have prototyped methods that take a relatively small set of examples of a relationship of interest (e.g. examples of gene-drug pairs in which the gene product metabolizes the drug) and then and other pairs that share a similar relationship. These methods can be applied to any relationship between our three key entity types. Thus, we propose an ambitious plan to (1) gather large corpora of biomedical text and extend existing lexicons for these entities, (2) build a database of all sentences/paragraphs relating these entities to one another, (3) create methods for accurately extracting semantically precise relationships from all pairs of entity types, and (4) validate these extracted relationships using both available gold standard data experimental sources and expert curator evaluation. In addition to directly supporting curation, our methods and extractions will be made available as general purpose resources for understanding drug action.             Project Narrative Published biological text contains an abundance of valuable information about how drugs work at the molecular, cellular and organism levels and how they affect patients. However, it is difﬁcult to gather all relevant information to support discovery of new drugs and optimal use of existing drugs. This proposal outlines a plan to develop methods for extracting drug-related information from huge collections of text, and using it to improve drug discovery and use.",Text Mining for High-fidelity Curation and Discovery of Gene-drug-phenotype Relationships,8963236,R01LM005652,"['Affect', 'Algorithms', 'Attention', 'Basic Science', 'Biological', 'Collection', 'Communities', 'Computing Methodologies', 'Data', 'Data Sources', 'Databases', 'Decision Making', 'Dependency', 'Drug effect disorder', 'Drug usage', 'Evaluation', 'Genes', 'Genetic', 'Genetic Variation', 'Goals', 'Gold', 'Health', 'Human', 'Individual', 'Investigation', 'Knowledge', 'Laboratories', 'Learning', 'Linguistics', 'Literature', 'Marketing', 'Metabolism', 'Methods', 'Molecular', 'Natural Language Processing', 'Organism', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Process', 'Proteins', 'Publishing', 'Reporting', 'Research', 'Resources', 'Role', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Text', 'Time', 'Triage', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'computer science', 'drug discovery', 'experience', 'gene discovery', 'improved', 'interest', 'knowledge base', 'novel therapeutics', 'research study', 'response', 'text searching', 'tool']",NLM,STANFORD UNIVERSITY,R01,2015,344659,0.12218775996109484
"VOCABULARY AND TEXT DATA EXTRACTION FROM THE EMR The electronic medical record (EMR) holds great allure to both the               medical informatics and health services research communities. In this            project, we propose to enhance the capability of electronic medical              record (EMR) systems by creating and evaluating tools to extract                 clinical vocabularies as well as patient data from narrative text                reports. We will apply advanced natural language processing tools from           the CLARIT system to both of the above problems. We contend that fast            and robust automated text processing methods are the only way that the           problems of vocabulary construction and narrative text extraction can            be solved.                                                                                                                                                        We will address the clinical vocabulary problem by utilizing the                 thesaurus extraction techniques already present in the CLARIT system.            Using several gigabytes of narrative text, including discharge                   summaries, progress notes, radiology reports, and other clinical text,           we plan to:                                                                      l. Identify empirically the terminology used in medicine.                        2. Compare the coverage of that terminology in several existing large            medical vocabularies: UMLS, SNOMED, and the Medical Entities                     Dictionary.                                                                      3. Discern the semantic characteristics of that terminology to allow             other structured vocabularies a richer substrate of terms as well as             providing us the opportunity to implement a clinical vocabulary schema           based on the methods of the MedSORT-II Project.                                  4. Evaluate how well our tools assist the vocabulary building efforts            of ourselves and others.                                                                                                                                          The narrative extraction problem will be approached differently than             in the past, building on the efforts of previous investigators who               have tackled this problem before but changing the perspective by                 focusing on the development of tools specific to researchers and                 others with a need to extract data from narrative text. This approach            will be applied in two domains:                                                  l.Consortium-based research in the use of esophogastroduodenoscopy               (EGD).                                                                           2.Practice guidelines implementation in blood product transfusion.                n/a",VOCABULARY AND TEXT DATA EXTRACTION FROM THE EMR,2332614,U01LM005879,"['abstracting', ' automated medical record system', ' blood transfusion', ' cooperative study', ' endoscopy', ' vocabulary development for information system']",NLM,OREGON HEALTH AND SCIENCE UNIVERSITY,U01,1997,294900,0.12517174811806758
"VOCABULARY AND TEXT DATA EXTRACTION FROM THE EMR The electronic medical record (EMR) holds great allure to both the  medical informatics and health services research communities. In this  project, we propose to enhance the capability of electronic medical  record (EMR) systems by creating and evaluating tools to extract  clinical vocabularies as well as patient data from narrative text  reports. We will apply advanced natural language processing tools from  the CLARIT system to both of the above problems. We contend that fast  and robust automated text processing methods are the only way that the  problems of vocabulary construction and narrative text extraction can  be solved.    We will address the clinical vocabulary problem by utilizing the  thesaurus extraction techniques already present in the CLARIT system.  Using several gigabytes of narrative text, including discharge  summaries, progress notes, radiology reports, and other clinical text,  we plan to:  l. Identify empirically the terminology used in medicine.  2. Compare the coverage of that terminology in several existing large  medical vocabularies: UMLS, SNOMED, and the Medical Entities  Dictionary.  3. Discern the semantic characteristics of that terminology to allow  other structured vocabularies a richer substrate of terms as well as  providing us the opportunity to implement a clinical vocabulary schema  based on the methods of the MedSORT-II Project.  4. Evaluate how well our tools assist the vocabulary building efforts  of ourselves and others.    The narrative extraction problem will be approached differently than  in the past, building on the efforts of previous investigators who  have tackled this problem before but changing the perspective by  focusing on the development of tools specific to researchers and  others with a need to extract data from narrative text. This approach  will be applied in two domains:  l.Consortium-based research in the use of esophogastroduodenoscopy  (EGD).  2.Practice guidelines implementation in blood product transfusion.  n/a",VOCABULARY AND TEXT DATA EXTRACTION FROM THE EMR,2238284,U01LM005879,"['abstracting', ' automated medical record system', ' blood transfusion', ' cooperative study', ' endoscopy', ' vocabulary development for information system']",NLM,OREGON HEALTH AND SCIENCE UNIVERSITY,U01,1995,375570,0.12517174811806758
"UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING DESCRIPTION (Taken from application abstract):  The long-term aim of this        project is to use natural language methods in order to enhance the               functionality of the electronic medical record, which is a source of             abundant clinical data.  However, the data is mostly in textual form and         therefore unusable for automated clinical applications, such as decision         support, research, quality assurance, and outcomes assessment.  By using a       natural language processor to map the clinical information in the reports        into structured codified clinical data, the data will be made readily            accessible so that it could be utilized by subsequent automated clinical         applications.  We have already shown that it is possible to build an             effective text processor that accurately codifies textual reports within the     specialized domain of radiology.  In this project we intend to build upon        our successful experience and will extend the processor to another limited       domain that is different from radiology and to a broad domain in order to        study the feasibility of transferring the processor to all of medicine.                                                                                           More specifically, we will broaden the processor so that it codifies             clinical information in the physical examination section of the discharge        summary and then to all of the discharge summary, where we will focus on         coding diagnoses.  The emphasis of our work will not only be concerned with      extending the language processor but will also focus on scalability,             evaluation of the performance, the effort, and the portability aspects.  In      addition, because discharge summaries are so complex and comprehensive, we       will have to extend the formal representational model of the clinical            information and also develop new natural language processing techniques and      new vocabulary development tools.  This work will continue to be performed       within an operational clinical setting.                                           n/a",UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING,2897383,R01LM006274,"['abstracting', ' automated medical record system', ' computer system design /evaluation', ' human data', ' information retrieval', ' method development', ' vocabulary development for information system']",NLM,QUEENS COLLEGE,R01,1999,209130,0.03124851096901566
"UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING DESCRIPTION (Taken from application abstract):  The long-term aim of this        project is to use natural language methods in order to enhance the               functionality of the electronic medical record, which is a source of             abundant clinical data.  However, the data is mostly in textual form and         therefore unusable for automated clinical applications, such as decision         support, research, quality assurance, and outcomes assessment.  By using a       natural language processor to map the clinical information in the reports        into structured codified clinical data, the data will be made readily            accessible so that it could be utilized by subsequent automated clinical         applications.  We have already shown that it is possible to build an             effective text processor that accurately codifies textual reports within the     specialized domain of radiology.  In this project we intend to build upon        our successful experience and will extend the processor to another limited       domain that is different from radiology and to a broad domain in order to        study the feasibility of transferring the processor to all of medicine.                                                                                           More specifically, we will broaden the processor so that it codifies             clinical information in the physical examination section of the discharge        summary and then to all of the discharge summary, where we will focus on         coding diagnoses.  The emphasis of our work will not only be concerned with      extending the language processor but will also focus on scalability,             evaluation of the performance, the effort, and the portability aspects.  In      addition, because discharge summaries are so complex and comprehensive, we       will have to extend the formal representational model of the clinical            information and also develop new natural language processing techniques and      new vocabulary development tools.  This work will continue to be performed       within an operational clinical setting.                                           n/a",UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING,2735428,R01LM006274,"['abstracting', ' automated medical record system', ' computer system design /evaluation', ' human data', ' information retrieval', ' method development', ' vocabulary development for information system']",NLM,QUEENS COLLEGE,R01,1998,204002,0.03124851096901566
"UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING DESCRIPTION (Taken from application abstract):  The long-term aim of this        project is to use natural language methods in order to enhance the               functionality of the electronic medical record, which is a source of             abundant clinical data.  However, the data is mostly in textual form and         therefore unusable for automated clinical applications, such as decision         support, research, quality assurance, and outcomes assessment.  By using a       natural language processor to map the clinical information in the reports        into structured codified clinical data, the data will be made readily            accessible so that it could be utilized by subsequent automated clinical         applications.  We have already shown that it is possible to build an             effective text processor that accurately codifies textual reports within the     specialized domain of radiology.  In this project we intend to build upon        our successful experience and will extend the processor to another limited       domain that is different from radiology and to a broad domain in order to        study the feasibility of transferring the processor to all of medicine.                                                                                           More specifically, we will broaden the processor so that it codifies             clinical information in the physical examination section of the discharge        summary and then to all of the discharge summary, where we will focus on         coding diagnoses.  The emphasis of our work will not only be concerned with      extending the language processor but will also focus on scalability,             evaluation of the performance, the effort, and the portability aspects.  In      addition, because discharge summaries are so complex and comprehensive, we       will have to extend the formal representational model of the clinical            information and also develop new natural language processing techniques and      new vocabulary development tools.  This work will continue to be performed       within an operational clinical setting.                                           n/a",UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING,2032409,R01LM006274,"['abstracting', ' automated medical record system', ' computer system design /evaluation', ' human data', ' information retrieval', ' method development', ' vocabulary development for information system']",NLM,QUEENS COLLEGE,R01,1997,218581,0.03124851096901566
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,7069599,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2006,398762,0.09783383665105223
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6896406,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2005,403171,0.09783383665105223
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6774132,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2004,411436,0.09783383665105223
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6744998,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2003,191306,0.08206570158673032
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6363593,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2001,306158,0.08206570158673032
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6165092,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2000,297119,0.08206570158673032
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,2744854,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,1999,343110,0.08206570158673032
"MEDLINK DESCRIPTION:  Access to comprehensive medical information is literally           a matter of life and death for health professionals.  The applicants             propose to develop a state-of-the-art medical information system based           on natural language processing (NLP) which provides innovative access            first to the literature of complementary medicine and then to the                traditional medical literature.  The goal of Phase I is dual:  to                develop a finely tuned, immediately usable information system for                alternative medicine, and to determine on an experimental level what             linguistic elements within the medical subject domain are critical to            optimizing retrieval.  For this purpose, an extensive analysis of                medical sublanguage and the text structure of medical documents will be          undertaken, and the results will be applied to each module of the system         in order to optimize it for medical domain.  The Phase I system will be          used by the medical community both for access to hard-to-find                    information, and to begin to assess the usefulness of complementary              medicine techniques in treating chronic problems.  Phase II will add             traditional medical literature to the system to provide a fully                  integrated solution for rich, precise access to medical information.                                                                                              PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                    n/a",MEDLINK,2867891,R43LM006671,"['alternative medicine', ' artificial intelligence', ' computer system design /evaluation', ' information retrieval', ' information systems', ' language']",NLM,"TEXTWISE, LLC",R43,1998,50000,0.023880521852597394
"MEDLINK DESCRIPTION:  Access to comprehensive medical information is literally           a matter of life and death for health professionals.  The applicants             propose to develop a state-of-the-art medical information system based           on natural language processing (NLP) which provides innovative access            first to the literature of complementary medicine and then to the                traditional medical literature.  The goal of Phase I is dual:  to                develop a finely tuned, immediately usable information system for                alternative medicine, and to determine on an experimental level what             linguistic elements within the medical subject domain are critical to            optimizing retrieval.  For this purpose, an extensive analysis of                medical sublanguage and the text structure of medical documents will be          undertaken, and the results will be applied to each module of the system         in order to optimize it for medical domain.  The Phase I system will be          used by the medical community both for access to hard-to-find                    information, and to begin to assess the usefulness of complementary              medicine techniques in treating chronic problems.  Phase II will add             traditional medical literature to the system to provide a fully                  integrated solution for rich, precise access to medical information.                                                                                              PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                    n/a",MEDLINK,2647486,R43LM006671,"['alternative medicine', ' artificial intelligence', ' computer system design /evaluation', ' information retrieval', ' information systems', ' language']",NLM,"TEXTWISE, LLC",R43,1998,98672,0.023880521852597394
"Accelerating Curation of GWAS Catalog by Automatic Text Mining     DESCRIPTION (provided by applicant): A genome-wide association study (GWAS) is an approach to detecting genetic variations associated with particular diseases or traits by scanning markers across the genomes of a large-scale sample of subjects in a high-throughput manner. In less than a decade, GWAS studies have been successfully producing discovery and replication of many new disease loci. Discovered genetic associations have led to development of better strategies to diagnose, treat and prevent diseases. The number of GWAS is growing rapidly. There is a need for a database that allows researchers to easily query and search for previous results. A well-curated database also provides a resource for overview and summarization investigations of associated genetic sites and may help suggest pleiotropic genes. Such a database has been created and maintained by the National Human Genome Research Institute (NHGRI), called ""A Catalog of Published Genome-Wide Association Studies"" (Catalog of GWAS). The catalog has led to interesting characterization of previous results in GWAS and NHGRI has continued to update and curate the catalog regularly. However, this is performed by manually extracting information from published GWAS articles. As a result, the coverage is low compared to the volume of all GWAS publications and would be impossible to catch up the pace of new publications. The goal of this project is to develop a new tool to automatically extract the information from research articles for the curation of the catalog of GWAS. Our proposal is to use the curated data currently available from NHGRI as the training examples and apply novel machine-learning algorithms to train an information extractor to allow accurate automatic extraction. Given our recent success in applying machine learning to biological text mining, we are confident that this will lead to a useful tool to improve the productivity of curators and solve the coverage problem. Our first specific aim is to develop an accurate information extractor. Our second specific aim is to develop an easy-to-use curation tool for curators to efficiently check and correct errors from automatic information extraction so that their curation productivity can be improved by 18 folds. Then we will adapt the tool to extraction and curation of research papers reporting association studies using data from next generation sequencing. Currently, study design and the reporting of GWAS results using NGS data are not standardized. These results have not been considered to be included in the catalog yet. However, we expect that the limitations will be overcome and the methodology will converge soon. We will closely monitor the progress and adapt the tool to allow for inclusion of the NGS data. Finally, we will distribute the software to the public domain so that volunteers or interested parties can create their own catalog locally. It is our goal to share the developed software with the research community to advance the field. The new algorithms developed in this project and the entire development cycle, from design to deployment, will also contribute to the state-of-the- arts of biological text mining.         PUBLIC HEALTH RELEVANCE: The goal of this project is to develop a new tool to automatically extract the information from research articles for the curation of the catalog of GWAS. The catalog contains information about SNP-trait/disease associations that are extracted from published genome-wide association studies. A well-curated catalog of GWAS allows researchers to easily query and search for previous results and provides a useful resource for overview and summarization investigations of associated genetic sites and may help suggest pleiotropic genes.            ",Accelerating Curation of GWAS Catalog by Automatic Text Mining,9052542,U01HG006894,"['Algorithms', 'Area', 'Biological', 'Cataloging', 'Catalogs', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Ensure', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Goals', 'Human Genetics', 'Investigation', 'Lead', 'Link', 'Machine Learning', 'Manuals', 'Methodology', 'Monitor', 'National Human Genome Research Institute', 'Paper', 'Productivity', 'Public Domains', 'Publications', 'Publishing', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Sampling', 'Scanning', 'Site', 'Standardization', 'Surveys', 'Technology', 'Text', 'Training', 'Update', 'Workload', 'base', 'design', 'genetic association', 'genome wide association study', 'improved', 'interest', 'next generation sequencing', 'novel', 'prevent', 'public health relevance', 'software development', 'success', 'text searching', 'tool', 'trait', 'user-friendly', 'volunteer']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U01,2015,130994,0.08810414141735179
"Accelerating Curation of GWAS Catalog by Automatic Text Mining     DESCRIPTION (provided by applicant): A genome-wide association study (GWAS) is an approach to detecting genetic variations associated with particular diseases or traits by scanning markers across the genomes of a large-scale sample of subjects in a high-throughput manner. In less than a decade, GWAS studies have been successfully producing discovery and replication of many new disease loci. Discovered genetic associations have led to development of better strategies to diagnose, treat and prevent diseases. The number of GWAS is growing rapidly. There is a need for a database that allows researchers to easily query and search for previous results. A well-curated database also provides a resource for overview and summarization investigations of associated genetic sites and may help suggest pleiotropic genes. Such a database has been created and maintained by the National Human Genome Research Institute (NHGRI), called ""A Catalog of Published Genome-Wide Association Studies"" (Catalog of GWAS). The catalog has led to interesting characterization of previous results in GWAS and NHGRI has continued to update and curate the catalog regularly. However, this is performed by manually extracting information from published GWAS articles. As a result, the coverage is low compared to the volume of all GWAS publications and would be impossible to catch up the pace of new publications. The goal of this project is to develop a new tool to automatically extract the information from research articles for the curation of the catalog of GWAS. Our proposal is to use the curated data currently available from NHGRI as the training examples and apply novel machine-learning algorithms to train an information extractor to allow accurate automatic extraction. Given our recent success in applying machine learning to biological text mining, we are confident that this will lead to a useful tool to improve the productivity of curators and solve the coverage problem. Our first specific aim is to develop an accurate information extractor. Our second specific aim is to develop an easy-to-use curation tool for curators to efficiently check and correct errors from automatic information extraction so that their curation productivity can be improved by 18 folds. Then we will adapt the tool to extraction and curation of research papers reporting association studies using data from next generation sequencing. Currently, study design and the reporting of GWAS results using NGS data are not standardized. These results have not been considered to be included in the catalog yet. However, we expect that the limitations will be overcome and the methodology will converge soon. We will closely monitor the progress and adapt the tool to allow for inclusion of the NGS data. Finally, we will distribute the software to the public domain so that volunteers or interested parties can create their own catalog locally. It is our goal to share the developed software with the research community to advance the field. The new algorithms developed in this project and the entire development cycle, from design to deployment, will also contribute to the state-of-the- arts of biological text mining.         PUBLIC HEALTH RELEVANCE: The goal of this project is to develop a new tool to automatically extract the information from research articles for the curation of the catalog of GWAS. The catalog contains information about SNP-trait/disease associations that are extracted from published genome-wide association studies. A well-curated catalog of GWAS allows researchers to easily query and search for previous results and provides a useful resource for overview and summarization investigations of associated genetic sites and may help suggest pleiotropic genes.            ",Accelerating Curation of GWAS Catalog by Automatic Text Mining,8686917,U01HG006894,"['Algorithms', 'Area', 'Biological', 'Cataloging', 'Catalogs', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Association', 'Ensure', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Goals', 'Human Genetics', 'Investigation', 'Lead', 'Link', 'Machine Learning', 'Manuals', 'Methodology', 'Monitor', 'National Human Genome Research Institute', 'Paper', 'Productivity', 'Public Domains', 'Publications', 'Publishing', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Sampling', 'Scanning', 'Site', 'Standardization', 'Surveys', 'Technology', 'Text', 'Training', 'Update', 'Workload', 'base', 'design', 'genetic association', 'genome wide association study', 'improved', 'interest', 'next generation sequencing', 'novel', 'prevent', 'public health relevance', 'software development', 'success', 'text searching', 'tool', 'trait', 'user-friendly', 'volunteer']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U01,2014,277488,0.08810414141735179
"Accelerating Curation of GWAS Catalog by Automatic Text Mining     DESCRIPTION (provided by applicant): A genome-wide association study (GWAS) is an approach to detecting genetic variations associated with particular diseases or traits by scanning markers across the genomes of a large-scale sample of subjects in a high-throughput manner. In less than a decade, GWAS studies have been successfully producing discovery and replication of many new disease loci. Discovered genetic associations have led to development of better strategies to diagnose, treat and prevent diseases. The number of GWAS is growing rapidly. There is a need for a database that allows researchers to easily query and search for previous results. A well-curated database also provides a resource for overview and summarization investigations of associated genetic sites and may help suggest pleiotropic genes. Such a database has been created and maintained by the National Human Genome Research Institute (NHGRI), called ""A Catalog of Published Genome-Wide Association Studies"" (Catalog of GWAS). The catalog has led to interesting characterization of previous results in GWAS and NHGRI has continued to update and curate the catalog regularly. However, this is performed by manually extracting information from published GWAS articles. As a result, the coverage is low compared to the volume of all GWAS publications and would be impossible to catch up the pace of new publications. The goal of this project is to develop a new tool to automatically extract the information from research articles for the curation of the catalog of GWAS. Our proposal is to use the curated data currently available from NHGRI as the training examples and apply novel machine-learning algorithms to train an information extractor to allow accurate automatic extraction. Given our recent success in applying machine learning to biological text mining, we are confident that this will lead to a useful tool to improve the productivity of curators and solve the coverage problem. Our first specific aim is to develop an accurate information extractor. Our second specific aim is to develop an easy-to-use curation tool for curators to efficiently check and correct errors from automatic information extraction so that their curation productivity can be improved by 18 folds. Then we will adapt the tool to extraction and curation of research papers reporting association studies using data from next generation sequencing. Currently, study design and the reporting of GWAS results using NGS data are not standardized. These results have not been considered to be included in the catalog yet. However, we expect that the limitations will be overcome and the methodology will converge soon. We will closely monitor the progress and adapt the tool to allow for inclusion of the NGS data. Finally, we will distribute the software to the public domain so that volunteers or interested parties can create their own catalog locally. It is our goal to share the developed software with the research community to advance the field. The new algorithms developed in this project and the entire development cycle, from design to deployment, will also contribute to the state-of-the- arts of biological text mining.         PUBLIC HEALTH RELEVANCE: The goal of this project is to develop a new tool to automatically extract the information from research articles for the curation of the catalog of GWAS. The catalog contains information about SNP-trait/disease associations that are extracted from published genome-wide association studies. A well-curated catalog of GWAS allows researchers to easily query and search for previous results and provides a useful resource for overview and summarization investigations of associated genetic sites and may help suggest pleiotropic genes.            ",Accelerating Curation of GWAS Catalog by Automatic Text Mining,8549925,U01HG006894,"['Algorithms', 'Area', 'Biological', 'Cataloging', 'Catalogs', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Association', 'Ensure', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Goals', 'Human Genetics', 'Investigation', 'Lead', 'Link', 'Machine Learning', 'Manuals', 'Methodology', 'Monitor', 'National Human Genome Research Institute', 'Paper', 'Productivity', 'Public Domains', 'Publications', 'Publishing', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Sampling', 'Scanning', 'Site', 'Standardization', 'Surveys', 'Technology', 'Text', 'Training', 'Update', 'Workload', 'base', 'design', 'genetic association', 'genome wide association study', 'improved', 'interest', 'next generation sequencing', 'novel', 'prevent', 'public health relevance', 'software development', 'success', 'text searching', 'tool', 'trait', 'user-friendly', 'volunteer']",NHGRI,UNIVERSITY OF SOUTHERN CALIFORNIA,U01,2013,54233,0.08810414141735179
"Accelerating Curation of GWAS Catalog by Automatic Text Mining     DESCRIPTION (provided by applicant): A genome-wide association study (GWAS) is an approach to detecting genetic variations associated with particular diseases or traits by scanning markers across the genomes of a large-scale sample of subjects in a high-throughput manner. In less than a decade, GWAS studies have been successfully producing discovery and replication of many new disease loci. Discovered genetic associations have led to development of better strategies to diagnose, treat and prevent diseases. The number of GWAS is growing rapidly. There is a need for a database that allows researchers to easily query and search for previous results. A well-curated database also provides a resource for overview and summarization investigations of associated genetic sites and may help suggest pleiotropic genes. Such a database has been created and maintained by the National Human Genome Research Institute (NHGRI), called ""A Catalog of Published Genome-Wide Association Studies"" (Catalog of GWAS). The catalog has led to interesting characterization of previous results in GWAS and NHGRI has continued to update and curate the catalog regularly. However, this is performed by manually extracting information from published GWAS articles. As a result, the coverage is low compared to the volume of all GWAS publications and would be impossible to catch up the pace of new publications. The goal of this project is to develop a new tool to automatically extract the information from research articles for the curation of the catalog of GWAS. Our proposal is to use the curated data currently available from NHGRI as the training examples and apply novel machine-learning algorithms to train an information extractor to allow accurate automatic extraction. Given our recent success in applying machine learning to biological text mining, we are confident that this will lead to a useful tool to improve the productivity of curators and solve the coverage problem. Our first specific aim is to develop an accurate information extractor. Our second specific aim is to develop an easy-to-use curation tool for curators to efficiently check and correct errors from automatic information extraction so that their curation productivity can be improved by 18 folds. Then we will adapt the tool to extraction and curation of research papers reporting association studies using data from next generation sequencing. Currently, study design and the reporting of GWAS results using NGS data are not standardized. These results have not been considered to be included in the catalog yet. However, we expect that the limitations will be overcome and the methodology will converge soon. We will closely monitor the progress and adapt the tool to allow for inclusion of the NGS data. Finally, we will distribute the software to the public domain so that volunteers or interested parties can create their own catalog locally. It is our goal to share the developed software with the research community to advance the field. The new algorithms developed in this project and the entire development cycle, from design to deployment, will also contribute to the state-of-the- arts of biological text mining.        PUBLIC HEALTH RELEVANCE: The goal of this project is to develop a new tool to automatically extract the information from research articles for the curation of the catalog of GWAS. The catalog contains information about SNP-trait/disease associations that are extracted from published genome-wide association studies. A well-curated catalog of GWAS allows researchers to easily query and search for previous results and provides a useful resource for overview and summarization investigations of associated genetic sites and may help suggest pleiotropic genes.              The goal of this project is to develop a new tool to automatically extract the information from research articles for the curation of the catalog of GWAS. The catalog contains information about SNP-trait/disease associations that are extracted from published genome-wide association studies. A well-curated catalog of GWAS allows researchers to easily query and search for previous results and provides a useful resource for overview and summarization investigations of associated genetic sites and may help suggest pleiotropic genes.            ",Accelerating Curation of GWAS Catalog by Automatic Text Mining,8348769,U01HG006894,"['Algorithms', 'Area', 'Biological', 'Cataloging', 'Catalogs', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Association', 'Ensure', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Goals', 'Human Genetics', 'Investigation', 'Lead', 'Link', 'Machine Learning', 'Manuals', 'Methodology', 'Monitor', 'National Human Genome Research Institute', 'Paper', 'Productivity', 'Public Domains', 'Publications', 'Publishing', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Sampling', 'Scanning', 'Site', 'Standardization', 'Surveys', 'Technology', 'Text', 'Training', 'Update', 'Workload', 'base', 'design', 'genetic association', 'genome wide association study', 'improved', 'interest', 'next generation', 'novel', 'prevent', 'software development', 'success', 'text searching', 'tool', 'trait', 'user-friendly', 'volunteer']",NHGRI,UNIVERSITY OF SOUTHERN CALIFORNIA,U01,2012,247879,0.05741421575725703
"Discovering and Applying Knowledge in Clinical Databases DESCRIPTION (provided by applicant):     With the advent of improved clinical information system products (e.g., ambulatory systems, order entry  systems), improved data entry technologies (e.g., speech recognition, text processing techniques), and  further adoption of data interchange standards, more institutions are generating electronic medical records, and these records will expand in breadth, depth, and degree of coding in the future. The records are used mainly for individual patient care, but exploiting the records for clinical research and quality functions has lagged behind. Major challenges include the wide range of complex data and missing and inaccurate data.      We propose to continue our work to develop and test methods to mine a clinical data repository. A  special emphasis will be to exploit the vast amount of information in the repository (latent associations and knowledge) and to use computer intensive techniques and advances in data representation and manipulation to better interpret what is in the database and to overcome the challenges of complex, missing, and inaccurate data. We hypothesize that data mining techniques can be applied to a repository to generate accurate clinical interpretations. We further hypothesize that associations latent in a clinically rich repository can be used to improve the classification of cases in that repository.      We aim to develop methods to prepare data for mining; to characterize the information in the clinical data repository; to develop similarity measures based on manipulation of natural language processor output and on information retrieval techniques; to apply nearest neighbor technique and case-based reasoning to improve classification; to develop a statistically based method to improve classification of cases with incomplete or inaccurate data; and to apply our methods to real clinical research questions and carry out additional data mining research.      The researchers in the Department of Medical Informatics at Columbia University are uniquely positioned to carry out this research, given the experience of the team (data mining, statistics, health data organization, health knowledge representation, natural language processing), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data. n/a",Discovering and Applying Knowledge in Clinical Databases,6892934,R01LM006910,"['artificial intelligence', 'classification', 'clinical research', 'computer assisted medical decision making', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'health care facility information system', 'human data', 'information system analysis', 'method development', 'vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,384538,0.023106670794259383
"Discovering and Applying Knowledge in Clinical Databases DESCRIPTION (provided by applicant):     With the advent of improved clinical information system products (e.g., ambulatory systems, order entry  systems), improved data entry technologies (e.g., speech recognition, text processing techniques), and  further adoption of data interchange standards, more institutions are generating electronic medical records, and these records will expand in breadth, depth, and degree of coding in the future. The records are used mainly for individual patient care, but exploiting the records for clinical research and quality functions has lagged behind. Major challenges include the wide range of complex data and missing and inaccurate data.      We propose to continue our work to develop and test methods to mine a clinical data repository. A  special emphasis will be to exploit the vast amount of information in the repository (latent associations and knowledge) and to use computer intensive techniques and advances in data representation and manipulation to better interpret what is in the database and to overcome the challenges of complex, missing, and inaccurate data. We hypothesize that data mining techniques can be applied to a repository to generate accurate clinical interpretations. We further hypothesize that associations latent in a clinically rich repository can be used to improve the classification of cases in that repository.      We aim to develop methods to prepare data for mining; to characterize the information in the clinical data repository; to develop similarity measures based on manipulation of natural language processor output and on information retrieval techniques; to apply nearest neighbor technique and case-based reasoning to improve classification; to develop a statistically based method to improve classification of cases with incomplete or inaccurate data; and to apply our methods to real clinical research questions and carry out additional data mining research.      The researchers in the Department of Medical Informatics at Columbia University are uniquely positioned to carry out this research, given the experience of the team (data mining, statistics, health data organization, health knowledge representation, natural language processing), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data. n/a",Discovering and Applying Knowledge in Clinical Databases,6754395,R01LM006910,"['artificial intelligence', 'classification', 'clinical research', 'computer assisted medical decision making', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'health care facility information system', 'human data', 'information system analysis', 'method development', 'vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,380979,0.023106670794259383
"Discovering and Applying Knowledge in Clinical Databases DESCRIPTION (provided by applicant):     With the advent of improved clinical information system products (e.g., ambulatory systems, order entry  systems), improved data entry technologies (e.g., speech recognition, text processing techniques), and  further adoption of data interchange standards, more institutions are generating electronic medical records, and these records will expand in breadth, depth, and degree of coding in the future. The records are used mainly for individual patient care, but exploiting the records for clinical research and quality functions has lagged behind. Major challenges include the wide range of complex data and missing and inaccurate data.      We propose to continue our work to develop and test methods to mine a clinical data repository. A  special emphasis will be to exploit the vast amount of information in the repository (latent associations and knowledge) and to use computer intensive techniques and advances in data representation and manipulation to better interpret what is in the database and to overcome the challenges of complex, missing, and inaccurate data. We hypothesize that data mining techniques can be applied to a repository to generate accurate clinical interpretations. We further hypothesize that associations latent in a clinically rich repository can be used to improve the classification of cases in that repository.      We aim to develop methods to prepare data for mining; to characterize the information in the clinical data repository; to develop similarity measures based on manipulation of natural language processor output and on information retrieval techniques; to apply nearest neighbor technique and case-based reasoning to improve classification; to develop a statistically based method to improve classification of cases with incomplete or inaccurate data; and to apply our methods to real clinical research questions and carry out additional data mining research.      The researchers in the Department of Medical Informatics at Columbia University are uniquely positioned to carry out this research, given the experience of the team (data mining, statistics, health data organization, health knowledge representation, natural language processing), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data. n/a",Discovering and Applying Knowledge in Clinical Databases,6630735,R01LM006910,"['artificial intelligence', ' classification', ' clinical research', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' information system analysis', ' method development', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,377617,0.023106670794259383
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7651469,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'genome database', 'improved', 'interest', 'natural language', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2009,273993,0.0796234508720995
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7465580,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Class', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'RGD (sequence)', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'desire', 'genome database', 'improved', 'interest', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2008,273993,0.0796234508720995
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7264196,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Class', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'RGD (sequence)', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'desire', 'genome database', 'improved', 'interest', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2007,279300,0.0796234508720995
"GeneScene: a toolkit for gene pathway analysis   DESCRIPTION (provided by applicant): The recent explosion of information in the      biomedical field has provided a greater opportunity to address significant           problems related to human disease than at any time in the past. Biomedical           researchers studying such processes as growth, differentiation, and cell death       have defined a large number of genetic and biochemical pathways that regulate        these processes, and have determined that disruption of these pathways are           known to occur in most disease states. It is widely believed that elucidation        of the genes and proteins that compose these biochemical pathways will define        the molecular targets for future drug therapies. Increasingly, it is recognized      that the various biochemical pathways that have been defined by researchers are      cross connected and form an exceedingly complex network involving hundreds of        genes and proteins. Therefore, before the promise of pathway mechanism based         drug therapies can be realized, the nature of the effect that manipulating any       one pathway might have on another must be understood.                                                                                                                     GeneScene is designed to utilize information derived from Medline, the primary       repository of the abstracts of biomedical research reports, to help suggest          possible interactions between genetic and biochemical pathways. It will assist       in reviewing existing literature, identifying gaps in existing knowledge,            comparing and integrating knowledge and data from different fields, and as such      help lead the way to new and interesting hypotheses and field research. There        are four parts to this goal: GeneScene will integrate the knowledge related to       gene pathway analysis contained in several journals, allow researchers to            browse and search the information and our knowledge representation, integrate        text based knowledge regarding gene pathway analysis with gene array data, and       allow personalization and collaboration by researchers.                                                                                                                   Our first objective is the extraction of gene pathway knowledge from text-based      sources. Our second objective is to let researchers browse and search the            knowledge map. Our third objective is to provide researchers the opportunity to      cooperate and to integrate gene array data into the knowledge map.                                                                                                        n/a",GeneScene: a toolkit for gene pathway analysis,6916279,R33LM007299,"['Internet', 'abstracting', 'clinical research', 'computer data analysis', 'computer program /software', 'genetic mapping', 'health science research analysis /evaluation', 'human subject', 'interview', 'journals', 'literature survey', 'microarray technology', 'molecular biology information system']",NLM,UNIVERSITY OF ARIZONA,R33,2005,453468,0.03800199914313515
"GeneScene: a toolkit for gene pathway analysis   DESCRIPTION (provided by applicant): The recent explosion of information in the      biomedical field has provided a greater opportunity to address significant           problems related to human disease than at any time in the past. Biomedical           researchers studying such processes as growth, differentiation, and cell death       have defined a large number of genetic and biochemical pathways that regulate        these processes, and have determined that disruption of these pathways are           known to occur in most disease states. It is widely believed that elucidation        of the genes and proteins that compose these biochemical pathways will define        the molecular targets for future drug therapies. Increasingly, it is recognized      that the various biochemical pathways that have been defined by researchers are      cross connected and form an exceedingly complex network involving hundreds of        genes and proteins. Therefore, before the promise of pathway mechanism based         drug therapies can be realized, the nature of the effect that manipulating any       one pathway might have on another must be understood.                                                                                                                     GeneScene is designed to utilize information derived from Medline, the primary       repository of the abstracts of biomedical research reports, to help suggest          possible interactions between genetic and biochemical pathways. It will assist       in reviewing existing literature, identifying gaps in existing knowledge,            comparing and integrating knowledge and data from different fields, and as such      help lead the way to new and interesting hypotheses and field research. There        are four parts to this goal: GeneScene will integrate the knowledge related to       gene pathway analysis contained in several journals, allow researchers to            browse and search the information and our knowledge representation, integrate        text based knowledge regarding gene pathway analysis with gene array data, and       allow personalization and collaboration by researchers.                                                                                                                   Our first objective is the extraction of gene pathway knowledge from text-based      sources. Our second objective is to let researchers browse and search the            knowledge map. Our third objective is to provide researchers the opportunity to      cooperate and to integrate gene array data into the knowledge map.                                                                                                        n/a",GeneScene: a toolkit for gene pathway analysis,6622199,R33LM007299,"['Internet', 'abstracting', 'clinical research', 'computer data analysis', 'computer program /software', 'genetic mapping', 'health science research analysis /evaluation', 'human subject', 'interview', 'journals', 'literature survey', 'microarray technology', 'molecular biology information system']",NLM,UNIVERSITY OF ARIZONA,R33,2004,440259,0.03800199914313515
"GeneScene: a toolkit for gene pathway analysis   DESCRIPTION (provided by applicant): The recent explosion of information in the      biomedical field has provided a greater opportunity to address significant           problems related to human disease than at any time in the past. Biomedical           researchers studying such processes as growth, differentiation, and cell death       have defined a large number of genetic and biochemical pathways that regulate        these processes, and have determined that disruption of these pathways are           known to occur in most disease states. It is widely believed that elucidation        of the genes and proteins that compose these biochemical pathways will define        the molecular targets for future drug therapies. Increasingly, it is recognized      that the various biochemical pathways that have been defined by researchers are      cross connected and form an exceedingly complex network involving hundreds of        genes and proteins. Therefore, before the promise of pathway mechanism based         drug therapies can be realized, the nature of the effect that manipulating any       one pathway might have on another must be understood.                                                                                                                     GeneScene is designed to utilize information derived from Medline, the primary       repository of the abstracts of biomedical research reports, to help suggest          possible interactions between genetic and biochemical pathways. It will assist       in reviewing existing literature, identifying gaps in existing knowledge,            comparing and integrating knowledge and data from different fields, and as such      help lead the way to new and interesting hypotheses and field research. There        are four parts to this goal: GeneScene will integrate the knowledge related to       gene pathway analysis contained in several journals, allow researchers to            browse and search the information and our knowledge representation, integrate        text based knowledge regarding gene pathway analysis with gene array data, and       allow personalization and collaboration by researchers.                                                                                                                   Our first objective is the extraction of gene pathway knowledge from text-based      sources. Our second objective is to let researchers browse and search the            knowledge map. Our third objective is to provide researchers the opportunity to      cooperate and to integrate gene array data into the knowledge map.                                                                                                        n/a",GeneScene: a toolkit for gene pathway analysis,6442174,R33LM007299,"['Internet', ' abstracting', ' clinical research', ' computer data analysis', ' computer program /software', ' genetic mapping', ' health science research analysis /evaluation', ' human subject', ' interview', ' journals', ' literature survey', ' microarray technology', ' molecular biology information system']",NLM,UNIVERSITY OF ARIZONA,R33,2002,427435,0.03800199914313515
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,9306202,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Molecular Analysis', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2017,591990,0.12365790213838063
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,9113614,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2016,601709,0.12365790213838063
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,8866232,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2015,602189,0.12365790213838063
"Developing and applying information extraction resources and technology to create     DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another.              Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,8694375,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2014,746561,0.12365790213838063
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7908946,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'meetings', 'natural language', 'population based', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2009,71200,0.06617271109237592
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7414601,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Language', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Population', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Today', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2008,142400,0.06617271109237592
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7195053,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Language', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Population', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Today', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2007,145510,0.06617271109237592
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7019753,G08LM008983,"['clinical research', 'public health']",NLM,SYRACUSE UNIVERSITY,G08,2006,149472,0.06617271109237592
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7840891,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,39294,0.08558847197922939
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7662449,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,167303,0.08558847197922939
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7906366,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,123630,0.08558847197922939
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7470146,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Rate', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'concept', 'drug discovery', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2008,278208,0.08558847197922939
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,8151670,R01LM009153,[' '],NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,14785,0.08558847197922939
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,8133305,R01LM009153,[' '],NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,109860,0.08558847197922939
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7260682,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Rate', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'concept', 'drug discovery', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2007,291350,0.08558847197922939
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8318224,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2012,572436,0.13340717790479972
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8139258,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2011,513952,0.13340717790479972
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,7935408,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,515594,0.13340717790479972
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,7781934,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2009,505564,0.13340717790479972
"Beyond Abstracts:  Issues in Mining Full Texts    DESCRIPTION (provided by applicant):     Biomedical language processing, the application of computational techniques to human-generated texts in biomedicine, is an increasingly important enabling technology for basic and applied biomedical research. The exponential growth of the peer-reviewed literature and the breakdown of disciplinary boundaries associated with high-throughput techniques have increased the importance of automated tools for keeping scientists abreast of all of the published material relevant to their work. However, despite decades of research, the performance of state-of-the-art tools for basic language processing tasks like information extraction and document retrieval remain below the level necessary for adequate utility and widespread adoption of this technology. The development, performance and evaluation of text mining systems depend crucially on the availability of appropriate corpora: collections of representative documents that have been annotated with human judgments relevant to a language-processing task. Corpora play two roles in the development of this technology: first, they act as ""gold standards"" by which alternative automated methods can be fairly compared, and second, they provide data for the training of statistical and machine learning systems that create empirical models of patterns in language use. The conventional view is that corpora are neutral, random samples of the domain of interest. Our preliminary work suggests that the restrictions in size, quality, genre, and representational schema of the small number of existing corpora are themselves a critical limiting factor for near-term breakthroughs in biomedical text processing technology. Therefore, we propose to test the following hypothesis: Creation of large, high-quality, biomedical corpora from multiple genres will lead to significant improvements in the performance of biomedical text mining systems and the creation of new approaches to text mining tasks. Specific aims include constructing several large corpora covering a range of genres and incorporating a rich knowledge representation; identifying factors that affect differential performance on full text versus abstracts; and developing new methods for language processing, especially of full text. Because improvements in the ability to automatically extract information from many textual genres will assist scientists and clinicians in the crucial task of keeping up with the burgeoning biomedical literature, the potential public health impact is quite large.          n/a",Beyond Abstracts:  Issues in Mining Full Texts,7488396,R01LM009254,"['Adoption', 'Affect', 'Agreement', 'Arts', 'Biomedical Research', 'Body of uterus', 'Collection', 'Computational Technique', 'Data', 'Development', 'Evaluation', 'Gold', 'Growth', 'Human', 'Judgment', 'Language', 'Lead', 'Literature', 'Machine Learning', 'Memory', 'Methods', 'Metric', 'Mining', 'Modeling', 'Molecular Biology', 'Numbers', 'Pattern', 'Peer Review', 'Performance', 'Play', 'Process', 'Public Health', 'Publishing', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Retrieval', 'Review Literature', 'Role', 'Sampling', 'Scheme', 'Scientist', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'abstracting', 'base', 'concept', 'improved', 'information organization', 'interest', 'journal article', 'language processing', 'novel strategies', 'prototype', 'size', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2008,268713,0.1804762091645111
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9477110,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'analytical method', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2018,548298,0.07439506160608778
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9442241,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'analytical method', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2017,57870,0.07439506160608778
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9266490,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'analytical method', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2017,552544,0.07439506160608778
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9065611,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Process', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'abstracting', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'meetings', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2016,548438,0.07439506160608778
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.             Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,8819017,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Process', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'abstracting', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'meetings', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2015,548439,0.07439506160608778
"Beyond Abstracts:  Issues in Mining Full Texts    DESCRIPTION (provided by applicant):     Biomedical language processing, the application of computational techniques to human-generated texts in biomedicine, is an increasingly important enabling technology for basic and applied biomedical research. The exponential growth of the peer-reviewed literature and the breakdown of disciplinary boundaries associated with high-throughput techniques have increased the importance of automated tools for keeping scientists abreast of all of the published material relevant to their work. However, despite decades of research, the performance of state-of-the-art tools for basic language processing tasks like information extraction and document retrieval remain below the level necessary for adequate utility and widespread adoption of this technology. The development, performance and evaluation of text mining systems depend crucially on the availability of appropriate corpora: collections of representative documents that have been annotated with human judgments relevant to a language-processing task. Corpora play two roles in the development of this technology: first, they act as ""gold standards"" by which alternative automated methods can be fairly compared, and second, they provide data for the training of statistical and machine learning systems that create empirical models of patterns in language use. The conventional view is that corpora are neutral, random samples of the domain of interest. Our preliminary work suggests that the restrictions in size, quality, genre, and representational schema of the small number of existing corpora are themselves a critical limiting factor for near-term breakthroughs in biomedical text processing technology. Therefore, we propose to test the following hypothesis: Creation of large, high-quality, biomedical corpora from multiple genres will lead to significant improvements in the performance of biomedical text mining systems and the creation of new approaches to text mining tasks. Specific aims include constructing several large corpora covering a range of genres and incorporating a rich knowledge representation; identifying factors that affect differential performance on full text versus abstracts; and developing new methods for language processing, especially of full text. Because improvements in the ability to automatically extract information from many textual genres will assist scientists and clinicians in the crucial task of keeping up with the burgeoning biomedical literature, the potential public health impact is quite large.          n/a",Beyond Abstracts:  Issues in Mining Full Texts,7287359,R01LM009254,"['Adoption', 'Affect', 'Agreement', 'Arts', 'Biomedical Research', 'Body of uterus', 'Collection', 'Computational Technique', 'Data', 'Development', 'Evaluation', 'Gold', 'Growth', 'Human', 'Judgment', 'Language', 'Lead', 'Literature', 'Machine Learning', 'Memory', 'Methods', 'Metric', 'Mining', 'Modeling', 'Molecular Biology', 'Numbers', 'Pattern', 'Peer Review', 'Performance', 'Play', 'Process', 'Public Health', 'Publishing', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Retrieval', 'Review Literature', 'Role', 'Sampling', 'Scheme', 'Scientist', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'abstracting', 'base', 'concept', 'improved', 'information organization', 'interest', 'journal article', 'language processing', 'novel strategies', 'prototype', 'size', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2007,350638,0.1804762091645111
"Beyond Abstracts:  Issues in Mining Full Texts    DESCRIPTION (provided by applicant):     Biomedical language processing, the application of computational techniques to human-generated texts in biomedicine, is an increasingly important enabling technology for basic and applied biomedical research. The exponential growth of the peer-reviewed literature and the breakdown of disciplinary boundaries associated with high-throughput techniques have increased the importance of automated tools for keeping scientists abreast of all of the published material relevant to their work. However, despite decades of research, the performance of state-of-the-art tools for basic language processing tasks like information extraction and document retrieval remain below the level necessary for adequate utility and widespread adoption of this technology. The development, performance and evaluation of text mining systems depend crucially on the availability of appropriate corpora: collections of representative documents that have been annotated with human judgments relevant to a language-processing task. Corpora play two roles in the development of this technology: first, they act as ""gold standards"" by which alternative automated methods can be fairly compared, and second, they provide data for the training of statistical and machine learning systems that create empirical models of patterns in language use. The conventional view is that corpora are neutral, random samples of the domain of interest. Our preliminary work suggests that the restrictions in size, quality, genre, and representational schema of the small number of existing corpora are themselves a critical limiting factor for near-term breakthroughs in biomedical text processing technology. Therefore, we propose to test the following hypothesis: Creation of large, high-quality, biomedical corpora from multiple genres will lead to significant improvements in the performance of biomedical text mining systems and the creation of new approaches to text mining tasks. Specific aims include constructing several large corpora covering a range of genres and incorporating a rich knowledge representation; identifying factors that affect differential performance on full text versus abstracts; and developing new methods for language processing, especially of full text. Because improvements in the ability to automatically extract information from many textual genres will assist scientists and clinicians in the crucial task of keeping up with the burgeoning biomedical literature, the potential public health impact is quite large.          n/a",Beyond Abstracts:  Issues in Mining Full Texts,7135482,R01LM009254,"['abstracting', 'human', 'language', 'performance', 'training']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2006,369593,0.1804762091645111
"Text Mining as a Translational Tool in Biomedicine    DESCRIPTION (provided by applicant):      The candidate's long-term career goal is to become an independent investigator in biomedical informatics, with a specific focus on text mining as a translational tool in biomedical research. The candidate's immediate goals are to strengthen his independent position as a faculty at Yale University, to position biomedical text mining as an essential part of the University's curriculum and to build strong collaboration with scientists in both computational linguistics and basic biomedical research. The candidate career development plan is a logical continuation of his previous work. The proposed project will help to establish the candidate as an independent researcher in the wider biomedical informatics community. The general aim of this proposal is to develop a text mining-based translation informatics tool that helps geneticists in pinpointing likely disease candidate genes from whole genome linkage scans. A collaborative environment here at Yale enables the experimental validation of these findings, and an iterative refinement of analytical methods. While the focus of the project is on genetics, there is a growing demand to analyze and make sense of whole genome high throughput information in various disciplines. The candidate aims at establishing text mining as an important tool for such purposes.          n/a",Text Mining as a Translational Tool in Biomedicine,7483271,K22LM009255,"['Binding', 'Biomedical Research', 'Candidate Disease Gene', 'Collaborations', 'Communities', 'Data', 'Development Plans', 'Discipline', 'Disease', 'Educational Curriculum', 'Environment', 'Faculty', 'Genes', 'Genetic', 'Genetic Research', 'Genome', 'Goals', 'Head', 'Human Genome', 'Informatics', 'Linguistics', 'Maps', 'Methods', 'Molecular', 'Positioning Attribute', 'Process', 'Purpose', 'Research Design', 'Research Personnel', 'Scanning', 'Scientist', 'Translations', 'Universities', 'Validation', 'Work', 'analytical method', 'base', 'biomedical informatics', 'career', 'protein protein interaction', 'text searching', 'tool']",NLM,YALE UNIVERSITY,K22,2008,144819,0.11241895676714297
"Text Mining as a Translational Tool in Biomedicine    DESCRIPTION (provided by applicant):      The candidate's long-term career goal is to become an independent investigator in biomedical informatics, with a specific focus on text mining as a translational tool in biomedical research. The candidate's immediate goals are to strengthen his independent position as a faculty at Yale University, to position biomedical text mining as an essential part of the University's curriculum and to build strong collaboration with scientists in both computational linguistics and basic biomedical research. The candidate career development plan is a logical continuation of his previous work. The proposed project will help to establish the candidate as an independent researcher in the wider biomedical informatics community. The general aim of this proposal is to develop a text mining-based translation informatics tool that helps geneticists in pinpointing likely disease candidate genes from whole genome linkage scans. A collaborative environment here at Yale enables the experimental validation of these findings, and an iterative refinement of analytical methods. While the focus of the project is on genetics, there is a growing demand to analyze and make sense of whole genome high throughput information in various disciplines. The candidate aims at establishing text mining as an important tool for such purposes.          n/a",Text Mining as a Translational Tool in Biomedicine,7290386,K22LM009255,"['Binding', 'Biomedical Research', 'Candidate Disease Gene', 'Collaborations', 'Communities', 'Data', 'Development Plans', 'Discipline', 'Disease', 'Educational Curriculum', 'Environment', 'Faculty', 'Genes', 'Genetic', 'Genetic Research', 'Genome', 'Goals', 'Head', 'Human Genome', 'Informatics', 'Linguistics', 'Maps', 'Methods', 'Molecular', 'Positioning Attribute', 'Process', 'Purpose', 'Research Design', 'Research Personnel', 'Scanning', 'Scientist', 'Translations', 'Universities', 'Validation', 'Work', 'analytical method', 'base', 'biomedical informatics', 'career', 'protein protein interaction', 'text searching', 'tool']",NLM,YALE UNIVERSITY,K22,2007,144819,0.11241895676714297
"Text Mining as a Translational Tool in Biomedicine    DESCRIPTION (provided by applicant):      The candidate's long-term career goal is to become an independent investigator in biomedical informatics, with a specific focus on text mining as a translational tool in biomedical research. The candidate's immediate goals are to strengthen his independent position as a faculty at Yale University, to position biomedical text mining as an essential part of the University's curriculum and to build strong collaboration with scientists in both computational linguistics and basic biomedical research. The candidate career development plan is a logical continuation of his previous work. The proposed project will help to establish the candidate as an independent researcher in the wider biomedical informatics community. The general aim of this proposal is to develop a text mining-based translation informatics tool that helps geneticists in pinpointing likely disease candidate genes from whole genome linkage scans. A collaborative environment here at Yale enables the experimental validation of these findings, and an iterative refinement of analytical methods. While the focus of the project is on genetics, there is a growing demand to analyze and make sense of whole genome high throughput information in various disciplines. The candidate aims at establishing text mining as an important tool for such purposes.          n/a",Text Mining as a Translational Tool in Biomedicine,7133136,K22LM009255,"['career', 'genes', 'genetics', 'genome', 'informatics', 'university']",NLM,YALE UNIVERSITY,K22,2006,150219,0.11241895676714297
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7673720,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2009,142851,0.13985068428120814
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7872692,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2009,66015,0.13985068428120814
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7495148,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Facility Construction Funding Category', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Numbers', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Rate', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Standards of Weights and Measures', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Today', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2008,132030,0.13985068428120814
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7301251,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Facility Construction Funding Category', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Numbers', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Rate', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Standards of Weights and Measures', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Today', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2007,130432,0.13985068428120814
"Beyond information extraction: Identifying Gene Ontology concepts in text    DESCRIPTION (provided by applicant):       There has been growing interest in recent years in developing methods that automatically identify Gene Ontology (GO) concepts in the unstructured text of scientific articles. This interest is motivated in part by the need to automate the task of model-organism database curation. In addition, however, methods that automatically identify GO concepts in text will enable data mining tools that compile and interpret information extracted from text, tools that will benefit a large number of people across the scientific enterprise. This project builds on recently completed work in which we used the literature of S. cerevisiae and annotations in the Saccharomyces Genome Database (SGD) to develop methods that determine what molecular function claims are being made in an article and what experimental evidence there is in the article for those claims. The data generated in this project contains a wealth of information that could lead to greatly improved methods for identifying GO concepts in text. The specific aims of this project are: (1) to develop a representation for GO molecular function concepts that captures information not only about the language of a GO term but also the biomedical entity the term refers to; and (2) to analyze the results of the S. cerevisiae data mining project using the GO representations formulated in (1) to determine which are likely to produce improved GO term recognition. The analysis will be performed on 276 true positive results, 29,276 false positive results, and 336 false negative results to see if a new GO concept representation can reduce the number of false positives or false negatives without losing any true positives. The data mining tools of this proposal can be extended to ontologies other than GO, thereby leveraging the effort expended on ontology development.           n/a",Beyond information extraction: Identifying Gene Ontology concepts in text,7918188,R03LM009752,"['Biological Assay', 'Biological Process', 'Controlled Vocabulary', 'Data', 'Data Set', 'Databases', 'Development', 'Ensure', 'Frequencies', 'Genes', 'Individual', 'Information Retrieval', 'Language', 'Lead', 'Literature', 'Methods', 'Molecular', 'Ontology', 'Problem Solving', 'Proteins', 'Reporting', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Text', 'Work', 'base', 'biomedical scientist', 'data mining', 'gene correction', 'gene function', 'genome database', 'improved', 'interest', 'model organisms databases', 'natural language', 'phrases', 'success', 'text searching', 'tool']",NLM,"CONVERSPEECH, LLC",R03,2010,50000,-0.038528494060716914
"Beyond information extraction: Identifying Gene Ontology concepts in text    DESCRIPTION (provided by applicant):       There has been growing interest in recent years in developing methods that automatically identify Gene Ontology (GO) concepts in the unstructured text of scientific articles. This interest is motivated in part by the need to automate the task of model-organism database curation. In addition, however, methods that automatically identify GO concepts in text will enable data mining tools that compile and interpret information extracted from text, tools that will benefit a large number of people across the scientific enterprise. This project builds on recently completed work in which we used the literature of S. cerevisiae and annotations in the Saccharomyces Genome Database (SGD) to develop methods that determine what molecular function claims are being made in an article and what experimental evidence there is in the article for those claims. The data generated in this project contains a wealth of information that could lead to greatly improved methods for identifying GO concepts in text. The specific aims of this project are: (1) to develop a representation for GO molecular function concepts that captures information not only about the language of a GO term but also the biomedical entity the term refers to; and (2) to analyze the results of the S. cerevisiae data mining project using the GO representations formulated in (1) to determine which are likely to produce improved GO term recognition. The analysis will be performed on 276 true positive results, 29,276 false positive results, and 336 false negative results to see if a new GO concept representation can reduce the number of false positives or false negatives without losing any true positives. The data mining tools of this proposal can be extended to ontologies other than GO, thereby leveraging the effort expended on ontology development.           n/a",Beyond information extraction: Identifying Gene Ontology concepts in text,7362877,R03LM009752,"['Biological Assay', 'Biological Process', 'Controlled Vocabulary', 'Data', 'Data Set', 'Databases', 'Development', 'Ensure', 'Frequencies', 'Genes', 'Individual', 'Information Retrieval', 'Language', 'Lead', 'Literature', 'Methods', 'Molecular', 'Ontology', 'Problem Solving', 'Proteins', 'Reporting', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Text', 'Work', 'base', 'biomedical scientist', 'data mining', 'gene correction', 'gene function', 'genome database', 'improved', 'interest', 'model organisms databases', 'natural language', 'phrases', 'success', 'text searching', 'tool']",NLM,"CONVERSPEECH, LLC",R03,2009,50000,-0.038528494060716914
"Adapting Natural Language Processing Tools for Biosurveillance    DESCRIPTION (provided by applicant):       Early detection of disease outbreaks can decrease patient morbidity and mortality and minimize the spread of diseases. Early detection requires accurate classification of patient symptoms early in the course of their illness. One approach is biosurveillance, in which electronic symptom data are captured early in the course of illness, and analyzed for signals that might indicate an outbreak requiring investigation and response by the public health system. Emergency department (ED) patient records are particularly useful for biosurveillance, given their timely, electronic availability. ED data elements used in surveillance systems include the chief complaint (a brief description of the patient's primary symptom(s)), and triage nurses' note (also known as history of present illness).The chief complaint is the most widely used ED data element, because it is recorded electronically by the majority of EDs. One study showed that adding triage notes increased the sensitivity of biosurveillance case detection. The increased sensitivity is because the triage note increases the amount of data available: instead of one symptom in a chief complaint (e.g., fever), triage notes may contain multiple symptoms (e.g., ""fever, cough & shortness of breath for 12 hours""). Surveillance efforts are hampered, however, by the wide variability of free text data in ED chief complaints and triage notes. They often include misspellings, abbreviations, acronyms and other lexical and semantic variants that are difficult to group into symptom clusters (e.g., fever, temp 104, fvr, febrile). Tools are needed to address the lexical and semantic variation in symptom terms in ED data in order to improve biosurveillance. Natural language processing tools have been shown to facilitate concept extraction from more structured clinical data such as radiology reports, but there has been limited application of these techniques to free text ED triage notes. The project team developed the Emergency Medical Text Processor (EMT-P) to preprocess the chief complaint. EMT-P cleans and normalizes brief chief complaint entries and then extracts standardized concepts, but it is not sufficient in its current state to preprocess longer, more complex text passages such as triage notes. This proposed project will further strengthen biosurveillance by adapting EMT-P and other statistical and classical natural language processing tools to develop a system that extracts concepts from triage notes for biosurveillance.           Project Narrative Relevance: The public health system is responsible for monitoring large amounts of timely, electronic health data and needs more sophisticated tools to faciliate detection of, and response to, emerging infectious diseases and potential bioterrorism threats. The proposed project addresses this need by developing a system to extract relevant information from emergency department records.",Adapting Natural Language Processing Tools for Biosurveillance,8144459,G08LM009787,"['Abbreviations', 'Accident and Emergency department', 'Acute', 'Address', 'American', 'Avian Influenza', 'Bioterrorism', 'Bird Flu vaccine', 'Breathing', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Coughing', 'Country', 'Data', 'Data Element', 'Data Quality', 'Detection', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Early Diagnosis', 'Electronic Health Record', 'Electronics', 'Emergency Medicine', 'Emergency Situation', 'Emerging Communicable Diseases', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Fever', 'Genetic Transcription', 'Goals', 'Gold', 'Health', 'Health Services', 'Health system', 'Hour', 'Intervention', 'Investigation', 'Manuals', 'Measures', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'North Carolina', 'Nurses', 'Patients', 'Performance', 'Pertussis', 'Physicians', 'Predictive Value', 'Process', 'Public Health', 'Radiology Specialty', 'Recording of previous events', 'Records', 'Reporting', 'Sampling', 'Screening procedure', 'Semantics', 'Sensitivity and Specificity', 'Shortness of Breath', 'Signal Transduction', 'Smallpox', 'Structure', 'Symptoms', 'System', 'Techniques', 'Temperature', 'Text', 'Translating', 'Triage', 'Variant', 'acronyms', 'base', 'experience', 'improved', 'lexical', 'mortality', 'pandemic disease', 'population based', 'prototype', 'research to practice', 'response', 'satisfaction', 'syntax', 'tool']",NLM,UNIV OF NORTH CAROLINA CHAPEL HILL,G08,2011,147161,0.03398639798085784
"Adapting Natural Language Processing Tools for Biosurveillance    DESCRIPTION (provided by applicant):       Early detection of disease outbreaks can decrease patient morbidity and mortality and minimize the spread of diseases. Early detection requires accurate classification of patient symptoms early in the course of their illness. One approach is biosurveillance, in which electronic symptom data are captured early in the course of illness, and analyzed for signals that might indicate an outbreak requiring investigation and response by the public health system. Emergency department (ED) patient records are particularly useful for biosurveillance, given their timely, electronic availability. ED data elements used in surveillance systems include the chief complaint (a brief description of the patient's primary symptom(s)), and triage nurses' note (also known as history of present illness).The chief complaint is the most widely used ED data element, because it is recorded electronically by the majority of EDs. One study showed that adding triage notes increased the sensitivity of biosurveillance case detection. The increased sensitivity is because the triage note increases the amount of data available: instead of one symptom in a chief complaint (e.g., fever), triage notes may contain multiple symptoms (e.g., ""fever, cough & shortness of breath for 12 hours""). Surveillance efforts are hampered, however, by the wide variability of free text data in ED chief complaints and triage notes. They often include misspellings, abbreviations, acronyms and other lexical and semantic variants that are difficult to group into symptom clusters (e.g., fever, temp 104, fvr, febrile). Tools are needed to address the lexical and semantic variation in symptom terms in ED data in order to improve biosurveillance. Natural language processing tools have been shown to facilitate concept extraction from more structured clinical data such as radiology reports, but there has been limited application of these techniques to free text ED triage notes. The project team developed the Emergency Medical Text Processor (EMT-P) to preprocess the chief complaint. EMT-P cleans and normalizes brief chief complaint entries and then extracts standardized concepts, but it is not sufficient in its current state to preprocess longer, more complex text passages such as triage notes. This proposed project will further strengthen biosurveillance by adapting EMT-P and other statistical and classical natural language processing tools to develop a system that extracts concepts from triage notes for biosurveillance.           Project Narrative Relevance: The public health system is responsible for monitoring large amounts of timely, electronic health data and needs more sophisticated tools to faciliate detection of, and response to, emerging infectious diseases and potential bioterrorism threats. The proposed project addresses this need by developing a system to extract relevant information from emergency department records.",Adapting Natural Language Processing Tools for Biosurveillance,7921455,G08LM009787,"['Abbreviations', 'Accident and Emergency department', 'Acute', 'Address', 'American', 'Avian Influenza', 'Bioterrorism', 'Bird Flu vaccine', 'Breathing', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Coughing', 'Country', 'Data', 'Data Element', 'Data Quality', 'Detection', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Early Diagnosis', 'Electronic Health Record', 'Electronics', 'Emergency Medicine', 'Emergency Situation', 'Emerging Communicable Diseases', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Fever', 'Genetic Transcription', 'Goals', 'Gold', 'Health', 'Health Services', 'Health system', 'Hour', 'Intervention', 'Investigation', 'Manuals', 'Measures', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'North Carolina', 'Nurses', 'Patients', 'Performance', 'Pertussis', 'Physicians', 'Predictive Value', 'Process', 'Public Health', 'Radiology Specialty', 'Recording of previous events', 'Records', 'Reporting', 'Sampling', 'Screening procedure', 'Semantics', 'Sensitivity and Specificity', 'Shortness of Breath', 'Signal Transduction', 'Smallpox', 'Structure', 'Symptoms', 'System', 'Techniques', 'Temperature', 'Text', 'Translating', 'Triage', 'Variant', 'acronyms', 'base', 'experience', 'improved', 'lexical', 'mortality', 'pandemic disease', 'population based', 'prototype', 'research to practice', 'response', 'satisfaction', 'syntax', 'tool']",NLM,UNIV OF NORTH CAROLINA CHAPEL HILL,G08,2010,148350,0.03398639798085784
"Adapting Natural Language Processing Tools for Biosurveillance    DESCRIPTION (provided by applicant):       Early detection of disease outbreaks can decrease patient morbidity and mortality and minimize the spread of diseases. Early detection requires accurate classification of patient symptoms early in the course of their illness. One approach is biosurveillance, in which electronic symptom data are captured early in the course of illness, and analyzed for signals that might indicate an outbreak requiring investigation and response by the public health system. Emergency department (ED) patient records are particularly useful for biosurveillance, given their timely, electronic availability. ED data elements used in surveillance systems include the chief complaint (a brief description of the patient's primary symptom(s)), and triage nurses' note (also known as history of present illness).The chief complaint is the most widely used ED data element, because it is recorded electronically by the majority of EDs. One study showed that adding triage notes increased the sensitivity of biosurveillance case detection. The increased sensitivity is because the triage note increases the amount of data available: instead of one symptom in a chief complaint (e.g., fever), triage notes may contain multiple symptoms (e.g., ""fever, cough & shortness of breath for 12 hours""). Surveillance efforts are hampered, however, by the wide variability of free text data in ED chief complaints and triage notes. They often include misspellings, abbreviations, acronyms and other lexical and semantic variants that are difficult to group into symptom clusters (e.g., fever, temp 104, fvr, febrile). Tools are needed to address the lexical and semantic variation in symptom terms in ED data in order to improve biosurveillance. Natural language processing tools have been shown to facilitate concept extraction from more structured clinical data such as radiology reports, but there has been limited application of these techniques to free text ED triage notes. The project team developed the Emergency Medical Text Processor (EMT-P) to preprocess the chief complaint. EMT-P cleans and normalizes brief chief complaint entries and then extracts standardized concepts, but it is not sufficient in its current state to preprocess longer, more complex text passages such as triage notes. This proposed project will further strengthen biosurveillance by adapting EMT-P and other statistical and classical natural language processing tools to develop a system that extracts concepts from triage notes for biosurveillance.           Project Narrative Relevance: The public health system is responsible for monitoring large amounts of timely, electronic health data and needs more sophisticated tools to faciliate detection of, and response to, emerging infectious diseases and potential bioterrorism threats. The proposed project addresses this need by developing a system to extract relevant information from emergency department records.",Adapting Natural Language Processing Tools for Biosurveillance,7693117,G08LM009787,"['Abbreviations', 'Accident and Emergency department', 'Acute', 'Address', 'American', 'Avian Influenza', 'Bioterrorism', 'Bird Flu vaccine', 'Breathing', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Coughing', 'Country', 'Data', 'Data Element', 'Data Quality', 'Detection', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Early Diagnosis', 'Electronic Health Record', 'Electronics', 'Emergency Medicine', 'Emergency Situation', 'Emerging Communicable Diseases', 'Epidemiologist', 'Evaluation', 'Event', 'Fever', 'Genetic Transcription', 'Goals', 'Gold', 'Health', 'Health Services', 'Health system', 'Hour', 'Intervention', 'Investigation', 'Manuals', 'Measures', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'North Carolina', 'Nurses', 'Patients', 'Performance', 'Pertussis', 'Physicians', 'Predictive Value', 'Process', 'Public Health', 'Radiology Specialty', 'Recording of previous events', 'Records', 'Reporting', 'Sampling', 'Screening procedure', 'Semantics', 'Sensitivity and Specificity', 'Shortness of Breath', 'Signal Transduction', 'Smallpox', 'Structure', 'Symptoms', 'System', 'Techniques', 'Temperature', 'Text', 'Translating', 'Triage', 'Variant', 'acronyms', 'base', 'experience', 'improved', 'lexical', 'mortality', 'pandemic disease', 'population based', 'prototype', 'research to practice', 'response', 'satisfaction', 'syntax', 'tool']",NLM,UNIV OF NORTH CAROLINA CHAPEL HILL,G08,2009,145926,0.03398639798085784
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7690941,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'clinical practice', 'design', 'foot', 'journal article', 'language processing', 'meetings', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2009,351549,0.0604768488346597
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7908952,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'clinical practice', 'design', 'foot', 'journal article', 'language processing', 'meetings', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2009,170662,0.0604768488346597
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7502749,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Numbers', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'design', 'foot', 'journal article', 'language processing', 'mecarzole', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2008,352226,0.0604768488346597
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7380099,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Numbers', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'design', 'foot', 'journal article', 'language processing', 'mecarzole', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2007,383550,0.0604768488346597
"Advancing Literature Mining through Image Processing and Analysis    DESCRIPTION (provided by applicant): We propose to advance biomedical literature mining by providing new technologies for searching and retrieving biomedical images. The content of these images is a good representation of the research results discussed in scientific articles, and often contains additional facts not explicitly mentioned in the article or image caption. While the inherent structure in biomedical images facilitates automated image content extraction, the extraction process can be made more accurate by concurrent processing of text and images. Text-enhanced image analysis results in richly annotated images, which open up new possibilities for locating images of interest. The specific aims are to 1) develop methods for extracting structured image content through image processing and analysis, to 2) design methods to boost accuracy of image understanding through concurrent processing of text and images, to 3) devise methods for searching across structured image content, and 4) to develop an image search tool that demonstrates the power of using structured Image content for accessing the biomedical literature.           We propose to advance our ability to search Millions of published research articles by looking into the images within those articles. Our methodology understands the text and layout within images, allowing for very precise image searches. The methodology should have a broad impact on our ability to access the biological and medical literature.",Advancing Literature Mining through Image Processing and Analysis,8138362,R01LM009956,"['Biological', 'Biological Sciences', 'Health Professional', 'Image', 'Image Analysis', 'Literature', 'Medical', 'Methodology', 'Methods', 'Performance', 'Process', 'Publishing', 'Research', 'Research Personnel', 'Structure', 'Text', 'bioimaging', 'design', 'image processing', 'improved', 'interest', 'new technology', 'text searching', 'tool']",NLM,YALE UNIVERSITY,R01,2011,321995,0.15565548834300583
"Advancing Literature Mining through Image Processing and Analysis    DESCRIPTION (provided by applicant): We propose to advance biomedical literature mining by providing new technologies for searching and retrieving biomedical images. The content of these images is a good representation of the research results discussed in scientific articles, and often contains additional facts not explicitly mentioned in the article or image caption. While the inherent structure in biomedical images facilitates automated image content extraction, the extraction process can be made more accurate by concurrent processing of text and images. Text-enhanced image analysis results in richly annotated images, which open up new possibilities for locating images of interest. The specific aims are to 1) develop methods for extracting structured image content through image processing and analysis, to 2) design methods to boost accuracy of image understanding through concurrent processing of text and images, to 3) devise methods for searching across structured image content, and 4) to develop an image search tool that demonstrates the power of using structured Image content for accessing the biomedical literature.           We propose to advance our ability to search Millions of published research articles by looking into the images within those articles. Our methodology understands the text and layout within images, allowing for very precise image searches. The methodology should have a broad impact on our ability to access the biological and medical literature.",Advancing Literature Mining through Image Processing and Analysis,7727854,R01LM009956,"['Biological', 'Biological Sciences', 'Health Professional', 'Image', 'Image Analysis', 'Literature', 'Medical', 'Methodology', 'Methods', 'Performance', 'Process', 'Publishing', 'Research', 'Research Personnel', 'Structure', 'Text', 'bioimaging', 'design', 'image processing', 'improved', 'interest', 'new technology', 'text searching', 'tool']",NLM,YALE UNIVERSITY,R01,2009,338835,0.15565548834300583
"Advancing Literature Mining through Image Processing and Analysis    DESCRIPTION (provided by applicant): We propose to advance biomedical literature mining by providing new technologies for searching and retrieving biomedical images. The content of these images is a good representation of the research results discussed in scientific articles, and often contains additional facts not explicitly mentioned in the article or image caption. While the inherent structure in biomedical images facilitates automated image content extraction, the extraction process can be made more accurate by concurrent processing of text and images. Text-enhanced image analysis results in richly annotated images, which open up new possibilities for locating images of interest. The specific aims are to 1) develop methods for extracting structured image content through image processing and analysis, to 2) design methods to boost accuracy of image understanding through concurrent processing of text and images, to 3) devise methods for searching across structured image content, and 4) to develop an image search tool that demonstrates the power of using structured Image content for accessing the biomedical literature.           We propose to advance our ability to search Millions of published research articles by looking into the images within those articles. Our methodology understands the text and layout within images, allowing for very precise image searches. The methodology should have a broad impact on our ability to access the biological and medical literature.",Advancing Literature Mining through Image Processing and Analysis,7921438,R01LM009956,"['Biological', 'Biological Sciences', 'Health Professional', 'Image', 'Image Analysis', 'Literature', 'Medical', 'Methodology', 'Methods', 'Performance', 'Process', 'Publishing', 'Research', 'Research Personnel', 'Structure', 'Text', 'bioimaging', 'design', 'image processing', 'improved', 'interest', 'new technology', 'text searching', 'tool']",NLM,YALE UNIVERSITY,R01,2010,335430,0.15565548834300583
"Onto-BioThesaurus: ontological representation of gene/protein names for biomedica    DESCRIPTION (provided by applicant):       The long-term goal of our research is to develop resources and tools for knowledge retrieval management in the biomedical domain. As the pace of biomedical research accelerates, researchers become more and more dependent on computers to manage the explosive amount of biomedical information being published. The high quality of many databases is guaranteed by database curators who extract and synthesize information stored in literature or other databases. It is important to accurately recognize biomedical entity names in text and map the identified names to corresponding records in biomedical databases. Usually, a biomedical database provides a list of names either entered by curators or extracted from other databases. Those names could be used to retrieve records from databases or map names to database records by NLP systems. However, there are several characteristics associated with biomedical entity names, namely: synonymy (i.e., different names refer to the same database entry), ambiguity (i.e., one name is associated with different entries), and novelty (i.e., names or entities are not present in databases or knowledge bases) which make the task of retrieving database records using names and the task of associating names in text to database records very daunting. Additionally, biomedical entities can appear in text as short forms (SFs) abbreviated from their long forms (LFs). The prevalent use of SFs representing biomedical entities is another challenge faced by end users and NLP applications because of the high ambiguity of SFs.       Recently, ontology-based knowledge management is becoming increasingly popular since ontologies provide formal, machine-processable, and human-interpretable representations of the biomedical entities and their relations. We hypothesize that biomedical ontologies can be used to reduce the difficulty associated with retrieving records using names or mapping names in text to database records. Specific aims and the corresponding hypotheses are: i) develop onto-BioThesaurus by enriching BioThesaurus with gene/protein-related ontologies (Hypothesis: aligning gene/protein names to gene/protein-related ontologies can reduce the complexity associated with gene/protein names); ii) harvest synonyms for gene/protein classes and entities from online resources and text (Hypothesis: harvesting synonyms especially gene/protein SFs is critical since SFs are frequently used to represent gene/protein entities); iii) build a web user interface for gene/protein names and entries search and query through ontology-enabled onto-BioThesaurus (Hypothesis: enhancing BioThesaurus with gene/protein-related ontologies would enable us to build heuristic rules to enable machine reasoning); and iv) evaluate and distribute research methods/outcome (Hypothesis: evaluating and distributing research methods/outcome are critical to advance both basic and applied biomedical science.            The proposed research is critical for biomedical knowledge retrieval and management. It serves as one of the foundation for storing, retrieving, and extracting knowledge and information in the biomedical domain. Additionally, the proposed research will benefit biomedical researchers and general community for understanding and managing biomedical text through web interfaces and automated systems.",Onto-BioThesaurus: ontological representation of gene/protein names for biomedica,7654995,R01LM009959,"['Abbreviations', 'Biomedical Research', 'Characteristics', 'Communities', 'Computers', 'Databases', 'Expert Opinion', 'Foundations', 'Gene Proteins', 'Genes', 'Goals', 'Harvest', 'Human', 'Information Resources', 'Information Resources Management', 'Internet', 'Investigation', 'Knowledge', 'Literature', 'Manuals', 'Maps', 'Names', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Outcome', 'Peer Review', 'Process', 'Proteins', 'Publishing', 'Records', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Retrieval', 'Review Literature', 'Science', 'Services', 'System', 'Techniques', 'Terminology', 'Text', 'Thesauri', 'Time', 'acronyms', 'base', 'biomedical ontology', 'heuristics', 'knowledge base', 'tool', 'web interface', 'web site']",NLM,GEORGETOWN UNIVERSITY,R01,2009,608650,-0.09507315128982469
"POET: Consolidated, Comprehensive Clinical Text Preprocessing    DESCRIPTION (provided by applicant):       As electronic health records (EHRs) continue their expansion into clinical settings, there has been a corresponding increase in interest in mining the data they contain, both for research as well as for clinical decision support. Informaticists are increasingly studying ways to mine EHR textual content. This is an important trend, because there is a wealth of information contained in clinical text not represented anywhere else in the EHR. There is a low level text-as-data issue which presents a significant obstacle to the widespread use of available medical NLP systems: hand-typed clinical narratives in EHRs are usually ungrammatical; short or telegraphic in style; full of abbreviations, acronyms, and misspellings; formatted in a templated or pseudo-tabular form; and contain embedded non-text such as a list of laboratory values cut-and-pasted from elsewhere in the EHR. As we show in the Preliminary Studies Section, this makes high-level processing by popular tools like MedLEE and MetaMap effectively useless for all but a few ""clean"" document types like discharge summaries or consult reports (e.g., pathology or radiology reports). This in turn explains why there is so little published about what is certainly the preponderance of clinical texts, those that are not as well-behaved lexically and syntactically as a discharge summary.       In this application we distinguish clinical narratives (e.g., a progress note) from biomedical narratives (e.g., a PubMed abstract). We are interested in texts that arise in the clinical or research setting; texts that are composed by clinicians and researchers directly into a computer system. We propose to build and publish a tool called POET (Parsable Output Extracted from Text). POET will be designed to accept unstructured textual documents and return structured, linguistic equivalents that are, to the extent possible, parsable by higher-level NLP engines. POET will have an architecture is modular, extensible, and based on open-source platforms and sources (e.g., Java, Perl, UMLS, NegEx, the Stanford Parser, HL7 Clinical Document Architecture, caGRID, etc.). To implement POET, we will collect, program, and evaluate published as well as novel algorithms for: acronym/abbreviation resolution; spelling correction; template and pseudo-table re-writing; and removal of embedded non-text. To test POET we will use a large corpus of cross-discipline (e.g., medical, nursing, pharmacy, etc.) clinical note types, as well as the clinical research texts MedWatch reports and IRB adverse event reports. The development of POET will combine the best practices found in the literature and new research efforts as part of the project. To validate the fidelity of POET processing we plan a formal analysis of information loss and information gain pre- and post-process. To ensure broad access to the tools, POET will be released under an open-source license. Finally, we plan to assess the feasibility of offering POET as a Web service for remote processing.           Public Health Relevance This project attempts the construction of POET, a low-level preprocessing system for full text that can be used to open up large portions of the electronic health record (EHR) to high-level NLP systems. The potential public health implications are: 1) POET will allow the expansion of the use of well-proven clinical NLP systems  (currently limited to only a few document types found in the EHR) to the entire  clinical text record; with the entirety of the clinical record accessible to NLP,  serious and realistic attempts at real-time clinical text surveillance can be  mounted to improve patient safety and quality of care; 2) POET will be made available through open source distribution and other means  to encourage the practical deployment of innovative decision support systems  using large healthcare network EHRs across the country; 3) POET meets an important translational public health informatics need by solving  persistent low-level barriers to effective data mining of clinical marriages in the  EHR. The wider public health implications include promoting effective computerized decision support and data mining to improve both personal and public health outcomes.","POET: Consolidated, Comprehensive Clinical Text Preprocessing",7847940,R21LM009967,"['Abbreviations', 'Adverse event', 'Algorithms', 'Architecture', 'Body of uterus', 'Clinical', 'Clinical Pharmacists', 'Clinical Research', 'Computer Systems', 'Consult', 'Country', 'Data', 'Decision Support Systems', 'Development', 'Discipline', 'Discipline of Nursing', 'Electronic Health Record', 'Ensure', 'Excision', 'Hand', 'Healthcare', 'Internet', 'Java', 'Laboratories', 'Licensing', 'Linguistics', 'Literature', 'Marriage', 'Medical', 'Mind', 'Mining', 'Natural Language Processing', 'Nature', 'Nurses', 'Outcome', 'Output', 'Paste substance', 'Pathology', 'Pathology Report', 'Pharmacy facility', 'Physical assessment', 'Process', 'PubMed', 'Public Health', 'Public Health Informatics', 'Publishing', 'Quality of Care', 'Radiology Specialty', 'Report (document)', 'Reporting', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resolution', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'Study Section', 'System', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'Vocabulary', 'Work', 'Writing', 'abstracting', 'acronyms', 'base', 'caGrid', 'computerized', 'data mining', 'design', 'discrete data', 'improved', 'innovation', 'interest', 'meetings', 'novel', 'open source', 'patient safety', 'programs', 'public health relevance', 'spelling', 'tool', 'trend']",NLM,UNIVERSITY OF UTAH,R21,2009,84657,-0.022303224652472888
"POET: Consolidated, Comprehensive Clinical Text Preprocessing    DESCRIPTION (provided by applicant):       As electronic health records (EHRs) continue their expansion into clinical settings, there has been a corresponding increase in interest in mining the data they contain, both for research as well as for clinical decision support. Informaticists are increasingly studying ways to mine EHR textual content. This is an important trend, because there is a wealth of information contained in clinical text not represented anywhere else in the EHR. There is a low level text-as-data issue which presents a significant obstacle to the widespread use of available medical NLP systems: hand-typed clinical narratives in EHRs are usually ungrammatical; short or telegraphic in style; full of abbreviations, acronyms, and misspellings; formatted in a templated or pseudo-tabular form; and contain embedded non-text such as a list of laboratory values cut-and-pasted from elsewhere in the EHR. As we show in the Preliminary Studies Section, this makes high-level processing by popular tools like MedLEE and MetaMap effectively useless for all but a few ""clean"" document types like discharge summaries or consult reports (e.g., pathology or radiology reports). This in turn explains why there is so little published about what is certainly the preponderance of clinical texts, those that are not as well-behaved lexically and syntactically as a discharge summary.       In this application we distinguish clinical narratives (e.g., a progress note) from biomedical narratives (e.g., a PubMed abstract). We are interested in texts that arise in the clinical or research setting; texts that are composed by clinicians and researchers directly into a computer system. We propose to build and publish a tool called POET (Parsable Output Extracted from Text). POET will be designed to accept unstructured textual documents and return structured, linguistic equivalents that are, to the extent possible, parsable by higher-level NLP engines. POET will have an architecture is modular, extensible, and based on open-source platforms and sources (e.g., Java, Perl, UMLS, NegEx, the Stanford Parser, HL7 Clinical Document Architecture, caGRID, etc.). To implement POET, we will collect, program, and evaluate published as well as novel algorithms for: acronym/abbreviation resolution; spelling correction; template and pseudo-table re-writing; and removal of embedded non-text. To test POET we will use a large corpus of cross-discipline (e.g., medical, nursing, pharmacy, etc.) clinical note types, as well as the clinical research texts MedWatch reports and IRB adverse event reports. The development of POET will combine the best practices found in the literature and new research efforts as part of the project. To validate the fidelity of POET processing we plan a formal analysis of information loss and information gain pre- and post-process. To ensure broad access to the tools, POET will be released under an open-source license. Finally, we plan to assess the feasibility of offering POET as a Web service for remote processing.           Public Health Relevance This project attempts the construction of POET, a low-level preprocessing system for full text that can be used to open up large portions of the electronic health record (EHR) to high-level NLP systems. The potential public health implications are: 1) POET will allow the expansion of the use of well-proven clinical NLP systems  (currently limited to only a few document types found in the EHR) to the entire  clinical text record; with the entirety of the clinical record accessible to NLP,  serious and realistic attempts at real-time clinical text surveillance can be  mounted to improve patient safety and quality of care; 2) POET will be made available through open source distribution and other means  to encourage the practical deployment of innovative decision support systems  using large healthcare network EHRs across the country; 3) POET meets an important translational public health informatics need by solving  persistent low-level barriers to effective data mining of clinical marriages in the  EHR. The wider public health implications include promoting effective computerized decision support and data mining to improve both personal and public health outcomes.","POET: Consolidated, Comprehensive Clinical Text Preprocessing",7689273,R21LM009967,"['Abbreviations', 'Adverse event', 'Algorithms', 'Architecture', 'Body of uterus', 'Clinical', 'Clinical Pharmacists', 'Clinical Research', 'Computer Systems', 'Consult', 'Country', 'Data', 'Decision Support Systems', 'Development', 'Discipline', 'Discipline of Nursing', 'Electronic Health Record', 'Ensure', 'Excision', 'Hand', 'Healthcare', 'Internet', 'Java', 'Laboratories', 'Licensing', 'Linguistics', 'Literature', 'Marriage', 'Medical', 'Mind', 'Mining', 'Natural Language Processing', 'Nature', 'Nurses', 'Outcome', 'Output', 'Paste substance', 'Pathology', 'Pathology Report', 'Pharmacy facility', 'Physical assessment', 'Process', 'PubMed', 'Public Health', 'Public Health Informatics', 'Publishing', 'Quality of Care', 'Radiology Specialty', 'Report (document)', 'Reporting', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resolution', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'Study Section', 'System', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'Vocabulary', 'Work', 'Writing', 'abstracting', 'acronyms', 'base', 'caGrid', 'computerized', 'data mining', 'design', 'discrete data', 'improved', 'innovation', 'interest', 'meetings', 'novel', 'open source', 'patient safety', 'programs', 'public health relevance', 'spelling', 'tool', 'trend']",NLM,UNIVERSITY OF UTAH,R21,2009,166590,-0.022303224652472888
"POET: Consolidated, Comprehensive Clinical Text Preprocessing    DESCRIPTION (provided by applicant):       As electronic health records (EHRs) continue their expansion into clinical settings, there has been a corresponding increase in interest in mining the data they contain, both for research as well as for clinical decision support. Informaticists are increasingly studying ways to mine EHR textual content. This is an important trend, because there is a wealth of information contained in clinical text not represented anywhere else in the EHR. There is a low level text-as-data issue which presents a significant obstacle to the widespread use of available medical NLP systems: hand-typed clinical narratives in EHRs are usually ungrammatical; short or telegraphic in style; full of abbreviations, acronyms, and misspellings; formatted in a templated or pseudo-tabular form; and contain embedded non-text such as a list of laboratory values cut-and-pasted from elsewhere in the EHR. As we show in the Preliminary Studies Section, this makes high-level processing by popular tools like MedLEE and MetaMap effectively useless for all but a few ""clean"" document types like discharge summaries or consult reports (e.g., pathology or radiology reports). This in turn explains why there is so little published about what is certainly the preponderance of clinical texts, those that are not as well-behaved lexically and syntactically as a discharge summary.       In this application we distinguish clinical narratives (e.g., a progress note) from biomedical narratives (e.g., a PubMed abstract). We are interested in texts that arise in the clinical or research setting; texts that are composed by clinicians and researchers directly into a computer system. We propose to build and publish a tool called POET (Parsable Output Extracted from Text). POET will be designed to accept unstructured textual documents and return structured, linguistic equivalents that are, to the extent possible, parsable by higher-level NLP engines. POET will have an architecture is modular, extensible, and based on open-source platforms and sources (e.g., Java, Perl, UMLS, NegEx, the Stanford Parser, HL7 Clinical Document Architecture, caGRID, etc.). To implement POET, we will collect, program, and evaluate published as well as novel algorithms for: acronym/abbreviation resolution; spelling correction; template and pseudo-table re-writing; and removal of embedded non-text. To test POET we will use a large corpus of cross-discipline (e.g., medical, nursing, pharmacy, etc.) clinical note types, as well as the clinical research texts MedWatch reports and IRB adverse event reports. The development of POET will combine the best practices found in the literature and new research efforts as part of the project. To validate the fidelity of POET processing we plan a formal analysis of information loss and information gain pre- and post-process. To ensure broad access to the tools, POET will be released under an open-source license. Finally, we plan to assess the feasibility of offering POET as a Web service for remote processing.           Public Health Relevance This project attempts the construction of POET, a low-level preprocessing system for full text that can be used to open up large portions of the electronic health record (EHR) to high-level NLP systems. The potential public health implications are: 1) POET will allow the expansion of the use of well-proven clinical NLP systems  (currently limited to only a few document types found in the EHR) to the entire  clinical text record; with the entirety of the clinical record accessible to NLP,  serious and realistic attempts at real-time clinical text surveillance can be  mounted to improve patient safety and quality of care; 2) POET will be made available through open source distribution and other means  to encourage the practical deployment of innovative decision support systems  using large healthcare network EHRs across the country; 3) POET meets an important translational public health informatics need by solving  persistent low-level barriers to effective data mining of clinical marriages in the  EHR. The wider public health implications include promoting effective computerized decision support and data mining to improve both personal and public health outcomes.","POET: Consolidated, Comprehensive Clinical Text Preprocessing",7570254,R21LM009967,"['Abbreviations', 'Adverse event', 'Algorithms', 'Architecture', 'Body of uterus', 'Clinical', 'Clinical Pharmacists', 'Clinical Research', 'Computer Systems', 'Consult', 'Data', 'Development', 'Discipline', 'Discipline of Nursing', 'Electronic Health Record', 'Ensure', 'Excision', 'Hand', 'Internet', 'Java', 'Laboratories', 'Licensing', 'Linguistics', 'Literature', 'Medical', 'Mind', 'Mining', 'Natural Language Processing', 'Nature', 'Nurses', 'Output', 'Paste substance', 'Pathology', 'Pathology Report', 'Pharmacy facility', 'Physical assessment', 'Process', 'PubMed', 'Publishing', 'Radiology Specialty', 'Report (document)', 'Reporting', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resolution', 'Services', 'Source', 'Specific qualifier value', 'Standards of Weights and Measures', 'Structure', 'Study Section', 'System', 'Testing', 'Text', 'Thinking', 'Unified Medical Language System', 'Vocabulary', 'Work', 'Writing', 'abstracting', 'base', 'data mining', 'design', 'discrete data', 'interest', 'novel', 'open source', 'programs', 'spelling', 'tool', 'trend']",NLM,UNIVERSITY OF UTAH,R21,2008,169313,-0.022303224652472888
"Temporal relation discovery for clinical text ﻿    DESCRIPTION (provided by applicant):         The overarching long-term vision of our research is to create novel technologies for processing clinical free text. We will build upon the previous work of our ongoing project ""Temporal relation discovery for clinical text"" (R01LM010090) dubbed Temporal Histories of Your Medical Events (THYME; thyme.healthnlp.org) which has been focusing on methodology for event, temporal expressions and temporal relations discovery from the clinical text residing in the Electronic Health Records (EHR). We developed a comprehensive approach to temporality in the clinical text and innovated in computable temporal representations, methods for temporal relation discovery and their evaluation, rendering temporality to end users - resulting in over 35+ papers and presentations. Our dissemination is international and far-reaching as the best performing methods are released open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (ctakes.apache.org). The methods we developed are now being used in such nation-wide initiatives as the Electronic Medical Records and Genomics (eMERGE), Pharmacogenomics Network (PGRN), Informatics for Integrating the Biology and the Bedside (i2b2), Patient Centered Outcomes Research Institute and National Cancer Institute's Informatics Technology for Cancer Research (ITCR). Through our participation in organizing major international bakeoffs - CLEF/ShARe 2014, SemEval 2014 Analysis of Clinical Text Task 7, SemEval 2015 Analysis of Clinical Text Task 14, SemEval 2015 Clinical TempEval Task 6 - we further disseminated the THYME resources and challenged the international research community to explore new solutions to the unsolved temporality task. Through all these activities it became clear that computational approaches to temporality still present great challenges and usability of the output is still limited. Therefore, we propose to further innovate on methodologies and end user experience.             Specific Aim 1: Extract enhanced representations and novel features to support deriving timeline information.     Specific Aim 2: Develop methods to amalgamate individual patient episode timelines into an aggregate patient-level timeline.     Specific Aim 3: Mine the EHR - the unstructured clinical text and the structured codified information - for full patient-level temporality.     Specific Aim 4: Develop a comprehensive temporal visualization tool     Specific Aim 5: Develop methodology for and perform extrinsic evaluation on specific use case.     Specific Aim 6: (1) Evaluate state-of-the-art of temporal relations through organizing international challenges under the auspices of SemEval, (2) Disseminate the results through publications, presentations, and open source code in Apache cTAKES. Functional testing.             Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EHR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,8927274,R01LM010090,"['Apache Indians', 'Automobile Driving', 'Biology', 'Chronology', 'Clinical', 'Collection', 'Colon Carcinoma', 'Communication', 'Communities', 'Complex', 'Computerized Medical Record', 'Data', 'Data Set', 'Disease', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Evaluation', 'Event', 'Genomics', 'Goals', 'Human', 'Imagery', 'Individual', 'Informatics', 'Information Retrieval', 'International', 'Joints', 'Knowledge Extraction', 'Language', 'Life', 'Link', 'Machine Learning', 'Malignant neoplasm of brain', 'Medical', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Multiple Sclerosis', 'National Cancer Institute', 'Outcomes Research', 'Output', 'Paper', 'Patient-Focused Outcomes', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Process', 'Publications', 'Recording of previous events', 'Records', 'Research', 'Research Institute', 'Resolution', 'Resources', 'Science', 'Semantics', 'Signs and Symptoms', 'Solutions', 'Source Code', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Thyme', 'Time', 'TimeLine', 'Translating', 'Translational Research', 'Trees', 'Vision', 'Work', 'abstracting', 'anticancer research', 'autism spectrum disorder', 'clinically relevant', 'data mining', 'experience', 'innovation', 'new technology', 'next generation', 'novel', 'open source', 'syntax', 'tool', 'usability']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2015,720481,0.08457881723756211
"Temporal relation discovery for clinical text    DESCRIPTION (provided by applicant): The overarching long-term vision of our research is to create novel technologies for processing clinical free text. Such technologies will enable sophisticated and efficient indexing, retrieval and data mining over the ever increasing amounts of electronic clinical data. Processing free text poses a number of challenges to which the fields of Artificial intelligence, natural language processing and computer science in general have made advances. Methods for processing free text are informed by linguistic theory combined with the power of statistical inferencing. A key component to the next step, natural language understanding, is discovering events and their relations on a timeline. Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles.        The goal of our current proposal is to discover temporal relations from clinical free text through achieving four specific aims:        Specific Aim 1: Develop (1) a temporal relation annotation schema and guidelines for clinical free text based on TimeML, which will require extensions to Treebank, PropBank and VerbNet annotation guidelines to the clinical domain, (2) an annotated corpus following the temporal relations schema with additions to Treebank, PropBank and VerbNet, (3) a descriptive study comparing temporal relations in the clinical and general domains.        Specific Aim 2: Extend and evaluate existing methods and/or develop new algorithms for temporal relation discovery in the clinical domain. Component-level evaluation        Specific Aim 3: Integrate best method and/or a variety of methods for temporal relation discovery into the open source Mayo Clinic IE pipeline and release as open source annotators in the pipeline. Functional testing. Dissemination activities.        Specific Aim 4: System-level evaluation. Test the functionality of the enhanced Mayo Clinic IE pipeline on translational research use cases, e.g. the progression of colon cancer as documented in clinical notes and pathology reports, the progression of brain tumor as documented in radiology reports.        The methods we will use for the temporal relation discovery are based on machine learning, e.g., Support Vector Machine technology. Such methods require the annotation of a reference standard from which the computations are derived. The best methods will be released as part of the Mayo Clinic Information Extraction System for the larger community to use and contribute to. We will test the methods against biomedical queries.           Relevance (max 2-3 sentences) Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and create a timeline.",Temporal relation discovery for clinical text,7983243,R01LM010090,"['Algorithms', 'Artificial Intelligence', 'Automated Annotation', 'Brain Neoplasms', 'Clinic', 'Clinical', 'Clinical Data', 'Colon Carcinoma', 'Communities', 'Data', 'Development', 'Disease', 'Electronics', 'Evaluation', 'Event', 'Goals', 'Guidelines', 'Linguistics', 'Link', 'Machine Learning', 'Medical Records', 'Methods', 'Modeling', 'Natural Language Processing', 'Pathology Report', 'Performance', 'Process', 'Radiology Specialty', 'Reference Standards', 'Reporting', 'Research', 'Retrieval', 'Signs and Symptoms', 'System', 'Technology', 'Testing', 'Text', 'TimeLine', 'Translational Research', 'Vision', 'base', 'clinically relevant', 'computer science', 'data mining', 'indexing', 'new technology', 'next generation', 'open source', 'theories']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2010,775112,0.07026655803554208
"Temporal relation discovery for clinical text    DESCRIPTION (provided by applicant): The overarching long-term vision of our research is to create novel technologies for processing clinical free text. Such technologies will enable sophisticated and efficient indexing, retrieval and data mining over the ever increasing amounts of electronic clinical data. Processing free text poses a number of challenges to which the fields of Artificial intelligence, natural language processing and computer science in general have made advances. Methods for processing free text are informed by linguistic theory combined with the power of statistical inferencing. A key component to the next step, natural language understanding, is discovering events and their relations on a timeline. Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles.        The goal of our current proposal is to discover temporal relations from clinical free text through achieving four specific aims:        Specific Aim 1: Develop (1) a temporal relation annotation schema and guidelines for clinical free text based on TimeML, which will require extensions to Treebank, PropBank and VerbNet annotation guidelines to the clinical domain, (2) an annotated corpus following the temporal relations schema with additions to Treebank, PropBank and VerbNet, (3) a descriptive study comparing temporal relations in the clinical and general domains.        Specific Aim 2: Extend and evaluate existing methods and/or develop new algorithms for temporal relation discovery in the clinical domain. Component-level evaluation        Specific Aim 3: Integrate best method and/or a variety of methods for temporal relation discovery into the open source Mayo Clinic IE pipeline and release as open source annotators in the pipeline. Functional testing. Dissemination activities.        Specific Aim 4: System-level evaluation. Test the functionality of the enhanced Mayo Clinic IE pipeline on translational research use cases, e.g. the progression of colon cancer as documented in clinical notes and pathology reports, the progression of brain tumor as documented in radiology reports.        The methods we will use for the temporal relation discovery are based on machine learning, e.g., Support Vector Machine technology. Such methods require the annotation of a reference standard from which the computations are derived. The best methods will be released as part of the Mayo Clinic Information Extraction System for the larger community to use and contribute to. We will test the methods against biomedical queries.           Relevance (max 2-3 sentences) Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and create a timeline.",Temporal relation discovery for clinical text,8324662,R01LM010090,"['Algorithms', 'Artificial Intelligence', 'Automated Annotation', 'Brain Neoplasms', 'Clinic', 'Clinical', 'Clinical Data', 'Colon Carcinoma', 'Communities', 'Data', 'Development', 'Disease', 'Electronics', 'Evaluation', 'Event', 'Goals', 'Guidelines', 'Linguistics', 'Link', 'Machine Learning', 'Medical Records', 'Methods', 'Modeling', 'Natural Language Processing', 'Pathology Report', 'Performance', 'Process', 'Radiology Specialty', 'Reference Standards', 'Reporting', 'Research', 'Retrieval', 'Signs and Symptoms', 'System', 'Technology', 'Testing', 'Text', 'TimeLine', 'Translational Research', 'Vision', 'base', 'clinically relevant', 'computer science', 'data mining', 'indexing', 'new technology', 'next generation', 'open source', 'theories']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2012,745770,0.07026655803554208
"Temporal relation discovery for clinical text    DESCRIPTION (provided by applicant): The overarching long-term vision of our research is to create novel technologies for processing clinical free text. Such technologies will enable sophisticated and efficient indexing, retrieval and data mining over the ever increasing amounts of electronic clinical data. Processing free text poses a number of challenges to which the fields of Artificial intelligence, natural language processing and computer science in general have made advances. Methods for processing free text are informed by linguistic theory combined with the power of statistical inferencing. A key component to the next step, natural language understanding, is discovering events and their relations on a timeline. Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles.        The goal of our current proposal is to discover temporal relations from clinical free text through achieving four specific aims:        Specific Aim 1: Develop (1) a temporal relation annotation schema and guidelines for clinical free text based on TimeML, which will require extensions to Treebank, PropBank and VerbNet annotation guidelines to the clinical domain, (2) an annotated corpus following the temporal relations schema with additions to Treebank, PropBank and VerbNet, (3) a descriptive study comparing temporal relations in the clinical and general domains.        Specific Aim 2: Extend and evaluate existing methods and/or develop new algorithms for temporal relation discovery in the clinical domain. Component-level evaluation        Specific Aim 3: Integrate best method and/or a variety of methods for temporal relation discovery into the open source Mayo Clinic IE pipeline and release as open source annotators in the pipeline. Functional testing. Dissemination activities.        Specific Aim 4: System-level evaluation. Test the functionality of the enhanced Mayo Clinic IE pipeline on translational research use cases, e.g. the progression of colon cancer as documented in clinical notes and pathology reports, the progression of brain tumor as documented in radiology reports.        The methods we will use for the temporal relation discovery are based on machine learning, e.g., Support Vector Machine technology. Such methods require the annotation of a reference standard from which the computations are derived. The best methods will be released as part of the Mayo Clinic Information Extraction System for the larger community to use and contribute to. We will test the methods against biomedical queries.           Relevance (max 2-3 sentences) Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and create a timeline.",Temporal relation discovery for clinical text,8138604,R01LM010090,"['Algorithms', 'Artificial Intelligence', 'Automated Annotation', 'Brain Neoplasms', 'Clinic', 'Clinical', 'Clinical Data', 'Colon Carcinoma', 'Communities', 'Data', 'Development', 'Disease', 'Electronics', 'Evaluation', 'Event', 'Goals', 'Guidelines', 'Linguistics', 'Link', 'Machine Learning', 'Medical Records', 'Methods', 'Modeling', 'Natural Language Processing', 'Pathology Report', 'Performance', 'Process', 'Radiology Specialty', 'Reference Standards', 'Reporting', 'Research', 'Retrieval', 'Signs and Symptoms', 'System', 'Technology', 'Testing', 'Text', 'TimeLine', 'Translational Research', 'Vision', 'base', 'clinically relevant', 'computer science', 'data mining', 'indexing', 'new technology', 'next generation', 'open source', 'theories']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2011,695125,0.07026655803554208
"Temporal relation discovery for clinical text ﻿    DESCRIPTION (provided by applicant):         The overarching long-term vision of our research is to create novel technologies for processing clinical free text. We will build upon the previous work of our ongoing project ""Temporal relation discovery for clinical text"" (R01LM010090) dubbed Temporal Histories of Your Medical Events (THYME; thyme.healthnlp.org) which has been focusing on methodology for event, temporal expressions and temporal relations discovery from the clinical text residing in the Electronic Health Records (EHR). We developed a comprehensive approach to temporality in the clinical text and innovated in computable temporal representations, methods for temporal relation discovery and their evaluation, rendering temporality to end users - resulting in over 35+ papers and presentations. Our dissemination is international and far-reaching as the best performing methods are released open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (ctakes.apache.org). The methods we developed are now being used in such nation-wide initiatives as the Electronic Medical Records and Genomics (eMERGE), Pharmacogenomics Network (PGRN), Informatics for Integrating the Biology and the Bedside (i2b2), Patient Centered Outcomes Research Institute and National Cancer Institute's Informatics Technology for Cancer Research (ITCR). Through our participation in organizing major international bakeoffs - CLEF/ShARe 2014, SemEval 2014 Analysis of Clinical Text Task 7, SemEval 2015 Analysis of Clinical Text Task 14, SemEval 2015 Clinical TempEval Task 6 - we further disseminated the THYME resources and challenged the international research community to explore new solutions to the unsolved temporality task. Through all these activities it became clear that computational approaches to temporality still present great challenges and usability of the output is still limited. Therefore, we propose to further innovate on methodologies and end user experience.             Specific Aim 1: Extract enhanced representations and novel features to support deriving timeline information.     Specific Aim 2: Develop methods to amalgamate individual patient episode timelines into an aggregate patient-level timeline.     Specific Aim 3: Mine the EHR - the unstructured clinical text and the structured codified information - for full patient-level temporality.     Specific Aim 4: Develop a comprehensive temporal visualization tool     Specific Aim 5: Develop methodology for and perform extrinsic evaluation on specific use case.     Specific Aim 6: (1) Evaluate state-of-the-art of temporal relations through organizing international challenges under the auspices of SemEval, (2) Disseminate the results through publications, presentations, and open source code in Apache cTAKES. Functional testing. Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EHR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,9337497,R01LM010090,"['Apache', 'Automobile Driving', 'Biology', 'Chronology', 'Clinical', 'Collection', 'Colon Carcinoma', 'Communication', 'Communities', 'Complex', 'Computerized Medical Record', 'Data', 'Data Set', 'Disease', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Evaluation', 'Event', 'Genomics', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'Intuition', 'Joints', 'Knowledge Extraction', 'Language', 'Life', 'Link', 'Machine Learning', 'Malignant neoplasm of brain', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Cancer Institute', 'Output', 'Paper', 'Patient-Focused Outcomes', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Publications', 'Recording of previous events', 'Records', 'Research', 'Research Institute', 'Resolution', 'Resources', 'Science', 'Semantics', 'Signs and Symptoms', 'Source Code', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Thyme', 'Time', 'TimeLine', 'Translational Research', 'Trees', 'Vision', 'Visualization software', 'Work', 'anticancer research', 'autism spectrum disorder', 'clinically relevant', 'data mining', 'electronic structure', 'experience', 'individual patient', 'innovation', 'new technology', 'next generation', 'novel', 'open source', 'symptom treatment', 'syntax', 'usability']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2017,643621,0.08457881723756211
"Temporal relation discovery for clinical text ﻿    DESCRIPTION (provided by applicant):         The overarching long-term vision of our research is to create novel technologies for processing clinical free text. We will build upon the previous work of our ongoing project ""Temporal relation discovery for clinical text"" (R01LM010090) dubbed Temporal Histories of Your Medical Events (THYME; thyme.healthnlp.org) which has been focusing on methodology for event, temporal expressions and temporal relations discovery from the clinical text residing in the Electronic Health Records (EHR). We developed a comprehensive approach to temporality in the clinical text and innovated in computable temporal representations, methods for temporal relation discovery and their evaluation, rendering temporality to end users - resulting in over 35+ papers and presentations. Our dissemination is international and far-reaching as the best performing methods are released open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (ctakes.apache.org). The methods we developed are now being used in such nation-wide initiatives as the Electronic Medical Records and Genomics (eMERGE), Pharmacogenomics Network (PGRN), Informatics for Integrating the Biology and the Bedside (i2b2), Patient Centered Outcomes Research Institute and National Cancer Institute's Informatics Technology for Cancer Research (ITCR). Through our participation in organizing major international bakeoffs - CLEF/ShARe 2014, SemEval 2014 Analysis of Clinical Text Task 7, SemEval 2015 Analysis of Clinical Text Task 14, SemEval 2015 Clinical TempEval Task 6 - we further disseminated the THYME resources and challenged the international research community to explore new solutions to the unsolved temporality task. Through all these activities it became clear that computational approaches to temporality still present great challenges and usability of the output is still limited. Therefore, we propose to further innovate on methodologies and end user experience.             Specific Aim 1: Extract enhanced representations and novel features to support deriving timeline information.     Specific Aim 2: Develop methods to amalgamate individual patient episode timelines into an aggregate patient-level timeline.     Specific Aim 3: Mine the EHR - the unstructured clinical text and the structured codified information - for full patient-level temporality.     Specific Aim 4: Develop a comprehensive temporal visualization tool     Specific Aim 5: Develop methodology for and perform extrinsic evaluation on specific use case.     Specific Aim 6: (1) Evaluate state-of-the-art of temporal relations through organizing international challenges under the auspices of SemEval, (2) Disseminate the results through publications, presentations, and open source code in Apache cTAKES. Functional testing. Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EHR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,9146765,R01LM010090,"['Apache', 'Automobile Driving', 'Biology', 'Chronology', 'Clinical', 'Collection', 'Colon Carcinoma', 'Communication', 'Communities', 'Complex', 'Computerized Medical Record', 'Data', 'Data Set', 'Disease', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Evaluation', 'Event', 'Genomics', 'Goals', 'Human', 'Imagery', 'Informatics', 'Information Retrieval', 'International', 'Joints', 'Knowledge Extraction', 'Language', 'Life', 'Link', 'Machine Learning', 'Malignant neoplasm of brain', 'Medical', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Multiple Sclerosis', 'National Cancer Institute', 'Outcomes Research', 'Output', 'Paper', 'Patient-Focused Outcomes', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Process', 'Publications', 'Recording of previous events', 'Records', 'Research', 'Research Institute', 'Resolution', 'Resources', 'Science', 'Semantics', 'Signs and Symptoms', 'Source Code', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Thyme', 'Time', 'TimeLine', 'Translating', 'Translational Research', 'Trees', 'Vision', 'Work', 'abstracting', 'anticancer research', 'autism spectrum disorder', 'clinically relevant', 'data mining', 'experience', 'individual patient', 'innovation', 'new technology', 'next generation', 'novel', 'open source', 'symptom treatment', 'syntax', 'tool', 'usability']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2016,643863,0.08457881723756211
"OAMiner: Integrative Knowledge Anchored Hypothesis Discovery    DESCRIPTION (provided by applicant):The objective of this proposal, which is designed to address the ever increasing need for integrated knowledge discovery in biology and medicine, is to enable the discovery, verification, and validation of hypotheses concerning interrelationships between image-based, phenotypic, and bio-molecular features in heterogeneous data sets by leveraging multiple conceptual knowledge sources - ultimately supporting ""high throughput"" knowledge-driven translational science. To provide for a manageable project scope, Osteoarthritis Initiative (OAI) data sets will be used as a primary, motivating use case for the development and evaluation of the projected research products. This project necessarily involves analysis of initial hypotheses by subject matter experts (SMEs) for system training and verification. However, the ultimate goal of our proposed approach is to minimize the need for human intervention to identify or validate knowledge-anchored hypotheses. In order to generate such hypotheses, four interrelated knowledge sources are used: 1) full-text published bio-medical literature accessed by both conventional text mining and NLP analyses of articles as found in the Medline database and associated full text repositories; 2) publically available ontologies included in the National Library of Medicine's Unified Medical Language System (UMLS); 3) one or more databases containing phenotypic and functional (e.g. quality of life, psychological, strength and performance measures) data; and 4) computerized-image analysis derived features (e.g. cross-sectional area of the quadriceps).            Project Narrative: Public Health Relevance Statement Clinical and translational studies produce heterogeneous sources of data. For example, the Cancer Genome Atlas (TCGA) project is collecting tumor biospecimens together with clinical and histopathological data in order to understand the molecular basis of cancer through the application of genome analysis technologies. Similarly, the Osteoarthritis Initiative (OAI), a multi-center, longitudinal study is designed to assess the incidence and progression of knee osteoarthritis (OA) by collecting anthropometric, biochemical, genetic, and imaging procedures from 4796 enrollees. In these and many other similar studies, the collected data are made publicly available, yet, there is an acute lack of sufficient biomedical informatics tools to effectively discover, verify and validate hypotheses based upon the contents of these heterogeneous data sources. Given these characteristics of the modern research environment, an essential biomedical informatics challenge is to apply knowledge-anchored reasoning to heterogeneous and multi-dimensional data sets in order to discover novel hypotheses concerning such data that may be tested in order to maximize clinical impact and ultimately support broad public health interventions.",OAMiner: Integrative Knowledge Anchored Hypothesis Discovery,7715205,R01LM010119,"['Acute', 'Address', 'Area', 'Atlases', 'Biochemical Genetics', 'Biology', 'Characteristics', 'Clinical', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Degenerative polyarthritis', 'Development', 'Environment', 'Evaluation', 'Expert Systems', 'Genome', 'Goals', 'Human', 'Image', 'Image Analysis', 'Imaging Techniques', 'Incidence', 'Intervention', 'Knee Osteoarthritis', 'Knowledge', 'Literature', 'Longitudinal Studies', 'Malignant Neoplasms', 'Measures', 'Medical', 'Medicine', 'Molecular', 'Ontology', 'Performance', 'Public Health', 'Publishing', 'Quality of life', 'Research', 'Research Project Grants', 'Source', 'Technology', 'Testing', 'Text', 'Training', 'Translational Research', 'Unified Medical Language System', 'United States National Library of Medicine', 'base', 'biomedical informatics', 'cancer genome', 'computerized', 'design', 'novel', 'psychologic', 'public health relevance', 'repository', 'text searching', 'tool', 'translational study', 'tumor', 'verification and validation']",NLM,OHIO STATE UNIVERSITY,R01,2009,580358,0.01892206228695385
"OAMiner: Integrative Knowledge Anchored Hypothesis Discovery    DESCRIPTION (provided by applicant):The objective of this proposal, which is designed to address the ever increasing need for integrated knowledge discovery in biology and medicine, is to enable the discovery, verification, and validation of hypotheses concerning interrelationships between image-based, phenotypic, and bio-molecular features in heterogeneous data sets by leveraging multiple conceptual knowledge sources - ultimately supporting ""high throughput"" knowledge-driven translational science. To provide for a manageable project scope, Osteoarthritis Initiative (OAI) data sets will be used as a primary, motivating use case for the development and evaluation of the projected research products. This project necessarily involves analysis of initial hypotheses by subject matter experts (SMEs) for system training and verification. However, the ultimate goal of our proposed approach is to minimize the need for human intervention to identify or validate knowledge-anchored hypotheses. In order to generate such hypotheses, four interrelated knowledge sources are used: 1) full-text published bio-medical literature accessed by both conventional text mining and NLP analyses of articles as found in the Medline database and associated full text repositories; 2) publically available ontologies included in the National Library of Medicine's Unified Medical Language System (UMLS); 3) one or more databases containing phenotypic and functional (e.g. quality of life, psychological, strength and performance measures) data; and 4) computerized-image analysis derived features (e.g. cross-sectional area of the quadriceps).            Project Narrative: Public Health Relevance Statement Clinical and translational studies produce heterogeneous sources of data. For example, the Cancer Genome Atlas (TCGA) project is collecting tumor biospecimens together with clinical and histopathological data in order to understand the molecular basis of cancer through the application of genome analysis technologies. Similarly, the Osteoarthritis Initiative (OAI), a multi-center, longitudinal study is designed to assess the incidence and progression of knee osteoarthritis (OA) by collecting anthropometric, biochemical, genetic, and imaging procedures from 4796 enrollees. In these and many other similar studies, the collected data are made publicly available, yet, there is an acute lack of sufficient biomedical informatics tools to effectively discover, verify and validate hypotheses based upon the contents of these heterogeneous data sources. Given these characteristics of the modern research environment, an essential biomedical informatics challenge is to apply knowledge-anchored reasoning to heterogeneous and multi-dimensional data sets in order to discover novel hypotheses concerning such data that may be tested in order to maximize clinical impact and ultimately support broad public health interventions.",OAMiner: Integrative Knowledge Anchored Hypothesis Discovery,7828221,R01LM010119,"['Acute', 'Address', 'Area', 'Atlases', 'Biochemical Genetics', 'Biology', 'Characteristics', 'Clinical', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Degenerative polyarthritis', 'Development', 'Environment', 'Evaluation', 'Expert Systems', 'Genome', 'Goals', 'Human', 'Image', 'Image Analysis', 'Imaging Techniques', 'Incidence', 'Intervention', 'Knee Osteoarthritis', 'Knowledge', 'Literature', 'Longitudinal Studies', 'Malignant Neoplasms', 'Measures', 'Medical', 'Medicine', 'Molecular', 'Ontology', 'Performance', 'Public Health', 'Publishing', 'Quality of life', 'Research', 'Research Project Grants', 'Source', 'Technology', 'Testing', 'Text', 'Training', 'Translational Research', 'Unified Medical Language System', 'United States National Library of Medicine', 'base', 'biomedical informatics', 'cancer genome', 'computerized', 'design', 'novel', 'psychologic', 'public health relevance', 'quadriceps muscle', 'repository', 'text searching', 'tool', 'translational study', 'tumor', 'verification and validation']",NLM,OHIO STATE UNIVERSITY,R01,2010,584176,0.01892206228695385
"Automated Literature Mining for Validation of High-Throughput Function Prediction    DESCRIPTION (provided by applicant): The function of millions of proteins remains unknown, and automated protein function prediction systems have a poor record of performance. We will test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques through a novel automated system that will mine the literature for targeted information relevant to those predictions. The impact of our work will be to enable large-scale, validated, annotation of protein function and in turn to facilitate progress in tackling drug discovery for treatment of diseases.       High-throughput experiments and bioinformatics techniques are creating an exploding volume of data with which we hope to transcribe the genetic blueprints of life. Targeted experiments are required to validate biomedical discoveries from these sources. Fortunately, the information to confirm or refute a prediction is often already available in an existing publication and the biologist can take advantage of this supporting evidence for validation. However, the sheer volume of predictions from high throughput methods exceeds the capacity of researchers to perform even the necessary literature searches. This gap in capacity must be addressed using automated literature mining methods that perform comparably to a human expert; indeed, development of such methods is a grand challenge of modern Biology.       We will mine the full text literature to validate computational predictions of functional sites in proteins. The innovations in our approach include: (1) using computational predictions as the context for a literature search; (2) information extraction of protein functional sites from full text journal publications; (3) high-throughput text mining; and (4) using primary information in protein databases to evaluate the methods.       Understanding of protein function is a critical bottleneck in the progress of biomedical research. It is time to truly integrate the biological literature into the protein function prediction problem. By doing so, we will enable a critical advance in high-throughput protein function prediction           Project Narrative The goals of this research are to test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques. Our approach is to develop a revolutionary system that will automatically mine the literature for targeted information relevant to those predictions. We will produce reliable protein functional site predictions that can in turn be exploited for in silico high- throughput drug design.",Automated Literature Mining for Validation of High-Throughput Function Prediction,7843633,R01LM010120,"['Address', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Computational Biology', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Drug Design', 'Genetic', 'Goals', 'Human', 'Journals', 'Life', 'Literature', 'Methods', 'Mining', 'Performance', 'Protein Databases', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Site', 'Source', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Validation', 'Work', 'drug discovery', 'innovation', 'novel', 'protein function', 'research study', 'text searching']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,711389,0.07523123249889743
"Automated Literature Mining for Validation of High-Throughput Function Prediction    DESCRIPTION (provided by applicant): The function of millions of proteins remains unknown, and automated protein function prediction systems have a poor record of performance. We will test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques through a novel automated system that will mine the literature for targeted information relevant to those predictions. The impact of our work will be to enable large-scale, validated, annotation of protein function and in turn to facilitate progress in tackling drug discovery for treatment of diseases.       High-throughput experiments and bioinformatics techniques are creating an exploding volume of data with which we hope to transcribe the genetic blueprints of life. Targeted experiments are required to validate biomedical discoveries from these sources. Fortunately, the information to confirm or refute a prediction is often already available in an existing publication and the biologist can take advantage of this supporting evidence for validation. However, the sheer volume of predictions from high throughput methods exceeds the capacity of researchers to perform even the necessary literature searches. This gap in capacity must be addressed using automated literature mining methods that perform comparably to a human expert; indeed, development of such methods is a grand challenge of modern Biology.       We will mine the full text literature to validate computational predictions of functional sites in proteins. The innovations in our approach include: (1) using computational predictions as the context for a literature search; (2) information extraction of protein functional sites from full text journal publications; (3) high-throughput text mining; and (4) using primary information in protein databases to evaluate the methods.       Understanding of protein function is a critical bottleneck in the progress of biomedical research. It is time to truly integrate the biological literature into the protein function prediction problem. By doing so, we will enable a critical advance in high-throughput protein function prediction           Project Narrative The goals of this research are to test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques. Our approach is to develop a revolutionary system that will automatically mine the literature for targeted information relevant to those predictions. We will produce reliable protein functional site predictions that can in turn be exploited for in silico high- throughput drug design.",Automated Literature Mining for Validation of High-Throughput Function Prediction,8144625,R01LM010120,"['Address', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Computational Biology', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Drug Design', 'Genetic', 'Goals', 'Human', 'Journals', 'Life', 'Literature', 'Methods', 'Mining', 'Performance', 'Protein Databases', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Site', 'Source', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Validation', 'Work', 'drug discovery', 'innovation', 'novel', 'protein function', 'research study', 'text searching']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,80113,0.07523123249889743
"Automated Literature Mining for Validation of High-Throughput Function Prediction    DESCRIPTION (provided by applicant): The function of millions of proteins remains unknown, and automated protein function prediction systems have a poor record of performance. We will test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques through a novel automated system that will mine the literature for targeted information relevant to those predictions. The impact of our work will be to enable large-scale, validated, annotation of protein function and in turn to facilitate progress in tackling drug discovery for treatment of diseases.       High-throughput experiments and bioinformatics techniques are creating an exploding volume of data with which we hope to transcribe the genetic blueprints of life. Targeted experiments are required to validate biomedical discoveries from these sources. Fortunately, the information to confirm or refute a prediction is often already available in an existing publication and the biologist can take advantage of this supporting evidence for validation. However, the sheer volume of predictions from high throughput methods exceeds the capacity of researchers to perform even the necessary literature searches. This gap in capacity must be addressed using automated literature mining methods that perform comparably to a human expert; indeed, development of such methods is a grand challenge of modern Biology.       We will mine the full text literature to validate computational predictions of functional sites in proteins. The innovations in our approach include: (1) using computational predictions as the context for a literature search; (2) information extraction of protein functional sites from full text journal publications; (3) high-throughput text mining; and (4) using primary information in protein databases to evaluate the methods.       Understanding of protein function is a critical bottleneck in the progress of biomedical research. It is time to truly integrate the biological literature into the protein function prediction problem. By doing so, we will enable a critical advance in high-throughput protein function prediction           Project Narrative The goals of this research are to test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques. Our approach is to develop a revolutionary system that will automatically mine the literature for targeted information relevant to those predictions. We will produce reliable protein functional site predictions that can in turn be exploited for in silico high- throughput drug design.",Automated Literature Mining for Validation of High-Throughput Function Prediction,7724794,R01LM010120,"['Address', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Computational Biology', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Drug Design', 'Genetic', 'Goals', 'Human', 'Journals', 'Life', 'Literature', 'Methods', 'Mining', 'Performance', 'Protein Databases', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Site', 'Source', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Validation', 'Work', 'drug discovery', 'innovation', 'novel', 'protein function', 'research study', 'text searching']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2009,721448,0.07523123249889743
"Improved manuscript search through PubSeq    DESCRIPTION (provided by applicant): Most top-level searches of scientific literature include querying of structured fields such as author, subject, or affiliation. A free-text search of abstracts or full texts entries would be more flexible allowing queries with any word combination including ranges of names and identifiers. Unfortunately, free text searches usually yield incomplete and often erroneous results since the naming of biologically important molecules (genes, proteins, substrates) is not standardized. Unless a specific query issued to a retrieval service (e.g. PubMed) covers all possible aliases of a given protein or gene the results may be insufficient or simply wrong. The system proposed here translates the problem of looking up literature pertaining to a certain protein to the sequence level. By correlating existing identifiers, names, and synonyms of proteins with their sequences this lookup increases the accuracy and coverage of the results. A particular challenge that our system will uniquely address is the following. Increasingly structural and functional genomics projects bring up proteins for which nothing is known. If someone published some new experimental that will actually name such a protein, this important knowledge will likely be lost to the genomics investigator because PubMed alarms need to be activated by keywords and names. Our system could fill in the gap: users will be able to deposit sequences corresponding to proteins of unknown function/name. If experimental information will be published for the same or a related sequence the original investigator will be notified.    PUBLIC HEALTH RELEVANCE: The experimental and computational data appearing daily in publications is critical to the advancement of biological research. However, the sheer quantity and high frequency in which new data is published turns bench scientists into research librarians trying to sift through the flood of information while searching for relevant and reliable data. Furthermore, as biological research is increasingly driven by the study of proteins and genes that mostly lack annotations, or even an identifiers, there is a need to access the literature by using sequence data alone. By automating the process of searching and discovering relevant information as it becomes available, the proposed system promises to save time and increase the coverage of relevant and reliable data retrieved by a given search in an intuitive and ""easy to consume"" format.           Relevance: The experimental and computational data appearing daily in publications is critical to the advancement of biological research. However, the sheer quantity and high frequency in which new data is published turns bench scientists into research librarians trying to sift through the flood of information while searching for relevant and reliable data. Furthermore as biological research is increasingly driven by the study of proteins and genes that mostly lack annotations, or even an identifiers, there is a need to access the literature by using sequence data alone. By automating the process of searching and discovering relevant information as it becomes available, the proposed system promises to save time and increase the coverage of relevant and reliable data retrieved by a given search in an intuitive and ""easy to consume"" format.",Improved manuscript search through PubSeq,7748592,R43LM010156,"['Address', 'Amino Acid Sequence', 'Arts', 'Base Sequence', 'Blast Cell', 'Companions', 'DNA Sequence', 'Data', 'Databases', 'Deposition', 'Development', 'Drosophila genus', 'Floods', 'Frequencies', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Knowledge', 'Librarians', 'Literature', 'Malaria', 'Manuscripts', 'Maps', 'Monitor', 'Names', 'Notification', 'Peptide Sequence Determination', 'Process', 'Proteins', 'Protocols documentation', 'PubMed', 'Publications', 'Publishing', 'Records', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Services', 'Structure', 'System', 'Text', 'Time', 'Translating', 'Update', 'abstracting', 'base', 'biological research', 'flexibility', 'functional/structural genomics', 'improved', 'malignant breast neoplasm', 'public health relevance', 'repository', 'research study', 'text searching', 'tool']",NLM,"BIOSOF, LLC",R43,2009,97198,0.0862426229901241
"Linking Text Mining and Data Mining for Biomedical Knowledge Discovery    DESCRIPTION (provided by applicant):       Systems integration is becoming the driving force for the 21st century biology. Researchers are systematically tackling gene functions and complex regulatory processes by studying organisms at different levels of organization, from genomes, transcriptomes and proteomes to metabolomes and interactomes. To fully realize the value of such high-throughput data requires advanced bioinformatics for integration, mining, comparative analysis, and functional interpretation. Furthermore, with an ever-increasing volume of biomedical literature now available electronically, there is both a pressing need and a great opportunity to fully utilize text mining tools for knowledge extraction. However, despite recent advancements, text mining tools are not being broadly used by biologists. Such a gap is partly due to the lack of close interactions between the text mining and the biological user communities. The goal of this application is to develop a digital research infrastructure that links text mining with data mining in the systems biology context for biomedical knowledge discovery, with a special focus on the utility and usability of the system for real world scientific applications. Building upon the bioinformatics framework we have already developed, as well as our close interactions with the biomedical research community, the specific aims are to: (i) integrate existing text mining tools to identify and extract protein and network information from scientific literature, (ii) connect text mining and data mining with omics data integration and web interface to capture and visualize network knowledge, and (iii) conduct user studies, develop scientific use cases, provide training and outreach, and disseminate the system to the broad biomedical user community. The digital information resource proposed herein will serve as an enabling environment for biomedical researchers to decipher knowledge from a plethora of information available in the literature and public databases, gaining a better understanding of biological and disease processes as a key to the basic understanding of human health and disease.           NARRATIVE The proposed digital information resource will serve as an enabling environment for biomedical researchers to gain a better understanding of biological and disease processes in the systems biology context, thereby facilitating target discovery and disease diagnosis, and translating ""bench"" knowledge into ""bedside"" benefits.",Linking Text Mining and Data Mining for Biomedical Knowledge Discovery,8318246,G08LM010720,"['Adopted', 'Architecture', 'Benchmarking', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Communities', 'Complex', 'Computer software', 'Controlled Vocabulary', 'Data', 'Data Quality', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Education and Outreach', 'Educational workshop', 'Environment', 'Evaluation', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Goals', 'Health', 'Human', 'Imagery', 'Informatics', 'Information Networks', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Link', 'Literature', 'Maps', 'Methods', 'Metric', 'Mining', 'Molecular', 'Names', 'Online Systems', 'Ontology', 'Organism', 'Pathway interactions', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteome', 'PubMed', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Semantics', 'Services', 'Signal Pathway', 'Standardization', 'Surveys', 'System', 'Systems Biology', 'Systems Integration', 'Training', 'Translating', 'base', 'comparative', 'data integration', 'data mining', 'digital', 'disease diagnosis', 'driving force', 'gene function', 'knowledge base', 'open source', 'outreach', 'protein protein interaction', 'text searching', 'therapeutic target', 'tool', 'usability', 'web interface']",NLM,UNIVERSITY OF DELAWARE,G08,2012,143942,0.15996836166131573
"Linking Text Mining and Data Mining for Biomedical Knowledge Discovery    DESCRIPTION (provided by applicant):       Systems integration is becoming the driving force for the 21st century biology. Researchers are systematically tackling gene functions and complex regulatory processes by studying organisms at different levels of organization, from genomes, transcriptomes and proteomes to metabolomes and interactomes. To fully realize the value of such high-throughput data requires advanced bioinformatics for integration, mining, comparative analysis, and functional interpretation. Furthermore, with an ever-increasing volume of biomedical literature now available electronically, there is both a pressing need and a great opportunity to fully utilize text mining tools for knowledge extraction. However, despite recent advancements, text mining tools are not being broadly used by biologists. Such a gap is partly due to the lack of close interactions between the text mining and the biological user communities. The goal of this application is to develop a digital research infrastructure that links text mining with data mining in the systems biology context for biomedical knowledge discovery, with a special focus on the utility and usability of the system for real world scientific applications. Building upon the bioinformatics framework we have already developed, as well as our close interactions with the biomedical research community, the specific aims are to: (i) integrate existing text mining tools to identify and extract protein and network information from scientific literature, (ii) connect text mining and data mining with omics data integration and web interface to capture and visualize network knowledge, and (iii) conduct user studies, develop scientific use cases, provide training and outreach, and disseminate the system to the broad biomedical user community. The digital information resource proposed herein will serve as an enabling environment for biomedical researchers to decipher knowledge from a plethora of information available in the literature and public databases, gaining a better understanding of biological and disease processes as a key to the basic understanding of human health and disease.           NARRATIVE The proposed digital information resource will serve as an enabling environment for biomedical researchers to gain a better understanding of biological and disease processes in the systems biology context, thereby facilitating target discovery and disease diagnosis, and translating ""bench"" knowledge into ""bedside"" benefits.",Linking Text Mining and Data Mining for Biomedical Knowledge Discovery,8130991,G08LM010720,"['Adopted', 'Architecture', 'Benchmarking', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Communities', 'Complex', 'Computer software', 'Controlled Vocabulary', 'Data', 'Data Quality', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Education and Outreach', 'Educational workshop', 'Environment', 'Evaluation', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Goals', 'Health', 'Human', 'Imagery', 'Informatics', 'Information Networks', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Link', 'Literature', 'Maps', 'Methods', 'Metric', 'Mining', 'Molecular', 'Names', 'Online Systems', 'Ontology', 'Organism', 'Pathway interactions', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteome', 'PubMed', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Semantics', 'Services', 'Signal Pathway', 'Standardization', 'Surveys', 'System', 'Systems Biology', 'Systems Integration', 'Training', 'Translating', 'base', 'comparative', 'data integration', 'data mining', 'digital', 'disease diagnosis', 'driving force', 'gene function', 'knowledge base', 'open source', 'outreach', 'protein protein interaction', 'text searching', 'therapeutic target', 'tool', 'usability', 'web interface']",NLM,UNIVERSITY OF DELAWARE,G08,2011,144000,0.15996836166131573
"Linking Text Mining and Data Mining for Biomedical Knowledge Discovery    DESCRIPTION (provided by applicant):       Systems integration is becoming the driving force for the 21st century biology. Researchers are systematically tackling gene functions and complex regulatory processes by studying organisms at different levels of organization, from genomes, transcriptomes and proteomes to metabolomes and interactomes. To fully realize the value of such high-throughput data requires advanced bioinformatics for integration, mining, comparative analysis, and functional interpretation. Furthermore, with an ever-increasing volume of biomedical literature now available electronically, there is both a pressing need and a great opportunity to fully utilize text mining tools for knowledge extraction. However, despite recent advancements, text mining tools are not being broadly used by biologists. Such a gap is partly due to the lack of close interactions between the text mining and the biological user communities. The goal of this application is to develop a digital research infrastructure that links text mining with data mining in the systems biology context for biomedical knowledge discovery, with a special focus on the utility and usability of the system for real world scientific applications. Building upon the bioinformatics framework we have already developed, as well as our close interactions with the biomedical research community, the specific aims are to: (i) integrate existing text mining tools to identify and extract protein and network information from scientific literature, (ii) connect text mining and data mining with omics data integration and web interface to capture and visualize network knowledge, and (iii) conduct user studies, develop scientific use cases, provide training and outreach, and disseminate the system to the broad biomedical user community. The digital information resource proposed herein will serve as an enabling environment for biomedical researchers to decipher knowledge from a plethora of information available in the literature and public databases, gaining a better understanding of biological and disease processes as a key to the basic understanding of human health and disease.           NARRATIVE The proposed digital information resource will serve as an enabling environment for biomedical researchers to gain a better understanding of biological and disease processes in the systems biology context, thereby facilitating target discovery and disease diagnosis, and translating ""bench"" knowledge into ""bedside"" benefits.",Linking Text Mining and Data Mining for Biomedical Knowledge Discovery,7886453,G08LM010720,"['Adopted', 'Architecture', 'Benchmarking', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Communities', 'Complex', 'Computer software', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Quality', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Education and Outreach', 'Educational workshop', 'Environment', 'Evaluation', 'Future', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Goals', 'Health', 'Human', 'Imagery', 'Informatics', 'Information Networks', 'Information Resources', 'Knowledge', 'Knowledge Extraction', 'Link', 'Literature', 'Maps', 'Methods', 'Metric', 'Mining', 'Molecular', 'Names', 'Online Systems', 'Ontology', 'Organism', 'Pathway interactions', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteome', 'PubMed', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Semantics', 'Services', 'Signal Pathway', 'Standardization', 'Surveys', 'System', 'Systems Biology', 'Systems Integration', 'Training', 'Translating', 'base', 'comparative', 'data integration', 'data mining', 'digital', 'disease diagnosis', 'driving force', 'gene function', 'knowledge base', 'open source', 'outreach', 'protein protein interaction', 'text searching', 'therapeutic target', 'tool', 'usability', 'web interface']",NLM,UNIVERSITY OF DELAWARE,G08,2010,150000,0.15996836166131573
"POET-2: High-performance computing for advanced clinical narrative preprocessing    DESCRIPTION (provided by applicant):       This project focuses on clinical natural language processing (cNLP), a field of emerging importance in informatics. Starting with the Linguistic String Project's Medical Language Processor (New York University) in the 1970s, researchers have made steady gains in cNLP through empirical studies and by building sophisticated high-level cNLP software applications (e.g., Columbia's MedLEE). There are no fewer than four scientific conferences devoted exclusively to biomedical/clinical NLP. The cNLP literature has been growing over the past decade, and this will gain momentum as more clinical text repositories are released, such as the MIMIC II and University of Pittsburgh BLU Lab corpora.       However, sustained success in the field of cNLP is hampered by the reality that clinical texts have a far more noise than do texts traditionally studied in NLP, such as newswire articles, biomedical abstracts, and discharge summaries. Noise in this context is defined by the parseability characteristics of the language and the linguistic structures that appear in text. Clinical texts come in a striking variety of note types, with the best studied types being discharge summaries, radiology reports, and pathology reports. These note types share an important feature: they are written to communicate care issues between healthcare providers and hence typically are well-composed, well-edited, and often are dictated. But the vast majority of notes in the electronic health record are written primarily to document care issues. They communicate as well, of course, but much less care is used in their creation than with discharge summaries and reports. As a result they are often ungrammatical; are composed of short, telegraphic phrases; are replete with misspellings and shorthand (e.g., abbreviations); are ill-formatted with templates and liberal use of white space; and are embedded with ""non-prose"" (e.g., strings of laboratory values). All of these sources of noise complicate otherwise straightforward NLP tasks like tokenization, sentence segmentation, and ultimately information extraction itself.       We propose a systematic study of ways to increase the signal-to-noise ratio in clinical narratives to improve cNLP. This work extends our preliminary research (under the POET project) and has the following aims:        o Develop and implement a suite of parseability improvement tools designed for all clinical note types from multiple healthcare institutions.     o Evaluate the empirical and the functional success of the parseability improvement tools.     o Design and implement a HIPAA-compliant UlMA-based pipeline cNLP framework for use in a typical high-performance, multi-processor computing environment.              Project Narrative We can see in the multi-billion dollar investment in electronic health records (EHRs) by the ARRA that mining clinical data electronically will continue to be essential to informatics research. Most data in the EHR resides as unstructured text, and POET2 provides a means to unlock that data through combining a new, HIPAA- complaint high-performance computing architecture with sophisticated text preprocessing.",POET-2: High-performance computing for advanced clinical narrative preprocessing,8536940,R01LM010981,"['Abbreviations', 'Active Learning', 'Address', 'Architecture', 'Area', 'Authorization documentation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Data', 'Computer software', 'Data', 'Electronic Health Record', 'Electronics', 'Employee Strikes', 'Ensure', 'Environment', 'Evaluation', 'Face', 'Gold', 'Growth', 'Health Care Reform', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'High Performance Computing', 'Informatics', 'Inpatients', 'Institution', 'Institutional Review Boards', 'Investments', 'Laboratories', 'Language', 'Linguistics', 'Literature', 'Maps', 'Medical', 'Mining', 'Modeling', 'Natural Language Processing', 'New York', 'Noise', 'Occupations', 'Outpatients', 'Paper', 'Pathology', 'Pathology Report', 'Patients', 'Performance', 'Proliferating', 'Publishing', 'Radiology Specialty', 'Records', 'Report (document)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Support', 'Resolution', 'Series', 'Shorthand', 'Signal Transduction', 'Source', 'Structure', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'Voting', 'Work', 'Writing', 'abstracting', 'base', 'cluster computing', 'data mining', 'design', 'improved', 'meetings', 'novel', 'phrases', 'pressure', 'repaired', 'repository', 'research study', 'success', 'symposium', 'tool', 'web services']",NLM,UNIVERSITY OF UTAH,R01,2013,292186,-0.0039745491572586245
"POET-2: High-performance computing for advanced clinical narrative preprocessing    DESCRIPTION (provided by applicant):       This project focuses on clinical natural language processing (cNLP), a field of emerging importance in informatics. Starting with the Linguistic String Project's Medical Language Processor (New York University) in the 1970s, researchers have made steady gains in cNLP through empirical studies and by building sophisticated high-level cNLP software applications (e.g., Columbia's MedLEE). There are no fewer than four scientific conferences devoted exclusively to biomedical/clinical NLP. The cNLP literature has been growing over the past decade, and this will gain momentum as more clinical text repositories are released, such as the MIMIC II and University of Pittsburgh BLU Lab corpora.       However, sustained success in the field of cNLP is hampered by the reality that clinical texts have a far more noise than do texts traditionally studied in NLP, such as newswire articles, biomedical abstracts, and discharge summaries. Noise in this context is defined by the parseability characteristics of the language and the linguistic structures that appear in text. Clinical texts come in a striking variety of note types, with the best studied types being discharge summaries, radiology reports, and pathology reports. These note types share an important feature: they are written to communicate care issues between healthcare providers and hence typically are well-composed, well-edited, and often are dictated. But the vast majority of notes in the electronic health record are written primarily to document care issues. They communicate as well, of course, but much less care is used in their creation than with discharge summaries and reports. As a result they are often ungrammatical; are composed of short, telegraphic phrases; are replete with misspellings and shorthand (e.g., abbreviations); are ill-formatted with templates and liberal use of white space; and are embedded with ""non-prose"" (e.g., strings of laboratory values). All of these sources of noise complicate otherwise straightforward NLP tasks like tokenization, sentence segmentation, and ultimately information extraction itself.       We propose a systematic study of ways to increase the signal-to-noise ratio in clinical narratives to improve cNLP. This work extends our preliminary research (under the POET project) and has the following aims:        o Develop and implement a suite of parseability improvement tools designed for all clinical note types from multiple healthcare institutions.     o Evaluate the empirical and the functional success of the parseability improvement tools.     o Design and implement a HIPAA-compliant UlMA-based pipeline cNLP framework for use in a typical high-performance, multi-processor computing environment.              Project Narrative We can see in the multi-billion dollar investment in electronic health records (EHRs) by the ARRA that mining clinical data electronically will continue to be essential to informatics research. Most data in the EHR resides as unstructured text, and POET2 provides a means to unlock that data through combining a new, HIPAA- complaint high-performance computing architecture with sophisticated text preprocessing.",POET-2: High-performance computing for advanced clinical narrative preprocessing,8326648,R01LM010981,"['Abbreviations', 'Active Learning', 'Address', 'Architecture', 'Area', 'Authorization documentation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Data', 'Computer software', 'Data', 'Electronic Health Record', 'Electronics', 'Employee Strikes', 'Ensure', 'Environment', 'Evaluation', 'Face', 'Gold', 'Growth', 'Health Care Reform', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'High Performance Computing', 'Informatics', 'Inpatients', 'Institution', 'Institutional Review Boards', 'Investments', 'Laboratories', 'Language', 'Linguistics', 'Literature', 'Maps', 'Medical', 'Mining', 'Modeling', 'Natural Language Processing', 'New York', 'Noise', 'Occupations', 'Outpatients', 'Paper', 'Pathology', 'Pathology Report', 'Patients', 'Performance', 'Proliferating', 'Publishing', 'Radiology Specialty', 'Records', 'Report (document)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Support', 'Resolution', 'Series', 'Shorthand', 'Signal Transduction', 'Source', 'Structure', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'Voting', 'Work', 'Writing', 'abstracting', 'base', 'cluster computing', 'data mining', 'design', 'improved', 'meetings', 'novel', 'phrases', 'pressure', 'repaired', 'repository', 'research study', 'success', 'symposium', 'tool', 'web services']",NLM,UNIVERSITY OF UTAH,R01,2012,318393,-0.0039745491572586245
"POET-2: High-performance computing for advanced clinical narrative preprocessing    DESCRIPTION (provided by applicant):       This project focuses on clinical natural language processing (cNLP), a field of emerging importance in informatics. Starting with the Linguistic String Project's Medical Language Processor (New York University) in the 1970s, researchers have made steady gains in cNLP through empirical studies and by building sophisticated high-level cNLP software applications (e.g., Columbia's MedLEE). There are no fewer than four scientific conferences devoted exclusively to biomedical/clinical NLP. The cNLP literature has been growing over the past decade, and this will gain momentum as more clinical text repositories are released, such as the MIMIC II and University of Pittsburgh BLU Lab corpora.       However, sustained success in the field of cNLP is hampered by the reality that clinical texts have a far more noise than do texts traditionally studied in NLP, such as newswire articles, biomedical abstracts, and discharge summaries. Noise in this context is defined by the parseability characteristics of the language and the linguistic structures that appear in text. Clinical texts come in a striking variety of note types, with the best studied types being discharge summaries, radiology reports, and pathology reports. These note types share an important feature: they are written to communicate care issues between healthcare providers and hence typically are well-composed, well-edited, and often are dictated. But the vast majority of notes in the electronic health record are written primarily to document care issues. They communicate as well, of course, but much less care is used in their creation than with discharge summaries and reports. As a result they are often ungrammatical; are composed of short, telegraphic phrases; are replete with misspellings and shorthand (e.g., abbreviations); are ill-formatted with templates and liberal use of white space; and are embedded with ""non-prose"" (e.g., strings of laboratory values). All of these sources of noise complicate otherwise straightforward NLP tasks like tokenization, sentence segmentation, and ultimately information extraction itself.       We propose a systematic study of ways to increase the signal-to-noise ratio in clinical narratives to improve cNLP. This work extends our preliminary research (under the POET project) and has the following aims:        o Develop and implement a suite of parseability improvement tools designed for all clinical note types from multiple healthcare institutions.     o Evaluate the empirical and the functional success of the parseability improvement tools.     o Design and implement a HIPAA-compliant UlMA-based pipeline cNLP framework for use in a typical high-performance, multi-processor computing environment.              Project Narrative We can see in the multi-billion dollar investment in electronic health records (EHRs) by the ARRA that mining clinical data electronically will continue to be essential to informatics research. Most data in the EHR resides as unstructured text, and POET2 provides a means to unlock that data through combining a new, HIPAA- complaint high-performance computing architecture with sophisticated text preprocessing.",POET-2: High-performance computing for advanced clinical narrative preprocessing,8182025,R01LM010981,"['Abbreviations', 'Active Learning', 'Address', 'Architecture', 'Area', 'Authorization documentation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Data', 'Computer software', 'Data', 'Electronic Health Record', 'Electronics', 'Employee Strikes', 'Ensure', 'Environment', 'Evaluation', 'Face', 'Gold', 'Growth', 'Health Care Reform', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'High Performance Computing', 'Informatics', 'Inpatients', 'Institution', 'Institutional Review Boards', 'Investments', 'Laboratories', 'Language', 'Linguistics', 'Literature', 'Maps', 'Medical', 'Mining', 'Modeling', 'Natural Language Processing', 'New York', 'Noise', 'Occupations', 'Outpatients', 'Paper', 'Pathology', 'Pathology Report', 'Patients', 'Performance', 'Proliferating', 'Publishing', 'Radiology Specialty', 'Records', 'Report (document)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Support', 'Resolution', 'Series', 'Shorthand', 'Signal Transduction', 'Source', 'Structure', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'Voting', 'Work', 'Writing', 'abstracting', 'base', 'cluster computing', 'data mining', 'design', 'improved', 'meetings', 'novel', 'phrases', 'pressure', 'repaired', 'repository', 'research study', 'success', 'symposium', 'tool', 'web services']",NLM,UNIVERSITY OF UTAH,R01,2011,325163,-0.0039745491572586245
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8714053,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2014,307471,0.11561978611762719
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8535824,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2013,291730,0.11561978611762719
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8326650,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2012,317212,0.11561978611762719
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8202896,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2011,312599,0.11561978611762719
"Utilizing Imaged-based Features in Biomedical Literature Classification DESCRIPTION (provided by applicant): The proposed research aims to support and improve effective access to the biomedical literature, by utilizing the rich, highly-informative image data within publications, in addition to text. The biomedical literature is expanding at a rate of about  1,000,000 new publications a year. Scientists and physicians, as part of their daily work, go through a myriad of publications searching for relevant information. The task is even more arduous for scientific database curators (bio- curators, in organizations such as FlyBase or UniProt), who have to identify the literature most relevant to the database area, locate within it high-quality evidence concerning genes, proteins, organisms, or disease, and curate the findings within a database entry, with references to the relevant literature. Notably, much of the evidence within publications lies in figures. Accordingly, images are used by scientists and database curators as indicators for relevance.  To assist and expedite the search for information within the literature, automated text-mining tools are being developed; still, several shared tasks and competitive challenges demonstrated that the need for more effective automated identification of relevant information in biomedical publications remains a bottleneck for bio-curation and for scientific discovery. While image analysis within and outside the biomedical domain is an active research area, most current work on biomedical image processing focuses on retrieval and understanding of images as a primary form of data. Likewise, most efforts on biomedical literature retrieval and mining focus on text alone. Little has been done so far to use images within publications, which provide important cues as to the relevance of information embedded in papers.  The hypothesis underlying our proposal is that useful information can be derived directly from images within publications and integrated with text-based methods, leading to improved identification of relevant publications and of informative portions within them. The proposed research comprises extensive comparative study of highly-informative features within images, development and identification of such image-features, development of tools that extract such features and information from images, and integration of image-based information into the textual articles-classification process, aiming to determine the publications' relevance to well-defined biomedical needs. The fundamental research tasks we shall address are: A) Identification and comparative study of useful features for image-representation, focusing on their utility for specific biomedical needs; B) Classification of biomedical images and biomedical documents based on image-data; C) Document classification through integration of text- and image-based classifiers. To ground the research in genuine needs, secure access to much image data, and ensure broad-applicability of the results, we shall work within three diverse areas for which we have secured access to expertise and data: Finding articles about cis-regulatory regions (Cyrene project at Brown University); Evidence for gene expression in the mouse (Jackson Lab's GXD); Experimental evidence for protein-protein interaction (Delaware's Protein Information Resource). The successful completion of the proposed project will provide integrated methods and tools, utilizing both image-based and text-based features, leading to more focused and effective retrieval and mining tools, thus better supporting data-intensive biomedical discovery. Physicians and bio-medical scientists rely on the vast published literature as their main source of information about current developments and findings, on which they base both patient treatment and ongoing research toward bio- medical discovery. The proposed research will support physicians and scientists, speed-up their search for information and improve their effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage not only of text but also of the highly-informative image data within publications. The successful outcome of this research will lead to the development of focused, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting targeted biomedical discovery including better understanding of disease mechanism, uncovering possible means for accurate diagnoses, and revealing potential new drugs and drug-targets.",Utilizing Imaged-based Features in Biomedical Literature Classification,8916181,R56LM011354,"['Address', 'Area', 'Bioinformatics', 'Biological', 'Biological Phenomena', 'Categories', 'Classification', 'Comparative Study', 'Computational Biology', 'Computer-Assisted Image Analysis', 'Computers', 'Coupled', 'Cues', 'Data', 'Data Set', 'Databases', 'Delaware', 'Development', 'Dimensions', 'Disease', 'Drug Targeting', 'Ensure', 'Entropy', 'Gene Expression', 'Gene Proteins', 'Generic Drugs', 'Goals', 'Gold', 'Gray unit of radiation dose', 'Harvest', 'Image', 'Image Analysis', 'Improve Access', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Knowledge', 'Lead', 'Literature', 'Measures', 'Medical', 'Methods', 'Mining', 'Modeling', 'Mus', 'Nucleic Acid Regulatory Sequences', 'Organism', 'Outcomes Research', 'Paper', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Property', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reader', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Retrieval', 'Scanning', 'Scientist', 'Secure', 'Series', 'Shapes', 'Source', 'Speed', 'System', 'Testing', 'Text', 'Textbooks', 'Texture', 'Training', 'Universities', 'Voting', 'Work', 'accurate diagnosis', 'authority', 'base', 'bioimaging', 'evaluation/testing', 'experience', 'fundamental research', 'image processing', 'improved', 'indexing', 'insight', 'mouse genome', 'protein protein interaction', 'research study', 'text searching', 'tool', 'tool development']",NLM,UNIVERSITY OF DELAWARE,R56,2015,280000,0.12166923091043189
"Utilizing Imaged-based Features in Biomedical Literature Classification DESCRIPTION (provided by applicant): The proposed research aims to support and improve effective access to the biomedical literature, by utilizing the rich, highly-informative image data within publications, in addition to text. The biomedical literature is expanding at a rate of about  1,000,000 new publications a year. Scientists and physicians, as part of their daily work, go through a myriad of publications searching for relevant information. The task is even more arduous for scientific database curators (bio- curators, in organizations such as FlyBase or UniProt), who have to identify the literature most relevant to the database area, locate within it high-quality evidence concerning genes, proteins, organisms, or disease, and curate the findings within a database entry, with references to the relevant literature. Notably, much of the evidence within publications lies in figures. Accordingly, images are used by scientists and database curators as indicators for relevance.  To assist and expedite the search for information within the literature, automated text-mining tools are being developed; still, several shared tasks and competitive challenges demonstrated that the need for more effective automated identification of relevant information in biomedical publications remains a bottleneck for bio-curation and for scientific discovery. While image analysis within and outside the biomedical domain is an active research area, most current work on biomedical image processing focuses on retrieval and understanding of images as a primary form of data. Likewise, most efforts on biomedical literature retrieval and mining focus on text alone. Little has been done so far to use images within publications, which provide important cues as to the relevance of information embedded in papers.  The hypothesis underlying our proposal is that useful information can be derived directly from images within publications and integrated with text-based methods, leading to improved identification of relevant publications and of informative portions within them. The proposed research comprises extensive comparative study of highly-informative features within images, development and identification of such image-features, development of tools that extract such features and information from images, and integration of image-based information into the textual articles-classification process, aiming to determine the publications' relevance to well-defined biomedical needs. The fundamental research tasks we shall address are: A) Identification and comparative study of useful features for image-representation, focusing on their utility for specific biomedical needs; B) Classification of biomedical images and biomedical documents based on image-data; C) Document classification through integration of text- and image-based classifiers. To ground the research in genuine needs, secure access to much image data, and ensure broad-applicability of the results, we shall work within three diverse areas for which we have secured access to expertise and data: Finding articles about cis-regulatory regions (Cyrene project at Brown University); Evidence for gene expression in the mouse (Jackson Lab's GXD); Experimental evidence for protein-protein interaction (Delaware's Protein Information Resource). The successful completion of the proposed project will provide integrated methods and tools, utilizing both image-based and text-based features, leading to more focused and effective retrieval and mining tools, thus better supporting data-intensive biomedical discovery. Physicians and bio-medical scientists rely on the vast published literature as their main source of information about current developments and findings, on which they base both patient treatment and ongoing research toward bio- medical discovery. The proposed research will support physicians and scientists, speed-up their search for information and improve their effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage not only of text but also of the highly-informative image data within publications. The successful outcome of this research will lead to the development of focused, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting targeted biomedical discovery including better understanding of disease mechanism, uncovering possible means for accurate diagnoses, and revealing potential new drugs and drug-targets.",Utilizing Imaged-based Features in Biomedical Literature Classification,8892560,R56LM011354,"['Address', 'Area', 'Bioinformatics', 'Biological', 'Biological Phenomena', 'Categories', 'Classification', 'Comparative Study', 'Computational Biology', 'Computer-Assisted Image Analysis', 'Computers', 'Coupled', 'Cues', 'Data', 'Data Set', 'Databases', 'Delaware', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Drug Targeting', 'Ensure', 'Entropy', 'Gene Expression', 'Gene Proteins', 'Generic Drugs', 'Goals', 'Gold', 'Gray unit of radiation dose', 'Harvest', 'Image', 'Image Analysis', 'Improve Access', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Knowledge', 'Lead', 'Literature', 'Measures', 'Medical', 'Methods', 'Metric', 'Mining', 'Modeling', 'Mus', 'Nucleic Acid Regulatory Sequences', 'Organism', 'Outcomes Research', 'Paper', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Property', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reader', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Retrieval', 'Scanning', 'Scientist', 'Secure', 'Series', 'Shapes', 'Source', 'Speed', 'System', 'Testing', 'Text', 'Textbooks', 'Texture', 'Training', 'Universities', 'Voting', 'Work', 'authority', 'base', 'bioimaging', 'evaluation/testing', 'experience', 'fundamental research', 'image processing', 'improved', 'indexing', 'insight', 'mouse genome', 'protein protein interaction', 'research study', 'text searching', 'tool', 'tool development']",NLM,UNIVERSITY OF DELAWARE,R56,2014,280000,0.12166923091043189
"Scalable and Robust Clinical Text De-Identification Tools     DESCRIPTION (provided by applicant): Exploiting the full potential of information rich and rapidly growing repositories of patient clinical text is hampered by the absence of scalable and robust de-identification tools. Clinical text contains protected health information (PHI), and the Health Insurance Portability and Accountability Act (HIPAA) restricts research use of patient information containing PHI to specific, limited, IRB-approved projects. As a result, vast repositories of clinical text remain under-used by internal researchers, and are even less available for external transmission to outside collaborators or for centralized processing by state-of-the-art natural language processing (NLP) technologies. De-identification, which is the removal of PHI from clinical text, is challenging. Despite their availability for over a decade, commercially available automated systems are expensive, require local tailoring, and have not gained widespread market penetration. Manual methods are costly and do not scale, yet continue to be used despite the small amount of residual PHI they leave behind. Open source de-identification tools based on state-of-the-art machine learning technologies can perform at or above the level of manual approaches but also suffer from the residual PHI problem. Current de-identification approaches, then, also severely limit the use and mobility of clinical text while exposing patients to privacy risks. These approaches redact PHI, blacking it out or replacing it with symbols (e.g., ""Here for cardiac eval is Mr. **PT_NAME<AA>, a **AGE<60s> yo male with his son Doug ...""). Traditional approaches leave residual PHI (""Doug"" in this example) to be easily noticed by readers of the text, as it remains plainly visible among the prominent redactions. We developed and pilot tested an alternative approach we believe addresses the residual PHI problem. Our approach uses the strategy of concealing, rather than trying to eliminate, residual PHI. We call it the ""Hiding In Plain Sight"" (HIPS) approach. HIPS replaces all known PHI with ""surrogate"" PHI- fictional names, ages, etc.-that look real but do not refer to any actual patient. A HIPS version of the above text is: ""Here for cardiac eval is Mr. Jones, a 64 yo male with his son Doug ..."" where the name ""Jones"" and age ""64"" are fictional surrogates, but the name ""Doug"" is residual PHI. To a reader, the surrogates and the residual PHI are indistinguishable. This prevents the reader from detecting the latter, avoiding disclosure. Our preliminary studies suggest that HIPS can reduce the risk of disclosure of residual PHI by a factor of 10. This yields overall performance that far surpasses the performance attainable by manual methods, and is unlikely to be matched, we believe, by additional incremental improvements in PHI tagging models (i.e., efforts to reduce residual PHI). Our pilot studies indicate IRBs would welcome the HIPS approach if it were shown to be effective through rigorous evaluation. To expand usage of clinical text and enhance patient privacy, we propose to formalize rules of effective surrogate generation (Aim 1), extend related de-identification confidence scoring methods (Aim 2), and conduct rigorous efficacy testing of HIPS in diverse institutional settings (Aim 3).                  All known automated de-identification methods leave behind a small amount of residual protected health information (PHI), which presents a risk of disclosing patient privacy and creates barriers to more widespread internal use and external sharing of information-rich clinical text for broad research purpose. This project advances and evaluates the efficacy of a novel method, called the Hiding In Plain Sight (HIPS) approach, which conceals residual PHI by replacing all other instance of PHI found in a document with realistic appearing but fictitious surrogates. Rigorous efficacy testing is needed to confirm that HIPS surrogates effectively reduce risk of exposing patient privacy by concealing the small amount of residual PHI all known de-identification leave behind.",Scalable and Robust Clinical Text De-Identification Tools,8722030,R01LM011366,"['Address', 'Age', 'Applied Research', 'Cardiac', 'Clinical', 'Detection', 'Disclosure', 'Evaluation', 'Excision', 'Foundations', 'Generations', 'Health', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Human', 'Information Theory', 'Institutional Review Boards', 'Left', 'Machine Learning', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Monitor', 'Names', 'Natural Language Processing', 'Patients', 'Penetration', 'Performance', 'Pilot Projects', 'Plant Roots', 'Privacy', 'Process', 'Publishing', 'Reader', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Risk', 'Scoring Method', 'Simulate', 'Son', 'Source', 'System', 'Technology', 'Testing', 'Text', 'Validation', 'Vision', 'Work', 'base', 'efficacy testing', 'male', 'novel', 'open source', 'patient privacy', 'prevent', 'programs', 'repository', 'software systems', 'tool', 'transmission process']",NLM,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,R01,2014,260727,0.10691547964762044
"Scalable and Robust Clinical Text De-Identification Tools     DESCRIPTION (provided by applicant): Exploiting the full potential of information rich and rapidly growing repositories of patient clinical text is hampered by the absence of scalable and robust de-identification tools. Clinical text contains protected health information (PHI), and the Health Insurance Portability and Accountability Act (HIPAA) restricts research use of patient information containing PHI to specific, limited, IRB-approved projects. As a result, vast repositories of clinical text remain under-used by internal researchers, and are even less available for external transmission to outside collaborators or for centralized processing by state-of-the-art natural language processing (NLP) technologies. De-identification, which is the removal of PHI from clinical text, is challenging. Despite their availability for over a decade, commercially available automated systems are expensive, require local tailoring, and have not gained widespread market penetration. Manual methods are costly and do not scale, yet continue to be used despite the small amount of residual PHI they leave behind. Open source de-identification tools based on state-of-the-art machine learning technologies can perform at or above the level of manual approaches but also suffer from the residual PHI problem. Current de-identification approaches, then, also severely limit the use and mobility of clinical text while exposing patients to privacy risks. These approaches redact PHI, blacking it out or replacing it with symbols (e.g., ""Here for cardiac eval is Mr. **PT_NAME<AA>, a **AGE<60s> yo male with his son Doug ...""). Traditional approaches leave residual PHI (""Doug"" in this example) to be easily noticed by readers of the text, as it remains plainly visible among the prominent redactions. We developed and pilot tested an alternative approach we believe addresses the residual PHI problem. Our approach uses the strategy of concealing, rather than trying to eliminate, residual PHI. We call it the ""Hiding In Plain Sight"" (HIPS) approach. HIPS replaces all known PHI with ""surrogate"" PHI- fictional names, ages, etc.-that look real but do not refer to any actual patient. A HIPS version of the above text is: ""Here for cardiac eval is Mr. Jones, a 64 yo male with his son Doug ..."" where the name ""Jones"" and age ""64"" are fictional surrogates, but the name ""Doug"" is residual PHI. To a reader, the surrogates and the residual PHI are indistinguishable. This prevents the reader from detecting the latter, avoiding disclosure. Our preliminary studies suggest that HIPS can reduce the risk of disclosure of residual PHI by a factor of 10. This yields overall performance that far surpasses the performance attainable by manual methods, and is unlikely to be matched, we believe, by additional incremental improvements in PHI tagging models (i.e., efforts to reduce residual PHI). Our pilot studies indicate IRBs would welcome the HIPS approach if it were shown to be effective through rigorous evaluation. To expand usage of clinical text and enhance patient privacy, we propose to formalize rules of effective surrogate generation (Aim 1), extend related de-identification confidence scoring methods (Aim 2), and conduct rigorous efficacy testing of HIPS in diverse institutional settings (Aim 3).                  All known automated de-identification methods leave behind a small amount of residual protected health information (PHI), which presents a risk of disclosing patient privacy and creates barriers to more widespread internal use and external sharing of information-rich clinical text for broad research purpose. This project advances and evaluates the efficacy of a novel method, called the Hiding In Plain Sight (HIPS) approach, which conceals residual PHI by replacing all other instance of PHI found in a document with realistic appearing but fictitious surrogates. Rigorous efficacy testing is needed to confirm that HIPS surrogates effectively reduce risk of exposing patient privacy by concealing the small amount of residual PHI all known de-identification leave behind.",Scalable and Robust Clinical Text De-Identification Tools,8532984,R01LM011366,"['Address', 'Age', 'Applied Research', 'Cardiac', 'Clinical', 'Detection', 'Disclosure', 'Evaluation', 'Excision', 'Foundations', 'Generations', 'Health', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Human', 'Information Theory', 'Institutional Review Boards', 'Left', 'Machine Learning', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Monitor', 'Names', 'Natural Language Processing', 'Patients', 'Penetration', 'Performance', 'Pilot Projects', 'Plant Roots', 'Privacy', 'Process', 'Publishing', 'Reader', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Risk', 'Scoring Method', 'Simulate', 'Son', 'Source', 'System', 'Technology', 'Testing', 'Text', 'Validation', 'Vision', 'Work', 'base', 'efficacy testing', 'male', 'novel', 'open source', 'patient privacy', 'prevent', 'programs', 'repository', 'software systems', 'tool', 'transmission process']",NLM,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,R01,2013,247288,0.10691547964762044
"Scalable and Robust Clinical Text De-Identification Tools     DESCRIPTION (provided by applicant): Exploiting the full potential of information rich and rapidly growing repositories of patient clinical text is hampered by the absence of scalable and robust de-identification tools. Clinical text contains protected health information (PHI), and the Health Insurance Portability and Accountability Act (HIPAA) restricts research use of patient information containing PHI to specific, limited, IRB-approved projects. As a result, vast repositories of clinical text remain under-used by internal researchers, and are even less available for external transmission to outside collaborators or for centralized processing by state-of-the-art natural language processing (NLP) technologies. De-identification, which is the removal of PHI from clinical text, is challenging. Despite their availability for over a decade, commercially available automated systems are expensive, require local tailoring, and have not gained widespread market penetration. Manual methods are costly and do not scale, yet continue to be used despite the small amount of residual PHI they leave behind. Open source de-identification tools based on state-of-the-art machine learning technologies can perform at or above the level of manual approaches but also suffer from the residual PHI problem. Current de-identification approaches, then, also severely limit the use and mobility of clinical text while exposing patients to privacy risks. These approaches redact PHI, blacking it out or replacing it with symbols (e.g., ""Here for cardiac eval is Mr. **PT_NAME<AA>, a **AGE<60s> yo male with his son Doug ...""). Traditional approaches leave residual PHI (""Doug"" in this example) to be easily noticed by readers of the text, as it remains plainly visible among the prominent redactions. We developed and pilot tested an alternative approach we believe addresses the residual PHI problem. Our approach uses the strategy of concealing, rather than trying to eliminate, residual PHI. We call it the ""Hiding In Plain Sight"" (HIPS) approach. HIPS replaces all known PHI with ""surrogate"" PHI- fictional names, ages, etc.-that look real but do not refer to any actual patient. A HIPS version of the above text is: ""Here for cardiac eval is Mr. Jones, a 64 yo male with his son Doug ..."" where the name ""Jones"" and age ""64"" are fictional surrogates, but the name ""Doug"" is residual PHI. To a reader, the surrogates and the residual PHI are indistinguishable. This prevents the reader from detecting the latter, avoiding disclosure. Our preliminary studies suggest that HIPS can reduce the risk of disclosure of residual PHI by a factor of 10. This yields overall performance that far surpasses the performance attainable by manual methods, and is unlikely to be matched, we believe, by additional incremental improvements in PHI tagging models (i.e., efforts to reduce residual PHI). Our pilot studies indicate IRBs would welcome the HIPS approach if it were shown to be effective through rigorous evaluation. To expand usage of clinical text and enhance patient privacy, we propose to formalize rules of effective surrogate generation (Aim 1), extend related de-identification confidence scoring methods (Aim 2), and conduct rigorous efficacy testing of HIPS in diverse institutional settings (Aim 3).                  All known automated de-identification methods leave behind a small amount of residual protected health information (PHI), which presents a risk of disclosing patient privacy and creates barriers to more widespread internal use and external sharing of information-rich clinical text for broad research purpose. This project advances and evaluates the efficacy of a novel method, called the Hiding In Plain Sight (HIPS) approach, which conceals residual PHI by replacing all other instance of PHI found in a document with realistic appearing but fictitious surrogates. Rigorous efficacy testing is needed to confirm that HIPS surrogates effectively reduce risk of exposing patient privacy by concealing the small amount of residual PHI all known de-identification leave behind.",Scalable and Robust Clinical Text De-Identification Tools,8345041,R01LM011366,"['Address', 'Age', 'Applied Research', 'Cardiac', 'Clinical', 'Detection', 'Disclosure', 'Evaluation', 'Excision', 'Foundations', 'Generations', 'Health', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Human', 'Information Theory', 'Institutional Review Boards', 'Left', 'Machine Learning', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Monitor', 'Names', 'Natural Language Processing', 'Patients', 'Penetration', 'Performance', 'Pilot Projects', 'Plant Roots', 'Privacy', 'Process', 'Publishing', 'Reader', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Risk', 'Scoring Method', 'Simulate', 'Son', 'Source', 'System', 'Technology', 'Testing', 'Text', 'Validation', 'Vision', 'Work', 'base', 'efficacy testing', 'male', 'novel', 'open source', 'patient privacy', 'prevent', 'programs', 'repository', 'software systems', 'tool', 'transmission process']",NLM,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,R01,2012,318849,0.10691547964762044
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs     DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences.             Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8727093,R00LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R00,2014,223898,0.12456376172317377
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs     DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences.              Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8442618,K99LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,MAYO CLINIC ROCHESTER,K99,2012,96552,0.12456376172317377
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences. Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8906938,R00LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R00,2015,217378,0.12456376172317377
"Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical DESCRIPTION (provided by applicant): The proposed research aims to provide effective, large-scale means for obtaining reliable information about drug-drug interactions (DDIs), by focusing on and utilizing the multiple distinct types of evidence used in reporting DDIs. DDIs are a significant cause of adverse drug reactions, leading to emergency room visits and hospitalizations. DDI research aims to link between molecular mechanisms that underlie interactions and their actual clinical consequences, through several types of evidence. We distinguish three types of DDI evidence that are often provided in the literature: in vitro, in viv, and clinical. In vitro studies investigate molecular mechanisms of interaction; In vivo studies evaluate whether these interactions impact drug exposure in human subjects; Clinical studies test whether drug interactions change the actual response to drugs (e.g. drug-efficacy or adverse drug reactions). As such studies span several disciplines, typically the three types of evidence are not simultaneously available or reported. Missing evidence along any of the three types, creates a knowledge gap that can hinder translational research. For instance, if adverse interaction effects are clinically observed, but the molecular underpinnings are not yet reported, it is difficult to identify a safe, alternative drug treatment.      In this project we propose to develop and use large-scale text-mining methods and tools to mine drug- interaction information from PubMed abstracts and from FDA drug labels. These tools will be designed to explicitly identify gaps across the three levels of DDI evidence, and to help close such gaps. While automated discovery of DDI mentions in text is an active research area, no other text-based work is concerned with identifying explicit evidence for DDI, while separately taking into consideration the distinct types of interaction evidence. As a follow-up step, we also propose to conduct selective molecular pharmacology experiments to close the identified knowledge-gaps at the in vitro evidence level. Specifically: In Aim 1, we shall construct the needed lexica and new text corpora pertaining to in vitro, in vivo, and clinical DDI evidence; In Aim 2, a suite of text mining tools to separately identify the three types of DDI evidence will be developed, utilizing the corpora created in Aim 1; In Aim 3, clinically significant DDIs that have no sufficient in vitro evidence will be selected using the tools developed in Aim 2, and experiments will be conducted to evaluate in vitro metabolic enzyme- based DDI mechanisms. To the best of our knowledge we are the first group that sets out to distinguish among - and make use of - the different types of text-based DDI evidence in a systematic way. Following the text- based discovery with a selective molecular pharmacology experimental evaluation, is another unique interdisciplinary characteristic that adds to the significance of the proposed work. The successful completion of the proposed project will provide methods and tools for large-scale extraction of DDIs from the literature, along with their supporting evidence at the three distinct levels. Moreover, DDIs that will be reliably supported by one type of evidence but not another will be identified as strong candidates for future pharmacology research. Project Narrative: Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical Drug-drug interactions (DDIs) lead to adverse drug reactions, emergency room visits and hospitalization, thus posing a major challenge to public health. However, evidence for DDI is hard to gather, as it broadly varies from descriptions of molecular interactions in basic-science journals, to clinical descriptions of adverse-effects in a myriad of medical publications. The proposed research aims to develop tools that focus directly on identifying and gathering diverse types of reliable DDI evidence from diverse sources, and supply them to clinicians and biologists.","Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical",9336353,R01LM011945,"['Address', 'Adverse effects', 'Area', 'Basic Science', 'Binding', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Conflict (Psychology)', 'Development', 'Discipline', 'Drug Exposure', 'Drug Interactions', 'Drug Kinetics', 'Emergency department visit', 'Enzymes', 'Evaluation', 'FDA approved', 'Future', 'Health', 'Hospitalization', 'Human', 'In Vitro', 'Incidence', 'Journals', 'Knowledge', 'Label', 'Lead', 'Level of Evidence', 'Link', 'Literature', 'Medical', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Numerical value', 'Ontology', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Pharmacology', 'Pharmacotherapy', 'Polypharmacy', 'PubMed', 'Public Health', 'Publications', 'Reaction', 'Reporting', 'Research', 'Research Design', 'Retrieval', 'Sampling', 'Source', 'Terminology', 'Testing', 'Text', 'Translational Research', 'Transportation', 'United States', 'Work', 'base', 'clinical effect', 'clinically significant', 'design', 'drug development', 'drug discovery', 'drug efficacy', 'evidence base', 'experimental study', 'follow-up', 'human subject', 'in vivo', 'novel therapeutics', 'prevent', 'response', 'statistics', 'text searching', 'tool']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2017,394297,0.07694264535307885
"Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical DESCRIPTION (provided by applicant): The proposed research aims to provide effective, large-scale means for obtaining reliable information about drug-drug interactions (DDIs), by focusing on and utilizing the multiple distinct types of evidence used in reporting DDIs. DDIs are a significant cause of adverse drug reactions, leading to emergency room visits and hospitalizations. DDI research aims to link between molecular mechanisms that underlie interactions and their actual clinical consequences, through several types of evidence. We distinguish three types of DDI evidence that are often provided in the literature: in vitro, in viv, and clinical. In vitro studies investigate molecular mechanisms of interaction; In vivo studies evaluate whether these interactions impact drug exposure in human subjects; Clinical studies test whether drug interactions change the actual response to drugs (e.g. drug-efficacy or adverse drug reactions). As such studies span several disciplines, typically the three types of evidence are not simultaneously available or reported. Missing evidence along any of the three types, creates a knowledge gap that can hinder translational research. For instance, if adverse interaction effects are clinically observed, but the molecular underpinnings are not yet reported, it is difficult to identify a safe, alternative drug treatment.      In this project we propose to develop and use large-scale text-mining methods and tools to mine drug- interaction information from PubMed abstracts and from FDA drug labels. These tools will be designed to explicitly identify gaps across the three levels of DDI evidence, and to help close such gaps. While automated discovery of DDI mentions in text is an active research area, no other text-based work is concerned with identifying explicit evidence for DDI, while separately taking into consideration the distinct types of interaction evidence. As a follow-up step, we also propose to conduct selective molecular pharmacology experiments to close the identified knowledge-gaps at the in vitro evidence level. Specifically: In Aim 1, we shall construct the needed lexica and new text corpora pertaining to in vitro, in vivo, and clinical DDI evidence; In Aim 2, a suite of text mining tools to separately identify the three types of DDI evidence will be developed, utilizing the corpora created in Aim 1; In Aim 3, clinically significant DDIs that have no sufficient in vitro evidence will be selected using the tools developed in Aim 2, and experiments will be conducted to evaluate in vitro metabolic enzyme- based DDI mechanisms. To the best of our knowledge we are the first group that sets out to distinguish among - and make use of - the different types of text-based DDI evidence in a systematic way. Following the text- based discovery with a selective molecular pharmacology experimental evaluation, is another unique interdisciplinary characteristic that adds to the significance of the proposed work. The successful completion of the proposed project will provide methods and tools for large-scale extraction of DDIs from the literature, along with their supporting evidence at the three distinct levels. Moreover, DDIs that will be reliably supported by one type of evidence but not another will be identified as strong candidates for future pharmacology research. Project Narrative: Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical Drug-drug interactions (DDIs) lead to adverse drug reactions, emergency room visits and hospitalization, thus posing a major challenge to public health. However, evidence for DDI is hard to gather, as it broadly varies from descriptions of molecular interactions in basic-science journals, to clinical descriptions of adverse-effects in a myriad of medical publications. The proposed research aims to develop tools that focus directly on identifying and gathering diverse types of reliable DDI evidence from diverse sources, and supply them to clinicians and biologists.","Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical",9119045,R01LM011945,"['Address', 'Adverse effects', 'Area', 'Basic Science', 'Binding', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Conflict (Psychology)', 'Development', 'Discipline', 'Drug Exposure', 'Drug Interactions', 'Drug Kinetics', 'Emergency department visit', 'Enzymes', 'Evaluation', 'FDA approved', 'Future', 'Health', 'Hospitalization', 'Human', 'In Vitro', 'Incidence', 'Journals', 'Knowledge', 'Label', 'Lead', 'Left', 'Level of Evidence', 'Link', 'Literature', 'Medical', 'Metabolic', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Numerical value', 'Ontology', 'Pharmaceutical Preparations', 'Pharmacology', 'Pharmacotherapy', 'Polypharmacy', 'PubMed', 'Public Health', 'Publications', 'Reaction', 'Reporting', 'Research', 'Research Design', 'Retrieval', 'Sampling', 'Source', 'Terminology', 'Testing', 'Text', 'Translational Research', 'Transportation', 'United States', 'Work', 'abstracting', 'base', 'clinically significant', 'design', 'drug development', 'drug efficacy', 'epidemiology study', 'evidence base', 'follow-up', 'human subject', 'in vivo', 'novel therapeutics', 'prevent', 'research study', 'response', 'statistics', 'text searching', 'tool']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2016,397279,0.07694264535307885
"Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical DESCRIPTION (provided by applicant): The proposed research aims to provide effective, large-scale means for obtaining reliable information about drug-drug interactions (DDIs), by focusing on and utilizing the multiple distinct types of evidence used in reporting DDIs. DDIs are a significant cause of adverse drug reactions, leading to emergency room visits and hospitalizations. DDI research aims to link between molecular mechanisms that underlie interactions and their actual clinical consequences, through several types of evidence. We distinguish three types of DDI evidence that are often provided in the literature: in vitro, in viv, and clinical. In vitro studies investigate molecular mechanisms of interaction; In vivo studies evaluate whether these interactions impact drug exposure in human subjects; Clinical studies test whether drug interactions change the actual response to drugs (e.g. drug-efficacy or adverse drug reactions). As such studies span several disciplines, typically the three types of evidence are not simultaneously available or reported. Missing evidence along any of the three types, creates a knowledge gap that can hinder translational research. For instance, if adverse interaction effects are clinically observed, but the molecular underpinnings are not yet reported, it is difficult to identify a safe, alternative drug treatment.      In this project we propose to develop and use large-scale text-mining methods and tools to mine drug- interaction information from PubMed abstracts and from FDA drug labels. These tools will be designed to explicitly identify gaps across the three levels of DDI evidence, and to help close such gaps. While automated discovery of DDI mentions in text is an active research area, no other text-based work is concerned with identifying explicit evidence for DDI, while separately taking into consideration the distinct types of interaction evidence. As a follow-up step, we also propose to conduct selective molecular pharmacology experiments to close the identified knowledge-gaps at the in vitro evidence level. Specifically: In Aim 1, we shall construct the needed lexica and new text corpora pertaining to in vitro, in vivo, and clinical DDI evidence; In Aim 2, a suite of text mining tools to separately identify the three types of DDI evidence will be developed, utilizing the corpora created in Aim 1; In Aim 3, clinically significant DDIs that have no sufficient in vitro evidence will be selected using the tools developed in Aim 2, and experiments will be conducted to evaluate in vitro metabolic enzyme- based DDI mechanisms. To the best of our knowledge we are the first group that sets out to distinguish among - and make use of - the different types of text-based DDI evidence in a systematic way. Following the text- based discovery with a selective molecular pharmacology experimental evaluation, is another unique interdisciplinary characteristic that adds to the significance of the proposed work. The successful completion of the proposed project will provide methods and tools for large-scale extraction of DDIs from the literature, along with their supporting evidence at the three distinct levels. Moreover, DDIs that will be reliably supported by one type of evidence but not another will be identified as strong candidates for future pharmacology research. Project Narrative: Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical Drug-drug interactions (DDIs) lead to adverse drug reactions, emergency room visits and hospitalization, thus posing a major challenge to public health. However, evidence for DDI is hard to gather, as it broadly varies from descriptions of molecular interactions in basic-science journals, to clinical descriptions of adverse-effects in a myriad of medical publications. The proposed research aims to develop tools that focus directly on identifying and gathering diverse types of reliable DDI evidence from diverse sources, and supply them to clinicians and biologists.","Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical",8930729,R01LM011945,"['Address', 'Adverse effects', 'Area', 'Basic Science', 'Binding', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Conflict (Psychology)', 'Development', 'Discipline', 'Drug Exposure', 'Drug Interactions', 'Drug Kinetics', 'Emergency department visit', 'Enzymes', 'Evaluation', 'FDA approved', 'Future', 'Health', 'Hospitalization', 'Human', 'In Vitro', 'Incidence', 'Journals', 'Knowledge', 'Label', 'Lead', 'Left', 'Level of Evidence', 'Link', 'Literature', 'Medical', 'Metabolic', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Numerical value', 'Ontology', 'Pharmaceutical Preparations', 'Pharmacology', 'Polypharmacy', 'PubMed', 'Public Health', 'Publications', 'Reaction', 'Reporting', 'Research', 'Research Design', 'Retrieval', 'Sampling', 'Source', 'Terminology', 'Testing', 'Text', 'Translational Research', 'Transportation', 'United States', 'Work', 'abstracting', 'base', 'clinically significant', 'design', 'drug development', 'drug efficacy', 'epidemiology study', 'evidence base', 'follow-up', 'human subject', 'in vivo', 'prevent', 'research study', 'response', 'statistics', 'text searching', 'tool']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2015,481051,0.07694264535307885
"Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical     DESCRIPTION (provided by applicant): The proposed research aims to provide effective, large-scale means for obtaining reliable information about drug-drug interactions (DDIs), by focusing on and utilizing the multiple distinct types of evidence used in reporting DDIs. DDIs are a significant cause of adverse drug reactions, leading to emergency room visits and hospitalizations. DDI research aims to link between molecular mechanisms that underlie interactions and their actual clinical consequences, through several types of evidence. We distinguish three types of DDI evidence that are often provided in the literature: in vitro, in viv, and clinical. In vitro studies investigate molecular mechanisms of interaction; In vivo studies evaluate whether these interactions impact drug exposure in human subjects; Clinical studies test whether drug interactions change the actual response to drugs (e.g. drug-efficacy or adverse drug reactions). As such studies span several disciplines, typically the three types of evidence are not simultaneously available or reported. Missing evidence along any of the three types, creates a knowledge gap that can hinder translational research. For instance, if adverse interaction effects are clinically observed, but the molecular underpinnings are not yet reported, it is difficult to identify a safe, alternative drug treatment.      In this project we propose to develop and use large-scale text-mining methods and tools to mine drug- interaction information from PubMed abstracts and from FDA drug labels. These tools will be designed to explicitly identify gaps across the three levels of DDI evidence, and to help close such gaps. While automated discovery of DDI mentions in text is an active research area, no other text-based work is concerned with identifying explicit evidence for DDI, while separately taking into consideration the distinct types of interaction evidence. As a follow-up step, we also propose to conduct selective molecular pharmacology experiments to close the identified knowledge-gaps at the in vitro evidence level. Specifically: In Aim 1, we shall construct the needed lexica and new text corpora pertaining to in vitro, in vivo, and clinical DDI evidence; In Aim 2, a suite of text mining tools to separately identify the three types of DDI evidence will be developed, utilizing the corpora created in Aim 1; In Aim 3, clinically significant DDIs that have no sufficient in vitro evidence will be selected using the tools developed in Aim 2, and experiments will be conducted to evaluate in vitro metabolic enzyme- based DDI mechanisms. To the best of our knowledge we are the first group that sets out to distinguish among - and make use of - the different types of text-based DDI evidence in a systematic way. Following the text- based discovery with a selective molecular pharmacology experimental evaluation, is another unique interdisciplinary characteristic that adds to the significance of the proposed work. The successful completion of the proposed project will provide methods and tools for large-scale extraction of DDIs from the literature, along with their supporting evidence at the three distinct levels. Moreover, DDIs that will be reliably supported by one type of evidence but not another will be identified as strong candidates for future pharmacology research.              Project Narrative: Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical Drug-drug interactions (DDIs) lead to adverse drug reactions, emergency room visits and hospitalization, thus posing a major challenge to public health. However, evidence for DDI is hard to gather, as it broadly varies from descriptions of molecular interactions in basic-science journals, to clinical descriptions of adverse-effects in a myriad of medical publications. The proposed research aims to develop tools that focus directly on identifying and gathering diverse types of reliable DDI evidence from diverse sources, and supply them to clinicians and biologists.","Evidence-based Drug-Interaction Discovery: In-Vivo, In-Vitro and Clinical",8686117,R01LM011945,"['Accident and Emergency department', 'Address', 'Adverse effects', 'Area', 'Basic Science', 'Binding', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Conflict (Psychology)', 'Development', 'Discipline', 'Drug Exposure', 'Drug Interactions', 'Drug Kinetics', 'Enzymes', 'Evaluation', 'FDA approved', 'Future', 'Health', 'Hospitalization', 'Human', 'In Vitro', 'Incidence', 'Journals', 'Knowledge', 'Label', 'Lead', 'Left', 'Level of Evidence', 'Link', 'Literature', 'Medical', 'Metabolic', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Numerical value', 'Ontology', 'Pharmaceutical Preparations', 'Pharmacology', 'Polypharmacy', 'PubMed', 'Public Health', 'Publications', 'Reaction', 'Reporting', 'Research', 'Research Design', 'Retrieval', 'Sampling', 'Source', 'Terminology', 'Testing', 'Text', 'Translational Research', 'Transportation', 'United States', 'Visit', 'Work', 'abstracting', 'base', 'clinically significant', 'design', 'drug development', 'drug efficacy', 'epidemiology study', 'evidence base', 'follow-up', 'human subject', 'in vivo', 'prevent', 'research study', 'response', 'statistics', 'text searching', 'tool']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2014,483211,0.07694264535307885
"Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers ﻿    DESCRIPTION (provided by applicant): Lifespans continue to increase, chronic disease survival rates are drastically improved, and treatments are being discovered for a variety of illnesses. This rapidly changing scenario requires patients to participate in their recovery, understand written information and directions thereby calling upon patients to have increasingly more complex health literacy. However, time availability of practitioners or other resources to explain the required information has not increased to match. As a result, finding efficient means to improving patient health literacy is an increasingly important topic in healthcare. Increased health literacy may promote healthy lifestyle behaviors and increase access to health services by the population. It has been argued that for the Patient Protection and Affordable Care Act to be successful, more effort is needed to increase the health literacy of millions of Americans. Similarly, the Healthy People 2020 statement by the Department of Health and Human Services identified improving health literacy (HC/HIT-1) as an important national goal. The broad- long term objectives of this project are to contribute to increasing the health literacy of patients and health information consumers and provide caregivers an evidence-based tool for simplifying text. The most commonly used tool for estimating the difficulty of text is the readability formula. They are not sufficient, however, because there is no evidence to support a connection between their use and decreases in difficulty. This problem is addressed by using modern resources and techniques for discovering traits that make health-related text difficult and developing a tool to guide the simplification of text. . There are four specific aims of this project: 1) Identify differentiating features of easy versus difficult texts, 2) Design a simplification strategy using computer algorithms, 3) Measure the impact of simplification on perceived and actual text difficulty with online participants and a representative community sample, 4) Create free, online software that incorporates proven features algorithmically. Corpus analysis will be conducted to compare easy and difficult texts with each other and discover lexical, grammatical, semantic, and composition and discourse features typical for each. Then, simplification algorithms will be designed and developed relying on rule-based techniques to leverage available resources, e.g., vocabularies, or on machine learning approaches for discovering the best combinations of features for simplification. A representative writer will simplify text by relying on the suggestios provided by an online that tool that uses simplification algorithms. The effect of simplification wll be tested in comprehensive user studies to evaluate the effect on both actual and perceived difficulty. Features successfully shown to decrease text difficulty will be incorporated in an onlie software program designed to reduce text difficulty. PUBLIC HEALTH RELEVANCE: Improving health literacy is an important national goal and necessary trait for a healthy population. Providing understandable information is critical but few tools exist to help write understandable text. We aim to discover features indicative of difficult text, design translation algorithms and create a free, online software tool for rewriting health-related text with demonstrated impact on perceived and actual text difficulty",Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers,9521525,R01LM011975,"['Address', 'Advocate', 'Affect', 'Affordable Care Act', 'Algorithms', 'American', 'Arizona', 'Behavior', 'Caregivers', 'Chronic Disease', 'Communities', 'Complement', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Conflict (Psychology)', 'Data Set', 'Education', 'Education Projects', 'Ensure', 'Faculty', 'Feedback', 'Funding', 'Goals', 'Health', 'Health Promotion', 'Health Services Accessibility', 'Health behavior', 'Health education', 'Healthcare', 'Healthy People 2020', 'Longevity', 'Machine Learning', 'Measures', 'Medical', 'Medical Informatics', 'Medicine', 'Methods', 'Minority', 'Modernization', 'Natural Language Processing', 'Outcome', 'Participant', 'Patients', 'Pilot Projects', 'Population', 'Process', 'Public Health', 'Readability', 'Reader', 'Recovery', 'Research', 'Research Design', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Survival Rate', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Translations', 'Underserved Population', 'United States Dept. of Health and Human Services', 'Universities', 'Vocabulary', 'Work', 'Writing', 'base', 'college', 'combat', 'community based participatory research', 'cost effective', 'design', 'evidence base', 'health literacy', 'healthy lifestyle', 'improved', 'individual patient', 'lexical', 'programs', 'public health relevance', 'recruit', 'support tools', 'tool', 'trait', 'user-friendly', 'volunteer']",NLM,UNIVERSITY OF ARIZONA,R01,2018,356845,0.1578748986021615
"Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers ﻿    DESCRIPTION (provided by applicant): Lifespans continue to increase, chronic disease survival rates are drastically improved, and treatments are being discovered for a variety of illnesses. This rapidly changing scenario requires patients to participate in their recovery, understand written information and directions thereby calling upon patients to have increasingly more complex health literacy. However, time availability of practitioners or other resources to explain the required information has not increased to match. As a result, finding efficient means to improving patient health literacy is an increasingly important topic in healthcare. Increased health literacy may promote healthy lifestyle behaviors and increase access to health services by the population. It has been argued that for the Patient Protection and Affordable Care Act to be successful, more effort is needed to increase the health literacy of millions of Americans. Similarly, the Healthy People 2020 statement by the Department of Health and Human Services identified improving health literacy (HC/HIT-1) as an important national goal. The broad- long term objectives of this project are to contribute to increasing the health literacy of patients and health information consumers and provide caregivers an evidence-based tool for simplifying text. The most commonly used tool for estimating the difficulty of text is the readability formula. They are not sufficient, however, because there is no evidence to support a connection between their use and decreases in difficulty. This problem is addressed by using modern resources and techniques for discovering traits that make health-related text difficult and developing a tool to guide the simplification of text. . There are four specific aims of this project: 1) Identify differentiating features of easy versus difficult texts, 2) Design a simplification strategy using computer algorithms, 3) Measure the impact of simplification on perceived and actual text difficulty with online participants and a representative community sample, 4) Create free, online software that incorporates proven features algorithmically. Corpus analysis will be conducted to compare easy and difficult texts with each other and discover lexical, grammatical, semantic, and composition and discourse features typical for each. Then, simplification algorithms will be designed and developed relying on rule-based techniques to leverage available resources, e.g., vocabularies, or on machine learning approaches for discovering the best combinations of features for simplification. A representative writer will simplify text by relying on the suggestios provided by an online that tool that uses simplification algorithms. The effect of simplification wll be tested in comprehensive user studies to evaluate the effect on both actual and perceived difficulty. Features successfully shown to decrease text difficulty will be incorporated in an onlie software program designed to reduce text difficulty. PUBLIC HEALTH RELEVANCE: Improving health literacy is an important national goal and necessary trait for a healthy population. Providing understandable information is critical but few tools exist to help write understandable text. We aim to discover features indicative of difficult text, design translation algorithms and create a free, online software tool for rewriting health-related text with demonstrated impact on perceived and actual text difficulty",Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers,9306183,R01LM011975,"['Address', 'Advocate', 'Affect', 'Affordable Care Act', 'Algorithms', 'American', 'Arizona', 'Behavior', 'Caregivers', 'Chronic Disease', 'Communities', 'Complement', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Conflict (Psychology)', 'Data Set', 'Education', 'Education Projects', 'Ensure', 'Faculty', 'Feedback', 'Funding', 'Goals', 'Health', 'Health Promotion', 'Health Services Accessibility', 'Health behavior', 'Health education', 'Healthcare', 'Healthy People 2020', 'Longevity', 'Machine Learning', 'Measures', 'Medical', 'Medical Informatics', 'Medicine', 'Methods', 'Minority', 'Modernization', 'Natural Language Processing', 'Outcome', 'Participant', 'Patients', 'Pilot Projects', 'Population', 'Process', 'Public Health', 'Readability', 'Reader', 'Recovery', 'Recruitment Activity', 'Research', 'Research Design', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Survival Rate', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Translations', 'Underserved Population', 'United States Dept. of Health and Human Services', 'Universities', 'Vocabulary', 'Work', 'Writing', 'base', 'college', 'combat', 'community based participatory research', 'cost effective', 'design', 'evidence base', 'health literacy', 'healthy lifestyle', 'improved', 'individual patient', 'lexical', 'programs', 'public health relevance', 'support tools', 'tool', 'trait', 'user-friendly', 'volunteer']",NLM,UNIVERSITY OF ARIZONA,R01,2017,368201,0.1578748986021615
"Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers ﻿    DESCRIPTION (provided by applicant): Lifespans continue to increase, chronic disease survival rates are drastically improved, and treatments are being discovered for a variety of illnesses. This rapidly changing scenario requires patients to participate in their recovery, understand written information and directions thereby calling upon patients to have increasingly more complex health literacy. However, time availability of practitioners or other resources to explain the required information has not increased to match. As a result, finding efficient means to improving patient health literacy is an increasingly important topic in healthcare. Increased health literacy may promote healthy lifestyle behaviors and increase access to health services by the population. It has been argued that for the Patient Protection and Affordable Care Act to be successful, more effort is needed to increase the health literacy of millions of Americans. Similarly, the Healthy People 2020 statement by the Department of Health and Human Services identified improving health literacy (HC/HIT-1) as an important national goal. The broad- long term objectives of this project are to contribute to increasing the health literacy of patients and health information consumers and provide caregivers an evidence-based tool for simplifying text. The most commonly used tool for estimating the difficulty of text is the readability formula. They are not sufficient, however, because there is no evidence to support a connection between their use and decreases in difficulty. This problem is addressed by using modern resources and techniques for discovering traits that make health-related text difficult and developing a tool to guide the simplification of text. . There are four specific aims of this project: 1) Identify differentiating features of easy versus difficult texts, 2) Design a simplification strategy using computer algorithms, 3) Measure the impact of simplification on perceived and actual text difficulty with online participants and a representative community sample, 4) Create free, online software that incorporates proven features algorithmically. Corpus analysis will be conducted to compare easy and difficult texts with each other and discover lexical, grammatical, semantic, and composition and discourse features typical for each. Then, simplification algorithms will be designed and developed relying on rule-based techniques to leverage available resources, e.g., vocabularies, or on machine learning approaches for discovering the best combinations of features for simplification. A representative writer will simplify text by relying on the suggestios provided by an online that tool that uses simplification algorithms. The effect of simplification wll be tested in comprehensive user studies to evaluate the effect on both actual and perceived difficulty. Features successfully shown to decrease text difficulty will be incorporated in an onlie software program designed to reduce text difficulty. PUBLIC HEALTH RELEVANCE: Improving health literacy is an important national goal and necessary trait for a healthy population. Providing understandable information is critical but few tools exist to help write understandable text. We aim to discover features indicative of difficult text, design translation algorithms and create a free, online software tool for rewriting health-related text with demonstrated impact on perceived and actual text difficulty",Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers,9127807,R01LM011975,"['Address', 'Advocate', 'Affect', 'Affordable Care Act', 'Algorithms', 'American', 'Arizona', 'Behavior', 'Caregivers', 'Chronic Disease', 'Communities', 'Complement', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Conflict (Psychology)', 'Data Set', 'Education', 'Education Projects', 'Ensure', 'Faculty', 'Feedback', 'Funding', 'Goals', 'Health', 'Health Promotion', 'Health Services Accessibility', 'Health behavior', 'Health education', 'Healthcare', 'Healthy People 2020', 'Lead', 'Longevity', 'Machine Learning', 'Measures', 'Medical', 'Medical Informatics', 'Medicine', 'Methods', 'Minority', 'Mullerian duct inhibiting substance', 'Natural Language Processing', 'Outcome', 'Participant', 'Patients', 'Pilot Projects', 'Population', 'Process', 'Public Health', 'Readability', 'Reader', 'Recovery', 'Recruitment Activity', 'Research', 'Research Design', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Survival Rate', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Translations', 'Underserved Population', 'United States Dept. of Health and Human Services', 'Universities', 'Vocabulary', 'Work', 'Writing', 'base', 'college', 'combat', 'community based participatory research', 'cost effective', 'design', 'evidence base', 'health literacy', 'healthy lifestyle', 'improved', 'individual patient', 'lexical', 'programs', 'support tools', 'tool', 'trait', 'user-friendly', 'volunteer']",NLM,UNIVERSITY OF ARIZONA,R01,2016,362613,0.1578748986021615
"Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers ﻿    DESCRIPTION (provided by applicant): Lifespans continue to increase, chronic disease survival rates are drastically improved, and treatments are being discovered for a variety of illnesses. This rapidly changing scenario requires patients to participate in their recovery, understand written information and directions thereby calling upon patients to have increasingly more complex health literacy. However, time availability of practitioners or other resources to explain the required information has not increased to match. As a result, finding efficient means to improving patient health literacy is an increasingly important topic in healthcare. Increased health literacy may promote healthy lifestyle behaviors and increase access to health services by the population. It has been argued that for the Patient Protection and Affordable Care Act to be successful, more effort is needed to increase the health literacy of millions of Americans. Similarly, the Healthy People 2020 statement by the Department of Health and Human Services identified improving health literacy (HC/HIT-1) as an important national goal. The broad- long term objectives of this project are to contribute to increasing the health literacy of patients and health information consumers and provide caregivers an evidence-based tool for simplifying text. The most commonly used tool for estimating the difficulty of text is the readability formula. They are not sufficient, however, because there is no evidence to support a connection between their use and decreases in difficulty. This problem is addressed by using modern resources and techniques for discovering traits that make health-related text difficult and developing a tool to guide the simplification of text. . There are four specific aims of this project: 1) Identify differentiating features of easy versus difficult texts, 2) Design a simplification strategy using computer algorithms, 3) Measure the impact of simplification on perceived and actual text difficulty with online participants and a representative community sample, 4) Create free, online software that incorporates proven features algorithmically. Corpus analysis will be conducted to compare easy and difficult texts with each other and discover lexical, grammatical, semantic, and composition and discourse features typical for each. Then, simplification algorithms will be designed and developed relying on rule-based techniques to leverage available resources, e.g., vocabularies, or on machine learning approaches for discovering the best combinations of features for simplification. A representative writer will simplify text by relying on the suggestios provided by an online that tool that uses simplification algorithms. The effect of simplification wll be tested in comprehensive user studies to evaluate the effect on both actual and perceived difficulty. Features successfully shown to decrease text difficulty will be incorporated in an onlie software program designed to reduce text difficulty.         PUBLIC HEALTH RELEVANCE: Improving health literacy is an important national goal and necessary trait for a healthy population. Providing understandable information is critical but few tools exist to help write understandable text. We aim to discover features indicative of difficult text, design translation algorithms and create a free, online software tool for rewriting health-related text with demonstrated impact on perceived and actual text difficulty            ",Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers,8884888,R01LM011975,"['Address', 'Advocate', 'Affect', 'Algorithms', 'American', 'Arizona', 'Behavior', 'Caregivers', 'Caring', 'Chronic Disease', 'Communities', 'Complement', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Conflict (Psychology)', 'Data Set', 'Education', 'Education Projects', 'Ensure', 'Faculty', 'Feedback', 'Funding', 'Goals', 'Health', 'Health Promotion', 'Health Services Accessibility', 'Health behavior', 'Health education', 'Healthcare', 'Healthy People 2020', 'Individual', 'Lead', 'Longevity', 'Machine Learning', 'Measures', 'Medical', 'Medical Informatics', 'Medicine', 'Methods', 'Minority', 'Mullerian duct inhibiting substance', 'Natural Language Processing', 'Outcome', 'Participant', 'Patients', 'Pilot Projects', 'Population', 'Process', 'Public Health', 'Readability', 'Reader', 'Recovery', 'Recruitment Activity', 'Research', 'Research Design', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Survival Rate', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Translations', 'Underserved Population', 'United States Dept. of Health and Human Services', 'Universities', 'Vocabulary', 'Work', 'Writing', 'base', 'college', 'combat', 'cost effective', 'design', 'evidence base', 'health literacy', 'healthy lifestyle', 'improved', 'lexical', 'programs', 'public health relevance', 'tool', 'trait', 'user-friendly', 'volunteer']",NLM,UNIVERSITY OF ARIZONA,R01,2015,376917,0.1578748986021615
"From Syntactic Relations to Semantic Predications: Porting Open Information Extraction to Biomedicine ﻿    DESCRIPTION (provided by applicant):ABSTRACT Owing to exponential growth in scientiﬁc literature, it has become increasingly difﬁcult for researchers to keep up with the latest developments in their ﬁelds of study. Hence, computational approaches that automatically mine large amounts of free text to extract essential information have gained popularity. This information is typically represented in the form of binary relations between different biomedical concepts. In this context, automatic extraction of meaningful relations from natural language narratives, a task often termed biomedical relation extraction (BRE), has garnered attention from informaticians. The relations extracted are used in high level applications including information retrieval (IR), literature based discovery (LBD), question answering, and text summarization. Most current BRE efforts tend to focus on a speciﬁc subdomain in biomedicine. For example, researchers built models that extract gene-protein or gene-gene interactions; in the clinical domain, recent results are focused on drug-drug and drug-disease interactions mentioned in clinical narratives. The only effort that extracts a broad set of relations adhering t a large standardized vocabulary is the rule based SemRep program being developed by researchers at the National Library of Medicine (NLM). SemRep extracts binary relations, called semantic predications, between biomedical entities from the UMLS Metathesaurus with predicates coming from an extension of the UMLS Semantic Network. Although SemRep achieves reasonable precision, its recall is very low on a gold standard dataset created for its evaluation. Given many applications in LBD and IR already use the predication database SemMedDB (obtained by running SemRep on all biomedical citations made available through PubMed), a predication extraction framework with a higher recall and a low acceptable loss in precision is more desirable especially if it can complement SemRep's extractions. We propose to build and evaluate a supervised BRE framework that converts syntactic relations obtained using the paradigm of open information extraction (OIE) to semantic predications by leveraging the existing database of predications in SemMedDB and relations from the UMLS Metathesaurus through distant supervision. We will conduct domain independent evaluation based on a gold standard dataset built by researchers at the NLM for evaluating SemRep. We will also conduct application oriented evaluations by simulating predication graph based document and passage retrieval using the Text REtrieval Conference (TREC) Genomics and OHSUMED datasets for IR experiments. We will also evaluate the quality of subgraphs resulting from LBD experiments to rediscover nine well known biomedical discoveries. We hypothesize that the predications extracted through our methods will complement those in SemMedDB and the combined predication dataset will result in improved overall performance compared with using SemMedDB alone. NARRATIVE Semantic predications are binary relations extracted from biomedical text by the SemRep program and connect biomedical entities with a ﬁxed set of relation types. Although SemRep extractions have reasonable precision, their recall is very low. We propose to build a supervised predication extraction framework whose results will complement SemRep's extractions in terms of improved performance in both direct gold standard evaluation and application oriented evaluation in the context of information retrieval and literature based discovery.",From Syntactic Relations to Semantic Predications: Porting Open Information Extraction to Biomedicine,9274042,R21LM012274,"['Area', 'Attention', 'Automated Abstracting', 'Classification', 'Clinical', 'Complement', 'Computer software', 'Data Set', 'Databases', 'Development', 'Discipline', 'Disease', 'Distant', 'Drug Interactions', 'Evaluation', 'Event', 'Explosion', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genomics', 'Gold', 'Graph', 'Growth', 'Ice', 'Information Retrieval', 'Literature', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Output', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Process', 'PubMed', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Running', 'Semantics', 'Standardization', 'Supervision', 'System', 'Testing', 'Text', 'Training', 'UML Entity', 'UMLS Metathesaurus', 'Unified Medical Language System', 'United States National Library of Medicine', 'Update', 'Vent', 'Vocabulary', 'Work', 'base', 'experimental study', 'field study', 'gene interaction', 'improved', 'indexing', 'metathesaurus', 'natural language', 'open source', 'programs', 'protein function', 'search engine', 'symposium', 'syntax', 'web services']",NLM,UNIVERSITY OF KENTUCKY,R21,2017,164263,0.09804483477299114
"From Syntactic Relations to Semantic Predications: Porting Open Information Extraction to Biomedicine ﻿    DESCRIPTION (provided by applicant):ABSTRACT Owing to exponential growth in scientiﬁc literature, it has become increasingly difﬁcult for researchers to keep up with the latest developments in their ﬁelds of study. Hence, computational approaches that automatically mine large amounts of free text to extract essential information have gained popularity. This information is typically represented in the form of binary relations between different biomedical concepts. In this context, automatic extraction of meaningful relations from natural language narratives, a task often termed biomedical relation extraction (BRE), has garnered attention from informaticians. The relations extracted are used in high level applications including information retrieval (IR), literature based discovery (LBD), question answering, and text summarization. Most current BRE efforts tend to focus on a speciﬁc subdomain in biomedicine. For example, researchers built models that extract gene-protein or gene-gene interactions; in the clinical domain, recent results are focused on drug-drug and drug-disease interactions mentioned in clinical narratives. The only effort that extracts a broad set of relations adhering t a large standardized vocabulary is the rule based SemRep program being developed by researchers at the National Library of Medicine (NLM). SemRep extracts binary relations, called semantic predications, between biomedical entities from the UMLS Metathesaurus with predicates coming from an extension of the UMLS Semantic Network. Although SemRep achieves reasonable precision, its recall is very low on a gold standard dataset created for its evaluation. Given many applications in LBD and IR already use the predication database SemMedDB (obtained by running SemRep on all biomedical citations made available through PubMed), a predication extraction framework with a higher recall and a low acceptable loss in precision is more desirable especially if it can complement SemRep's extractions. We propose to build and evaluate a supervised BRE framework that converts syntactic relations obtained using the paradigm of open information extraction (OIE) to semantic predications by leveraging the existing database of predications in SemMedDB and relations from the UMLS Metathesaurus through distant supervision. We will conduct domain independent evaluation based on a gold standard dataset built by researchers at the NLM for evaluating SemRep. We will also conduct application oriented evaluations by simulating predication graph based document and passage retrieval using the Text REtrieval Conference (TREC) Genomics and OHSUMED datasets for IR experiments. We will also evaluate the quality of subgraphs resulting from LBD experiments to rediscover nine well known biomedical discoveries. We hypothesize that the predications extracted through our methods will complement those in SemMedDB and the combined predication dataset will result in improved overall performance compared with using SemMedDB alone.             NARRATIVE Semantic predications are binary relations extracted from biomedical text by the SemRep program and connect biomedical entities with a ﬁxed set of relation types. Although SemRep extractions have reasonable precision, their recall is very low. We propose to build a supervised predication extraction framework whose results will complement SemRep's extractions in terms of improved performance in both direct gold standard evaluation and application oriented evaluation in the context of information retrieval and literature based discovery.",From Syntactic Relations to Semantic Predications: Porting Open Information Extraction to Biomedicine,9021081,R21LM012274,"['Area', 'Attention', 'Automated Abstracting', 'Clinical', 'Complement', 'Computer software', 'Data Set', 'Databases', 'Development', 'Discipline', 'Disease', 'Distant', 'Drug Interactions', 'Evaluation', 'Event', 'Explosion', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genomics', 'Gold', 'Graph', 'Growth', 'Information Retrieval', 'Literature', 'Measures', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Output', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Process', 'PubMed', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Running', 'Semantics', 'Supervision', 'System', 'Testing', 'Text', 'Training', 'UMLS Metathesaurus', 'Unified Medical Language System', 'United States National Library of Medicine', 'Update', 'Vent', 'Vocabulary', 'Work', 'abstracting', 'base', 'gene interaction', 'improved', 'indexing', 'metathesaurus', 'model building', 'natural language', 'open source', 'programs', 'protein function', 'research study', 'search engine', 'symposium', 'syntax', 'web services']",NLM,UNIVERSITY OF KENTUCKY,R21,2016,209697,0.09804483477299114
"A High-memory Supercomputer for Proteomics, Text Mining and Microbiome Research     DESCRIPTION (provided by applicant): We request funds to purchase an integrated supercomputer to unite 5 highly productive and collaborative laboratories with complementary expertise in the microbiome, proteomics, text mining, and supercomputing, and to extend these capabilities to the broader NIH-funded biomedical research community via cloud and web applications. The critical shared need not met by other systems on campus, unavailable in commercial clouds, and oversubscribed at national labs, is for a system that can run jobs that require high memory (8-32 GB/core) and long duration (>2 weeks wall-time), and is optimized for high-IO tasks that saturate network or storage on other systems. The system will consist of 128 servers, each using 2x8-core 2.93GHz Intel Sandybridge CPUs. 20 large-memory nodes will each have 512GB of RAM (32GB/core), and 100 compute nodes will each have 128GB of RAM (8GB/core). These 120 nodes will each use two 10Gbps Ethernet ports bonded together for a 20Gbps/node (2.5GB/s) connection to the rest of the system, and each node will have 2.4TB raw high- performance local storage. The total aggregate performance of these local disks is over 36GB/s sustained (>300MB/s per node). The remaining 8 nodes will be used for administration, support for advanced software tools and infrastructure, and user interaction. A central high-performance Lustre parallel file system will provide 1.15PB of usable scratch space and sustain 36GB/s to the 128 clients. An archival system of 4 drives/300 tapes will sustain >1GB/s aggregate (accounting for compression), provide 450TB of raw capacity, store ~4.5 PB of user data, and scale to 5x this size. The system, valued at $4.5 million but quoted at $2 million by HP due to the strategic importance of this partnership, will be housed in a state-of-the art machine room in the new Jennie Smoly Caruthers Biotechnology Building on the Boulder campus (opening Feb 2012), and connect to the rest of the campus at 40Gbps. The system will be a key enabling technology for key scientific areas where data growth is exponential and current systems on campus are end-of-life, solely dedicated to other purposes, or optimized for other tasks. The major users will use the instrument largely for time-consuming one-time tasks such as parameter optimization for microbiome and genome assembly workflows, building knowledgebases, and performing simulations and database searches that will provide resources that are re-used by much broader user communities (hundreds of collaborators; thousands of end users) who lack supercomputing access. One key innovative aspect of this proposal is configuration of part of the system as an academic cloud, which will allow us to pilot workflows that can later be deployed by diverse users on commercial clouds (e.g. Amazon EC2) and academic clouds (e.g. Magellan and DIAG) once those clouds are upgraded. The system will also build a broad expertise base in high-performance computing in the life sciences through outreach to promising new faculty and trainees on NIH training grants, and collaborations with new users of the Sequencing Core. The proposed instrument will thus have a profound impact on NIH-funded research.             n/a","A High-memory Supercomputer for Proteomics, Text Mining and Microbiome Research",8334437,S10OD012300,"['Accounting', 'Area', 'Biological Sciences', 'Biomedical Research', 'Biotechnology', 'Client', 'Collaborations', 'Communities', 'DNA Sequencing Facility', 'Data', 'Databases', 'Faculty', 'Funding', 'Genome', 'Grant', 'Growth', 'High Performance Computing', 'Housing', 'Internet', 'Laboratories', 'Memory', 'Occupations', 'Performance', 'Proteomics', 'Research', 'Research Infrastructure', 'Resources', 'Rest', 'Running', 'Software Tools', 'Supercomputing', 'System', 'Technology', 'Time', 'Training', 'United States National Institutes of Health', 'base', 'end of life', 'innovation', 'instrument', 'knowledge base', 'meetings', 'microbiome', 'outreach', 'simulation', 'supercomputer', 'text searching']",OD,UNIVERSITY OF COLORADO,S10,2013,1900000,0.049974003876059186
"Uncovering Clinical Evidence in COVID-19 Publications: An Integrated Search via Text & Images Project Summary: Uncovering clinical evidence in COVID19 publications: An integrated search via text & images The proposed research aims to develop and advance tools for using image-data appearing in scientific publications, in addition to text, in order to expedite effective access to COVID-19 published information. Current efforts aiming to address the COVID-19 pandemic include devising treatment, understanding virus mechanisms, detecting infection and antibodies, and ultimately – developing a vaccine. All these efforts require effective access to biomedical information related to the virus. The Allen Institute has recently released the CORD-19 dataset – a large, continually updated collection of scientific literature pertaining to COVID-19 and Corona viruses. This dataset comprises tens of thousands full text articles, forming a basis for text-mining tools that will support access to information pertaining to COVID-19. Notably, much of the evidence within these publications is provided in the form of figures. Furthermore, regions where such evidential images occur are rich in information. While biomedical text-based mining tools are being quickly developed and offered for accessing this dataset, images, which contain key clinical and biological information, are not considered. Even outside the COVID-19 realm, little has been done so far to utilize images within publications, despite the fact that they provide important cues about the relevance of the information embedded in articles. Our premise, which is supported by our own and by other informaticians and clinicians experience, is that information derived from images can (and should) be directly incorporated into the biomedical – and specifically into the COVID-19 – document retrieval and extraction. Doing so will improve accurate access to relevant articles, while pin-pointing significant evidence within them, and expediting access to much-needed critical information. The work on this project will result in methods and tools that take advantage of both image- and text-data, facilitating more effective and focused retrieval and mining, thus better supporting speedy data- intensive discovery in the context of COVID-19. Project Narrative: Uncovering clinical evidence in COVID19 publications: An integrated search via text & images Researchers and physicians looking to understand, treat and ultimately cure and vaccinate against the elusive and devastating COVID19, must search through vast amounts of published biomedical information. The proposed research aims to support and speed-up the search while improving effective access to the most relevant part of the COVID19 literature, by creating a tool that utilizes and searches for the highly-informative image data within publications. The successful outcome of this work will provide a well-targeted, effective search engine for finding information pertinent to the medical and research needs of scientists and physicians working to address COVID19, thus expediting discovery, and revealing potential complications, and their causes and their likely treatment.",Uncovering Clinical Evidence in COVID-19 Publications: An Integrated Search via Text & Images,10177479,R01LM012527,"['Access to Information', 'Address', 'Administrative Supplement', 'Antibodies', 'Biological', 'Biomedical Research', 'Blood', 'COVID-19', 'COVID-19 pandemic', 'Classification', 'Clinical', 'Collection', 'Computer Analysis', 'Coronavirus', 'Cues', 'Data', 'Data Set', 'Detection', 'Disease', 'Drug Targeting', 'Funding', 'Goals', 'Graph', 'Harvest', 'Image', 'Image Analysis', 'Image retrieval system', 'Individual', 'Infection', 'Information Retrieval', 'Institutes', 'Literature', 'Lung', 'MRI Scans', 'Medical Research', 'Methods', 'Mining', 'Modality', 'Molecular', 'Outcome', 'Oxygen', 'Physicians', 'Positioning Attribute', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Roentgen Rays', 'Scientist', 'Speed', 'System', 'Testing', 'Text', 'Update', 'Vaccinated', 'Vaccines', 'Virus', 'Visual', 'Visualization', 'Visualization software', 'Work', 'X-Ray Computed Tomography', 'base', 'experience', 'improved', 'indexing', 'information display', 'interest', 'microscopic imaging', 'search engine', 'text searching', 'therapy development', 'three dimensional structure', 'tool', 'vaccine development', 'visual search']",NLM,UNIVERSITY OF DELAWARE,R01,2020,74999,0.1336614739032566
"Incorporating Image-based Features into Biomedical Document Classification The proposed research aims to develop and advance tools for using image-data appearing in scientific publications, in addition to text, in order to support beneficial, targeted access to the biomedical literature. The number of biomedical publications grows at a rate of over one million new publications per year. Identifying relevant information requires scientists and physicians to scan daily through a myriad of papers. For scientific database curators (bio-curators, in organizations such as Jackson Labs or UniProt), the task is particularly onerous, as they must identify articles most significant to the database, locate within them high-quality evidence concerning disease, genes/proteins and mutations, and curate the findings in database entries along with references to relevant evidence in the articles. Notably, much of the evidence within publications lies in figures. Thus, images are rich and essential indicators for relevance.  While biomedical text mining tools are being developed to expedite search for information within publications, several competitive shared tasks underscored the need for more effective tools to overcome the bottleneck for bio-curation and for scientific discovery. Moreover, bio-curators point-out the importance of images as a key information source. While image analysis is an active research field, most current work on biomedical image processing focuses on image identification, understanding and indexing; Not on images as aids to document analysis. Similarly, most work on biomedical literature mining focuses on text alone. Thus, little has been done so far to utilize, in addition to text, images within publications that provide important cues about the relevance of the information embedded in articles.  Our premise, supported by bio-curators experience, is that information derived from images can (and should) be directly incorporated into biomedical document retrieval and classification, and will improve accurate identification of relevant articles (for a given user’s needs) while pin-pointing significant evidence within them. We will comprehensively identify, develop and compare informative image-features, develop methods and tools for representing both images and documents based on such features, and introduce means to effectively integrate image-based data into the text-based document classification process. The work will comprise the following fundamental tasks: A) Building robust tools for harvesting images from PDF articles and segmenting compound figures into individual image-panels; B) Identification and investigation of highly-informative features for biomedical image-representation, and categorization of biomedical images into significant types and classes; C) Effective representation of documents using text and image, and integration of text-based and image-based classifiers. We anchor our research in genuine needs, secure access to much image data, and strive for broad-applicability of the results, by working within several broad and diverse curation-areas within institutes with which we collaborate: Evidence for gene-expression & phenotypes in Mouse (Jackson Labs) and in worm (WormBase), and experimental evidence for protein-protein interaction (Protein Information Resource). The work on this project will result in new methods and tools that take advantage of both image- and text-data, facilitating more effective and focused retrieval and mining, thus better supporting bio-curation and data-intensive biomedical discovery. Published biomedical literature forms a vast information-source for biomedical scientists and physicians; both treatment decisions and research toward bio-medical discovery are based on such information. The proposed research aims to support and speed-up the search for information while improving effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage of the highly-informative image data within publications. The successful outcome of this research will lead to the development of well-targeted, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting focused biomedical discovery, including better understanding of the role of gene mutations in disease mechanisms, uncovering interactions among proteins, and revealing potential new drugs and drug-targets.",Incorporating Image-based Features into Biomedical Document Classification,9989894,R01LM012527,"['Address', 'Area', 'Biological Phenomena', 'Categories', 'Classification', 'Collaborations', 'Computer-Assisted Image Analysis', 'Cues', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Targeting', 'Failure', 'Fluorescence Microscopy', 'Foundations', 'Gel', 'Gene Expression', 'Gene Mutation', 'Gene Proteins', 'Genomics', 'Geometry', 'Goals', 'Grain', 'Harvest', 'Image', 'Image Analysis', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Letters', 'Literature', 'Medical', 'Methods', 'Mining', 'Modeling', 'Mus', 'Mutation', 'Outcomes Research', 'Paper', 'Phenotype', 'Physicians', 'Positioning Attribute', 'Process', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Publishing', 'Research', 'Resource Informatics', 'Retrieval', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Secure', 'Shapes', 'Solid', 'Source', 'Speed', 'System', 'Text', 'Texture', 'Training', 'Work', 'WormBase', 'base', 'bioimaging', 'biomedical scientist', 'decision research', 'evaluation/testing', 'experience', 'experimental study', 'feature extraction', 'image processing', 'improved', 'indexing', 'mouse genome', 'multimodality', 'new therapeutic target', 'novel', 'protein protein interaction', 'protein structure', 'text searching', 'tool']",NLM,UNIVERSITY OF DELAWARE,R01,2020,463024,0.15143074554818453
"Incorporating Image-based Features into Biomedical Document Classification The proposed research aims to develop and advance tools for using image-data appearing in scientific publications, in addition to text, in order to support beneficial, targeted access to the biomedical literature. The number of biomedical publications grows at a rate of over one million new publications per year. Identifying relevant information requires scientists and physicians to scan daily through a myriad of papers. For scientific database curators (bio-curators, in organizations such as Jackson Labs or UniProt), the task is particularly onerous, as they must identify articles most significant to the database, locate within them high-quality evidence concerning disease, genes/proteins and mutations, and curate the findings in database entries along with references to relevant evidence in the articles. Notably, much of the evidence within publications lies in figures. Thus, images are rich and essential indicators for relevance.  While biomedical text mining tools are being developed to expedite search for information within publications, several competitive shared tasks underscored the need for more effective tools to overcome the bottleneck for bio-curation and for scientific discovery. Moreover, bio-curators point-out the importance of images as a key information source. While image analysis is an active research field, most current work on biomedical image processing focuses on image identification, understanding and indexing; Not on images as aids to document analysis. Similarly, most work on biomedical literature mining focuses on text alone. Thus, little has been done so far to utilize, in addition to text, images within publications that provide important cues about the relevance of the information embedded in articles.  Our premise, supported by bio-curators experience, is that information derived from images can (and should) be directly incorporated into biomedical document retrieval and classification, and will improve accurate identification of relevant articles (for a given user’s needs) while pin-pointing significant evidence within them. We will comprehensively identify, develop and compare informative image-features, develop methods and tools for representing both images and documents based on such features, and introduce means to effectively integrate image-based data into the text-based document classification process. The work will comprise the following fundamental tasks: A) Building robust tools for harvesting images from PDF articles and segmenting compound figures into individual image-panels; B) Identification and investigation of highly-informative features for biomedical image-representation, and categorization of biomedical images into significant types and classes; C) Effective representation of documents using text and image, and integration of text-based and image-based classifiers. We anchor our research in genuine needs, secure access to much image data, and strive for broad-applicability of the results, by working within several broad and diverse curation-areas within institutes with which we collaborate: Evidence for gene-expression & phenotypes in Mouse (Jackson Labs) and in worm (WormBase), and experimental evidence for protein-protein interaction (Protein Information Resource). The work on this project will result in new methods and tools that take advantage of both image- and text-data, facilitating more effective and focused retrieval and mining, thus better supporting bio-curation and data-intensive biomedical discovery. Published biomedical literature forms a vast information-source for biomedical scientists and physicians; both treatment decisions and research toward bio-medical discovery are based on such information. The proposed research aims to support and speed-up the search for information while improving effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage of the highly-informative image data within publications. The successful outcome of this research will lead to the development of well-targeted, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting focused biomedical discovery, including better understanding of the role of gene mutations in disease mechanisms, uncovering interactions among proteins, and revealing potential new drugs and drug-targets.",Incorporating Image-based Features into Biomedical Document Classification,9762175,R01LM012527,"['Address', 'Area', 'Biological Phenomena', 'Categories', 'Classification', 'Collaborations', 'Computer-Assisted Image Analysis', 'Cues', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Targeting', 'Failure', 'Fluorescence Microscopy', 'Foundations', 'Gel', 'Gene Expression', 'Gene Mutation', 'Gene Proteins', 'Genomics', 'Geometry', 'Goals', 'Grain', 'Harvest', 'Image', 'Image Analysis', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Letters', 'Literature', 'Medical', 'Methods', 'Mining', 'Modeling', 'Mus', 'Mutation', 'Outcomes Research', 'Paper', 'Phenotype', 'Physicians', 'Positioning Attribute', 'Process', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Publishing', 'Research', 'Resource Informatics', 'Retrieval', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Secure', 'Shapes', 'Solid', 'Source', 'Speed', 'Structural Protein', 'System', 'Text', 'Texture', 'Training', 'Work', 'base', 'bioimaging', 'biomedical scientist', 'decision research', 'evaluation/testing', 'experience', 'experimental study', 'image processing', 'improved', 'indexing', 'mouse genome', 'multimodality', 'new therapeutic target', 'novel', 'protein protein interaction', 'protein structure', 'text searching', 'tool']",NLM,UNIVERSITY OF DELAWARE,R01,2019,463024,0.15143074554818453
"Incorporating Image-based Features into Biomedical Document Classification The proposed research aims to develop and advance tools for using image-data appearing in scientific publications, in addition to text, in order to support beneficial, targeted access to the biomedical literature. The number of biomedical publications grows at a rate of over one million new publications per year. Identifying relevant information requires scientists and physicians to scan daily through a myriad of papers. For scientific database curators (bio-curators, in organizations such as Jackson Labs or UniProt), the task is particularly onerous, as they must identify articles most significant to the database, locate within them high-quality evidence concerning disease, genes/proteins and mutations, and curate the findings in database entries along with references to relevant evidence in the articles. Notably, much of the evidence within publications lies in figures. Thus, images are rich and essential indicators for relevance.  While biomedical text mining tools are being developed to expedite search for information within publications, several competitive shared tasks underscored the need for more effective tools to overcome the bottleneck for bio-curation and for scientific discovery. Moreover, bio-curators point-out the importance of images as a key information source. While image analysis is an active research field, most current work on biomedical image processing focuses on image identification, understanding and indexing; Not on images as aids to document analysis. Similarly, most work on biomedical literature mining focuses on text alone. Thus, little has been done so far to utilize, in addition to text, images within publications that provide important cues about the relevance of the information embedded in articles.  Our premise, supported by bio-curators experience, is that information derived from images can (and should) be directly incorporated into biomedical document retrieval and classification, and will improve accurate identification of relevant articles (for a given user’s needs) while pin-pointing significant evidence within them. We will comprehensively identify, develop and compare informative image-features, develop methods and tools for representing both images and documents based on such features, and introduce means to effectively integrate image-based data into the text-based document classification process. The work will comprise the following fundamental tasks: A) Building robust tools for harvesting images from PDF articles and segmenting compound figures into individual image-panels; B) Identification and investigation of highly-informative features for biomedical image-representation, and categorization of biomedical images into significant types and classes; C) Effective representation of documents using text and image, and integration of text-based and image-based classifiers. We anchor our research in genuine needs, secure access to much image data, and strive for broad-applicability of the results, by working within several broad and diverse curation-areas within institutes with which we collaborate: Evidence for gene-expression & phenotypes in Mouse (Jackson Labs) and in worm (WormBase), and experimental evidence for protein-protein interaction (Protein Information Resource). The work on this project will result in new methods and tools that take advantage of both image- and text-data, facilitating more effective and focused retrieval and mining, thus better supporting bio-curation and data-intensive biomedical discovery. Published biomedical literature forms a vast information-source for biomedical scientists and physicians; both treatment decisions and research toward bio-medical discovery are based on such information. The proposed research aims to support and speed-up the search for information while improving effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage of the highly-informative image data within publications. The successful outcome of this research will lead to the development of well-targeted, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting focused biomedical discovery, including better understanding of the role of gene mutations in disease mechanisms, uncovering interactions among proteins, and revealing potential new drugs and drug-targets.",Incorporating Image-based Features into Biomedical Document Classification,9565648,R01LM012527,"['Address', 'Area', 'Biological Phenomena', 'Categories', 'Classification', 'Collaborations', 'Computer-Assisted Image Analysis', 'Cues', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Targeting', 'Failure', 'Fluorescence Microscopy', 'Foundations', 'Gel', 'Gene Expression', 'Gene Mutation', 'Gene Proteins', 'Genomics', 'Geometry', 'Goals', 'Grain', 'Gray unit of radiation dose', 'Harvest', 'Image', 'Image Analysis', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Letters', 'Literature', 'Medical', 'Methods', 'Mining', 'Modality', 'Modeling', 'Mus', 'Mutation', 'Outcomes Research', 'Paper', 'Phenotype', 'Physicians', 'Positioning Attribute', 'Process', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Publishing', 'Research', 'Resource Informatics', 'Retrieval', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Secure', 'Shapes', 'Solid', 'Source', 'Speed', 'System', 'Text', 'Texture', 'Training', 'Work', 'base', 'bioimaging', 'biomedical scientist', 'decision research', 'evaluation/testing', 'experience', 'experimental study', 'image processing', 'improved', 'indexing', 'mouse genome', 'new therapeutic target', 'novel', 'protein protein interaction', 'protein structure', 'text searching', 'tool']",NLM,UNIVERSITY OF DELAWARE,R01,2018,437876,0.15143074554818453
"Incorporating Image-based Features into Biomedical Document Classification The proposed research aims to develop and advance tools for using image-data appearing in scientific publications, in addition to text, in order to support beneficial, targeted access to the biomedical literature. The number of biomedical publications grows at a rate of over one million new publications per year. Identifying relevant information requires scientists and physicians to scan daily through a myriad of papers. For scientific database curators (bio-curators, in organizations such as Jackson Labs or UniProt), the task is particularly onerous, as they must identify articles most significant to the database, locate within them high-quality evidence concerning disease, genes/proteins and mutations, and curate the findings in database entries along with references to relevant evidence in the articles. Notably, much of the evidence within publications lies in figures. Thus, images are rich and essential indicators for relevance.  While biomedical text mining tools are being developed to expedite search for information within publications, several competitive shared tasks underscored the need for more effective tools to overcome the bottleneck for bio-curation and for scientific discovery. Moreover, bio-curators point-out the importance of images as a key information source. While image analysis is an active research field, most current work on biomedical image processing focuses on image identification, understanding and indexing; Not on images as aids to document analysis. Similarly, most work on biomedical literature mining focuses on text alone. Thus, little has been done so far to utilize, in addition to text, images within publications that provide important cues about the relevance of the information embedded in articles.  Our premise, supported by bio-curators experience, is that information derived from images can (and should) be directly incorporated into biomedical document retrieval and classification, and will improve accurate identification of relevant articles (for a given user’s needs) while pin-pointing significant evidence within them. We will comprehensively identify, develop and compare informative image-features, develop methods and tools for representing both images and documents based on such features, and introduce means to effectively integrate image-based data into the text-based document classification process. The work will comprise the following fundamental tasks: A) Building robust tools for harvesting images from PDF articles and segmenting compound figures into individual image-panels; B) Identification and investigation of highly-informative features for biomedical image-representation, and categorization of biomedical images into significant types and classes; C) Effective representation of documents using text and image, and integration of text-based and image-based classifiers. We anchor our research in genuine needs, secure access to much image data, and strive for broad-applicability of the results, by working within several broad and diverse curation-areas within institutes with which we collaborate: Evidence for gene-expression & phenotypes in Mouse (Jackson Labs) and in worm (WormBase), and experimental evidence for protein-protein interaction (Protein Information Resource). The work on this project will result in new methods and tools that take advantage of both image- and text-data, facilitating more effective and focused retrieval and mining, thus better supporting bio-curation and data-intensive biomedical discovery. Published biomedical literature forms a vast information-source for biomedical scientists and physicians; both treatment decisions and research toward bio-medical discovery are based on such information. The proposed research aims to support and speed-up the search for information while improving effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage of the highly-informative image data within publications. The successful outcome of this research will lead to the development of well-targeted, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting focused biomedical discovery, including better understanding of the role of gene mutations in disease mechanisms, uncovering interactions among proteins, and revealing potential new drugs and drug-targets.",Incorporating Image-based Features into Biomedical Document Classification,9457095,R01LM012527,"['Address', 'Area', 'Biological Phenomena', 'Categories', 'Cereals', 'Classification', 'Collaborations', 'Computer-Assisted Image Analysis', 'Cues', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Failure', 'Fluorescence Microscopy', 'Foundations', 'Gel', 'Gene Expression', 'Gene Mutation', 'Gene Proteins', 'Genomics', 'Geometry', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Image', 'Image Analysis', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Letters', 'Literature', 'Medical', 'Methods', 'Mining', 'Modality', 'Modeling', 'Mus', 'Mutation', 'Outcomes Research', 'Paper', 'Phenotype', 'Physicians', 'Positioning Attribute', 'Process', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Publishing', 'Research', 'Resource Informatics', 'Retrieval', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Secure', 'Shapes', 'Solid', 'Source', 'Speed', 'System', 'Text', 'Texture', 'Training', 'Work', 'base', 'bioimaging', 'biomedical scientist', 'decision research', 'evaluation/testing', 'experience', 'experimental study', 'image processing', 'improved', 'indexing', 'mouse genome', 'new therapeutic target', 'novel', 'novel therapeutics', 'protein protein interaction', 'protein structure', 'text searching', 'tool']",NLM,UNIVERSITY OF DELAWARE,R01,2017,488239,0.15143074554818453
"Image-guided Biocuration of Disease Pathways From Scientific Literature Realization of precision medicine ideas requires an unprecedented rapid pace of translation of biomedical discoveries into clinical practice. However, while many non-canonical disease pathways and uncommon drug actions, which are of vital importance for understanding individual patient-specific disease pathways, are accumulated in the literature, most are not organized in databases. Currently, such knowledge is curated manually or semi-automatically in a very limited scope. Meanwhile, the volume of biomedical information in PubMed (currently 28 million publications) keeps growing by more than a million articles per year, which demands more efficient and effective biocuration approaches.  To address this challenge, a novel biocuration method for automatic extraction of disease pathways from figures and text of biomedical articles will be developed.  Specific Aim 1: To develop focused benchmark sets of articles to assess the performance of the biocuration pipeline.  Specific Aim 2: To develop a method for extraction of components of disease pathways from articles’ figures based on deep-learning techniques.  Specific Aim 3: To develop a method for reconstruction of disease-specific pathways through enrichment and through graph neural network (GNN) approaches.  Specific Aim 4: To conduct a comprehensive evaluation of the pipeline.  The overarching goal of this project is to develop a computer-based automatic biocuration ecosystem for rapid transformation of free-text biomedical literature into a machine-processable format for medical applications.  The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. It will especially benefit cancer patients for which up-to-date knowledge of newly discovered molecular mechanisms and drug actions is critical. The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. In this project, a novel biocuration method for an automatic extraction of disease mechanisms from figures and text in scientific literature will be developed. These mechanisms will be stored in a database for further querying to assist in medical diagnosis and treatment.",Image-guided Biocuration of Disease Pathways From Scientific Literature,9987133,R01LM013392,"['Address', 'Architecture', 'Benchmarking', 'Biological', 'Cancer Patient', 'Communities', 'Computers', 'Databases', 'Deposition', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Pathway', 'Ecosystem', 'Elements', 'Evaluation', 'Feedback', 'Genes', 'Goals', 'Graph', 'Health', 'Image', 'Informatics', 'Knowledge', 'Label', 'Language', 'Link', 'Literature', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Measures', 'Medical', 'Methods', 'Molecular', 'Molecular Analysis', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Oxidative Stress', 'Pathway interactions', 'Patients', 'Performance', 'Phenotype', 'PubMed', 'Publications', 'Regulation', 'Reporting', 'Research', 'Retrieval', 'Selection Criteria', 'Signal Pathway', 'Source', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Translations', 'Visual', 'Work', 'base', 'clinical practice', 'deep learning', 'design', 'detector', 'drug action', 'image guided', 'improved', 'individual patient', 'knowledge base', 'knowledge curation', 'multimodality', 'neural network', 'neural network architecture', 'novel', 'precision medicine', 'reconstruction', 'success', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2020,313495,0.09390842166832182
"Data Mining to Enhance Medical Research of Clincal Data    DESCRIPTION (provided by applicant):    The purpose of this proposal is to develop the use of text mining and data mining tools to investigate the relationship between physician practice and health outcomes. In the absence of specific guidelines, there remains substantial variability in physician practice for patients with similar diagnoses. It is possible to investigate that variability to find an optimal practice pattern. Since much of the information concerning patient outcomes is written as chart notes, text mining must be used to extract useful intelligence from those notes.   Aim I. To develop an algorithm to extract useful information from clinical records. This includes abstract  information from databases, physician notes in patient records, and text data electronically recorded in  pharmacy, nursing, and care management databases. This will enable to automatic extraction of meaningful intelligence from patient charts when the current method is primarily by using coders who perform manual extraction.   Aim II. To apply the developed tools to longitudinal data collected during pharmacist-consultant review of patient databases to determine the extent of polypharmacy in patients in long-term and hospital care who were treated for cardiovascular conditions and other co-morbid diseases. Standard statistical techniques cannot generally be used to examine polypharmacy because of the complexity of the data. Text mining can be used to automatically relate similar medications into meaningful clusters so that the relationship between drug interaction and medical outcome can be examined.   Aim III.To explore the relationship between treatment and disease. Data mining tools can be used to examine the relationship between physician treatment and quality. Text mining can be used to categorize descriptive measures of patient quality.   Aim IV. To provide a series of workshops to present the results to medical researchers to demonstrate the benefits of data mining techniques in analyzing clinical outcomes and to build a program of medical internships for students enrolled in the recently approved PhD program in Industrial and Applied Mathematics. The PhD program has a mandatory application requirement and internship.         n/a",Data Mining to Enhance Medical Research of Clincal Data,6700108,R15RR017285,"['abstracting', 'clinical research', 'computer program /software', 'data management', 'health care facility information system', 'health care quality', 'hospital utilization', 'human data', 'information retrieval', 'mathematics', 'medical records', 'nursing care', 'pharmacy']",NCRR,UNIVERSITY OF LOUISVILLE,R15,2004,146916,0.11289566445458524
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,7033080,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2006,133459,0.1161274424601392
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6865478,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2005,162750,0.1161274424601392
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6709988,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2004,161668,0.1161274424601392
"BIGDATA Small Project Structurization and Direct Search of Medical Image Data DESCRIPTION (provided by applicant): IBM estimates that 30% of the entire data in the world is medical information. Medical images occupy a significant portion of medical records with approximately 100 million scans in US and growing every year. In addition, the data size from each scan steadfastly increases as the image resolution improves. These BigData are not structured and due to lack of standardized imaging protocols, they are highly heterogeneous with different spatial resolutions, contrasts, slice orientations, etc. In this project, we will deelop a technology to structure and search medical imaging information, which will make the past data available for education and evidence-based clinical decision-making. In this grant, we will focus on brain MRI, which comprises the largest portion of MRI data. The target community will be physicians who make decisions and the patients will be the ultimate beneficiaries. Currently, radiological image data are stored in clinical database called PACS. The image data in PACS are not structured. Consequently, once the diagnosis of a patient is completed, most of the data in PACS are currently discarded in the archive. Radiologists rely on their experience and education to reach medical decisions. This is a typical problem in medical practice that calls for objective evidence-based medicine. There are many ongoing attempts to structure the text fields of PACS, which include natural language processing of free-text radiological reports, clinical information, and diagnosis. In our approach, we propose to structure the image data, not text fields, to support direct search of images. Namely, physicians will submit an image of a new patient and search past images with similar anatomical phenotypes. Then, the clinical reports of the retrieved data will be compiled for a statistical report of the diagnosis and prognosis. We believe this image structuration is the key to ""unlock the vast amounts of information currently stored"" in PACS and use them for education and modern evidence-based medical decisions. The specific aims are; Objective 1: To develop and test the accuracy of high-throughput image structuration technologies Objective 2: To develop and test the image search engine Objective 3: Capacity Building Requirement: To develop prototype cloud system for data structuration / search services for research and educational purposes n/a",BIGDATA Small Project Structurization and Direct Search of Medical Image Data,8852613,R01EB017638,"['Archives', 'Brain', 'Clinical', 'Communities', 'Data', 'Databases', 'Decision Making', 'Diagnosis', 'Education', 'Evidence Based Medicine', 'Grant', 'Health Services Research', 'Image', 'Information Systems', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Medical Records', 'Natural Language Processing', 'Patients', 'Phenotype', 'Physicians', 'Protocols documentation', 'Reporting', 'Resolution', 'Scanning', 'Slice', 'Structure', 'Technology', 'Testing', 'Text', 'beneficiary', 'clinical decision-making', 'evidence base', 'experience', 'improved', 'outcome forecast', 'prototype', 'radiologist']",NIBIB,JOHNS HOPKINS UNIVERSITY,R01,2015,213115,0.04978902021990553
"BIGDATA Small Project Structurization and Direct Search of Medical Image Data     DESCRIPTION (provided by applicant): IBM estimates that 30% of the entire data in the world is medical information. Medical images occupy a significant portion of medical records with approximately 100 million scans in US and growing every year. In addition, the data size from each scan steadfastly increases as the image resolution improves. These BigData are not structured and due to lack of standardized imaging protocols, they are highly heterogeneous with different spatial resolutions, contrasts, slice orientations, etc. In this project, we will deelop a technology to structure and search medical imaging information, which will make the past data available for education and evidence-based clinical decision-making. In this grant, we will focus on brain MRI, which comprises the largest portion of MRI data. The target community will be physicians who make decisions and the patients will be the ultimate beneficiaries. Currently, radiological image data are stored in clinical database called PACS. The image data in PACS are not structured. Consequently, once the diagnosis of a patient is completed, most of the data in PACS are currently discarded in the archive. Radiologists rely on their experience and education to reach medical decisions. This is a typical problem in medical practice that calls for objective evidence-based medicine. There are many ongoing attempts to structure the text fields of PACS, which include natural language processing of free-text radiological reports, clinical information, and diagnosis. In our approach, we propose to structure the image data, not text fields, to support direct search of images. Namely, physicians will submit an image of a new patient and search past images with similar anatomical phenotypes. Then, the clinical reports of the retrieved data will be compiled for a statistical report of the diagnosis and prognosis. We believe this image structuration is the key to ""unlock the vast amounts of information currently stored"" in PACS and use them for education and modern evidence-based medical decisions. The specific aims are; Objective 1: To develop and test the accuracy of high-throughput image structuration technologies Objective 2: To develop and test the image search engine Objective 3: Capacity Building Requirement: To develop prototype cloud system for data structuration / search services for research and educational purposes                  n/a",BIGDATA Small Project Structurization and Direct Search of Medical Image Data,8664845,R01EB017638,"['Archives', 'Brain', 'Clinical', 'Communities', 'Data', 'Databases', 'Decision Making', 'Diagnosis', 'Education', 'Evidence Based Medicine', 'Grant', 'Health Services Research', 'Image', 'Information Systems', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Medical Records', 'Natural Language Processing', 'Patients', 'Phenotype', 'Physicians', 'Protocols documentation', 'Reporting', 'Resolution', 'Scanning', 'Slice', 'Structure', 'Technology', 'Testing', 'Text', 'beneficiary', 'clinical decision-making', 'evidence base', 'experience', 'improved', 'outcome forecast', 'prototype', 'radiologist']",NIBIB,JOHNS HOPKINS UNIVERSITY,R01,2014,214609,0.04978902021990553
"BIGDATA Small Project Structurization and Direct Search of Medical Image Data     DESCRIPTION (provided by applicant): IBM estimates that 30% of the entire data in the world is medical information. Medical images occupy a significant portion of medical records with approximately 100 million scans in US and growing every year. In addition, the data size from each scan steadfastly increases as the image resolution improves. These BigData are not structured and due to lack of standardized imaging protocols, they are highly heterogeneous with different spatial resolutions, contrasts, slice orientations, etc. In this project, we will deelop a technology to structure and search medical imaging information, which will make the past data available for education and evidence-based clinical decision-making. In this grant, we will focus on brain MRI, which comprises the largest portion of MRI data. The target community will be physicians who make decisions and the patients will be the ultimate beneficiaries. Currently, radiological image data are stored in clinical database called PACS. The image data in PACS are not structured. Consequently, once the diagnosis of a patient is completed, most of the data in PACS are currently discarded in the archive. Radiologists rely on their experience and education to reach medical decisions. This is a typical problem in medical practice that calls for objective evidence-based medicine. There are many ongoing attempts to structure the text fields of PACS, which include natural language processing of free-text radiological reports, clinical information, and diagnosis. In our approach, we propose to structure the image data, not text fields, to support direct search of images. Namely, physicians will submit an image of a new patient and search past images with similar anatomical phenotypes. Then, the clinical reports of the retrieved data will be compiled for a statistical report of the diagnosis and prognosis. We believe this image structuration is the key to ""unlock the vast amounts of information currently stored"" in PACS and use them for education and modern evidence-based medical decisions. The specific aims are; Objective 1: To develop and test the accuracy of high-throughput image structuration technologies Objective 2: To develop and test the image search engine Objective 3: Capacity Building Requirement: To develop prototype cloud system for data structuration / search services for research and educational purposes                  n/a",BIGDATA Small Project Structurization and Direct Search of Medical Image Data,8599843,R01EB017638,"['Archives', 'Brain', 'Clinical', 'Communities', 'Data', 'Databases', 'Decision Making', 'Diagnosis', 'Education', 'Evidence Based Medicine', 'Grant', 'Health Services Research', 'Image', 'Information Systems', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Medical Records', 'Natural Language Processing', 'Patients', 'Phenotype', 'Physicians', 'Protocols documentation', 'Reporting', 'Resolution', 'Scanning', 'Slice', 'Structure', 'Technology', 'Testing', 'Text', 'beneficiary', 'clinical decision-making', 'evidence base', 'experience', 'improved', 'outcome forecast', 'prototype', 'radiologist']",NIBIB,JOHNS HOPKINS UNIVERSITY,R01,2013,224942,0.04978902021990553
"Human Subject Research Enhancements Program We propose to enhance the data consistency and integrity of oversight and tracking systems for human subjects research at Mayo Foundation. Our specific aims include: 1) a comprehensive information modeling exercise to understand the interrelationships and dependencies of administrative and clinical data elements related to human subjects research oversight; 2) building common application components that will simplify the creation of research protocols, IRB application, research subject enrollment and consent, and administrative tracking; 3) providing full text and natural language processing based indices to project abstracts, applications, minutes, and administrative notes, to facilitate the authorized searching and retrieval of materials human subject related to human subject review; and 4) coordinating the information model, modular software tools, and textual indexing, as preliminary work for a competitive informatics proposal for adverse event recognition, pattern detection, and the consistent recording of drugs, devices and outcomes measures. n/a",Human Subject Research Enhancements Program,6591449,S07RR018225,"['abstracting', ' behavioral /social science research tag', ' clinical research', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' health science research support', ' human rights', ' information systems']",NCRR,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",S07,2002,183410,-0.011711231338578956
"Resolving Biological Entity References (Text/Databases)    DESCRIPTION (provided by applicant): The ""Big Idea"" driving this project is that human memory is small, the body of scientific knowledge is vast and that breakthroughs are possible if software can do a better job of connecting researchers with knowledge in text and databases. The first step is to stop looking for words (as a search engine does) in data but instead try to find facts in data. A fact is something claimed in text or a database explicitly. A fact may not be true but we want to develop software that finds those facts. What does a fact look like? Consider the following sentence from MEDLINE: ""Recently, we have found that Htt is an antiapoptotic protein in striatal cells and acts by preventing caspase-3 activity."" It contains the fact that the gene id 6532 (in the Entrez Gene database) regulates gene id 836. Software can extract such facts from a sentence like this. But the current state-of-the-art is not doing a great job of it. The reason is simple-current systems are focused on not making mistakes which means that they miss a lot of opportunities to find facts. The best reported performance is around 40% of the facts being found which we think is severely compromising the usefulness of text mining technologies in bioinformatics. This is where we are trying a different approach-we are focused on finding all the facts. We call this ""total recall"" which we demonstrated was possible in Phase I but total recall comes with a price: we make lots of mistakes. The key innovation is that we keep score of how confident we are of any given fact which gives us an important point of leverage in sifting good from bad facts. Our Phase II proposal focuses on developing techniques to reason over such fact heavy analysis by exploring soft clustering approaches, structured classification and effective user interface design. We have partnered with Harvard, Columbia and Pfizer to keep our research effort focused on problems that actually matter for genomics experiments and early phase drug discovery. In addition we fit into the NIH's data sharing policy by making our software free (with source code) to organizations who make their data free too. We, as do many others, believe that many great scientific discoveries lay implicit and just below the surface of the research literature. All that is required is for the right researcher to see the right sentence or database entry to form a novel hypothesis and cure a disease. Total recall approaches to fact extraction make that all the more likely an outcome. The dominant paradigm in text mining is to treat the text like a database. But researchers would be better served with a more ""search"" like approach to extracting and correlating facts in text and databases. We are committed to making all the facts, or total recall, available to scientists which is currently not available.          n/a",Resolving Biological Entity References (Text/Databases),7475803,R44RR020259,"['Address', 'Algorithms', 'Apoptosis', 'Arts', 'Autistic Disorder', 'Automobile Driving', 'Binding', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological databases', 'Biology', 'Cations', 'Cells', 'Classification', 'Clinical', 'Commit', 'Computer software', 'Corpus striatum structure', 'Data', 'Databases', 'Disease', 'Epigenetic Process', 'Genes', 'Genomics', 'Graph', 'Human', 'Internet', 'Joints', 'Knowledge', 'Literature', 'MEDLINE', 'MeSH Thesaurus', 'Memory', 'Mental disorders', 'Methylation', 'Modality', 'Modeling', 'Occupations', 'Ontology', 'Outcome', 'Pathway interactions', 'Pattern', 'Performance', 'Phase', 'Policy Making', 'Price', 'Probability', 'Proteins', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Scientist', 'Score', 'Source', 'Source Code', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Thinking', 'Training', 'Variant', 'Walking', 'Work', 'abstracting', 'base', 'caspase-3', 'design', 'drug discovery', 'high throughput analysis', 'innovation', 'novel', 'open source', 'prevent', 'prototype', 'research and development', 'research study', 'software development', 'systems research', 'text searching', 'tool', 'uptake']",NCRR,ALIAS-I,R44,2008,373278,0.14904426934878226
"Resolving Biological Entity References (Text/Databases)    DESCRIPTION (provided by applicant): The ""Big Idea"" driving this project is that human memory is small, the body of scientific knowledge is vast and that breakthroughs are possible if software can do a better job of connecting researchers with knowledge in text and databases. The first step is to stop looking for words (as a search engine does) in data but instead try to find facts in data. A fact is something claimed in text or a database explicitly. A fact may not be true but we want to develop software that finds those facts. What does a fact look like? Consider the following sentence from MEDLINE: ""Recently, we have found that Htt is an antiapoptotic protein in striatal cells and acts by preventing caspase-3 activity."" It contains the fact that the gene id 6532 (in the Entrez Gene database) regulates gene id 836. Software can extract such facts from a sentence like this. But the current state-of-the-art is not doing a great job of it. The reason is simple-current systems are focused on not making mistakes which means that they miss a lot of opportunities to find facts. The best reported performance is around 40% of the facts being found which we think is severely compromising the usefulness of text mining technologies in bioinformatics. This is where we are trying a different approach-we are focused on finding all the facts. We call this ""total recall"" which we demonstrated was possible in Phase I but total recall comes with a price: we make lots of mistakes. The key innovation is that we keep score of how confident we are of any given fact which gives us an important point of leverage in sifting good from bad facts. Our Phase II proposal focuses on developing techniques to reason over such fact heavy analysis by exploring soft clustering approaches, structured classification and effective user interface design. We have partnered with Harvard, Columbia and Pfizer to keep our research effort focused on problems that actually matter for genomics experiments and early phase drug discovery. In addition we fit into the NIH's data sharing policy by making our software free (with source code) to organizations who make their data free too. We, as do many others, believe that many great scientific discoveries lay implicit and just below the surface of the research literature. All that is required is for the right researcher to see the right sentence or database entry to form a novel hypothesis and cure a disease. Total recall approaches to fact extraction make that all the more likely an outcome. The dominant paradigm in text mining is to treat the text like a database. But researchers would be better served with a more ""search"" like approach to extracting and correlating facts in text and databases. We are committed to making all the facts, or total recall, available to scientists which is currently not available.          n/a",Resolving Biological Entity References (Text/Databases),7327948,R44RR020259,"['Address', 'Algorithms', 'Apoptosis', 'Arts', 'Autistic Disorder', 'Automobile Driving', 'Binding', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological databases', 'Biology', 'Cations', 'Cells', 'Classification', 'Clinical', 'Commit', 'Computer software', 'Corpus striatum structure', 'Data', 'Databases', 'Disease', 'Epigenetic Process', 'Genes', 'Genomics', 'Graph', 'Human', 'Internet', 'Joints', 'Knowledge', 'Literature', 'MEDLINE', 'MeSH Thesaurus', 'Memory', 'Mental disorders', 'Methylation', 'Modality', 'Modeling', 'Occupations', 'Ontology', 'Outcome', 'Pathway interactions', 'Pattern', 'Performance', 'Phase', 'Policy Making', 'Price', 'Probability', 'Proteins', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Scientist', 'Score', 'Source', 'Source Code', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Thinking', 'Training', 'Variant', 'Walking', 'Work', 'abstracting', 'base', 'caspase-3', 'design', 'drug discovery', 'high throughput analysis', 'innovation', 'novel', 'open source', 'prevent', 'prototype', 'research and development', 'research study', 'software development', 'systems research', 'text searching', 'tool', 'uptake']",NCRR,ALIAS-I,R44,2007,374415,0.14904426934878226
"Sci-Score, a tool to support rigor and transparency guidelines Project Summary While  standards  in  reporting  of  scientific  methods  are  absolutely  critical  to  producing  reproducible  science,  meeting  such  standards  is  difficult.  Checklists  and  instructions  are  tough  to  follow  often  resulting  in  low  and inconsistent  compliance.  Scientific  journals  and  societies  as  well  as  the  National  Institutes  of  Health  are  now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods  (e.g.,  http://www.cell.com/star-­methods),  but  the  trickier  part  will  be  to  train  the  biomedical  community  to  use these standards to effectively improve how scientific methods are communicated. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency of  Key Biological Resources, we propose to build Sci-­Score a text mining based tool suite to help authors meet the  standard.  Sci-­Score  will  provide  an  automated  check  on  compliance  with  the  RRID  standard  already  implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. The innovation behind Sci-­ score is the provision of a score, which can be obtained by individual investigators, which reflects a numerical  validation of the quality of their methods reporting. We posit that the score will serve as a tool that investigators  and journals can use to compete with themselves and each other, or in the very least allow them to see how  close they are to the average in meeting quality requirements.   Recently, our group has developed a text mining algorithm that has now been successfully been used to detect software tools and databases from the SciCrunch Registry in published papers. Digital tools are one of four resource types that the RRID standard identifies. We propose to extend this approach to the other types of entities: antibodies, cell lines and model organisms. Resource identification along with other quality metrics twill be used to train an algorithm to score the overall quality of the methods document. If successful, the tool could be used by editors, reviewers, and investigators to improve the number of RRIDs, therefore the quality of descriptors of key biological resources in published papers. This SBIR project will build a set of algorithms similar to the resource finding pipeline and develop it into an industrial robust and reconfigurable software system. Our Phase I specific aims include to 1) creating gold sets of data for each resource type and training a set of algorithms for each resource type; 2) designing and evaluating the scoring system; 3) designing and evaluating a report generating system based on the previous aims. In Phase II, we will develop a scalable backend infrastructure to serve the needs of scientific publishers and research community. Standards for scientific methods reporting are absolutely critical to producing reproducible science, but meeting  such standards is difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent  compliance. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency, we propose to build Sci-Score text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. Sci-Score will provide a score rating the quality of  methods reporting in submitted articles, which provides feedback to authors, reviewers and editors on how to improvecompliance with RRIDs and other standards. ","Sci-Score, a tool to support rigor and transparency guidelines",9345707,R43OD024432,"['Address', 'Agreement', 'Algorithms', 'Animal Model', 'Antibodies', 'Area', 'Big Data', 'Biological', 'California', 'Cell Line', 'Cell model', 'Cells', 'Communities', 'Data Set', 'Databases', 'Descriptor', 'Elements', 'Ensure', 'Evaluation', 'Feedback', 'Funding', 'Glare', 'Gold', 'Guidelines', 'Habits', 'Human', 'Individual', 'Industrialization', 'Instruction', 'Journals', 'Learning', 'Literature', 'Machine Learning', 'Manuscripts', 'Methods', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neurosciences', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Process', 'Publications', 'Publishing', 'Readability', 'Reader', 'Reading', 'Reagent', 'Registries', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sales', 'Science', 'Scoring Method', 'Services', 'Small Business Innovation Research Grant', 'Societies', 'Software Tools', 'System', 'Technology', 'Text', 'To specify', 'Training', 'United States National Institutes of Health', 'Universities', 'Validation', 'Work', 'Writing', 'base', 'biological systems', 'computerized tools', 'design', 'digital', 'improved', 'innovation', 'interest', 'meetings', 'prototype', 'software systems', 'sound', 'text searching', 'tool', 'vigilance', 'web site']",OD,"SCICRUNCH, INC.",R43,2017,221865,0.08413931702121794
"Extracting Semantic Knowledge from Clinical Reports    DESCRIPTION (provided by applicant): Analyzing and processing free-text medical reports for data mining and clinical data interchange is one of the most challenging problems in medical informatics, yet it is crucial for continued research advances and improvements in clinical care. Natural language processing (NLP) is an important enabling technology, but has been held back because it is difficult to understand human language, since it requires extensive domain knowledge. In Phase I, we developed new statistical and machine learning methods that apply domain specific knowledge to the semantic analysis of free-text radiology reports. The methods enabled the creation of two new prototype applications - a SNOMED CT (Systematized Nomenclature of Medicine--Clinical Terms) coding service called SnomedCoder, and a text mining tool for analyzing a large corpus of medical reports, called DataMiner. In Phase II, we will accomplish the following specific aims: 1) Improve the semantic extraction methods developed in Phase I, 2) Expand the semantic knowledge base and classify at least two million new unique sentences from multiple medical institutions, 3) Provide a SNOMED CT auto coding service (alpha service) to participating Indiana Health Information Exchange hospitals, and 4) Build a commercial version of the DataMiner software, and test its functionality using researchers at the Regenstrief Institute.       These scientific innovations will revolutionize the ability of health care researchers to analyze vast repositories of clinical information currently locked up in electronic medical records, and correlate this data with new biomedical discoveries in proteonomics and genomics. The ability to codify text rapidly will extend the potential for clinical decision support beyond its narrow base of numeric and structured medical data, and enable SNOMED CT to become a useful coding standard. Phase III will offer coding and data mining services to healthcare payers (both private and government), pharmaceuticals, and academic researchers. A key advantage of our approach over other NLP systems is that we attempt to codify all the information in the report and not just a limited subset, and insist on expert validation which provides a high degree of confidence in the accuracy of the coded data.Project Narrative           n/a",Extracting Semantic Knowledge from Clinical Reports,7554153,R44RR024929,"['Address', 'Algorithms', 'Back', 'Bioinformatics', 'Body of uterus', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Code', 'Collection', 'Computer software', 'Computerized Medical Record', 'Data', 'Decision Making', 'Effectiveness', 'Genomics', 'Goals', 'Government', 'Health', 'Healthcare', 'Hospitals', 'Human', 'Indiana', 'Institutes', 'Institution', 'Journals', 'Knowledge', 'Language', 'Longitudinal Studies', 'Machine Learning', 'Medical', 'Medical Informatics', 'Medical Records', 'Methods', 'Natural Language Processing', 'Paper', 'Pharmacologic Substance', 'Phase', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Services', 'Speed', 'Structure', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Testing', 'Text', 'Trees', 'Trust', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Validation', 'base', 'clinical care', 'computerized', 'data mining', 'health care quality', 'improved', 'indexing', 'innovation', 'knowledge base', 'novel strategies', 'patient safety', 'prototype', 'repository', 'research and development', 'success', 'text searching', 'tool']",NCRR,"LOGICAL SEMANTICS, INC.",R44,2009,422728,-0.03179832903574138
"Extracting Semantic Knowledge from Clinical Reports    DESCRIPTION (provided by applicant): Analyzing and processing free-text medical reports for data mining and clinical data interchange is one of the most challenging problems in medical informatics, yet it is crucial for continued research advances and improvements in clinical care. Natural language processing (NLP) is an important enabling technology, but has been held back because it is difficult to understand human language, since it requires extensive domain knowledge. In Phase I, we developed new statistical and machine learning methods that apply domain specific knowledge to the semantic analysis of free-text radiology reports. The methods enabled the creation of two new prototype applications - a SNOMED CT (Systematized Nomenclature of Medicine--Clinical Terms) coding service called SnomedCoder, and a text mining tool for analyzing a large corpus of medical reports, called DataMiner. In Phase II, we will accomplish the following specific aims: 1) Improve the semantic extraction methods developed in Phase I, 2) Expand the semantic knowledge base and classify at least two million new unique sentences from multiple medical institutions, 3) Provide a SNOMED CT auto coding service (alpha service) to participating Indiana Health Information Exchange hospitals, and 4) Build a commercial version of the DataMiner software, and test its functionality using researchers at the Regenstrief Institute.       These scientific innovations will revolutionize the ability of health care researchers to analyze vast repositories of clinical information currently locked up in electronic medical records, and correlate this data with new biomedical discoveries in proteonomics and genomics. The ability to codify text rapidly will extend the potential for clinical decision support beyond its narrow base of numeric and structured medical data, and enable SNOMED CT to become a useful coding standard. Phase III will offer coding and data mining services to healthcare payers (both private and government), pharmaceuticals, and academic researchers. A key advantage of our approach over other NLP systems is that we attempt to codify all the information in the report and not just a limited subset, and insist on expert validation which provides a high degree of confidence in the accuracy of the coded data.Project Narrative           n/a",Extracting Semantic Knowledge from Clinical Reports,7394699,R44RR024929,"['Address', 'Algorithms', 'Back', 'Bioinformatics', 'Body of uterus', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Code', 'Collection', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Reporting', 'Decision Making', 'Effectiveness', 'Genomics', 'Goals', 'Government', 'Health', 'Healthcare', 'Hospitals', 'Human', 'Indiana', 'Institutes', 'Institution', 'Journals', 'Knowledge', 'Language', 'Longitudinal Studies', 'Machine Learning', 'Medical', 'Medical Informatics', 'Medical Records', 'Methods', 'Natural Language Processing', 'Paper', 'Pharmacologic Substance', 'Phase', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Services', 'Speed', 'Standards of Weights and Measures', 'Structure', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Testing', 'Text', 'Thinking', 'Trees', 'Trust', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Validation', 'base', 'computerized', 'data mining', 'health care quality', 'improved', 'indexing', 'innovation', 'knowledge base', 'novel strategies', 'patient safety', 'prototype', 'repository', 'research and development', 'success', 'text searching', 'tool']",NCRR,"LOGICAL SEMANTICS, INC.",R44,2008,429955,-0.03179832903574138
"Towards the Building of a Comprehensive Searchable Biological Experiment Database    DESCRIPTION (provided by applicant):       The rapid growth of the biomedical literature and the expansion in disciplinary biomedical research, heralded by high-throughput genome sciences and technologies, have overwhelmed scientists who attempt to assimilate information necessary for their research. The widespread adoption of title/abstract word searches, such as highly desirable the National Library of Medicine's PubMed system, has provided the first major advance in the way bioscientists find relevant publications since the origin of Index Medicus in 1879 (Hunter and Cohen 2006). The importance of developing valid information retrieval systems for bioscientists has led to the development of information systems worldwide (e.g., Arrowsmith (Smalheiser and Swanson 1998), BioText (Hearst 2003), GeneWays (Friedman et al. 2001; Rzhetsky et al. 2004), iHOP (Hoffmann and Valencia 2005), and BioMedQA (Lee et al. 2006a), and annotated databases (e.g., SWISSPROT, OMIM (Hamosh et al. 2005) and BIND (Alfarano et al. 2005)).      However, most of information systems target only text information and fail to provide access to other important data such as images (e.g., figures). More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biological articles nearly always incorporate figures/images that are the crucial content of the biomedical literature. Our examination of biological articles in the Proceedings of the National Academy of Sciences (PNAS) revealed the occurrence of 5.2 images per article on average (Yu and Lee 2006a). Biologists need to access image data to validate research facts and to formulate or to test novel research hypotheses. It has been evaluated that textual statements reported in literature frequently are noisy (i.e., containing ""false facts"") (Krauthammer et al. 2002). Capturing images that are experimental ""evidence"" to support the textual ""fact"" will benefit bioscience information systems, databases, and bioscientists.      Unfortunately, this wealth of information remains virtually inaccessible without automatic systems to organize these images. We propose the development of advanced natural language processing (NLP) tools to semantically organize images. We hypothesize that text that associated with images semantically entails the image content and natural language processing techniques can be developed to accurately associate the text to their images. Furthermore, we hypothesize that images can be semantically organized by categories specified by standard biological ontology, and that natural language processing approaches can accurately assign the ontological categories to images.      Our specific aims are:      Aim 1: To develop and evaluate NLP techniques for identifying textual statements that correspond to images in full-text articles. We will develop different approaches for two types of the associations. We will first propose rule-based and statistical approaches to identify the associated text that appears in the full-text articles. We will then develop hybrid approaches to link sentences in abstracts to images in the body of the articles.      Aim 2: To develop and evaluate NLP techniques for automatic classification of experimental results into categories (e.g., Western-Blot, PCR verification, etc) specified in the experimental protocol Protocol-Online.      We will explore the use of dictionary-based, rule-based, image classification, and machine-learning approaches for accomplishing this aim.      Aim 3: To develop and evaluate NLP techniques for automatic assignment of Gene Ontology categories to experiments, which will provide a knowledge-based organization of experiments according to biological properties (e.g., catalytic activity). We will develop statistical and machine-learning approaches for accomplishing this aim.      We found that most of the images that appear in full-text biological articles are figure images (Yu and Lee 2006a) and we therefore focus on figure images only in this proposal. The deliverable of Specific Aim 1 will be an effective user-interface BioEx from which bioscientists can access images directly from sentences in the abstracts. BioEx has the promise of improvement over the traditional single-document-per-article format that has dominated bioscience publications since the first scientific article appeared in 1665 (Gross 2002). The deliverables of Specific Aim 2 and 3 will be open-source algorithms and tools that accurately map images to categories specified by the Gene Ontology and the Protocol Online. Those algorithms and tools will enhance bioscience information retrieval, information extraction, summarization, and question answering.          n/a",Towards the Building of a Comprehensive Searchable Biological Experiment Database,7534822,R21RR024933,"['Adoption', 'Advanced Development', 'Algorithms', 'Binding', 'Biological', 'Biomedical Research', 'Categories', 'Classification', 'Data', 'Databases', 'Development', 'Dictionary', 'Documentation', 'Flowcharts', 'Genes', 'Genome', 'Hybrids', 'Image', 'Index Medicus', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Link', 'Literature', 'Machine Learning', 'Maps', 'Natural Language Processing', 'Online Mendelian Inheritance In Man', 'Ontology', 'Principal Investigator', 'Property', 'Protocols documentation', 'PubMed', 'Publications', 'Reporting', 'Research', 'Science', 'Scientist', 'Specific qualifier value', 'SwissProt', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'United States National Academy of Sciences', 'United States National Library of Medicine', 'Western Blotting', 'abstracting', 'base', 'knowledge base', 'novel', 'open source', 'programs', 'rapid growth', 'research study', 'tool']",NCRR,UNIVERSITY OF WISCONSIN MILWAUKEE,R21,2009,179517,-0.03697493555410921
"Towards the Building of a Comprehensive Searchable Biological Experiment Database    DESCRIPTION (provided by applicant):       The rapid growth of the biomedical literature and the expansion in disciplinary biomedical research, heralded by high-throughput genome sciences and technologies, have overwhelmed scientists who attempt to assimilate information necessary for their research. The widespread adoption of title/abstract word searches, such as highly desirable the National Library of Medicine's PubMed system, has provided the first major advance in the way bioscientists find relevant publications since the origin of Index Medicus in 1879 (Hunter and Cohen 2006). The importance of developing valid information retrieval systems for bioscientists has led to the development of information systems worldwide (e.g., Arrowsmith (Smalheiser and Swanson 1998), BioText (Hearst 2003), GeneWays (Friedman et al. 2001; Rzhetsky et al. 2004), iHOP (Hoffmann and Valencia 2005), and BioMedQA (Lee et al. 2006a), and annotated databases (e.g., SWISSPROT, OMIM (Hamosh et al. 2005) and BIND (Alfarano et al. 2005)).      However, most of information systems target only text information and fail to provide access to other important data such as images (e.g., figures). More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biological articles nearly always incorporate figures/images that are the crucial content of the biomedical literature. Our examination of biological articles in the Proceedings of the National Academy of Sciences (PNAS) revealed the occurrence of 5.2 images per article on average (Yu and Lee 2006a). Biologists need to access image data to validate research facts and to formulate or to test novel research hypotheses. It has been evaluated that textual statements reported in literature frequently are noisy (i.e., containing ""false facts"") (Krauthammer et al. 2002). Capturing images that are experimental ""evidence"" to support the textual ""fact"" will benefit bioscience information systems, databases, and bioscientists.      Unfortunately, this wealth of information remains virtually inaccessible without automatic systems to organize these images. We propose the development of advanced natural language processing (NLP) tools to semantically organize images. We hypothesize that text that associated with images semantically entails the image content and natural language processing techniques can be developed to accurately associate the text to their images. Furthermore, we hypothesize that images can be semantically organized by categories specified by standard biological ontology, and that natural language processing approaches can accurately assign the ontological categories to images.      Our specific aims are:      Aim 1: To develop and evaluate NLP techniques for identifying textual statements that correspond to images in full-text articles. We will develop different approaches for two types of the associations. We will first propose rule-based and statistical approaches to identify the associated text that appears in the full-text articles. We will then develop hybrid approaches to link sentences in abstracts to images in the body of the articles.      Aim 2: To develop and evaluate NLP techniques for automatic classification of experimental results into categories (e.g., Western-Blot, PCR verification, etc) specified in the experimental protocol Protocol-Online.      We will explore the use of dictionary-based, rule-based, image classification, and machine-learning approaches for accomplishing this aim.      Aim 3: To develop and evaluate NLP techniques for automatic assignment of Gene Ontology categories to experiments, which will provide a knowledge-based organization of experiments according to biological properties (e.g., catalytic activity). We will develop statistical and machine-learning approaches for accomplishing this aim.      We found that most of the images that appear in full-text biological articles are figure images (Yu and Lee 2006a) and we therefore focus on figure images only in this proposal. The deliverable of Specific Aim 1 will be an effective user-interface BioEx from which bioscientists can access images directly from sentences in the abstracts. BioEx has the promise of improvement over the traditional single-document-per-article format that has dominated bioscience publications since the first scientific article appeared in 1665 (Gross 2002). The deliverables of Specific Aim 2 and 3 will be open-source algorithms and tools that accurately map images to categories specified by the Gene Ontology and the Protocol Online. Those algorithms and tools will enhance bioscience information retrieval, information extraction, summarization, and question answering.          n/a",Towards the Building of a Comprehensive Searchable Biological Experiment Database,7314689,R21RR024933,"['Adoption', 'Advanced Development', 'Algorithms', 'Binding', 'Biological', 'Biomedical Research', 'Categories', 'Classification', 'Data', 'Databases', 'Development', 'Dictionary', 'Documentation', 'Flowcharts', 'Genes', 'Genome', 'Hybrids', 'Image', 'Index Medicus', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Link', 'Literature', 'Machine Learning', 'Maps', 'Natural Language Processing', 'Online Mendelian Inheritance In Man', 'Ontology', 'Polymerase Chain Reaction', 'Principal Investigator', 'Property', 'Protocols documentation', 'PubMed', 'Publications', 'Reporting', 'Research', 'Science', 'Scientist', 'Specific qualifier value', 'Standards of Weights and Measures', 'SwissProt', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Title', 'United States National Academy of Sciences', 'United States National Library of Medicine', 'Western Blotting', 'abstracting', 'base', 'knowledge base', 'novel', 'open source', 'programs', 'rapid growth', 'research study', 'tool']",NCRR,UNIVERSITY OF WISCONSIN MILWAUKEE,R21,2008,230085,-0.03697493555410921
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6375859,P01CA051198,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NCI,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2001,2071652,0.02510484479981226
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6094628,P01CA051198,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NCI,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2000,1926078,0.02510484479981226
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6923756,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,395905,0.05622652405714036
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6777028,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,341671,0.05622652405714036
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6936159,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,52940,0.05622652405714036
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6685421,R01GM061372,"['Internet', ' artificial intelligence', ' automated data processing', ' biological signal transduction', ' biomedical automation', ' computer system design /evaluation', ' functional /structural genomics', ' high throughput technology', ' intermolecular interaction', ' method development', ' molecular biology information system', ' statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,323936,0.05622652405714036
"EXEMPLAR-BASED MEDICAL TEXT CLASSIFICATION The goal of this research is to develop improved techniques, both fully          automated and computer-assisted, for classification of medical text.  The        technical approach is exemplar-based: robust information retrieval methods       find similar, previously-classified texts, and corresponding codes are           used to suggest likely classifications for a new text.  Phase I focused          upon implementing experimental software to establish baseline performance        with several variations of the exemplar-based approach.                                                                                                           Phase II builds upon this work to implement a complete Coder's Workstation       (CWS).  Based upon Phase I results and assessments of commercial                 opportunities, Phase II will focus upon shorter texts (<12 words), which         are best suited for automated methods.  A ""short-similarity"" capability          will be added to the Phase I approach to further enhance performance with        shorter texts.  To evaluate and refine the CWS, Phase II will include            extensive ""beta testing"" of the software at the Brigham and Women's              Hospital and the Mayo Clinic.                                                                                                                                     The major technical innovation of this project is the development of             highly automated classification software that is sensitive to term               similarities.  The major health-related contributions are large potential        savings in coding expenses, reduced time demands upon physicians for             coding, and improved consistency in classification of free text for              research studies.                                                                                                                                                 PROPOSED COMMERCIAL APPLICATION:  The proposed technology will have              important commercial application within hospitals, insurance companies,          and pharmaceutical companies which currently expend significant resources        on coding of free text (ICD9, CPT4, COSTART, etc.).  The founders of             Belmont Research Inc. have extensive experience in creating and marketing        software to support biomedical applications.                                      n/a",EXEMPLAR-BASED MEDICAL TEXT CLASSIFICATION,2429835,R44CA065250,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' disease /disorder classification', ' human data', ' information retrieval', ' medical records', ' vocabulary development for information system']",NCI,"BELMONT RESEARCH, INC.",R44,1997,387794,0.08043468256307729
"EXEMPLAR-BASED MEDICAL TEXT CLASSIFICATION The goal of this research is to develop improved techniques for fully automated and computer-assisted classification of medical text. A primary focus will be mechanisms that support easy-to-construct classifiers, thus enabling research on existing collections of free-text that are now difficult to analyze.  The proposed approach is exemplar-based: i.e., compare new text with a training set of previously classified texts, and use the classifications of the closest retrieved texts to generate suggested codes for the new text. Natural language extraction techniques will preprocess texts to assist the retrieval machinery.  Phase I will conduct a series of experiments to test the approach, utilizing coded radiology and pathology reports from Brigham and Women's Hospital and HCHP in Boston. Phase II will develop a full software prototype and deploy it for on-site evaluation. Advanced retrieval techniques and expert system back ends for alarming will also be explored in Phase II.  The major technical innovation is a novel combination of document retrieval and natural language extraction technologies to permit easy construction of automated medical text classifiers. The major health- related contribution is an enhanced ability to classify existing free-text records to permit statistical analysis for research and clinical quality- measurement initiatives.  n/a",EXEMPLAR-BASED MEDICAL TEXT CLASSIFICATION,2108095,R43CA065250,"['artificial intelligence', ' computer system design /evaluation', ' disease /disorder classification', ' human data', ' information retrieval', ' medical records']",NCI,"BELMONT RESEARCH, INC.",R43,1994,80985,0.13710829134129476
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7672256,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Readability', 'Reader', 'Reading', 'Self Care', 'Self Management', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'fourth grade', 'improved', 'instrument', 'literacy', 'ninth grade', 'patient oriented', 'prevent', 'programs', 'tenth grade', 'tool', 'web site']",NIDDK,UNIVERSITY OF UTAH,R01,2009,398216,0.11554746211460148
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7475712,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2008,438476,0.11554746211460148
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7671784,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2008,48482,0.11554746211460148
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7303652,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2007,435682,0.11554746211460148
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance  existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of  health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted  readability levels with no critical information loss, using statistical natural language processing  techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive  impact on reader comprehension. We will use as a test bed for our system a general internal medicine  clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public. n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7492453,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2007,47853,0.11554746211460148
"Probabilistic Modeling of Information from Images and Text in Online Journals    DESCRIPTION (provided by applicant): The goal of this project is to build a software toolkit that will enable a biologist to create, from a collection of on-line articles, a database of protein subcellular localization information that can be queried, browsed, or used to support data-mining activities. We have developed a system, called SLIF, which can harvest fluorescence microscope images from online papers, analyze them using image-processing methods, and annotate them with information appearing in the accompanying textual description. We propose to improve and extend this system so as to produce a robust, comprehensive toolkit for extracting information about subcellular localization from the text and images found in online journals, as well as analyzing, verifying and querying the resulting body of information.           n/a",Probabilistic Modeling of Information from Images and Text in Online Journals,7458122,R01GM078622,"['3-Dimensional', 'Architecture', 'Area', 'Arts', 'Biomedical Research', 'Blast Cell', 'Classification', 'Collection', 'Computer software', 'Condition', 'Data', 'Data Set', 'Databases', 'Depth', 'Exhibits', 'Facility Construction Funding Category', 'Feeds', 'Fluorescence Microscopy', 'Future', 'Genes', 'Glycine decarboxylase', 'Goals', 'Harvest', 'Image', 'Individual', 'Journals', 'Literature', 'Location', 'Measures', 'Methods', 'Names', 'Numbers', 'Ontology', 'Operative Surgical Procedures', 'Paper', 'Pattern', 'Property', 'Protein Databases', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Range', 'Reading', 'Research Personnel', 'Resources', 'Sampling', 'Scientist', 'Semantics', 'Standards of Weights and Measures', 'Statistical Models', 'Structure', 'Subcellular structure', 'System', 'Text', 'Time', 'Work', 'abstracting', 'base', 'cell type', 'data mining', 'fluorescence microscope', 'genome sequencing', 'high throughput screening', 'image processing', 'improved', 'intracellular protein transport', 'open source', 'programs', 'protein localization location', 'research study', 'text searching', 'tool', 'two-dimensional']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2008,255967,0.10274457757469721
"Probabilistic Modeling of Information from Images and Text in Online Journals    DESCRIPTION (provided by applicant): The goal of this project is to build a software toolkit that will enable a biologist to create, from a collection of on-line articles, a database of protein subcellular localization information that can be queried, browsed, or used to support data-mining activities. We have developed a system, called SLIF, which can harvest fluorescence microscope images from online papers, analyze them using image-processing methods, and annotate them with information appearing in the accompanying textual description. We propose to improve and extend this system so as to produce a robust, comprehensive toolkit for extracting information about subcellular localization from the text and images found in online journals, as well as analyzing, verifying and querying the resulting body of information.           n/a",Probabilistic Modeling of Information from Images and Text in Online Journals,7241547,R01GM078622,"['3-Dimensional', 'Architecture', 'Area', 'Arts', 'Biomedical Research', 'Blast Cell', 'Classification', 'Collection', 'Computer software', 'Condition', 'Data', 'Data Set', 'Databases', 'Depth', 'Exhibits', 'Facility Construction Funding Category', 'Feeds', 'Fluorescence Microscopy', 'Future', 'Genes', 'Glycine decarboxylase', 'Goals', 'Harvest', 'Image', 'Individual', 'Journals', 'Literature', 'Location', 'Measures', 'Methods', 'Names', 'Numbers', 'Ontology', 'Operative Surgical Procedures', 'Paper', 'Pattern', 'Property', 'Protein Databases', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Range', 'Reading', 'Research Personnel', 'Resources', 'Sampling', 'Scientist', 'Semantics', 'Standards of Weights and Measures', 'Statistical Models', 'Structure', 'Subcellular structure', 'System', 'Text', 'Time', 'Work', 'abstracting', 'base', 'cell type', 'data mining', 'fluorescence microscope', 'genome sequencing', 'high throughput screening', 'image processing', 'improved', 'intracellular protein transport', 'open source', 'programs', 'protein localization location', 'research study', 'text searching', 'tool', 'two-dimensional']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2007,256309,0.10274457757469721
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8840267,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2015,490160,0.1126827699155982
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8734439,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2014,489755,0.1126827699155982
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8474789,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2013,486484,0.1126827699155982
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us    DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems.       This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8309015,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2012,179306,0.1126827699155982
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us    DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems.      PUBLIC HEALTH RELEVANCE: This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.          This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8106768,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2011,500273,0.09421094546543309
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8701603,R01GM095476,[' '],NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2012,332000,0.1126827699155982
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.        Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8733018,UH3HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH3,2013,500000,0.033940031080042954
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.        Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8303361,UH2HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH2,2012,516448,0.033940031080042954
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.      PUBLIC HEALTH RELEVANCE: Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.           Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8145134,UH2HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH2,2011,540294,0.059897795508723414
"Biocreative Conference Grant DESCRIPTION (provided by applicant): The goal of this proposal is to provide a venue for the evaluation of biomedical text mining tools via the shared task paradigm, particularly tools with relevance to problems in biocuration. Biocurators will help define use cases and desired functionalities; stakeholders will come to agreement on task definitions, data sets, and evaluation metrics; text mining teams will then work on these consensus tasks and come together to share their results and gain further feedback from biocurators. PUBLIC HEALTH RELEVANCE: This project consists of the evaluation of biomedical text mining tools, particularly ones applied to projects in biocuration. The results of biocuration are extensively used in modern medical practice, particularly personalized medicine, so the work of the grant has the potential to have a substantial impact on the cutting edge of human healthcare.",Biocreative Conference Grant,9135507,R13GM109648,"['Agreement', 'Algorithms', 'Awareness', 'Bioinformatics', 'Clinical', 'Consensus', 'Data Set', 'Databases', 'Development', 'Doctor of Philosophy', 'Educational workshop', 'Evaluation', 'Feedback', 'Fostering', 'Goals', 'Grant', 'Health', 'Healthcare', 'Housing', 'Human', 'Medical', 'Methods', 'Publishing', 'Research Personnel', 'Series', 'Staging', 'Work', 'career', 'interdisciplinary collaboration', 'personalized medicine', 'student participation', 'symposium', 'text searching', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R13,2016,12000,0.12265197111216057
"Biocreative Conference Grant DESCRIPTION (provided by applicant): The goal of this proposal is to provide a venue for the evaluation of biomedical text mining tools via the shared task paradigm, particularly tools with relevance to problems in biocuration. Biocurators will help define use cases and desired functionalities; stakeholders will come to agreement on task definitions, data sets, and evaluation metrics; text mining teams will then work on these consensus tasks and come together to share their results and gain further feedback from biocurators. PUBLIC HEALTH RELEVANCE: This project consists of the evaluation of biomedical text mining tools, particularly ones applied to projects in biocuration. The results of biocuration are extensively used in modern medical practice, particularly personalized medicine, so the work of the grant has the potential to have a substantial impact on the cutting edge of human healthcare.",Biocreative Conference Grant,8912507,R13GM109648,"['Agreement', 'Algorithms', 'Awareness', 'Bioinformatics', 'Clinical', 'Consensus', 'Data Set', 'Databases', 'Development', 'Doctor of Philosophy', 'Educational workshop', 'Evaluation', 'Feedback', 'Fostering', 'Goals', 'Grant', 'Health', 'Healthcare', 'Housing', 'Human', 'Medical', 'Methods', 'Publishing', 'Research Personnel', 'Series', 'Staging', 'Students', 'Work', 'career', 'interdisciplinary collaboration', 'personalized medicine', 'symposium', 'text searching', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R13,2015,12000,0.12265197111216057
"Biocreative Conference Grant     DESCRIPTION (provided by applicant): The goal of this proposal is to provide a venue for the evaluation of biomedical text mining tools via the shared task paradigm, particularly tools with relevance to problems in biocuration. Biocurators will help define use cases and desired functionalities; stakeholders will come to agreement on task definitions, data sets, and evaluation metrics; text mining teams will then work on these consensus tasks and come together to share their results and gain further feedback from biocurators.         PUBLIC HEALTH RELEVANCE: This project consists of the evaluation of biomedical text mining tools, particularly ones applied to projects in biocuration. The results of biocuration are extensively used in modern medical practice, particularly personalized medicine, so the work of the grant has the potential to have a substantial impact on the cutting edge of human healthcare.            ",Biocreative Conference Grant,8786017,R13GM109648,"['Agreement', 'Algorithms', 'Awareness', 'Bioinformatics', 'Clinical', 'Consensus', 'Data Set', 'Databases', 'Development', 'Doctor of Philosophy', 'Educational workshop', 'Evaluation', 'Feedback', 'Fostering', 'Goals', 'Grant', 'Healthcare', 'Housing', 'Human', 'Medical', 'Medicine', 'Methods', 'Metric', 'Publishing', 'Research Personnel', 'Series', 'Staging', 'Students', 'Work', 'career', 'interdisciplinary collaboration', 'public health relevance', 'symposium', 'text searching', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R13,2014,12000,0.12265197111216057
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9407024,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2018,395628,-0.010931493076656865
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9193091,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2017,396248,-0.010931493076656865
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8985684,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2016,405708,-0.010931493076656865
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS     DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans.         PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.            ",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8817212,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Books', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2015,406247,-0.010931493076656865
"Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing ﻿    DESCRIPTION (provided by applicant): The adoption of Electronic Health Record (EHR) systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format, with tremendous potentials, but also equally growing concern for patient confidentiality breaches. Secondary use of clinical data is essential to fulfill the potentials for high quality healthcare, improved healthcare management, and effective clinical research. NIH expects that larger research projects share their research data in a way that protects the confidentiality of the research subjects. De-identification of patient data has been proposed as a solution to both facilitate secondary uses of clinical data, and protect patient data confidentiality. The majority of clinical data found in the EHR is represented as narrative text clinical notes, and de-identification of clinical text is a tedious ad costly manual endeavor. Automated approaches based on Natural Language Processing have been implemented and evaluated, allowing for much faster de- identification than manual approaches. Clinacuity, Inc. proposes to develop a new system to automatically de-identify clinical notes found in the EHR, to then improve the availability of clinical text for secondary uses, as well as ameliorate the protection of patient data confidentiality. To establish the merit and feasibility of such a system, we will work on the following objectives: 1) create a reference standard for training and testing the text de-identification application, a reference standard that will include a random sample of clinical narratives with protected health information annotated by domain experts; 2) develop a prototype to automatically de-identify clinical text in near real-time, a prototype that will implement a novel stepwise hybrid approach to maximize sensitivity first (our priority for de-identification), and then filter out false positives to enhance positive predictive value, and also replace PHI with realistic substitutes for improved protection; and 3) test the prototype with the aforementioned reference standard, using a cross-validation approach for training and testing, and also train and test the prototype with a reference standard from another healthcare organization. Commercial application: To strengthen patient information confidentiality protection, the HITECH Act heightened financial penalties incurred for breaches of PHI, even introducing criminal penalties. These new severe consequences for violation of patient information confidentiality render protection requirements even more obvious, and automatic high-accuracy clinical text de-identification, as offered by the system Clinacuity proposes, will strongly help healthcare and clinical research organizations avoid such consequences. This system has potential commercial applications in clinical research and in healthcare settings. It will improve access to richer, more detailed, and more accurate clinical data (in clinical text) for clinical researchers. It will also ease research data sharing, and help healthcare organizations protect patient data confidentiality. PUBLIC HEALTH RELEVANCE: The adoption of Electronic Health Record systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical daa becoming available in electronic format, with tremendous potentials, but also equally growing concern for patient confidentiality breaches. Secondary use of clinical data is essential to fulfil the potentials for high quality healthcare and effective clinical research. De-identification of patient data has been proposed as a solution to both facilitate secondary uses of clinical data, and protect patient data confidentiality. The majority of clinical data found in the EHR is represented as narrative text clinical notes that have been dictated and transcribed or directly typed in, and de-identification of clinical text is a tedious and costly manual endeavor. Automated approaches based on Natural Language Processing have been implemented and evaluated, allowing for much faster de-identification than manual approaches. The overall goal of this project is to develop a new system to automatically de-identify clinical narrative text in he Electronic Health Record, to then improve the availability of clinical text for secondary uses, as well as ameliorate the protection of patient data confidentiality.",Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing,8982010,R41GM116479,"['Abbreviations', 'Adoption', 'Affect', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Confidentiality of Patient Information', 'Consent', 'Data', 'Direct Costs', 'Electronic Health Record', 'Electronics', 'Enrollment', 'Eponyms', 'Gilbert Disease', 'Goals', 'Growth', 'Health', 'Health Care Research', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Hybrids', 'Improve Access', 'Incentives', 'Informed Consent', 'Manuals', 'Measurement', 'Measures', 'Methods', 'National Institute of General Medical Sciences', 'Natural Language Processing', 'Patient Care', 'Patient Data Privacy', 'Patients', 'Population', 'Predictive Value', 'Privacy', 'Reference Standards', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Retrospective Studies', 'Risk', 'Sampling', 'Structure', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'United States Centers for Medicare and Medicaid Services', 'United States National Institutes of Health', 'Validation', 'Vision', 'Work', 'base', 'commercial application', 'common rule', 'data sharing', 'flexibility', 'health care quality', 'improved', 'novel', 'patient privacy', 'payment', 'prototype', 'statistics', 'text searching']",NIGMS,"CLINACUITY,INC.",R41,2016,223924,0.045841853916680705
"Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing The adoption of Electronic Health Record (EHR) systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format with tremendous potential but an equally large concern for patient confidentiality breaches. Secondary use of clinical data is essential to fulfill the potential for high quality healthcare, improved healthcare management, and effective clinical research. NIH expects that larger research projects share their research data in a way that protects the confidentiality of research subjects. De-identification of patient data has been proposed as a solution to both facilitate secondary use of clinical data and protect patient data confidentiality. The majority of clinical data found in the EHR is represented as narrative text clinical notes, and de-identification of clinical text is a tedious and costly manual endeavor. Automated approaches based on Natural Language Processing have been implemented and evaluated, allowing for higher accuracy and much faster de- identification than manual approaches. Clinacuity, Inc. proposes to advance a text de-identification system from a prototype to an accurate, adaptable, and robust system, integrated into the research infrastructure at our implementation and testing site (Medical University of South Carolina, Charleston, SC), and ready for commercialization efforts. To accomplish this undertaking, we will focus on the following specific aims and related objectives, while continuing to prepare the commercialization of the integrated system, with detailed market analysis, commercial roadmap development, and modern media communication: 1) Enhance the text de-identification system performance, scalability, and quality to produce an enterprise-grade solution ready for deployment; 2) Enable use of structured data for enhanced text de-identification (when structured PII is available) and for complete patient records de-identification (i.e., records combining structured and unstructured data). This aim also includes implementing “one-way” pseudo-identifier cryptographic hashing to enable securely linking already de-identified patient records; 3) Integrate the text de-identification system with a research data capture and management system. This includes implementation of the de-identification system as a secure web service, with standards-based access and integration. This de-identification system has potential commercial applications in clinical research and in healthcare settings. It will improve access to richer, more detailed, and more accurate clinical data (in clinical text) for clinical researchers. It will ease research data sharing (as expected for larger NIH-funded research projects) and help healthcare organizations protect patient data confidentiality. Significant time-savings will also be offered, with a process at least 200-1000 times faster than manual de-identification. The adoption of Electronic Health Record systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format, with tremendous potential, but also equally growing concern for patient confidentiality breaches. De-identification of patient data has been proposed as a solution to both facilitate secondary uses of clinical data and protect patient data confidentiality. This project will advance a text de-identification system from a prototype to an accurate, adaptable and robust system allowing for complete patient records de-identification, integrated in the research infrastructure at our implementation and testing site and ready for commercialization efforts. It will improve access to richer, more detailed, and more accurate clinical data for clinical researchers, ease research data sharing and help healthcare organizations protect patient data confidentiality. !",Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing,9908962,R42GM116479,"['Adoption', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Communications Media', 'Confidentiality of Patient Information', 'Data', 'Development', 'Electronic Health Record', 'Enrollment', 'Environment', 'Fast Healthcare Interoperability Resources', 'Funding', 'Growth', 'Health Insurance Portability and Accountability Act', 'Improve Access', 'Link', 'Manuals', 'Medical', 'Modernization', 'National Institute of General Medical Sciences', 'Natural Language Processing', 'Patient Care', 'Patient Data Privacy', 'Patients', 'Performance', 'Personally Identifiable Information', 'Phase', 'Privacy', 'Process', 'Records', 'Reference Standards', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Risk', 'Savings', 'Secure', 'Site', 'South Carolina', 'Speed', 'Structure', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'Universities', 'Visualization', 'base', 'commercial application', 'commercialization', 'cost', 'cryptography', 'data reuse', 'data sharing', 'health care quality', 'health care service organization', 'health care settings', 'health management', 'improved', 'large scale data', 'prototype', 'software development', 'standard measure', 'structured data', 'systems research', 'unstructured data', 'web services']",NIGMS,"CLINACUITY,INC.",R42,2020,759330,0.05068730948420163
"Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines Project​ ​Summary While standards in reporting of scientific methods are absolutely critical to producing reproducible science, meeting such standards is tedious and difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent compliance. Scientific journals and societies as well as the National Institutes of Health are now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods,​ ​but​ ​the​ ​trickier​ ​part​ ​is​ ​to​ ​train​ ​the​ ​biomedical​ ​community​ ​to​ ​use​ ​these​ ​standards​ ​to​ ​effectively. To support new standards in methods reporting, especially the RRID standard for Rigor and Transparency of Key Biological Resources, we propose to build Sci-Score a text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard already implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife and other Rigor and transparency standards put forward by the NIH. The innovation behind Sci-score is the provision of a score, which can be obtained by individual investigators or journals. This score reflects an aspect of quality of methods reporting. We posit that the score will serve as a tool that investigators can use to compete with themselves​ ​and​ ​each​ ​other,​ ​the​ ​way​ ​they​ ​currently​ ​compete​ ​on​ ​metrics​ ​of​ ​popularity,​ ​i.e.,​ ​the​ ​H-index. In Phase I of this project and before, our group has successfully developed a text mining algorithm that can detect antibodies, cell lines, organisms and digital resources (all 4 RRID types) and has created a preliminary score. We propose to extend this approach to all research inputs, like chemicals and plasmids that are requested as part of Cell press’ STAR Methods (http://www.cell.com/star-methods)​. We also propose to build a set of algorithms to detect whether authors discuss the major sources of irreproducibility outlined by NIH, including investigator blinding, proper randomization and sufficient reporting of sex and other biological variables. Resource identification along with other quality metrics will be used to score the quality of scientific methods section text. If successful, the tool could be used by editors, reviewers, and investigators to improve the​ ​quality​ ​of​ ​the​ ​scientific​ ​paper. Our Phase II specific aims include 1) enhancing and hardening the core natural language processing pipelines to recognize a broader range of sentences in near real time; 2) building a set of modular tools that will be provided for different groups of users to take advantage of the text mining capability we develop in aim 1. At the end of Phase II, we should have a commercially viable product that will be able to be licensed to serve the needs​ ​of​ ​the​ ​publishers​ ​and​ ​the​ ​broader​ ​research​ ​community. Standards​ ​for​ ​scientific​ ​methods​ ​reporting​ ​are​ ​absolutely​ ​critical​ ​to​ ​producing​ ​reproducible​ ​science,​ ​but​ ​meeting such​ ​standards​ ​is​ ​difficult.​ ​Checklists​ ​and​ ​instructions​ ​are​ ​tough​ ​to​ ​follow​ ​often​ ​resulting​ ​in​ ​low​ ​and​ ​inconsistent compliance.​ ​To​ ​support​ ​new​ ​standards​ ​in​ ​methods​ ​reporting,​ ​especially​ ​the​ ​RRID​ ​standard​ ​for​ ​Rigor​ ​and Transparency,​ ​we​ ​propose​ ​to​ ​build​ ​​Sci-Score​​ ​a​ ​text​ ​mining​ ​based​ ​tool​ ​suite​ ​to​ ​help​ ​authors​ ​meet​ ​the​ ​standard. Sci-Score​ ​will​ ​provide​ ​an​ ​automated​ ​check​ ​on​ ​compliance​ ​with​ ​the​ ​RRID​ ​standard​ ​implemented​ ​by​ ​over​ ​100 journals​ ​including​ ​Cell,​ ​Journal​ ​of​ ​Neuroscience,​ ​and​ ​eLife.​ ​Sci-score​ ​provides​ ​an​ ​automated​ ​rating​ ​the​ ​quality of​ ​methods​ ​reporting​ ​in​ ​submitted​ ​articles,​ ​which​ ​provides​ ​feedback​ ​to​ ​authors,​ ​reviewers​ ​and​ ​editors​ ​on​ ​how to​ ​improve​ ​compliance​ ​with​ ​RRIDs​ ​and​ ​other​ ​standards.","Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines",9621771,R44MH119094,"['Address', 'Adherence', 'Algorithms', 'Antibodies', 'Award', 'Biological', 'Cell Line', 'Cells', 'Chemicals', 'Communities', 'Databases', 'Ethics', 'Feedback', 'Guidelines', 'Health', 'Human', 'Individual', 'Instruction', 'Journals', 'Methods', 'Mission', 'Natural Language Processing', 'Neurosciences', 'Oligonucleotide Probes', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Plasmids', 'Publications', 'Publishing', 'Randomized', 'Reader', 'Reagent', 'Recommendation', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Societies', 'Software Tools', 'Source', 'Specific qualifier value', 'System', 'Talents', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'base', 'complex biological systems', 'digital', 'improved', 'indexing', 'innovation', 'meetings', 'sex', 'sound', 'text searching', 'tool']",NIMH,"SCICRUNCH, INC.",R44,2018,748748,0.08704305666892159
"Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines Project​ ​Summary While standards in reporting of scientific methods are absolutely critical to producing reproducible science, meeting such standards is tedious and difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent compliance. Scientific journals and societies as well as the National Institutes of Health are now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods,​ ​but​ ​the​ ​trickier​ ​part​ ​is​ ​to​ ​train​ ​the​ ​biomedical​ ​community​ ​to​ ​use​ ​these​ ​standards​ ​to​ ​effectively. To support new standards in methods reporting, especially the RRID standard for Rigor and Transparency of Key Biological Resources, we propose to build Sci-Score a text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard already implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife and other Rigor and transparency standards put forward by the NIH. The innovation behind Sci-score is the provision of a score, which can be obtained by individual investigators or journals. This score reflects an aspect of quality of methods reporting. We posit that the score will serve as a tool that investigators can use to compete with themselves​ ​and​ ​each​ ​other,​ ​the​ ​way​ ​they​ ​currently​ ​compete​ ​on​ ​metrics​ ​of​ ​popularity,​ ​i.e.,​ ​the​ ​H-index. In Phase I of this project and before, our group has successfully developed a text mining algorithm that can detect antibodies, cell lines, organisms and digital resources (all 4 RRID types) and has created a preliminary score. We propose to extend this approach to all research inputs, like chemicals and plasmids that are requested as part of Cell press’ STAR Methods (http://www.cell.com/star-methods)​. We also propose to build a set of algorithms to detect whether authors discuss the major sources of irreproducibility outlined by NIH, including investigator blinding, proper randomization and sufficient reporting of sex and other biological variables. Resource identification along with other quality metrics will be used to score the quality of scientific methods section text. If successful, the tool could be used by editors, reviewers, and investigators to improve the​ ​quality​ ​of​ ​the​ ​scientific​ ​paper. Our Phase II specific aims include 1) enhancing and hardening the core natural language processing pipelines to recognize a broader range of sentences in near real time; 2) building a set of modular tools that will be provided for different groups of users to take advantage of the text mining capability we develop in aim 1. At the end of Phase II, we should have a commercially viable product that will be able to be licensed to serve the needs​ ​of​ ​the​ ​publishers​ ​and​ ​the​ ​broader​ ​research​ ​community. Standards​ ​for​ ​scientific​ ​methods​ ​reporting​ ​are​ ​absolutely​ ​critical​ ​to​ ​producing​ ​reproducible​ ​science,​ ​but​ ​meeting such​ ​standards​ ​is​ ​difficult.​ ​Checklists​ ​and​ ​instructions​ ​are​ ​tough​ ​to​ ​follow​ ​often​ ​resulting​ ​in​ ​low​ ​and​ ​inconsistent compliance.​ ​To​ ​support​ ​new​ ​standards​ ​in​ ​methods​ ​reporting,​ ​especially​ ​the​ ​RRID​ ​standard​ ​for​ ​Rigor​ ​and Transparency,​ ​we​ ​propose​ ​to​ ​build​ ​​Sci-Score​​ ​a​ ​text​ ​mining​ ​based​ ​tool​ ​suite​ ​to​ ​help​ ​authors​ ​meet​ ​the​ ​standard. Sci-Score​ ​will​ ​provide​ ​an​ ​automated​ ​check​ ​on​ ​compliance​ ​with​ ​the​ ​RRID​ ​standard​ ​implemented​ ​by​ ​over​ ​100 journals​ ​including​ ​Cell,​ ​Journal​ ​of​ ​Neuroscience,​ ​and​ ​eLife.​ ​Sci-score​ ​provides​ ​an​ ​automated​ ​rating​ ​the​ ​quality of​ ​methods​ ​reporting​ ​in​ ​submitted​ ​articles,​ ​which​ ​provides​ ​feedback​ ​to​ ​authors,​ ​reviewers​ ​and​ ​editors​ ​on​ ​how to​ ​improve​ ​compliance​ ​with​ ​RRIDs​ ​and​ ​other​ ​standards.","Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines",9786847,R44MH119094,"['Address', 'Adherence', 'Algorithms', 'Antibodies', 'Award', 'Biological', 'Cell Line', 'Cells', 'Chemicals', 'Communities', 'Databases', 'Ethics', 'Feedback', 'Guidelines', 'Health', 'Human', 'Individual', 'Instruction', 'Journals', 'Methods', 'Mission', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Neurosciences', 'Oligonucleotide Probes', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Plasmids', 'Publications', 'Publishing', 'Randomized', 'Reader', 'Reagent', 'Recommendation', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Societies', 'Software Tools', 'Source', 'Specific qualifier value', 'System', 'Talents', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'base', 'complex biological systems', 'digital', 'improved', 'indexing', 'innovation', 'meetings', 'sex', 'sound', 'text searching', 'tool']",NIMH,"SCICRUNCH, INC.",R44,2019,747591,0.08704305666892159
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9532909,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'Exposure to', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Post-Translational Regulation', 'Process', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Site', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'bioinformatics resource', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'individualized medicine', 'information organization', 'innovation', 'interoperability', 'kinase inhibitor', 'knowledge base', 'knowledge integration', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2018,365327,0.0979614324134333
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9326315,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'MicroRNAs', 'Mining', 'Modification', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Post-Translational Regulation', 'Process', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Site', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'information organization', 'innovation', 'interoperability', 'kinase inhibitor', 'knowledge base', 'knowledge integration', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2017,370434,0.0979614324134333
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9195864,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphorylation Site', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Process', 'Protein Databases', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Regulation', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Staging', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'abstracting', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'information organization', 'innovation', 'kinase inhibitor', 'knowledge base', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2016,374400,0.0979614324134333
"mDiet: A Text Message Intervention for Weight Loss    DESCRIPTION (provided by applicant):       We propose a randomized controlled trial to evaluate how well a weight loss intervention based primarily on text messages sent to/from mobile phones, mDIET, promotes weight loss and weight loss maintenance in overweight and obese men and women age 21 through 60. This study is based on promising formative research and a pilot randomized controlled trial (RCT) recently concluded among 75 overweight/obese men and women (mean age: 49.9) with the support of an NCI R21. To our knowledge the mDIET pilot study was the first RCT to address overweight/obesity with an intervention that is based primarily on the use of text (SMS) messages. At 16 weeks we found a significant between-group difference in weight with mDIET users losing 3.16% more weight (2.88 kg) than controls and very high acceptability. SMS messaging is becoming increasingly common and can be viewed on essentially all mobile phone platforms. Thus, the population reach of behavioral interventions using this approach can be extensive. In this project we will expand and strengthen the expert system logic underlying mDIET, strengthen content targeted at energy intake behaviors that contribute to weight loss and develop additional content related to physical activity and sedentary behavior -- all changes aimed at enabling mDIET to promote and sustain a 5-10% weight loss at 12 months. We will conduct additional formative research among those whose first language is Spanish, and develop mDIET for both English and Spanish-language speaking men and women. Then, 309 overweight/obese (BMI 25.1-39.9) women and men, age 21-60 will be randomized to one of three 12-month conditions: a) an SMS only version of mDIET; b) An SMS+Phone version of mDIET that includes regular brief counseling calls from a trained weight loss specialist; or c) a usual care Standard Print-based weight intervention. We will enroll approximately 30% in the Spanish language preference group to assure our ability to perform meaningful exploratory analyses in this population. mDIET is based in behavioral theory and can support delivery of tailored messages to each user. The primary outcome will be the effect of mDIET on percent weight loss at 12 months. Secondary outcomes will include assessments at 6 and 12 months of mDIET effects on proportion of each group that lose at least 5% of initial body weight, behavioral, and psychosocial constructs, quality of life & depression, satisfaction with the intervention and process measures related to use of intervention elements.      PUBLIC HEALTH RELEVANCE:       This project explores whether a program based mainly on the use of automated text messages sent to and from a person's mobile phone can help overweight/obese people lose weight. Messages are sent a few times each day based upon expert system rules and promote improved behaviors important to weight loss and weight loss maintenance. Some messages request a reply encouraging continued engagement and interaction with the program. Because of the widespread use of text messages, if found efficacious the public health impact of this program could be great.                 Project Narrative This project explores whether a program based mainly on the use of automated text messages sent to and from a person's mobile phone can help overweight/obese people lose weight. Messages are sent a few times each day based upon expert system rules and promote improved behaviors important to weight loss and weight loss maintenance. Some messages request a reply encouraging continued engagement and interaction with the program. Because of the widespread use of text messages, if found efficacious the public health impact of this program could be great.",mDiet: A Text Message Intervention for Weight Loss,8234202,R01CA138730,"['21 year old', 'Address', 'Age', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Body Weight', 'Body Weight decreased', 'Car Phone', 'Cardiovascular system', 'Characteristics', 'Clinical', 'Counseling', 'Coupled', 'Diet', 'Dietary Intervention', 'Dose', 'Eating Behavior', 'Educational Background', 'Electronics', 'Elements', 'Energy Intake', 'Enrollment', 'Environment', 'Equilibrium', 'Expert Systems', 'Funding', 'Genes', 'Grant', 'Health', 'Health behavior', 'Home environment', 'Intake', 'Intervention', 'Language', 'Logic', 'Maintenance', 'Measurement', 'Mediator of activation protein', 'Mental Depression', 'Monitor', 'Multimedia', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neighborhoods', 'Non-Insulin-Dependent Diabetes Mellitus', 'Obesity', 'Outcome', 'Overweight', 'Participant', 'Persons', 'Physical activity', 'Pilot Projects', 'Policies', 'Population', 'Printing', 'Process Measure', 'Professional counselor', 'Public Health', 'Quality of life', 'Randomized', 'Randomized Controlled Trials', 'Research', 'Risk', 'Services', 'Specialist', 'Technology', 'Telephone', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'Weight', 'Woman', 'Work', 'Youth', 'base', 'behavior change', 'group intervention', 'improved', 'intervention effect', 'meetings', 'men', 'preference', 'primary outcome', 'programs', 'psychosocial', 'public health relevance', 'response', 'satisfaction', 'secondary outcome', 'sedentary', 'sex', 'theories', 'treatment as usual', 'treatment effect', 'usability', 'web site', 'weight loss intervention']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,519250,0.07669560797044048
"mDiet: A Text Message Intervention for Weight Loss    DESCRIPTION (provided by applicant):       We propose a randomized controlled trial to evaluate how well a weight loss intervention based primarily on text messages sent to/from mobile phones, mDIET, promotes weight loss and weight loss maintenance in overweight and obese men and women age 21 through 60. This study is based on promising formative research and a pilot randomized controlled trial (RCT) recently concluded among 75 overweight/obese men and women (mean age: 49.9) with the support of an NCI R21. To our knowledge the mDIET pilot study was the first RCT to address overweight/obesity with an intervention that is based primarily on the use of text (SMS) messages. At 16 weeks we found a significant between-group difference in weight with mDIET users losing 3.16% more weight (2.88 kg) than controls and very high acceptability. SMS messaging is becoming increasingly common and can be viewed on essentially all mobile phone platforms. Thus, the population reach of behavioral interventions using this approach can be extensive. In this project we will expand and strengthen the expert system logic underlying mDIET, strengthen content targeted at energy intake behaviors that contribute to weight loss and develop additional content related to physical activity and sedentary behavior -- all changes aimed at enabling mDIET to promote and sustain a 5-10% weight loss at 12 months. We will conduct additional formative research among those whose first language is Spanish, and develop mDIET for both English and Spanish-language speaking men and women. Then, 309 overweight/obese (BMI 25.1-39.9) women and men, age 21-60 will be randomized to one of three 12-month conditions: a) an SMS only version of mDIET; b) An SMS+Phone version of mDIET that includes regular brief counseling calls from a trained weight loss specialist; or c) a usual care Standard Print-based weight intervention. We will enroll approximately 30% in the Spanish language preference group to assure our ability to perform meaningful exploratory analyses in this population. mDIET is based in behavioral theory and can support delivery of tailored messages to each user. The primary outcome will be the effect of mDIET on percent weight loss at 12 months. Secondary outcomes will include assessments at 6 and 12 months of mDIET effects on proportion of each group that lose at least 5% of initial body weight, behavioral, and psychosocial constructs, quality of life & depression, satisfaction with the intervention and process measures related to use of intervention elements.      PUBLIC HEALTH RELEVANCE:       This project explores whether a program based mainly on the use of automated text messages sent to and from a person's mobile phone can help overweight/obese people lose weight. Messages are sent a few times each day based upon expert system rules and promote improved behaviors important to weight loss and weight loss maintenance. Some messages request a reply encouraging continued engagement and interaction with the program. Because of the widespread use of text messages, if found efficacious the public health impact of this program could be great.                 Project Narrative This project explores whether a program based mainly on the use of automated text messages sent to and from a person's mobile phone can help overweight/obese people lose weight. Messages are sent a few times each day based upon expert system rules and promote improved behaviors important to weight loss and weight loss maintenance. Some messages request a reply encouraging continued engagement and interaction with the program. Because of the widespread use of text messages, if found efficacious the public health impact of this program could be great.",mDiet: A Text Message Intervention for Weight Loss,8527899,R01CA138730,"['21 year old', 'Address', 'Age', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Body Weight', 'Body Weight decreased', 'Car Phone', 'Cardiovascular system', 'Characteristics', 'Clinical', 'Counseling', 'Coupled', 'Diet', 'Dietary Intervention', 'Dose', 'Eating Behavior', 'Educational Background', 'Electronics', 'Elements', 'Energy Intake', 'Enrollment', 'Environment', 'Equilibrium', 'Expert Systems', 'Funding', 'Genes', 'Grant', 'Health', 'Health behavior', 'Home environment', 'Intake', 'Intervention', 'Language', 'Logic', 'Maintenance', 'Measurement', 'Mediator of activation protein', 'Mental Depression', 'Monitor', 'Multimedia', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neighborhoods', 'Non-Insulin-Dependent Diabetes Mellitus', 'Obesity', 'Outcome', 'Overweight', 'Participant', 'Persons', 'Physical activity', 'Pilot Projects', 'Policies', 'Population', 'Printing', 'Process Measure', 'Professional counselor', 'Public Health', 'Quality of life', 'Randomized', 'Randomized Controlled Trials', 'Research', 'Risk', 'Services', 'Specialist', 'Technology', 'Telephone', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'Weight', 'Woman', 'Work', 'Youth', 'base', 'behavior change', 'group intervention', 'improved', 'intervention effect', 'meetings', 'men', 'preference', 'primary outcome', 'programs', 'psychosocial', 'public health relevance', 'response', 'satisfaction', 'secondary outcome', 'sedentary', 'sex', 'theories', 'treatment as usual', 'treatment effect', 'usability', 'web site', 'weight loss intervention']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,32142,0.07669560797044048
"mDiet: A Text Message Intervention for Weight Loss    DESCRIPTION (provided by applicant):       We propose a randomized controlled trial to evaluate how well a weight loss intervention based primarily on text messages sent to/from mobile phones, mDIET, promotes weight loss and weight loss maintenance in overweight and obese men and women age 21 through 60. This study is based on promising formative research and a pilot randomized controlled trial (RCT) recently concluded among 75 overweight/obese men and women (mean age: 49.9) with the support of an NCI R21. To our knowledge the mDIET pilot study was the first RCT to address overweight/obesity with an intervention that is based primarily on the use of text (SMS) messages. At 16 weeks we found a significant between-group difference in weight with mDIET users losing 3.16% more weight (2.88 kg) than controls and very high acceptability. SMS messaging is becoming increasingly common and can be viewed on essentially all mobile phone platforms. Thus, the population reach of behavioral interventions using this approach can be extensive. In this project we will expand and strengthen the expert system logic underlying mDIET, strengthen content targeted at energy intake behaviors that contribute to weight loss and develop additional content related to physical activity and sedentary behavior -- all changes aimed at enabling mDIET to promote and sustain a 5-10% weight loss at 12 months. We will conduct additional formative research among those whose first language is Spanish, and develop mDIET for both English and Spanish-language speaking men and women. Then, 309 overweight/obese (BMI 25.1-39.9) women and men, age 21-60 will be randomized to one of three 12-month conditions: a) an SMS only version of mDIET; b) An SMS+Phone version of mDIET that includes regular brief counseling calls from a trained weight loss specialist; or c) a usual care Standard Print-based weight intervention. We will enroll approximately 30% in the Spanish language preference group to assure our ability to perform meaningful exploratory analyses in this population. mDIET is based in behavioral theory and can support delivery of tailored messages to each user. The primary outcome will be the effect of mDIET on percent weight loss at 12 months. Secondary outcomes will include assessments at 6 and 12 months of mDIET effects on proportion of each group that lose at least 5% of initial body weight, behavioral, and psychosocial constructs, quality of life & depression, satisfaction with the intervention and process measures related to use of intervention elements.      PUBLIC HEALTH RELEVANCE:       This project explores whether a program based mainly on the use of automated text messages sent to and from a person's mobile phone can help overweight/obese people lose weight. Messages are sent a few times each day based upon expert system rules and promote improved behaviors important to weight loss and weight loss maintenance. Some messages request a reply encouraging continued engagement and interaction with the program. Because of the widespread use of text messages, if found efficacious the public health impact of this program could be great.                 Project Narrative This project explores whether a program based mainly on the use of automated text messages sent to and from a person's mobile phone can help overweight/obese people lose weight. Messages are sent a few times each day based upon expert system rules and promote improved behaviors important to weight loss and weight loss maintenance. Some messages request a reply encouraging continued engagement and interaction with the program. Because of the widespread use of text messages, if found efficacious the public health impact of this program could be great.",mDiet: A Text Message Intervention for Weight Loss,8067039,R01CA138730,"['21 year old', 'Address', 'Age', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Body Weight', 'Body Weight decreased', 'Car Phone', 'Cardiovascular system', 'Characteristics', 'Clinical', 'Counseling', 'Coupled', 'Diet', 'Dietary Intervention', 'Dose', 'Eating Behavior', 'Educational Background', 'Electronics', 'Elements', 'Energy Intake', 'Enrollment', 'Environment', 'Equilibrium', 'Expert Systems', 'Funding', 'Genes', 'Grant', 'Health', 'Health behavior', 'Home environment', 'Intake', 'Intervention', 'Language', 'Logic', 'Maintenance', 'Measurement', 'Mediator of activation protein', 'Mental Depression', 'Monitor', 'Multimedia', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neighborhoods', 'Non-Insulin-Dependent Diabetes Mellitus', 'Obesity', 'Outcome', 'Overweight', 'Participant', 'Persons', 'Physical activity', 'Pilot Projects', 'Policies', 'Population', 'Printing', 'Process Measure', 'Professional counselor', 'Public Health', 'Quality of life', 'Randomized', 'Randomized Controlled Trials', 'Research', 'Risk', 'Services', 'Specialist', 'Technology', 'Telephone', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'Weight', 'Woman', 'Work', 'Youth', 'base', 'behavior change', 'group intervention', 'improved', 'intervention effect', 'meetings', 'men', 'preference', 'primary outcome', 'programs', 'psychosocial', 'public health relevance', 'response', 'satisfaction', 'secondary outcome', 'sedentary', 'sex', 'theories', 'treatment as usual', 'treatment effect', 'usability', 'web site', 'weight loss intervention']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,561629,0.07669560797044048
"mDiet: A Text Message Intervention for Weight Loss    DESCRIPTION (provided by applicant):       We propose a randomized controlled trial to evaluate how well a weight loss intervention based primarily on text messages sent to/from mobile phones, mDIET, promotes weight loss and weight loss maintenance in overweight and obese men and women age 21 through 60. This study is based on promising formative research and a pilot randomized controlled trial (RCT) recently concluded among 75 overweight/obese men and women (mean age: 49.9) with the support of an NCI R21. To our knowledge the mDIET pilot study was the first RCT to address overweight/obesity with an intervention that is based primarily on the use of text (SMS) messages. At 16 weeks we found a significant between-group difference in weight with mDIET users losing 3.16% more weight (2.88 kg) than controls and very high acceptability. SMS messaging is becoming increasingly common and can be viewed on essentially all mobile phone platforms. Thus, the population reach of behavioral interventions using this approach can be extensive. In this project we will expand and strengthen the expert system logic underlying mDIET, strengthen content targeted at energy intake behaviors that contribute to weight loss and develop additional content related to physical activity and sedentary behavior -- all changes aimed at enabling mDIET to promote and sustain a 5-10% weight loss at 12 months. We will conduct additional formative research among those whose first language is Spanish, and develop mDIET for both English and Spanish-language speaking men and women. Then, 309 overweight/obese (BMI 25.1-39.9) women and men, age 21-60 will be randomized to one of three 12-month conditions: a) an SMS only version of mDIET; b) An SMS+Phone version of mDIET that includes regular brief counseling calls from a trained weight loss specialist; or c) a usual care Standard Print-based weight intervention. We will enroll approximately 30% in the Spanish language preference group to assure our ability to perform meaningful exploratory analyses in this population. mDIET is based in behavioral theory and can support delivery of tailored messages to each user. The primary outcome will be the effect of mDIET on percent weight loss at 12 months. Secondary outcomes will include assessments at 6 and 12 months of mDIET effects on proportion of each group that lose at least 5% of initial body weight, behavioral, and psychosocial constructs, quality of life & depression, satisfaction with the intervention and process measures related to use of intervention elements.      PUBLIC HEALTH RELEVANCE:       This project explores whether a program based mainly on the use of automated text messages sent to and from a person's mobile phone can help overweight/obese people lose weight. Messages are sent a few times each day based upon expert system rules and promote improved behaviors important to weight loss and weight loss maintenance. Some messages request a reply encouraging continued engagement and interaction with the program. Because of the widespread use of text messages, if found efficacious the public health impact of this program could be great.                 Project Narrative This project explores whether a program based mainly on the use of automated text messages sent to and from a person's mobile phone can help overweight/obese people lose weight. Messages are sent a few times each day based upon expert system rules and promote improved behaviors important to weight loss and weight loss maintenance. Some messages request a reply encouraging continued engagement and interaction with the program. Because of the widespread use of text messages, if found efficacious the public health impact of this program could be great.",mDiet: A Text Message Intervention for Weight Loss,7783102,R01CA138730,"['21 year old', 'Address', 'Age', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Body Weight', 'Body Weight decreased', 'Car Phone', 'Cardiovascular Diseases', 'Characteristics', 'Clinical', 'Counseling', 'Coupled', 'Diet', 'Dietary Intervention', 'Dose', 'Eating Behavior', 'Educational Background', 'Electronics', 'Elements', 'Energy Intake', 'Enrollment', 'Environment', 'Equilibrium', 'Expert Systems', 'Funding', 'Genes', 'Grant', 'Health', 'Health behavior', 'Home environment', 'Intake', 'Intervention', 'Knowledge', 'Language', 'Logic', 'Maintenance', 'Measurement', 'Mediator of activation protein', 'Mental Depression', 'Monitor', 'Multimedia', 'Neighborhoods', 'Non-Insulin-Dependent Diabetes Mellitus', 'Obesity', 'Outcome', 'Overweight', 'Participant', 'Persons', 'Physical activity', 'Pilot Projects', 'Policies', 'Population', 'Printing', 'Process Measure', 'Professional counselor', 'Public Health', 'Quality of life', 'Randomized', 'Randomized Controlled Trials', 'Research', 'Risk', 'Services', 'Specialist', 'Technology', 'Telephone', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'Weight', 'Woman', 'Work', 'Youth', 'base', 'behavior change', 'group intervention', 'improved', 'intervention effect', 'meetings', 'men', 'preference', 'primary outcome', 'programs', 'psychosocial', 'public health relevance', 'response', 'satisfaction', 'secondary outcome', 'sedentary', 'sex', 'theories', 'treatment as usual', 'treatment effect', 'usability', 'web site', 'weight loss intervention']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2010,653480,0.07669560797044048
"mDiet: A Text Message Intervention for Weight Loss    DESCRIPTION (provided by applicant):       We propose a randomized controlled trial to evaluate how well a weight loss intervention based primarily on text messages sent to/from mobile phones, mDIET, promotes weight loss and weight loss maintenance in overweight and obese men and women age 21 through 60. This study is based on promising formative research and a pilot randomized controlled trial (RCT) recently concluded among 75 overweight/obese men and women (mean age: 49.9) with the support of an NCI R21. To our knowledge the mDIET pilot study was the first RCT to address overweight/obesity with an intervention that is based primarily on the use of text (SMS) messages. At 16 weeks we found a significant between-group difference in weight with mDIET users losing 3.16% more weight (2.88 kg) than controls and very high acceptability. SMS messaging is becoming increasingly common and can be viewed on essentially all mobile phone platforms. Thus, the population reach of behavioral interventions using this approach can be extensive. In this project we will expand and strengthen the expert system logic underlying mDIET, strengthen content targeted at energy intake behaviors that contribute to weight loss and develop additional content related to physical activity and sedentary behavior -- all changes aimed at enabling mDIET to promote and sustain a 5-10% weight loss at 12 months. We will conduct additional formative research among those whose first language is Spanish, and develop mDIET for both English and Spanish-language speaking men and women. Then, 309 overweight/obese (BMI 25.1-39.9) women and men, age 21-60 will be randomized to one of three 12-month conditions: a) an SMS only version of mDIET; b) An SMS+Phone version of mDIET that includes regular brief counseling calls from a trained weight loss specialist; or c) a usual care Standard Print-based weight intervention. We will enroll approximately 30% in the Spanish language preference group to assure our ability to perform meaningful exploratory analyses in this population. mDIET is based in behavioral theory and can support delivery of tailored messages to each user. The primary outcome will be the effect of mDIET on percent weight loss at 12 months. Secondary outcomes will include assessments at 6 and 12 months of mDIET effects on proportion of each group that lose at least 5% of initial body weight, behavioral, and psychosocial constructs, quality of life & depression, satisfaction with the intervention and process measures related to use of intervention elements.       PUBLIC HEALTH RELEVANCE:       This project explores whether a program based mainly on the use of automated text messages sent to and from a person's mobile phone can help overweight/obese people lose weight. Messages are sent a few times each day based upon expert system rules and promote improved behaviors important to weight loss and weight loss maintenance. Some messages request a reply encouraging continued engagement and interaction with the program. Because of the widespread use of text messages, if found efficacious the public health impact of this program could be great.               ",mDiet: A Text Message Intervention for Weight Loss,8444330,R01CA138730,"['21 year old', 'Address', 'Age', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Body Weight', 'Body Weight decreased', 'Car Phone', 'Cardiovascular system', 'Characteristics', 'Clinical', 'Counseling', 'Coupled', 'Diet', 'Dietary Intervention', 'Dose', 'Eating Behavior', 'Educational Background', 'Electronics', 'Elements', 'Energy Intake', 'Enrollment', 'Environment', 'Equilibrium', 'Expert Systems', 'Funding', 'Genes', 'Grant', 'Health', 'Health behavior', 'Home environment', 'Intake', 'Intervention', 'Language', 'Logic', 'Maintenance', 'Measurement', 'Mediator of activation protein', 'Mental Depression', 'Monitor', 'Multimedia', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neighborhoods', 'Non-Insulin-Dependent Diabetes Mellitus', 'Obesity', 'Outcome', 'Overweight', 'Participant', 'Persons', 'Physical activity', 'Pilot Projects', 'Policies', 'Population', 'Printing', 'Process Measure', 'Professional counselor', 'Public Health', 'Quality of life', 'Randomized', 'Randomized Controlled Trials', 'Research', 'Risk', 'Services', 'Specialist', 'Technology', 'Telephone', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'Weight', 'Woman', 'Work', 'Youth', 'base', 'behavior change', 'group intervention', 'improved', 'intervention effect', 'meetings', 'men', 'preference', 'primary outcome', 'programs', 'psychosocial', 'public health relevance', 'response', 'satisfaction', 'secondary outcome', 'sedentary', 'sex', 'theories', 'treatment as usual', 'treatment effect', 'usability', 'web site', 'weight loss intervention']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,373571,0.06362230816890586
"mDiet: A Text Message Intervention for Weight Loss    DESCRIPTION (provided by applicant):       We propose a randomized controlled trial to evaluate how well a weight loss intervention based primarily on text messages sent to/from mobile phones, mDIET, promotes weight loss and weight loss maintenance in overweight and obese men and women age 21 through 60. This study is based on promising formative research and a pilot randomized controlled trial (RCT) recently concluded among 75 overweight/obese men and women (mean age: 49.9) with the support of an NCI R21. To our knowledge the mDIET pilot study was the first RCT to address overweight/obesity with an intervention that is based primarily on the use of text (SMS) messages. At 16 weeks we found a significant between-group difference in weight with mDIET users losing 3.16% more weight (2.88 kg) than controls and very high acceptability. SMS messaging is becoming increasingly common and can be viewed on essentially all mobile phone platforms. Thus, the population reach of behavioral interventions using this approach can be extensive. In this project we will expand and strengthen the expert system logic underlying mDIET, strengthen content targeted at energy intake behaviors that contribute to weight loss and develop additional content related to physical activity and sedentary behavior -- all changes aimed at enabling mDIET to promote and sustain a 5-10% weight loss at 12 months. We will conduct additional formative research among those whose first language is Spanish, and develop mDIET for both English and Spanish-language speaking men and women. Then, 309 overweight/obese (BMI 25.1-39.9) women and men, age 21-60 will be randomized to one of three 12-month conditions: a) an SMS only version of mDIET; b) An SMS+Phone version of mDIET that includes regular brief counseling calls from a trained weight loss specialist; or c) a usual care Standard Print-based weight intervention. We will enroll approximately 30% in the Spanish language preference group to assure our ability to perform meaningful exploratory analyses in this population. mDIET is based in behavioral theory and can support delivery of tailored messages to each user. The primary outcome will be the effect of mDIET on percent weight loss at 12 months. Secondary outcomes will include assessments at 6 and 12 months of mDIET effects on proportion of each group that lose at least 5% of initial body weight, behavioral, and psychosocial constructs, quality of life & depression, satisfaction with the intervention and process measures related to use of intervention elements.       PUBLIC HEALTH RELEVANCE:       This project explores whether a program based mainly on the use of automated text messages sent to and from a person's mobile phone can help overweight/obese people lose weight. Messages are sent a few times each day based upon expert system rules and promote improved behaviors important to weight loss and weight loss maintenance. Some messages request a reply encouraging continued engagement and interaction with the program. Because of the widespread use of text messages, if found efficacious the public health impact of this program could be great.               ",mDiet: A Text Message Intervention for Weight Loss,8625847,R01CA138730,"['21 year old', 'Address', 'Age', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Body Weight', 'Body Weight decreased', 'Car Phone', 'Cardiovascular system', 'Characteristics', 'Clinical', 'Counseling', 'Coupled', 'Diet', 'Dietary Intervention', 'Dose', 'Eating Behavior', 'Educational Background', 'Electronics', 'Elements', 'Energy Intake', 'Enrollment', 'Environment', 'Equilibrium', 'Expert Systems', 'Funding', 'Genes', 'Grant', 'Health', 'Health behavior', 'Home environment', 'Intake', 'Intervention', 'Language', 'Logic', 'Maintenance', 'Measurement', 'Mediator of activation protein', 'Mental Depression', 'Monitor', 'Multimedia', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neighborhoods', 'Non-Insulin-Dependent Diabetes Mellitus', 'Obesity', 'Outcome', 'Overweight', 'Participant', 'Persons', 'Physical activity', 'Pilot Projects', 'Policies', 'Population', 'Printing', 'Process Measure', 'Professional counselor', 'Public Health', 'Quality of life', 'Randomized', 'Randomized Controlled Trials', 'Research', 'Risk', 'Services', 'Specialist', 'Technology', 'Telephone', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'Weight', 'Woman', 'Work', 'Youth', 'base', 'behavior change', 'group intervention', 'improved', 'intervention effect', 'meetings', 'men', 'preference', 'primary outcome', 'programs', 'psychosocial', 'public health relevance', 'response', 'satisfaction', 'secondary outcome', 'sedentary', 'sex', 'theories', 'treatment as usual', 'treatment effect', 'usability', 'web site', 'weight loss intervention']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,58956,0.06362230816890586
"USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING    DESCRIPTION (provided by applicant): Understanding mechanisms of action is key to improving psychosocial interventions for cancer and other chronic disease conditions. In cancer, emotional expression has been identified as one possible mediator of the effect of psychosocial intervention on patient-reported outcomes. However, scientific evaluations of psychological mechanisms of adjustment to cancer and other chronic diseases are constrained by limitations associated with self-report measures. Because self-care resources, peer-to-peer networks, and more recent forms of psychosocial intervention are increasingly being delivered online, linguistic and behavioral data can be used to characterize internal coping processes, social interactions, and other manifest behaviors. Few tools are currently available for harnessing text as a potential data source, and signal detection indices of existing tools leave room for considerable improvement in these methodologies (Bantum & Owen, 2009). In the present study, natural language processing and other tools of computational linguistics will be used to develop a machine-learning classifier to identify emotional expression in electronic text data. The aims of the study are: 1) to annotate a large text corpus from cancer survivors using an objective and reliable emotion-coding procedure, 2) to incorporate linguistic and psychological features into a machine-learning classification method and identify which of these features are most strongly associated with codes assigned by trained human raters, and 3) to develop combined psychological and natural language processing (NLP) methods for identifying linguistic markers of emotional coping behaviors. To accomplish these aims, a comprehensive corpus of emotionally-laden cancer communications will be developed from 5 existing linguistic datasets. Five raters will be selected and undergo a rigorous training procedure for coding emotional expression using an emotion-coding system previously developed by the research. Coding will take place using an Internet-based coding interface that will allow the investigators to continuously monitor inter-rater reliability. Simultaneous with the coding process, the investigators will link the electronic text data with key linguistic and psychological features, including Linguistic Inquiry and Word Count (LIWC), Affective Norms for English Words (ANEW), WordNet, part of speech tags, patterns of capitalization and punctuation, emoticons, and textual context. A machine-learning classifier, using tools of natural language processing, will then be applied to the text/feature data and validated against human-rated emotion codes. The long-term objective of this research is to advance a methodology for objectively identifying coping behavior, particularly emotional expression, in order to supplement self-report measures and improve scientific understanding of adjustment to chronic disease, trauma, or other psychological conditions. This work is essential for identifying mechanisms of action in psychosocial interventions for cancer survivors and others and has significance for the fields of medicine, psychology, computational linguistics, and artificial intelligence.      PUBLIC HEALTH RELEVANCE: Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.           Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.",USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING,8120220,R21CA143642,"['Affective', 'Artificial Intelligence', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Cancer Intervention', 'Cancer Survivor', 'Categories', 'Characteristics', 'Chronic Disease', 'Classification', 'Code', 'Cognitive', 'Communication', 'Coping Behavior', 'Coping Skills', 'Data', 'Data Set', 'Data Sources', 'Decision Making', 'Detection', 'Distress', 'Educational process of instructing', 'Effectiveness of Interventions', 'Electronics', 'Emotional', 'Emotions', 'Goals', 'Health', 'Health behavior', 'Heart Rate', 'Human', 'Hydrocortisone', 'Individual', 'Internet', 'Intervention', 'Intervention Studies', 'Left', 'Life', 'Linguistics', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediator of activation protein', 'Medicine', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Pattern', 'Personal Satisfaction', 'Physiological', 'Predictive Value', 'Problem Solving', 'Procedures', 'Process', 'Psychological adjustment', 'Psychology', 'Publishing', 'Quality of life', 'Recommendation', 'Recording of previous events', 'Recovery', 'Regulation', 'Relative (related person)', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Sampling', 'Scientific Evaluation', 'Screening procedure', 'Self Care', 'Signal Transduction', 'Social Interaction', 'Social support', 'Specificity', 'Speech', 'Survey Methodology', 'Sympathetic Nervous System', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trauma', 'Treatment/Psychosocial Effects', 'Work', 'anticancer research', 'base', 'behavior observation', 'computerized', 'computerized tools', 'coping', 'effective intervention', 'emotional experience', 'experience', 'improved', 'indexing', 'innovation', 'lexical', 'natural language', 'peer', 'programs', 'psychologic', 'psychosocial', 'public health relevance', 'showing emotion', 'skills', 'skills training', 'symptom management', 'tool']",NCI,LOMA LINDA UNIVERSITY,R21,2011,161797,-0.005128594163967033
"USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING    DESCRIPTION (provided by applicant): Understanding mechanisms of action is key to improving psychosocial interventions for cancer and other chronic disease conditions. In cancer, emotional expression has been identified as one possible mediator of the effect of psychosocial intervention on patient-reported outcomes. However, scientific evaluations of psychological mechanisms of adjustment to cancer and other chronic diseases are constrained by limitations associated with self-report measures. Because self-care resources, peer-to-peer networks, and more recent forms of psychosocial intervention are increasingly being delivered online, linguistic and behavioral data can be used to characterize internal coping processes, social interactions, and other manifest behaviors. Few tools are currently available for harnessing text as a potential data source, and signal detection indices of existing tools leave room for considerable improvement in these methodologies (Bantum & Owen, 2009). In the present study, natural language processing and other tools of computational linguistics will be used to develop a machine-learning classifier to identify emotional expression in electronic text data. The aims of the study are: 1) to annotate a large text corpus from cancer survivors using an objective and reliable emotion-coding procedure, 2) to incorporate linguistic and psychological features into a machine-learning classification method and identify which of these features are most strongly associated with codes assigned by trained human raters, and 3) to develop combined psychological and natural language processing (NLP) methods for identifying linguistic markers of emotional coping behaviors. To accomplish these aims, a comprehensive corpus of emotionally-laden cancer communications will be developed from 5 existing linguistic datasets. Five raters will be selected and undergo a rigorous training procedure for coding emotional expression using an emotion-coding system previously developed by the research. Coding will take place using an Internet-based coding interface that will allow the investigators to continuously monitor inter-rater reliability. Simultaneous with the coding process, the investigators will link the electronic text data with key linguistic and psychological features, including Linguistic Inquiry and Word Count (LIWC), Affective Norms for English Words (ANEW), WordNet, part of speech tags, patterns of capitalization and punctuation, emoticons, and textual context. A machine-learning classifier, using tools of natural language processing, will then be applied to the text/feature data and validated against human-rated emotion codes. The long-term objective of this research is to advance a methodology for objectively identifying coping behavior, particularly emotional expression, in order to supplement self-report measures and improve scientific understanding of adjustment to chronic disease, trauma, or other psychological conditions. This work is essential for identifying mechanisms of action in psychosocial interventions for cancer survivors and others and has significance for the fields of medicine, psychology, computational linguistics, and artificial intelligence.      PUBLIC HEALTH RELEVANCE: Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.           Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.",USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING,7991498,R21CA143642,"['Affective', 'Artificial Intelligence', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Cancer Intervention', 'Cancer Survivor', 'Categories', 'Characteristics', 'Chronic Disease', 'Classification', 'Code', 'Cognitive', 'Communication', 'Coping Behavior', 'Coping Skills', 'Data', 'Data Set', 'Data Sources', 'Decision Making', 'Detection', 'Distress', 'Educational process of instructing', 'Effectiveness of Interventions', 'Electronics', 'Emotional', 'Emotions', 'Goals', 'Health', 'Health behavior', 'Heart Rate', 'Human', 'Hydrocortisone', 'Individual', 'Internet', 'Intervention', 'Intervention Studies', 'Left', 'Life', 'Linguistics', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediator of activation protein', 'Medicine', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Pattern', 'Personal Satisfaction', 'Physiological', 'Predictive Value', 'Problem Solving', 'Procedures', 'Process', 'Psychological adjustment', 'Psychology', 'Publishing', 'Quality of life', 'Recommendation', 'Recording of previous events', 'Recovery', 'Regulation', 'Relative (related person)', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Sampling', 'Scientific Evaluation', 'Screening procedure', 'Self Care', 'Signal Transduction', 'Social Interaction', 'Social support', 'Specificity', 'Speech', 'Survey Methodology', 'Sympathetic Nervous System', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trauma', 'Treatment/Psychosocial Effects', 'Work', 'anticancer research', 'base', 'behavior observation', 'computerized', 'computerized tools', 'coping', 'effective intervention', 'emotional experience', 'experience', 'improved', 'indexing', 'innovation', 'lexical', 'natural language', 'peer', 'programs', 'psychologic', 'psychosocial', 'public health relevance', 'showing emotion', 'skills', 'skills training', 'symptom management', 'tool']",NCI,LOMA LINDA UNIVERSITY,R21,2010,223207,-0.005128594163967033
"Personalized Patient data and behavioral nudges to improve adherence to chronic cardiovascular medications Abstract Our objective in this project is to employ population level pharmacy data and delivery of nudges via cell phone text messaging and artificially intelligent (AI) interactive chat bot to improve medication adherence and patient outcomes in 3 integrated healthcare delivery systems (HCS): University of Colorado Health System (UCHealth), VA Eastern Colorado Health Care System (VA), and Denver Health Medical Center (DH). We will identify patients with chronic cardiovascular (CV) conditions taking medications to treat hypertension, atrial fibrillation, coronary artery disease, diabetes and/or hyperlipidemia. We will leverage pharmacy refill data to identify episodes of non-adherence through gaps in medication refills and randomize individuals to 1 of 4 study arms when they have a first refill gap: 1) usual care; 2) generic text message reminder; 3) tailored and engaging text messages optimized to facilitate behavior change; or 4) optimized text messages plus a pre- programmed AI interactive chat bot designed to support identification and resolution of barriers to medication refill and adherence. In the UG3 phase (year 1), we will develop and program a theoretically informed technology-based (a) text message library and (b) chat bot content library using multiple and iterative N of 1 within subject studies to optimize content for a range of diverse patients. These outcomes will inform a pilot intervention to demonstrate feasibility of delivering the intervention and preliminary effects in all 3 HCS. We will also engage patient, provider and health systems stakeholders in designing, refining, and implementing the pilot intervention. In the UH3 phase (years 2-5), we will conduct a pragmatic patient-level randomized intervention across 3 HCS to improve adherence to chronic CV medications. We will evaluate the intervention using a mixed methods approach and apply the RE-AIM (reach, effectiveness, adoption, implementation, and maintenance) framework. In addition, we will assess the context and implementation processes to inform local tailoring, adaptations and modifications, and eventual expansion of the intervention. Project Narrative Given the increasing prevalence of chronic diseases that require ongoing treatment with medications, the problem of medication non-adherence is unlikely to go away and will grow. This study will employ population level pharmacy data to identify non-adherent patients and utilize ubiquitous cell phone technology to send them tailored, engaging and motivating text messages and text message based chat through an artificially intelligent (AI) interactive chat bot to improve cardiac medication adherence and patient outcomes in 3 integrated healthcare delivery systems. This study will advance medication adherence research and the intervention could be applied to multiple clinical conditions requiring medications for long term treatment and other NIH institutes.!",Personalized Patient data and behavioral nudges to improve adherence to chronic cardiovascular medications,9986013,UH3HL144163,"['Acute myocardial infarction', 'Adherence', 'Adoption', 'Artificial Intelligence', 'Atrial Fibrillation', 'Attention', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Sciences', 'Body Weight decreased', 'Cardiac', 'Cardiovascular system', 'Cellular Phone', 'Chronic', 'Chronic Disease', 'Clinical', 'Colorado', 'Coronary Arteriosclerosis', 'Data', 'Decision Making', 'Diabetes Mellitus', 'Disease', 'Effectiveness', 'Event', 'Goals', 'Health', 'Health Care Costs', 'Health Promotion', 'Health Technology', 'Health behavior', 'Health system', 'Healthcare Systems', 'Hospitalization', 'Hospitals', 'Human', 'Hyperlipidemia', 'Hypertension', 'Individual', 'Institutes', 'Integrated Delivery of Health Care', 'Intervention', 'Intervention Studies', 'Libraries', 'Maintenance', 'Measures', 'Medical center', 'Methods', 'Modification', 'Morbidity - disease rate', 'Outcome', 'Patient Education', 'Patient Noncompliance', 'Patient-Focused Outcomes', 'Patients', 'Pharmaceutical Preparations', 'Pharmacists', 'Pharmacy facility', 'Phase', 'Pilot Projects', 'Population', 'Prevalence', 'Process', 'Provider', 'Randomized', 'Recommendation', 'Research', 'Resolution', 'Resources', 'Self Management', 'Study Subject', 'System', 'Technology', 'Testing', 'Text Messaging', 'United States National Institutes of Health', 'Universities', 'base', 'behavior change', 'behavioral economics', 'blood pressure regulation', 'chatbot', 'copayment', 'cost', 'design', 'digital', 'evidence base', 'financial incentive', 'four-arm study', 'health care delivery', 'health care service utilization', 'improved', 'individual response', 'medication compliance', 'medication nonadherence', 'mortality', 'primary outcome', 'programs', 'randomized trial', 'response', 'safety net', 'secondary outcome', 'smoking cessation', 'theories', 'treatment as usual']",NHLBI,UNIVERSITY OF COLORADO DENVER,UH3,2020,1436253,0.03783706046454725
"Personalized Patient data and behavioral nudges to improve adherence to chronic cardiovascular medications Abstract Our objective in this project is to employ population level pharmacy data and delivery of nudges via cell phone text messaging and artificially intelligent (AI) interactive chat bot to improve medication adherence and patient outcomes in 3 integrated healthcare delivery systems (HCS): University of Colorado Health System (UCHealth), VA Eastern Colorado Health Care System (VA), and Denver Health Medical Center (DH). We will identify patients with chronic cardiovascular (CV) conditions taking medications to treat hypertension, atrial fibrillation, coronary artery disease, diabetes and/or hyperlipidemia. We will leverage pharmacy refill data to identify episodes of non-adherence through gaps in medication refills and randomize individuals to 1 of 4 study arms when they have a first refill gap: 1) usual care; 2) generic text message reminder; 3) tailored and engaging text messages optimized to facilitate behavior change; or 4) optimized text messages plus a pre- programmed AI interactive chat bot designed to support identification and resolution of barriers to medication refill and adherence. In the UG3 phase (year 1), we will develop and program a theoretically informed technology-based (a) text message library and (b) chat bot content library using multiple and iterative N of 1 within subject studies to optimize content for a range of diverse patients. These outcomes will inform a pilot intervention to demonstrate feasibility of delivering the intervention and preliminary effects in all 3 HCS. We will also engage patient, provider and health systems stakeholders in designing, refining, and implementing the pilot intervention. In the UH3 phase (years 2-5), we will conduct a pragmatic patient-level randomized intervention across 3 HCS to improve adherence to chronic CV medications. We will evaluate the intervention using a mixed methods approach and apply the RE-AIM (reach, effectiveness, adoption, implementation, and maintenance) framework. In addition, we will assess the context and implementation processes to inform local tailoring, adaptations and modifications, and eventual expansion of the intervention. Project Narrative Given the increasing prevalence of chronic diseases that require ongoing treatment with medications, the problem of medication non-adherence is unlikely to go away and will grow. This study will employ population level pharmacy data to identify non-adherent patients and utilize ubiquitous cell phone technology to send them tailored, engaging and motivating text messages and text message based chat through an artificially intelligent (AI) interactive chat bot to improve cardiac medication adherence and patient outcomes in 3 integrated healthcare delivery systems. This study will advance medication adherence research and the intervention could be applied to multiple clinical conditions requiring medications for long term treatment and other NIH institutes.!",Personalized Patient data and behavioral nudges to improve adherence to chronic cardiovascular medications,9750927,UH3HL144163,"['Acute myocardial infarction', 'Adherence', 'Adoption', 'Artificial Intelligence', 'Atrial Fibrillation', 'Attention', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Sciences', 'Body Weight decreased', 'Cardiac', 'Cardiovascular system', 'Cellular Phone', 'Chronic', 'Chronic Disease', 'Clinical', 'Colorado', 'Coronary Arteriosclerosis', 'Data', 'Decision Making', 'Diabetes Mellitus', 'Disease', 'Effectiveness', 'Event', 'Goals', 'Health', 'Health Care Costs', 'Health Promotion', 'Health Technology', 'Health behavior', 'Health system', 'Healthcare Systems', 'Hospitalization', 'Hospitals', 'Human', 'Hyperlipidemia', 'Hypertension', 'Individual', 'Institutes', 'Integrated Delivery of Health Care', 'Intervention', 'Intervention Studies', 'Libraries', 'Maintenance', 'Measures', 'Medical center', 'Methods', 'Modification', 'Morbidity - disease rate', 'Outcome', 'Patient Education', 'Patient Noncompliance', 'Patient-Focused Outcomes', 'Patients', 'Pharmaceutical Preparations', 'Pharmacists', 'Pharmacy facility', 'Phase', 'Pilot Projects', 'Population', 'Prevalence', 'Process', 'Provider', 'Randomized', 'Recommendation', 'Research', 'Resolution', 'Resources', 'Self Management', 'Study Subject', 'System', 'Technology', 'Testing', 'Text Messaging', 'United States National Institutes of Health', 'Universities', 'arm', 'base', 'behavior change', 'behavioral economics', 'blood pressure regulation', 'chatbot', 'copayment', 'cost', 'design', 'digital', 'evidence base', 'financial incentive', 'health care delivery', 'health care service utilization', 'improved', 'individual response', 'medication compliance', 'medication nonadherence', 'mortality', 'primary outcome', 'programs', 'randomized trial', 'response', 'safety net', 'secondary outcome', 'smoking cessation', 'theories', 'treatment as usual']",NHLBI,UNIVERSITY OF COLORADO DENVER,UH3,2019,1455863,0.03783706046454725
"Knowledge Management Center for Illuminating the Druggable Genome SUMMARY The understudied protein targets that are the focus of the implementation phase of the Illuminating the Druggable Genome (IDG) project need to be placed in the contexts of gene-sets/pathways, drugs/small-molecules, diseases/phenotypes, and cells/tissues. By extending our previous methods, we will impute knowledge about the understudied potential target protein kinases, GPCRs, and ion channels listed in the RFA using machine learning strategies. To establish this classification system, we will organize data from many omics- and literature- based resources into attribute tables where genes are the rows and their attributes are the columns. Examples of such attribute tables include gene or protein expression in cancer cell lines (CCLE) or human tissues (GTEx), changes in expression in response to drug perturbations or single-gene knockdowns (LINCS), regulation by transcription factors based on ChIP-seq data (ENCODE), and phenotypes in mice observed when single genes are knocked out (KOMP). In total, we will process and abstract data from over 100 resources. We will then predict target functions, target association with pathways, small-molecules/drugs that modulate the activity and expression of the target, and target relevance to human disease. To further validate such predictions, we will employ text mining to identify knowledge that corroborates with the data mining predictions, perform molecular docking of predicted small molecules using homology modeling, and seek associations between variants and human diseases by mining electronic medical records (EMR) together with genomic profiling of thousands of patients. In addition, we will develop innovative data visualization tools to allow users to interact with all the collected data, and develop social networking software to build communities centered around proteins/genes/targets as well as biological topics including pathways, cell types, drugs/small-molecules, and diseases. Overall, we will develop an invaluable resource that will accelerate target and drug discovery. NARRATIVE The Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) project will facilitate translational research by integrating and mining data about understudied druggable targets from numerous repositories and other resources. The KMC for IDG team will develop novel tools to analyze these data for the purpose of finding connections between genes/proteins/targets and diseases/phenotypes, cells/tissues, pathways/gene-sets, and drugs/small-molecules in order to identify potential applications to treat diseases and for other biological contexts of clinical relevance.",Knowledge Management Center for Illuminating the Druggable Genome,9453421,U24CA224260,"['Achievement', 'Adverse effects', 'Award', 'Biological', 'Cancer cell line', 'Cells', 'ChIP-seq', 'Classification', 'Communication', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Disease', 'Docking', 'Gene Expression', 'Gene Proteins', 'Gene Targeting', 'Generations', 'Genes', 'Genome', 'Genotype-Tissue Expression Project', 'Goals', 'Grant', 'Graph', 'Homology Modeling', 'Human', 'Information Resources Management', 'Internet', 'Ion Channel', 'Knock-out', 'Knockout Mice', 'Knowledge', 'Learning', 'Link', 'Literature', 'Machine Learning', 'Methods', 'Mining', 'Molecular', 'Mus', 'Online Systems', 'Paper', 'Pathway interactions', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Phosphotransferases', 'Process', 'Protein Kinase', 'Proteins', 'Public Domains', 'Publications', 'Publishing', 'Reproducibility', 'Research Personnel', 'Resources', 'Social Network', 'Supervision', 'System', 'Time trend', 'Tissues', 'Transcriptional Regulation', 'Translational Research', 'Update', 'Variant', 'Visit', 'Visualization software', 'Work', 'base', 'biobank', 'cell type', 'clinically relevant', 'community center', 'computerized data processing', 'data integration', 'data mining', 'data resource', 'data visualization', 'disease phenotype', 'drug discovery', 'druggable target', 'genomic profiles', 'human disease', 'human tissue', 'innovation', 'knock-down', 'learning strategy', 'novel', 'online community', 'outreach', 'profiles in patients', 'programs', 'protein expression', 'repository', 'response', 'small molecule', 'success', 'text searching', 'tool', 'transcription factor', 'unsupervised learning', 'web site']",NCI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U24,2018,249999,0.04193653671464891
"Knowledge Management Center for Illuminating the Druggable Genome SUMMARY The understudied protein targets that are the focus of the implementation phase of the Illuminating the Druggable Genome (IDG) project need to be placed in the contexts of gene-sets/pathways, drugs/small-molecules, diseases/phenotypes, and cells/tissues. By extending our previous methods, we will impute knowledge about the understudied potential target protein kinases, GPCRs, and ion channels listed in the RFA using machine learning strategies. To establish this classification system, we will organize data from many omics- and literature- based resources into attribute tables where genes are the rows and their attributes are the columns. Examples of such attribute tables include gene or protein expression in cancer cell lines (CCLE) or human tissues (GTEx), changes in expression in response to drug perturbations or single-gene knockdowns (LINCS), regulation by transcription factors based on ChIP-seq data (ENCODE), and phenotypes in mice observed when single genes are knocked out (KOMP). In total, we will process and abstract data from over 100 resources. We will then predict target functions, target association with pathways, small-molecules/drugs that modulate the activity and expression of the target, and target relevance to human disease. To further validate such predictions, we will employ text mining to identify knowledge that corroborates with the data mining predictions, perform molecular docking of predicted small molecules using homology modeling, and seek associations between variants and human diseases by mining electronic medical records (EMR) together with genomic profiling of thousands of patients. In addition, we will develop innovative data visualization tools to allow users to interact with all the collected data, and develop social networking software to build communities centered around proteins/genes/targets as well as biological topics including pathways, cell types, drugs/small-molecules, and diseases. Overall, we will develop an invaluable resource that will accelerate target and drug discovery. NARRATIVE The Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) project will facilitate translational research by integrating and mining data about understudied druggable targets from numerous repositories and other resources. The KMC for IDG team will develop novel tools to analyze these data for the purpose of finding connections between genes/proteins/targets and diseases/phenotypes, cells/tissues, pathways/gene-sets, and drugs/small-molecules in order to identify potential applications to treat diseases and for other biological contexts of clinical relevance.",Knowledge Management Center for Illuminating the Druggable Genome,9843449,U24CA224260,"['Achievement', 'Award', 'Biological', 'Cancer cell line', 'Cells', 'ChIP-seq', 'Classification', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Disease', 'Docking', 'G-Protein-Coupled Receptors', 'Gene Expression', 'Gene Proteins', 'Gene Targeting', 'Generations', 'Genes', 'Genome', 'Genotype-Tissue Expression Project', 'Goals', 'Grant', 'Graph', 'Homology Modeling', 'Human', 'Information Resources Management', 'Internet', 'Ion Channel', 'Knock-out', 'Knockout Mice', 'Knowledge', 'Learning', 'Link', 'Literature', 'Machine Learning', 'Methods', 'Mining', 'Molecular', 'Mus', 'Online Systems', 'Paper', 'Pathway interactions', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Phosphotransferases', 'Process', 'Protein Kinase', 'Proteins', 'Public Domains', 'Publications', 'Publishing', 'Reproducibility', 'Research Personnel', 'Resources', 'Social Network', 'Supervision', 'System', 'Time trend', 'Tissues', 'Transcriptional Regulation', 'Translational Research', 'Update', 'Variant', 'Visit', 'Visualization software', 'Work', 'base', 'biobank', 'cell type', 'clinically relevant', 'community center', 'computerized data processing', 'data integration', 'data mining', 'data resource', 'data visualization', 'disease phenotype', 'drug discovery', 'druggable target', 'genomic profiles', 'human disease', 'human tissue', 'in silico', 'innovation', 'knock-down', 'learning strategy', 'machine learning method', 'novel', 'online community', 'outreach', 'profiles in patients', 'programs', 'protein expression', 'repository', 'response', 'side effect', 'small molecule', 'success', 'text searching', 'tool', 'transcription factor', 'unsupervised learning', 'web site']",NCI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U24,2020,249999,0.04193653671464891
"Knowledge Management Center for Illuminating the Druggable Genome SUMMARY The understudied protein targets that are the focus of the implementation phase of the Illuminating the Druggable Genome (IDG) project need to be placed in the contexts of gene-sets/pathways, drugs/small-molecules, diseases/phenotypes, and cells/tissues. By extending our previous methods, we will impute knowledge about the understudied potential target protein kinases, GPCRs, and ion channels listed in the RFA using machine learning strategies. To establish this classification system, we will organize data from many omics- and literature- based resources into attribute tables where genes are the rows and their attributes are the columns. Examples of such attribute tables include gene or protein expression in cancer cell lines (CCLE) or human tissues (GTEx), changes in expression in response to drug perturbations or single-gene knockdowns (LINCS), regulation by transcription factors based on ChIP-seq data (ENCODE), and phenotypes in mice observed when single genes are knocked out (KOMP). In total, we will process and abstract data from over 100 resources. We will then predict target functions, target association with pathways, small-molecules/drugs that modulate the activity and expression of the target, and target relevance to human disease. To further validate such predictions, we will employ text mining to identify knowledge that corroborates with the data mining predictions, perform molecular docking of predicted small molecules using homology modeling, and seek associations between variants and human diseases by mining electronic medical records (EMR) together with genomic profiling of thousands of patients. In addition, we will develop innovative data visualization tools to allow users to interact with all the collected data, and develop social networking software to build communities centered around proteins/genes/targets as well as biological topics including pathways, cell types, drugs/small-molecules, and diseases. Overall, we will develop an invaluable resource that will accelerate target and drug discovery. NARRATIVE The Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) project will facilitate translational research by integrating and mining data about understudied druggable targets from numerous repositories and other resources. The KMC for IDG team will develop novel tools to analyze these data for the purpose of finding connections between genes/proteins/targets and diseases/phenotypes, cells/tissues, pathways/gene-sets, and drugs/small-molecules in order to identify potential applications to treat diseases and for other biological contexts of clinical relevance.",Knowledge Management Center for Illuminating the Druggable Genome,9623330,U24CA224260,"['Achievement', 'Award', 'Biological', 'Cancer cell line', 'Cells', 'ChIP-seq', 'Classification', 'Communication', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Disease', 'Docking', 'G-Protein-Coupled Receptors', 'Gene Expression', 'Gene Proteins', 'Gene Targeting', 'Generations', 'Genes', 'Genome', 'Genotype-Tissue Expression Project', 'Goals', 'Grant', 'Graph', 'Homology Modeling', 'Human', 'Information Resources Management', 'Internet', 'Ion Channel', 'Knock-out', 'Knockout Mice', 'Knowledge', 'Learning', 'Link', 'Literature', 'Machine Learning', 'Methods', 'Mining', 'Molecular', 'Mus', 'Online Systems', 'Paper', 'Pathway interactions', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Phosphotransferases', 'Process', 'Protein Kinase', 'Proteins', 'Public Domains', 'Publications', 'Publishing', 'Reproducibility', 'Research Personnel', 'Resources', 'Social Network', 'Supervision', 'System', 'Time trend', 'Tissues', 'Transcriptional Regulation', 'Translational Research', 'Update', 'Variant', 'Visit', 'Visualization software', 'Work', 'base', 'biobank', 'cell type', 'clinically relevant', 'community center', 'computerized data processing', 'data integration', 'data mining', 'data resource', 'data visualization', 'disease phenotype', 'drug discovery', 'druggable target', 'genomic profiles', 'human disease', 'human tissue', 'innovation', 'knock-down', 'learning strategy', 'novel', 'online community', 'outreach', 'profiles in patients', 'programs', 'protein expression', 'repository', 'response', 'side effect', 'small molecule', 'success', 'text searching', 'tool', 'transcription factor', 'unsupervised learning', 'web site']",NCI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U24,2019,249999,0.04193653671464891
"Statistical Support Services for the NIEHS, Subproject: Bioinformatics support for optional contract periods This contract provides bioinformatics support to the Division of Intramural Research (DIR) and Division of the National Toxicology Program (DNTP) research efforts regarding genomics, metabolomics and proteomics.  Support includes assistance with study design of genomics studies, analyses of genomics and cheminformatics data, and text mining for DNTP literature searches. n/a","Statistical Support Services for the NIEHS, Subproject: Bioinformatics support for optional contract periods",9430360,73201500077U,"['Bioinformatics', 'Contracts', 'Data', 'Experimental Designs', 'Genomics', 'Intramural Research', 'Literature', 'Methods', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Proteomics', 'Research', 'Research Design', 'Research Personnel', 'Sample Size', 'Services', 'cheminformatics', 'data mining', 'experimental study', 'metabolomics', 'text searching']",NIEHS,"SRA INTERNATIONAL, INC.",N01,2017,171243,0.061141611203132475
"Statistical Support Services for the NIEHS, Subproject: Bioinformatics support for optional contract periods This contract provides bioinformatics support to the Division of Intramural Research (DIR) and Division of the National Toxicology Program (DNTP) research efforts regarding genomics, metabolomics and proteomics.  Support includes assistance with study design of genomics studies, analyses of genomics and cheminformatics data, and text mining for DNTP literature searches. n/a","Statistical Support Services for the NIEHS, Subproject: Bioinformatics support for optional contract periods",9356634,73201500077U,"['Bioinformatics', 'Contracts', 'Data', 'Experimental Designs', 'Genomics', 'Intramural Research', 'Methods', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Proteomics', 'Research', 'Research Design', 'Research Personnel', 'Sample Size', 'Services', 'cheminformatics', 'metabolomics', 'research study', 'text searching']",NIEHS,"SRA INTERNATIONAL, INC.",N01,2016,1012054,0.061141611203132475
"AUTOMATED ANALYSIS OF BIOMEDICAL TEXT The purpose of this contract is to develop methods to analyze and represent information in biomedical texts.  This will require sophisticated natural language processing capabilities, involving lexical, syntactic, semantic and pragmatic analysis of these texts. The Natural Language Systems group of the Lister Hill Center for Biomedical Communications is pursuing this work as an intramural research project and seeks to collaborate with outside organizations presently conducting closely related research.  The overall objective of this contract is to establish methods for testing the hypothesis that access to a bibliographic database, such as the National Library of Medicine's MEDLINE database, can be improved by automated analysis of the free test in the system.  The project work will involve modification and extension of aspects of a natural language parser.  The MEDLINE database has citation records for several million articles in biomedicine, representing several thousand journals. Each citation record includes the title, an author prepared abstract when available, author and journal names, and a set of Medical Subject Headings under which the article has been indexed by expert indexers.  The free text in the system is found in the title and abstract fields of the citation records.  Titles are normally complex noun phrases, while abstracts are composed of well-formed, although highly specialized, English sentences.  The purpose of the work under this contract is to develop methods for parsing this text.  The parsing procedure should result in a set of well-specified logical forms, representing the meaning of the phrases and sentences in the citation record.  n/a",AUTOMATED ANALYSIS OF BIOMEDICAL TEXT,2320317,01LM083521,"['abstracting', ' information retrieval', ' information systems', ' language', ' literature survey', ' semantics']",NLM,UNISYS,N01,1988,155723,0.11892908726550348

text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"SmartTool for Anomaly Detection in Radiotherapy Treatment Plan Data    DESCRIPTION (provided by applicant):  Adverse events and medical errors result in thousands of accidental deaths and over one million excess injuries each year. To avoid medical errors in radiation cancer treatment, careful attention needs to be made to ensure accurate implementation of the intended treatment plan. We propose a SmartTool to automatically detect and highlight potential errors in a radiotherapy treatment plan, in real time and before its execution. SmartTool will double check all the treatment parameters in the background against a previously built Predictive Model of a Medical Error (PMME) and flag the operator, [post human QA,] if there is a discrepancy in the treatment plan, by stopping execution, highlighting the outlier treatment parameter and prompting human intervention. To build the PMME we will mine the dataset of previously treated cancer patients, by clustering the data in the groups based on treatment parameter similarity, labeling the clusters and using an innovative algorithm to build a highly accurate anomaly detection tool. PMME will also be dynamically updated [to include new treatment data instances coming in to the system, and updating the model should any treatment flags be identified as false positive or false negative]. The vastly innovative aspect of SmartTool is in the novel use of machine learning techniques to automatically build an anomaly prediction model on unlabeled data (customarily a labeled data is required to build a predictive model) and provide an automatic, real time and unobtrusive intelligent computational treatment checking algorithm. Moreover, having an analytical model of an outlier/anomaly offers the capability to describe the conditions of the outlier being created and is the essential in gaining investigative (and medical) insight in what went wrong and how to improve the process in the future. SmartTool can also be applied in a variety of other medical areas (e.g. predicting errors in pharmacy, laboratory data, and treatment procedure data), to detect anomalies and describe them, offering potential novel medical discoveries and a prospect of saving thousands more lives, with a vast commercialization aspect.      PUBLIC HEALTH RELEVANCE:  The proposal is aimed at promoting research and development in biomedical computational science and technology that is consistent with the objective of the NIH and NCI to support rapid progress in areas of scientific opportunity in biomedical research, and enhancing the public health. If the project is successfully completed, this proof of concept study will result in a valuable health information technology tool for automatic detection of catastrophic errors in cancer radiotherapy, which adds another safeguard for patient safety.            The proposal is aimed at promoting research and development in biomedical computational science and technology that is consistent with the objective of the NIH and NCI to support rapid progress in areas of scientific opportunity in biomedical research, and enhancing the public health. If the project is successfully completed, this proof of concept study will result in a valuable health information technology tool for automatic detection of catastrophic errors in cancer radiotherapy, which adds another safeguard for patient safety.         ",SmartTool for Anomaly Detection in Radiotherapy Treatment Plan Data,8250930,R43TR000629,"['Adverse event', 'Algorithms', 'Area', 'Attention', 'Biomedical Research', 'California', 'Cancer Center', 'Cancer Patient', 'Cessation of life', 'Classification', 'Computational Science', 'Computer software', 'Data', 'Data Set', 'Detection', 'Ensure', 'Evaluation', 'Event', 'Funding', 'Future', 'Healthcare', 'Human', 'Information Systems', 'Injury', 'Intervention', 'Label', 'Laboratories', 'Learning', 'Machine Learning', 'Medical', 'Medical Errors', 'Methods', 'Mining', 'Modeling', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Phase', 'Procedures', 'Process', 'Public Health', 'Radiation', 'Radiation therapy', 'Research', 'Severities', 'System', 'Techniques', 'Technology', 'Time', 'United States National Institutes of Health', 'Universities', 'Update', 'Work', 'base', 'cancer radiation therapy', 'cancer therapy', 'commercialization', 'design', 'health information technology', 'improved', 'innovation', 'insight', 'novel', 'patient safety', 'predictive modeling', 'prevent', 'research and development', 'tool', 'treatment planning']",NCATS,"SCIBERQUEST, INC.",R43,2012,148660,0.15568446897905847
"SYMPTOM EVOLUTION IN APHASIA: A NEUROPSYCHOLOGICAL STUDY Two related projects are proposed as part of an application for a Research Career Development Award.  First, it is suggested that information about the evolution of patients' symptoms during recovery from aphasia will provide an important source of constraint on the postulation of explanatory models to account for particular aphasis syndromes.  Three separate experimental batteries are developed to investigate the nature of the underlying deficit in agrammatic Broca's aphasia, conduction aphasia and anomic aphasia.  Specific hypotheses are developed concerning the possible relationships among co-occuring symptoms and their underlying basis. Patients meeting the selection criteria will be tested at several intervals over the course of the first year post-onset using one of the experimental batteries.  A second project is proposed to develop a computer-based system for the simulation and classification of reading and writing errors produced by dyslexic and dysgraphic patients.  This system will focus on the classification of errors involving the translation between graphemes and phonemes, and is intended to investigate the incidence of errors that are related to grapheme-to-phoneme or phoneme-to-grapheme conversion.  n/a",SYMPTOM EVOLUTION IN APHASIA: A NEUROPSYCHOLOGICAL STUDY,3074727,K04NS000851,"['aphasia', ' artificial intelligence', ' chronic brain damage', ' communication disorders', ' computer simulation', ' human subject', ' neuropsychological tests', ' neuropsychology', ' pathologic process', ' reading disorder', ' sensory disorder diagnosis']",NINDS,UNIVERSITY OF MARYLAND BALTIMORE,K04,1988,50593,0.07938667284075408
"SYMPTOM EVOLUTION IN APHASIA: A NEUROPSYCHOLOGICAL STUDY Two related projects are proposed as part of an application for a Research Career Development Award.  First, it is suggested that information about the evolution of patients' symptoms during recovery from aphasia will provide an important source of constraint on the postulation of explanatory models to account for particular aphasis syndromes.  Three separate experimental batteries are developed to investigate the nature of the underlying deficit in agrammatic Broca's aphasia, conduction aphasia and anomic aphasia.  Specific hypotheses are developed concerning the possible relationships among co-occuring symptoms and their underlying basis. Patients meeting the selection criteria will be tested at several intervals over the course of the first year post-onset using one of the experimental batteries.  A second project is proposed to develop a computer-based system for the simulation and classification of reading and writing errors produced by dyslexic and dysgraphic patients.  This system will focus on the classification of errors involving the translation between graphemes and phonemes, and is intended to investigate the incidence of errors that are related to grapheme-to-phoneme or phoneme-to-grapheme conversion.  n/a",SYMPTOM EVOLUTION IN APHASIA: A NEUROPSYCHOLOGICAL STUDY,3074726,K04NS000851,"['aphasia', ' artificial intelligence', ' chronic brain damage', ' communication disorders', ' computer simulation', ' human subject', ' neuropsychological tests', ' neuropsychology', ' pathologic process', ' reading disorder', ' sensory disorder diagnosis']",NINDS,UNIVERSITY OF MARYLAND BALTIMORE,K04,1987,49559,0.07938667284075408
"SYMPTOM EVOLUTION IN APHASIA: A NEUROPSYCHOLOGICAL STUDY Two related projects are proposed as part of an application for a Research Career Development Award.  First, it is suggested that information about the evolution of patients' symptoms during recovery from aphasia will provide an important source of constraint on the postulation of explanatory models to account for particular aphasis syndromes.  Three separate experimental batteries are developed to investigate the nature of the underlying deficit in agrammatic Broca's aphasia, conduction aphasia and anomic aphasia.  Specific hypotheses are developed concerning the possible relationships among co-occuring symptoms and their underlying basis. Patients meeting the selection criteria will be tested at several intervals over the course of the first year post-onset using one of the experimental batteries.  A second project is proposed to develop a computer-based system for the simulation and classification of reading and writing errors produced by dyslexic and dysgraphic patients.  This system will focus on the classification of errors involving the translation between graphemes and phonemes, and is intended to investigate the incidence of errors that are related to grapheme-to-phoneme or phoneme-to-grapheme conversion.  n/a",SYMPTOM EVOLUTION IN APHASIA: A NEUROPSYCHOLOGICAL STUDY,3074725,K04NS000851,"['aphasia', ' artificial intelligence', ' chronic brain damage', ' communication disorders', ' computer simulation', ' human subject', ' neuropsychological tests', ' neuropsychology', ' pathologic process', ' reading disorder', ' sensory disorder diagnosis']",NINDS,UNIVERSITY OF MARYLAND BALTIMORE,K04,1986,50298,0.07938667284075408
"SYMPTOM EVOLUTION IN APHASIA: A NEUROPSYCHOLOGICAL STUDY Two related projects are proposed as part of an application for a Research Career Development Award.  First, it is suggested that information about the evolution of patients' symptoms during recovery from aphasia will provide an important source of constraint on the postulation of explanatory models to account for particular aphasis syndromes.  Three separate experimental batteries are developed to investigate the nature of the underlying deficit in agrammatic Broca's aphasia, conduction aphasia and anomic aphasia.  Specific hypotheses are developed concerning the possible relationships among co-occuring symptoms and their underlying basis. Patients meeting the selection criteria will be tested at several intervals over the course of the first year post-onset using one of the experimental batteries.  A second project is proposed to develop a computer-based system for the simulation and classification of reading and writing errors produced by dyslexic and dysgraphic patients.  This system will focus on the classification of errors involving the translation between graphemes and phonemes, and is intended to investigate the incidence of errors that are related to grapheme-to-phoneme or phoneme-to-grapheme conversion.  n/a",SYMPTOM EVOLUTION IN APHASIA: A NEUROPSYCHOLOGICAL STUDY,3074724,K04NS000851,"['aphasia', ' artificial intelligence', ' chronic brain damage', ' communication disorders', ' computer simulation', ' human subject', ' neuropsychological tests', ' neuropsychology', ' pathologic process', ' reading disorder', ' sensory disorder diagnosis']",NINDS,UNIVERSITY OF MARYLAND BALTIMORE,K04,1985,53136,0.07938667284075408
"Applying Usability to A Knowledge Based System A cancer genetics-tracking database will be redesigned using usability engineering techniques to improve the functionality and usability of the current system. This is important because it will lead to a system that is easier to use and learn, will decrease the chance of errors, and will increase productivity, and user satisfaction. The current state of informatics offers the potential for the creation of tools to assist in the reduction of medical errors. The redesign of this tracking database will be completed through a three-phase process. The first phase will use the results of a usability analysis to redesign and prototype the cancer genetics-tracking database. In the second phase, usability studies will then be conducted to ensure that the system is functional, easy to use, easy to learn, and meets the goals of the users. The usability studies will include heuristic evaluations, keystroke level models, talk-aloud methods, and cognitive walkthrough techniques. The system will be modified based upon the results of these studies. Research will be compiled on the advantages and disadvantages of ICD coding Vs. SNOMED followed by the selection of the most useful system for coding medical information. In the third phase the final redesign will be compared to the old system using a within-subject design to determine if the redesign decreases the error rate, increases productivity, and user satisfaction. This will be followed-up with a survey to determine the perceived usability of the redesigned application. Throughout the redesign process, specific usability guidelines will be developed for designing healthcare software that is computational and knowledge- based in nature.  n/a",Applying Usability to A Knowledge Based System,6538226,F38LM007188,"['artificial intelligence', ' cancer information system', ' cancer registry /resource', ' computer assisted medical decision making', ' computer human interaction', ' computer system design /evaluation', ' family genetics', ' human data', ' neoplasm /cancer genetics']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,F38,2002,66954,0.15539458640130646
"Applying Usability to A Knowledge Based System A cancer genetics-tracking database will be redesigned using usability engineering techniques to improve the functionality and usability of the current system. This is important because it will lead to a system that is easier to use and learn, will decrease the chance of errors, and will increase productivity, and user satisfaction. The current state of informatics offers the potential for the creation of tools to assist in the reduction of medical errors. The redesign of this tracking database will be completed through a three-phase process. The first phase will use the results of a usability analysis to redesign and prototype the cancer genetics-tracking database. In the second phase, usability studies will then be conducted to ensure that the system is functional, easy to use, easy to learn, and meets the goals of the users. The usability studies will include heuristic evaluations, keystroke level models, talk-aloud methods, and cognitive walkthrough techniques. The system will be modified based upon the results of these studies. Research will be compiled on the advantages and disadvantages of ICD coding Vs. SNOMED followed by the selection of the most useful system for coding medical information. In the third phase the final redesign will be compared to the old system using a within-subject design to determine if the redesign decreases the error rate, increases productivity, and user satisfaction. This will be followed-up with a survey to determine the perceived usability of the redesigned application. Throughout the redesign process, specific usability guidelines will be developed for designing healthcare software that is computational and knowledge- based in nature.  n/a",Applying Usability to A Knowledge Based System,6340157,F38LM007188,"['artificial intelligence', ' cancer information system', ' cancer registry /resource', ' computer assisted medical decision making', ' computer human interaction', ' computer system design /evaluation', ' family genetics', ' human data', ' neoplasm /cancer genetics']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,F38,2001,68753,0.15539458640130646
"Partitioning to Support Auditing and Extending the UMLS    DESCRIPTION (provided by applicant): The UMLS is an invaluable resource to the biomedical community. However, the Metathesaurus's (META's) size and complexity can hinder its usefulness. We propose a number of divide and conquer techniques that facilitate the tasks of auditing and extending the META. The divide and conquer approach breaks down a large collection of items into more manageable units. In this research, the outcome will be new partitions of the META. Each partition will be employed in auditing the META. The UMLS's Semantic Network (SN) does not partition the META into disjoint sets. We first create a Refined Semantic Network (RSN) that does partition the META into disjoint sets of concepts (extents). Derived from the SN, the RSN incorporates intersection types along with pure semantic types and improves the UMLS in a number of ways, e.g., it contains types with semantically uniform and (usually) smaller extents. In addition to the RSN, three additional, novel partitioning techniques will be applied to extents: Partition along the dimension of relationship similarity; create singly rooted subgroups; and subdivide tangled, multi-rooted hierarchies. Each partition will (normally) expose new errors. We propose a large scale audit of the 2006AA UMLS concentrating on small groups obtained by the partition. The kinds of errors expected to be uncovered include: concept ambiguity and synonymy, wrong and missing IS-A and semantic relationships, inconsistent and incorrect classifications and omissions. We will perform an enhanced audit of the genomic component of the UMLS. For this we will create a Refined Genomic SN (RGSN). We will build an auditing software tool that incorporates the RSN and the three additional levels of partitioning. A formal evaluation, with human subjects, of the tool and a two-level expert evaluation of the auditing results are planned. Finally we focus on the integration of new terminologies into the UMLS, based on the RSN. In summary we will: 1) Create the RSN that partitions META into disjoint units; 2) Design three new partitioning techniques for extents of semantic types; 3) Audit the concepts of small groups; 4) Design a Refined Genomic SN (RGSN) for auditing genomic concepts; 5) Build an auditing tool; 6) Design and evaluate an integration technique for terminologies.           n/a",Partitioning to Support Auditing and Extending the UMLS,7865538,R01LM008445,"['Back', 'Classification', 'Collection', 'Communities', 'Development', 'Dimensions', 'Evaluation', 'Genomics', 'Goals', 'Individual', 'Maintenance', 'Neurofibrillary Tangles', 'Outcomes Research', 'Parents', 'Plant Roots', 'Process', 'Research', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Staging', 'Subgroup', 'Techniques', 'Terminology', 'Unified Medical Language System', 'Work', 'base', 'design', 'human subject', 'improved', 'metathesaurus', 'novel', 'prevent', 'tool']",NLM,NEW JERSEY INSTITUTE OF TECHNOLOGY,R01,2009,238300,0.08170157309185712
"Partitioning to Support Auditing and Extending the UMLS    DESCRIPTION (provided by applicant): The UMLS is an invaluable resource to the biomedical community. However, the Metathesaurus's (META's) size and complexity can hinder its usefulness. We propose a number of divide and conquer techniques that facilitate the tasks of auditing and extending the META. The divide and conquer approach breaks down a large collection of items into more manageable units. In this research, the outcome will be new partitions of the META. Each partition will be employed in auditing the META. The UMLS's Semantic Network (SN) does not partition the META into disjoint sets. We first create a Refined Semantic Network (RSN) that does partition the META into disjoint sets of concepts (extents). Derived from the SN, the RSN incorporates intersection types along with pure semantic types and improves the UMLS in a number of ways, e.g., it contains types with semantically uniform and (usually) smaller extents. In addition to the RSN, three additional, novel partitioning techniques will be applied to extents: Partition along the dimension of relationship similarity; create singly rooted subgroups; and subdivide tangled, multi-rooted hierarchies. Each partition will (normally) expose new errors. We propose a large scale audit of the 2006AA UMLS concentrating on small groups obtained by the partition. The kinds of errors expected to be uncovered include: concept ambiguity and synonymy, wrong and missing IS-A and semantic relationships, inconsistent and incorrect classifications and omissions. We will perform an enhanced audit of the genomic component of the UMLS. For this we will create a Refined Genomic SN (RGSN). We will build an auditing software tool that incorporates the RSN and the three additional levels of partitioning. A formal evaluation, with human subjects, of the tool and a two-level expert evaluation of the auditing results are planned. Finally we focus on the integration of new terminologies into the UMLS, based on the RSN. In summary we will: 1) Create the RSN that partitions META into disjoint units; 2) Design three new partitioning techniques for extents of semantic types; 3) Audit the concepts of small groups; 4) Design a Refined Genomic SN (RGSN) for auditing genomic concepts; 5) Build an auditing tool; 6) Design and evaluate an integration technique for terminologies.           n/a",Partitioning to Support Auditing and Extending the UMLS,7413280,R01LM008445,"['Back', 'Classification', 'Collection', 'Communities', 'Depth', 'Development', 'Dimensions', 'Evaluation', 'Genomics', 'Goals', 'Individual', 'Maintenance', 'Neurofibrillary Tangles', 'Numbers', 'Outcomes Research', 'Parents', 'Plant Roots', 'Process', 'Research', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Staging', 'Subgroup', 'Techniques', 'Terminology', 'Unified Medical Language System', 'Work', 'base', 'concept', 'design', 'human subject', 'improved', 'metathesaurus', 'novel', 'prevent', 'size', 'tool']",NLM,NEW JERSEY INSTITUTE OF TECHNOLOGY,R01,2008,476602,0.08170157309185712
"Partitioning to Support Auditing and Extending the UMLS    DESCRIPTION (provided by applicant): The UMLS is an invaluable resource to the biomedical community. However, the Metathesaurus's (META's) size and complexity can hinder its usefulness. We propose a number of divide and conquer techniques that facilitate the tasks of auditing and extending the META. The divide and conquer approach breaks down a large collection of items into more manageable units. In this research, the outcome will be new partitions of the META. Each partition will be employed in auditing the META. The UMLS's Semantic Network (SN) does not partition the META into disjoint sets. We first create a Refined Semantic Network (RSN) that does partition the META into disjoint sets of concepts (extents). Derived from the SN, the RSN incorporates intersection types along with pure semantic types and improves the UMLS in a number of ways, e.g., it contains types with semantically uniform and (usually) smaller extents. In addition to the RSN, three additional, novel partitioning techniques will be applied to extents: Partition along the dimension of relationship similarity; create singly rooted subgroups; and subdivide tangled, multi-rooted hierarchies. Each partition will (normally) expose new errors. We propose a large scale audit of the 2006AA UMLS concentrating on small groups obtained by the partition. The kinds of errors expected to be uncovered include: concept ambiguity and synonymy, wrong and missing IS-A and semantic relationships, inconsistent and incorrect classifications and omissions. We will perform an enhanced audit of the genomic component of the UMLS. For this we will create a Refined Genomic SN (RGSN). We will build an auditing software tool that incorporates the RSN and the three additional levels of partitioning. A formal evaluation, with human subjects, of the tool and a two-level expert evaluation of the auditing results are planned. Finally we focus on the integration of new terminologies into the UMLS, based on the RSN. In summary we will: 1) Create the RSN that partitions META into disjoint units; 2) Design three new partitioning techniques for extents of semantic types; 3) Audit the concepts of small groups; 4) Design a Refined Genomic SN (RGSN) for auditing genomic concepts; 5) Build an auditing tool; 6) Design and evaluate an integration technique for terminologies.           n/a",Partitioning to Support Auditing and Extending the UMLS,7916891,R01LM008445,"['Back', 'Classification', 'Collection', 'Communities', 'Development', 'Dimensions', 'Evaluation', 'Genomics', 'Goals', 'Individual', 'Maintenance', 'Neurofibrillary Tangles', 'Outcomes Research', 'Parents', 'Plant Roots', 'Process', 'Research', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Staging', 'Subgroup', 'Techniques', 'Terminology', 'Unified Medical Language System', 'Work', 'base', 'design', 'human subject', 'improved', 'metathesaurus', 'novel', 'prevent', 'tool']",NLM,NEW JERSEY INSTITUTE OF TECHNOLOGY,R01,2009,92418,0.08170157309185712
"Partitioning to Support Auditing and Extending the UMLS    DESCRIPTION (provided by applicant): The UMLS is an invaluable resource to the biomedical community. However, the Metathesaurus's (META's) size and complexity can hinder its usefulness. We propose a number of divide and conquer techniques that facilitate the tasks of auditing and extending the META. The divide and conquer approach breaks down a large collection of items into more manageable units. In this research, the outcome will be new partitions of the META. Each partition will be employed in auditing the META. The UMLS's Semantic Network (SN) does not partition the META into disjoint sets. We first create a Refined Semantic Network (RSN) that does partition the META into disjoint sets of concepts (extents). Derived from the SN, the RSN incorporates intersection types along with pure semantic types and improves the UMLS in a number of ways, e.g., it contains types with semantically uniform and (usually) smaller extents. In addition to the RSN, three additional, novel partitioning techniques will be applied to extents: Partition along the dimension of relationship similarity; create singly rooted subgroups; and subdivide tangled, multi-rooted hierarchies. Each partition will (normally) expose new errors. We propose a large scale audit of the 2006AA UMLS concentrating on small groups obtained by the partition. The kinds of errors expected to be uncovered include: concept ambiguity and synonymy, wrong and missing IS-A and semantic relationships, inconsistent and incorrect classifications and omissions. We will perform an enhanced audit of the genomic component of the UMLS. For this we will create a Refined Genomic SN (RGSN). We will build an auditing software tool that incorporates the RSN and the three additional levels of partitioning. A formal evaluation, with human subjects, of the tool and a two-level expert evaluation of the auditing results are planned. Finally we focus on the integration of new terminologies into the UMLS, based on the RSN. In summary we will: 1) Create the RSN that partitions META into disjoint units; 2) Design three new partitioning techniques for extents of semantic types; 3) Audit the concepts of small groups; 4) Design a Refined Genomic SN (RGSN) for auditing genomic concepts; 5) Build an auditing tool; 6) Design and evaluate an integration technique for terminologies.           n/a",Partitioning to Support Auditing and Extending the UMLS,7236213,R01LM008445,"['Back', 'Classification', 'Collection', 'Communities', 'Depth', 'Development', 'Dimensions', 'Evaluation', 'Genomics', 'Goals', 'Individual', 'Maintenance', 'Neurofibrillary Tangles', 'Numbers', 'Outcomes Research', 'Parents', 'Plant Roots', 'Process', 'Research', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Staging', 'Subgroup', 'Techniques', 'Terminology', 'Unified Medical Language System', 'Work', 'base', 'concept', 'design', 'human subject', 'improved', 'metathesaurus', 'novel', 'prevent', 'size', 'tool']",NLM,NEW JERSEY INSTITUTE OF TECHNOLOGY,R01,2007,456425,0.08170157309185712
"Partitioning to Support Auditing and Extending the UMLS    DESCRIPTION (provided by applicant): The UMLS is an invaluable resource to the biomedical community. However, the Metathesaurus's (META's) size and complexity can hinder its usefulness. We propose a number of divide and conquer techniques that facilitate the tasks of auditing and extending the META. The divide and conquer approach breaks down a large collection of items into more manageable units. In this research, the outcome will be new partitions of the META. Each partition will be employed in auditing the META. The UMLS's Semantic Network (SN) does not partition the META into disjoint sets. We first create a Refined Semantic Network (RSN) that does partition the META into disjoint sets of concepts (extents). Derived from the SN, the RSN incorporates intersection types along with pure semantic types and improves the UMLS in a number of ways, e.g., it contains types with semantically uniform and (usually) smaller extents. In addition to the RSN, three additional, novel partitioning techniques will be applied to extents: Partition along the dimension of relationship similarity; create singly rooted subgroups; and subdivide tangled, multi-rooted hierarchies. Each partition will (normally) expose new errors. We propose a large scale audit of the 2006AA UMLS concentrating on small groups obtained by the partition. The kinds of errors expected to be uncovered include: concept ambiguity and synonymy, wrong and missing IS-A and semantic relationships, inconsistent and incorrect classifications and omissions. We will perform an enhanced audit of the genomic component of the UMLS. For this we will create a Refined Genomic SN (RGSN). We will build an auditing software tool that incorporates the RSN and the three additional levels of partitioning. A formal evaluation, with human subjects, of the tool and a two-level expert evaluation of the auditing results are planned. Finally we focus on the integration of new terminologies into the UMLS, based on the RSN. In summary we will: 1) Create the RSN that partitions META into disjoint units; 2) Design three new partitioning techniques for extents of semantic types; 3) Audit the concepts of small groups; 4) Design a Refined Genomic SN (RGSN) for auditing genomic concepts; 5) Build an auditing tool; 6) Design and evaluate an integration technique for terminologies.           n/a",Partitioning to Support Auditing and Extending the UMLS,7098530,R01LM008445,"['classification', 'clinical research', 'root', 'semantics']",NLM,NEW JERSEY INSTITUTE OF TECHNOLOGY,R01,2006,476401,0.08170157309185712
"Automated Detection of Medical Errors DESCRIPTION:    The long-term goal of this proposal is to use the electronic medical record, including narrative text, to understand and encode the process of care for individual patients in order to improve patient safety.   Achieving this goal has the potential to help detect adverse events, and to differentiate medical errors from appropriately tailored care. The specific aims for this proposal are as follows: 1) To understand and encode the process of care for individual patients using data in the electronic medical record, including narrative text.   2) To use a more detailed understanding of patients' processes of care to improve automated adverse event detection. 3) To match processes of care for individual patients against accepted care pathways in order to identify discrepancies. We will capitalize on three core technologies that are in active use by clinicians and researchers in our busy clinical setting: 1) a Web-based clinical information system and its associated clinical data repository (WebCIS), 2) a full medical language parser (MedLEE), and 3) a semi-structured, electronic physician documentation system built by the applicant specifically to support this project (eNote).   Methods will include evaluating the performance (sensitivity, specificity and positive predictive value) of our system, DETER+MINE (DETecting ERrors Mining Narrative Electronically), to model the care process and detect adverse events and pathway deviations. We will utilize explicit process criteria and manual, retrospective chart review as a gold standard.   This research is intended to provide proof of concept that combining natural language processing of clinical narrative with traditional sources of coded data is required for effective screening with automated defection systems. This approach has the potential to impact significantly on our ability to detect and investigate medical errors, adverse medical events, and pathway deviations by reducing reliance on costly and slow manual chart reviews. n/a",Automated Detection of Medical Errors,7282343,K22LM008805,"['Address', 'Adverse event', 'Applications Grants', 'Caring', 'Causations', 'Cause of Death', 'Clinical', 'Clinical Data', 'Clinical Decision Support Systems', 'Clinical Pathways', 'Code', 'Computerized Medical Record', 'Critical Care', 'Data', 'Databases', 'Decision Support Systems', 'Detection', 'Documentation', 'Effectiveness', 'Electronics', 'Event', 'Goals', 'Gold', 'Guidelines', 'Hospitals', 'Human', 'Individual', 'Information Systems', 'Institute of Medicine (U.S.)', 'Language', 'Manuals', 'Medical', 'Medical Errors', 'Methods', 'Mining', 'Modeling', 'Natural Language Processing', 'Negligence', 'Online Systems', 'Pathway interactions', 'Patients', 'Performance', 'Physicians', 'Predictive Value', 'Prevention', 'Process', 'Range', 'Recommendation', 'Reliance', 'Reporting', 'Research', 'Research Personnel', 'Screening procedure', 'Sensitivity and Specificity', 'Source Code', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'United States', 'base', 'computerized', 'concept', 'experience', 'improved', 'patient safety', 'programs', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,K22,2007,135000,0.06410623999991938
"Automated Detection of Medical Errors DESCRIPTION:    The long-term goal of this proposal is to use the electronic medical record, including narrative text, to understand and encode the process of care for individual patients in order to improve patient safety.   Achieving this goal has the potential to help detect adverse events, and to differentiate medical errors from appropriately tailored care. The specific aims for this proposal are as follows: 1) To understand and encode the process of care for individual patients using data in the electronic medical record, including narrative text.   2) To use a more detailed understanding of patients' processes of care to improve automated adverse event detection. 3) To match processes of care for individual patients against accepted care pathways in order to identify discrepancies. We will capitalize on three core technologies that are in active use by clinicians and researchers in our busy clinical setting: 1) a Web-based clinical information system and its associated clinical data repository (WebCIS), 2) a full medical language parser (MedLEE), and 3) a semi-structured, electronic physician documentation system built by the applicant specifically to support this project (eNote).   Methods will include evaluating the performance (sensitivity, specificity and positive predictive value) of our system, DETER+MINE (DETecting ERrors Mining Narrative Electronically), to model the care process and detect adverse events and pathway deviations. We will utilize explicit process criteria and manual, retrospective chart review as a gold standard.   This research is intended to provide proof of concept that combining natural language processing of clinical narrative with traditional sources of coded data is required for effective screening with automated defection systems. This approach has the potential to impact significantly on our ability to detect and investigate medical errors, adverse medical events, and pathway deviations by reducing reliance on costly and slow manual chart reviews. n/a",Automated Detection of Medical Errors,7124706,K22LM008805,"['automated medical record system', 'behavioral /social science research tag', 'biomedical automation', 'clinical research', 'health care quality', 'health services research tag', 'human data', 'information retrieval', 'medical records', 'method development', 'patient care management', 'patient safety /medical error']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,K22,2006,135000,0.06410623999991938
"Automated Detection of Medical Errors DESCRIPTION:    The long-term goal of this proposal is to use the electronic medical record, including narrative text, to understand and encode the process of care for individual patients in order to improve patient safety.   Achieving this goal has the potential to help detect adverse events, and to differentiate medical errors from appropriately tailored care. The specific aims for this proposal are as follows: 1) To understand and encode the process of care for individual patients using data in the electronic medical record, including narrative text.   2) To use a more detailed understanding of patients' processes of care to improve automated adverse event detection. 3) To match processes of care for individual patients against accepted care pathways in order to identify discrepancies. We will capitalize on three core technologies that are in active use by clinicians and researchers in our busy clinical setting: 1) a Web-based clinical information system and its associated clinical data repository (WebCIS), 2) a full medical language parser (MedLEE), and 3) a semi-structured, electronic physician documentation system built by the applicant specifically to support this project (eNote).   Methods will include evaluating the performance (sensitivity, specificity and positive predictive value) of our system, DETER+MINE (DETecting ERrors Mining Narrative Electronically), to model the care process and detect adverse events and pathway deviations. We will utilize explicit process criteria and manual, retrospective chart review as a gold standard.   This research is intended to provide proof of concept that combining natural language processing of clinical narrative with traditional sources of coded data is required for effective screening with automated defection systems. This approach has the potential to impact significantly on our ability to detect and investigate medical errors, adverse medical events, and pathway deviations by reducing reliance on costly and slow manual chart reviews. n/a",Automated Detection of Medical Errors,6958394,K22LM008805,"['automated medical record system', 'behavioral /social science research tag', 'biomedical automation', 'clinical research', 'health care quality', 'health services research tag', 'human data', 'information retrieval', 'medical records', 'method development', 'patient care management', 'patient safety /medical error']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,K22,2005,134800,0.06410623999991938
"Evidence based anomaly detection in clinical databases.    DESCRIPTION (provided by applicant):       Medical errors and their timely identification remain an important problem in clinical practice. Electronic medical record repositories and electronic data processing offer an opportunity to identify such errors in time to prevent them or at least attenuate their harm.  Typical computer-based error detection methods rely on the use of clinical knowledge, such as expert-derived rules, that is incorporated into the monitoring and alerting systems. Alerting that is based on knowledge is generally reliable; however, it is time-consuming and costly to extract and codify such knowledge, and as a consequence such systems are relatively narrow in their scope.  We propose to develop and evaluate a data-based approach for detecting clinical outliers (anomalies) that is complementary to knowledge-based approaches. This new approach is based on comparing clinical actions, such as medications given and labs ordered, taken for the current patient to those actions taken for similar patients in the recent past, as recorded in a clinical database. If a clinical action for the current patient is highly unusual, then a cautionary alert is raised along with an explanation for why the action appears to be unusual. Key advantages of the new technique are that it works with minimal prior knowledge, and it may detect anomalies for which no rules have yet been written. Thus, this data-driven approach to clinical anomaly detection is expected to complement knowledge-based alerting methods. We propose to implement a data-driven anomaly detection method, and then evaluate it in a laboratory setting using retrospective data for the cohort of surgical cardiac patients.  The project investigators comprise a multidisciplinary team with expertise in rule-based alerting in a hospital setting, clinical pharmacy, laboratory medicine, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories.          n/a",Evidence based anomaly detection in clinical databases.,7385073,R21LM009102,"['Anticoagulants', 'Arts', 'Attenuated', 'Automatic Data Processing', 'Blood Platelets', 'Cardiac', 'Catheters', 'Cessation of life', 'Clinical', 'Complement', 'Computerized Medical Record', 'Computers', 'Condition', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Detection', 'Development', 'Disadvantaged', 'Dose', 'Drops', 'Evaluation', 'Event', 'Flushing', 'General Population', 'Healthcare', 'Heparin', 'Hospitals', 'Imagery', 'Information Resources Management', 'Invasive', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Lead', 'Left', 'Life', 'Low-Molecular-Weight Heparin', 'Machine Learning', 'Manuals', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Monitor', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Patient Monitoring', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Physicians', 'Platelet Count measurement', 'Platelet Transfusion', 'Practice Management', 'Procedures', 'Range', 'Records', 'Research', 'Research Personnel', 'Risk', 'Signal Transduction', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'System', 'Techniques', 'Testing', 'Thrombocytopenia', 'Thrombosis', 'Time', 'Validation', 'Warfarin', 'Work', 'Writing', 'base', 'biomedical informatics', 'cohort', 'computer based statistical methods', 'cost', 'design', 'improved', 'interest', 'knowledge base', 'medical specialties', 'multidisciplinary', 'novel', 'novel strategies', 'prevent', 'prophylactic', 'repository', 'response', 'tool']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2008,193536,0.09234009955064455
"Evidence based anomaly detection in clinical databases.    DESCRIPTION (provided by applicant):       Medical errors and their timely identification remain an important problem in clinical practice. Electronic medical record repositories and electronic data processing offer an opportunity to identify such errors in time to prevent them or at least attenuate their harm.  Typical computer-based error detection methods rely on the use of clinical knowledge, such as expert-derived rules, that is incorporated into the monitoring and alerting systems. Alerting that is based on knowledge is generally reliable; however, it is time-consuming and costly to extract and codify such knowledge, and as a consequence such systems are relatively narrow in their scope.  We propose to develop and evaluate a data-based approach for detecting clinical outliers (anomalies) that is complementary to knowledge-based approaches. This new approach is based on comparing clinical actions, such as medications given and labs ordered, taken for the current patient to those actions taken for similar patients in the recent past, as recorded in a clinical database. If a clinical action for the current patient is highly unusual, then a cautionary alert is raised along with an explanation for why the action appears to be unusual. Key advantages of the new technique are that it works with minimal prior knowledge, and it may detect anomalies for which no rules have yet been written. Thus, this data-driven approach to clinical anomaly detection is expected to complement knowledge-based alerting methods. We propose to implement a data-driven anomaly detection method, and then evaluate it in a laboratory setting using retrospective data for the cohort of surgical cardiac patients.  The project investigators comprise a multidisciplinary team with expertise in rule-based alerting in a hospital setting, clinical pharmacy, laboratory medicine, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories.          n/a",Evidence based anomaly detection in clinical databases.,7197167,R21LM009102,"['Anticoagulants', 'Arts', 'Attenuated', 'Automatic Data Processing', 'Blood Platelets', 'Cardiac', 'Catheters', 'Cessation of life', 'Clinical', 'Complement', 'Computerized Medical Record', 'Computers', 'Condition', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Detection', 'Development', 'Disadvantaged', 'Dose', 'Drops', 'Evaluation', 'Event', 'Flushing', 'General Population', 'Healthcare', 'Heparin', 'Hospitals', 'Imagery', 'Information Resources Management', 'Invasive', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Lead', 'Left', 'Life', 'Low-Molecular-Weight Heparin', 'Machine Learning', 'Manuals', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Monitor', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Patient Monitoring', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Physicians', 'Platelet Count measurement', 'Platelet Transfusion', 'Practice Management', 'Procedures', 'Range', 'Records', 'Research', 'Research Personnel', 'Risk', 'Signal Transduction', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'Statistical Models', 'Subgroup', 'System', 'Techniques', 'Testing', 'Thrombocytopenia', 'Thrombosis', 'Time', 'Validation', 'Warfarin', 'Work', 'Writing', 'base', 'biomedical informatics', 'cohort', 'computer based statistical methods', 'cost', 'design', 'improved', 'interest', 'knowledge base', 'medical specialties', 'multidisciplinary', 'novel', 'novel strategies', 'prevent', 'prophylactic', 'repository', 'response', 'tool']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2007,161562,0.09234009955064455
"The Use of Mathematic Algorithms in the Prevention of Improper Medical Payments    DESCRIPTION (provided by applicant): The goal of this research is to create software that uses mathematical algorithms to detect medical billing coding errors prior to payment. The well-publicized failure of current healthcare cost containment technologies to prevent improper payments in both the commercial healthcare market and the federal Medicare program highlights the urgent need for a new approach to the growing problem of out of control medical costs. A recent federal study by the GAO estimated that improper payments by Medicare alone were in excess of 21 billion dollars, a truly staggering 48.1 percent of all improper payments by federal programs. Like SPAM, whose dynamic nature makes static or post hoc remedies ineffective, effective cost containment in one area often merely leads to the creation of new areas of abuse. Clearly, the ideal solution is a system that can evaluate the fairness of payments before they are made, and that can respond to dynamic patterns of abuse. The first step in creating such a system is the creation of robust method for sorting bills for appropriate rule-based analysis on the basis of the type of bill. Currently neither Medicare nor major insurers are capable of making this classification reliably except through the use of inefficient, static rules and the use of manual sorting--a costly and inefficient approach to assuring timely payment to hospitals and medical providers. We propose a novel method for using mathematical algorithms that utilize machine-learning (ML) methods to address the problem of medical bill categorization, the first step in coding error detection. Specifically, we propose the evaluation of a variety of genetic algorithms that are well adapted to the problems of large, dynamic datasets and can be ""trained"" using real world correctly coded datasets in healthcare claims. This work is particularly timely due to recent Medicare contracting reform. Using more than 50 contractors and carriers, bill classification is largely determined by the carrier's contract. Centralizing this process to only four payment centers will require the classification system we propose. [This research is directed toward the development of software applications that will detect billing errors and perform proper edits to payment of medical bills. Current anticipated changes and reforms in the Medicare system will require these systems, which do not currently exist in the public or private sector.]             n/a",The Use of Mathematic Algorithms in the Prevention of Improper Medical Payments,7316071,R43LM009190,"['Address', 'Age', 'Algorithms', 'Area', 'Arts', 'Classification', 'Code', 'Collaborations', 'Computer Simulation', 'Computer software', 'Contractor', 'Contracts', 'Cost Control', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Development', 'Elements', 'Environment', 'Evaluation', 'Failure', 'Genetic Programming', 'Goals', 'Health Care Costs', 'Health Care Fraud', 'Health Personnel', 'Healthcare', 'Healthcare Market', 'Healthcare Systems', 'Hospitals', 'Industry', 'Inpatients', 'Insurance Carriers', 'Learning', 'Machine Learning', 'Manuals', 'Mathematics', 'Medical', 'Medicare', 'Methods', 'Mining', 'Modeling', 'Nature', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Outpatients', 'Pattern', 'Phase', 'Policies', 'Population', 'Prevention', 'Private Sector', 'Process', 'Provider', 'Rate', 'Reporting', 'Research', 'Running', 'Small Business Technology Transfer Research', 'Solutions', 'Sorting - Cell Movement', 'Standards of Weights and Measures', 'System', 'Technology', 'Testing', 'Training', 'Work', 'base', 'college', 'computerized', 'cost', 'design', 'experience', 'improved', 'mathematical algorithm', 'novel', 'novel strategies', 'payment', 'prevent', 'programs', 'size', 'software development', 'stem', 'success']",NLM,"QMEDTRIX SYSTEMS, INC.",R43,2007,92482,0.027185334293126227
"Assessing quality of individual predictions in medical decision support systems    DESCRIPTION (provided by applicant):      Medical decision support tools are increasingly available on the Internet and are being used by lay persons as well as health care professionals. The goal of some of these tools is to provide an ""individualized"" prediction of future health care related events such as prognosis in breast cancer given specific information about the individual. These tools are usually based on models synthesized from data with a fine granularity of information. Under the umbrella of ""personalized"" medicine, these individualized prognostic assessments are sought as a means to replace general prognostic information given to patients with specific probability estimates that pertain to a small stratum to which the patient belongs, and ultimately specifically to each patient (i.e., a stratum with n=1). Subsequently, these estimates are used to inform decision making and are therefore of critical importance for public health.   Responsible utilization of prognostic models for patient counseling and medical decision making requires thorough model validation. Verification that the estimated or predicted event probabilities reflect the underlying true probability for a particular individual (i.e., verifying the calibration of the prognostic model) is a critical but often overlooked step in evaluation, which usually favors the verification of the discriminatory ability of the model. Selection of the best predictive model for a given problem should be based on robust comparison that takes into account errors in individual predictions, calibration, and discrimination indices. A robust test for comparison of calibration across different models does not currently exist.   Our specific aims are to: (1) Characterize the main deficiencies of existing calibration indices in the context of individualized predictions and develop a new model-independent calibration index and comparison test that can be used to assess and compare predictive models based on both statistical regression and machine earning methods; (2) Unify the theories on decomposition of error into discrimination and calibration components stemming from the statistical and machine learning communities to derive a refined measure of alteration that can be calculated from measures of error and discrimination. We will compare the performance of the new methods with existing ones in different predictive models derived from real clinical data related to different medical domains.          n/a",Assessing quality of individual predictions in medical decision support systems,8142647,R01LM009520,"['Accounting', 'American', 'Area', 'Biological Neural Networks', 'Calibration', 'Characteristics', 'Clinical Data', 'Communities', 'Computer software', 'Coronary heart disease', 'Counseling', 'Data', 'Data Set', 'Decision Making', 'Decision Support Systems', 'Discrimination', 'Entropy', 'Evaluation', 'Evaluation Indexes', 'Event', 'Exhibits', 'Future', 'Goals', 'Guidelines', 'Health Personnel', 'Health Professional', 'Healthcare', 'Heart', 'Individual', 'Internet', 'Knowledge', 'Learning', 'Literature', 'Logistic Regressions', 'Machine Learning', 'Measures', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Persons', 'Probability', 'Public Health', 'Publishing', 'ROC Curve', 'Regression Analysis', 'Reporting', 'Research', 'Risk', 'Risk Assessment', 'Site', 'System', 'Testing', 'Validation', 'base', 'computer based statistical methods', 'indexing', 'interest', 'malignant breast neoplasm', 'outcome forecast', 'predictive modeling', 'prognostic', 'statistics', 'stem', 'theories', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2010,159993,0.07066011745694104
"Assessing quality of individual predictions in medical decision support systems    DESCRIPTION (provided by applicant):      Medical decision support tools are increasingly available on the Internet and are being used by lay persons as well as health care professionals. The goal of some of these tools is to provide an ""individualized"" prediction of future health care related events such as prognosis in breast cancer given specific information about the individual. These tools are usually based on models synthesized from data with a fine granularity of information. Under the umbrella of ""personalized"" medicine, these individualized prognostic assessments are sought as a means to replace general prognostic information given to patients with specific probability estimates that pertain to a small stratum to which the patient belongs, and ultimately specifically to each patient (i.e., a stratum with n=1). Subsequently, these estimates are used to inform decision making and are therefore of critical importance for public health.   Responsible utilization of prognostic models for patient counseling and medical decision making requires thorough model validation. Verification that the estimated or predicted event probabilities reflect the underlying true probability for a particular individual (i.e., verifying the calibration of the prognostic model) is a critical but often overlooked step in evaluation, which usually favors the verification of the discriminatory ability of the model. Selection of the best predictive model for a given problem should be based on robust comparison that takes into account errors in individual predictions, calibration, and discrimination indices. A robust test for comparison of calibration across different models does not currently exist.   Our specific aims are to: (1) Characterize the main deficiencies of existing calibration indices in the context of individualized predictions and develop a new model-independent calibration index and comparison test that can be used to assess and compare predictive models based on both statistical regression and machine earning methods; (2) Unify the theories on decomposition of error into discrimination and calibration components stemming from the statistical and machine learning communities to derive a refined measure of alteration that can be calculated from measures of error and discrimination. We will compare the performance of the new methods with existing ones in different predictive models derived from real clinical data related to different medical domains.          n/a",Assessing quality of individual predictions in medical decision support systems,7938216,R01LM009520,"['Accounting', 'American', 'Area', 'Biological Neural Networks', 'Calibration', 'Characteristics', 'Clinical Data', 'Communities', 'Computer software', 'Coronary heart disease', 'Counseling', 'Data', 'Data Set', 'Decision Making', 'Decision Support Systems', 'Discrimination', 'Entropy', 'Evaluation', 'Evaluation Indexes', 'Event', 'Exhibits', 'Future', 'Goals', 'Guidelines', 'Health Personnel', 'Health Professional', 'Healthcare', 'Heart', 'Individual', 'Internet', 'Knowledge', 'Learning', 'Literature', 'Logistic Regressions', 'Machine Learning', 'Measures', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Persons', 'Probability', 'Public Health', 'Publishing', 'ROC Curve', 'Regression Analysis', 'Reporting', 'Research', 'Risk', 'Risk Assessment', 'Site', 'System', 'Testing', 'Validation', 'base', 'computer based statistical methods', 'indexing', 'interest', 'malignant breast neoplasm', 'outcome forecast', 'predictive modeling', 'prognostic', 'statistics', 'stem', 'theories', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2009,164827,0.07066011745694104
"Assessing quality of individual predictions in medical decision support systems    DESCRIPTION (provided by applicant):      Medical decision support tools are increasingly available on the Internet and are being used by lay persons as well as health care professionals. The goal of some of these tools is to provide an ""individualized"" prediction of future health care related events such as prognosis in breast cancer given specific information about the individual. These tools are usually based on models synthesized from data with a fine granularity of information. Under the umbrella of ""personalized"" medicine, these individualized prognostic assessments are sought as a means to replace general prognostic information given to patients with specific probability estimates that pertain to a small stratum to which the patient belongs, and ultimately specifically to each patient (i.e., a stratum with n=1). Subsequently, these estimates are used to inform decision making and are therefore of critical importance for public health.   Responsible utilization of prognostic models for patient counseling and medical decision making requires thorough model validation. Verification that the estimated or predicted event probabilities reflect the underlying true probability for a particular individual (i.e., verifying the calibration of the prognostic model) is a critical but often overlooked step in evaluation, which usually favors the verification of the discriminatory ability of the model. Selection of the best predictive model for a given problem should be based on robust comparison that takes into account errors in individual predictions, calibration, and discrimination indices. A robust test for comparison of calibration across different models does not currently exist.   Our specific aims are to: (1) Characterize the main deficiencies of existing calibration indices in the context of individualized predictions and develop a new model-independent calibration index and comparison test that can be used to assess and compare predictive models based on both statistical regression and machine earning methods; (2) Unify the theories on decomposition of error into discrimination and calibration components stemming from the statistical and machine learning communities to derive a refined measure of alteration that can be calculated from measures of error and discrimination. We will compare the performance of the new methods with existing ones in different predictive models derived from real clinical data related to different medical domains.          n/a",Assessing quality of individual predictions in medical decision support systems,7932595,R01LM009520,"['Accounting', 'American', 'Area', 'Biological Neural Networks', 'Calibration', 'Characteristics', 'Clinical Data', 'Communities', 'Computer software', 'Coronary heart disease', 'Counseling', 'Data', 'Data Set', 'Decision Making', 'Decision Support Systems', 'Discrimination', 'Entropy', 'Evaluation', 'Evaluation Indexes', 'Event', 'Exhibits', 'Future', 'Goals', 'Guidelines', 'Health Personnel', 'Health Professional', 'Healthcare', 'Heart', 'Individual', 'Internet', 'Knowledge', 'Learning', 'Literature', 'Logistic Regressions', 'Machine Learning', 'Measures', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Persons', 'Probability', 'Public Health', 'Publishing', 'ROC Curve', 'Regression Analysis', 'Reporting', 'Research', 'Risk', 'Risk Assessment', 'Site', 'System', 'Testing', 'Validation', 'base', 'computer based statistical methods', 'indexing', 'interest', 'malignant breast neoplasm', 'outcome forecast', 'predictive modeling', 'prognostic', 'statistics', 'stem', 'theories', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2009,329652,0.07066011745694104
"Assessing quality of individual predictions in medical decision support systems    DESCRIPTION (provided by applicant):      Medical decision support tools are increasingly available on the Internet and are being used by lay persons as well as health care professionals. The goal of some of these tools is to provide an ""individualized"" prediction of future health care related events such as prognosis in breast cancer given specific information about the individual. These tools are usually based on models synthesized from data with a fine granularity of information. Under the umbrella of ""personalized"" medicine, these individualized prognostic assessments are sought as a means to replace general prognostic information given to patients with specific probability estimates that pertain to a small stratum to which the patient belongs, and ultimately specifically to each patient (i.e., a stratum with n=1). Subsequently, these estimates are used to inform decision making and are therefore of critical importance for public health.   Responsible utilization of prognostic models for patient counseling and medical decision making requires thorough model validation. Verification that the estimated or predicted event probabilities reflect the underlying true probability for a particular individual (i.e., verifying the calibration of the prognostic model) is a critical but often overlooked step in evaluation, which usually favors the verification of the discriminatory ability of the model. Selection of the best predictive model for a given problem should be based on robust comparison that takes into account errors in individual predictions, calibration, and discrimination indices. A robust test for comparison of calibration across different models does not currently exist.   Our specific aims are to: (1) Characterize the main deficiencies of existing calibration indices in the context of individualized predictions and develop a new model-independent calibration index and comparison test that can be used to assess and compare predictive models based on both statistical regression and machine earning methods; (2) Unify the theories on decomposition of error into discrimination and calibration components stemming from the statistical and machine learning communities to derive a refined measure of alteration that can be calculated from measures of error and discrimination. We will compare the performance of the new methods with existing ones in different predictive models derived from real clinical data related to different medical domains.          n/a",Assessing quality of individual predictions in medical decision support systems,7435263,R01LM009520,"['Accounting', 'American', 'Area', 'Biological Neural Networks', 'Calibration', 'Characteristics', 'Clinical Data', 'Communities', 'Computer software', 'Coronary heart disease', 'Counseling', 'Data', 'Data Set', 'Decision Making', 'Decision Support Systems', 'Discrimination', 'Entropy', 'Evaluation', 'Evaluation Indexes', 'Event', 'Exhibits', 'Facility Construction Funding Category', 'Future', 'Goals', 'Guidelines', 'Health Personnel', 'Health Professional', 'Healthcare', 'Heart', 'Individual', 'Internet', 'Knowledge', 'Learning', 'Literature', 'Logistic Models', 'Logistic Regressions', 'Machine Learning', 'Measures', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Numbers', 'Outcome', 'Patients', 'Performance', 'Persons', 'Probability', 'Public Health', 'Publishing', 'ROC Curve', 'Regression Analysis', 'Reporting', 'Research', 'Risk', 'Risk Assessment', 'Site', 'System', 'Testing', 'Validation', 'base', 'computer based statistical methods', 'indexing', 'interest', 'malignant breast neoplasm', 'outcome forecast', 'predictive modeling', 'prognostic', 'statistics', 'stem', 'theories', 'tool']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2008,373392,0.07066011745694104
"Assessing quality of individual predictions in medical decision support systems    DESCRIPTION (provided by applicant):      Medical decision support tools are increasingly available on the Internet and are being used by lay persons as well as health care professionals. The goal of some of these tools is to provide an ""individualized"" prediction of future health care related events such as prognosis in breast cancer given specific information about the individual. These tools are usually based on models synthesized from data with a fine granularity of information. Under the umbrella of ""personalized"" medicine, these individualized prognostic assessments are sought as a means to replace general prognostic information given to patients with specific probability estimates that pertain to a small stratum to which the patient belongs, and ultimately specifically to each patient (i.e., a stratum with n=1). Subsequently, these estimates are used to inform decision making and are therefore of critical importance for public health.   Responsible utilization of prognostic models for patient counseling and medical decision making requires thorough model validation. Verification that the estimated or predicted event probabilities reflect the underlying true probability for a particular individual (i.e., verifying the calibration of the prognostic model) is a critical but often overlooked step in evaluation, which usually favors the verification of the discriminatory ability of the model. Selection of the best predictive model for a given problem should be based on robust comparison that takes into account errors in individual predictions, calibration, and discrimination indices. A robust test for comparison of calibration across different models does not currently exist.   Our specific aims are to: (1) Characterize the main deficiencies of existing calibration indices in the context of individualized predictions and develop a new model-independent calibration index and comparison test that can be used to assess and compare predictive models based on both statistical regression and machine earning methods; (2) Unify the theories on decomposition of error into discrimination and calibration components stemming from the statistical and machine learning communities to derive a refined measure of alteration that can be calculated from measures of error and discrimination. We will compare the performance of the new methods with existing ones in different predictive models derived from real clinical data related to different medical domains.          n/a",Assessing quality of individual predictions in medical decision support systems,7245581,R01LM009520,[' '],NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2007,380625,0.07066011745694104
"Automated creation of clinical progress notes with machine learning    DESCRIPTION (provided by applicant): Clinical encounters require the creation of an enormous amount of documentation. This documentation is tedious, time-consuming, and, in practice, is usually created hours after the encounter has occurred. The requirement to create this documentation places a tremendous burden on the time of clinical staff, and due to pressing workloads, can lead to incorrect recall and clinical data errors. The project intends to prove that progress note portion of this documentation process can be automated by novel application of recent advances in machine learning. A software system will be built with Support Vector Machine theory at its core. The software system can learn from existing clinical progress notes, and then apply those learned method by auto-generating the subjective/analytical portions of the note. The project will also examine learnings from individual physician notes compared to collections of multiple physician notes in order to built a superior model. The project's impact on the quality, and cost, of care should be dramatic. Reductions in documentation error rates and increases in physician productivity equate to an incredible array of quantitative benefits. Qualitatively, the project should make providing healthcare a more enjoyable experience for all involved, given the reduction of administrative time on the part of the highly skilled clinical staff.      PUBLIC HEALTH RELEVANCE: Clinical encounter documentation provides the detailed patient health data which enables all care providers to have an accurate picture of the patient's clinical activities. Currently, creating that documentation requires a manual, and time-consuming process, which reduces the amount of time clinicians spend with patients, and increases the possibility of data errors. We propose to build a machine learning-based software system that can automatically generate the required clinical documentation, thereby saving time and reducing errors, which will improve the overall quality of patient care.              PROJECT NARRATIVE (RELEVANCE) Clinical encounter documentation provides the detailed patient health data which enables all care providers to have an accurate picture of the patient's clinical activities. Currently, creating that documentation requires a manual, and time-consuming process, which reduces the amount of time clinicians spend with patients, and increases the possibility of data errors. We propose to build a machine learning-based software system that can automatically generate the required clinical documentation, thereby saving time and reducing errors, which will improve the overall quality of patient care.",Automated creation of clinical progress notes with machine learning,7926080,R43LM010775,"['Administrator', 'Algorithms', 'Automation', 'Caring', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Data', 'Disease Management', 'Documentation', 'Education', 'Educational process of instructing', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Foundations', 'Health', 'Health Personnel', 'Healthcare', 'Hour', 'Individual', 'Institution', 'Lead', 'Learning', 'Machine Learning', 'Manuals', 'Medical Research', 'Medical Students', 'Methodology', 'Methods', 'Modeling', 'Patient Care', 'Patient Care Planning', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Productivity', 'Provider', 'Research Personnel', 'Series', 'Standardization', 'Statistical Models', 'System', 'Time', 'Training', 'Workload', 'base', 'clinical Diagnosis', 'clinical care', 'clinical decision-making', 'clinically relevant', 'cost', 'experience', 'health organization', 'improved', 'novel', 'public health relevance', 'quality assurance', 'software systems', 'theories', 'tool', 'treatment planning']",NLM,BLENDERHOUSE,R43,2010,199404,0.16997885337789792
"Technology, Cognitive Work, and Patient Safety: A Information-Oriented Model DESCRIPTION (provided by applicant):          Managing information for safe care, especially in the new health IT environment, continues to be cognitively challenging work for care providers. Adequate cognitive support to providers when building health IT systems can increase provider effectiveness in their work by reducing workarounds and medical errors. To model information support, cognitive engineers traditionally have modeled the task-technology-people triad. But, preliminary work from the investigator suggests that low-level information really binds care tasks and how providers perform their tasks with technologies. The researcher has piloted an information-based cognitive work design technique for modeling low-level healthcare information providers create, use and share in their cognitive work. The technique, when extended to medical error modeling, will improve patient safety. However, for the extension to succeed, the researcher needs to enhance her skills in health informatics, and medical error modeling in addition to acquiring a comprehensive understanding of health outcomes modeling. This Pathway to Independence (K99/R00) grant application describes a training and career development plan that will allow the candidate, a Post Doctoral Research Fellow in the Quality and Safety Research Group at Johns Hopkins School of Medicine to achieve these objectives. The training component will be carried out under the mentorship of Dr. H. Lehmann (JHU SOM, Health Informatics). Drs. Peter Pronovost (JHU SOM, Patient Safety), Ayse Gurses (JHU SOM, Patient Safety) and Ann Bisantz (SUNY, Buffalo, Cognitive Systems Engineering) will provide additional mentoring in their areas of expertise. The long-term goal of this Pathway to Independence (K99/R00) project is to design health IT systems for provider work based on low-level information they create and use for care. During the award period, research will be focused on the following specific aims: (1) Map provider process workflows corresponding to patient state changes; (2) Identify low-level information attributes providers create and use in care processes, and relate them to potential medical errors at each major care process step; and (3) demonstrate utility of low-level information attributes as design bases for health IT support systems. Project Narrative (Public Health Relevance) Project aims are to understand and model how health care providers create, use and transform health information in their work. When health IT support systems are designed based on information providers create or use in their work, workarounds will be avoided, so medical errors from poor health IT support systems can be reduced.","Technology, Cognitive Work, and Patient Safety: A Information-Oriented Model",8899625,R00LM011309,"['Address', 'Adverse event', 'Age', 'Applications Grants', 'Area', 'Award', 'Awareness', 'Binding', 'Boxing', 'Buffaloes', 'Caring', 'Cognitive', 'Complex', 'Computer software', 'Cues', 'Data', 'Descriptor', 'Development Plans', 'Effectiveness', 'Engineering', 'Environment', 'Event', 'Focus Groups', 'Goals', 'Health', 'Health Personnel', 'Healthcare', 'Hospitals', 'Hypersensitivity', 'Information Systems', 'Intervention', 'Interview', 'Knowledge', 'Link', 'Maps', 'Medical Errors', 'Mentors', 'Mentorship', 'Modeling', 'Names', 'Nature', 'Nurses', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patient Discharge', 'Patients', 'Phase', 'Physicians', 'Postdoctoral Fellow', 'Process', 'Property', 'Provider', 'Public Health Informatics', 'Research', 'Research Personnel', 'Safety', 'Severities', 'Shadowing (Histology)', 'Software Engineering', 'Support System', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Triad Acrylic Resin', 'Work', 'base', 'career development', 'cognitive system', 'design', 'improved', 'information model', 'information organization', 'medical schools', 'patient safety', 'public health relevance', 'skills']",NLM,UNIVERSITY OF IOWA,R00,2015,217227,0.06732225306034612
"Technology, Cognitive Work, and Patient Safety: A Information-Oriented Model     DESCRIPTION (provided by applicant):          Managing information for safe care, especially in the new health IT environment, continues to be cognitively challenging work for care providers. Adequate cognitive support to providers when building health IT systems can increase provider effectiveness in their work by reducing workarounds and medical errors. To model information support, cognitive engineers traditionally have modeled the task-technology-people triad. But, preliminary work from the investigator suggests that low-level information really binds care tasks and how providers perform their tasks with technologies. The researcher has piloted an information-based cognitive work design technique for modeling low-level healthcare information providers create, use and share in their cognitive work. The technique, when extended to medical error modeling, will improve patient safety. However, for the extension to succeed, the researcher needs to enhance her skills in health informatics, and medical error modeling in addition to acquiring a comprehensive understanding of health outcomes modeling. This Pathway to Independence (K99/R00) grant application describes a training and career development plan that will allow the candidate, a Post Doctoral Research Fellow in the Quality and Safety Research Group at Johns Hopkins School of Medicine to achieve these objectives. The training component will be carried out under the mentorship of Dr. H. Lehmann (JHU SOM, Health Informatics). Drs. Peter Pronovost (JHU SOM, Patient Safety), Ayse Gurses (JHU SOM, Patient Safety) and Ann Bisantz (SUNY, Buffalo, Cognitive Systems Engineering) will provide additional mentoring in their areas of expertise. The long-term goal of this Pathway to Independence (K99/R00) project is to design health IT systems for provider work based on low-level information they create and use for care. During the award period, research will be focused on the following specific aims: (1) Map provider process workflows corresponding to patient state changes; (2) Identify low-level information attributes providers create and use in care processes, and relate them to potential medical errors at each major care process step; and (3) demonstrate utility of low-level information attributes as design bases for health IT support systems.                 Project Narrative (Public Health Relevance) Project aims are to understand and model how health care providers create, use and transform health information in their work. When health IT support systems are designed based on information providers create or use in their work, workarounds will be avoided, so medical errors from poor health IT support systems can be reduced.","Technology, Cognitive Work, and Patient Safety: A Information-Oriented Model",8725229,R00LM011309,"['Address', 'Adverse event', 'Age', 'Applications Grants', 'Area', 'Award', 'Awareness', 'Binding', 'Boxing', 'Buffaloes', 'Caring', 'Cognitive', 'Complex', 'Computer software', 'Cues', 'Data', 'Descriptor', 'Development Plans', 'Effectiveness', 'Engineering', 'Environment', 'Event', 'Focus Groups', 'Goals', 'Health', 'Health Personnel', 'Healthcare', 'Hospitals', 'Hypersensitivity', 'Information Systems', 'Intervention', 'Interview', 'Knowledge', 'Link', 'Maps', 'Medical Errors', 'Mentors', 'Mentorship', 'Modeling', 'Names', 'Nature', 'Nurses', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patient Discharge', 'Patients', 'Phase', 'Physicians', 'Postdoctoral Fellow', 'Process', 'Property', 'Provider', 'Public Health Informatics', 'Research', 'Research Personnel', 'Safety', 'Severities', 'Shadowing (Histology)', 'Software Engineering', 'Support System', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Triad Acrylic Resin', 'Work', 'base', 'career development', 'cognitive system', 'design', 'improved', 'information model', 'information organization', 'medical schools', 'patient safety', 'public health relevance', 'skills']",NLM,UNIVERSITY OF IOWA,R00,2014,223831,0.06732225306034612
"Technology, Cognitive Work, and Patient Safety: A Information-Oriented Model     DESCRIPTION (provided by applicant):          Managing information for safe care, especially in the new health IT environment, continues to be cognitively challenging work for care providers. Adequate cognitive support to providers when building health IT systems can increase provider effectiveness in their work by reducing workarounds and medical errors. To model information support, cognitive engineers traditionally have modeled the task-technology-people triad. But, preliminary work from the investigator suggests that low-level information really binds care tasks and how providers perform their tasks with technologies. The researcher has piloted an information-based cognitive work design technique for modeling low-level healthcare information providers create, use and share in their cognitive work. The technique, when extended to medical error modeling, will improve patient safety. However, for the extension to succeed, the researcher needs to enhance her skills in health informatics, and medical error modeling in addition to acquiring a comprehensive understanding of health outcomes modeling. This Pathway to Independence (K99/R00) grant application describes a training and career development plan that will allow the candidate, a Post Doctoral Research Fellow in the Quality and Safety Research Group at Johns Hopkins School of Medicine to achieve these objectives. The training component will be carried out under the mentorship of Dr. H. Lehmann (JHU SOM, Health Informatics). Drs. Peter Pronovost (JHU SOM, Patient Safety), Ayse Gurses (JHU SOM, Patient Safety) and Ann Bisantz (SUNY, Buffalo, Cognitive Systems Engineering) will provide additional mentoring in their areas of expertise. The long-term goal of this Pathway to Independence (K99/R00) project is to design health IT systems for provider work based on low-level information they create and use for care. During the award period, research will be focused on the following specific aims: (1) Map provider process workflows corresponding to patient state changes; (2) Identify low-level information attributes providers create and use in care processes, and relate them to potential medical errors at each major care process step; and (3) demonstrate utility of low-level information attributes as design bases for health IT support systems.                  Project Narrative (Public Health Relevance) Project aims are to understand and model how health care providers create, use and transform health information in their work. When health IT support systems are designed based on information providers create or use in their work, workarounds will be avoided, so medical errors from poor health IT support systems can be reduced.","Technology, Cognitive Work, and Patient Safety: A Information-Oriented Model",8589974,R00LM011309,"['Address', 'Adverse event', 'Age', 'Applications Grants', 'Area', 'Award', 'Awareness', 'Binding', 'Boxing', 'Buffaloes', 'Caring', 'Cognitive', 'Complex', 'Computer software', 'Cues', 'Data', 'Descriptor', 'Development Plans', 'Effectiveness', 'Engineering', 'Environment', 'Event', 'Focus Groups', 'Goals', 'Health', 'Health Personnel', 'Healthcare', 'Hospitals', 'Hypersensitivity', 'Information Systems', 'Intervention', 'Interview', 'Knowledge', 'Link', 'Maps', 'Medical Errors', 'Mentors', 'Mentorship', 'Modeling', 'Names', 'Nature', 'Nurses', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patient Discharge', 'Patients', 'Phase', 'Physicians', 'Postdoctoral Fellow', 'Process', 'Property', 'Provider', 'Public Health Informatics', 'Research', 'Research Personnel', 'Safety', 'Severities', 'Shadowing (Histology)', 'Software Engineering', 'Support System', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Triad Acrylic Resin', 'Work', 'base', 'career development', 'cognitive system', 'design', 'improved', 'information model', 'information organization', 'medical schools', 'patient safety', 'public health relevance', 'skills']",NLM,UNIVERSITY OF IOWA,R00,2013,223972,0.06732225306034612
"Technology, Cognitive Work, and Patient Safety: A Information-Oriented Model     DESCRIPTION (provided by applicant):          Managing information for safe care, especially in the new health IT environment, continues to be cognitively challenging work for care providers. Adequate cognitive support to providers when building health IT systems can increase provider effectiveness in their work by reducing workarounds and medical errors. To model information support, cognitive engineers traditionally have modeled the task-technology-people triad. But, preliminary work from the investigator suggests that low-level information really binds care tasks and how providers perform their tasks with technologies. The researcher has piloted an information-based cognitive work design technique for modeling low-level healthcare information providers create, use and share in their cognitive work. The technique, when extended to medical error modeling, will improve patient safety. However, for the extension to succeed, the researcher needs to enhance her skills in health informatics, and medical error modeling in addition to acquiring a comprehensive understanding of health outcomes modeling. This Pathway to Independence (K99/R00) grant application describes a training and career development plan that will allow the candidate, a Post Doctoral Research Fellow in the Quality and Safety Research Group at Johns Hopkins School of Medicine to achieve these objectives. The training component will be carried out under the mentorship of Dr. H. Lehmann (JHU SOM, Health Informatics). Drs. Peter Pronovost (JHU SOM, Patient Safety), Ayse Gurses (JHU SOM, Patient Safety) and Ann Bisantz (SUNY, Buffalo, Cognitive Systems Engineering) will provide additional mentoring in their areas of expertise. The long-term goal of this Pathway to Independence (K99/R00) project is to design health IT systems for provider work based on low-level information they create and use for care. During the award period, research will be focused on the following specific aims: (1) Map provider process workflows corresponding to patient state changes; (2) Identify low-level information attributes providers create and use in care processes, and relate them to potential medical errors at each major care process step; and (3) demonstrate utility of low-level information attributes as design bases for health IT support systems.                  Project Narrative (Public Health Relevance) Project aims are to understand and model how health care providers create, use and transform health information in their work. When health IT support systems are designed based on information providers create or use in their work, workarounds will be avoided, so medical errors from poor health IT support systems can be reduced.","Technology, Cognitive Work, and Patient Safety: A Information-Oriented Model",8279720,K99LM011309,"['Address', 'Adverse event', 'Age', 'Applications Grants', 'Area', 'Award', 'Awareness', 'Binding', 'Boxing', 'Buffaloes', 'Caring', 'Cognitive', 'Complex', 'Computer software', 'Cues', 'Data', 'Descriptor', 'Development Plans', 'Effectiveness', 'Engineering', 'Environment', 'Event', 'Focus Groups', 'Goals', 'Health', 'Health Personnel', 'Healthcare', 'Hospitals', 'Hypersensitivity', 'Information Systems', 'Intervention', 'Interview', 'Knowledge', 'Link', 'Maps', 'Medical Errors', 'Mentors', 'Mentorship', 'Modeling', 'Names', 'Nature', 'Nurses', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patient Discharge', 'Patients', 'Phase', 'Physicians', 'Postdoctoral Fellow', 'Process', 'Property', 'Provider', 'Public Health Informatics', 'Research', 'Research Personnel', 'Safety', 'Severities', 'Shadowing (Histology)', 'Software Engineering', 'Support System', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Triad Acrylic Resin', 'Work', 'base', 'career development', 'cognitive system', 'design', 'improved', 'information model', 'information organization', 'medical schools', 'patient safety', 'public health relevance', 'skills']",NLM,UNIVERSITY OF IOWA,K99,2012,90000,0.06732225306034612
"Mining Complex Clinical Data for Patient Safety Research Medical errors hurt patients, cost money, and undermine the health care system. The first step to reducing errors is detecting them, for what cannot be detected cannot be managed. A number of approaches have been applied to medical error detection, including mandatory event reporting, voluntary near-miss reporting, chart review, and automated surveillance using information systems. Automated surveillance promises large-scale detection, minimal labor, and, potentially, detection in real time to prevent or recover from errors. Unfortunately, large amounts of important clinical information lie locked in narrative reports, unavailable to automated decision support systems. A number of tools have emerged from medical informatics and computer science- natural language processing, visualization tools, and machine learning- as well as methods for understanding cognitive processes. We hypothesize that the electronic medical record contains information useful for detecting errors and that natural language processing and other tools will allow us to retrieve the information. We will assemble a team skilled in natural language processing, data mining, terminology, patient safety research, and health care. We will use a clinical repository with ten years of data on two million patients. It includes administrative, laboratory, and pharmacy coded information as well as a wide range of narrative reports including discharge summaries, operative reports, outpatient notes, autopsy reports, resident signout notes, nursing notes, and reports from numerous ancillary services (radiology, pathology, etc.). We will apply a proven natural language processor called MedLEE to code the information and measure the accuracy of automated queries to detect and characterize errors. We will target several areas: explicit error reporting in the medical record, NYPORTS mandatory event reporting, clinical conflicts in record, and other sources of error information. We will use a systems approach to errors and cognitive analysis to uncover cues to improve error detection. We will incorporate the system into the hospital's current event surveillance program and assess the impact on error detection. We will adhere to strict privacy policies and security procedures. This project represents a unique opportunity to apply the most advanced medical language processing system to a large, comprehensive clinical repository to advance patient safety research.  n/a",Mining Complex Clinical Data for Patient Safety Research,6657426,R18HS011806,"['automated medical record system', ' clinical research', ' computer data analysis', ' diagnosis quality /standard', ' health service demonstration project', ' human data', ' iatrogenic disease', ' patient care', ' patient safety /medical error']",AHRQ,COLUMBIA UNIVERSITY HEALTH SCIENCES,R18,2003,369494,0.1594563375800116
"Mining Complex Clinical Data for Patient Safety Research Medical errors hurt patients, cost money, and undermine the health care system. The first step to reducing errors is detecting them, for what cannot be detected cannot be managed. A number of approaches have been applied to medical error detection, including mandatory event reporting, voluntary near-miss reporting, chart review, and automated surveillance using information systems. Automated surveillance promises large-scale detection, minimal labor, and, potentially, detection in real time to prevent or recover from errors. Unfortunately, large amounts of important clinical information lie locked in narrative reports, unavailable to automated decision support systems. A number of tools have emerged from medical informatics and computer science- natural language processing, visualization tools, and machine learning- as well as methods for understanding cognitive processes. We hypothesize that the electronic medical record contains information useful for detecting errors and that natural language processing and other tools will allow us to retrieve the information. We will assemble a team skilled in natural language processing, data mining, terminology, patient safety research, and health care. We will use a clinical repository with ten years of data on two million patients. It includes administrative, laboratory, and pharmacy coded information as well as a wide range of narrative reports including discharge summaries, operative reports, outpatient notes, autopsy reports, resident signout notes, nursing notes, and reports from numerous ancillary services (radiology, pathology, etc.). We will apply a proven natural language processor called MedLEE to code the information and measure the accuracy of automated queries to detect and characterize errors. We will target several areas: explicit error reporting in the medical record, NYPORTS mandatory event reporting, clinical conflicts in record, and other sources of error information. We will use a systems approach to errors and cognitive analysis to uncover cues to improve error detection. We will incorporate the system into the hospital's current event surveillance program and assess the impact on error detection. We will adhere to strict privacy policies and security procedures. This project represents a unique opportunity to apply the most advanced medical language processing system to a large, comprehensive clinical repository to advance patient safety research.  n/a",Mining Complex Clinical Data for Patient Safety Research,6528316,R18HS011806,"['automated medical record system', ' clinical research', ' computer data analysis', ' diagnosis quality /standard', ' health service demonstration project', ' human data', ' iatrogenic disease', ' patient care', ' patient safety /medical error']",AHRQ,COLUMBIA UNIVERSITY HEALTH SCIENCES,R18,2002,360015,0.1594563375800116
"Mining Complex Clinical Data for Patient Safety Research Medical errors hurt patients, cost money, and undermine the health care system. The first step to reducing errors is detecting them, for what cannot be detected cannot be managed. A number of approaches have been applied to medical error detection, including mandatory event reporting, voluntary near-miss reporting, chart review, and automated surveillance using information systems. Automated surveillance promises large-scale detection, minimal labor, and, potentially, detection in real time to prevent or recover from errors. Unfortunately, large amounts of important clinical information lie locked in narrative reports, unavailable to automated decision support systems. A number of tools have emerged from medical informatics and computer science- natural language processing, visualization tools, and machine learning- as well as methods for understanding cognitive processes. We hypothesize that the electronic medical record contains information useful for detecting errors and that natural language processing and other tools will allow us to retrieve the information. We will assemble a team skilled in natural language processing, data mining, terminology, patient safety research, and health care. We will use a clinical repository with ten years of data on two million patients. It includes administrative, laboratory, and pharmacy coded information as well as a wide range of narrative reports including discharge summaries, operative reports, outpatient notes, autopsy reports, resident signout notes, nursing notes, and reports from numerous ancillary services (radiology, pathology, etc.). We will apply a proven natural language processor called MedLEE to code the information and measure the accuracy of automated queries to detect and characterize errors. We will target several areas: explicit error reporting in the medical record, NYPORTS mandatory event reporting, clinical conflicts in record, and other sources of error information. We will use a systems approach to errors and cognitive analysis to uncover cues to improve error detection. We will incorporate the system into the hospital's current event surveillance program and assess the impact on error detection. We will adhere to strict privacy policies and security procedures. This project represents a unique opportunity to apply the most advanced medical language processing system to a large, comprehensive clinical repository to advance patient safety research.  n/a",Mining Complex Clinical Data for Patient Safety Research,6448720,R18HS011806,"['automated medical record system', ' clinical research', ' computer data analysis', ' diagnosis quality /standard', ' health service demonstration project', ' human data', ' iatrogenic disease', ' patient care', ' patient safety /medical error']",AHRQ,COLUMBIA UNIVERSITY HEALTH SCIENCES,R18,2001,356099,0.1594563375800116
"Intention-aware Recommender System for Improving Trauma Resuscitation Outcomes PROJECT SUMMARY Critically injured patients have a four-fold higher risk of death from medical errors than other hospitalized patients, with nearly half of preventable deaths related to errors during the initial resuscitation phase. Although protocols, simulation, and leadership training improve team performance in this setting, as many as 12 protocol deviations per resuscitation have been observed, even with experienced teams. Given adverse outcomes that can result from performance gaps, there is a critical need to establish novel approaches for applying real-time decision support in critical-care settings. The long-term goal is to implement decision support for trauma resuscitation and other fast-paced, high-risk critical care settings that improves performance, reduces errors, and prevents adverse outcomes. The overall objective for this renewal is to vertically advance what was achieved during the first funding period by designing, implementing and testing an intention-aware recommender system that (1) recognizes and tracks current goals using sensor data, the output from patient monitors, and data captured from digital devices, (2) derives recommendations that support adherence to goal- based protocols, and (3) displays these recommendations in real time on wall displays. The central hypothesis is that decision support aligning with intentions (“intended” or “current” goals) will enhance protocol compliance, leading to improved outcomes related to trauma resuscitation. The rationale for this renewal is that recommendations supporting protocol compliance that are aligned with team intentions are more likely to be adopted by being less distracting and associated with lower cognitive load. Guided by preliminary data, the central hypothesis will be tested by pursuing two specific aims: 1) design and implement an automated real- time approach for predicting and monitoring the assessment and treatment goals of trauma resuscitation; and 2) generate and display a recommended plan of activities that supports current goal pursuit during trauma resuscitation. For the first Aim, machine learning approaches will be applied for recognizing goals using data obtained from sensors and other digital data sources. Under the second Aim, a machine learning strategy will be implemented and tested that generates recommendations responsive to team intentions. The proposed research is innovative because it focuses on development of real-time methods that integrate goals as an input for making recommendations that meet the most current and relevant information needs. The proposed research is significant because it is expected to improve the care of severely injured and other critically ill patients by promoting timely and appropriate achievement of critical assessment and treatment goals in settings that remain at high-risk for medical errors. The results of this research continuum are expected to have an important positive impact on the outcome by addressing the mismatch between complex decision-making and human vulnerability to error that remain in critical care settings. PROJECT NARRATIVE The proposed project is relevant to public health because it focuses on the design and implementation of a novel real-time decision support system that will reduce errors associated with adverse patient outcomes by providing recommendations that support the current information needs of multidisciplinary trauma teams. The proposed project is relevant to the part of the NLM's mission related to development of biomedical communications systems, methods, and technologies, and information dissemination and utilization among health professionals.",Intention-aware Recommender System for Improving Trauma Resuscitation Outcomes,9739552,R01LM011834,"['Achievement', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Area', 'Awareness', 'Caring', 'Cessation of life', 'Communication', 'Complex', 'Computer Vision Systems', 'Critical Care', 'Critical Illness', 'Data', 'Data Sources', 'Decision Making', 'Decision Support Systems', 'Development', 'Devices', 'Failure', 'Funding', 'Goals', 'Health Professional', 'Hemorrhage', 'Human', 'Human Activities', 'Information Dissemination', 'Injury', 'Intention', 'Leadership', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Medical Errors', 'Methods', 'Mission', 'Monitor', 'Morbidity - disease rate', 'Multiple Trauma', 'Outcome', 'Output', 'Patient Monitoring', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Phase', 'Process', 'Progress Reports', 'Protocol Compliance', 'Protocols documentation', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Resuscitation', 'Risk', 'Safety', 'Stream', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Trauma', 'Variant', 'Work', 'adverse outcome', 'base', 'cognitive load', 'computerized', 'design', 'digital', 'disability', 'experience', 'high risk', 'improved', 'improved outcome', 'injured', 'innovation', 'instrument', 'learning strategy', 'member', 'mortality risk', 'multidisciplinary', 'novel', 'novel strategies', 'preference', 'prevent', 'preventable death', 'radio frequency', 'sensor', 'simulation', 'success']",NLM,CHILDREN'S RESEARCH INSTITUTE,R01,2019,709825,0.1278226784222311
"Intention-aware Recommender System for Improving Trauma Resuscitation Outcomes PROJECT SUMMARY Critically injured patients have a four-fold higher risk of death from medical errors than other hospitalized patients, with nearly half of preventable deaths related to errors during the initial resuscitation phase. Although protocols, simulation, and leadership training improve team performance in this setting, as many as 12 protocol deviations per resuscitation have been observed, even with experienced teams. Given adverse outcomes that can result from performance gaps, there is a critical need to establish novel approaches for applying real-time decision support in critical-care settings. The long-term goal is to implement decision support for trauma resuscitation and other fast-paced, high-risk critical care settings that improves performance, reduces errors, and prevents adverse outcomes. The overall objective for this renewal is to vertically advance what was achieved during the first funding period by designing, implementing and testing an intention-aware recommender system that (1) recognizes and tracks current goals using sensor data, the output from patient monitors, and data captured from digital devices, (2) derives recommendations that support adherence to goal- based protocols, and (3) displays these recommendations in real time on wall displays. The central hypothesis is that decision support aligning with intentions (“intended” or “current” goals) will enhance protocol compliance, leading to improved outcomes related to trauma resuscitation. The rationale for this renewal is that recommendations supporting protocol compliance that are aligned with team intentions are more likely to be adopted by being less distracting and associated with lower cognitive load. Guided by preliminary data, the central hypothesis will be tested by pursuing two specific aims: 1) design and implement an automated real- time approach for predicting and monitoring the assessment and treatment goals of trauma resuscitation; and 2) generate and display a recommended plan of activities that supports current goal pursuit during trauma resuscitation. For the first Aim, machine learning approaches will be applied for recognizing goals using data obtained from sensors and other digital data sources. Under the second Aim, a machine learning strategy will be implemented and tested that generates recommendations responsive to team intentions. The proposed research is innovative because it focuses on development of real-time methods that integrate goals as an input for making recommendations that meet the most current and relevant information needs. The proposed research is significant because it is expected to improve the care of severely injured and other critically ill patients by promoting timely and appropriate achievement of critical assessment and treatment goals in settings that remain at high-risk for medical errors. The results of this research continuum are expected to have an important positive impact on the outcome by addressing the mismatch between complex decision-making and human vulnerability to error that remain in critical care settings. PROJECT NARRATIVE The proposed project is relevant to public health because it focuses on the design and implementation of a novel real-time decision support system that will reduce errors associated with adverse patient outcomes by providing recommendations that support the current information needs of multidisciplinary trauma teams. The proposed project is relevant to the part of the NLM's mission related to development of biomedical communications systems, methods, and technologies, and information dissemination and utilization among health professionals.",Intention-aware Recommender System for Improving Trauma Resuscitation Outcomes,9932473,R01LM011834,"['Achievement', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Area', 'Awareness', 'Caring', 'Cessation of life', 'Communication', 'Complex', 'Computer Vision Systems', 'Critical Care', 'Critical Illness', 'Data', 'Data Sources', 'Decision Making', 'Decision Support Systems', 'Development', 'Devices', 'Failure', 'Funding', 'Goals', 'Health Professional', 'Hemorrhage', 'Human', 'Human Activities', 'Information Dissemination', 'Injury', 'Intention', 'Leadership', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Medical Errors', 'Methods', 'Mission', 'Monitor', 'Morbidity - disease rate', 'Multiple Trauma', 'Outcome', 'Output', 'Patient Monitoring', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Phase', 'Process', 'Progress Reports', 'Protocol Compliance', 'Protocols documentation', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Resuscitation', 'Risk', 'Safety', 'Stream', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Trauma', 'Variant', 'Work', 'adverse outcome', 'base', 'cognitive load', 'computerized', 'design', 'digital', 'disability', 'experience', 'high risk', 'improved', 'improved outcome', 'innovation', 'instrument', 'learning strategy', 'member', 'mortality risk', 'multidisciplinary', 'novel', 'novel strategies', 'preference', 'prevent', 'preventable death', 'radio frequency', 'sensor', 'severe injury', 'simulation', 'success']",NLM,CHILDREN'S RESEARCH INSTITUTE,R01,2020,658135,0.1278226784222311
"Advanced End-to-End Relation Extraction with Deep Neural Networks ABSTRACT Relations linking various biomedical entities constitute a crucial resource that enables biomedical data science applications and knowledge discovery. Relational information spans the translational science spectrum going from biology (e.g., protein–protein interactions) to translational bioinformatics (e.g., gene–disease associations), and eventually to clinical care (e.g., drug–drug interactions). Scientists report newly discovered relations in nat- ural language through peer-reviewed literature and physicians may communicate them in clinical notes. More recently, patients are also reporting side-effects and adverse events on social media. With exponential growth in textual data, advances in biomedical natural language processing (BioNLP) methods are gaining prominence for biomedical relation extraction (BRE) from text. Most current efforts in BRE follow a pipeline approach containing named entity recognition (NER), entity normalization (EN), and relation classiﬁcation (RC) as subtasks. They typically suffer from error snowballing — errors in a component of the pipeline leading to more downstream errors — resulting in lower performance of the overall BRE system. This situation has lead to evaluation of different BRE substaks conducted in isolation. In this proposal we make a strong case for strictly end-to-end evaluations where relations are to be produced from raw text. We propose novel deep neural network architectures that model BRE in an end-to-end fashion and directly identify relations and corresponding entity spans in a single pass. We also extend our architectures to n-ary and cross-sentence settings where more than two entities may need to be linked even as the relation is expressed across multiple sentences. We also propose to create two new gold standard BRE datasets, one for drug–disease treatment relations and another ﬁrst of a kind dataset for combination drug therapies. Our main hypothesis is that our end-to-end extraction models will yield supe- rior performance when compared with traditional pipelines. We test this through (1). intrinsic evaluations based on standard performance measures with several gold standard datasets and (2). extrinsic application oriented assessments of relations extracted with use-cases in information retrieval, question answering, and knowledge base completion. All software and data developed as part of this project will be made available for public use and we hope this will foster rigorous end-to-end benchmarking of BRE systems. NARRATIVE Relations connecting biomedical entities are at the heart of biomedical research given they encapsulate mech- anisms of disease etiology, progression, and treatment. As most such relations are ﬁrst disclosed in textual narratives (scientiﬁc literature or clinical notes), methods to extract and represent them in a structured format are essential to facilitate applications such as hypotheses generation, question answering, and information retrieval. The high level objective of this project is to develop and evaluate novel end-to-end supervised machine learning methods for biomedical relation extraction using latest advances in deep neural networks.",Advanced End-to-End Relation Extraction with Deep Neural Networks,10052028,R01LM013240,"['Adverse event', 'Architecture', 'Area', 'Benchmarking', 'Bioinformatics', 'Biology', 'Biomedical Research', 'Classification', 'Clinical', 'Code', 'Collaborations', 'Combination Drug Therapy', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Set', 'Dependence', 'Disease', 'Distant', 'Drug Interactions', 'Encapsulated', 'Etiology', 'Evaluation', 'Fostering', 'Funding', 'Future', 'Generations', 'Genes', 'Gold', 'Growth', 'Hand', 'Heart', 'Information Retrieval', 'Information Sciences', 'Intramural Research', 'Joints', 'Knowledge Discovery', 'Label', 'Language', 'Lead', 'Link', 'Literature', 'Manuals', 'Maps', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Patients', 'Peer Review', 'Performance', 'Periodicity', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Psychological Transfer', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Review Literature', 'Scientist', 'Semantics', 'Software Tools', 'Source', 'Standardization', 'Structure', 'Supervision', 'System', 'Terminology', 'Testing', 'Text', 'Training', 'Translational Research', 'Trees', 'base', 'biomedical data science', 'clinical care', 'deep neural network', 'improved', 'insight', 'interest', 'knowledge base', 'machine learning method', 'natural language', 'neural network', 'neural network architecture', 'new therapeutic target', 'novel', 'off-label use', 'protein protein interaction', 'relating to nervous system', 'side effect', 'social media', 'supervised learning', 'syntax']",NLM,UNIVERSITY OF KENTUCKY,R01,2020,358691,0.0033290798823571244
"Computer-Aided Interpretation of Oculometric Data    DESCRIPTION (provided by applicant): The primary goal of the proposed research program is to develop computer software tools with embedded artificial intelligence (AI) that can perform instantaneous, automated analysis and clinical interpretation of wavefront error measurements of the human eye and cornea. Secondary goals are to improve the overall design of oculometric data visualization tools, provide information that will help to establish clinical and scientific standards for ocular measurements and procedures, and improve our understanding of the fundamental relationship between optical performance and visual performance. We hypothesize that a) AI-based algorithms will detect complex patterns of wavefront errors; b) these patterns are specific to and significantly correlated with certain diseases and disorders; and c) AI-based interpretation of complex data will be superior to that performed by expert humans, who are the gold standard for interpreting clinical data. Specifically, we will (1) develop, train, and test AI-based algorithms (Bayesian and neural networks) to interpret the significance of complex wavefront error data obtained retrospectively from examination records of patients with various ocular diseases, disorders, or surgical interventions, as well as normal eyes; (2) simulate wavefront error data using computer models based on statistical distributions of actual ocular aberrations from patient population samples for the purpose of investigating the importance of individual higher order aberrations to retinal image formation and potential visual performance, as well as to generate new data that will enhance the overall AI training and testing process, and (3) establish standard methods to acquire and analyze wavefront error data. AI-based tools will assist vision scientists to efficiently develop study databases and analyze aberration data. Clinicians will diagnose patients faster, more accurately, and with a greater degree of confidence. For patients, refractive surgery outcomes will be more predictable, and they will benefit from earlier detection of diseases such as cataracts and corneal ectasias.         n/a",Computer-Aided Interpretation of Oculometric Data,7287568,R01EY014162,"['artificial intelligence', 'bioimaging /biomedical imaging', 'computer assisted diagnosis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'diagnosis design /evaluation', 'eye disorder diagnosis', 'eye refractometry', 'human data', 'image processing', 'ophthalmoscopy']",NEI,LOUISIANA STATE UNIV HSC NEW ORLEANS,R01,2006,38558,0.11698258634401634
"Computer-Aided Interpretation of Oculometric Data    DESCRIPTION (provided by applicant): The primary goal of the proposed research program is to develop computer software tools with embedded artificial intelligence (AI) that can perform instantaneous, automated analysis and clinical interpretation of wavefront error measurements of the human eye and cornea. Secondary goals are to improve the overall design of oculometric data visualization tools, provide information that will help to establish clinical and scientific standards for ocular measurements and procedures, and improve our understanding of the fundamental relationship between optical performance and visual performance. We hypothesize that a) AI-based algorithms will detect complex patterns of wavefront errors; b) these patterns are specific to and significantly correlated with certain diseases and disorders; and c) AI-based interpretation of complex data will be superior to that performed by expert humans, who are the gold standard for interpreting clinical data. Specifically, we will (1) develop, train, and test AI-based algorithms (Bayesian and neural networks) to interpret the significance of complex wavefront error data obtained retrospectively from examination records of patients with various ocular diseases, disorders, or surgical interventions, as well as normal eyes; (2) simulate wavefront error data using computer models based on statistical distributions of actual ocular aberrations from patient population samples for the purpose of investigating the importance of individual higher order aberrations to retinal image formation and potential visual performance, as well as to generate new data that will enhance the overall AI training and testing process, and (3) establish standard methods to acquire and analyze wavefront error data. AI-based tools will assist vision scientists to efficiently develop study databases and analyze aberration data. Clinicians will diagnose patients faster, more accurately, and with a greater degree of confidence. For patients, refractive surgery outcomes will be more predictable, and they will benefit from earlier detection of diseases such as cataracts and corneal ectasias.         n/a",Computer-Aided Interpretation of Oculometric Data,6910621,R01EY014162,"['artificial intelligence', 'bioimaging /biomedical imaging', 'computer assisted diagnosis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'diagnosis design /evaluation', 'eye disorder diagnosis', 'eye refractometry', 'human data', 'image processing', 'ophthalmoscopy']",NEI,LOUISIANA STATE UNIV HSC NEW ORLEANS,R01,2005,245768,0.11698258634401634
"Computer-Aided Interpretation of Oculometric Data    DESCRIPTION (provided by applicant): The primary goal of the proposed research program is to develop computer software tools with embedded artificial intelligence (AI) that can perform instantaneous, automated analysis and clinical interpretation of wavefront error measurements of the human eye and cornea. Secondary goals are to improve the overall design of oculometric data visualization tools, provide information that will help to establish clinical and scientific standards for ocular measurements and procedures, and improve our understanding of the fundamental relationship between optical performance and visual performance. We hypothesize that a) AI-based algorithms will detect complex patterns of wavefront errors; b) these patterns are specific to and significantly correlated with certain diseases and disorders; and c) AI-based interpretation of complex data will be superior to that performed by expert humans, who are the gold standard for interpreting clinical data. Specifically, we will (1) develop, train, and test AI-based algorithms (Bayesian and neural networks) to interpret the significance of complex wavefront error data obtained retrospectively from examination records of patients with various ocular diseases, disorders, or surgical interventions, as well as normal eyes; (2) simulate wavefront error data using computer models based on statistical distributions of actual ocular aberrations from patient population samples for the purpose of investigating the importance of individual higher order aberrations to retinal image formation and potential visual performance, as well as to generate new data that will enhance the overall AI training and testing process, and (3) establish standard methods to acquire and analyze wavefront error data. AI-based tools will assist vision scientists to efficiently develop study databases and analyze aberration data. Clinicians will diagnose patients faster, more accurately, and with a greater degree of confidence. For patients, refractive surgery outcomes will be more predictable, and they will benefit from earlier detection of diseases such as cataracts and corneal ectasias.         n/a",Computer-Aided Interpretation of Oculometric Data,6774688,R01EY014162,"['artificial intelligence', 'bioimaging /biomedical imaging', 'computer assisted diagnosis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'diagnosis design /evaluation', 'eye disorder diagnosis', 'eye refractometry', 'human data', 'image processing', 'ophthalmoscopy']",NEI,LOUISIANA STATE UNIV HSC NEW ORLEANS,R01,2004,242058,0.11698258634401634
"Computer-Aided Interpretation of Oculometric Data    DESCRIPTION (provided by applicant): The primary goal of the proposed research program is to develop computer software tools with embedded artificial intelligence (AI) that can perform instantaneous, automated analysis and clinical interpretation of wavefront error measurements of the human eye and cornea. Secondary goals are to improve the overall design of oculometric data visualization tools, provide information that will help to establish clinical and scientific standards for ocular measurements and procedures, and improve our understanding of the fundamental relationship between optical performance and visual performance. We hypothesize that a) AI-based algorithms will detect complex patterns of wavefront errors; b) these patterns are specific to and significantly correlated with certain diseases and disorders; and c) AI-based interpretation of complex data will be superior to that performed by expert humans, who are the gold standard for interpreting clinical data. Specifically, we will (1) develop, train, and test AI-based algorithms (Bayesian and neural networks) to interpret the significance of complex wavefront error data obtained retrospectively from examination records of patients with various ocular diseases, disorders, or surgical interventions, as well as normal eyes; (2) simulate wavefront error data using computer models based on statistical distributions of actual ocular aberrations from patient population samples for the purpose of investigating the importance of individual higher order aberrations to retinal image formation and potential visual performance, as well as to generate new data that will enhance the overall AI training and testing process, and (3) establish standard methods to acquire and analyze wavefront error data. AI-based tools will assist vision scientists to efficiently develop study databases and analyze aberration data. Clinicians will diagnose patients faster, more accurately, and with a greater degree of confidence. For patients, refractive surgery outcomes will be more predictable, and they will benefit from earlier detection of diseases such as cataracts and corneal ectasias.         n/a",Computer-Aided Interpretation of Oculometric Data,6617187,R01EY014162,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer assisted diagnosis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' diagnosis design /evaluation', ' eye disorder diagnosis', ' eye refractometry', ' human data', ' image processing', ' ophthalmoscopy']",NEI,LOUISIANA STATE UNIV HSC NEW ORLEANS,R01,2003,241570,0.11698258634401634
"Reducing Drug Name Confusion With Better Search Software    DESCRIPTION (provided by applicant): Confusions between drug names that look and sound alike (e.g., Keppra(r) and Kaletra(r), Indocid(r) and Endocet(r)) continue to occur frequently, and each confusion poses a threat to patient safety.2-5 Our long term objective is to design, build, test and continuously improve tools that minimize the harm caused by drug name confusion errors. For a patient to be harmed, an error must occur and it must go undetected until it reaches the patient. Harm is minimized either by preventing the error from occurring in the first place or by rapidly detecting the error so its adverse effects can be mitigated. Both prevention and mitigation efforts have been hindered by the lack of valid, reliable and efficient methods for assessing name confusion error rates. The gold standard for measuring medication error rates is direct observation of the prescribing-dispensing- administering process. This method is valid and reliable but is too time consuming and expensive to be widely used. As a result, many error reduction interventions have been designed, but few have been tested, and their effectiveness is, for the most part, unknown. Similarly, efforts to mitigate the effects of wrong drug errors are virtually non-existent because there has been no accurate and efficient way to detect such errors after they occur. The key to improving both prevention and mitigation of harm is the development of scalable, efficient, valid and reliable methods for detecting these drug name confusion errors. Our short-term goal is to develop and validate an algorithm for detecting drug name confusion errors by analyzing suspicious patterns in real-world prescription drug databases (in our case, integrated electronic medical records from the US Veterans Health Administration). We plan to test the following three hypotheses: 1. Computerized measures of drug name confusability can be used to identify wrong-drug errors in real-world prescription drug databases. 2. The number of errors detected will increase as the predicted probability of confusion increases. 3. The classification performance of the error detection algorithm (i.e., its accuracy, sensitivity and specificity) can be enhanced by applying machine learning techniques and by incorporating additional information from the electronic medical record (e.g., time between refills, diagnosis, lab values, demographics, etc.) To test these hypotheses, we propose studies with the following specific aims: 1. To design and implement an algorithm for the detection of suspicious patterns in prescription drug databases. 2. To test and validate this algorithm using real-world prescription data from the US Veterans Health Administration. 3. To use machine learning techniques to optimize and further validate the performance of the error detection algorithm, incorporating additional information from the electronic medical record. Health care professionals often confuse drug names that look and sound alike. Wrong drug errors occur in hospitals and in community pharmacies and can cause serious harm to patients. Our project seeks to improve patient safety by developing and testing new techniques for detecting wrong drug errors in integrated electronic medical records.             n/a",Reducing Drug Name Confusion With Better Search Software,7501496,R44RR021232,"['Adverse effects', 'Algorithms', 'Classification', 'Community Pharmacy', 'Computer software', 'Computerized Medical Record', 'Confusion', 'Data', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Drug Prescriptions', 'Effectiveness', 'Goals', 'Gold', 'Health', 'Health Professional', 'Hospitals', 'Indocid', 'Intervention', 'Keppra', 'Lopinavir/Ritonavir', 'Machine Learning', 'Measures', 'Medication Errors', 'Methods', 'Names', 'Numbers', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Prevention', 'Probability', 'Process', 'Rate', 'Sensitivity and Specificity', 'Standards of Weights and Measures', 'Techniques', 'Testing', 'Time', 'Veterans', 'computerized', 'demographics', 'design', 'health administration', 'improved', 'patient safety', 'prescription document', 'prescription procedure', 'prevent', 'sound', 'tool']",NCRR,PHARM I.R.,R44,2008,384760,0.2411643384044344
"Reducing Drug Name Confusion With Better Search Software    DESCRIPTION (provided by applicant): Confusions between drug names that look and sound alike (e.g., Keppra(r) and Kaletra(r), Indocid(r) and Endocet(r)) continue to occur frequently, and each confusion poses a threat to patient safety.2-5 Our long term objective is to design, build, test and continuously improve tools that minimize the harm caused by drug name confusion errors. For a patient to be harmed, an error must occur and it must go undetected until it reaches the patient. Harm is minimized either by preventing the error from occurring in the first place or by rapidly detecting the error so its adverse effects can be mitigated. Both prevention and mitigation efforts have been hindered by the lack of valid, reliable and efficient methods for assessing name confusion error rates. The gold standard for measuring medication error rates is direct observation of the prescribing-dispensing- administering process. This method is valid and reliable but is too time consuming and expensive to be widely used. As a result, many error reduction interventions have been designed, but few have been tested, and their effectiveness is, for the most part, unknown. Similarly, efforts to mitigate the effects of wrong drug errors are virtually non-existent because there has been no accurate and efficient way to detect such errors after they occur. The key to improving both prevention and mitigation of harm is the development of scalable, efficient, valid and reliable methods for detecting these drug name confusion errors. Our short-term goal is to develop and validate an algorithm for detecting drug name confusion errors by analyzing suspicious patterns in real-world prescription drug databases (in our case, integrated electronic medical records from the US Veterans Health Administration). We plan to test the following three hypotheses: 1. Computerized measures of drug name confusability can be used to identify wrong-drug errors in real-world prescription drug databases. 2. The number of errors detected will increase as the predicted probability of confusion increases. 3. The classification performance of the error detection algorithm (i.e., its accuracy, sensitivity and specificity) can be enhanced by applying machine learning techniques and by incorporating additional information from the electronic medical record (e.g., time between refills, diagnosis, lab values, demographics, etc.) To test these hypotheses, we propose studies with the following specific aims: 1. To design and implement an algorithm for the detection of suspicious patterns in prescription drug databases. 2. To test and validate this algorithm using real-world prescription data from the US Veterans Health Administration. 3. To use machine learning techniques to optimize and further validate the performance of the error detection algorithm, incorporating additional information from the electronic medical record. Health care professionals often confuse drug names that look and sound alike. Wrong drug errors occur in hospitals and in community pharmacies and can cause serious harm to patients. Our project seeks to improve patient safety by developing and testing new techniques for detecting wrong drug errors in integrated electronic medical records.             n/a",Reducing Drug Name Confusion With Better Search Software,7273372,R44RR021232,"['Adverse effects', 'Algorithms', 'Classification', 'Community Pharmacy', 'Computer software', 'Computerized Medical Record', 'Confusion', 'Data', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Drug Prescriptions', 'Effectiveness', 'Goals', 'Gold', 'Health', 'Health Professional', 'Hospitals', 'Indocid', 'Intervention', 'Keppra', 'Lopinavir/Ritonavir', 'Machine Learning', 'Measures', 'Medication Errors', 'Methods', 'Names', 'Numbers', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Prevention', 'Probability', 'Process', 'Rate', 'Sensitivity and Specificity', 'Standards of Weights and Measures', 'Techniques', 'Testing', 'Time', 'Veterans', 'computerized', 'demographics', 'design', 'health administration', 'improved', 'patient safety', 'prescription document', 'prescription procedure', 'prevent', 'sound', 'tool']",NCRR,PHARM I.R.,R44,2007,388793,0.2411643384044344
"Learning from patient safety events: A case base tool kit DESCRIPTION (provided by applicant): Medical error is one of the leading causes of death in the US. The study and reduction of medical errors have become a major concern in healthcare today. It is believed that medical error reporting systems could be a good resource to share and to learn from errors if medical error data are collected in a properly structured format and are useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Effectively gathering information from previous lessons and timely informing the subsequent action are the two major goals for the design, development and utilization of such a system. The Common Formats (CFs) suggested by AHRQ tend to unify the future reporting format, which holds promise in improving data consistency and reducing unsafe conditions through lessons learned. However, effective gathering medical incident data does not merely rely on a unified structure. To be able to learn from previous lessons, it heavily depends upon the quality reports and learning features offered by systems. Medical incident data are always the key components and invaluable assets in patient safety research. The long term goal of the project is to understand the occurrence and causes of medical incidents in real practice and to develop interventions based on collection of incident reports to minimize the recurrence of similar incidents that have been reported. The objective of this application is to improve the utilization f voluntary reporting systems that each healthcare institution has been put in use by developing a learning toolkit that can systematically collect and analyze incident reports, automatically link historical reports with WebM&M, the highest quality of voluntary reports and expert reviews in patient safety. As moving toward CFs, the researchers propose a user-centered, learning-supportive, and ontological approach that will help reporters generate complete and accurate reports through user-friendly guidance and offer timely comments and relevant peer reviews through educational tools during and after incident reporting. The researchers employ a case-based reasoning and natural language processing techniques to demonstrate the feasibility and effectiveness of the knowledge-based toolkit which helps reporters improve the communication about patient safety through clear working definitions and advance training that builds knowledge about the safety culture and then provides continuing education through the system. The project holds promise in revolutionizing the design of voluntary medical incident reporting systems from an incident data repository to an advanced resource promoting complete and accurate incident reporting and learning toward a just and learning culture. PUBLIC HEALTH RELEVANCE: Timely reporting and effective learning from medical incidents is considered an effective way in developing strategies for reducing medical errors. Utilizing an innovative a user-centered, learning-supportive, and ontological approach combining with case-based reasoning and natural language processing techniques, we propose to develop a knowledgebase and learning toolkit that can systematically collect and analyze incident reports, linking historical reports with WebM&M, the highest quality of voluntary reports and expert reviews on patient safety. We envision that the innovative approach will facilitate timely, quality reporting and learning from the incidents and ultimately cultivating a just and learning culture of patient safety.",Learning from patient safety events: A case base tool kit,9567932,R01HS022895,[' '],AHRQ,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2018,249135,0.07765831482366664
"Learning from patient safety events: A case base tool kit DESCRIPTION (provided by applicant): Medical error is one of the leading causes of death in the US. The study and reduction of medical errors have become a major concern in healthcare today. It is believed that medical error reporting systems could be a good resource to share and to learn from errors if medical error data are collected in a properly structured format and are useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Effectively gathering information from previous lessons and timely informing the subsequent action are the two major goals for the design, development and utilization of such a system. The Common Formats (CFs) suggested by AHRQ tend to unify the future reporting format, which holds promise in improving data consistency and reducing unsafe conditions through lessons learned. However, effective gathering medical incident data does not merely rely on a unified structure. To be able to learn from previous lessons, it heavily depends upon the quality reports and learning features offered by systems. Medical incident data are always the key components and invaluable assets in patient safety research. The long term goal of the project is to understand the occurrence and causes of medical incidents in real practice and to develop interventions based on collection of incident reports to minimize the recurrence of similar incidents that have been reported. The objective of this application is to improve the utilization f voluntary reporting systems that each healthcare institution has been put in use by developing a learning toolkit that can systematically collect and analyze incident reports, automatically link historical reports with WebM&M, the highest quality of voluntary reports and expert reviews in patient safety. As moving toward CFs, the researchers propose a user-centered, learning-supportive, and ontological approach that will help reporters generate complete and accurate reports through user-friendly guidance and offer timely comments and relevant peer reviews through educational tools during and after incident reporting. The researchers employ a case-based reasoning and natural language processing techniques to demonstrate the feasibility and effectiveness of the knowledge-based toolkit which helps reporters improve the communication about patient safety through clear working definitions and advance training that builds knowledge about the safety culture and then provides continuing education through the system. The project holds promise in revolutionizing the design of voluntary medical incident reporting systems from an incident data repository to an advanced resource promoting complete and accurate incident reporting and learning toward a just and learning culture. PUBLIC HEALTH RELEVANCE: Timely reporting and effective learning from medical incidents is considered an effective way in developing strategies for reducing medical errors. Utilizing an innovative a user-centered, learning-supportive, and ontological approach combining with case-based reasoning and natural language processing techniques, we propose to develop a knowledgebase and learning toolkit that can systematically collect and analyze incident reports, linking historical reports with WebM&M, the highest quality of voluntary reports and expert reviews on patient safety. We envision that the innovative approach will facilitate timely, quality reporting and learning from the incidents and ultimately cultivating a just and learning culture of patient safety.",Learning from patient safety events: A case base tool kit,9352770,R01HS022895,[' '],AHRQ,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2017,249135,0.07765831482366664
"Learning from patient safety events: A case base tool kit DESCRIPTION (provided by applicant): Medical error is one of the leading causes of death in the US. The study and reduction of medical errors have become a major concern in healthcare today. It is believed that medical error reporting systems could be a good resource to share and to learn from errors if medical error data are collected in a properly structured format and are useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Effectively gathering information from previous lessons and timely informing the subsequent action are the two major goals for the design, development and utilization of such a system. The Common Formats (CFs) suggested by AHRQ tend to unify the future reporting format, which holds promise in improving data consistency and reducing unsafe conditions through lessons learned. However, effective gathering medical incident data does not merely rely on a unified structure. To be able to learn from previous lessons, it heavily depends upon the quality reports and learning features offered by systems. Medical incident data are always the key components and invaluable assets in patient safety research. The long term goal of the project is to understand the occurrence and causes of medical incidents in real practice and to develop interventions based on collection of incident reports to minimize the recurrence of similar incidents that have been reported. The objective of this application is to improve the utilization f voluntary reporting systems that each healthcare institution has been put in use by developing a learning toolkit that can systematically collect and analyze incident reports, automatically link historical reports with WebM&M, the highest quality of voluntary reports and expert reviews in patient safety. As moving toward CFs, the researchers propose a user-centered, learning-supportive, and ontological approach that will help reporters generate complete and accurate reports through user-friendly guidance and offer timely comments and relevant peer reviews through educational tools during and after incident reporting. The researchers employ a case-based reasoning and natural language processing techniques to demonstrate the feasibility and effectiveness of the knowledge-based toolkit which helps reporters improve the communication about patient safety through clear working definitions and advance training that builds knowledge about the safety culture and then provides continuing education through the system. The project holds promise in revolutionizing the design of voluntary medical incident reporting systems from an incident data repository to an advanced resource promoting complete and accurate incident reporting and learning toward a just and learning culture. PUBLIC HEALTH RELEVANCE: Timely reporting and effective learning from medical incidents is considered an effective way in developing strategies for reducing medical errors. Utilizing an innovative a user-centered, learning-supportive, and ontological approach combining with case-based reasoning and natural language processing techniques, we propose to develop a knowledgebase and learning toolkit that can systematically collect and analyze incident reports, linking historical reports with WebM&M, the highest quality of voluntary reports and expert reviews on patient safety. We envision that the innovative approach will facilitate timely, quality reporting and learning from the incidents and ultimately cultivating a just and learning culture of patient safety.",Learning from patient safety events: A case base tool kit,9144757,R01HS022895,[' '],AHRQ,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2016,249135,0.07765831482366664
"Learning from patient safety events: A case base tool kit DESCRIPTION (provided by applicant): Medical error is one of the leading causes of death in the US. The study and reduction of medical errors have become a major concern in healthcare today. It is believed that medical error reporting systems could be a good resource to share and to learn from errors if medical error data are collected in a properly structured format and are useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Effectively gathering information from previous lessons and timely informing the subsequent action are the two major goals for the design, development and utilization of such a system. The Common Formats (CFs) suggested by AHRQ tend to unify the future reporting format, which holds promise in improving data consistency and reducing unsafe conditions through lessons learned. However, effective gathering medical incident data does not merely rely on a unified structure. To be able to learn from previous lessons, it heavily depends upon the quality reports and learning features offered by systems. Medical incident data are always the key components and invaluable assets in patient safety research. The long term goal of the project is to understand the occurrence and causes of medical incidents in real practice and to develop interventions based on collection of incident reports to minimize the recurrence of similar incidents that have been reported. The objective of this application is to improve the utilization f voluntary reporting systems that each healthcare institution has been put in use by developing a learning toolkit that can systematically collect and analyze incident reports, automatically link historical reports with WebM&M, the highest quality of voluntary reports and expert reviews in patient safety. As moving toward CFs, the researchers propose a user-centered, learning-supportive, and ontological approach that will help reporters generate complete and accurate reports through user-friendly guidance and offer timely comments and relevant peer reviews through educational tools during and after incident reporting. The researchers employ a case-based reasoning and natural language processing techniques to demonstrate the feasibility and effectiveness of the knowledge-based toolkit which helps reporters improve the communication about patient safety through clear working definitions and advance training that builds knowledge about the safety culture and then provides continuing education through the system. The project holds promise in revolutionizing the design of voluntary medical incident reporting systems from an incident data repository to an advanced resource promoting complete and accurate incident reporting and learning toward a just and learning culture. PUBLIC HEALTH RELEVANCE: Timely reporting and effective learning from medical incidents is considered an effective way in developing strategies for reducing medical errors. Utilizing an innovative a user-centered, learning-supportive, and ontological approach combining with case-based reasoning and natural language processing techniques, we propose to develop a knowledgebase and learning toolkit that can systematically collect and analyze incident reports, linking historical reports with WebM&M, the highest quality of voluntary reports and expert reviews on patient safety. We envision that the innovative approach will facilitate timely, quality reporting and learning from the incidents and ultimately cultivating a just and learning culture of patient safety.",Learning from patient safety events: A case base tool kit,8928596,R01HS022895,[' '],AHRQ,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2015,249655,0.07765831482366664
"Learning from patient safety events: A case base tool kit     DESCRIPTION (provided by applicant): Medical error is one of the leading causes of death in the US. The study and reduction of medical errors have become a major concern in healthcare today. It is believed that medical error reporting systems could be a good resource to share and to learn from errors if medical error data are collected in a properly structured format and are useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Effectively gathering information from previous lessons and timely informing the subsequent action are the two major goals for the design, development and utilization of such a system. The Common Formats (CFs) suggested by AHRQ tend to unify the future reporting format, which holds promise in improving data consistency and reducing unsafe conditions through lessons learned. However, effective gathering medical incident data does not merely rely on a unified structure. To be able to learn from previous lessons, it heavily depends upon the quality reports and learning features offered by systems. Medical incident data are always the key components and invaluable assets in patient safety research. The long term goal of the project is to understand the occurrence and causes of medical incidents in real practice and to develop interventions based on collection of incident reports to minimize the recurrence of similar incidents that have been reported. The objective of this application is to improve the utilization f voluntary reporting systems that each healthcare institution has been put in use by developing a learning toolkit that can systematically collect and analyze incident reports, automatically link historical reports with WebM&M, the highest quality of voluntary reports and expert reviews in patient safety. As moving toward CFs, the researchers propose a user-centered, learning-supportive, and ontological approach that will help reporters generate complete and accurate reports through user-friendly guidance and offer timely comments and relevant peer reviews through educational tools during and after incident reporting. The researchers employ a case-based reasoning and natural language processing techniques to demonstrate the feasibility and effectiveness of the knowledge-based toolkit which helps reporters improve the communication about patient safety through clear working definitions and advance training that builds knowledge about the safety culture and then provides continuing education through the system. The project holds promise in revolutionizing the design of voluntary medical incident reporting systems from an incident data repository to an advanced resource promoting complete and accurate incident reporting and learning toward a just and learning culture.         PUBLIC HEALTH RELEVANCE: Timely reporting and effective learning from medical incidents is considered an effective way in developing strategies for reducing medical errors. Utilizing an innovative a user-centered, learning-supportive, and ontological approach combining with case-based reasoning and natural language processing techniques, we propose to develop a knowledgebase and learning toolkit that can systematically collect and analyze incident reports, linking historical reports with WebM&M, the highest quality of voluntary reports and expert reviews on patient safety. We envision that the innovative approach will facilitate timely, quality reporting and learning from the incidents and ultimately cultivating a just and learning culture of patient safety.            ",Learning from patient safety events: A case base tool kit,8818528,R01HS022895,[' '],AHRQ,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2014,249655,0.07765831482366664
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,8139155,R44RR024094,"['AIDS/HIV problem', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cell physiology', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2011,449663,0.04440313472122386
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,7999420,R44RR024094,"['Acquired Immunodeficiency Syndrome', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'HIV', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2010,449663,0.04440313472122386
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005). PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9352296,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2017,249995,0.1757280223881217
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005). PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9146893,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2016,249994,0.1757280223881217
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005).         PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.            ",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9004939,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2015,250000,0.1757280223881217
"Development of a low-cost, portable autorefractor for the measuring of refractive errors in low-resource settings ﻿    DESCRIPTION (provided by applicant): ""Development of a low-cost, extended-range, autorefractor""    This  project  proposal  seeks  to  develop  technologies  that  will  lower  the cost  of  and  increase  the  accessibility  of  refractive  eye  care,  especially  in  low-resoure  settings.  We propose to advance our previously developed low-cost, handheld device capable of automatically measuring the optical properties of the eye based on the technique of wavefront aberrometry.  The  proposed  work  will  specifically  focus  on  the development of optical and software systems will extend the device's measurement  range  to  work  on  a  larger  range  of  refractive  errors.  First,  optical  systems  with  no  moving  parts  will  be  developed  which  enable  the  device  to  work  on  subjects  with  severe  myopia  or  hyperopia  (<-­-6  diopters or  >6  diopters  of  refractive  error).  Second,  algorithms  that  make  the  device  easy  and  intuitive  to  use  will  be  implemented.  Third,  a  handheld  prototype  incorporating  these  improvements  will  be  constructed  and  validated  in  model  eye  systems.  The  output  of  this  project  will  be  a  functional,  extended-range  prototype  that  will  facilitate  the  fild-testing  and  commercialization  of  a  low-cost,  easy-to-use  device  to  dispense  eyeglass  prescriptions. PUBLIC HEALTH RELEVANCE: ""Development of a low-cost, extended-range, autorefractor""    This  project  proposal  seeks  to  develop  technologies  that  will  lower  the  cost  and  increase  the  accessibility  of  refractive  eye  care,  especially  in  low-resource  settings.  Specifically,  optical  and  software  systems  will  be  developed  that  will  extend  the  measurement  range  of  a  low-cost  device  that  automatically  prescribes  eyeglasses  for patients with a large range of refractive errors.","Development of a low-cost, portable autorefractor for the measuring of refractive errors in low-resource settings",9349858,R43EY025452,"['Adoption', 'Algorithms', 'Brazil', 'Caring', 'China', 'Client satisfaction', 'Clinical Trials', 'Communities', 'Databases', 'Detection', 'Development', 'Devices', 'Education', 'Ensure', 'Eye', 'Eyeglasses', 'Feedback', 'Goals', 'Health', 'Hospitals', 'Human', 'Human Resources', 'Hyperopia', 'Image', 'India', 'Laser In Situ Keratomileusis', 'Letters', 'Licensing', 'Lighting', 'Machine Learning', 'Marketing', 'Measurement', 'Measures', 'Methods', 'Mission', 'Modeling', 'Myopia', 'New England', 'Nurses', 'Operative Surgical Procedures', 'Optics', 'Optometrist', 'Optometry', 'Output', 'Patients', 'Performance', 'Pharmacists', 'Phase', 'Population', 'Positioning Attribute', 'Productivity', 'Property', 'Protocols documentation', 'Provider', 'Pupil', 'Quality of life', 'Refractive Errors', 'Resources', 'Rest', 'Retina', 'Source', 'Spottings', 'System', 'Target Populations', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Vision', 'Visual Acuity', 'Work', 'base', 'college', 'commercialization', 'cost', 'design', 'digital', 'disability', 'field study', 'handheld equipment', 'improved', 'lens', 'meetings', 'prototype', 'software systems', 'standard of care', 'success', 'usability']",NEI,"PLENOPTIKA, INC.",R43,2016,25000,0.06774912698646793
"Development of a low-cost, portable autorefractor for the measuring of refractive errors in low-resource settings ﻿    DESCRIPTION (provided by applicant): ""Development of a low-cost, extended-range, autorefractor""    This  project  proposal  seeks  to  develop  technologies  that  will  lower  the cost  of  and  increase  the  accessibility  of  refractive  eye  care,  especially  in  low-resoure  settings.  We propose to advance our previously developed low-cost, handheld device capable of automatically measuring the optical properties of the eye based on the technique of wavefront aberrometry.  The  proposed  work  will  specifically  focus  on  the development of optical and software systems will extend the device's measurement  range  to  work  on  a  larger  range  of  refractive  errors.  First,  optical  systems  with  no  moving  parts  will  be  developed  which  enable  the  device  to  work  on  subjects  with  severe  myopia  or  hyperopia  (<-­-6  diopters or  >6  diopters  of  refractive  error).  Second,  algorithms  that  make  the  device  easy  and  intuitive  to  use  will  be  implemented.  Third,  a  handheld  prototype  incorporating  these  improvements  will  be  constructed  and  validated  in  model  eye  systems.  The  output  of  this  project  will  be  a  functional,  extended-range  prototype  that  will  facilitate  the  fild-testing  and  commercialization  of  a  low-cost,  easy-to-use  device  to  dispense  eyeglass  prescriptions.                 PUBLIC HEALTH RELEVANCE: ""Development of a low-cost, extended-range, autorefractor""    This  project  proposal  seeks  to  develop  technologies  that  will  lower  the  cost  and  increase  the  accessibility  of  refractive  eye  care,  especially  in  low-resource  settings.  Specifically,  optical  and  software  systems  will  be  developed  that  will  extend  the  measurement  range  of  a  low-cost  device  that  automatically  prescribes  eyeglasses  for patients with a large range of refractive errors.             ","Development of a low-cost, portable autorefractor for the measuring of refractive errors in low-resource settings",8981552,R43EY025452,"['Adoption', 'Algorithms', 'Brazil', 'Caring', 'China', 'Client satisfaction', 'Clinical Trials', 'Communities', 'Databases', 'Detection', 'Development', 'Devices', 'Education', 'Ensure', 'Eye', 'Eyeglasses', 'Feedback', 'Goals', 'Health', 'Hospitals', 'Human', 'Human Resources', 'Hyperopia', 'Image', 'India', 'Laser In Situ Keratomileusis', 'Letters', 'Licensing', 'Lighting', 'Machine Learning', 'Marketing', 'Measurement', 'Measures', 'Methods', 'Mission', 'Modeling', 'Myopia', 'New England', 'Nurses', 'Operative Surgical Procedures', 'Optics', 'Optometrist', 'Optometry', 'Output', 'Patients', 'Performance', 'Pharmacists', 'Phase', 'Population', 'Positioning Attribute', 'Productivity', 'Property', 'Protocols documentation', 'Provider', 'Pupil', 'Quality of life', 'Refractive Errors', 'Resources', 'Rest', 'Retina', 'Source', 'Spottings', 'System', 'Target Populations', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Vision', 'Visual Acuity', 'Work', 'base', 'college', 'commercialization', 'cost', 'design', 'digital', 'disability', 'improved', 'lens', 'meetings', 'prototype', 'software systems', 'success', 'usability']",NEI,"PLENOPTIKA, INC.",R43,2015,149265,0.06774912698646793
"SCH:Smartphone Wound Image Parameter Analysis and Decision Support in Mobile Env PROJECT SUMMARY (See instructions): Chronic wounds affect 6.5 million patients in the U.S., with an estimated treatment cost of $25 billion. Our team proposes research to advance our existing NSF-funded smartphone wound analysis system, which helps patients monitor their diabetic foot ulcers, providing them with instant feedback on healing progress. Our wound system analyzes a smartphone image of the patients' wound, detects the wound area and tissue composition, and generates a proprietary healing score by comparing the current image with a past image. Our envisioned chronic wound assessment system will support evidence-based decisions by the care team while visiting patients, and move wound care toward digital objectivity. We define digital objectivity as the synthesis of wound assessment metrics that are extracted autonomously from images in order to generate objective actionable feedback, enabling clinicians not trained as wound specialists to deliver ""standardized wound care"". Digital objectivity contrasts with the current practice of subjective, visual inspection of wounds based on physician experience. The first aim will develop image processing algorithms to mitigate wound analysis errors caused by non-ideal lighting in some clinical or home settings, and when the wound is photographed from arbitrary camera angles and distance. While our previous wound system worked well in ideal conditions, non-ideal lighting caused large errors and healthy skin was detected as the wound area in extreme cases. The second aim extends our existing wound analysis system that targets only diabetic wounds to handle arterial, venous and pressure ulcers, expanding the potential user. The third aim will synthesize algorithms that autonomously generate actionable wound decision rules that are learned from decisions taken by actual wound clinicians. This research is joint work of Worcester Polytechnic Institute (WPI) (technical expertise in image processing, machine learning and smartphone programming) and University of Massachusetts Medical School (UMMS) (clinical expertise on wounds, and wound patient recruitment to validate our work) RELEVANCE (See instructions): We propose research to advance our existing smartphone wound analysis system, which detects the wound area and tissue composition, and generates a proprietary healing score from a wound image. Our wound assessment system will give patients instant, actionable feedback and enable clinicians not trained as wound specialists to make objective, evidence-based wound care decisions and deliver standardized care.",SCH:Smartphone Wound Image Parameter Analysis and Decision Support in Mobile Env,9618878,R01EB025801,"['Affect', 'Algorithms', 'Area', 'Caring', 'Cellular Phone', 'Clinical', 'Decubitus ulcer', 'Diabetic Foot Ulcer', 'Diabetic wound', 'Feedback', 'Funding', 'Home environment', 'Image', 'Institutes', 'Instruction', 'Joints', 'Lighting', 'Machine Learning', 'Massachusetts', 'Patient Monitoring', 'Patient Recruitments', 'Patient imaging', 'Patients', 'Physicians', 'Research', 'Skin', 'Specialist', 'Standardization', 'System', 'Systems Analysis', 'Technical Expertise', 'Tissues', 'Treatment Cost', 'Universities', 'Varicose Ulcer', 'Visit', 'Visual', 'Work', 'base', 'chronic wound', 'digital', 'evidence base', 'experience', 'healing', 'image processing', 'medical schools', 'standardized care', 'wound']",NIBIB,WORCESTER POLYTECHNIC INSTITUTE,R01,2019,401916,0.06928392666362823
"SCH:Smartphone Wound Image Parameter Analysis and Decision Support in Mobile Env PROJECT SUMMARY (See instructions): Chronic wounds affect 6.5 million patients in the U.S., with an estimated treatment cost of $25 billion. Our team proposes research to advance our existing NSF-funded smartphone wound analysis system, which helps patients monitor their diabetic foot ulcers, providing them with instant feedback on healing progress. Our wound system analyzes a smartphone image of the patients' wound, detects the wound area and tissue composition, and generates a proprietary healing score by comparing the current image with a past image. Our envisioned chronic wound assessment system will support evidence-based decisions by the care team while visiting patients, and move wound care toward digital objectivity. We define digital objectivity as the synthesis of wound assessment metrics that are extracted autonomously from images in order to generate objective actionable feedback, enabling clinicians not trained as wound specialists to deliver ""standardized wound care"". Digital objectivity contrasts with the current practice of subjective, visual inspection of wounds based on physician experience. The first aim will develop image processing algorithms to mitigate wound analysis errors caused by non-ideal lighting in some clinical or home settings, and when the wound is photographed from arbitrary camera angles and distance. While our previous wound system worked well in ideal conditions, non-ideal lighting caused large errors and healthy skin was detected as the wound area in extreme cases. The second aim extends our existing wound analysis system that targets only diabetic wounds to handle arterial, venous and pressure ulcers, expanding the potential user. The third aim will synthesize algorithms that autonomously generate actionable wound decision rules that are learned from decisions taken by actual wound clinicians. This research is joint work of Worcester Polytechnic Institute (WPI) (technical expertise in image processing, machine learning and smartphone programming) and University of Massachusetts Medical School (UMMS) (clinical expertise on wounds, and wound patient recruitment to validate our work) RELEVANCE (See instructions): We propose research to advance our existing smartphone wound analysis system, which detects the wound area and tissue composition, and generates a proprietary healing score from a wound image. Our wound assessment system will give patients instant, actionable feedback and enable clinicians not trained as wound specialists to make objective, evidence-based wound care decisions and deliver standardized care.",SCH:Smartphone Wound Image Parameter Analysis and Decision Support in Mobile Env,9496652,R01EB025801,"['Affect', 'Algorithms', 'Area', 'Caring', 'Cellular Phone', 'Clinical', 'Decubitus ulcer', 'Diabetic Foot Ulcer', 'Diabetic wound', 'Feedback', 'Funding', 'Home environment', 'Image', 'Institutes', 'Instruction', 'Joints', 'Lighting', 'Machine Learning', 'Massachusetts', 'Patient Monitoring', 'Patient Recruitments', 'Patients', 'Physicians', 'Research', 'Skin', 'Specialist', 'Standardization', 'System', 'Systems Analysis', 'Technical Expertise', 'Tissues', 'Treatment Cost', 'Universities', 'Varicose Ulcer', 'Visit', 'Visual', 'Work', 'base', 'chronic wound', 'digital', 'evidence base', 'experience', 'healing', 'image processing', 'medical schools', 'standardized care', 'wound']",NIBIB,WORCESTER POLYTECHNIC INSTITUTE,R01,2018,425994,0.06928392666362823
"SCH:Smartphone Wound Image Parameter Analysis and Decision Support in Mobile Env PROJECT SUMMARY (See instructions): Chronic wounds affect 6.5 million patients in the U.S., with an estimated treatment cost of $25 billion. Our team proposes research to advance our existing NSF-funded smartphone wound analysis system, which helps patients monitor their diabetic foot ulcers, providing them with instant feedback on healing progress. Our wound system analyzes a smartphone image of the patients' wound, detects the wound area and tissue composition, and generates a proprietary healing score by comparing the current image with a past image. Our envisioned chronic wound assessment system will support evidence-based decisions by the care team while visiting patients, and move wound care toward digital objectivity. We define digital objectivity as the synthesis of wound assessment metrics that are extracted autonomously from images in order to generate objective actionable feedback, enabling clinicians not trained as wound specialists to deliver ""standardized wound care"". Digital objectivity contrasts with the current practice of subjective, visual inspection of wounds based on physician experience. The first aim will develop image processing algorithms to mitigate wound analysis errors caused by non-ideal lighting in some clinical or home settings, and when the wound is photographed from arbitrary camera angles and distance. While our previous wound system worked well in ideal conditions, non-ideal lighting caused large errors and healthy skin was detected as the wound area in extreme cases. The second aim extends our existing wound analysis system that targets only diabetic wounds to handle arterial, venous and pressure ulcers, expanding the potential user. The third aim will synthesize algorithms that autonomously generate actionable wound decision rules that are learned from decisions taken by actual wound clinicians. This research is joint work of Worcester Polytechnic Institute (WPI) (technical expertise in image processing, machine learning and smartphone programming) and University of Massachusetts Medical School (UMMS) (clinical expertise on wounds, and wound patient recruitment to validate our work) RELEVANCE (See instructions): We propose research to advance our existing smartphone wound analysis system, which detects the wound area and tissue composition, and generates a proprietary healing score from a wound image. Our wound assessment system will give patients instant, actionable feedback and enable clinicians not trained as wound specialists to make objective, evidence-based wound care decisions and deliver standardized care.",SCH:Smartphone Wound Image Parameter Analysis and Decision Support in Mobile Env,9823881,R01EB025801,"['Affect', 'Algorithms', 'Area', 'Caring', 'Cellular Phone', 'Clinical', 'Diabetic Foot Ulcer', 'Feedback', 'Funding', 'Home environment', 'Image', 'Institutes', 'Instruction', 'Joints', 'Lighting', 'Machine Learning', 'Massachusetts', 'Patient Monitoring', 'Patient Recruitments', 'Patient imaging', 'Patients', 'Physicians', 'Research', 'Skin', 'Specialist', 'Standardization', 'System', 'Systems Analysis', 'Technical Expertise', 'Tissues', 'Treatment Cost', 'Universities', 'Varicose Ulcer', 'Visit', 'Visual', 'Work', 'base', 'chronic wound', 'decubitus ulcer', 'diabetic ulcer', 'digital', 'evidence base', 'experience', 'healing', 'image processing', 'medical schools', 'standardized care', 'wound', 'wound care']",NIBIB,WORCESTER POLYTECHNIC INSTITUTE,R01,2020,401916,0.06928392666362823
"Application of a Machine Learning to Enhance e-Triggers to Detect and Learn from Diagnostic Safety Events The frequency of diagnostic errors in emergency departments (ED) is largely unknown but likely to be significant. There is a compelling need to create measurement methods that provide diagnostic safety data to clinicians and leaders who in turn can act upon these data to prevent diagnostic harm. Electronic trigger (e- trigger) tools mine vast amounts of clinical and administrative data to identify signals for likely adverse events and have demonstrated capability to identify diagnostic errors. Such tools are more efficient and effective than other methods and can reduce the number of records requiring human review to those at highest risk of harm.  In prior work, we used rules-based e-trigger algorithms to identify patterns of care suggestive of missed or delayed diagnoses in primary care and inpatient settings. For instance, a clinic visit followed several days later by an unplanned hospitalization could be indicative of potential problems with the diagnostic process at the clinic visit. We also proposed a knowledge discovery framework, the Safer Dx Trigger Tools Framework, to enable health care organizations (HCOs) to develop and implement e-trigger tools to measure diagnostic errors using comprehensive electronic health record (EHR) data. Review and analysis of these cases can uncover safety concerns and provide information on diagnostic process breakdowns and related contributory factors, which in turn could generate learning and feedback for improvement purposes.  Sophisticated techniques from machine learning (ML) and data science could help inform ‘second generation’ e-trigger algorithms that better identify diagnostic errors and/or harm than rules-based e-triggers that require substantial manual effort and chart reviews. In contrast to rules-based systems, ML techniques could help learn from examples and accurately retrieve charts with diagnostic error without the need for “hand crafting” of an e-trigger. We will apply e-triggers to comprehensive EHRs that contain longitudinal patient care data (progress notes, tests, referrals) that provide an extensive picture of patients’ diagnostic journeys. Using national VA data, including data from 9 million veterans, and data from Geisinger health system, a pioneer HCO that serves approximately 3 million patients, we propose the following aims: Aim 1 – To develop, refine, test, and apply Safer Dx e-triggers to enable detection, measurement, and learning from diagnostic errors in diverse emergency department (ED) settings. We will calculate the frequency of diagnostic errors in the ED based on these e-triggers and describe the burden of preventable diagnostic harm. Aim 2 - To explore machine learning techniques that yield robust, accurate models to predict diagnostic errors using EHR-enriched data derived from expert-labeled patient records containing diagnostic errors (from Aim 1).  To our knowledge this is the first ML application in diagnostic error measurement, which could help scale up expert-driven e-trigger development and refinement. Newly developed e-triggers can be pilot tested and implemented at other HCOs, enabling them to create actionable safety-related insights from digital data. Narrative The process of making a diagnosis in emergency departments can be vulnerable to error. To study the risks involved, we will develop strategies to better identify electronic medical records of patients who may have had a diagnostic error in their care. We will test the use of human-derived rule-based algorithms and data-driven machine learning methods that can more accurately and efficiently identify these medical records than other currently available methods used in measurement of patient safety.",Application of a Machine Learning to Enhance e-Triggers to Detect and Learn from Diagnostic Safety Events,10018015,R01HS027363,[' '],AHRQ,BAYLOR COLLEGE OF MEDICINE,R01,2020,498859,0.1958112629664101
"Application of a Machine Learning to Enhance e-Triggers to Detect and Learn from Diagnostic Safety Events The frequency of diagnostic errors in emergency departments (ED) is largely unknown but likely to be significant. There is a compelling need to create measurement methods that provide diagnostic safety data to clinicians and leaders who in turn can act upon these data to prevent diagnostic harm. Electronic trigger (e- trigger) tools mine vast amounts of clinical and administrative data to identify signals for likely adverse events and have demonstrated capability to identify diagnostic errors. Such tools are more efficient and effective than other methods and can reduce the number of records requiring human review to those at highest risk of harm.  In prior work, we used rules-based e-trigger algorithms to identify patterns of care suggestive of missed or delayed diagnoses in primary care and inpatient settings. For instance, a clinic visit followed several days later by an unplanned hospitalization could be indicative of potential problems with the diagnostic process at the clinic visit. We also proposed a knowledge discovery framework, the Safer Dx Trigger Tools Framework, to enable health care organizations (HCOs) to develop and implement e-trigger tools to measure diagnostic errors using comprehensive electronic health record (EHR) data. Review and analysis of these cases can uncover safety concerns and provide information on diagnostic process breakdowns and related contributory factors, which in turn could generate learning and feedback for improvement purposes.  Sophisticated techniques from machine learning (ML) and data science could help inform ‘second generation’ e-trigger algorithms that better identify diagnostic errors and/or harm than rules-based e-triggers that require substantial manual effort and chart reviews. In contrast to rules-based systems, ML techniques could help learn from examples and accurately retrieve charts with diagnostic error without the need for “hand crafting” of an e-trigger. We will apply e-triggers to comprehensive EHRs that contain longitudinal patient care data (progress notes, tests, referrals) that provide an extensive picture of patients’ diagnostic journeys. Using national VA data, including data from 9 million veterans, and data from Geisinger health system, a pioneer HCO that serves approximately 3 million patients, we propose the following aims: Aim 1 – To develop, refine, test, and apply Safer Dx e-triggers to enable detection, measurement, and learning from diagnostic errors in diverse emergency department (ED) settings. We will calculate the frequency of diagnostic errors in the ED based on these e-triggers and describe the burden of preventable diagnostic harm. Aim 2 - To explore machine learning techniques that yield robust, accurate models to predict diagnostic errors using EHR-enriched data derived from expert-labeled patient records containing diagnostic errors (from Aim 1).  To our knowledge this is the first ML application in diagnostic error measurement, which could help scale up expert-driven e-trigger development and refinement. Newly developed e-triggers can be pilot tested and implemented at other HCOs, enabling them to create actionable safety-related insights from digital data. Narrative The process of making a diagnosis in emergency departments can be vulnerable to error. To study the risks involved, we will develop strategies to better identify electronic medical records of patients who may have had a diagnostic error in their care. We will test the use of human-derived rule-based algorithms and data-driven machine learning methods that can more accurately and efficiently identify these medical records than other currently available methods used in measurement of patient safety.",Application of a Machine Learning to Enhance e-Triggers to Detect and Learn from Diagnostic Safety Events,9938114,R01HS027363,[' '],AHRQ,BAYLOR COLLEGE OF MEDICINE,R01,2019,496121,0.1958112629664101
"Utility of Predictive Systems to identify Inpatient Diagnostic Errors: The UPSIDE Study PROJECT SUMMARY/ABSTRACT  While much research has been conducted on patient safety since the Institute of Medicine published “To Err is Human” in 2000, there is a comparative dearth of research on diagnostic errors in the hospital setting. The broad, long-term objectives of the proposed research is to better understand the incidence, causes, and risk factors for diagnostic errors in the inpatient setting. This work will provide foundational research for the development of interventions to reduce these errors, including predictive tools, targets for intervention, and a methodology for outcome assessment in future trials of interventions. To achieve this overall goal, we will carry out the following specific aims: 1) To determine the incidence of diagnostic errors among patients who die in hospital or are transferred to the ICU two days or more after admission to a general medicine service through a structured, standardized adjudication process of patient records, 2) To combine adjudication data with data from Vizient to determine which specific factors contribute to risks for diagnostic errors, and to use risk estimates to calculate incidence and impact of factors contributing to those errors, and 3)To create machine- learning models that can be used to retrospectively identify patients in whom a diagnostic error was likely to have taken place. The research will involve a retrospective evaluation of 2000 patients admitted to general medicine units at 20 US hospitals participating in a national research collaborative and which also contribute data to a benchmarking and purchasing organization (Vizient). Using the Safer-Diagnosis (Safer-Dx) and Diagnostic Error Evaluation and Research (DEER) taxonomy tools, both adapted for the inpatient setting, adjudicators will review electronic medical record data and determine the presence or absence of diagnostic errors using a rigorous training and continuous review process to ensure reliability across sites, adjudicators, and time. Standard modelling techniques will be used to understand the population-attributable risk of each of the DEER process failure points to diagnostic error as well as the contributions of several patient, provider, and system-level risk factors. Lastly, advanced machine-learning methods will be used to create models that can identify patients in whom diagnostic error occurred, with superior performance to standard approaches such as logistic regression. Together, these approaches will provide a broad and representative picture of the incidence of diagnostic errors among hospitalized patients who have suffered harm, develop models of patient and system-based factors that make a diagnostic error more or less likely, and build advanced, efficient, and scalable tools needed to support future surveillance and improvement programs for a variety of institutions. This research will establish a foundation from which healthcare systems can assess and achieve excellence in diagnosis in the inpatient setting. PROJECT NARRATIVE  This study seeks to accurately define the incidence of diagnostic errors among patients suffering serious inpatient events in a large network of US hospitals. Without a reliable method for determining the presence of diagnostic errors across many organizations, it is not otherwise possible to understand the incidence, impact, predictors, and underlying causes of these errors, to create and optimize future solutions to reduce diagnostic errors, to directly test the effects of these solutions, or to teach physicians how to avoid diagnostic pitfalls in the future. Our study addresses these issues while being responsive to the RFA’s goals of developing robust estimates of incidence and risk and using approaches that leverage electronic data, and our approach represents a novel application of rigorous outcome adjudication and advanced modeling techniques to the problem of inpatient diagnostic errors.",Utility of Predictive Systems to identify Inpatient Diagnostic Errors: The UPSIDE Study,10020962,R01HS027369,[' '],AHRQ,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2020,496734,0.23435781256782318
"Utility of Predictive Systems to identify Inpatient Diagnostic Errors: The UPSIDE Study PROJECT SUMMARY/ABSTRACT  While much research has been conducted on patient safety since the Institute of Medicine published “To Err is Human” in 2000, there is a comparative dearth of research on diagnostic errors in the hospital setting. The broad, long-term objectives of the proposed research is to better understand the incidence, causes, and risk factors for diagnostic errors in the inpatient setting. This work will provide foundational research for the development of interventions to reduce these errors, including predictive tools, targets for intervention, and a methodology for outcome assessment in future trials of interventions. To achieve this overall goal, we will carry out the following specific aims: 1) To determine the incidence of diagnostic errors among patients who die in hospital or are transferred to the ICU two days or more after admission to a general medicine service through a structured, standardized adjudication process of patient records, 2) To combine adjudication data with data from Vizient to determine which specific factors contribute to risks for diagnostic errors, and to use risk estimates to calculate incidence and impact of factors contributing to those errors, and 3)To create machine- learning models that can be used to retrospectively identify patients in whom a diagnostic error was likely to have taken place. The research will involve a retrospective evaluation of 2000 patients admitted to general medicine units at 20 US hospitals participating in a national research collaborative and which also contribute data to a benchmarking and purchasing organization (Vizient). Using the Safer-Diagnosis (Safer-Dx) and Diagnostic Error Evaluation and Research (DEER) taxonomy tools, both adapted for the inpatient setting, adjudicators will review electronic medical record data and determine the presence or absence of diagnostic errors using a rigorous training and continuous review process to ensure reliability across sites, adjudicators, and time. Standard modelling techniques will be used to understand the population-attributable risk of each of the DEER process failure points to diagnostic error as well as the contributions of several patient, provider, and system-level risk factors. Lastly, advanced machine-learning methods will be used to create models that can identify patients in whom diagnostic error occurred, with superior performance to standard approaches such as logistic regression. Together, these approaches will provide a broad and representative picture of the incidence of diagnostic errors among hospitalized patients who have suffered harm, develop models of patient and system-based factors that make a diagnostic error more or less likely, and build advanced, efficient, and scalable tools needed to support future surveillance and improvement programs for a variety of institutions. This research will establish a foundation from which healthcare systems can assess and achieve excellence in diagnosis in the inpatient setting. PROJECT NARRATIVE  This study seeks to accurately define the incidence of diagnostic errors among patients suffering serious inpatient events in a large network of US hospitals. Without a reliable method for determining the presence of diagnostic errors across many organizations, it is not otherwise possible to understand the incidence, impact, predictors, and underlying causes of these errors, to create and optimize future solutions to reduce diagnostic errors, to directly test the effects of these solutions, or to teach physicians how to avoid diagnostic pitfalls in the future. Our study addresses these issues while being responsive to the RFA’s goals of developing robust estimates of incidence and risk and using approaches that leverage electronic data, and our approach represents a novel application of rigorous outcome adjudication and advanced modeling techniques to the problem of inpatient diagnostic errors.",Utility of Predictive Systems to identify Inpatient Diagnostic Errors: The UPSIDE Study,9938134,R01HS027369,[' '],AHRQ,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2019,497444,0.23435781256782318
"An expert-guided machine-learning approach to estimate the incidence, risk and harms associated with diagnostic delays for infectious diseases. Project Summary / Abstract Diagnostic errors are increasingly recognized as a cause of pain, suffering and increased healthcare costs. Diagnostic delays are an important class of diagnostic errors. While many diagnostic errors occur in hospital settings, emergency departments visits may be especially important to consider because they treat critically ill patients and because most decisions to admit patients to the hospital are made in emergency departments. Thus, to enable a more complete understanding of diagnostic delays requires consideration of healthcare visits across a range of healthcare settings including clinic visits, emergency department visits and hospitalizations. Delays in diagnosing infectious diseases are important to consider. For contagious infectious diseases, diagnostic delays increase the risk of additional exposures, potentially generating more cases. Second, many infectious diseases can be effectively treated, but even short delays in treatment lead to worse clinical outcomes. However, with the exception of a few infectious diseases (e.g., tuberculosis), diagnostic delays for infectious diseases are understudied. Thus, there is a critical need to investigate the incidence, risk factors and clinical impact for diagnostic delays for infectious diseases. The overarching goal of our research is to investigate diagnostic delays associated with infectious diseases using existing data along with methods from the fields of computer science and statistics. While our research relies upon “big data”, we will also use clinical experts to review and contribute to all of our results. Our subject matter experts incorporate expertise in infectious diseases, emergency medicine, acute care, medical education, diagnostic reasoning, healthcare epidemiology, public health, industry, and professional infectious disease societies. Specifically, we will 1) determine the incidence of diagnostic delays for a wide range of infectious diseases; 2) identify the risk factors associated with diagnostic delays for infectious diseases that are frequently delayed or have serious outcomes; and 3) estimate the impact of diagnostic delays in terms of healthcare costs and mortality. With our data, methods and clinical experts, we will be able to translate our results into future interventions designed to decrease diagnostic delays and improve healthcare outcomes. In addition, while our proposal focuses on infectious diseases, the methods and approaches that we will develop can be adopted to investigate non-infectious diseases and conditions. Project Narrative Diagnostic delays for infectious diseases contribute to worse clinical outcomes, increased healthcare costs and, for some infectious diseases, outbreaks of great public health importance. We will use existing large data sets along with machine-learning techniques and expert clinical guidance to detect patterns of healthcare visits representing diagnostic delays. Our goal is to characterize the incidence, risk factors and clinical impact of diagnostic delays for a wide range of infectious diseases to inform future interventions.","An expert-guided machine-learning approach to estimate the incidence, risk and harms associated with diagnostic delays for infectious diseases.",10017203,R01HS027375,[' '],AHRQ,UNIVERSITY OF IOWA,R01,2020,496235,0.04997380290804537
"An expert-guided machine-learning approach to estimate the incidence, risk and harms associated with diagnostic delays for infectious diseases. Project Summary / Abstract Diagnostic errors are increasingly recognized as a cause of pain, suffering and increased healthcare costs. Diagnostic delays are an important class of diagnostic errors. While many diagnostic errors occur in hospital settings, emergency departments visits may be especially important to consider because they treat critically ill patients and because most decisions to admit patients to the hospital are made in emergency departments. Thus, to enable a more complete understanding of diagnostic delays requires consideration of healthcare visits across a range of healthcare settings including clinic visits, emergency department visits and hospitalizations. Delays in diagnosing infectious diseases are important to consider. For contagious infectious diseases, diagnostic delays increase the risk of additional exposures, potentially generating more cases. Second, many infectious diseases can be effectively treated, but even short delays in treatment lead to worse clinical outcomes. However, with the exception of a few infectious diseases (e.g., tuberculosis), diagnostic delays for infectious diseases are understudied. Thus, there is a critical need to investigate the incidence, risk factors and clinical impact for diagnostic delays for infectious diseases. The overarching goal of our research is to investigate diagnostic delays associated with infectious diseases using existing data along with methods from the fields of computer science and statistics. While our research relies upon “big data”, we will also use clinical experts to review and contribute to all of our results. Our subject matter experts incorporate expertise in infectious diseases, emergency medicine, acute care, medical education, diagnostic reasoning, healthcare epidemiology, public health, industry, and professional infectious disease societies. Specifically, we will 1) determine the incidence of diagnostic delays for a wide range of infectious diseases; 2) identify the risk factors associated with diagnostic delays for infectious diseases that are frequently delayed or have serious outcomes; and 3) estimate the impact of diagnostic delays in terms of healthcare costs and mortality. With our data, methods and clinical experts, we will be able to translate our results into future interventions designed to decrease diagnostic delays and improve healthcare outcomes. In addition, while our proposal focuses on infectious diseases, the methods and approaches that we will develop can be adopted to investigate non-infectious diseases and conditions. Project Narrative Diagnostic delays for infectious diseases contribute to worse clinical outcomes, increased healthcare costs and, for some infectious diseases, outbreaks of great public health importance. We will use existing large data sets along with machine-learning techniques and expert clinical guidance to detect patterns of healthcare visits representing diagnostic delays. Our goal is to characterize the incidence, risk factors and clinical impact of diagnostic delays for a wide range of infectious diseases to inform future interventions.","An expert-guided machine-learning approach to estimate the incidence, risk and harms associated with diagnostic delays for infectious diseases.",9938200,R01HS027375,[' '],AHRQ,UNIVERSITY OF IOWA,R01,2019,497273,0.04997380290804537
"THEORETICAL STUDIES OF MACROMOLECULES We will carry out calculations in four main areas.  First, we describe a pattern-matching approach to prediction of protein secondary structure. This program can be coupled to the tertiary packing algorithms of Cohen et.al. to produce protein-like structures.  In the best case with proteins that have been studied crystallographically, discrepancies of ca. 4 angstroms are observed.  Methods of reducing these errors are evaluated. Second, we seek a detailed model for the motions of ligands in proteins. Conventional molecular dynamics and trajectory calculations will be compared to methods that reveal packing defects and channels in proteins in a direct way.  Third, we wish to extend our docking calculations to macromolecular interactions and to systematic structural comparison of sets of bioactive compounds.  Fourth, methods are discussed to evaluate the potential functions used in molecular mechanics and molecular dynamics.  n/a",THEORETICAL STUDIES OF MACROMOLECULES,3279540,R01GM031497,"['artificial intelligence', ' chemical binding', ' computer simulation', ' conformation', ' creatine kinase', ' mathematical model', ' molecular dynamics', ' molecular stacking', ' myoglobin', ' nuclear magnetic resonance spectroscopy', ' nucleic acid structure', ' oncogenes', ' protein structure']",NIGMS,UNIVERSITY OF CALIFORNIA SAN FRANCISCO,R01,1990,132850,0.03988229813778619
"THEORETICAL STUDIES OF MACROMOLECULES We will carry out calculations in four main areas.  First, we describe a pattern-matching approach to prediction of protein secondary structure. This program can be coupled to the tertiary packing algorithms of Cohen et.al. to produce protein-like structures.  In the best case with proteins that have been studied crystallographically, discrepancies of ca. 4 angstroms are observed.  Methods of reducing these errors are evaluated. Second, we seek a detailed model for the motions of ligands in proteins. Conventional molecular dynamics and trajectory calculations will be compared to methods that reveal packing defects and channels in proteins in a direct way.  Third, we wish to extend our docking calculations to macromolecular interactions and to systematic structural comparison of sets of bioactive compounds.  Fourth, methods are discussed to evaluate the potential functions used in molecular mechanics and molecular dynamics.  n/a",THEORETICAL STUDIES OF MACROMOLECULES,3279539,R01GM031497,"['artificial intelligence', ' chemical binding', ' computer simulation', ' conformation', ' creatine kinase', ' mathematical model', ' molecular dynamics', ' molecular stacking', ' myoglobin', ' nuclear magnetic resonance spectroscopy', ' nucleic acid structure', ' oncogenes', ' protein structure']",NIGMS,UNIVERSITY OF CALIFORNIA SAN FRANCISCO,R01,1989,122754,0.03988229813778619
"THEORETICAL STUDIES OF MACROMOLECULES We will carry out calculations in four main areas.  First, we describe a pattern-matching approach to prediction of protein secondary structure. This program can be coupled to the tertiary packing algorithms of Cohen et.al. to produce protein-like structures.  In the best case with proteins that have been studied crystallographically, discrepancies of ca. 4 angstroms are observed.  Methods of reducing these errors are evaluated. Second, we seek a detailed model for the motions of ligands in proteins. Conventional molecular dynamics and trajectory calculations will be compared to methods that reveal packing defects and channels in proteins in a direct way.  Third, we wish to extend our docking calculations to macromolecular interactions and to systematic structural comparison of sets of bioactive compounds.  Fourth, methods are discussed to evaluate the potential functions used in molecular mechanics and molecular dynamics.  n/a",THEORETICAL STUDIES OF MACROMOLECULES,3279538,R01GM031497,"['artificial intelligence', ' chemical binding', ' computer simulation', ' conformation', ' creatine kinase', ' mathematical model', ' molecular dynamics', ' molecular stacking', ' myoglobin', ' nuclear magnetic resonance spectroscopy', ' nucleic acid structure', ' oncogenes', ' protein structure']",NIGMS,UNIVERSITY OF CALIFORNIA SAN FRANCISCO,R01,1988,116246,0.03988229813778619
"THEORETICAL STUDIES OF MACROMOLECULES We will carry out calculations in four main areas.  First, we describe a pattern-matching approach to prediction of protein secondary structure. This program can be coupled to the tertiary packing algorithms of Cohen et.al. to produce protein-like structures.  In the best case with proteins that have been studied crystallographically, discrepancies of ca. 4 angstroms are observed.  Methods of reducing these errors are evaluated. Second, we seek a detailed model for the motions of ligands in proteins. Conventional molecular dynamics and trajectory calculations will be compared to methods that reveal packing defects and channels in proteins in a direct way.  Third, we wish to extend our docking calculations to macromolecular interactions and to systematic structural comparison of sets of bioactive compounds.  Fourth, methods are discussed to evaluate the potential functions used in molecular mechanics and molecular dynamics.  n/a",THEORETICAL STUDIES OF MACROMOLECULES,3279537,R01GM031497,"['artificial intelligence', ' computer simulation', ' conformation', ' creatine kinase', ' intermolecular interaction', ' ligands', ' mathematical model', ' molecular stacking', ' myoglobin', ' nuclear magnetic resonance spectroscopy', ' nucleic acid structure', ' oncogenes']",NIGMS,UNIVERSITY OF CALIFORNIA SAN FRANCISCO,R01,1987,117648,0.03988229813778619
"THEORETICAL STUDIES OF MACROMOLECULES We will carry out calculations in four main areas.  First, we describe a pattern-matching approach to prediction of protein secondary structure. This program can be coupled to the tertiary packing algorithms of Cohen et.al. to produce protein-like structures.  In the best case with proteins that have been studied crystallographically, discrepancies of ca. 4 angstroms are observed.  Methods of reducing these errors are evaluated. Second, we seek a detailed model for the motions of ligands in proteins. Conventional molecular dynamics and trajectory calculations will be compared to methods that reveal packing defects and channels in proteins in a direct way.  Third, we wish to extend our docking calculations to macromolecular interactions and to systematic structural comparison of sets of bioactive compounds.  Fourth, methods are discussed to evaluate the potential functions used in molecular mechanics and molecular dynamics.  n/a",THEORETICAL STUDIES OF MACROMOLECULES,3279533,R01GM031497,"['artificial intelligence', ' computer simulation', ' conformation', ' creatine kinase', ' intermolecular interaction', ' ligands', ' mathematical model', ' molecular stacking', ' myoglobin', ' nuclear magnetic resonance spectroscopy', ' nucleic acid structure', ' oncogenes']",NIGMS,UNIVERSITY OF CALIFORNIA SAN FRANCISCO,R01,1986,99883,0.03988229813778619
"RECOVERING A DENSITY FROM INCOMPLETE TOMOGRAPHIC DATA The proposed research is in the field of tomography and has direct relevence for the uses of X-ray and other radiation in clinical diagnostic medicine.  The goal of the research is to discover and implement an effective computer algorithm to recover the density of the part of the body outside of a specified region from tomographic data (line integrals) from X-ray beams that do not pass through that region.  The same question will be studied for the plane integrals that are tomographic data gotten in one type of nuclear magnetic resonance zeugmatography.  Each algorithm will be tested on mathematical phantoms and medical data.  There are a number of compelling medical reasons for such an algorithm. For example, the beating heart creates error in regular chest tomograms, and bone or metal pins can create error in tomograms.  It is important to be able to get good tomographic reconstructions of the organs around these regions.  The way to do this is to develop the proposed algorithm.  Two methods of solution are proposed, one of which is based on the singular value decomposition of Professor Quinto.  Preliminary refinements are described that demonstrate the good potential of this method.  n/a",RECOVERING A DENSITY FROM INCOMPLETE TOMOGRAPHIC DATA,3170612,R01CA032743,"['X ray', ' computer simulation', ' human subject', ' mathematical model', ' mathematics', ' tomography']",NCI,TUFTS UNIVERSITY MEDFORD,R01,1985,23559,0.09789161453365432
"MODEL MISSPECIFICATION DETECTION IN DIPOLE ANALYSIS DESCRIPTION (Adapted from applicant's abstract): This project is intended to  provide computational resources to users of Dipole Source Localization (DSL)  methods to allow them to decide whether or not their DSL results are distorted  by certain errors called model misspecification errors. These errors are  essentially due to incorrect weighing constants in the computation of the  relationship between source dipole parameters and the surface potential. Such  errors occur when head model simplifications are used, such as a one-shell as  contrasted to a more realistic 3-shell model, and when a spherical model is  employed instead of a more realistic head model. Errors in single dipole localization are magnified when multiple dipoles are required to model the  sources of brain potentials.  A relatively simple method for estimating  misspecification errors has been devised (called the AB-plane technique) and  examined using simulated data for models using one or two dipoles. The method permits the identification of models that are insufficiently accurate to produce valid DSL, and to compare models for their relative accuracy. In order  to use the method on real (as contrasted to simulated) data in which the  contributions of each dipole is unknown an extension of the AB-plane estimation technique called the EF-plane method is proposed. Although accurate  definition of the EF plane is not possible, since the variables that define it  are unknown, it is proposed that a close enough estimate might be obtained  using singular value decomposition. It is indicated that even if this approach is not entirely accurate, that it will nevertheless be sufficiently close to  the AB-plane estimate to disclose model misspecification errors.  n/a",MODEL MISSPECIFICATION DETECTION IN DIPOLE ANALYSIS,2272266,R43NS033439,"['brain electrical activity', ' computational neuroscience', ' dipole moment', ' electric field', ' model design /development']",NINDS,ABRATECH CORPORATION,R43,1994,72953,0.23655643575532317
"COGNITIVE TOOLS-PROBLEM SOLVING IN TRANSFUSION MEDICINE We are currently completing a study of the cognitive processes involved in problem-solving tasks that arise in transfusion services laboratories. This study has included a series of empirical studies of both medical technology students and practitioners in order to identify cognitive errors and inefficiencies associated with such problem-solving tasks and to provide the basis for developing cognitive models of expert and less-than-expert performances.  These studies raise serious questions about current education and practice.  We have found that there is little agreement among laboratories on procedures for such basic tasks as antibody identification. (Indeed some laboratories have no standard protocols at all for such tasks.) We have also found very significant differences among practitioners in terms of their levels of expertise.  Finally, we have found that even highly expert practitioners make slips during problem solving.  We therefore propose a combination study which will build upon these results to develop a tool kit to:  1. Support problem-solving activities in the laboratory in order to reduce errors and improve efficiency;  2.  Provide embedded training to increase the expertise of practitioners while on the job.  This tool kit will provide a set of cognitive tools that the practitioner can access, and will be an enhancement of a functional tutoring system, (the Transfusion Medicine Tutor) that we have already developed.  The final product will be a prototype tool kit which has been extensively evaluated, as well as documentation of our design process (which can serve as a model for developing other medical support systems).  n/a",COGNITIVE TOOLS-PROBLEM SOLVING IN TRANSFUSION MEDICINE,3355130,R01HL038776,"['artificial intelligence', ' blood transfusion', ' cognition', ' computer human interaction', ' computer program /software', ' decision making', ' health science profession', ' human subject', ' method development', ' problem solving', ' questionnaires', ' training aid']",NHLBI,OHIO STATE UNIVERSITY,R01,1993,272112,0.020374519128290882
"COGNITIVE TOOLS-PROBLEM SOLVING IN TRANSFUSION MEDICINE We are currently completing a study of the cognitive processes involved in problem-solving tasks that arise in transfusion services laboratories. This study has included a series of empirical studies of both medical technology students and practitioners in order to identify cognitive errors and inefficiencies associated with such problem-solving tasks and to provide the basis for developing cognitive models of expert and less-than-expert performances.  These studies raise serious questions about current education and practice.  We have found that there is little agreement among laboratories on procedures for such basic tasks as antibody identification. (Indeed some laboratories have no standard protocols at all for such tasks.) We have also found very significant differences among practitioners in terms of their levels of expertise.  Finally, we have found that even highly expert practitioners make slips during problem solving.  We therefore propose a combination study which will build upon these results to develop a tool kit to:  1. Support problem-solving activities in the laboratory in order to reduce errors and improve efficiency;  2.  Provide embedded training to increase the expertise of practitioners while on the job.  This tool kit will provide a set of cognitive tools that the practitioner can access, and will be an enhancement of a functional tutoring system, (the Transfusion Medicine Tutor) that we have already developed.  The final product will be a prototype tool kit which has been extensively evaluated, as well as documentation of our design process (which can serve as a model for developing other medical support systems).  n/a",COGNITIVE TOOLS-PROBLEM SOLVING IN TRANSFUSION MEDICINE,3355125,R01HL038776,"['artificial intelligence', ' blood transfusion', ' cognition', ' computer human interaction', ' decision making', ' health science profession', ' human subject', ' method development', ' problem solving', ' questionnaires', ' training aid']",NHLBI,OHIO STATE UNIVERSITY,R01,1992,286404,0.020374519128290882
"Decoding the dynamic representation of reward predictions across mesocorticostriatal circuits during learning Reward learning is a fundamental cognitive function, and the brain has a dedicated neuromodulatory system – based on dopamine – that supports this process. Changes to the dopamine system that are triggered by exposure to drugs of abuse are thought to underlie the behavioral changes observed in addiction. Here we propose to use a treasure trove of previously recorded neural data from throughout the mesocorticostriatal circuitry that supports reward learning, to elucidate the computational role of each component of the circuit, their interactions, and how these components are affected by cocaine. Our brains constantly generate predictions about what rewards might be available, and compare these predictions to actual outcomes. The neuromodulator dopamine is thought to report these ‘prediction error’ signals, the result of the ongoing comparison between expected and obtained rewards, that are key to updating predictions so they are more accurate in the future. Predicting the timing of rewards, and not just their identity or value, is an important component of this process, but it remains a mystery how the brain forms and uses predictions about time in reward learning. Based on a novel theoretical model we recently developed, we will test the computational role of three key brain areas that comprise the brain circuit critical for reward learning, using a state-of-the- art methods from machine learning to jointly decode the learning processes that drive neural activity from multiple brain areas along with behavior as rats perform a reward learning task. In Aim 1, we hypothesize that neural activity in the orbitofrontal cortex is uniquely important for representing high level ‘task states’ and will test for patterns in OFC neural activity that follow the hidden structure of the task. In Aim 2, we will decode the representation of reward predictions about the amount and timing of rewards, and test whether they are separable in VS neural activity. In Aim 3, we will test how activity in VS and OFC controls dopamine activity, and in particular how each input component enables prediction errors to be temporally precise. In Aim 4, we will test how exposure to cocaine changes neural activity that represents reward predictions in the VS, and the impact of this disruption on dopamine prediction errors in the VTA. This innovative multi-level study will leverage numerous existing neural and behavioral data from rats performing a well-validated reward-learning task, to reveal the computational, neural and behavioral mechanisms of the reward prediction and learning circuitry in the brain, and the source of their disruption in addiction. Dysfunction of mesolimbic and corticostriatal circuitry is implicated in addiction, and is known to produce critical deficits in timing (for example, impulsivity) and reward learning. Here we will use a treasure trove of 8 existing datasets to test novel predictions about the neural basis of timing and reward learning, employing state-of-the-art tools from machine learning to ‘decode’ reward predictions jointly from multiple brain areas and from behavior, in intact neural circuits as well as after damage from lesions or exposure to cocaine. Our findings will lay bare the neural computations that support reward prediction and will allow us to link aberrant reward learning in addiction back to its basis in the circuitry of the brain.",Decoding the dynamic representation of reward predictions across mesocorticostriatal circuits during learning,9947251,R01DA050647,"['Adaptive Behaviors', 'Affect', 'Animals', 'Area', 'Back', 'Basal Ganglia', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Behavioral Mechanisms', 'Behavioral Model', 'Brain', 'Cocaine', 'Collaborations', 'Computer Models', 'Data', 'Data Set', 'Disease', 'Dopamine', 'Event', 'Exposure to', 'Functional disorder', 'Future', 'Impairment', 'Impulsivity', 'International', 'Joints', 'Learning', 'Lesion', 'Link', 'Machine Learning', 'Neuromodulator', 'Neurons', 'Odors', 'Outcome', 'Pattern', 'Process', 'Rattus', 'Reporting', 'Research', 'Rewards', 'Role', 'Shapes', 'Signal Transduction', 'Source', 'Structure', 'System', 'Techniques', 'Testing', 'Theoretical model', 'Time', 'Update', 'Ventral Striatum', 'Ventral Tegmental Area', 'Work', 'addiction', 'base', 'brain circuitry', 'cocaine exposure', 'cognitive function', 'computer framework', 'cost', 'dopamine system', 'drug of abuse', 'experience', 'frontal lobe', 'innovation', 'machine learning method', 'neural circuit', 'neural correlate', 'neuromechanism', 'neurophysiology', 'neuroregulation', 'novel', 'programs', 'relating to nervous system', 'reward circuitry', 'synergism', 'theories', 'tool']",NIDA,PRINCETON UNIVERSITY,R01,2020,283500,0.009562789202374182
"High Resolution in Single Particle Reconstruction DESCRIPTION (provided by applicant): The focus of this renewal application will be on the development of single particle cryo-EM structure determination methods that incorporate validation, assessment of alignment errors, and cross-validation of observed conformational variability. We will concentrate on three specific areas: (1) establishment of a well- defined goal function for structure determination, (2) structure refinement methods that incorporate validation of the outcome, and (3) quantitative analysis of conformational variability cross-validated by X-ray model-based simulations.  In (1), we will establish a goal function (a single-valued function of the 3D map and/or projection data) that would have a global extremum for the correct structure and which can be related to intuitive notion of resolution of the map. A goal function with such properties would make possible a critical evaluation of the ability of existing structure determination methods to deliver optimal structures and establish a theoretical basis for unification and rationalization of 3D-EM single particle structure determination methodology. In (2), we will introduce novel quantitative single particle cryo-EM methodology, based on jackknife-d resampling, designed to yield an estimate of alignment errors and eliminate ""reference bias"" that results in artifactual features that are indistinguishable from genuine ones in the absence of external standards. Incorporation of these developments into cryo-EM structure determination algorithms will make possible objective validation of a refined 3D map. They will provide, for the first time, a simple but robust way to eliminate artifacts and will thus increase the level of confidence in cryo-EM results. In (3), we will use our projection data resampling methodology to calculate (directly from the data) eigenvectors characterizing the conformational variability of a structure. We will then develop a deconvolution algorithm that will use this eigenvector information to eliminate from a 3D map the blurring caused by residual alignment errors. We will also use the eigenanalysis to characterize local mobility of a macromolecule and bridge the gap between experimental cryo-EM structure determination and simulations of conformational variability based on physical models. By cross-validating our methodology with the results of molecular dynamics simulations we will provide a novel tool for analyzing the energy landscape of large macromolecules.  Rather than incremental improvements, the methods we propose to develop will put single particle cryo-EM analysis on a new path towards full reliability of the results, eliminating the uncertainty that currently hinder fulfillment of cryo-EM's full potential. To assure multi-platform portability and immediate dissemination, these new methods will be implemented within the SPARX image processing package. PUBLIC HEALTH RELEVANCE: High-resolution cryo-electron microscopy (cryo-EM) has become an important tool for the structure/function determination of large macromolecular complexes. Even at limited resolution cryo-EM maps provide a wealth of structural information, eventually leading to determination of the secondary structure, as demonstrated by our work on the structure of the ribosome. In addition, cryo-EM is a unique structural technique in its ability to detect conformational variability of large molecular assemblies within one sample that may contain a mixture of complexes in various conformational states. We propose development of dedicated data processing and statistical tools for reliable cryo-EM structure determination, particularly in the absence of external information, and for studies of conformational modes of the structure, as directly obtained from the EM data.",High Resolution in Single Particle Reconstruction,9132862,R01GM060635,"['Algorithms', 'Area', 'Berlin', 'Complex', 'Complex Mixtures', 'Computing Methodologies', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Development', 'Electron Microscopy', 'Elements', 'Environment', 'Evaluation', 'Foundations', 'Freezing', 'Genetic Transcription', 'Goals', 'Health', 'Image', 'Laboratories', 'Libraries', 'Ligand Binding', 'Link', 'Macromolecular Complexes', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular Conformation', 'Morphologic artifacts', 'Noise', 'Outcome', 'Peptide Elongation Factor G', 'Principal Component Analysis', 'Procedures', 'Property', 'Rationalization', 'Reliability of Results', 'Reproducibility', 'Research Institute', 'Residual state', 'Resolution', 'Ribosomes', 'Rice', 'Roentgen Rays', 'Sampling', 'Specimen', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Uncertainty', 'Universities', 'Validation', 'Work', 'base', 'blind', 'computerized data processing', 'computerized tools', 'design', 'image processing', 'macromolecular assembly', 'macromolecule', 'microscopic imaging', 'models and simulation', 'molecular assembly/self assembly', 'molecular dynamics', 'novel', 'particle', 'physical model', 'portability', 'programs', 'reconstruction', 'simulation', 'three dimensional structure', 'three-dimensional modeling', 'tool']",NIGMS,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2016,304000,0.0709572879192078
"High Resolution in Single Particle Reconstruction DESCRIPTION (provided by applicant): The focus of this renewal application will be on the development of single particle cryo-EM structure determination methods that incorporate validation, assessment of alignment errors, and cross-validation of observed conformational variability. We will concentrate on three specific areas: (1) establishment of a well- defined goal function for structure determination, (2) structure refinement methods that incorporate validation of the outcome, and (3) quantitative analysis of conformational variability cross-validated by X-ray model-based simulations.  In (1), we will establish a goal function (a single-valued function of the 3D map and/or projection data) that would have a global extremum for the correct structure and which can be related to intuitive notion of resolution of the map. A goal function with such properties would make possible a critical evaluation of the ability of existing structure determination methods to deliver optimal structures and establish a theoretical basis for unification and rationalization of 3D-EM single particle structure determination methodology. In (2), we will introduce novel quantitative single particle cryo-EM methodology, based on jackknife-d resampling, designed to yield an estimate of alignment errors and eliminate ""reference bias"" that results in artifactual features that are indistinguishable from genuine ones in the absence of external standards. Incorporation of these developments into cryo-EM structure determination algorithms will make possible objective validation of a refined 3D map. They will provide, for the first time, a simple but robust way to eliminate artifacts and will thus increase the level of confidence in cryo-EM results. In (3), we will use our projection data resampling methodology to calculate (directly from the data) eigenvectors characterizing the conformational variability of a structure. We will then develop a deconvolution algorithm that will use this eigenvector information to eliminate from a 3D map the blurring caused by residual alignment errors. We will also use the eigenanalysis to characterize local mobility of a macromolecule and bridge the gap between experimental cryo-EM structure determination and simulations of conformational variability based on physical models. By cross-validating our methodology with the results of molecular dynamics simulations we will provide a novel tool for analyzing the energy landscape of large macromolecules.  Rather than incremental improvements, the methods we propose to develop will put single particle cryo-EM analysis on a new path towards full reliability of the results, eliminating the uncertainty that currently hinder fulfillment of cryo-EM's full potential. To assure multi-platform portability and immediate dissemination, these new methods will be implemented within the SPARX image processing package. PUBLIC HEALTH RELEVANCE: High-resolution cryo-electron microscopy (cryo-EM) has become an important tool for the structure/function determination of large macromolecular complexes. Even at limited resolution cryo-EM maps provide a wealth of structural information, eventually leading to determination of the secondary structure, as demonstrated by our work on the structure of the ribosome. In addition, cryo-EM is a unique structural technique in its ability to detect conformational variability of large molecular assemblies within one sample that may contain a mixture of complexes in various conformational states. We propose development of dedicated data processing and statistical tools for reliable cryo-EM structure determination, particularly in the absence of external information, and for studies of conformational modes of the structure, as directly obtained from the EM data.",High Resolution in Single Particle Reconstruction,8926448,R01GM060635,"['Algorithms', 'Area', 'Berlin', 'Complex', 'Complex Mixtures', 'Computing Methodologies', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Development', 'Electron Microscopy', 'Elements', 'Environment', 'Evaluation', 'Foundations', 'Freezing', 'Genetic Transcription', 'Goals', 'Health', 'Image', 'Laboratories', 'Libraries', 'Ligand Binding', 'Link', 'Macromolecular Complexes', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular Conformation', 'Morphologic artifacts', 'Noise', 'Outcome', 'Peptide Elongation Factor G', 'Principal Component Analysis', 'Procedures', 'Property', 'Rationalization', 'Reliability of Results', 'Reproducibility', 'Research Institute', 'Residual state', 'Resolution', 'Ribosomes', 'Rice', 'Roentgen Rays', 'Sampling', 'Solutions', 'Specimen', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Uncertainty', 'Universities', 'Validation', 'Work', 'base', 'blind', 'computerized data processing', 'computerized tools', 'design', 'image processing', 'macromolecular assembly', 'macromolecule', 'models and simulation', 'molecular assembly/self assembly', 'molecular dynamics', 'novel', 'particle', 'physical model', 'portability', 'programs', 'reconstruction', 'simulation', 'three dimensional structure', 'three-dimensional modeling', 'tool']",NIGMS,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2015,304000,0.0709572879192078
"High Resolution in Single Particle Reconstruction     DESCRIPTION (provided by applicant): The focus of this renewal application will be on the development of single particle cryo-EM structure determination methods that incorporate validation, assessment of alignment errors, and cross-validation of observed conformational variability. We will concentrate on three specific areas: (1) establishment of a well- defined goal function for structure determination, (2) structure refinement methods that incorporate validation of the outcome, and (3) quantitative analysis of conformational variability cross-validated by X-ray model-based simulations.  In (1), we will establish a goal function (a single-valued function of the 3D map and/or projection data) that would have a global extremum for the correct structure and which can be related to intuitive notion of resolution of the map. A goal function with such properties would make possible a critical evaluation of the ability of existing structure determination methods to deliver optimal structures and establish a theoretical basis for unification and rationalization of 3D-EM single particle structure determination methodology. In (2), we will introduce novel quantitative single particle cryo-EM methodology, based on jackknife-d resampling, designed to yield an estimate of alignment errors and eliminate ""reference bias"" that results in artifactual features that are indistinguishable from genuine ones in the absence of external standards. Incorporation of these developments into cryo-EM structure determination algorithms will make possible objective validation of a refined 3D map. They will provide, for the first time, a simple but robust way to eliminate artifacts and will thus increase the level of confidence in cryo-EM results. In (3), we will use our projection data resampling methodology to calculate (directly from the data) eigenvectors characterizing the conformational variability of a structure. We will then develop a deconvolution algorithm that will use this eigenvector information to eliminate from a 3D map the blurring caused by residual alignment errors. We will also use the eigenanalysis to characterize local mobility of a macromolecule and bridge the gap between experimental cryo-EM structure determination and simulations of conformational variability based on physical models. By cross-validating our methodology with the results of molecular dynamics simulations we will provide a novel tool for analyzing the energy landscape of large macromolecules.  Rather than incremental improvements, the methods we propose to develop will put single particle cryo-EM analysis on a new path towards full reliability of the results, eliminating the uncertainty that currently hinder fulfillment of cryo-EM's full potential. To assure multi-platform portability and immediate dissemination, these new methods will be implemented within the SPARX image processing package.         PUBLIC HEALTH RELEVANCE: High-resolution cryo-electron microscopy (cryo-EM) has become an important tool for the structure/function determination of large macromolecular complexes. Even at limited resolution cryo-EM maps provide a wealth of structural information, eventually leading to determination of the secondary structure, as demonstrated by our work on the structure of the ribosome. In addition, cryo-EM is a unique structural technique in its ability to detect conformational variability of large molecular assemblies within one sample that may contain a mixture of complexes in various conformational states. We propose development of dedicated data processing and statistical tools for reliable cryo-EM structure determination, particularly in the absence of external information, and for studies of conformational modes of the structure, as directly obtained from the EM data.            ",High Resolution in Single Particle Reconstruction,8744276,R01GM060635,"['Algorithms', 'Area', 'Berlin', 'Complex', 'Complex Mixtures', 'Computing Methodologies', 'Cryoelectron Microscopy', 'Data', 'Data Set', 'Development', 'Electron Microscopy', 'Elements', 'Environment', 'Evaluation', 'Foundations', 'Freezing', 'Genetic Transcription', 'Goals', 'Image', 'Laboratories', 'Libraries', 'Ligand Binding', 'Link', 'Macromolecular Complexes', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular Conformation', 'Morphologic artifacts', 'Noise', 'Outcome', 'Peptide Elongation Factor G', 'Principal Component Analysis', 'Procedures', 'Property', 'Rationalization', 'Reliability of Results', 'Reproducibility', 'Research Institute', 'Residual state', 'Resolution', 'Ribosomes', 'Rice', 'Roentgen Rays', 'Sampling', 'Solutions', 'Specimen', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Uncertainty', 'Universities', 'Validation', 'Work', 'base', 'blind', 'computerized data processing', 'computerized tools', 'design', 'image processing', 'macromolecular assembly', 'macromolecule', 'models and simulation', 'molecular assembly/self assembly', 'molecular dynamics', 'novel', 'particle', 'physical model', 'portability', 'programs', 'public health relevance', 'reconstruction', 'simulation', 'three dimensional structure', 'three-dimensional modeling', 'tool']",NIGMS,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2014,304000,0.0709572879192078
"Intelligent and Automatic Image Segmentation Software for High ThroughputAnalysi     DESCRIPTION (provided by applicant): It is well established that aging and many chronic diseases, such as cancer and heart failure, are associated with significant losses in skeletal muscle mass and strength in humans. There is agreement across the muscle biology community that important morphological characteristics of muscle fibers, such as fiber area, the number and position of myonuclei, cellular infiltration and fibrosis are critical factors that determine the health and function of the muscle. However, at this time, quantification of muscle characteristics from standard histological and immunohistological techniques is still a manual or, at best, a semi-automatic process. This process is labor intensive and can be prone to errors, leading to high inter-observer variability. On the other hand, when muscle characteristics are calculated by computer-aided image analysis, data acquisition times decrease and objectivity improves significantly. The objective of this Phase I STTR project is to build a fully automatic, intelligent, and high throughput image acquisition and analysis software for quantitative muscle morphological analysis on digitized muscle cross-sections. We propose to utilize the most recent technical advances in machine learning and biomedical image analysis. This includes a newly developed deformable model and mean-shift based seed detection algorithm for better segmentation accuracy; an asymmetric online boosting based machine learning algorithm which allows the software to learn from errors and adjust its segmentation strategies adaptively; and a data parallelization schema using the graphic processing unit (GPU) to handle the computational bottleneck for extremely large scale image, such as whole slide scanned specimens. We believe that this software, equipped with the most advanced technical innovations, will be commercially attractive for the skeletal muscle research community including basic scientists, clinician scientists, and the pharmaceutical industry. The specific aim are: 1) Develop, implement, and validate an automatic biological image analysis software package for skeletal muscle tissue; 2) Develop a novel online updated intelligent artificial intelligence unit to enable the software to learn from errors; 3) Build a novel high performance computing unit to enable fast and high throughput automatic image analysis, which is capable of processing whole slide scanned muscle specimens. The analysis approach proposed will provide more consistent, accurate, and objective quantification of skeletal muscle morphological properties and the time for data analysis will be reduced by over a factor of 100 for standalone version and 2000 for parallel version. The long-term goal of Cytoinformatics, LLC for the Phase II stage is to apply the software to analyze histology/pathology from human muscle biopsy samples and extension of the software to other biological tissues, such as adipose tissue.         PUBLIC HEALTH RELEVANCE: Important features of muscle fibers, such as fiber area, the number and position of myonuclei, cellular infiltration and fibrosis are critical factors that determine the health of the muscle. However quantification of muscle features from digitized images is still a manual or, at best, a semi-automatic process. The objective of this Phase I STTR project is to build software using the most recent technical advances in machine learning and biomedical image analyses to significantly move the skeletal muscle basic and clinical research fields ahead.            ",Intelligent and Automatic Image Segmentation Software for High ThroughputAnalysi,8699686,R41AR064596,"['Adipose tissue', 'Aging', 'Agreement', 'Algorithms', 'Appearance', 'Area', 'Artificial Intelligence', 'Basic Science', 'Biological', 'Biology', 'Biometry', 'Biopsy Specimen', 'Cell Nucleus', 'Cellular Infiltration', 'Characteristics', 'Chronic Disease', 'Clinical Research', 'Communities', 'Computational Science', 'Computer Assisted', 'Computer software', 'Data', 'Data Analyses', 'Defect', 'Detection', 'Drug Industry', 'E-learning', 'Eosine Yellowish', 'Exhibits', 'Fiber', 'Fibrosis', 'Freezing', 'Future', 'Goals', 'Health', 'Heart failure', 'Hematoxylin', 'High Performance Computing', 'Histology', 'Human', 'Human Pathology', 'Image', 'Image Analysis', 'Interobserver Variability', 'Intervention', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Morphologic artifacts', 'Muscle', 'Muscle Fibers', 'Muscle function', 'NIH Program Announcements', 'Performance', 'Phase', 'Pilot Projects', 'Positioning Attribute', 'Process', 'Property', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Seeds', 'Shapes', 'Site', 'Skeletal Muscle', 'Slide', 'Small Business Technology Transfer Research', 'Specimen', 'Speed', 'Staging', 'Staining method', 'Stains', 'Techniques', 'Technology', 'Time', 'Tissues', 'United States National Institutes of Health', 'Update', 'Yang', 'base', 'bioimaging', 'biomedical informatics', 'cohort', 'computer science', 'data acquisition', 'design', 'fluorescence imaging', 'imaging Segmentation', 'improved', 'innovation', 'muscle form', 'muscle strength', 'novel', 'public health relevance']",NIAMS,"CYTOINFORMATICS, LLC",R41,2014,142837,0.06642686302556988
"Intelligent and Automatic Image Segmentation Software for High ThroughputAnalysi     DESCRIPTION (provided by applicant): It is well established that aging and many chronic diseases, such as cancer and heart failure, are associated with significant losses in skeletal muscle mass and strength in humans. There is agreement across the muscle biology community that important morphological characteristics of muscle fibers, such as fiber area, the number and position of myonuclei, cellular infiltration and fibrosis are critical factors that determine the health and function of the muscle. However, at this time, quantification of muscle characteristics from standard histological and immunohistological techniques is still a manual or, at best, a semi-automatic process. This process is labor intensive and can be prone to errors, leading to high inter-observer variability. On the other hand, when muscle characteristics are calculated by computer-aided image analysis, data acquisition times decrease and objectivity improves significantly. The objective of this Phase I STTR project is to build a fully automatic, intelligent, and high throughput image acquisition and analysis software for quantitative muscle morphological analysis on digitized muscle cross-sections. We propose to utilize the most recent technical advances in machine learning and biomedical image analysis. This includes a newly developed deformable model and mean-shift based seed detection algorithm for better segmentation accuracy; an asymmetric online boosting based machine learning algorithm which allows the software to learn from errors and adjust its segmentation strategies adaptively; and a data parallelization schema using the graphic processing unit (GPU) to handle the computational bottleneck for extremely large scale image, such as whole slide scanned specimens. We believe that this software, equipped with the most advanced technical innovations, will be commercially attractive for the skeletal muscle research community including basic scientists, clinician scientists, and the pharmaceutical industry. The specific aim are: 1) Develop, implement, and validate an automatic biological image analysis software package for skeletal muscle tissue; 2) Develop a novel online updated intelligent artificial intelligence unit to enable the software to learn from errors; 3) Build a novel high performance computing unit to enable fast and high throughput automatic image analysis, which is capable of processing whole slide scanned muscle specimens. The analysis approach proposed will provide more consistent, accurate, and objective quantification of skeletal muscle morphological properties and the time for data analysis will be reduced by over a factor of 100 for standalone version and 2000 for parallel version. The long-term goal of Cytoinformatics, LLC for the Phase II stage is to apply the software to analyze histology/pathology from human muscle biopsy samples and extension of the software to other biological tissues, such as adipose tissue.         PUBLIC HEALTH RELEVANCE: Important features of muscle fibers, such as fiber area, the number and position of myonuclei, cellular infiltration and fibrosis are critical factors that determine the health of the muscle. However quantification of muscle features from digitized images is still a manual or, at best, a semi-automatic process. The objective of this Phase I STTR project is to build software using the most recent technical advances in machine learning and biomedical image analyses to significantly move the skeletal muscle basic and clinical research fields ahead.            ",Intelligent and Automatic Image Segmentation Software for High ThroughputAnalysi,8522756,R41AR064596,"['Adipose tissue', 'Aging', 'Agreement', 'Algorithms', 'Appearance', 'Area', 'Artificial Intelligence', 'Basic Science', 'Biological', 'Biology', 'Biometry', 'Biopsy Specimen', 'Cell Nucleus', 'Cellular Infiltration', 'Characteristics', 'Chronic Disease', 'Clinical Research', 'Communities', 'Computational Science', 'Computer Assisted', 'Computer software', 'Data', 'Data Analyses', 'Defect', 'Detection', 'Drug Industry', 'Eosine Yellowish', 'Exhibits', 'Fiber', 'Fibrosis', 'Freezing', 'Future', 'Goals', 'Health', 'Heart failure', 'Hematoxylin', 'High Performance Computing', 'Histology', 'Human', 'Human Pathology', 'Image', 'Image Analysis', 'Interobserver Variability', 'Intervention', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Morphologic artifacts', 'Muscle', 'Muscle Fibers', 'Muscle function', 'NIH Program Announcements', 'Performance', 'Phase', 'Pilot Projects', 'Positioning Attribute', 'Process', 'Property', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Seeds', 'Shapes', 'Site', 'Skeletal Muscle', 'Slide', 'Small Business Technology Transfer Research', 'Specimen', 'Speed', 'Staging', 'Staining method', 'Stains', 'Techniques', 'Technology', 'Time', 'Tissues', 'United States National Institutes of Health', 'Update', 'Yang', 'base', 'bioimaging', 'biomedical informatics', 'cohort', 'computer science', 'data acquisition', 'design', 'fluorescence imaging', 'imaging Segmentation', 'improved', 'innovation', 'muscle form', 'muscle strength', 'novel', 'public health relevance']",NIAMS,"CYTOINFORMATICS, LLC",R41,2013,150000,0.06642686302556988
"Paperless Quality Donor System with Decision Making    DESCRIPTION (provided by applicant):    The principal aim of this proposal is for Talisman to develop, implement and evaluate a computerized, integrated system for processing blood donors that incorporates a series of decision aids and decision algorithms.  The system is expected to significantly reduce donor and staff errors and omissions, increase blood availability, improve staff efficiency and have substantial commercial appeal to blood centers and blood collecting hospitals.      The secondary aim of the proposal is for Talisman to continue assessment of the effectiveness of its computer-assisted, audio-video donor health history self interviewing system, QDS, in reducing donor lying on sexual and other sensitive questions, increasing the frequency of donor returns, reducing staff errors and omissions, and increasing staff efficiency.         n/a",Paperless Quality Donor System with Decision Making,6745080,R44HL072635,"['artificial intelligence', 'blood bank /supply contamination', 'blood donor', 'case history', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'computer program /software', 'computer system design /evaluation', 'decision making', 'human subject', 'interview', 'patient safety /medical error', 'phlebotomy']",NHLBI,"TALISMAN, LTD",R44,2004,359477,0.0915790605985243
"Paperless Quality Donor System with Decision Making    DESCRIPTION (provided by applicant):    The principal aim of this proposal is for Talisman to develop, implement and evaluate a computerized, integrated system for processing blood donors that incorporates a series of decision aids and decision algorithms.  The system is expected to significantly reduce donor and staff errors and omissions, increase blood availability, improve staff efficiency and have substantial commercial appeal to blood centers and blood collecting hospitals.      The secondary aim of the proposal is for Talisman to continue assessment of the effectiveness of its computer-assisted, audio-video donor health history self interviewing system, QDS, in reducing donor lying on sexual and other sensitive questions, increasing the frequency of donor returns, reducing staff errors and omissions, and increasing staff efficiency.         n/a",Paperless Quality Donor System with Decision Making,6930165,R44HL072635,"['artificial intelligence', 'blood bank /supply contamination', 'blood donor', 'case history', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'computer program /software', 'computer system design /evaluation', 'decision making', 'human subject', 'interview', 'patient safety /medical error', 'phlebotomy']",NHLBI,"TALISMAN, LTD",R44,2004,394563,0.0915790605985243
"Paperless Quality Donor System with Decision Making    DESCRIPTION (provided by applicant):    The principal aim of this proposal is for Talisman to develop, implement and evaluate a computerized, integrated system for processing blood donors that incorporates a series of decision aids and decision algorithms.  The system is expected to significantly reduce donor and staff errors and omissions, increase blood availability, improve staff efficiency and have substantial commercial appeal to blood centers and blood collecting hospitals.      The secondary aim of the proposal is for Talisman to continue assessment of the effectiveness of its computer-assisted, audio-video donor health history self interviewing system, QDS, in reducing donor lying on sexual and other sensitive questions, increasing the frequency of donor returns, reducing staff errors and omissions, and increasing staff efficiency.         n/a",Paperless Quality Donor System with Decision Making,6728674,R44HL072635,"['artificial intelligence', ' blood bank /supply contamination', ' blood donor', ' case history', ' clinical research', ' computer assisted diagnosis', ' computer assisted medical decision making', ' computer program /software', ' computer system design /evaluation', ' decision making', ' human subject', ' interview', ' patient safety /medical error', ' phlebotomy']",NHLBI,"TALISMAN, LTD",R44,2003,514079,0.0915790605985243
"Paperless Quality Donor System with Decision Making    DESCRIPTION (provided by applicant):    The principal aim of this proposal is for Talisman to develop, implement and evaluate a computerized, integrated system for processing blood donors that incorporates a series of decision aids and decision algorithms.  The system is expected to significantly reduce donor and staff errors and omissions, increase blood availability, improve staff efficiency and have substantial commercial appeal to blood centers and blood collecting hospitals.      The secondary aim of the proposal is for Talisman to continue assessment of the effectiveness of its computer-assisted, audio-video donor health history self interviewing system, QDS, in reducing donor lying on sexual and other sensitive questions, increasing the frequency of donor returns, reducing staff errors and omissions, and increasing staff efficiency.         n/a",Paperless Quality Donor System with Decision Making,6585908,R44HL072635,"['artificial intelligence', ' blood bank /supply contamination', ' blood donor', ' case history', ' clinical research', ' computer assisted diagnosis', ' computer assisted medical decision making', ' computer program /software', ' computer system design /evaluation', ' decision making', ' human subject', ' interview', ' patient safety /medical error', ' phlebotomy']",NHLBI,"TALISMAN, LTD",R44,2003,119826,0.0915790605985243
"EHR-based patient safety: Automated error detection in neonatal intensive care     DESCRIPTION (provided by applicant): In the field of neonatal patient safety, the paucity of systematic research is a critical barrier to progress. Notably missing are studies that meticulously investigate Electronic Health Records (EHR) and information technology in detecting neonatal intensive care-related errors. The expert panel at the National Institute of Child Health and Human Development (NICHHD) identified multiple gaps in the current knowledge of neonatal patient safety research. The proposed work is a well focused response to three dimensions of the Funding Opportunity Announcement:  1.Develop prospective and retrospective study designs to collect data on patient safety and adverse events.  2.Study the strength and limitations of current methods of error reporting systems.  3.Study the usefulness of commercial IT systems and EHRs in reducing medical errors.  In our study we seek to shift patient safety research toward an automated and computerized approach to achieve a more comprehensive patient safety paradigm. We will develop novel Electronic Health Record (EHR) content-based automated algorithms that are new to patient safety research to 1) detect errors (Aim 1) and 2) categorize subsequent harm (Aim 2). State of the art information extraction and statistical classification techniques from the field of clinical Natural Language Processing (NLP) will be adapted to the patient safety research tasks.  In Aim 1 we will fill the gap in the literatre by implementing a focused manual review of 700 charts (one full year of patient admissions at our institution) in one of the largest Neonatal Intensive Care Units (NICU) in the nation. Using a trigger tool, we will identify errors occurring in three specified categories - laboratory test errrs, medication/fluid errors, and airway management errors. We will develop novel algorithms for automated EHR-based detection of the errors and evaluate the performance of the new algorithms against the performance of both trigger tool review by human chart reviewers (current gold standard) and the voluntary incident reporting system (accepted standard). In Aim 2, we will study the utility of novel EHR-based information extraction and statistical algorithms for the automated categorization of errors according to the resulting level of harm.  Our proposed work has the potential to accomplish a paradigm shift in the methods of neonatal patient safety research and practice. The study is a fundamental step to automating patient safety monitoring on a large scale and improving error identification and patient safety in NICUs for millions of children every year.          We are developing an automated error detection technique to improve the safety of newborn babies during hospital care. Our work is the first known attempt to use text analysis in the electronic health records on a large scale to reduce the cost while at the same time increase the speed and comprehensiveness of error detection in the clinical care of newborns.            ",EHR-based patient safety: Automated error detection in neonatal intensive care,8517787,R21HD072883,"['Adverse event', 'Algorithms', 'Caring', 'Categories', 'Cessation of life', 'Characteristics', 'Child', 'Child health care', 'Classification', 'Clinical', 'Data', 'Detection', 'Dimensions', 'Electronic Health Record', 'Foundations', 'Funding Opportunities', 'Gold', 'Hospitals', 'Hour', 'Human', 'Human Development', 'Information Systems', 'Information Technology', 'Institutes', 'Institution', 'Intervention', 'Knowledge', 'Laboratories', 'Liquid substance', 'Literature', 'Manuals', 'Medical Errors', 'Medication Errors', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Neonatal', 'Neonatal Intensive Care', 'Neonatal Intensive Care Units', 'Newborn Infant', 'Patient Admission', 'Patient Monitoring', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Prevention', 'Process', 'Prospective Studies', 'Reporting', 'Research', 'Research Design', 'Retrospective Studies', 'Risk', 'Safety', 'Solid', 'Specific qualifier value', 'Speed', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Work', 'base', 'clinical care', 'computerized', 'cost', 'design', 'experience', 'improved', 'indexing', 'innovation', 'novel', 'patient safety', 'phrases', 'response', 'tool']",NICHD,CINCINNATI CHILDRENS HOSP MED CTR,R21,2013,254095,0.11725986251989373
"EHR-based patient safety: Automated error detection in neonatal intensive care     DESCRIPTION (provided by applicant): In the field of neonatal patient safety, the paucity of systematic research is a critical barrier to progress. Notably missing are studies that meticulously investigate Electronic Health Records (EHR) and information technology in detecting neonatal intensive care-related errors. The expert panel at the National Institute of Child Health and Human Development (NICHHD) identified multiple gaps in the current knowledge of neonatal patient safety research. The proposed work is a well focused response to three dimensions of the Funding Opportunity Announcement:  1.Develop prospective and retrospective study designs to collect data on patient safety and adverse events.  2.Study the strength and limitations of current methods of error reporting systems.  3.Study the usefulness of commercial IT systems and EHRs in reducing medical errors.  In our study we seek to shift patient safety research toward an automated and computerized approach to achieve a more comprehensive patient safety paradigm. We will develop novel Electronic Health Record (EHR) content-based automated algorithms that are new to patient safety research to 1) detect errors (Aim 1) and 2) categorize subsequent harm (Aim 2). State of the art information extraction and statistical classification techniques from the field of clinical Natural Language Processing (NLP) will be adapted to the patient safety research tasks.  In Aim 1 we will fill the gap in the literatre by implementing a focused manual review of 700 charts (one full year of patient admissions at our institution) in one of the largest Neonatal Intensive Care Units (NICU) in the nation. Using a trigger tool, we will identify errors occurring in three specified categories - laboratory test errrs, medication/fluid errors, and airway management errors. We will develop novel algorithms for automated EHR-based detection of the errors and evaluate the performance of the new algorithms against the performance of both trigger tool review by human chart reviewers (current gold standard) and the voluntary incident reporting system (accepted standard). In Aim 2, we will study the utility of novel EHR-based information extraction and statistical algorithms for the automated categorization of errors according to the resulting level of harm.  Our proposed work has the potential to accomplish a paradigm shift in the methods of neonatal patient safety research and practice. The study is a fundamental step to automating patient safety monitoring on a large scale and improving error identification and patient safety in NICUs for millions of children every year.        PUBLIC HEALTH RELEVANCE: We are developing an automated error detection technique to improve the safety of newborn babies during hospital care. Our work is the first known attempt to use text analysis in the electronic health records on a large scale to reduce the cost while at the same time increase the speed and comprehensiveness of error detection in the clinical care of newborns.              We are developing an automated error detection technique to improve the safety of newborn babies during hospital care. Our work is the first known attempt to use text analysis in the electronic health records on a large scale to reduce the cost while at the same time increase the speed and comprehensiveness of error detection in the clinical care of newborns.            ",EHR-based patient safety: Automated error detection in neonatal intensive care,8334934,R21HD072883,"['Adverse event', 'Algorithms', 'Caring', 'Categories', 'Cessation of life', 'Characteristics', 'Child', 'Child health care', 'Classification', 'Clinical', 'Data', 'Detection', 'Dimensions', 'Electronic Health Record', 'Foundations', 'Funding Opportunities', 'Gold', 'Hospitals', 'Hour', 'Human', 'Human Development', 'Information Systems', 'Information Technology', 'Institutes', 'Institution', 'Intervention', 'Knowledge', 'Laboratories', 'Liquid substance', 'Literature', 'Manuals', 'Medical Errors', 'Medication Errors', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Neonatal', 'Neonatal Intensive Care', 'Neonatal Intensive Care Units', 'Newborn Infant', 'Patient Admission', 'Patient Monitoring', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Prevention', 'Process', 'Prospective Studies', 'Reporting', 'Research', 'Research Design', 'Retrospective Studies', 'Risk', 'Safety', 'Solid', 'Specific qualifier value', 'Speed', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Work', 'base', 'clinical care', 'computerized', 'cost', 'design', 'experience', 'improved', 'indexing', 'innovation', 'novel', 'patient safety', 'phrases', 'response', 'tool']",NICHD,CINCINNATI CHILDRENS HOSP MED CTR,R21,2012,153000,0.1232507884625926
"Real-time detection of deviations in clinical care in ICU data streams DESCRIPTION (provided by applicant): Timely detection of severe patient conditions or concerning events and their mitigation remains an important problem in clinical practice. This is especially true in the critically ill patient. Typical computer-based detection methods developed for this purpose rely on the use of clinical knowledge, such as expert-derived rules, that are incorporated into monitoring and alerting systems. However, it is often time-consuming, costly, and difficult to extract and implement such knowledge in existing monitoring systems. The research work in this proposal offers computational, rather than expert-based, solutions that build alert systems from data stored in patient data repositories, such as electronic medical records. Briefly, our approach uses advanced machine learning algorithms to identify unusual clinical management patterns in individual patients, relative to patterns associated with comparable patients, and raises an alert signaling this discrepancy. Our previous studies provide support that such deviations indicate clinically important events at false alert rates belo 50%, which is very promising. We propose to further improve the new methodology, and build a real-time monitoring and alerting system integrated with production electronic medical records. We propose an evaluation of the system using physicians' assessment of alerts raised by our real-time system for intensive-care unit (ICU) patient cases. The project investigators comprise a multidisciplinary team with expertise in critical care medicine, computer science, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories. PUBLIC HEALTH RELEVANCE: There remain numerous opportunities to reduce medical errors in the intensive care unit (ICU). This project develops and evaluates a new clinical monitoring and alerting framework that uses electronic medical records and machine-learning methods to send alerts concerning clinical decisions in the ICU that are unexpected given the clinical context and may represent medical errors.",Real-time detection of deviations in clinical care in ICU data streams,9278178,R01GM088224,"['Algorithms', 'Archives', 'Area', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Management', 'Clinical Trials', 'Complication', 'Computerized Medical Record', 'Computers', 'Critical Care', 'Critical Illness', 'Data', 'Databases', 'Detection', 'Development', 'Environment', 'Evaluation', 'Event', 'Feedback', 'Funding', 'Health Personnel', 'Healthcare', 'Hospitals', 'Immunosuppressive Agents', 'Inpatients', 'Intensive Care Units', 'Knowledge', 'Laboratories', 'Machine Learning', 'Medical', 'Medical Errors', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Operative Surgical Procedures', 'Oral', 'Outpatients', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Play', 'Practice Management', 'Production', 'Real-Time Systems', 'Records', 'Research', 'Research Personnel', 'Signal Transduction', 'Stream', 'System', 'Tacrolimus', 'Techniques', 'Testing', 'Time', 'United States National Institutes of Health', 'Work', 'base', 'biomedical informatics', 'clinical care', 'clinical data warehouse', 'clinical practice', 'computer science', 'cost', 'data archive', 'design', 'improved', 'individual patient', 'knowledge base', 'learning strategy', 'liver transplantation', 'multidisciplinary', 'prototype', 'public health relevance']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2017,543819,0.07464210491849949
"Real-time detection of deviations in clinical care in ICU data streams DESCRIPTION (provided by applicant): Timely detection of severe patient conditions or concerning events and their mitigation remains an important problem in clinical practice. This is especially true in the critically ill patient. Typical computer-based detection methods developed for this purpose rely on the use of clinical knowledge, such as expert-derived rules, that are incorporated into monitoring and alerting systems. However, it is often time-consuming, costly, and difficult to extract and implement such knowledge in existing monitoring systems. The research work in this proposal offers computational, rather than expert-based, solutions that build alert systems from data stored in patient data repositories, such as electronic medical records. Briefly, our approach uses advanced machine learning algorithms to identify unusual clinical management patterns in individual patients, relative to patterns associated with comparable patients, and raises an alert signaling this discrepancy. Our previous studies provide support that such deviations indicate clinically important events at false alert rates belo 50%, which is very promising. We propose to further improve the new methodology, and build a real-time monitoring and alerting system integrated with production electronic medical records. We propose an evaluation of the system using physicians' assessment of alerts raised by our real-time system for intensive-care unit (ICU) patient cases. The project investigators comprise a multidisciplinary team with expertise in critical care medicine, computer science, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories. PUBLIC HEALTH RELEVANCE: There remain numerous opportunities to reduce medical errors in the intensive care unit (ICU). This project develops and evaluates a new clinical monitoring and alerting framework that uses electronic medical records and machine-learning methods to send alerts concerning clinical decisions in the ICU that are unexpected given the clinical context and may represent medical errors.",Real-time detection of deviations in clinical care in ICU data streams,9095389,R01GM088224,"['Algorithms', 'Archives', 'Area', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Management', 'Clinical Trials', 'Complication', 'Computerized Medical Record', 'Computers', 'Critical Care', 'Critical Illness', 'Data', 'Databases', 'Decision Making', 'Detection', 'Development', 'Electronics', 'Environment', 'Evaluation', 'Event', 'Feedback', 'Funding', 'Health', 'Health Personnel', 'Healthcare', 'Hospitals', 'Immunosuppressive Agents', 'Information Systems', 'Inpatients', 'Intensive Care Units', 'Knowledge', 'Laboratories', 'Machine Learning', 'Medical', 'Medical Errors', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Operative Surgical Procedures', 'Oral', 'Outpatients', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Play', 'Practice Management', 'Production', 'Real-Time Systems', 'Records', 'Research', 'Research Personnel', 'Signal Transduction', 'Stream', 'System', 'Tacrolimus', 'Techniques', 'Testing', 'Time', 'United States National Institutes of Health', 'Work', 'base', 'biomedical informatics', 'clinical care', 'clinical practice', 'computer science', 'data archive', 'design', 'improved', 'individual patient', 'knowledge base', 'learning strategy', 'liver transplantation', 'multidisciplinary', 'prototype']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2016,548500,0.07464210491849949
"Real-time detection of deviations in clinical care in ICU data streams DESCRIPTION (provided by applicant): Timely detection of severe patient conditions or concerning events and their mitigation remains an important problem in clinical practice. This is especially true in the critically ill patient. Typical computer-based detection methods developed for this purpose rely on the use of clinical knowledge, such as expert-derived rules, that are incorporated into monitoring and alerting systems. However, it is often time-consuming, costly, and difficult to extract and implement such knowledge in existing monitoring systems. The research work in this proposal offers computational, rather than expert-based, solutions that build alert systems from data stored in patient data repositories, such as electronic medical records. Briefly, our approach uses advanced machine learning algorithms to identify unusual clinical management patterns in individual patients, relative to patterns associated with comparable patients, and raises an alert signaling this discrepancy. Our previous studies provide support that such deviations indicate clinically important events at false alert rates belo 50%, which is very promising. We propose to further improve the new methodology, and build a real-time monitoring and alerting system integrated with production electronic medical records. We propose an evaluation of the system using physicians' assessment of alerts raised by our real-time system for intensive-care unit (ICU) patient cases. The project investigators comprise a multidisciplinary team with expertise in critical care medicine, computer science, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories. PUBLIC HEALTH RELEVANCE: There remain numerous opportunities to reduce medical errors in the intensive care unit (ICU). This project develops and evaluates a new clinical monitoring and alerting framework that uses electronic medical records and machine-learning methods to send alerts concerning clinical decisions in the ICU that are unexpected given the clinical context and may represent medical errors.",Real-time detection of deviations in clinical care in ICU data streams,8912480,R01GM088224,"['Algorithms', 'Archives', 'Area', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Management', 'Clinical Trials', 'Complication', 'Computerized Medical Record', 'Computers', 'Critical Care', 'Critical Illness', 'Data', 'Databases', 'Decision Making', 'Detection', 'Development', 'Electronics', 'Environment', 'Evaluation', 'Event', 'Feedback', 'Funding', 'Health', 'Health Personnel', 'Healthcare', 'Hospitals', 'Immunosuppressive Agents', 'Individual', 'Information Systems', 'Inpatients', 'Intensive Care Units', 'Knowledge', 'Laboratories', 'Machine Learning', 'Medical', 'Medical Errors', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Operative Surgical Procedures', 'Oral', 'Outpatients', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Play', 'Practice Management', 'Production', 'Real-Time Systems', 'Records', 'Relative (related person)', 'Research', 'Research Personnel', 'Signal Transduction', 'Solutions', 'Stream', 'System', 'Tacrolimus', 'Techniques', 'Testing', 'Time', 'United States National Institutes of Health', 'Work', 'base', 'biomedical informatics', 'clinical care', 'clinical practice', 'computer science', 'design', 'improved', 'knowledge base', 'liver transplantation', 'multidisciplinary', 'prototype']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2015,583203,0.07464210491849949
"Real-time detection of deviations in clinical care in ICU data streams  PROJECT SUMMARY / ABSTRACT Timely detection of severe patient conditions or concerning events and their mitigation remains an important problem in clinical practice. This is especially true in the critically ill patient. Typical computer-based detection methods developed for this purpose rely on the use of clinical knowledge, such as expert-derived rules, that are incorporated into monitoring and alerting systems. However, it is often time-consuming, costly, and difficult to extract and implement such knowledge in existing monitoring systems. The research work in this proposal offers computational, rather than expert-based, solutions that build alert systems from data stored in patient data repositories, such as electronic medical records. Briefly, our approach uses advanced machine learning algorithms to identify unusual clinical management patterns in individual patients, relative to patterns associated with comparable patients, and raises an alert signaling this discrepancy. Our previous studies provide support that such deviations indicate clinically important events at false alert rates below 50%, which is very promising. We propose to further improve the new methodology, and build a real-time monitoring and alerting system integrated with production electronic medical records. We propose an evaluation of the system using physicians' assessment of alerts raised by our real-time system for intensive-care unit (ICU) patient cases. The project investigators comprise a multidisciplinary team with expertise in critical care medicine, computer science, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories. PUBLIC HEALTH RELEVANCE: There remain numerous opportunities to reduce medical errors in the intensive care unit (ICU). This project develops and evaluates a new clinical monitoring and alerting framework that uses electronic medical records and machine-learning methods to send alerts concerning clinical decisions in the ICU that are unexpected given the clinical context and may represent medical errors.                ",Real-time detection of deviations in clinical care in ICU data streams,8641014,R01GM088224,"['Algorithms', 'Archives', 'Area', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Management', 'Clinical Trials', 'Complication', 'Computerized Medical Record', 'Computers', 'Critical Care', 'Critical Illness', 'Data', 'Databases', 'Decision Making', 'Detection', 'Development', 'Electronics', 'Environment', 'Evaluation', 'Event', 'Feedback', 'Funding', 'Health Personnel', 'Healthcare', 'Hospitals', 'Immunosuppressive Agents', 'Individual', 'Information Systems', 'Inpatients', 'Intensive Care Units', 'Knowledge', 'Laboratories', 'Machine Learning', 'Medical', 'Medical Errors', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Operative Surgical Procedures', 'Oral', 'Outpatients', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Play', 'Practice Management', 'Production', 'Real-Time Systems', 'Records', 'Relative (related person)', 'Research', 'Research Personnel', 'Signal Transduction', 'Solutions', 'Stream', 'System', 'Tacrolimus', 'Techniques', 'Testing', 'Time', 'United States National Institutes of Health', 'Work', 'base', 'biomedical informatics', 'clinical care', 'clinical practice', 'computer science', 'design', 'improved', 'knowledge base', 'liver transplantation', 'multidisciplinary', 'prototype', 'public health relevance']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2014,580180,0.07464210491849949
"Detecting deviations in clinical care in ICU data streams    DESCRIPTION (provided by applicant): Timely detection of severe patient conditions or concerning events and their mitigation remains an important problem in clinical practice. This is especially true in the critically ill patient [1]. Typical computer-based detection methods developed for this purpose rely on the use of clinical knowledge, such as expert-derived rules, that are incorporated into monitoring and alerting systems. However, it is often time-consuming, costly, and difficult to extract and implement such knowledge in existing monitoring systems. The research work in this proposal offers computational, rather than expert-based, solutions that build alert systems from data stored in patient data repositories, such as electronic medical records. Briefly, our approach uses advanced machine learning algorithms to identify unusual clinical management patterns in individual patients, relative to patterns associated with comparable patients, and raises an alert signaling this discrepancy. Our preliminary studies provide support that such deviations often indicate clinically important events for which it is worthwhile to raise an alert. We propose an evaluation based on physician assessment of alerts that are generated from a retrospective set of intensive-care unit (ICU) patient cases. The project investigators comprise a multidisciplinary team with expertise in critical care medicine, computer science, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories.    PUBLIC HEALTH RELEVANCE: There remain numerous opportunities to reduce medical errors in critical care by sending computer-based reminders and alerts to clinicians. This project uses past patient data, which is stored in electronic form, and machine-learning methods to help develop and refine computer-based alerts to improve healthcare quality and reduce costs.           PROJECT NARRATIVE There remain numerous opportunities to reduce medical errors in critical care by sending computer-based reminders and alerts to clinicians. This project uses past patient data, which is stored in electronic form, and machine-learning methods to help develop and refine computer-based alerts to improve healthcare quality and reduce costs.",Detecting deviations in clinical care in ICU data streams,7698505,R01GM088224,"['Algorithms', 'Anticoagulants', 'Belief', 'Cardiac', 'Caring', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Management', 'Computerized Medical Record', 'Computers', 'Critical Care', 'Critical Illness', 'Data', 'Databases', 'Detection', 'Development', 'Disadvantaged', 'Drops', 'Electronics', 'Evaluation', 'Event', 'Future', 'Healthcare', 'Heparin', 'Individual', 'Industry', 'Information Systems', 'Intensive Care Units', 'Knowledge', 'Lead', 'Left', 'Machine Learning', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Operative Surgical Procedures', 'Patient Care', 'Patient Monitoring', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Platelet Count measurement', 'Play', 'Practice Management', 'Quality of Care', 'Relative (related person)', 'Research', 'Research Personnel', 'Role', 'Services', 'Signal Transduction', 'Solutions', 'Statistical Models', 'Stream', 'System', 'Testing', 'Thrombocytopenia', 'Thrombosis', 'Time', 'Work', 'base', 'biomedical informatics', 'clinical care', 'clinical practice', 'computer science', 'cost', 'experience', 'follow-up', 'health care quality', 'improved', 'knowledge base', 'multidisciplinary', 'public health relevance', 'research study', 'tool']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,499603,0.0633802469220799
"Detecting deviations in clinical care in ICU data streams    DESCRIPTION (provided by applicant): Timely detection of severe patient conditions or concerning events and their mitigation remains an important problem in clinical practice. This is especially true in the critically ill patient [1]. Typical computer-based detection methods developed for this purpose rely on the use of clinical knowledge, such as expert-derived rules, that are incorporated into monitoring and alerting systems. However, it is often time-consuming, costly, and difficult to extract and implement such knowledge in existing monitoring systems. The research work in this proposal offers computational, rather than expert-based, solutions that build alert systems from data stored in patient data repositories, such as electronic medical records. Briefly, our approach uses advanced machine learning algorithms to identify unusual clinical management patterns in individual patients, relative to patterns associated with comparable patients, and raises an alert signaling this discrepancy. Our preliminary studies provide support that such deviations often indicate clinically important events for which it is worthwhile to raise an alert. We propose an evaluation based on physician assessment of alerts that are generated from a retrospective set of intensive-care unit (ICU) patient cases. The project investigators comprise a multidisciplinary team with expertise in critical care medicine, computer science, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories.    PUBLIC HEALTH RELEVANCE: There remain numerous opportunities to reduce medical errors in critical care by sending computer-based reminders and alerts to clinicians. This project uses past patient data, which is stored in electronic form, and machine-learning methods to help develop and refine computer-based alerts to improve healthcare quality and reduce costs.           PROJECT NARRATIVE There remain numerous opportunities to reduce medical errors in critical care by sending computer-based reminders and alerts to clinicians. This project uses past patient data, which is stored in electronic form, and machine-learning methods to help develop and refine computer-based alerts to improve healthcare quality and reduce costs.",Detecting deviations in clinical care in ICU data streams,8098786,R01GM088224,"['Algorithms', 'Anticoagulants', 'Belief', 'Cardiac', 'Caring', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Management', 'Computerized Medical Record', 'Computers', 'Critical Care', 'Critical Illness', 'Data', 'Databases', 'Detection', 'Development', 'Disadvantaged', 'Drops', 'Electronics', 'Evaluation', 'Event', 'Future', 'Healthcare', 'Heparin', 'Individual', 'Industry', 'Information Systems', 'Intensive Care Units', 'Knowledge', 'Lead', 'Left', 'Machine Learning', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Operative Surgical Procedures', 'Patient Care', 'Patient Monitoring', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Platelet Count measurement', 'Play', 'Practice Management', 'Quality of Care', 'Relative (related person)', 'Research', 'Research Personnel', 'Services', 'Signal Transduction', 'Solutions', 'Statistical Models', 'Stream', 'System', 'Testing', 'Thrombocytopenia', 'Thrombosis', 'Time', 'Work', 'base', 'biomedical informatics', 'clinical care', 'clinical practice', 'computer science', 'cost', 'experience', 'follow-up', 'health care quality', 'improved', 'knowledge base', 'multidisciplinary', 'public health relevance', 'research study', 'tool']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2011,498467,0.0633802469220799
"Detecting deviations in clinical care in ICU data streams    DESCRIPTION (provided by applicant): Timely detection of severe patient conditions or concerning events and their mitigation remains an important problem in clinical practice. This is especially true in the critically ill patient [1]. Typical computer-based detection methods developed for this purpose rely on the use of clinical knowledge, such as expert-derived rules, that are incorporated into monitoring and alerting systems. However, it is often time-consuming, costly, and difficult to extract and implement such knowledge in existing monitoring systems. The research work in this proposal offers computational, rather than expert-based, solutions that build alert systems from data stored in patient data repositories, such as electronic medical records. Briefly, our approach uses advanced machine learning algorithms to identify unusual clinical management patterns in individual patients, relative to patterns associated with comparable patients, and raises an alert signaling this discrepancy. Our preliminary studies provide support that such deviations often indicate clinically important events for which it is worthwhile to raise an alert. We propose an evaluation based on physician assessment of alerts that are generated from a retrospective set of intensive-care unit (ICU) patient cases. The project investigators comprise a multidisciplinary team with expertise in critical care medicine, computer science, biomedical informatics, statistical machine learning, knowledge based systems, and clinical data repositories.    PUBLIC HEALTH RELEVANCE: There remain numerous opportunities to reduce medical errors in critical care by sending computer-based reminders and alerts to clinicians. This project uses past patient data, which is stored in electronic form, and machine-learning methods to help develop and refine computer-based alerts to improve healthcare quality and reduce costs.           PROJECT NARRATIVE There remain numerous opportunities to reduce medical errors in critical care by sending computer-based reminders and alerts to clinicians. This project uses past patient data, which is stored in electronic form, and machine-learning methods to help develop and refine computer-based alerts to improve healthcare quality and reduce costs.",Detecting deviations in clinical care in ICU data streams,7918929,R01GM088224,"['Algorithms', 'Anticoagulants', 'Belief', 'Cardiac', 'Caring', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Management', 'Computerized Medical Record', 'Computers', 'Critical Care', 'Critical Illness', 'Data', 'Databases', 'Detection', 'Development', 'Disadvantaged', 'Drops', 'Electronics', 'Evaluation', 'Event', 'Future', 'Healthcare', 'Heparin', 'Individual', 'Industry', 'Information Systems', 'Intensive Care Units', 'Knowledge', 'Lead', 'Left', 'Machine Learning', 'Measures', 'Medical Errors', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Operative Surgical Procedures', 'Patient Care', 'Patient Monitoring', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Platelet Count measurement', 'Play', 'Practice Management', 'Quality of Care', 'Relative (related person)', 'Research', 'Research Personnel', 'Role', 'Services', 'Signal Transduction', 'Solutions', 'Statistical Models', 'Stream', 'System', 'Testing', 'Thrombocytopenia', 'Thrombosis', 'Time', 'Work', 'base', 'biomedical informatics', 'clinical care', 'clinical practice', 'computer science', 'cost', 'experience', 'follow-up', 'health care quality', 'improved', 'knowledge base', 'multidisciplinary', 'public health relevance', 'research study', 'tool']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2010,493140,0.0633802469220799
"A computer aided chromosome imaging technique for cancer diagnosis    DESCRIPTION (provided by applicant): Identification of recurrent chromosomal aberrations is important for diagnosis, prognosis, and therapy of most hematological malignancies. Due to difficulties with culture of tumor cells, low mitotic index, poor chromosomal morphologies, and low prevalence, it takes tremendous effort and time for a cytogenetic clinician to obtain a sufficient number of analyzable metaphase cells under microscope before he/she can make an accurate clinical diagnosis. This process is not only very inefficient but also subject to human errors. In order to improve the efficiency and accuracy of leukemia diagnosis, we propose to develop a computer aided chromosome imaging technique. Specifically, we will develop an innovative high-speed microscopic imaging system based on a time-delay-integration technique. The system can scan the entire sample-slide at high magnification to obtain high resolution digital images to reveal metaphase chromosomes as required by clinical diagnosis. We will also develop a novel computer aided diagnosis (CAD) scheme including four specific modules to (1) detect analyzable metaphase chromosome cells, (2) segment overlapped chromosomes, (3) identify and classify distorted chromosomes associated with cancer cells, and (4) predict the cancer prognosis. After identification and segmentation of analyzable chromosomes, we will compute and search for the effective and robust image features. Genetic algorithm will be used to train and optimize an artificial neural network and a Bayesian belief network for the classification and prediction tasks, respectively. Using the integrated CAD workstation, we will conduct an observer performance study to assess the performance of the technique and its clinical feasibility. In summary, the proposed imaging technique is highly efficient, and no or only minimal human interventions are required from initial slide-scanning up to the presentation of CAD results. With such a new computerized clinical tool, cytogeneticists can effectively focus their efforts on analyzing/verifying chromosomal abnormal patterns and making final diagnostic decisions. It is therefore expected that the proposed technology can significantly improve the efficiency and accuracy of cancer (i.e., leukemia) diagnosis. The proposed technique has significant clinical potentials in monitoring therapeutic efficacy of cancer treatment as well.          n/a",A computer aided chromosome imaging technique for cancer diagnosis,7841805,R01CA115320,"['Algorithms', 'Belief', 'Biological Neural Networks', 'Cancer Prognosis', 'Cells', 'Chromosome Pairing', 'Chromosome abnormality', 'Chromosomes', 'Chromosomes, Human, Pair 3', 'Classification', 'Clinical', 'Computer Assisted', 'Computer software', 'Computer-Assisted Diagnosis', 'Computers', 'Congenital chromosomal disease', 'Cultured Tumor Cells', 'Custom', 'Cytogenetics', 'Databases', 'Deletion Mutation', 'Diagnosis', 'Diagnostic', 'Disease', 'Doctor of Philosophy', 'Effectiveness', 'Future', 'Genetic', 'Genetic Programming', 'Graph', 'Hematologic Neoplasms', 'Hour', 'Human', 'Image', 'Imaging Techniques', 'Intervention', 'Label', 'Laboratories', 'Location', 'Low Prevalence', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Metaphase', 'Methods', 'Microscope', 'Microscopic', 'Molecular', 'Monitor', 'Morphology', 'Normal Cell', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Process', 'Receiver Operating Characteristics', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'S-Phase Fraction', 'Sampling', 'Scanning', 'Scheme', 'Skin', 'Slide', 'Solutions', 'Speed', 'Staging', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Treatment Efficacy', 'base', 'cancer cell', 'cancer diagnosis', 'cancer therapy', 'cancer type', 'clinical Diagnosis', 'clinical application', 'computerized', 'design', 'detector', 'digital imaging', 'experience', 'improved', 'innovation', 'interest', 'leukemia', 'malignant breast neoplasm', 'novel', 'outcome forecast', 'programs', 'tool', 'user-friendly']",NCI,UNIVERSITY OF OKLAHOMA NORMAN,R01,2010,313901,0.043804997224006756
"A computer aided chromosome imaging technique for cancer diagnosis    DESCRIPTION (provided by applicant): Identification of recurrent chromosomal aberrations is important for diagnosis, prognosis, and therapy of most hematological malignancies. Due to difficulties with culture of tumor cells, low mitotic index, poor chromosomal morphologies, and low prevalence, it takes tremendous effort and time for a cytogenetic clinician to obtain a sufficient number of analyzable metaphase cells under microscope before he/she can make an accurate clinical diagnosis. This process is not only very inefficient but also subject to human errors. In order to improve the efficiency and accuracy of leukemia diagnosis, we propose to develop a computer aided chromosome imaging technique. Specifically, we will develop an innovative high-speed microscopic imaging system based on a time-delay-integration technique. The system can scan the entire sample-slide at high magnification to obtain high resolution digital images to reveal metaphase chromosomes as required by clinical diagnosis. We will also develop a novel computer aided diagnosis (CAD) scheme including four specific modules to (1) detect analyzable metaphase chromosome cells, (2) segment overlapped chromosomes, (3) identify and classify distorted chromosomes associated with cancer cells, and (4) predict the cancer prognosis. After identification and segmentation of analyzable chromosomes, we will compute and search for the effective and robust image features. Genetic algorithm will be used to train and optimize an artificial neural network and a Bayesian belief network for the classification and prediction tasks, respectively. Using the integrated CAD workstation, we will conduct an observer performance study to assess the performance of the technique and its clinical feasibility. In summary, the proposed imaging technique is highly efficient, and no or only minimal human interventions are required from initial slide-scanning up to the presentation of CAD results. With such a new computerized clinical tool, cytogeneticists can effectively focus their efforts on analyzing/verifying chromosomal abnormal patterns and making final diagnostic decisions. It is therefore expected that the proposed technology can significantly improve the efficiency and accuracy of cancer (i.e., leukemia) diagnosis. The proposed technique has significant clinical potentials in monitoring therapeutic efficacy of cancer treatment as well.          n/a",A computer aided chromosome imaging technique for cancer diagnosis,7609064,R01CA115320,"['Algorithms', 'Belief', 'Biological Neural Networks', 'Cancer Prognosis', 'Cells', 'Chromosome Pairing', 'Chromosome abnormality', 'Chromosomes', 'Chromosomes, Human, Pair 3', 'Classification', 'Clinical', 'Computer Assisted', 'Computer software', 'Computer-Assisted Diagnosis', 'Computers', 'Congenital chromosomal disease', 'Cultured Tumor Cells', 'Custom', 'Cytogenetics', 'Databases', 'Deletion Mutation', 'Diagnosis', 'Diagnostic', 'Disease', 'Doctor of Philosophy', 'Effectiveness', 'Future', 'Genetic', 'Genetic Programming', 'Graph', 'Hematologic Neoplasms', 'Hour', 'Human', 'Image', 'Imaging Techniques', 'Intervention', 'Label', 'Laboratories', 'Location', 'Low Prevalence', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Metaphase', 'Methods', 'Microscope', 'Microscopic', 'Molecular', 'Monitor', 'Morphology', 'Normal Cell', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Process', 'Receiver Operating Characteristics', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'S-Phase Fraction', 'Sampling', 'Scanning', 'Scheme', 'Skin', 'Slide', 'Solutions', 'Speed', 'Staging', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Treatment Efficacy', 'base', 'cancer cell', 'cancer diagnosis', 'cancer therapy', 'cancer type', 'clinical Diagnosis', 'clinical application', 'computerized', 'design', 'detector', 'digital imaging', 'experience', 'improved', 'innovation', 'interest', 'leukemia', 'malignant breast neoplasm', 'novel', 'outcome forecast', 'programs', 'tool', 'user-friendly']",NCI,UNIVERSITY OF OKLAHOMA NORMAN,R01,2009,308319,0.043804997224006756
"A computer aided chromosome imaging technique for cancer diagnosis    DESCRIPTION (provided by applicant): Identification of recurrent chromosomal aberrations is important for diagnosis, prognosis, and therapy of most hematological malignancies. Due to difficulties with culture of tumor cells, low mitotic index, poor chromosomal morphologies, and low prevalence, it takes tremendous effort and time for a cytogenetic clinician to obtain a sufficient number of analyzable metaphase cells under microscope before he/she can make an accurate clinical diagnosis. This process is not only very inefficient but also subject to human errors. In order to improve the efficiency and accuracy of leukemia diagnosis, we propose to develop a computer aided chromosome imaging technique. Specifically, we will develop an innovative high-speed microscopic imaging system based on a time-delay-integration technique. The system can scan the entire sample-slide at high magnification to obtain high resolution digital images to reveal metaphase chromosomes as required by clinical diagnosis. We will also develop a novel computer aided diagnosis (CAD) scheme including four specific modules to (1) detect analyzable metaphase chromosome cells, (2) segment overlapped chromosomes, (3) identify and classify distorted chromosomes associated with cancer cells, and (4) predict the cancer prognosis. After identification and segmentation of analyzable chromosomes, we will compute and search for the effective and robust image features. Genetic algorithm will be used to train and optimize an artificial neural network and a Bayesian belief network for the classification and prediction tasks, respectively. Using the integrated CAD workstation, we will conduct an observer performance study to assess the performance of the technique and its clinical feasibility. In summary, the proposed imaging technique is highly efficient, and no or only minimal human interventions are required from initial slide-scanning up to the presentation of CAD results. With such a new computerized clinical tool, cytogeneticists can effectively focus their efforts on analyzing/verifying chromosomal abnormal patterns and making final diagnostic decisions. It is therefore expected that the proposed technology can significantly improve the efficiency and accuracy of cancer (i.e., leukemia) diagnosis. The proposed technique has significant clinical potentials in monitoring therapeutic efficacy of cancer treatment as well.          n/a",A computer aided chromosome imaging technique for cancer diagnosis,7423851,R01CA115320,"['Algorithms', 'Belief', 'Biological Neural Networks', 'Cancer Prognosis', 'Cells', 'Chromosome Pairing', 'Chromosome abnormality', 'Chromosomes', 'Chromosomes, Human, Pair 3', 'Classification', 'Clinical', 'Computer Assisted', 'Computer software', 'Computer-Assisted Diagnosis', 'Computers', 'Congenital chromosomal disease', 'Cultured Tumor Cells', 'Custom', 'Cytogenetics', 'Databases', 'Deletion Mutation', 'Diagnosis', 'Diagnostic', 'Disease', 'Doctor of Philosophy', 'Effectiveness', 'Future', 'Genetic', 'Genetic Programming', 'Graph', 'Hematologic Neoplasms', 'Hour', 'Human', 'Image', 'Imaging Techniques', 'Intervention', 'Label', 'Laboratories', 'Location', 'Low Prevalence', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Metaphase', 'Methods', 'Microscope', 'Microscopic', 'Molecular', 'Monitor', 'Morphology', 'Normal Cell', 'Numbers', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Process', 'Rate', 'Receiver Operating Characteristics', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'S-Phase Fraction', 'Sampling', 'Scanning', 'Scheme', 'Skin', 'Slide', 'Solutions', 'Speed', 'Staging', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Treatment Efficacy', 'base', 'cancer cell', 'cancer diagnosis', 'cancer therapy', 'cancer type', 'clinical Diagnosis', 'clinical application', 'computerized', 'concept', 'day', 'design', 'detector', 'digital imaging', 'experience', 'improved', 'innovation', 'interest', 'leukemia', 'malignant breast neoplasm', 'novel', 'outcome forecast', 'programs', 'size', 'tool', 'user-friendly']",NCI,UNIVERSITY OF OKLAHOMA NORMAN,R01,2008,312688,0.043804997224006756
"A computer aided chromosome imaging technique for cancer diagnosis    DESCRIPTION (provided by applicant): Identification of recurrent chromosomal aberrations is important for diagnosis, prognosis, and therapy of most hematological malignancies. Due to difficulties with culture of tumor cells, low mitotic index, poor chromosomal morphologies, and low prevalence, it takes tremendous effort and time for a cytogenetic clinician to obtain a sufficient number of analyzable metaphase cells under microscope before he/she can make an accurate clinical diagnosis. This process is not only very inefficient but also subject to human errors. In order to improve the efficiency and accuracy of leukemia diagnosis, we propose to develop a computer aided chromosome imaging technique. Specifically, we will develop an innovative high-speed microscopic imaging system based on a time-delay-integration technique. The system can scan the entire sample-slide at high magnification to obtain high resolution digital images to reveal metaphase chromosomes as required by clinical diagnosis. We will also develop a novel computer aided diagnosis (CAD) scheme including four specific modules to (1) detect analyzable metaphase chromosome cells, (2) segment overlapped chromosomes, (3) identify and classify distorted chromosomes associated with cancer cells, and (4) predict the cancer prognosis. After identification and segmentation of analyzable chromosomes, we will compute and search for the effective and robust image features. Genetic algorithm will be used to train and optimize an artificial neural network and a Bayesian belief network for the classification and prediction tasks, respectively. Using the integrated CAD workstation, we will conduct an observer performance study to assess the performance of the technique and its clinical feasibility. In summary, the proposed imaging technique is highly efficient, and no or only minimal human interventions are required from initial slide-scanning up to the presentation of CAD results. With such a new computerized clinical tool, cytogeneticists can effectively focus their efforts on analyzing/verifying chromosomal abnormal patterns and making final diagnostic decisions. It is therefore expected that the proposed technology can significantly improve the efficiency and accuracy of cancer (i.e., leukemia) diagnosis. The proposed technique has significant clinical potentials in monitoring therapeutic efficacy of cancer treatment as well.          n/a",A computer aided chromosome imaging technique for cancer diagnosis,7261406,R01CA115320,"['Algorithms', 'Belief', 'Biological Neural Networks', 'Cancer Prognosis', 'Cells', 'Chromosome Pairing', 'Chromosome abnormality', 'Chromosomes', 'Chromosomes, Human, Pair 3', 'Classification', 'Clinical', 'Computer Assisted', 'Computer software', 'Computer-Assisted Diagnosis', 'Computers', 'Congenital chromosomal disease', 'Cultured Tumor Cells', 'Custom', 'Cytogenetics', 'Databases', 'Deletion Mutation', 'Diagnosis', 'Diagnostic', 'Disease', 'Doctor of Philosophy', 'Effectiveness', 'Future', 'Genetic', 'Genetic Programming', 'Graph', 'Hematologic Neoplasms', 'Hour', 'Human', 'Image', 'Imaging Techniques', 'Intervention', 'Label', 'Laboratories', 'Location', 'Low Prevalence', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Metaphase', 'Methods', 'Microscope', 'Microscopic', 'Molecular', 'Monitor', 'Morphology', 'Normal Cell', 'Numbers', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Process', 'Rate', 'Receiver Operating Characteristics', 'Recurrence', 'Research', 'Research Personnel', 'Resolution', 'S-Phase Fraction', 'Sampling', 'Scanning', 'Scheme', 'Skin', 'Slide', 'Solutions', 'Speed', 'Staging', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Treatment Efficacy', 'base', 'cancer cell', 'cancer diagnosis', 'cancer therapy', 'cancer type', 'clinical Diagnosis', 'clinical application', 'computerized', 'concept', 'day', 'design', 'detector', 'digital imaging', 'experience', 'improved', 'innovation', 'interest', 'leukemia', 'malignant breast neoplasm', 'novel', 'outcome forecast', 'programs', 'size', 'tool', 'user-friendly']",NCI,UNIVERSITY OF OKLAHOMA NORMAN,R01,2007,309600,0.043804997224006756
"A computer aided chromosome imaging technique for cancer diagnosis    DESCRIPTION (provided by applicant): Identification of recurrent chromosomal aberrations is important for diagnosis, prognosis, and therapy of most hematological malignancies. Due to difficulties with culture of tumor cells, low mitotic index, poor chromosomal morphologies, and low prevalence, it takes tremendous effort and time for a cytogenetic clinician to obtain a sufficient number of analyzable metaphase cells under microscope before he/she can make an accurate clinical diagnosis. This process is not only very inefficient but also subject to human errors. In order to improve the efficiency and accuracy of leukemia diagnosis, we propose to develop a computer aided chromosome imaging technique. Specifically, we will develop an innovative high-speed microscopic imaging system based on a time-delay-integration technique. The system can scan the entire sample-slide at high magnification to obtain high resolution digital images to reveal metaphase chromosomes as required by clinical diagnosis. We will also develop a novel computer aided diagnosis (CAD) scheme including four specific modules to (1) detect analyzable metaphase chromosome cells, (2) segment overlapped chromosomes, (3) identify and classify distorted chromosomes associated with cancer cells, and (4) predict the cancer prognosis. After identification and segmentation of analyzable chromosomes, we will compute and search for the effective and robust image features. Genetic algorithm will be used to train and optimize an artificial neural network and a Bayesian belief network for the classification and prediction tasks, respectively. Using the integrated CAD workstation, we will conduct an observer performance study to assess the performance of the technique and its clinical feasibility. In summary, the proposed imaging technique is highly efficient, and no or only minimal human interventions are required from initial slide-scanning up to the presentation of CAD results. With such a new computerized clinical tool, cytogeneticists can effectively focus their efforts on analyzing/verifying chromosomal abnormal patterns and making final diagnostic decisions. It is therefore expected that the proposed technology can significantly improve the efficiency and accuracy of cancer (i.e., leukemia) diagnosis. The proposed technique has significant clinical potentials in monitoring therapeutic efficacy of cancer treatment as well.          n/a",A computer aided chromosome imaging technique for cancer diagnosis,7088672,R01CA115320,"['chromosomes', 'computers', 'diagnosis', 'leukemia', 'neoplasm /cancer', 'neoplasm /cancer diagnosis']",NCI,UNIVERSITY OF OKLAHOMA NORMAN,R01,2006,333554,0.043804997224006756
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk. PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9665255,R01AI117011,"['Animals', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease Surveillance', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Infrastructure', 'Knowledge', 'Link', 'Literature', 'Location', 'Manuals', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'Zoonoses', 'improved', 'information model', 'interest', 'journal article', 'molecular sequence database', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2019,461012,0.017038849532168256
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk. PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9454246,R01AI117011,"['Animals', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease Surveillance', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Knowledge', 'Link', 'Literature', 'Location', 'Manuals', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'Zoonoses', 'improved', 'information model', 'interest', 'journal article', 'molecular sequence database', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2018,461012,0.017038849532168256
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk. PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9249484,R01AI117011,"['Animals', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Knowledge', 'Link', 'Literature', 'Location', 'Manuals', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'Zoonoses', 'improved', 'information model', 'interest', 'journal article', 'molecular sequence database', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2017,461012,0.017038849532168256
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk.         PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.                ",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9065021,R01AI117011,"['Accounting', 'Animals', 'Applied Research', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Knowledge', 'Link', 'Literature', 'Location', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Taxon', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'improved', 'information model', 'interest', 'journal article', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2016,479735,0.017038849532168256
"Receptors, microcircuits and hierarchical connectivity in predictive coding and sensory awareness SUMMARY The standard view of how we make sense of the world around us focuses on reconstructing our environment from the information received by our sensory organs. In this view, low-level brain areas (e.g., primary sensory cortex) represent basic features of objects, which are elaborated on in successive processing stages, until representations become increasingly complex in high-level areas (e.g., frontal cortex). An alternative view is predictive coding (PC), in which we model our environment to generate sensory predictions. In PC, high-level brain areas generate predictions of sensory activity and transmit them to low-level areas. A prediction that does not match the sensory information gives rise to a prediction error. This error signal is sent from low- to high-level brain areas to update the model of our environment, thereby improving future predictions to minimize errors. Modeling studies show PC is a fast and efficient way to process sensory information, and PC provides innovative hypotheses for understanding sleep and anesthesia, particularly when disconnected consciousness occurs (consciousness without awareness of the environment), like dreaming. PC also holds great promise for conceptualizing and treating brain disorders, including schizophrenia and depression. But key central features of PC have not been empirically tested and little is known about the underlying neural mechanisms. The goal of the proposed project is to characterize the neural dynamics, circuits and receptors enabling PC. There are two principle hypotheses. First, predictions depend on N-methyl-D-aspartate receptors (NMDAR) because NMDAR influence the activity of high-level brain areas where predictions are generated, and NMDAR are enriched on neurons in lower-level areas receiving predictions. Second, in disconnected consciousness, a breakdown of information transmission from low-level to high-level brain areas, as well as a breakdown of computations within each area, explains why models of our environment are not updated by external sensory information. These breakdowns prevent the comparison of predictions and sensory information, as well as the transmission of prediction errors to high-level brain areas. To test these hypotheses, we use a cross-species experimental design connecting cellular, circuit and systems levels to behavior. We will perform electroencephalography, machine learning and computational modeling to define the neural basis of PC in humans performing prediction tasks. Then we will manipulate PC using different anesthetic agents with diverse mechanisms, establishing causal relationships between receptors, large-scale brain networks and PC. In parallel, we will simultaneously record activity from sensory and high-level brain areas of non-human primates (NHPs) using the same PC tasks and pharmacological interventions to measure cellular and circuit level contributions to PC. Investigating PC will illuminate the fundamental mechanisms of perception, providing critical insights to guide therapeutic development for multiple health conditions. NARRATIVE This research advances our understanding of how the brain generates predictions which shape how we perceive the world, otherwise known as predictive coding. Predictive coding is relevant to public health because it holds great promise in conceptualizing and treating brain disorders, including depression, schizophrenia, delirium and dementia, as well as understanding sleep and anesthesia. Progress in understanding the basic mechanisms of predictive coding, as well as predictive coding deficits, is a necessary step in developing more effective treatment strategies for multiple health conditions.","Receptors, microcircuits and hierarchical connectivity in predictive coding and sensory awareness",10034682,R01NS117901,"['Adrenergic Receptor', 'Agonist', 'Anesthesia procedures', 'Anesthetics', 'Area', 'Auditory', 'Auditory area', 'Awareness', 'Behavior', 'Brain', 'Brain Diseases', 'Code', 'Complex', 'Computer Models', 'Conscious', 'Cues', 'Data', 'Delirium', 'Dementia', 'Dexmedetomidine', 'Disinhibition', 'Dose', 'Dreams', 'Electroencephalography', 'Environment', 'Experimental Designs', 'Feedback', 'Future', 'Goals', 'Health', 'Human', 'Impairment', 'Individual', 'Interneurons', 'Intervention', 'Ketamine', 'Link', 'Macaca', 'Machine Learning', 'Measures', 'Mediating', 'Mental Depression', 'Modality', 'Modeling', 'Molecular Target', 'Monitor', 'N-Methyl-D-Aspartate Receptors', 'Neurons', 'Organ', 'Parietal Lobe', 'Pathway interactions', 'Perception', 'Pharmacology', 'Process', 'Propofol', 'Public Health', 'Pulvinar structure', 'Reaction Time', 'Reporting', 'Research', 'Role', 'Schizophrenia', 'Sedation procedure', 'Sensory', 'Sensory Process', 'Shapes', 'Signal Transduction', 'Sleep', 'Study models', 'System', 'Temporal Lobe', 'Testing', 'Thalamic Nuclei', 'Unconscious State', 'Update', 'base', 'effective therapy', 'experience', 'experimental study', 'frontal lobe', 'human data', 'improved', 'innovation', 'innovative technologies', 'insight', 'multi-electrode arrays', 'neural correlate', 'neuromechanism', 'neurophysiology', 'nonhuman primate', 'paired stimuli', 'postsynaptic', 'prevent', 'receptor', 'relating to nervous system', 'response', 'sedative', 'sensory cortex', 'sensory stimulus', 'therapeutic development', 'transmission process', 'treatment strategy', 'visual stimulus']",NINDS,UNIVERSITY OF WISCONSIN-MADISON,R01,2020,517182,-0.011179412494513987
"Eliminating Critical Systematic Errors In Structural Biology With Next-Generation Simulation PROJECT SUMMARY/ABSTRACT Data collection in macromolecular crystallography is subject to significant systematic errors that prevent successful data collection on many systems and, ultimately, limit the accuracy of resulting structures. Creating simulation technologies that can account for these errors will have significant impact on three fronts: 1) solving new structures by better accounting for radiation damage, which is responsible for 80% of failed anomalous phasing attempts, 2) improving multi-crystal averaging by simulating non-isomorphism, which will open the gateway to arbitrary gains in signal-to-noise, 3) discriminating hotly contested alternative interpretations such as the presence or absence of a bound ligand, by creating simulations with more realistic solvent models. To move towards “damage-free data” from a synchrotron, we will start by calibrating radiation damage curves on model and DBP samples. Using these curves we will incorporate realistic 3D models of radiation damage to non-cuboid crystals (RADDOSE 3D) into our diffraction image simulator (MLFSOM) to yield a 3D Dose Distribution and Illumination map along the crystal. This will result in a new generation of wavelength- dependent absorption factors for the crystal to complement existing absorption corrections. At the beamline, we will measure a 3D map of the crystal using cone beam online x-ray absorption radiography and a 2D map of the beam profile. These advances will allow us to generate zero-dose extrapolation values, in an open format, that account for experimental crystal and beam geometry. To improve multi-crystal averaging, we will begin by characterizing how non-isomorphism varies as a function of humidity, radiation damage, and functional state. By updating the classic “Crick and Magdoff” simulations of non-isomorphism with increasing complexity, we will develop a singular value decomposition approach to parameterize non-isomorphism. Using the corrections derived from this analysis, we will correct the non-isomorphism present in multi-crystal experiments, enabling the determination of novel structures, including those collected using serial crystallography at next-generation light sources. To enable enhanced simulation for robust interpretation of experimental data, we will leverage new solvent models in macromolecular crystallography and small angle X- ray scattering. Our work will create standard protocols for comparing solvent density to alternative interpretations and to quantitatively assess how likely each simulated situation is compared to the real macromolecular crystallography or SAXS data. In addition to distinguishing between different interpretations of the experimental data, improving solvent models will enhance understanding of how macromolecules influence and interact with other molecules near their surface. Collectively, we expect the benefits of eliminating these critical systematic errors be transformative to both methods development and functional studies. PROJECT NARRATIVE Data collection in macromolecular crystallography is subject to significant systematic errors that prevent successful soilution on many systems and, ultimately, limit the accuracy of resulting structures. Creating simulation technologies that can account for these errors will have significant impact on three fronts: 1) solving new structures by better accounting for radiation damage, which is responsible for 80% of failed anomalous phasing attempts, 2) improving multi-crystal averaging by simulating non-isomorphism, which will open the gateway to arbitrary gains in signal-to-noise, 3) discriminating hotly contested alternative interpretations such as the presence or absence of a bound ligand, by creating simulations with more realistic solvent models.",Eliminating Critical Systematic Errors In Structural Biology With Next-Generation Simulation,9951062,R01GM124149,"['3-Dimensional', 'Accounting', 'Active Sites', 'Area', 'Complement', 'Computer software', 'Cone', 'Crystallization', 'Crystallography', 'Data', 'Data Collection', 'Data Set', 'Diagnosis', 'Diagnostic radiologic examination', 'Disease', 'Dose', 'Error Sources', 'Evolution', 'Generations', 'Geometry', 'Humidity', 'Image', 'In Situ', 'Knowledge', 'Ligands', 'Light', 'Lighting', 'Maps', 'Measures', 'Metals', 'Methods', 'Minor', 'Modeling', 'Molecular Conformation', 'Muramidase', 'Noise', 'Phase', 'Positioning Attribute', 'Protein Region', 'Protocols documentation', 'Radiation induced damage', 'Reaction', 'Resolution', 'Roentgen Rays', 'Sampling', 'Side', 'Signal Transduction', 'Solvents', 'Source', 'Spottings', 'Structure', 'Surface', 'Synchrotrons', 'Syncope', 'System', 'Technology', 'Update', 'Weight', 'Work', 'absorption', 'beamline', 'computerized tools', 'conformer', 'curve fitting', 'density', 'electron density', 'experimental study', 'improved', 'interest', 'macromolecule', 'method development', 'next generation', 'non-Native', 'novel', 'novel strategies', 'prevent', 'simulation', 'structural biology', 'success', 'three-dimensional modeling', 'trend', 'vector']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2020,309526,0.10200950999737068
"Eliminating Critical Systematic Errors In Structural Biology With Next-Generation Simulation PROJECT SUMMARY/ABSTRACT Data collection in macromolecular crystallography is subject to significant systematic errors that prevent successful data collection on many systems and, ultimately, limit the accuracy of resulting structures. Creating simulation technologies that can account for these errors will have significant impact on three fronts: 1) solving new structures by better accounting for radiation damage, which is responsible for 80% of failed anomalous phasing attempts, 2) improving multi-crystal averaging by simulating non-isomorphism, which will open the gateway to arbitrary gains in signal-to-noise, 3) discriminating hotly contested alternative interpretations such as the presence or absence of a bound ligand, by creating simulations with more realistic solvent models. To move towards “damage-free data” from a synchrotron, we will start by calibrating radiation damage curves on model and DBP samples. Using these curves we will incorporate realistic 3D models of radiation damage to non-cuboid crystals (RADDOSE 3D) into our diffraction image simulator (MLFSOM) to yield a 3D Dose Distribution and Illumination map along the crystal. This will result in a new generation of wavelength- dependent absorption factors for the crystal to complement existing absorption corrections. At the beamline, we will measure a 3D map of the crystal using cone beam online x-ray absorption radiography and a 2D map of the beam profile. These advances will allow us to generate zero-dose extrapolation values, in an open format, that account for experimental crystal and beam geometry. To improve multi-crystal averaging, we will begin by characterizing how non-isomorphism varies as a function of humidity, radiation damage, and functional state. By updating the classic “Crick and Magdoff” simulations of non-isomorphism with increasing complexity, we will develop a singular value decomposition approach to parameterize non-isomorphism. Using the corrections derived from this analysis, we will correct the non-isomorphism present in multi-crystal experiments, enabling the determination of novel structures, including those collected using serial crystallography at next-generation light sources. To enable enhanced simulation for robust interpretation of experimental data, we will leverage new solvent models in macromolecular crystallography and small angle X- ray scattering. Our work will create standard protocols for comparing solvent density to alternative interpretations and to quantitatively assess how likely each simulated situation is compared to the real macromolecular crystallography or SAXS data. In addition to distinguishing between different interpretations of the experimental data, improving solvent models will enhance understanding of how macromolecules influence and interact with other molecules near their surface. Collectively, we expect the benefits of eliminating these critical systematic errors be transformative to both methods development and functional studies. PROJECT NARRATIVE Data collection in macromolecular crystallography is subject to significant systematic errors that prevent successful soilution on many systems and, ultimately, limit the accuracy of resulting structures. Creating simulation technologies that can account for these errors will have significant impact on three fronts: 1) solving new structures by better accounting for radiation damage, which is responsible for 80% of failed anomalous phasing attempts, 2) improving multi-crystal averaging by simulating non-isomorphism, which will open the gateway to arbitrary gains in signal-to-noise, 3) discriminating hotly contested alternative interpretations such as the presence or absence of a bound ligand, by creating simulations with more realistic solvent models.",Eliminating Critical Systematic Errors In Structural Biology With Next-Generation Simulation,9690452,R01GM124149,"['3-Dimensional', 'Accounting', 'Active Sites', 'Area', 'Complement', 'Computer software', 'Cone', 'Crystallization', 'Crystallography', 'Data', 'Data Collection', 'Data Set', 'Diagnosis', 'Diagnostic radiologic examination', 'Disease', 'Dose', 'Error Sources', 'Evolution', 'Generations', 'Geometry', 'Humidity', 'Image', 'In Situ', 'Knowledge', 'Ligands', 'Light', 'Lighting', 'Maps', 'Measures', 'Metals', 'Methods', 'Minor', 'Modeling', 'Molecular Conformation', 'Muramidase', 'Noise', 'Phase', 'Positioning Attribute', 'Protein Region', 'Protocols documentation', 'Radiation induced damage', 'Reaction', 'Resolution', 'Roentgen Rays', 'Sampling', 'Side', 'Signal Transduction', 'Solvents', 'Source', 'Spottings', 'Structure', 'Surface', 'Synchrotrons', 'Syncope', 'System', 'Technology', 'Update', 'Weight', 'Work', 'absorption', 'beamline', 'computerized tools', 'conformer', 'curve fitting', 'density', 'electron density', 'experimental study', 'improved', 'interest', 'macromolecule', 'method development', 'next generation', 'non-Native', 'novel', 'novel strategies', 'prevent', 'simulation', 'structural biology', 'success', 'three-dimensional modeling', 'trend', 'vector']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2019,309615,0.10200950999737068
"Eliminating Critical Systematic Errors In Structural Biology With Next-Generation Simulation PROJECT SUMMARY/ABSTRACT Data collection in macromolecular crystallography is subject to significant systematic errors that prevent successful data collection on many systems and, ultimately, limit the accuracy of resulting structures. Creating simulation technologies that can account for these errors will have significant impact on three fronts: 1) solving new structures by better accounting for radiation damage, which is responsible for 80% of failed anomalous phasing attempts, 2) improving multi-crystal averaging by simulating non-isomorphism, which will open the gateway to arbitrary gains in signal-to-noise, 3) discriminating hotly contested alternative interpretations such as the presence or absence of a bound ligand, by creating simulations with more realistic solvent models. To move towards “damage-free data” from a synchrotron, we will start by calibrating radiation damage curves on model and DBP samples. Using these curves we will incorporate realistic 3D models of radiation damage to non-cuboid crystals (RADDOSE 3D) into our diffraction image simulator (MLFSOM) to yield a 3D Dose Distribution and Illumination map along the crystal. This will result in a new generation of wavelength- dependent absorption factors for the crystal to complement existing absorption corrections. At the beamline, we will measure a 3D map of the crystal using cone beam online x-ray absorption radiography and a 2D map of the beam profile. These advances will allow us to generate zero-dose extrapolation values, in an open format, that account for experimental crystal and beam geometry. To improve multi-crystal averaging, we will begin by characterizing how non-isomorphism varies as a function of humidity, radiation damage, and functional state. By updating the classic “Crick and Magdoff” simulations of non-isomorphism with increasing complexity, we will develop a singular value decomposition approach to parameterize non-isomorphism. Using the corrections derived from this analysis, we will correct the non-isomorphism present in multi-crystal experiments, enabling the determination of novel structures, including those collected using serial crystallography at next-generation light sources. To enable enhanced simulation for robust interpretation of experimental data, we will leverage new solvent models in macromolecular crystallography and small angle X- ray scattering. Our work will create standard protocols for comparing solvent density to alternative interpretations and to quantitatively assess how likely each simulated situation is compared to the real macromolecular crystallography or SAXS data. In addition to distinguishing between different interpretations of the experimental data, improving solvent models will enhance understanding of how macromolecules influence and interact with other molecules near their surface. Collectively, we expect the benefits of eliminating these critical systematic errors be transformative to both methods development and functional studies. PROJECT NARRATIVE Data collection in macromolecular crystallography is subject to significant systematic errors that prevent successful soilution on many systems and, ultimately, limit the accuracy of resulting structures. Creating simulation technologies that can account for these errors will have significant impact on three fronts: 1) solving new structures by better accounting for radiation damage, which is responsible for 80% of failed anomalous phasing attempts, 2) improving multi-crystal averaging by simulating non-isomorphism, which will open the gateway to arbitrary gains in signal-to-noise, 3) discriminating hotly contested alternative interpretations such as the presence or absence of a bound ligand, by creating simulations with more realistic solvent models.",Eliminating Critical Systematic Errors In Structural Biology With Next-Generation Simulation,9707556,R01GM124149,"['Accounting', 'Active Sites', 'Area', 'Complement', 'Computer software', 'Cone', 'Crystallization', 'Crystallography', 'Data', 'Data Collection', 'Data Set', 'Diagnosis', 'Diagnostic radiologic examination', 'Disease', 'Dose', 'Error Sources', 'Evolution', 'Generations', 'Geometry', 'Humidity', 'Image', 'In Situ', 'Knowledge', 'Ligands', 'Light', 'Lighting', 'Maps', 'Measures', 'Metals', 'Methods', 'Minor', 'Modeling', 'Molecular Conformation', 'Muramidase', 'Noise', 'Phase', 'Positioning Attribute', 'Protein Region', 'Protocols documentation', 'Radiation induced damage', 'Reaction', 'Resolution', 'Roentgen Rays', 'Sampling', 'Side', 'Signal Transduction', 'Solvents', 'Source', 'Spottings', 'Structure', 'Surface', 'Synchrotrons', 'Syncope', 'System', 'Technology', 'Update', 'Weight', 'Work', 'absorption', 'beamline', 'computerized tools', 'conformer', 'curve fitting', 'density', 'electron density', 'experimental study', 'improved', 'interest', 'macromolecule', 'method development', 'next generation', 'non-Native', 'novel', 'novel strategies', 'prevent', 'simulation', 'structural biology', 'success', 'three-dimensional modeling', 'trend', 'vector']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2018,40554,0.10200950999737068
"Eliminating Critical Systematic Errors In Structural Biology With Next-Generation Simulation PROJECT SUMMARY/ABSTRACT Data collection in macromolecular crystallography is subject to significant systematic errors that prevent successful data collection on many systems and, ultimately, limit the accuracy of resulting structures. Creating simulation technologies that can account for these errors will have significant impact on three fronts: 1) solving new structures by better accounting for radiation damage, which is responsible for 80% of failed anomalous phasing attempts, 2) improving multi-crystal averaging by simulating non-isomorphism, which will open the gateway to arbitrary gains in signal-to-noise, 3) discriminating hotly contested alternative interpretations such as the presence or absence of a bound ligand, by creating simulations with more realistic solvent models. To move towards “damage-free data” from a synchrotron, we will start by calibrating radiation damage curves on model and DBP samples. Using these curves we will incorporate realistic 3D models of radiation damage to non-cuboid crystals (RADDOSE 3D) into our diffraction image simulator (MLFSOM) to yield a 3D Dose Distribution and Illumination map along the crystal. This will result in a new generation of wavelength- dependent absorption factors for the crystal to complement existing absorption corrections. At the beamline, we will measure a 3D map of the crystal using cone beam online x-ray absorption radiography and a 2D map of the beam profile. These advances will allow us to generate zero-dose extrapolation values, in an open format, that account for experimental crystal and beam geometry. To improve multi-crystal averaging, we will begin by characterizing how non-isomorphism varies as a function of humidity, radiation damage, and functional state. By updating the classic “Crick and Magdoff” simulations of non-isomorphism with increasing complexity, we will develop a singular value decomposition approach to parameterize non-isomorphism. Using the corrections derived from this analysis, we will correct the non-isomorphism present in multi-crystal experiments, enabling the determination of novel structures, including those collected using serial crystallography at next-generation light sources. To enable enhanced simulation for robust interpretation of experimental data, we will leverage new solvent models in macromolecular crystallography and small angle X- ray scattering. Our work will create standard protocols for comparing solvent density to alternative interpretations and to quantitatively assess how likely each simulated situation is compared to the real macromolecular crystallography or SAXS data. In addition to distinguishing between different interpretations of the experimental data, improving solvent models will enhance understanding of how macromolecules influence and interact with other molecules near their surface. Collectively, we expect the benefits of eliminating these critical systematic errors be transformative to both methods development and functional studies. PROJECT NARRATIVE Data collection in macromolecular crystallography is subject to significant systematic errors that prevent successful soilution on many systems and, ultimately, limit the accuracy of resulting structures. Creating simulation technologies that can account for these errors will have significant impact on three fronts: 1) solving new structures by better accounting for radiation damage, which is responsible for 80% of failed anomalous phasing attempts, 2) improving multi-crystal averaging by simulating non-isomorphism, which will open the gateway to arbitrary gains in signal-to-noise, 3) discriminating hotly contested alternative interpretations such as the presence or absence of a bound ligand, by creating simulations with more realistic solvent models.",Eliminating Critical Systematic Errors In Structural Biology With Next-Generation Simulation,9542857,R01GM124149,"['Accounting', 'Active Sites', 'Area', 'Complement', 'Computer software', 'Cone', 'Crystallization', 'Crystallography', 'Data', 'Data Collection', 'Data Set', 'Diagnosis', 'Diagnostic radiologic examination', 'Disease', 'Dose', 'Error Sources', 'Evolution', 'Generations', 'Geometry', 'Humidity', 'Image', 'In Situ', 'Knowledge', 'Ligands', 'Light', 'Lighting', 'Maps', 'Measures', 'Metals', 'Methods', 'Minor', 'Modeling', 'Molecular Conformation', 'Muramidase', 'Noise', 'Phase', 'Positioning Attribute', 'Protein Region', 'Protocols documentation', 'Radiation induced damage', 'Reaction', 'Resolution', 'Roentgen Rays', 'Sampling', 'Side', 'Signal Transduction', 'Solvents', 'Source', 'Spottings', 'Structure', 'Surface', 'Synchrotrons', 'Syncope', 'System', 'Technology', 'Update', 'Weight', 'Work', 'absorption', 'beamline', 'computerized tools', 'conformer', 'curve fitting', 'density', 'electron density', 'experimental study', 'improved', 'interest', 'macromolecule', 'method development', 'next generation', 'non-Native', 'novel', 'novel strategies', 'prevent', 'simulation', 'structural biology', 'success', 'three-dimensional modeling', 'trend', 'vector']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2018,309662,0.10200950999737068
"Eliminating Critical Systematic Errors In Structural Biology With Next-Generation Simulation PROJECT SUMMARY/ABSTRACT Data collection in macromolecular crystallography is subject to significant systematic errors that prevent successful data collection on many systems and, ultimately, limit the accuracy of resulting structures. Creating simulation technologies that can account for these errors will have significant impact on three fronts: 1) solving new structures by better accounting for radiation damage, which is responsible for 80% of failed anomalous phasing attempts, 2) improving multi-crystal averaging by simulating non-isomorphism, which will open the gateway to arbitrary gains in signal-to-noise, 3) discriminating hotly contested alternative interpretations such as the presence or absence of a bound ligand, by creating simulations with more realistic solvent models. To move towards “damage-free data” from a synchrotron, we will start by calibrating radiation damage curves on model and DBP samples. Using these curves we will incorporate realistic 3D models of radiation damage to non-cuboid crystals (RADDOSE 3D) into our diffraction image simulator (MLFSOM) to yield a 3D Dose Distribution and Illumination map along the crystal. This will result in a new generation of wavelength- dependent absorption factors for the crystal to complement existing absorption corrections. At the beamline, we will measure a 3D map of the crystal using cone beam online x-ray absorption radiography and a 2D map of the beam profile. These advances will allow us to generate zero-dose extrapolation values, in an open format, that account for experimental crystal and beam geometry. To improve multi-crystal averaging, we will begin by characterizing how non-isomorphism varies as a function of humidity, radiation damage, and functional state. By updating the classic “Crick and Magdoff” simulations of non-isomorphism with increasing complexity, we will develop a singular value decomposition approach to parameterize non-isomorphism. Using the corrections derived from this analysis, we will correct the non-isomorphism present in multi-crystal experiments, enabling the determination of novel structures, including those collected using serial crystallography at next-generation light sources. To enable enhanced simulation for robust interpretation of experimental data, we will leverage new solvent models in macromolecular crystallography and small angle X- ray scattering. Our work will create standard protocols for comparing solvent density to alternative interpretations and to quantitatively assess how likely each simulated situation is compared to the real macromolecular crystallography or SAXS data. In addition to distinguishing between different interpretations of the experimental data, improving solvent models will enhance understanding of how macromolecules influence and interact with other molecules near their surface. Collectively, we expect the benefits of eliminating these critical systematic errors be transformative to both methods development and functional studies. PROJECT NARRATIVE Data collection in macromolecular crystallography is subject to significant systematic errors that prevent successful soilution on many systems and, ultimately, limit the accuracy of resulting structures. Creating simulation technologies that can account for these errors will have significant impact on three fronts: 1) solving new structures by better accounting for radiation damage, which is responsible for 80% of failed anomalous phasing attempts, 2) improving multi-crystal averaging by simulating non-isomorphism, which will open the gateway to arbitrary gains in signal-to-noise, 3) discriminating hotly contested alternative interpretations such as the presence or absence of a bound ligand, by creating simulations with more realistic solvent models.",Eliminating Critical Systematic Errors In Structural Biology With Next-Generation Simulation,9365573,R01GM124149,"['Accounting', 'Active Sites', 'Area', 'Complement', 'Computer software', 'Cone', 'Crystallization', 'Crystallography', 'Data', 'Data Collection', 'Data Set', 'Diagnosis', 'Diagnostic radiologic examination', 'Disease', 'Dose', 'Error Sources', 'Evolution', 'Generations', 'Geometry', 'Humidity', 'Image', 'In Situ', 'Knowledge', 'Ligands', 'Light', 'Lighting', 'Maps', 'Measures', 'Metals', 'Methods', 'Minor', 'Modeling', 'Molecular Conformation', 'Muramidase', 'Noise', 'Phase', 'Positioning Attribute', 'Protein Region', 'Protocols documentation', 'Radiation induced damage', 'Reaction', 'Resolution', 'Roentgen Rays', 'Sampling', 'Side', 'Signal Transduction', 'Solvents', 'Source', 'Spottings', 'Structure', 'Surface', 'Synchrotrons', 'Syncope', 'System', 'Technology', 'Update', 'Weight', 'Work', 'absorption', 'beamline', 'computerized tools', 'conformer', 'curve fitting', 'density', 'electron density', 'experimental study', 'improved', 'interest', 'macromolecule', 'method development', 'next generation', 'non-Native', 'novel', 'novel strategies', 'prevent', 'simulation', 'structural biology', 'success', 'three-dimensional modeling', 'trend', 'vector']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2017,309710,0.10200950999737068
"Development of an adaptive machine learning platform for automated analysis of biomarkers in biomedical images ABSTRACT Manual analysis of biomedical images by researchers and pathologists has the potential to introduce bias and error that compromise the reliability of research and clinical findings. These problems are significant barriers to delivering the most beneficial evidence-based medicine, developing effective medical treatments, and promoting confidence in scientific inquiry. Identification of biomarkers and cellular targets following microscopy requires manual analysis of biomedical images, which is time intensive, difficult, and prone to bias and errors. Unintentional bias and attentional limitations during analysis of biomarkers can underlie poor reproducibility of findings in biomedical research and potentially introduce error in clinical diagnostics. We recently developed a “beta” software package designed to improve automation and standardization of image analysis, called “PIPSQUEAK” (Perineuronal net Intensity Program for the Standardization and Quantification of Extracellular matrix Analysis Kit). Since its publication in 2016, PIPSQUEAK beta has amassed approximately 1,300 users worldwide who use it to quantify the intensity and number of perineuronal nets and other neural markers in the brain. This technology significantly increases data reliability between image raters and decreases the time required for analysis by more than 100-fold. However, PIPSQUEAK beta currently uses target detection algorithms that require high-contrast images to automatically identify neurons as clusters of bright pixels on dark backgrounds. A significant current limitation to PIPSQUEAK beta, and other available imaging programs, is that detection of biomarkers can be difficult unless image conditions are ideal. Suboptimal conditions, like high background staining, off-target structures, overlapping or clustered biomarkers, and atypical morphologies, can lead to artifacts and consequently to inaccurate results and erroneous conclusions. Here, we propose to develop a user-friendly artificial intelligence (AI) platform for the automated detection of targeted biomarkers in digital microscopy that reduces this error by learning to distinguish between true cellular biomarkers and artifacts. We propose to integrate AI capabilities into our PIPSQUEAK technology to produce an adaptive, high-throughput, biomedical image analysis platform that quickly and accurately identifies biomarker targets from bench to bedside. A key advantage is that this AI program will be user friendly and available online, making it highly accessible to basic researchers and to technicians and clinicians identifying human pathologies. Thus, successful development of our AI program has a high translational potential. The goal of this proposal is 1) to develop and validate a machine learning model that is capable of detecting common histological marker morphologies in digital microscopy, and 2) to test the feasibility of adapting our AI platform to new biomarker datasets with minimal additional supervised training. Our end goal is to advance the reliability and speed of research findings and clinical diagnoses by making this technology widely available to researchers and clinicians. PROJECT NARRATIVE Manual analysis of biomedical images by researchers and pathologists has the potential to introduces bias and error that compromise the reliability of research and clinical findings; problems which are significant barriers to delivering the most beneficial evidence-based medicine and developing effective medical treatments. Application of artificial intelligence for the detection of disease or cellular targets has the potential to improve the reliability of research findings and clinical diagnoses, while reducing waste, time, and expense. We propose a method to improve the quality of biomedical research reproducibility and clinical diagnoses by developing a high-throughput, adaptive artificial intelligence platform for automated analysis of cellular and disease targets in digital microscopy images, which will be made available to scientists and clinicians as a user-friendly analysis platform.",Development of an adaptive machine learning platform for automated analysis of biomarkers in biomedical images,9845994,R43GM134789,"['Abbreviations', 'Algorithms', 'Artificial Intelligence', 'Attention', 'Automation', 'Biological Markers', 'Biomedical Research', 'Brain', 'Cell Line', 'Cell model', 'Cellular Morphology', 'Clinical', 'Computer software', 'Confocal Microscopy', 'Coupled', 'Custom', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Evidence Based Medicine', 'Extracellular Matrix', 'FOS gene', 'Fluorescence', 'Future', 'Glial Fibrillary Acidic Protein', 'Goals', 'Histologic', 'Histology', 'Human Pathology', 'Image', 'Image Analysis', 'Immunoassay', 'Immunohistochemistry', 'Lead', 'Learning', 'Location', 'Machine Learning', 'Manuals', 'Measurement', 'Medical', 'Methods', 'Microscopy', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Neurons', 'Nuclear', 'Pathologist', 'Performance', 'Procedures', 'Psychological Transfer', 'Publications', 'Rattus', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Shapes', 'Speed', 'Stains', 'Standardization', 'Structure', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissue Banks', 'Tissue Model', 'Tissue imaging', 'Tissues', 'Training', 'Zebrafish', 'automated analysis', 'base', 'bench to bedside', 'bioimaging', 'biomarker identification', 'cell type', 'cellular targeting', 'clinical Diagnosis', 'clinical diagnostics', 'contrast imaging', 'design', 'digital', 'digital imaging', 'extracellular', 'histological specimens', 'histological stains', 'imaging biomarker', 'imaging program', 'improved', 'interest', 'lateral line', 'microscopic imaging', 'predictive marker', 'programs', 'relating to nervous system', 'software as a service', 'statistics', 'targeted biomarker', 'tool', 'user-friendly', 'wasting']",NIGMS,"REWIRE NEUROSCIENCE, LLC",R43,2019,224915,0.09769800976393532
"Evaluation of Methods for Estimating Haplotype Effects in Survival Studies    DESCRIPTION (provided by applicant): With heightened interest in the identification of genetic loci that play a role in the causation of disease and interactions between genetic and environmental factors, a great deal of attention has been devoted to the estimation of relative risks for haplotypes in both case-control and cohort studies. Censored survival data stemming from cohort designs are commonly analyzed in genetic epidemiology studies. In such cases, the Cox proportional hazards regression model is typically employed to estimate the association between genetic factors and the time to some event of interest. Although semi-parametric maximum likelihood estimators have been developed for modeling of unobserved haplotypes, such methods are computational complex and are not currently available in standard statistical software packages. As such it would be of great interest to genetic epidemiologists if standard software could be used to estimate and draw valid inference for haplotype associations. In addition, because most retrospective genetic cohort studies sample cases first and controls from the relevant family, methods would ideally allow for valid estimation and inference when particular genotypes are over-sampled and hierarchical clustering exists. The goal of the research proposed here is to provide simple estimates of haplotype relative risks in the setting of censored survival data and to validate such methods via extensive simulation studies. In addition, we will consider the estimation of standard errors under hierarchical clustering and the impact of incorporating inverse probability of sampling weights for cohort studies where particular subgroups have been oversampled.               Project Narrative The goal of this research is to provide simple estimates of haplotype relative risks in the setting of censored survival data and to validate such methods via extensive simulation studies. In addition, we will consider the estimation of standard errors under hierarchical clustering and the impact of incorporating inverse probability of sampling weights for cohort studies where particular subgroups have been oversampled.",Evaluation of Methods for Estimating Haplotype Effects in Survival Studies,7780046,R03CA135691,"['Attention', 'Cohort Studies', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Environmental Risk Factor', 'Epidemiologist', 'Estimation Techniques', 'Etiology', 'Evaluation', 'Event', 'Family', 'Genetic', 'Genetic Models', 'Genotype', 'Goals', 'Haplotypes', 'Light', 'Methods', 'Modeling', 'Performance', 'Play', 'Population Genetics', 'Probability Samples', 'Relative Risks', 'Research', 'Research Design', 'Role', 'Sampling', 'Subgroup', 'Time Factors', 'Weight', 'case control', 'cohort', 'design', 'epidemiology study', 'genetic epidemiology', 'hazard', 'interest', 'large scale simulation', 'simulation', 'stem', 'user friendly software']",NCI,UNIVERSITY OF CALIFORNIA-IRVINE,R03,2010,68888,0.04767776475122355
"Evaluation of Methods for Estimating Haplotype Effects in Survival Studies    DESCRIPTION (provided by applicant): With heightened interest in the identification of genetic loci that play a role in the causation of disease and interactions between genetic and environmental factors, a great deal of attention has been devoted to the estimation of relative risks for haplotypes in both case-control and cohort studies. Censored survival data stemming from cohort designs are commonly analyzed in genetic epidemiology studies. In such cases, the Cox proportional hazards regression model is typically employed to estimate the association between genetic factors and the time to some event of interest. Although semi-parametric maximum likelihood estimators have been developed for modeling of unobserved haplotypes, such methods are computational complex and are not currently available in standard statistical software packages. As such it would be of great interest to genetic epidemiologists if standard software could be used to estimate and draw valid inference for haplotype associations. In addition, because most retrospective genetic cohort studies sample cases first and controls from the relevant family, methods would ideally allow for valid estimation and inference when particular genotypes are over-sampled and hierarchical clustering exists. The goal of the research proposed here is to provide simple estimates of haplotype relative risks in the setting of censored survival data and to validate such methods via extensive simulation studies. In addition, we will consider the estimation of standard errors under hierarchical clustering and the impact of incorporating inverse probability of sampling weights for cohort studies where particular subgroups have been oversampled.               Project Narrative The goal of this research is to provide simple estimates of haplotype relative risks in the setting of censored survival data and to validate such methods via extensive simulation studies. In addition, we will consider the estimation of standard errors under hierarchical clustering and the impact of incorporating inverse probability of sampling weights for cohort studies where particular subgroups have been oversampled.",Evaluation of Methods for Estimating Haplotype Effects in Survival Studies,7659215,R03CA135691,"['Attention', 'Cohort Studies', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Environmental Risk Factor', 'Epidemiologist', 'Estimation Techniques', 'Etiology', 'Evaluation', 'Event', 'Family', 'Genetic', 'Genetic Models', 'Genotype', 'Goals', 'Haplotypes', 'Light', 'Methods', 'Modeling', 'Performance', 'Play', 'Population Genetics', 'Probability Samples', 'Relative Risks', 'Research', 'Research Design', 'Role', 'Sampling', 'Subgroup', 'Time Factors', 'Weight', 'case control', 'cohort', 'design', 'epidemiology study', 'genetic epidemiology', 'hazard', 'interest', 'large scale simulation', 'simulation', 'stem', 'user friendly software']",NCI,UNIVERSITY OF CALIFORNIA-IRVINE,R03,2009,69777,0.04767776475122355
"Digital Pathology_Accuracy Viewing Behavior and Image Characterization DESCRIPTION (provided by applicant): Approximately 1.4 million women per year depend on pathologists to accurately interpret breast biopsies for a diagnosis of benign disease or cancer. Diagnostic errors are alarmingly frequent and likely lead to altered patient treatment, especially at the thresholds of atypical hyperplasia and ductal carcinoma in situ, where up to 50% of cases are misclassified. The causes underlying these errors remain largely unknown.  Technology similar to Google Maps now allows pan and zoom manipulation of high-resolution digital images of glass microscope slides. This technology has virtually replaced the microscope in medical schools and is rapidly diffusing into U.S. pathology practices. No research has evaluated the accuracy and efficiency of pathologists' interpretion of digital images vs. glass slides. However, these ""digital slides"" offer a novel opportunity to study the accuracy, efficiency, and viewing behavior of a large number of pathologists as they manipulate and interpret images. The innovative analytic techniques proposed in this application are similar to those used to improve the performance of pilots and air traffic controllers. Our specific aims are: Aim 1. Digital Image vs. Glass Slides: To compare the interpretive accuracy of pathologists viewing digitized slide images over the Internet to their performance viewing original glass slides under a microscope. A randomized national sample of pathologists (N=200) will interpret 240 test cases in one or both formats in two phases. Measures will include a diagnostic assessment for each test case and for digital slides, cursor- (i.e., mouse) tracking data and region of interest (ROI) markings. Completion of this aim will establish benchmarks for the comparative diagnostic accuracy of whole-slide digital images.  Aim 2. Interpretive Screening Behavior: To identify visual scanning patterns associated with diagnostic accuracy and efficiency. Detailed simultaneous eye-tracking and cursor-tracking data will be collected on 60 additional pathologists while they interpret digital slides to complement data from Aim 1. Viewing patterns will be analyzed from computer representations of raw movement data. Videos depicting accurate, efficient visual scanning and cursor movement will be valuable tools in educating the next generation of digital pathologists. Aim 3. Image Analyses: To examine and classify the image characteristics (including color, texture, and structure) of ROIs captured in Aims 1 and 2. Computer-based statistical learning techniques will be used to identify image characteristics that lead to correct and incorrect diagnoses. Characteristics of both diagnostic and distracting ROIs will be identified, linking all three aims. In summary, we will determine whether digitized whole-slide images are diagnostically equivalent to original glass slides. Our in-depth scientific evaluation of viewing patterns and characteristics of ROIs identified by pathologists will be critical to understanding diagnostic errors and sources of distraction. Optimization of viewing techniques will improve diagnostic performance and thus, the quality of clinical care. PUBLIC HEALTH RELEVANCE: Pathologist errors in the interpretation of biopsy specimens can result in significant harm to patients. Digital imaging technology is rapidly diffusing into medical settings, providing a unique opportunity to obtain data on the process used by pathologists when interpreting slides. The results will provide the foundation needed to improve interpretive accuracy and efficiency, support quality improvement efforts, and ultimately reduce the burden of misdiagnosis on patients and the health care delivery system.",Digital Pathology_Accuracy Viewing Behavior and Image Characterization,9194390,R01CA172343,"['Adoption', 'Air', 'Archives', 'Area', 'Attention', 'Atypical hyperplasia', 'Behavior', 'Benchmarking', 'Benign', 'Biopsy Specimen', 'Breast biopsy', 'Characteristics', 'Clinical', 'Color', 'Communities', 'Community Hospitals', 'Comparative Study', 'Complement', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Developing Countries', 'Diagnosis', 'Diagnostic', 'Diagnostic Errors', 'Diagnostic Imaging', 'Diffuse', 'Disease', 'Ensure', 'Error Sources', 'Eye', 'Foundations', 'Glass', 'Goals', 'Gold', 'Histopathology', 'Image', 'Image Analysis', 'Imaging technology', 'Internet', 'Investigation', 'Lead', 'Link', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medical Device', 'Medical Errors', 'Microscope', 'Microscopic', 'Movement', 'Mus', 'Noninfiltrating Intraductal Carcinoma', 'Pathologist', 'Pathology', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physicians', 'Process', 'Randomized', 'Recommendation', 'Research', 'Research Personnel', 'Resolution', 'Risk', 'Rural Hospitals', 'Sampling', 'Scanning', 'Scientific Evaluation', 'Slide', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Time', 'Training', 'Visit', 'Visual', 'Woman', 'analytical method', 'base', 'clinical care', 'comparative', 'computer monitor', 'diagnosis standard', 'diagnostic accuracy', 'digital', 'digital imaging', 'digital media', 'distraction', 'eye hand coordination', 'health care delivery', 'imaging system', 'improved', 'innovation', 'innovative technologies', 'interest', 'medical schools', 'migration', 'new technology', 'next generation', 'novel', 'pedagogy', 'public health relevance', 'sample fixation', 'screening', 'tool', 'transmission process', 'virtual']",NCI,UNIVERSITY OF WASHINGTON,R01,2017,611703,0.08152479126867856
"Digital Pathology_Accuracy Viewing Behavior and Image Characterization DESCRIPTION (provided by applicant): Approximately 1.4 million women per year depend on pathologists to accurately interpret breast biopsies for a diagnosis of benign disease or cancer. Diagnostic errors are alarmingly frequent and likely lead to altered patient treatment, especially at the thresholds of atypical hyperplasia and ductal carcinoma in situ, where up to 50% of cases are misclassified. The causes underlying these errors remain largely unknown.  Technology similar to Google Maps now allows pan and zoom manipulation of high-resolution digital images of glass microscope slides. This technology has virtually replaced the microscope in medical schools and is rapidly diffusing into U.S. pathology practices. No research has evaluated the accuracy and efficiency of pathologists' interpretion of digital images vs. glass slides. However, these ""digital slides"" offer a novel opportunity to study the accuracy, efficiency, and viewing behavior of a large number of pathologists as they manipulate and interpret images. The innovative analytic techniques proposed in this application are similar to those used to improve the performance of pilots and air traffic controllers. Our specific aims are: Aim 1. Digital Image vs. Glass Slides: To compare the interpretive accuracy of pathologists viewing digitized slide images over the Internet to their performance viewing original glass slides under a microscope. A randomized national sample of pathologists (N=200) will interpret 240 test cases in one or both formats in two phases. Measures will include a diagnostic assessment for each test case and for digital slides, cursor- (i.e., mouse) tracking data and region of interest (ROI) markings. Completion of this aim will establish benchmarks for the comparative diagnostic accuracy of whole-slide digital images.  Aim 2. Interpretive Screening Behavior: To identify visual scanning patterns associated with diagnostic accuracy and efficiency. Detailed simultaneous eye-tracking and cursor-tracking data will be collected on 60 additional pathologists while they interpret digital slides to complement data from Aim 1. Viewing patterns will be analyzed from computer representations of raw movement data. Videos depicting accurate, efficient visual scanning and cursor movement will be valuable tools in educating the next generation of digital pathologists. Aim 3. Image Analyses: To examine and classify the image characteristics (including color, texture, and structure) of ROIs captured in Aims 1 and 2. Computer-based statistical learning techniques will be used to identify image characteristics that lead to correct and incorrect diagnoses. Characteristics of both diagnostic and distracting ROIs will be identified, linking all three aims. In summary, we will determine whether digitized whole-slide images are diagnostically equivalent to original glass slides. Our in-depth scientific evaluation of viewing patterns and characteristics of ROIs identified by pathologists will be critical to understanding diagnostic errors and sources of distraction. Optimization of viewing techniques will improve diagnostic performance and thus, the quality of clinical care. PUBLIC HEALTH RELEVANCE: Pathologist errors in the interpretation of biopsy specimens can result in significant harm to patients. Digital imaging technology is rapidly diffusing into medical settings, providing a unique opportunity to obtain data on the process used by pathologists when interpreting slides. The results will provide the foundation needed to improve interpretive accuracy and efficiency, support quality improvement efforts, and ultimately reduce the burden of misdiagnosis on patients and the health care delivery system.",Digital Pathology_Accuracy Viewing Behavior and Image Characterization,8970690,R01CA172343,"['Adoption', 'Air', 'Archives', 'Area', 'Attention', 'Atypical hyperplasia', 'Behavior', 'Benchmarking', 'Benign', 'Biopsy Specimen', 'Breast biopsy', 'Characteristics', 'Clinical', 'Color', 'Communities', 'Community Hospitals', 'Comparative Study', 'Complement', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Developing Countries', 'Diagnosis', 'Diagnostic', 'Diagnostic Errors', 'Diffuse', 'Disease', 'Ensure', 'Error Sources', 'Eye', 'Foundations', 'Glass', 'Goals', 'Gold', 'Health', 'Histopathology', 'Image', 'Image Analysis', 'Imaging Techniques', 'Imaging technology', 'Internet', 'Lead', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medical Device', 'Medical Errors', 'Methods', 'Microscope', 'Microscopic', 'Movement', 'Mus', 'Noninfiltrating Intraductal Carcinoma', 'Pathologist', 'Pathology', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physicians', 'Process', 'Randomized', 'Recommendation', 'Research', 'Research Personnel', 'Resolution', 'Risk', 'Rural', 'Rural Hospitals', 'Sampling', 'Scanning', 'Scientific Evaluation', 'Slide', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Time', 'Training', 'Visit', 'Visual', 'Woman', 'base', 'clinical care', 'comparative', 'computer monitor', 'diagnosis standard', 'diagnostic accuracy', 'digital', 'digital imaging', 'digital media', 'distraction', 'eye hand coordination', 'health care delivery', 'imaging system', 'improved', 'innovation', 'innovative technologies', 'interest', 'medical schools', 'migration', 'new technology', 'next generation', 'novel', 'pedagogy', 'sample fixation', 'screening', 'tool', 'trafficking', 'transmission process']",NCI,UNIVERSITY OF WASHINGTON,R01,2016,624643,0.08152479126867856
"Digital Pathology_Accuracy Viewing Behavior and Image Characterization DESCRIPTION (provided by applicant): Approximately 1.4 million women per year depend on pathologists to accurately interpret breast biopsies for a diagnosis of benign disease or cancer. Diagnostic errors are alarmingly frequent and likely lead to altered patient treatment, especially at the thresholds of atypical hyperplasia and ductal carcinoma in situ, where up to 50% of cases are misclassified. The causes underlying these errors remain largely unknown.  Technology similar to Google Maps now allows pan and zoom manipulation of high-resolution digital images of glass microscope slides. This technology has virtually replaced the microscope in medical schools and is rapidly diffusing into U.S. pathology practices. No research has evaluated the accuracy and efficiency of pathologists' interpretion of digital images vs. glass slides. However, these ""digital slides"" offer a novel opportunity to study the accuracy, efficiency, and viewing behavior of a large number of pathologists as they manipulate and interpret images. The innovative analytic techniques proposed in this application are similar to those used to improve the performance of pilots and air traffic controllers. Our specific aims are: Aim 1. Digital Image vs. Glass Slides: To compare the interpretive accuracy of pathologists viewing digitized slide images over the Internet to their performance viewing original glass slides under a microscope. A randomized national sample of pathologists (N=200) will interpret 240 test cases in one or both formats in two phases. Measures will include a diagnostic assessment for each test case and for digital slides, cursor- (i.e., mouse) tracking data and region of interest (ROI) markings. Completion of this aim will establish benchmarks for the comparative diagnostic accuracy of whole-slide digital images.  Aim 2. Interpretive Screening Behavior: To identify visual scanning patterns associated with diagnostic accuracy and efficiency. Detailed simultaneous eye-tracking and cursor-tracking data will be collected on 60 additional pathologists while they interpret digital slides to complement data from Aim 1. Viewing patterns will be analyzed from computer representations of raw movement data. Videos depicting accurate, efficient visual scanning and cursor movement will be valuable tools in educating the next generation of digital pathologists. Aim 3. Image Analyses: To examine and classify the image characteristics (including color, texture, and structure) of ROIs captured in Aims 1 and 2. Computer-based statistical learning techniques will be used to identify image characteristics that lead to correct and incorrect diagnoses. Characteristics of both diagnostic and distracting ROIs will be identified, linking all three aims. In summary, we will determine whether digitized whole-slide images are diagnostically equivalent to original glass slides. Our in-depth scientific evaluation of viewing patterns and characteristics of ROIs identified by pathologists will be critical to understanding diagnostic errors and sources of distraction. Optimization of viewing techniques will improve diagnostic performance and thus, the quality of clinical care. PUBLIC HEALTH RELEVANCE: Pathologist errors in the interpretation of biopsy specimens can result in significant harm to patients. Digital imaging technology is rapidly diffusing into medical settings, providing a unique opportunity to obtain data on the process used by pathologists when interpreting slides. The results will provide the foundation needed to improve interpretive accuracy and efficiency, support quality improvement efforts, and ultimately reduce the burden of misdiagnosis on patients and the health care delivery system.",Digital Pathology_Accuracy Viewing Behavior and Image Characterization,8771432,R01CA172343,"['Adoption', 'Air', 'Archives', 'Area', 'Attention', 'Atypical hyperplasia', 'Behavior', 'Benchmarking', 'Benign', 'Biopsy Specimen', 'Breast biopsy', 'Characteristics', 'Clinical', 'Color', 'Communities', 'Community Hospitals', 'Comparative Study', 'Complement', 'Complex', 'Computers', 'Data', 'Developing Countries', 'Diagnosis', 'Diagnostic', 'Diagnostic Errors', 'Diffuse', 'Disease', 'Ensure', 'Error Sources', 'Eye', 'Foundations', 'Glass', 'Goals', 'Gold', 'Health', 'Histopathology', 'Image', 'Image Analysis', 'Imaging Techniques', 'Imaging technology', 'Internet', 'Lead', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medical Device', 'Medical Errors', 'Methods', 'Microscope', 'Microscopic', 'Movement', 'Mus', 'Noninfiltrating Intraductal Carcinoma', 'Pathologist', 'Pathology', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physicians', 'Process', 'Randomized', 'Recommendation', 'Research', 'Research Personnel', 'Resolution', 'Risk', 'Rural', 'Rural Hospitals', 'Sampling', 'Scanning', 'Scientific Evaluation', 'Slide', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Time', 'Training', 'Visit', 'Visual', 'Woman', 'base', 'clinical care', 'comparative', 'computer monitor', 'diagnosis standard', 'diagnostic accuracy', 'digital', 'digital imaging', 'distraction', 'eye hand coordination', 'health care delivery', 'imaging system', 'improved', 'innovation', 'innovative technologies', 'interest', 'medical schools', 'migration', 'new technology', 'next generation', 'novel', 'sample fixation', 'screening', 'tool', 'trafficking', 'transmission process']",NCI,UNIVERSITY OF WASHINGTON,R01,2015,639475,0.08152479126867856
"Digital Pathology_Accuracy Viewing Behavior and Image Characterization     DESCRIPTION (provided by applicant): Approximately 1.4 million women per year depend on pathologists to accurately interpret breast biopsies for a diagnosis of benign disease or cancer. Diagnostic errors are alarmingly frequent and likely lead to altered patient treatment, especially at the thresholds of atypical hyperplasia and ductal carcinoma in situ, where up to 50% of cases are misclassified. The causes underlying these errors remain largely unknown.  Technology similar to Google Maps now allows pan and zoom manipulation of high-resolution digital images of glass microscope slides. This technology has virtually replaced the microscope in medical schools and is rapidly diffusing into U.S. pathology practices. No research has evaluated the accuracy and efficiency of pathologists' interpretion of digital images vs. glass slides. However, these ""digital slides"" offer a novel opportunity to study the accuracy, efficiency, and viewing behavior of a large number of pathologists as they manipulate and interpret images. The innovative analytic techniques proposed in this application are similar to those used to improve the performance of pilots and air traffic controllers. Our specific aims are: Aim 1. Digital Image vs. Glass Slides: To compare the interpretive accuracy of pathologists viewing digitized slide images over the Internet to their performance viewing original glass slides under a microscope. A randomized national sample of pathologists (N=200) will interpret 240 test cases in one or both formats in two phases. Measures will include a diagnostic assessment for each test case and for digital slides, cursor- (i.e., mouse) tracking data and region of interest (ROI) markings. Completion of this aim will establish benchmarks for the comparative diagnostic accuracy of whole-slide digital images.  Aim 2. Interpretive Screening Behavior: To identify visual scanning patterns associated with diagnostic accuracy and efficiency. Detailed simultaneous eye-tracking and cursor-tracking data will be collected on 60 additional pathologists while they interpret digital slides to complement data from Aim 1. Viewing patterns will be analyzed from computer representations of raw movement data. Videos depicting accurate, efficient visual scanning and cursor movement will be valuable tools in educating the next generation of digital pathologists. Aim 3. Image Analyses: To examine and classify the image characteristics (including color, texture, and structure) of ROIs captured in Aims 1 and 2. Computer-based statistical learning techniques will be used to identify image characteristics that lead to correct and incorrect diagnoses. Characteristics of both diagnostic and distracting ROIs will be identified, linking all three aims. In summary, we will determine whether digitized whole-slide images are diagnostically equivalent to original glass slides. Our in-depth scientific evaluation of viewing patterns and characteristics of ROIs identified by pathologists will be critical to understanding diagnostic errors and sources of distraction. Optimization of viewing techniques will improve diagnostic performance and thus, the quality of clinical care.         PUBLIC HEALTH RELEVANCE: Pathologist errors in the interpretation of biopsy specimens can result in significant harm to patients. Digital imaging technology is rapidly diffusing into medical settings, providing a unique opportunity to obtain data on the process used by pathologists when interpreting slides. The results will provide the foundation needed to improve interpretive accuracy and efficiency, support quality improvement efforts, and ultimately reduce the burden of misdiagnosis on patients and the health care delivery system.            ",Digital Pathology_Accuracy Viewing Behavior and Image Characterization,8601692,R01CA172343,"['Adoption', 'Air', 'Archives', 'Area', 'Attention', 'Atypical hyperplasia', 'Behavior', 'Benchmarking', 'Benign', 'Biopsy', 'Biopsy Specimen', 'Breast', 'Characteristics', 'Clinical', 'Color', 'Communities', 'Community Hospitals', 'Comparative Study', 'Complement', 'Complex', 'Computers', 'Data', 'Developing Countries', 'Diagnosis', 'Diagnostic', 'Diagnostic Errors', 'Diffuse', 'Disease', 'Ensure', 'Error Sources', 'Eye', 'Foundations', 'Glass', 'Goals', 'Gold', 'Histopathology', 'Image', 'Image Analysis', 'Imaging Techniques', 'Imaging technology', 'Internet', 'Lead', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medical Device', 'Medical Errors', 'Methods', 'Microscope', 'Microscopic', 'Movement', 'Mus', 'Noninfiltrating Intraductal Carcinoma', 'Pathologist', 'Pathology', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physicians', 'Process', 'Randomized', 'Recommendation', 'Research', 'Research Personnel', 'Resolution', 'Risk', 'Rural', 'Rural Hospitals', 'Sampling', 'Scanning', 'Scientific Evaluation', 'Slide', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Time', 'Training', 'Visit', 'Visual', 'Woman', 'base', 'clinical care', 'comparative', 'computer monitor', 'diagnosis standard', 'diagnostic accuracy', 'digital', 'digital imaging', 'distraction', 'eye hand coordination', 'health care delivery', 'improved', 'innovation', 'innovative technologies', 'interest', 'medical schools', 'migration', 'new technology', 'next generation', 'novel', 'public health relevance', 'sample fixation', 'screening', 'tool', 'trafficking', 'transmission process']",NCI,UNIVERSITY OF WASHINGTON,R01,2014,623127,0.08152479126867856
"Digital Pathology_Accuracy Viewing Behavior and Image Characterization     DESCRIPTION (provided by applicant): Approximately 1.4 million women per year depend on pathologists to accurately interpret breast biopsies for a diagnosis of benign disease or cancer. Diagnostic errors are alarmingly frequent and likely lead to altered patient treatment, especially at the thresholds of atypical hyperplasia and ductal carcinoma in situ, where up to 50% of cases are misclassified. The causes underlying these errors remain largely unknown.  Technology similar to Google Maps now allows pan and zoom manipulation of high-resolution digital images of glass microscope slides. This technology has virtually replaced the microscope in medical schools and is rapidly diffusing into U.S. pathology practices. No research has evaluated the accuracy and efficiency of pathologists' interpretion of digital images vs. glass slides. However, these ""digital slides"" offer a novel opportunity to study the accuracy, efficiency, and viewing behavior of a large number of pathologists as they manipulate and interpret images. The innovative analytic techniques proposed in this application are similar to those used to improve the performance of pilots and air traffic controllers. Our specific aims are: Aim 1. Digital Image vs. Glass Slides: To compare the interpretive accuracy of pathologists viewing digitized slide images over the Internet to their performance viewing original glass slides under a microscope. A randomized national sample of pathologists (N=200) will interpret 240 test cases in one or both formats in two phases. Measures will include a diagnostic assessment for each test case and for digital slides, cursor- (i.e., mouse) tracking data and region of interest (ROI) markings. Completion of this aim will establish benchmarks for the comparative diagnostic accuracy of whole-slide digital images.  Aim 2. Interpretive Screening Behavior: To identify visual scanning patterns associated with diagnostic accuracy and efficiency. Detailed simultaneous eye-tracking and cursor-tracking data will be collected on 60 additional pathologists while they interpret digital slides to complement data from Aim 1. Viewing patterns will be analyzed from computer representations of raw movement data. Videos depicting accurate, efficient visual scanning and cursor movement will be valuable tools in educating the next generation of digital pathologists. Aim 3. Image Analyses: To examine and classify the image characteristics (including color, texture, and structure) of ROIs captured in Aims 1 and 2. Computer-based statistical learning techniques will be used to identify image characteristics that lead to correct and incorrect diagnoses. Characteristics of both diagnostic and distracting ROIs will be identified, linking all three aims. In summary, we will determine whether digitized whole-slide images are diagnostically equivalent to original glass slides. Our in-depth scientific evaluation of viewing patterns and characteristics of ROIs identified by pathologists will be critical to understanding diagnostic errors and sources of distraction. Optimization of viewing techniques will improve diagnostic performance and thus, the quality of clinical care.         PUBLIC HEALTH RELEVANCE: Pathologist errors in the interpretation of biopsy specimens can result in significant harm to patients. Digital imaging technology is rapidly diffusing into medical settings, providing a unique opportunity to obtain data on the process used by pathologists when interpreting slides. The results will provide the foundation needed to improve interpretive accuracy and efficiency, support quality improvement efforts, and ultimately reduce the burden of misdiagnosis on patients and the health care delivery system.            ",Digital Pathology_Accuracy Viewing Behavior and Image Characterization,8420220,R01CA172343,"['Adoption', 'Air', 'Archives', 'Area', 'Attention', 'Atypical hyperplasia', 'Behavior', 'Benchmarking', 'Benign', 'Biopsy', 'Biopsy Specimen', 'Breast', 'Characteristics', 'Clinical', 'Color', 'Communities', 'Community Hospitals', 'Comparative Study', 'Complement', 'Complex', 'Computers', 'Data', 'Developing Countries', 'Diagnosis', 'Diagnostic', 'Diagnostic Errors', 'Diffuse', 'Disease', 'Ensure', 'Error Sources', 'Eye', 'Foundations', 'Glass', 'Goals', 'Gold', 'Histopathology', 'Image', 'Image Analysis', 'Imaging Techniques', 'Imaging technology', 'Internet', 'Lead', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medical Device', 'Medical Errors', 'Methods', 'Microscope', 'Microscopic', 'Movement', 'Mus', 'Noninfiltrating Intraductal Carcinoma', 'Pathologist', 'Pathology', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physicians', 'Process', 'Randomized', 'Recommendation', 'Research', 'Research Personnel', 'Resolution', 'Risk', 'Rural', 'Rural Hospitals', 'Sampling', 'Scanning', 'Scientific Evaluation', 'Slide', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Time', 'Training', 'Visit', 'Visual', 'Woman', 'base', 'clinical care', 'comparative', 'computer monitor', 'diagnosis standard', 'diagnostic accuracy', 'digital', 'digital imaging', 'distraction', 'eye hand coordination', 'health care delivery', 'improved', 'innovation', 'innovative technologies', 'interest', 'medical schools', 'migration', 'new technology', 'next generation', 'novel', 'public health relevance', 'sample fixation', 'screening', 'tool', 'trafficking', 'transmission process']",NCI,UNIVERSITY OF WASHINGTON,R01,2013,682340,0.08152479126867856
"An Automated Patient Chart Error Detection System for Radiation Therapy Project Summary/Abstract Every year, approximately 1,200 severe mistreatments happen in radiation therapy. Radiation therapy lawsuits rank in the top third of all medical specialties with an average of $313,000 per claim settled or litigated. The current method for detecting treatment errors is by a weekly patient chart check, where each treatment record is manually reviewed on a weekly basis. This labor-intensive and inefficient method prevents us from detecting the treatment error at an early stage. Here we propose a novel software system, ChartAlert, for automating patient chart checking. ChartAlert is a prospective real-time adaptive electronic checking system that can be configured to support different clinical workflows and perform “smart” check using artificial intelligence. It supports two major treatment databases (Elekta MOSAIQ and Varian ARIA) in radiotherapy. In Phase I project, we have successfully developed ChartAlert for MOSAIQ prototype that is under clinical testing in two treatment centers. Our preliminary results demonstrated the significant improvement of effectiveness in patient chart checking and the flexibility of supporting different workflows. In this Phase II proposal, we will continue the ChartAlert development. We will demonstrate the feasibility of the ChartAlert approach and its advantages over the standard manual checking method. We will develop a prospective checking module, develop the ARIA data translation module for ChartAlert for ARIA, design and implement an AI-based “smart” check module, and verify the proposed system at the partner sites. Successful completion of these aims will demonstrate the feasibility and commercial potential of the ChartAlert approach. Ultimately, this work will result in an intelligent patient chart checking software, which will increase patient chart check efficiency, save staff time, improve cancer patient treatment safety, and preventing potential lawsuits. Project Narrative Treatment errors in radiation oncology occur at a rate of 2% per patient, and radiation therapy lawsuits rank in the top third of all medical specialties with regard to claims made, claims paid, and damages. Current methods of detecting treatment errors are manual and inefficient. There is a critical need for efficient treatment error detection in order to improve patient safety and save cost. We propose to develop a scalable and comprehensive software system (ChartAlert) for automated patient chart error detection in radiation therapy. ChartAlert can be extended to other types of patient charts to check treatment and prescription consistency and improve patient safety.",An Automated Patient Chart Error Detection System for Radiation Therapy,10015207,R42CA195819,"['Affect', 'Area', 'Artificial Intelligence', 'Cancer Patient', 'Cessation of life', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Data Analytics', 'Databases', 'Detection', 'Development', 'Effectiveness', 'Goals', 'Health Care Costs', 'Human', 'Human Resources', 'Individual', 'Information Systems', 'Innovation Corps', 'Insurance', 'Intelligence', 'Knowledge', 'Lead', 'Letters', 'Machine Learning', 'Manuals', 'Medical', 'Medical Technology', 'Methods', 'Paralysed', 'Patients', 'Pattern', 'Performance', 'Phase', 'Prevention', 'Radiation Oncology', 'Radiation therapy', 'Records', 'Safety', 'Site', 'Small Business Technology Transfer Research', 'Software Framework', 'System', 'Technology', 'Time', 'Training', 'Translations', 'United States', 'United States National Institutes of Health', 'Vendor', 'Work', 'base', 'commercialization', 'cost', 'data sharing', 'data streams', 'data structure', 'design', 'flexibility', 'improved', 'industry partner', 'maltreatment', 'medical specialties', 'member', 'novel', 'patient safety', 'prevent', 'programs', 'prospective', 'prototype', 'research and development', 'research clinical testing', 'software development', 'software systems', 'structured data', 'success', 'treatment center', 'treatment planning', 'treatment site', 'tumor']",NCI,"INFONDRIAN, LLC",R42,2020,855016,0.14896222841131124
"An Automated Patient Chart Error Detection System for Radiation Therapy Project Summary/Abstract Every year, approximately 1,200 severe mistreatments happen in radiation therapy. Radiation therapy lawsuits rank in the top third of all medical specialties with an average of $313,000 per claim settled or litigated. The current method for detecting treatment errors is by a weekly patient chart check, where each treatment record is manually reviewed on a weekly basis. This labor-intensive and inefficient method prevents us from detecting the treatment error at an early stage. Here we propose a novel software system, ChartAlert, for automating patient chart checking. ChartAlert is a prospective real-time adaptive electronic checking system that can be configured to support different clinical workflows and perform “smart” check using artificial intelligence. It supports two major treatment databases (Elekta MOSAIQ and Varian ARIA) in radiotherapy. In Phase I project, we have successfully developed ChartAlert for MOSAIQ prototype that is under clinical testing in two treatment centers. Our preliminary results demonstrated the significant improvement of effectiveness in patient chart checking and the flexibility of supporting different workflows. In this Phase II proposal, we will continue the ChartAlert development. We will demonstrate the feasibility of the ChartAlert approach and its advantages over the standard manual checking method. We will develop a prospective checking module, develop the ARIA data translation module for ChartAlert for ARIA, design and implement an AI-based “smart” check module, and verify the proposed system at the partner sites. Successful completion of these aims will demonstrate the feasibility and commercial potential of the ChartAlert approach. Ultimately, this work will result in an intelligent patient chart checking software, which will increase patient chart check efficiency, save staff time, improve cancer patient treatment safety, and preventing potential lawsuits. Project Narrative Treatment errors in radiation oncology occur at a rate of 2% per patient, and radiation therapy lawsuits rank in the top third of all medical specialties with regard to claims made, claims paid, and damages. Current methods of detecting treatment errors are manual and inefficient. There is a critical need for efficient treatment error detection in order to improve patient safety and save cost. We propose to develop a scalable and comprehensive software system (ChartAlert) for automated patient chart error detection in radiation therapy. ChartAlert can be extended to other types of patient charts to check treatment and prescription consistency and improve patient safety.",An Automated Patient Chart Error Detection System for Radiation Therapy,9845105,R42CA195819,"['Affect', 'Area', 'Artificial Intelligence', 'Cancer Patient', 'Cessation of life', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Data Analytics', 'Databases', 'Detection', 'Development', 'Effectiveness', 'Goals', 'Health Care Costs', 'Human', 'Human Resources', 'Individual', 'Information Systems', 'Innovation Corps', 'Insurance', 'Intelligence', 'Knowledge', 'Lead', 'Letters', 'Machine Learning', 'Manuals', 'Medical', 'Medical Technology', 'Methods', 'Paralysed', 'Patients', 'Pattern', 'Performance', 'Phase', 'Prevention', 'Radiation Oncology', 'Radiation therapy', 'Records', 'Safety', 'Site', 'Small Business Technology Transfer Research', 'Software Framework', 'Stream', 'System', 'Technology', 'Time', 'Training', 'Translations', 'United States', 'United States National Institutes of Health', 'Vendor', 'Work', 'base', 'commercialization', 'cost', 'data sharing', 'data structure', 'design', 'flexibility', 'improved', 'industry partner', 'maltreatment', 'medical specialties', 'member', 'novel', 'patient safety', 'prevent', 'programs', 'prospective', 'prototype', 'research and development', 'research clinical testing', 'software development', 'software systems', 'success', 'treatment center', 'treatment planning', 'treatment site', 'tumor']",NCI,"INFONDRIAN, LLC",R42,2019,894987,0.14896222841131124
"AC/DC: Artificial intelligence and Computer visioning to assess Dietary Composition ABSTRACT Dietary intake is a complex human behavior that drives disease risk and corresponding economic and healthcare burdens worldwide. Poor diet is the leading cause of death in the US and a known driver of obesity – a global epidemic. A major contributor to poor diet is food eaten away from home, such as restaurant foods. Research has shown that tracking one’s weight and dietary intake significantly improve success toward weight loss and maintenance goals; however, this type of tracking is burdensome, prone to error, and difficult to estimate for restaurant foods. Accurate approaches and tools to evaluate food and nutrient intake are essential in monitoring the nutritional status of individuals. There is a critical need for real-time data capture that minimizes burden and reduces error. While progress has been made, there is no tool available that accurately and automatically estimates foods left unconsumed in a meal. Two major limitations of existing systems is the reliance of a fiducial marker for food detection and volume estimation, and reliance on humans – either the respondent or a trained researcher – to estimate the portion of food leftover. This application leverages novel technology to remove those limitations. The long-term research goal is to utilize digital imaging (DI), artificial intelligence (AI) and computer vision (CV) techniques to develop a novel hybrid methodology for rapid, accurate measurement of dietary intake. To attain this goal, our objective in this R21 application is to refine and test a system architecture that (a) uses digital images to record dietary intake in real-time and (b) uses AI and CV techniques to identify food/beverage items and determine amounts leftover. We plan to build on our current prototype in which digital food images are captured before and after the meal, analyzed to detect the food items, a three-dimensional (3-D) virtual model constructed, and volume remaining after the meal estimated, which will be used to calculate the amount leftover based on the initial volume. Volume consumed will be converted to weight and linked to public-use nutrition information. These calorie estimates will be compared against calories those from (a) DIs coded by trained research staff and (b) weighed plate waste methodology. Our expectation is to develop a valid system architecture for rapidly estimating dietary intake. The outcome of this proposal is expected to have a significant positive impact, enabling nutrition and health researchers to collect high-quality food consumption data in real world settings, increasing knowledge of dietary patterns and improving capacity to assess dietary interventions. This work will lead to an R01 application that will expand food types and meal settings and test the utility of our system among consumers. Project Narrative Solutions to address the global obesity epidemic are urgently needed. Research has shown that tracking one’s weight and dietary intake significantly improve success toward weight loss and maintenance goals; however, this type of tracking is burdensome, prone to error, and difficult to estimate for restaurant foods, a known driver of obesity. This study integrates nutrition science, computer science, and engineering to develop and test a new method for assessing dietary intake, and if successful would yield a rapid, reliable, accurate and cost- effective tool.",AC/DC: Artificial intelligence and Computer visioning to assess Dietary Composition,9978495,R21CA250024,"['3-Dimensional', 'Address', 'Algorithms', 'Artificial Intelligence', 'Assessment tool', 'Behavior', 'Beverages', 'Body Weight decreased', 'Calories', 'Cause of Death', 'Cellular Phone', 'Code', 'Complex', 'Computer Vision Systems', 'Consumption', 'Data', 'Databases', 'Detection', 'Development', 'Diet', 'Diet Records', 'Dietary Assessment', 'Dietary Intervention', 'Dietary Practices', 'Dietary intake', 'Economics', 'Engineering', 'Epidemic', 'Food', 'Goals', 'Gold', 'Health', 'Healthcare', 'Home environment', 'Human', 'Human Resources', 'Hybrids', 'Image', 'Imaging Techniques', 'Individual', 'Intake', 'Intervention', 'Knowledge', 'Left', 'Link', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Monitor', 'Nutrient', 'Nutritional Science', 'Nutritional status', 'Obesity', 'Obesity Epidemic', 'Outcome', 'Output', 'Participant', 'Research', 'Research Personnel', 'Research Training', 'Respondent', 'Restaurants', 'Side', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Validation', 'Weight', 'Work', 'base', 'computer science', 'cost', 'cost effective', 'design', 'digital', 'digital imaging', 'disorder risk', 'expectation', 'food consumption', 'food quality', 'handheld mobile device', 'improved', 'knowledge base', 'new technology', 'novel', 'nutrition', 'prototype', 'success', 'system architecture', 'tool', 'virtual model', 'wasting', 'weight maintenance']",NCI,TUFTS UNIVERSITY BOSTON,R21,2020,222002,0.06071434705171014

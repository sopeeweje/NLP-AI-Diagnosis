text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES The goal of this proposal is to generate method and tools to link                knowledge-based systems (KBSs) to real clinical databases.  The linking          is done via quires that map conceptual entities in the KBS to actual             entries in the database. They may be used when a KBS is first created,           when an existing KBS is linked to a database or when a KBS is transferred        to another institution.  The primary purpose of the queries is to apply          the KBS to individual patients for direct patient care, rather than to           extend the knowledge base itself.  Several of the tools (Aims 3 and 4)           are specific to the Arden Syntax for Medical Logic Modules (MLMs).  All          the proposed tools will be applied to CPMC's MLM knowledge base, which           is actively used for patient care.                                                                                                                                AIM 1 to increase the accuracy while reducing the writing time and               technical skills required to author clinical database queries.  Query by         Review will provide a familiar interface to novice query authors,                enabling them to write queries by traversing the same type of result             review screens that they use every day in clinical care. When data are           selected from the screen, the tools will generate an appropriate query           (in Arden Syntax and HL7) that can be inserted into a KBS or used for            clinical research.                                                                                                                                                AIM 2 to facilitate the testing of queries, and to improve the match             between a query's result and the needs to a KBS.  The Clinical Database          Brower will allow author to characterize the data returned by a query in         order to determine if they query is appropriate.  It will also allow the         author to determine whether additional logic is necessary to convert the         raw data into form expected by the KBS.  The Brower's design is unique           in its use of a semantic network to aggregate complex categorical data.                                                                                           AIM 3 to provide an environment for inserting queries and additional             logic into the KBS.  The Advanced MLM Editor will include a mechanism for        inserting queries generated by the Query by Review tool into MLMs.  It           will also allow authors to reuse queries and logic employed in existing          MLMs.                                                                                                                                                             AIM 4 to facilitate the testing of queries within the environment of the         KBS.  The Event Playback tool will allow the MLM author to run an MLM            against the clinical database to see whether the MLM performs as                 expected.  Rather than presenting a snapshot of the database, the tool           will run the MLM as if medical events (e.g, clinical database                    transactions) were occurring in real time, better simulating actual use.         The Interactive MLM Interpreter will allow an author to debug an MLM by          running it line-by line; the author will be able to respond to each query        manually.  It will also support the batch testing of MLMs using a test           data set.                                                                                                                                                         AIM 5 to evaluate and disseminate the proposed tools.  The impact of the         Query by Review tool on query authoring time and query accuracy will be          measured.  To assess the usefulness of the tools for non-Arden Syntax            KBS, findings from the QMR vocabulary will be studied.  Usage of the             tools will be measured for CPMC's production KBS.  Tools, components, and        methods will be disseminated.                                                     n/a",LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES,2714213,R29LM005627,"['abstracting', ' artificial intelligence', ' computer assisted patient care', ' computer human interaction', ' health care facility information system', ' human subject', ' information retrieval', ' physicians', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R29,1998,117687,0.18336830153631015
"LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES The goal of this proposal is to generate method and tools to link                knowledge-based systems (KBSs) to real clinical databases.  The linking          is done via quires that map conceptual entities in the KBS to actual             entries in the database. They may be used when a KBS is first created,           when an existing KBS is linked to a database or when a KBS is transferred        to another institution.  The primary purpose of the queries is to apply          the KBS to individual patients for direct patient care, rather than to           extend the knowledge base itself.  Several of the tools (Aims 3 and 4)           are specific to the Arden Syntax for Medical Logic Modules (MLMs).  All          the proposed tools will be applied to CPMC's MLM knowledge base, which           is actively used for patient care.                                                                                                                                AIM 1 to increase the accuracy while reducing the writing time and               technical skills required to author clinical database queries.  Query by         Review will provide a familiar interface to novice query authors,                enabling them to write queries by traversing the same type of result             review screens that they use every day in clinical care. When data are           selected from the screen, the tools will generate an appropriate query           (in Arden Syntax and HL7) that can be inserted into a KBS or used for            clinical research.                                                                                                                                                AIM 2 to facilitate the testing of queries, and to improve the match             between a query's result and the needs to a KBS.  The Clinical Database          Brower will allow author to characterize the data returned by a query in         order to determine if they query is appropriate.  It will also allow the         author to determine whether additional logic is necessary to convert the         raw data into form expected by the KBS.  The Brower's design is unique           in its use of a semantic network to aggregate complex categorical data.                                                                                           AIM 3 to provide an environment for inserting queries and additional             logic into the KBS.  The Advanced MLM Editor will include a mechanism for        inserting queries generated by the Query by Review tool into MLMs.  It           will also allow authors to reuse queries and logic employed in existing          MLMs.                                                                                                                                                             AIM 4 to facilitate the testing of queries within the environment of the         KBS.  The Event Playback tool will allow the MLM author to run an MLM            against the clinical database to see whether the MLM performs as                 expected.  Rather than presenting a snapshot of the database, the tool           will run the MLM as if medical events (e.g, clinical database                    transactions) were occurring in real time, better simulating actual use.         The Interactive MLM Interpreter will allow an author to debug an MLM by          running it line-by line; the author will be able to respond to each query        manually.  It will also support the batch testing of MLMs using a test           data set.                                                                                                                                                         AIM 5 to evaluate and disseminate the proposed tools.  The impact of the         Query by Review tool on query authoring time and query accuracy will be          measured.  To assess the usefulness of the tools for non-Arden Syntax            KBS, findings from the QMR vocabulary will be studied.  Usage of the             tools will be measured for CPMC's production KBS.  Tools, components, and        methods will be disseminated.                                                     n/a",LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES,2430871,R29LM005627,"['abstracting', ' artificial intelligence', ' computer assisted patient care', ' computer human interaction', ' health care facility information system', ' human subject', ' information retrieval', ' physicians', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R29,1997,113161,0.18336830153631015
"LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES The goal of this proposal is to generate method and tools to link  knowledge-based systems (KBSs) to real clinical databases.  The linking  is done via quires that map conceptual entities in the KBS to actual  entries in the database. They may be used when a KBS is first created,  when an existing KBS is linked to a database or when a KBS is transferred  to another institution.  The primary purpose of the queries is to apply  the KBS to individual patients for direct patient care, rather than to  extend the knowledge base itself.  Several of the tools (Aims 3 and 4)  are specific to the Arden Syntax for Medical Logic Modules (MLMs).  All  the proposed tools will be applied to CPMC's MLM knowledge base, which  is actively used for patient care.    AIM 1 to increase the accuracy while reducing the writing time and  technical skills required to author clinical database queries.  Query by  Review will provide a familiar interface to novice query authors,  enabling them to write queries by traversing the same type of result  review screens that they use every day in clinical care. When data are  selected from the screen, the tools will generate an appropriate query  (in Arden Syntax and HL7) that can be inserted into a KBS or used for  clinical research.    AIM 2 to facilitate the testing of queries, and to improve the match  between a query's result and the needs to a KBS.  The Clinical Database  Brower will allow author to characterize the data returned by a query in  order to determine if they query is appropriate.  It will also allow the  author to determine whether additional logic is necessary to convert the  raw data into form expected by the KBS.  The Brower's design is unique  in its use of a semantic network to aggregate complex categorical data.    AIM 3 to provide an environment for inserting queries and additional  logic into the KBS.  The Advanced MLM Editor will include a mechanism for  inserting queries generated by the Query by Review tool into MLMs.  It  will also allow authors to reuse queries and logic employed in existing  MLMs.    AIM 4 to facilitate the testing of queries within the environment of the  KBS.  The Event Playback tool will allow the MLM author to run an MLM  against the clinical database to see whether the MLM performs as  expected.  Rather than presenting a snapshot of the database, the tool  will run the MLM as if medical events (e.g, clinical database  transactions) were occurring in real time, better simulating actual use.  The Interactive MLM Interpreter will allow an author to debug an MLM by  running it line-by line; the author will be able to respond to each query  manually.  It will also support the batch testing of MLMs using a test  data set.    AIM 5 to evaluate and disseminate the proposed tools.  The impact of the  Query by Review tool on query authoring time and query accuracy will be  measured.  To assess the usefulness of the tools for non-Arden Syntax  KBS, findings from the QMR vocabulary will be studied.  Usage of the  tools will be measured for CPMC's production KBS.  Tools, components, and  methods will be disseminated.  n/a",LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES,2237960,R29LM005627,"['abstracting', ' artificial intelligence', ' computer assisted patient care', ' computer human interaction', ' health care facility information system', ' human subject', ' information retrieval', ' physicians', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R29,1995,102119,0.18336830153631015
"LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES The goal of this proposal is to generate method and tools to link knowledge-based systems (KBSs) to real clinical databases.  The linking is done via quires that map conceptual entities in the KBS to actual entries in the database. They may be used when a KBS is first created, when an existing KBS is linked to a database or when a KBS is transferred to another institution.  The primary purpose of the queries is to apply the KBS to individual patients for direct patient care, rather than to extend the knowledge base itself.  Several of the tools (Aims 3 and 4) are specific to the Arden Syntax for Medical Logic Modules (MLMs).  All the proposed tools will be applied to CPMC's MLM knowledge base, which is actively used for patient care.  AIM 1 to increase the accuracy while reducing the writing time and technical skills required to author clinical database queries.  Query by Review will provide a familiar interface to novice query authors, enabling them to write queries by traversing the same type of result review screens that they use every day in clinical care. When data are selected from the screen, the tools will generate an appropriate query (in Arden Syntax and HL7) that can be inserted into a KBS or used for clinical research.  AIM 2 to facilitate the testing of queries, and to improve the match between a query's result and the needs to a KBS.  The Clinical Database Brower will allow author to characterize the data returned by a query in order to determine if they query is appropriate.  It will also allow the author to determine whether additional logic is necessary to convert the raw data into form expected by the KBS.  The Brower's design is unique in its use of a semantic network to aggregate complex categorical data.  AIM 3 to provide an environment for inserting queries and additional logic into the KBS.  The Advanced MLM Editor will include a mechanism for inserting queries generated by the Query by Review tool into MLMs.  It will also allow authors to reuse queries and logic employed in existing MLMs.  AIM 4 to facilitate the testing of queries within the environment of the KBS.  The Event Playback tool will allow the MLM author to run an MLM against the clinical database to see whether the MLM performs as expected.  Rather than presenting a snapshot of the database, the tool will run the MLM as if medical events (e.g, clinical database transactions) were occurring in real time, better simulating actual use. The Interactive MLM Interpreter will allow an author to debug an MLM by running it line-by line; the author will be able to respond to each query manually.  It will also support the batch testing of MLMs using a test data set.  AIM 5 to evaluate and disseminate the proposed tools.  The impact of the Query by Review tool on query authoring time and query accuracy will be measured.  To assess the usefulness of the tools for non-Arden Syntax KBS, findings from the QMR vocabulary will be studied.  Usage of the tools will be measured for CPMC's production KBS.  Tools, components, and methods will be disseminated.  n/a",LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES,2237959,R29LM005627,"['abstracting', ' artificial intelligence', ' computer assisted patient care', ' computer human interaction', ' health care facility information system', ' human subject', ' information retrieval', ' physicians', ' vocabulary development for information system']",NLM,COLUMBIA UNIV NEW YORK MORNINGSIDE,R29,1994,133612,0.18336830153631015
"AN EXPERT SYSTEM AS A SCREEN FOR QUALITY OF CARE We propose an experiment to evaluate the use of an expert system (Iliad) to detect diagnostic  errors that may lead to significant quality problems. We propose that these diagnostic errors are not detected by the current Peer Review Organization (PRO) nurse review procedures.  We shall compare the review results of 500 Utah PRO cases (inpatient Medicare and Medicaid care) using Iliad and using the current nurse review procedures to determine whether:  1. Iliad correctly detects more quality problems than nurse review.  2. Iliad detects fewer ""false positive"" quality problems than nurse review.  3. The quality problems detected by Iliad are more severe by PRO standards than the quality problems detected by nurse review.  n/a",AN EXPERT SYSTEM AS A SCREEN FOR QUALITY OF CARE,3427543,R03HS006947,"['artificial intelligence', ' computer assisted medical decision making', ' computer assisted patient care', ' diagnosis quality /standard', ' health care quality', ' health care service evaluation', ' nursing administration', ' statistics /biometry']",AHRQ,UNIVERSITY OF UTAH,R03,1991,21597,0.24512323453034807
"Web-based Resource on Evidence-based Dentistry    DESCRIPTION (provided by applicant):       The intent of this proposal is to develop a dental informatics resource to support evidence-based dental care. Evidence-based dentistry is an approach of integrating the scientific basis for clinical care, using the best available scientific evidence, with clinical and patient factors, to enable practitioners and their patients to make the best possible decision(s) about dental care. It is a means for disseminating scientific information to dental health care workers so that they can access the best available scientific evidence. To facilitate the implementation of evidence-based dentistry within the profession, the American Dental Association Foundation, using the personnel and support of the American Dental Association, will employ an interdisciplinary team to develop an Internet-based resource on evidence-based dentistry that will be available to all dental professionals, allied health care workers and the general public world wide. This resource will include a registry of clinical topics of interests to dental professionals, a searchable database of systematic reviews on topics related to dentistry, critical appraisal and summaries of systematic reviews written for dental professionals, and summaries of systematic reviews written for the general public. These resources will assist dental health care workers in making evidence-based dental treatment decisions and will provide evidence-based information for patients on dental treatment options.          n/a",Web-based Resource on Evidence-based Dentistry,7612115,G08LM008956,"['Ally', 'American Dental Association', 'Area', 'Clinical', 'Country', 'Databases', 'Dental', 'Dental Care', 'Dental General Practice', 'Dental Informatics', 'Dental Libraries', 'Dentistry', 'Dentists', 'Focus Groups', 'Foundations', 'General Population', 'Health', 'Health Personnel', 'Human Resources', 'Information Technology', 'Internet', 'Journals', 'MEDLINE', 'Medical', 'Meta-Analysis', 'Online Systems', 'Oral health', 'Patients', 'Physicians', 'Policies', 'Private Practice', 'Process', 'Publications', 'Published Database', 'Registries', 'Resources', 'Surveys', 'System', 'Writing', 'base', 'clinical care', 'clinical practice', 'design', 'evidence base', 'improved', 'interest', 'member', 'point of care', 'systematic review', 'text searching']",NLM,AMERICAN DENTAL ASSOCIATION FOUNDATION,G08,2009,150000,0.24210397648749116
"Web-based Resource on Evidence-based Dentistry    DESCRIPTION (provided by applicant):       The intent of this proposal is to develop a dental informatics resource to support evidence-based dental care. Evidence-based dentistry is an approach of integrating the scientific basis for clinical care, using the best available scientific evidence, with clinical and patient factors, to enable practitioners and their patients to make the best possible decision(s) about dental care. It is a means for disseminating scientific information to dental health care workers so that they can access the best available scientific evidence. To facilitate the implementation of evidence-based dentistry within the profession, the American Dental Association Foundation, using the personnel and support of the American Dental Association, will employ an interdisciplinary team to develop an Internet-based resource on evidence-based dentistry that will be available to all dental professionals, allied health care workers and the general public world wide. This resource will include a registry of clinical topics of interests to dental professionals, a searchable database of systematic reviews on topics related to dentistry, critical appraisal and summaries of systematic reviews written for dental professionals, and summaries of systematic reviews written for the general public. These resources will assist dental health care workers in making evidence-based dental treatment decisions and will provide evidence-based information for patients on dental treatment options.          n/a",Web-based Resource on Evidence-based Dentistry,7903613,G08LM008956,"['Ally', 'American Dental Association', 'Area', 'Clinical', 'Country', 'Databases', 'Dental', 'Dental Care', 'Dental General Practice', 'Dental Informatics', 'Dental Libraries', 'Dentistry', 'Dentists', 'Focus Groups', 'Foundations', 'General Population', 'Health', 'Health Personnel', 'Human Resources', 'Information Technology', 'Internet', 'Journals', 'MEDLINE', 'Medical', 'Meta-Analysis', 'Online Systems', 'Oral health', 'Patients', 'Physicians', 'Policies', 'Private Practice', 'Process', 'Publications', 'Published Database', 'Registries', 'Resources', 'Surveys', 'System', 'Writing', 'base', 'clinical care', 'clinical practice', 'design', 'evidence base', 'improved', 'interest', 'member', 'point of care', 'systematic review', 'text searching']",NLM,AMERICAN DENTAL ASSOCIATION FOUNDATION,G08,2009,75000,0.24210397648749116
"Web-based Resource on Evidence-based Dentistry    DESCRIPTION (provided by applicant):       The intent of this proposal is to develop a dental informatics resource to support evidence-based dental care. Evidence-based dentistry is an approach of integrating the scientific basis for clinical care, using the best available scientific evidence, with clinical and patient factors, to enable practitioners and their patients to make the best possible decision(s) about dental care. It is a means for disseminating scientific information to dental health care workers so that they can access the best available scientific evidence. To facilitate the implementation of evidence-based dentistry within the profession, the American Dental Association Foundation, using the personnel and support of the American Dental Association, will employ an interdisciplinary team to develop an Internet-based resource on evidence-based dentistry that will be available to all dental professionals, allied health care workers and the general public world wide. This resource will include a registry of clinical topics of interests to dental professionals, a searchable database of systematic reviews on topics related to dentistry, critical appraisal and summaries of systematic reviews written for dental professionals, and summaries of systematic reviews written for the general public. These resources will assist dental health care workers in making evidence-based dental treatment decisions and will provide evidence-based information for patients on dental treatment options.          n/a",Web-based Resource on Evidence-based Dentistry,7391602,G08LM008956,"['Ally', 'American Dental Association', 'Appendix', 'Area', 'Caring', 'Clinical', 'Country', 'Databases', 'Dental', 'Dental Care', 'Dental General Practice', 'Dental Informatics', 'Dental Libraries', 'Dentistry', 'Dentists', 'Focus Groups', 'Foundations', 'General Population', 'Health', 'Health Personnel', 'Human Resources', 'Information Technology', 'Internet', 'Journals', 'MEDLINE', 'Medical', 'Meta-Analysis', 'Numbers', 'Online Systems', 'Oral health', 'Patients', 'Physicians', 'Policies', 'Private Practice', 'Process', 'Publications', 'Published Database', 'Registries', 'Resources', 'Review, Systematic (PT)', 'Surveys', 'System', 'Writing', 'base', 'design', 'improved', 'interest', 'member', 'point of care', 'text searching']",NLM,AMERICAN DENTAL ASSOCIATION FOUNDATION,G08,2008,150000,0.24210397648749116
"Web-based Resource on Evidence-based Dentistry    DESCRIPTION (provided by applicant):       The intent of this proposal is to develop a dental informatics resource to support evidence-based dental care. Evidence-based dentistry is an approach of integrating the scientific basis for clinical care, using the best available scientific evidence, with clinical and patient factors, to enable practitioners and their patients to make the best possible decision(s) about dental care. It is a means for disseminating scientific information to dental health care workers so that they can access the best available scientific evidence. To facilitate the implementation of evidence-based dentistry within the profession, the American Dental Association Foundation, using the personnel and support of the American Dental Association, will employ an interdisciplinary team to develop an Internet-based resource on evidence-based dentistry that will be available to all dental professionals, allied health care workers and the general public world wide. This resource will include a registry of clinical topics of interests to dental professionals, a searchable database of systematic reviews on topics related to dentistry, critical appraisal and summaries of systematic reviews written for dental professionals, and summaries of systematic reviews written for the general public. These resources will assist dental health care workers in making evidence-based dental treatment decisions and will provide evidence-based information for patients on dental treatment options.          n/a",Web-based Resource on Evidence-based Dentistry,7258626,G08LM008956,"['Ally', 'American Dental Association', 'Appendix', 'Area', 'Caring', 'Clinical', 'Country', 'Databases', 'Dental', 'Dental Care', 'Dental General Practice', 'Dental Informatics', 'Dental Libraries', 'Dentistry', 'Dentists', 'Focus Groups', 'Foundations', 'General Population', 'Health', 'Health Personnel', 'Human Resources', 'Information Technology', 'Internet', 'Journals', 'MEDLINE', 'Medical', 'Meta-Analysis', 'Numbers', 'Online Systems', 'Oral health', 'Patients', 'Physicians', 'Policies', 'Private Practice', 'Process', 'Publications', 'Published Database', 'Registries', 'Resources', 'Review, Systematic (PT)', 'Surveys', 'System', 'Writing', 'base', 'design', 'improved', 'interest', 'member', 'point of care', 'text searching']",NLM,AMERICAN DENTAL ASSOCIATION FOUNDATION,G08,2007,150000,0.24210397648749116
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7664538,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'systematic review', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2009,318898,0.2709038475977617
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7468470,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2008,286582,0.2709038475977617
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7242352,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH AND SCI UNIVERSITY,R01,2007,292133,0.2709038475977617
"Data Structuring and Visualization System for Neuro-oncology    DESCRIPTION (provided by applicant):       The medical record for a neuro-oncology patient is complex, consisting of typically a large number of both text and imaging data. It includes descriptions of prior observations, interpretations, and interventions which need to be integrated into decisions regarding current patient care. An appropriate review of a patient's medical record often requires that a physician review multiple clinical documents while mentally noting issues related to what the findings were, the chronology of events, spatial/temporal patterns of disease progression, the effects of interventions, and the possible causal lines of explanation of observed findings. Additionally, imaging data and imaging- derived conclusions are poorly integrated into patient care and management decisions. The physician also needs to filter out those pieces of information not related to the current clinical context of care. Given the time constraints, data complexity and data volume associated with chronic patient cases, an appropriate review of a patient's chart is in reality rarely performed. Additionally, the lack of tools for formalizing the representation of the accounts of current and prior cases impedes the development of clinical databases that can be ultimately used to learn patterns of disease.       This proposal addresses the development of a system for facilitating the review of clinical patient data intended to promote an orderly process of medical problem understanding and care. The specific aims of the proposal are summarized as follows: 1) Development of a backend tool to facilitate the structured representation of observations, events, and inferences stated within medical reports. 2) Development of an application interface for visualizing, navigating, and editing structured patient data. 3) Evaluation of the effectiveness of the application in the domain of neuro-oncology.       Relation to public health. If the goals of this proposal can be realized, neuro-oncologist should be able to more easily seek desired patient data and detect patterns of evidence as compared to the current mode of operation (HIS, RIS, PACS). The structuring tools should lead to improvements in the quality of clinical research databases.           Narrative Medical records for neuro-oncology patients are difficult to review due to volume and complexity of information. A novel system for partitioning data along the information axes of space, time, existence, and causality is proposed to improve navigation and assimilation of data within the medical record.",Data Structuring and Visualization System for Neuro-oncology,7877057,R01LM009961,"['Abnormal coordination', 'Accounting', 'Address', 'Anatomy', 'Appearance', 'Assimilations', 'Caregivers', 'Caring', 'Chronic', 'Chronology', 'Clinic', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Complex', 'Computer software', 'Data', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Effectiveness', 'Etiology', 'Evaluation', 'Event', 'Goals', 'Image', 'Imagery', 'Intervention', 'Lead', 'Learning', 'Medical', 'Medical Records', 'Metric', 'Natural Language Processing', 'Oncologist', 'Patient Care', 'Patient Care Management', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Process', 'Property', 'Public Health', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Structure', 'System', 'Technology', 'Text', 'Time', 'Training', 'abstracting', 'follow-up', 'improved', 'innovation', 'intervention effect', 'neuro-oncology', 'novel', 'open source', 'operation', 'physical state', 'satisfaction', 'tool']",NLM,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2010,771496,0.15891678846996113
"Data Structuring and Visualization System for Neuro-oncology    DESCRIPTION (provided by applicant):       The medical record for a neuro-oncology patient is complex, consisting of typically a large number of both text and imaging data. It includes descriptions of prior observations, interpretations, and interventions which need to be integrated into decisions regarding current patient care. An appropriate review of a patient's medical record often requires that a physician review multiple clinical documents while mentally noting issues related to what the findings were, the chronology of events, spatial/temporal patterns of disease progression, the effects of interventions, and the possible causal lines of explanation of observed findings. Additionally, imaging data and imaging- derived conclusions are poorly integrated into patient care and management decisions. The physician also needs to filter out those pieces of information not related to the current clinical context of care. Given the time constraints, data complexity and data volume associated with chronic patient cases, an appropriate review of a patient's chart is in reality rarely performed. Additionally, the lack of tools for formalizing the representation of the accounts of current and prior cases impedes the development of clinical databases that can be ultimately used to learn patterns of disease.       This proposal addresses the development of a system for facilitating the review of clinical patient data intended to promote an orderly process of medical problem understanding and care. The specific aims of the proposal are summarized as follows: 1) Development of a backend tool to facilitate the structured representation of observations, events, and inferences stated within medical reports. 2) Development of an application interface for visualizing, navigating, and editing structured patient data. 3) Evaluation of the effectiveness of the application in the domain of neuro-oncology.       Relation to public health. If the goals of this proposal can be realized, neuro-oncologist should be able to more easily seek desired patient data and detect patterns of evidence as compared to the current mode of operation (HIS, RIS, PACS). The structuring tools should lead to improvements in the quality of clinical research databases.           Narrative Medical records for neuro-oncology patients are difficult to review due to volume and complexity of information. A novel system for partitioning data along the information axes of space, time, existence, and causality is proposed to improve navigation and assimilation of data within the medical record.",Data Structuring and Visualization System for Neuro-oncology,7567140,R01LM009961,"['Abnormal coordination', 'Accounting', 'Address', 'Anatomy', 'Appearance', 'Assimilations', 'Caregivers', 'Caring', 'Chronic', 'Chronology', 'Clinic', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Complex', 'Computer software', 'Data', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Effectiveness', 'Etiology', 'Evaluation', 'Event', 'Goals', 'Image', 'Imagery', 'Intervention', 'Lead', 'Learning', 'Medical', 'Medical Records', 'Metric', 'Natural Language Processing', 'Oncologist', 'Operative Surgical Procedures', 'Patient Care', 'Patient Care Management', 'Patients', 'Pattern', 'Performance', 'Physicians', 'Process', 'Property', 'Public Health', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Structure', 'System', 'Technology', 'Text', 'Time', 'Training', 'abstracting', 'follow-up', 'improved', 'innovation', 'intervention effect', 'neuro-oncology', 'novel', 'open source', 'physical state', 'satisfaction', 'tool']",NLM,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2009,953185,0.15891678846996113
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9731665,R01LM010817,"['Automation', 'Case-Control Studies', 'Clinical Trials', 'Cohort Studies', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'clinical care', 'flexibility', 'improved', 'informatics\xa0tool', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2019,599962,0.3646806942524266
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9525704,R01LM010817,"['Automation', 'Case-Control Studies', 'Clinical Trials', 'Cohort Studies', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'clinical care', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2018,599947,0.3646806942524266
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9310440,R01LM010817,"['Automation', 'Clinical Trials', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'case control', 'clinical care', 'cohort', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2017,599995,0.3646806942524266
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9176354,R01LM010817,"['Automation', 'Clinical Trials', 'Controlled Study', 'Cross-Sectional Studies', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Staging', 'Testing', 'Time', 'Writing', 'case control', 'clinical care', 'cohort', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2016,599995,0.3646806942524266
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine    DESCRIPTION (provided by applicant):  The Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine will combine important research in several areas of biomedical text mining that are necessary to enable much-needed improvements in the process of conducting systematic reviews via a text mining enhanced workflow. Our consortium will undertake three specific aims to support this work:        Aim 1. Study how to create a metasearch engine and database that collects information from important systematic review sources, indexes this information consistently, and provides a robust information retrieval system with high recall and precision for accessing this expanded literature collection.        Aim 2. Study how to create a literature classification and ranking system that is customizable and trainable for each user, systematic review group, and systematic review topic. This supervised learning based classification and ranking system takes as input the list of retrieved articles corresponding to a given query, and outputs them grouped by article type, in order of predicted probability of relevance to an individual writing a systematic review on the given topic.        Aim 3. Study how to create a study aggregator that collects together articles that refer to the same underlying clinical trial. This will save reviewers work and time as they will now have automated assistance in determining whether two articles are independent data sources, or derive their evidence from the same primary data.        Taken together, these results will inform construction of a text mining pipeline system that will decrease the manual burden of systematic reviewers during the literature collection and review process, and increase the proportion of reviewer time spent synthesizing evidence and performing meta-analyses. The system will lead to a real difference in the rate that high-quality evidence reports can be compiled. Ultimately, the coverage, dissemination, and acceptance of evidence- based medicine in the biomedical community will increase, resulting in better and more cost- effective clinical care.           This project will improve the process of summarizing the best available medical evidence for a wide range of medical conditions. These summaries are utilized by both medical practitioners and policy makers as an essential component of providing higher quality, more cost-effective medical care for everyone.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,8325177,R01LM010817,"['Affect', 'Area', 'Caring', 'Characteristics', 'Classification', 'Clinical', 'Clinical Trials', 'Collection', 'Communities', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Deposition', 'Editorial Policies', 'Evidence Based Medicine', 'Evolution', 'Gold', 'Gray unit of radiation dose', 'Guidelines', 'Individual', 'Information Retrieval Systems', 'Information Services', 'Lead', 'Learning', 'Literature', 'MEDLINE', 'Manuals', 'Medical', 'Meta-Analysis', 'Metadata', 'Modeling', 'Output', 'Peer Review', 'Performance', 'Policy Maker', 'Practice Guidelines', 'Principal Investigator', 'Probability', 'Process', 'PubMed', 'Publishing', 'Randomized Controlled Trials', 'Recording of previous events', 'Reporting', 'Research', 'Research Support', 'Review Literature', 'Savings', 'Source', 'Specific qualifier value', 'System', 'Testing', 'Text', 'Time', 'Training', 'Validation', 'Work', 'Writing', 'abstracting', 'base', 'clinical care', 'control trial', 'cost effective', 'improved', 'indexing', 'performance tests', 'programs', 'systematic review', 'text searching', 'web site']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2012,517925,0.2373485179858327
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine    DESCRIPTION (provided by applicant):  The Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine will combine important research in several areas of biomedical text mining that are necessary to enable much-needed improvements in the process of conducting systematic reviews via a text mining enhanced workflow. Our consortium will undertake three specific aims to support this work:        Aim 1. Study how to create a metasearch engine and database that collects information from important systematic review sources, indexes this information consistently, and provides a robust information retrieval system with high recall and precision for accessing this expanded literature collection.        Aim 2. Study how to create a literature classification and ranking system that is customizable and trainable for each user, systematic review group, and systematic review topic. This supervised learning based classification and ranking system takes as input the list of retrieved articles corresponding to a given query, and outputs them grouped by article type, in order of predicted probability of relevance to an individual writing a systematic review on the given topic.        Aim 3. Study how to create a study aggregator that collects together articles that refer to the same underlying clinical trial. This will save reviewers work and time as they will now have automated assistance in determining whether two articles are independent data sources, or derive their evidence from the same primary data.        Taken together, these results will inform construction of a text mining pipeline system that will decrease the manual burden of systematic reviewers during the literature collection and review process, and increase the proportion of reviewer time spent synthesizing evidence and performing meta-analyses. The system will lead to a real difference in the rate that high-quality evidence reports can be compiled. Ultimately, the coverage, dissemination, and acceptance of evidence- based medicine in the biomedical community will increase, resulting in better and more cost- effective clinical care.           This project will improve the process of summarizing the best available medical evidence for a wide range of medical conditions. These summaries are utilized by both medical practitioners and policy makers as an essential component of providing higher quality, more cost-effective medical care for everyone.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,8142241,R01LM010817,"['Affect', 'Area', 'Caring', 'Characteristics', 'Classification', 'Clinical', 'Clinical Trials', 'Collection', 'Communities', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Deposition', 'Editorial Policies', 'Evidence Based Medicine', 'Evolution', 'Gold', 'Gray unit of radiation dose', 'Guidelines', 'Individual', 'Information Retrieval Systems', 'Information Services', 'Lead', 'Learning', 'Literature', 'MEDLINE', 'Manuals', 'Medical', 'Meta-Analysis', 'Metadata', 'Modeling', 'Output', 'Peer Review', 'Performance', 'Policy Maker', 'Practice Guidelines', 'Principal Investigator', 'Probability', 'Process', 'PubMed', 'Publishing', 'Randomized Controlled Trials', 'Recording of previous events', 'Reporting', 'Research', 'Research Support', 'Review Literature', 'Savings', 'Source', 'Specific qualifier value', 'System', 'Testing', 'Text', 'Time', 'Training', 'Validation', 'Work', 'Writing', 'abstracting', 'base', 'clinical care', 'control trial', 'cost effective', 'improved', 'indexing', 'performance tests', 'programs', 'systematic review', 'text searching', 'web site']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2011,502655,0.2373485179858327
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine    DESCRIPTION (provided by applicant):  The Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine will combine important research in several areas of biomedical text mining that are necessary to enable much-needed improvements in the process of conducting systematic reviews via a text mining enhanced workflow. Our consortium will undertake three specific aims to support this work:        Aim 1. Study how to create a metasearch engine and database that collects information from important systematic review sources, indexes this information consistently, and provides a robust information retrieval system with high recall and precision for accessing this expanded literature collection.        Aim 2. Study how to create a literature classification and ranking system that is customizable and trainable for each user, systematic review group, and systematic review topic. This supervised learning based classification and ranking system takes as input the list of retrieved articles corresponding to a given query, and outputs them grouped by article type, in order of predicted probability of relevance to an individual writing a systematic review on the given topic.        Aim 3. Study how to create a study aggregator that collects together articles that refer to the same underlying clinical trial. This will save reviewers work and time as they will now have automated assistance in determining whether two articles are independent data sources, or derive their evidence from the same primary data.        Taken together, these results will inform construction of a text mining pipeline system that will decrease the manual burden of systematic reviewers during the literature collection and review process, and increase the proportion of reviewer time spent synthesizing evidence and performing meta-analyses. The system will lead to a real difference in the rate that high-quality evidence reports can be compiled. Ultimately, the coverage, dissemination, and acceptance of evidence- based medicine in the biomedical community will increase, resulting in better and more cost- effective clinical care.           This project will improve the process of summarizing the best available medical evidence for a wide range of medical conditions. These summaries are utilized by both medical practitioners and policy makers as an essential component of providing higher quality, more cost-effective medical care for everyone.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,7950308,R01LM010817,"['Affect', 'Area', 'Caring', 'Characteristics', 'Classification', 'Clinical', 'Clinical Trials', 'Collection', 'Communities', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Deposition', 'Editorial Policies', 'Evidence Based Medicine', 'Evolution', 'Gold', 'Gray unit of radiation dose', 'Guidelines', 'Individual', 'Information Retrieval Systems', 'Information Services', 'Lead', 'Learning', 'Literature', 'Manuals', 'Medical', 'Meta-Analysis', 'Metadata', 'Modeling', 'Output', 'Peer Review', 'Performance', 'Policy Maker', 'Practice Guidelines', 'Principal Investigator', 'Probability', 'Process', 'PubMed', 'Publishing', 'Randomized Controlled Trials', 'Recording of previous events', 'Reporting', 'Research', 'Research Support', 'Review Literature', 'Savings', 'Source', 'Specific qualifier value', 'System', 'Testing', 'Text', 'Time', 'Training', 'Validation', 'Work', 'Writing', 'abstracting', 'base', 'clinical care', 'control trial', 'cost', 'cost effective', 'improved', 'indexing', 'performance tests', 'programs', 'systematic review', 'text searching']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2010,576558,0.2373485179858327
"Development of guidelines for investigating heterogeneity in systematic reviews    DESCRIPTION (provided by applicant): Background: Systematic reviews can be prone to different types of heterogeneity. Variability in the participants, interventions and outcome characteristics may be termed clinical heterogeneity; variability in the trial design and quality is typically termed methodological heterogeneity; variability in summary treatment effects between trials can be termed statistical heterogeneity. While guidance exists on assessment of methodological and statistical heterogeneity, little attention has been given to clinical heterogeneity. Therefore, we propose to develop a set of method guidelines for assessing clinical heterogeneity in systematic reviews of randomized clinical trials. Also, we propose to assess a selection of published systematic reviews with the completed guidelines. Objectives: The primary objective of this project is to develop a set of consensus based guidelines for investigating clinical heterogeneity in systematic reviews of randomized clinical trials. Secondary objectives include: 1. Evaluation of how, relative to the guidelines created, current systematic reviews are investigating clinical heterogeneity; 2. To provide guidance to the PRISMA group and the Cochrane Collaboration on how their guidelines and handbook might incorporate our findings. Methods: Guideline development will consist of a modified Delphi process of three phases:    1. Premeeting item generation, 2. Face to face consensus meeting, and 3. Post- meeting feedback. During phase 1 identified participants will be sent articles collected during a literature search, then contacted by telephone and asked to suggest methods necessary for assessing clinical heterogeneity in systematic reviews of randomized clinical trials. Participants will be asked to consider methods based upon empirical evidence and secondarily common sense reasoning. In the second phase we will hold a consensus meeting during which each method will be discussed and debated for relevance to assessing clinical heterogeneity. Final method suggestions and operational definitions will be developed and modified based upon discussions and group consensus. In phase 3 the final guidance document will completed and circulated to meeting participants for feedback. Finally, we will sample 100 systematic reviews and assess them with the guidelines to identify deficiencies.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121531Reviewer 254254Reviewer 332543           This project will create a set of much needed guidelines in the area of systematic reviews. These guidelines will be a valuable tool for systematic reviewers, allowing them to use proper methods for investigating clinical heterogeneity.",Development of guidelines for investigating heterogeneity in systematic reviews,8144247,R21LM010832,"['Age', 'Area', 'Attention', 'Characteristics', 'Clinical', 'Clinical Trials Design', 'Collaborations', 'Consensus', 'Consensus Development', 'Consultations', 'Controlled Clinical Trials', 'Development', 'Evaluation', 'Feedback', 'Generations', 'Guidelines', 'Heterogeneity', 'Intervention', 'Meta-Analysis', 'Methods', 'Outcome', 'Participant', 'Phase', 'Process', 'Publishing', 'Randomized Clinical Trials', 'Relative (related person)', 'Reporting', 'Research', 'Sampling', 'Severity of illness', 'Suggestion', 'Telephone', 'Update', 'Work', 'base', 'handbook', 'improved', 'meetings', 'sex', 'systematic review', 'text searching', 'tool', 'treatment effect']",NLM,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R21,2011,185400,0.21709302115074355
"Development of guidelines for investigating heterogeneity in systematic reviews    DESCRIPTION (provided by applicant): Background: Systematic reviews can be prone to different types of heterogeneity. Variability in the participants, interventions and outcome characteristics may be termed clinical heterogeneity; variability in the trial design and quality is typically termed methodological heterogeneity; variability in summary treatment effects between trials can be termed statistical heterogeneity. While guidance exists on assessment of methodological and statistical heterogeneity, little attention has been given to clinical heterogeneity. Therefore, we propose to develop a set of method guidelines for assessing clinical heterogeneity in systematic reviews of randomized clinical trials. Also, we propose to assess a selection of published systematic reviews with the completed guidelines. Objectives: The primary objective of this project is to develop a set of consensus based guidelines for investigating clinical heterogeneity in systematic reviews of randomized clinical trials. Secondary objectives include: 1. Evaluation of how, relative to the guidelines created, current systematic reviews are investigating clinical heterogeneity; 2. To provide guidance to the PRISMA group and the Cochrane Collaboration on how their guidelines and handbook might incorporate our findings. Methods: Guideline development will consist of a modified Delphi process of three phases:    1. Premeeting item generation, 2. Face to face consensus meeting, and 3. Post- meeting feedback. During phase 1 identified participants will be sent articles collected during a literature search, then contacted by telephone and asked to suggest methods necessary for assessing clinical heterogeneity in systematic reviews of randomized clinical trials. Participants will be asked to consider methods based upon empirical evidence and secondarily common sense reasoning. In the second phase we will hold a consensus meeting during which each method will be discussed and debated for relevance to assessing clinical heterogeneity. Final method suggestions and operational definitions will be developed and modified based upon discussions and group consensus. In phase 3 the final guidance document will completed and circulated to meeting participants for feedback. Finally, we will sample 100 systematic reviews and assess them with the guidelines to identify deficiencies.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121531Reviewer 254254Reviewer 332543           This project will create a set of much needed guidelines in the area of systematic reviews. These guidelines will be a valuable tool for systematic reviewers, allowing them to use proper methods for investigating clinical heterogeneity.",Development of guidelines for investigating heterogeneity in systematic reviews,7963119,R21LM010832,"['Age', 'Area', 'Attention', 'Characteristics', 'Clinical', 'Clinical Trials Design', 'Collaborations', 'Consensus', 'Consensus Development', 'Consultations', 'Controlled Clinical Trials', 'Development', 'Evaluation', 'Face', 'Feedback', 'Generations', 'Guidelines', 'Heterogeneity', 'Intervention', 'Meta-Analysis', 'Methods', 'Outcome', 'Participant', 'Phase', 'Process', 'Publishing', 'Randomized Clinical Trials', 'Relative (related person)', 'Reporting', 'Research', 'Sampling', 'Severity of illness', 'Suggestion', 'Telephone', 'Update', 'Work', 'base', 'handbook', 'improved', 'meetings', 'prisma', 'sex', 'systematic review', 'text searching', 'tool', 'treatment effect']",NLM,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R21,2010,231750,0.21709302115074355
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence    DESCRIPTION (provided by applicant): Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.           Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8190163,K99LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Screening procedure', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2011,89802,0.32156392795396305
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8669161,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'screening', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2014,212994,0.3292011338234344
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8484438,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'screening', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2013,202118,0.3292011338234344
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8471822,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Screening procedure', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2012,224100,0.3292011338234344
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8333306,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,592423,0.0867867625844209
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8022026,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,591195,0.0867867625844209
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8714052,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,UNIVERSITY OF UTAH,R01,2014,579144,0.0867867625844209
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9565646,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2018,293252,0.24685404721849913
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9326367,R01LM012086,"['Age', 'Area', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2017,293503,0.24685404721849913
"Semi-Automating Data Extraction for Systematic Reviews Summary ​Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Evidence-based Medicine (EBM) aims to inform patient care using all available evidence. Realizing this aim in practice would require access to concise, comprehensive, and up-to-date structured summaries of the evidence relevant to a particular clinical question. Systematic reviews of biomedical literature aim to provide such summaries, and are a critical component of the EBM arsenal and modern medicine more generally. However, such reviews are extremely laborious to conduct. Furthermore, owing to the rapid expansion of the biomedical literature base, they tend to go out of date quickly as new evidence emerges. These factors hinder the practice of evidence-based care. In this renewal proposal, we seek to continue our ground-breaking efforts on developing, evaluating, and deploying novel machine learning (ML) and natural language processing (NLP) methods to automate or semi-automate the evidence synthesis process. This will extend our innovative and successful efforts developing RobotReviewer and related technologies under the current grant. Concretely, for this renewal we propose to move from extraction of clinically salient data elements from individual trials to synthesis of these elements across trials. Our first aim is to extend our ML and NLP models to produce (as one deliverable) a publicly available, continuously and automatically updated semi-structured evidence database, comprising extracted data for all evidence, both published and unpublished. Unpublished trials will be identified via trial registries. Taking this up-to-date evidence repository as a starting point, we then propose cutting-edge ML and NLP models that will generate first drafts of evidence syntheses, automatically. More specifically we propose novel neural cross-document summarization models that will capitalize on the semi-structured information automatically extracted by our existing models, in addition to article texts. These models will be deployed in a new version of RobotReviewer, called RobotReviewerLive, intended to be a prototype for “living” systematic reviews. To rigorously evaluate the practical utility of the proposed methodological innovations, we will pilot their use to support real, ongoing, exemplar living reviews. Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Narrative We propose novel machine learning and natural language processing methods that will aid biomedical literature summarization and synthesis, and thereby support the conduct of evidence-based medicine (EBM). The proposed models and technologies will motivate core methodological innovations and support real-time, up-to-date, semi-automated biomedical evidence syntheses (“systematic reviews”). Such approaches are necessary if we are to have any hope of practicing evidence-based care in our era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9990898,R01LM012086,"['American', 'Automation', 'Caring', 'Clinical', 'Collection', 'Consumption', 'Data', 'Data Element', 'Databases', 'Development', 'Elements', 'Evaluation', 'Evidence Based Medicine', 'Evidence based practice', 'Feedback', 'Grant', 'Hybrids', 'Individual', 'Informatics', 'Internet', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical Informatics', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Modern Medicine', 'Natural Language Processing', 'Outcome', 'Output', 'Paper', 'Patient Care', 'Population Intervention', 'Process', 'PubMed', 'Publications', 'Publishing', 'Registries', 'Reporting', 'Research', 'Resources', 'Risk', 'Stroke', 'Structure', 'Surveillance Methods', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'base', 'cardiovascular health', 'database structure', 'design', 'evidence base', 'improved', 'indexing', 'innovation', 'machine learning method', 'natural language', 'neural network', 'novel', 'open source', 'programs', 'prospective', 'prototype', 'recruit', 'relating to nervous system', 'repository', 'search engine', 'structured data', 'study characteristics', 'study population', 'success', 'systematic review', 'tool', 'usability', 'working group']",NLM,NORTHEASTERN UNIVERSITY,R01,2020,291873,0.24474826501200292
"Semi-Automating Data Extraction for Systematic Reviews Summary ​Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Evidence-based Medicine (EBM) aims to inform patient care using all available evidence. Realizing this aim in practice would require access to concise, comprehensive, and up-to-date structured summaries of the evidence relevant to a particular clinical question. Systematic reviews of biomedical literature aim to provide such summaries, and are a critical component of the EBM arsenal and modern medicine more generally. However, such reviews are extremely laborious to conduct. Furthermore, owing to the rapid expansion of the biomedical literature base, they tend to go out of date quickly as new evidence emerges. These factors hinder the practice of evidence-based care. In this renewal proposal, we seek to continue our ground-breaking efforts on developing, evaluating, and deploying novel machine learning (ML) and natural language processing (NLP) methods to automate or semi-automate the evidence synthesis process. This will extend our innovative and successful efforts developing RobotReviewer and related technologies under the current grant. Concretely, for this renewal we propose to move from extraction of clinically salient data elements from individual trials to synthesis of these elements across trials. Our first aim is to extend our ML and NLP models to produce (as one deliverable) a publicly available, continuously and automatically updated semi-structured evidence database, comprising extracted data for all evidence, both published and unpublished. Unpublished trials will be identified via trial registries. Taking this up-to-date evidence repository as a starting point, we then propose cutting-edge ML and NLP models that will generate first drafts of evidence syntheses, automatically. More specifically we propose novel neural cross-document summarization models that will capitalize on the semi-structured information automatically extracted by our existing models, in addition to article texts. These models will be deployed in a new version of RobotReviewer, called RobotReviewerLive, intended to be a prototype for “living” systematic reviews. To rigorously evaluate the practical utility of the proposed methodological innovations, we will pilot their use to support real, ongoing, exemplar living reviews. Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Narrative We propose novel machine learning and natural language processing methods that will aid biomedical literature summarization and synthesis, and thereby support the conduct of evidence-based medicine (EBM). The proposed models and technologies will motivate core methodological innovations and support real-time, up-to-date, semi-automated biomedical evidence syntheses (“systematic reviews”). Such approaches are necessary if we are to have any hope of practicing evidence-based care in our era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9818711,R01LM012086,"['American', 'Automation', 'Caring', 'Clinical', 'Collection', 'Consumption', 'Data', 'Data Element', 'Databases', 'Development', 'Elements', 'Evaluation', 'Evidence Based Medicine', 'Evidence based practice', 'Feedback', 'Grant', 'Hybrids', 'Individual', 'Informatics', 'Internet', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical Informatics', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Modern Medicine', 'Natural Language Processing', 'Outcome', 'Output', 'Paper', 'Patient Care', 'Population Intervention', 'Process', 'PubMed', 'Publications', 'Publishing', 'Registries', 'Reporting', 'Research', 'Resources', 'Risk', 'Stroke', 'Structure', 'Surveillance Methods', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'base', 'cardiovascular health', 'database structure', 'design', 'evidence base', 'improved', 'indexing', 'innovation', 'learning strategy', 'natural language', 'neural network', 'novel', 'open source', 'programs', 'prospective', 'prototype', 'recruit', 'relating to nervous system', 'repository', 'search engine', 'study characteristics', 'study population', 'success', 'systematic review', 'tool', 'usability', 'working group']",NLM,NORTHEASTERN UNIVERSITY,R01,2019,320194,0.24474826501200292
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9145775,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,NORTHEASTERN UNIVERSITY,R01,2016,293874,0.24685404721849913
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice.             Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9028559,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,"UNIVERSITY OF TEXAS, AUSTIN",R01,2015,317900,0.24685404721849913
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,8115129,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2011,136978,0.31790420181778545
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7933715,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2010,388462,0.31790420181778545
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7786337,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2009,362692,0.31790420181778545
"Developing Methods to Improve Systematic Reviews Using Clinical Trial Registries Project Summary Systematic reviews are a critical information source supporting policy and clinical decision-making, and are expected to provide a comprehensive, current, and unbiased assessment of what is known about a clinical intervention. Traditional systematic reviews are resource-intensive endeavors, and with the rapid and increasing pace of evidence production, it is becoming increasingly difficult to ensure that systematic reviews are kept up-to-date. While innovations addressing this challenge have previously focused on automating the specific tasks such as screening and data extraction, innovative approaches are now needed that leverage new data sources and consider efficiencies across sets of reviews. This proposal uses a unique resource— ClinicalTrials.gov—as a data source to develop tools that automatically identify relevant clinical trials, track them as they are completed and reported, and signal when a systematic review requires updating. We propose to investigate a corpus of systematic reviews related to interventions targeting obesity and type 2 diabetes to address the two following aims: (1) To develop and evaluate graph-based semi-supervised learning methods for identifying and linking relevant clinical trials from ClinicalTrials.gov to systematic reviews; and (2) To create and populate a dynamically-updated database of systematic reviews with data that reflects the most up-to-date view of emerging trial evidence. We will make the tools and database developed in this proposal freely available for use by the systematic review community. Thus, our work will not only introduce new ways to identify relevant evidence but will also provide an ongoing resource to support systematic reviewers in prioritizing systematic review updates and ensuring that reviews are a comprehensive and timely summary of the scientific evidence. Project Narrative Systematic reviews represent a critical source of evidence to guide clinical decision-making and are the basis of national practice guidelines and recommendations. As such, clinicians rely on systematic reviews to be inclusive of all available knowledge and to be updated on a regular basis. This is challenging given the immense workload associated with scoping, analyzing, and synthesizing trial evidence for every intervention. We propose the development of tools that take advantage of data from a large clinical trials registry to support systematic reviewers by automating the processes of identifying relevant clinical trials, tracking them as they are completed, and generating the data required to determine which systematic reviews require updating.",Developing Methods to Improve Systematic Reviews Using Clinical Trial Registries,9168208,R03HS024798,[' '],AHRQ,BOSTON CHILDREN'S HOSPITAL,R03,2016,88456,0.36437970074366244
"Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr) EMPIRICAL TESTING AND ENHANCEMENT OF WEB-BASED ABSTRACT SCREENING TOOL (ABSTRACKR)  In this year-long project, we aim to empirically assess the performance and efficiency of state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine and stakeholder-driven comparative effectiveness reviews. We have developed AbstrackrTM (hereon, Abstrackr), a human-guided computerized abstract screening tool that aims to reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. Abstrackr makes use of machine learning techniques, and is offered as a free web-based tool that enables management of the screening process.  We also aim to revise the web-interface of Abstrackr to make it more intuitive, user friendly, and add documentation and functionalities requested by users; and to revise Abstrackr’s back-end, which includes the way the software parses and analyses citations, fits machine learning models, and makes computations, to make it more efficient. These revisions will ensure that the tool becomes more robust, and that it remains usable for larger projects and for many teams.  The proposed work will be carried out by the developers of Abstrackr, comprising a highly experienced team of systematic review investigators and computer scientists at Brown University and the University of Texas at Austin, who have been working together for at least seven years. We will pursue dissemination of the findings of this assessment and of the revised tool through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its wider adoption by the Agency for Healthcare Research and Quality Evidence-based Practice Center Program, Cochrane Collaboration, and other groups conducting systematic reviews. We will also continue to make all code available online. Our aims are to: Aim 1. Empirically measure the efficiency and accuracy of the prediction algorithms in Abstrackr in the computer-assisted semi-automated screening of citations for eligibility in systematic reviews. Aim 2. Improve and add to the functionality of the Web-based Abstrackr software, based in part on enhancements suggested by a panel of identified heavy users. Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making and systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to assess the performance and efficiency of a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care, and to augment the functionality of its public implementation.",Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr),9168247,R03HS024812,[' '],AHRQ,BROWN UNIVERSITY,R03,2016,99999,0.22545070910017362
"Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing Abstract  Systematic reviews constitute the highest quality of evidence and form the cornerstone of evidence-based medicine (EBM). Such reviews now inform everything from national health policy guidelines to bedside care. However, systematic reviews are extremely laborious to produce; researchers can no longer keep pace with the massive amount of evidence now being published.  Semi-automation of systematic review production via machine learning (ML) has demonstrated the potential to substantially reduce reviewer workload while maintaining comprehensiveness. However, it is unlikely that machines will fully supplant human reviewers in the near future. Rather, human experts will probably remain in the loop, assisted by automated methods. Methods that exploit the intersection of human workers and ML models in the context of systematic reviews have not been explored at length. Furthermore, we believe there is substantial untapped potential in harnessing distributed crowd-workers to contribute to systematic reviews, and thus economize expert reviewer efforts. This novel avenue has largely been neglected as a means of increasing the efficiency of review production.  We propose addressing this gap by developing and evaluating novel, hybrid approaches to generating systematic reviews that jointly incorporate domain experts (systematic reviewers), layperson workers recruited via crowdworking platforms such as Amazon's Mechanical Turk and volunteer citizen scientists, while simultaneously capitalizing on ML models.  This innovative, hybrid approach will be the first in-depth exploration of intelligent ML/human systems that aim to reduce the workload in the production of biomedical systematic reviews. Our strong preliminary work demonstrates the promise of this general strategy.   We propose to develop hybrid approaches that combine crowdsourcing and machine learning methods to optimize the conduct of systematic reviews.  ",Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing,9223968,R03HS025024,[' '],AHRQ,NORTHEASTERN UNIVERSITY,R03,2016,98635,0.366730713639246
"Supporting Systematic Review Production with Article Similarity Network Visualization PROJECT SUMMARY Systematic reviews (SRs), or systematic reviews of literature, summarize evidence drawn from high quality studies, and are often the preferred source of evidence-based practice (EBP). However, conducting an SR is labor-intensive and time consuming, typically requiring several months to complete. It has been reported that more than ten thousands of SRs are needed to synthesize existing medical knowledge. An Article screening process is one of the most intensive and time consuming steps, which requires SR researchers to screen a large amount of references, ranging from hundreds to more than 10,000 articles, depending on the size of a SR. In the past 10 years, machine learning model training approaches24-29 were developed to accelerate the article selection process through automation. However, they are not widely used due to diffusion challenges.7,14 Major obstacles include 1) a training sample is required to generate the automation algorithm. If the training sample is biased, the article selection process will systematically fail; 2) the automation approach is not made available for non-computer science specialists, therefore SR researchers will not be able to “fine-tune” the automation algorithm for particular conditions in various SR topics; 3) As there is no global automation algorithm, the generalizability is significantly limited; 4) It is difficult to assess the actual workload saved, while finding every relevant article is required in SR. We propose a new approach to provide views of article relationships in an article network. This is different from other bibliometric networks constructing citation, co-author, or co-occurrence networks. Article network is a simple and logical concept: visualizing article relationships and distribution based on articles' similarities in titles, abstracts, keywords, publication types, etc. SR researchers can also alter the article distribution by adjusting the similarities. This approach does not aim to suggest an end-point of the screening process. Rather, it provides a view of distribution for included, excluded, and undecided articles. In the proposed research, we will integrate advanced techniques to sparsify article networks with mixed sparsification methods, and improve the quality and efficiency of large network visualization layouts by constructing a multi-level network structure and advanced force model. We aim to provide approaches to sparsify and visualize article networks with more than 10,000 articles. Our approach is highly generalizable that it can be used for any health science topics. By viewing the article distribution, SR researchers will be able to screen a large amount of literature more efficiently. This approach can be integrated into current SR technologies and used directly by SR researchers. The success of this project can support SR production on any health science topics, and thus streamline their ultimate application in EBP paradigms. PROJECT NARRATIVE Systematic reviews (SRs) provide the highest quality of research evidence for patient care. To accelerate the production of SRs, we will implement advanced visualization techniques to view article relationships and distribution with article networks and in a timely and human readable manner. The success of the project will support SR production and thus streamline their ultimate application to evidence-based practice.",Supporting Systematic Review Production with Article Similarity Network Visualization,9227858,R03HS025047,[' '],AHRQ,OHIO STATE UNIVERSITY,R03,2016,100000,0.22537333036498122
"SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system Project Summary More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence- based medicine, with each review requiring, on average, between six months to one year of effort to complete. In order to remain accurate, systematic reviews require regular updates after their initial publication, with most reviews out of date within five years. In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate citations is relevant to the research question under consideration. For each article considered, a researcher reads the title and abstract and evaluates its content with respect to the prespecified criteria. A typical review may require screening thousands or tens of thousands of articles in this manner. Under the assumption that it takes a skilled reviewer 30-90 seconds, on average, to screen a single abstract, dual-screening a set of 10,000 abstracts may require between 150 to 500 hours of labor. We have shown in previous work that automated machine learning methods for article prioritization can reduce by more than 50% the human effort required to screen articles for inclusion in a systematic review. Recently, we have further extended these methods and packaged them into a web-based, collaborative systematic review software application called SWIFT-Active Screener. Active Screener has been used successfully to reduce the effort required to screen articles for systematic reviews conducted at a variety of organizations including the National Institute of Environmental Health Science (NIEHS), the United States Environmental Protection Agency (EPA), the United States Department of Agriculture (USDA), The Endocrine Disruption Exchange (TEDX), and the Evidence Based Toxicology Collaboration (EBTC). These early adopters have provided us with an abundance of useful data and user feedback, and we have identified several areas where we can continue to improve our methods and software. Our goal for the current proposal is to conduct additional research and development to make significant improvements to SWIFT-Active Screener, including several innovations that will be necessary for commercial success. The research we propose encompasses three specific aims: (1) Investigate several improvements to statistical algorithms used for article prioritization and recall estimation. We will explore promising avenues for further improving the performance of our existing algorithms and address critical technical issues that limit the applicability of our current methods (Aim 1 – Improved Statistical Models). (2) Explore ways in which we can improve our models and methods to handle the scenario in which an existing systematic review is updated with new data several years after its initial publication (Aim 2 – New Methods for Systematic Review Updates). (3) Investigate several questions related to scaling the system to support hundreds to thousands of simultaneous screeners (Aim 3 - Software Engineering for Scalability, Usability and Full Text Extraction). Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a web-based, collaborative systematic review software application that uses machine learning to prioritize documents for screening, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.",SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system,9467160,R43ES029001,"['Address', 'Algorithms', 'Area', 'Collaborations', 'Computer software', 'Consensus', 'Data', 'Discipline', 'Endocrine disruption', 'Environmental Health', 'Evidence Based Medicine', 'Exclusion Criteria', 'Feedback', 'Goals', 'Health', 'Hour', 'Human', 'Literature', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'National Institute of Environmental Health Sciences', 'Online Systems', 'Performance', 'Phase', 'Process', 'Publications', 'Research', 'Research Personnel', 'Software Engineering', 'Statistical Algorithm', 'Statistical Models', 'System', 'Text', 'Toxicology', 'United States Department of Agriculture', 'United States Environmental Protection Agency', 'Update', 'Work', 'evidence base', 'improved', 'innovation', 'learning strategy', 'research and development', 'screening', 'success', 'systematic review', 'usability']",NIEHS,"SCIOME, LLC",R43,2017,211900,0.3250945784417832
"Research and development of an open, extensible, web-based information extraction workbench for systematic review Project Summary  1 More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence-based  2 medicine, with each review requiring, on average, between six months to one year of effort to complete. One of the most  3 time consuming and repetitive aspects of this endeavor involves extraction of detailed information from a large number  4 of scientific documents. The specific data items extracted differ among disciplines, but within a given scientific domain,  5 certain data points are extracted repeatedly for each review conducted. Research on use of natural language processing  6 (NLP) for extracting individual data elements has shown that it has the potential to greatly reduce the laborious, time  7 intensive, and repetitive nature of this step. However, there is currently no integrated, automatic data extraction platform  8 that meets the needs of the systematic review community. We propose a web-based data extraction software platform  9 specifically designed for usage in the domain of systematic review. By combining multiple state-of-the-art data extraction 10 methods utilizing NLP, text mining and machine learning, into a single, unified user interface, we will thereby empower 11 the end-user with a powerful and novel tool for automating an otherwise arduous task. 12 The research we propose encompasses three specific aims: (1) develop new data extraction models using deep learning 13 and a new technique called “data programming”; (2) develop a web-based platform to semi-automate the process; (3) 14 design protocols and standards for packaging extraction models as software components and integrating work done by 15 other research groups and vendors. In the first aim, we will contribute novel data extraction modules designed and 16 trained specifically to extract data elements of interest to those conducting systematic reviews in the domain of 17 environmental health. For this research, we will employ state-of-the-art machine learning, NLP and text mining 18 methodologies to train and evaluate several novel extraction components. In our second aim, we will develop a web- 19 based workbench which will allow users to upload scientific documents for automated data extraction. Our system will 20 also be designed to allow for integration of data extraction approaches (components) from other research groups, thus 21 enabling end users to choose from a wide variety of advanced data extraction methodologies within one unified and 22 intuitive software environment. In our third aim, we will develop new protocols to standardize the inputs and outputs 23 for data extraction components. The resulting interface, which will enable seamless integration of third party extraction 24 components into the workbench, will also facilitate the incorporation of feedback from users such that extraction 25 components can be continuously improved based on real-time data. 26 Our overarching goal is to translate emerging semi-automated extraction technologies out of the lab and into practical 27 software and to bring to market both the software itself as well as several premium data extraction components. The 28 results of the research conducted for Aims 1-3 represent the first step in this direction and will provide the foundation for 29 future developments. These result will take us one step closer to the dream of creating “living systematic reviews,” which 30 are maintained using automated or semi-automated methods and updated regularly as new evidence becomes available. Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a flexible, extensible software system that automates the crucial and resource-intensive process of extracting key data elements from scientific documents, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.","Research and development of an open, extensible, web-based information extraction workbench for systematic review",9623091,R43ES029901,"['Communities', 'Computer software', 'Consensus', 'Data', 'Data Element', 'Development', 'Discipline', 'Dreams', 'Environment', 'Environmental Health', 'Evidence Based Medicine', 'Feedback', 'Foundations', 'Future', 'Goals', 'Health', 'Individual', 'Internet', 'Intuition', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'One-Step dentin bonding system', 'Online Systems', 'Output', 'Process', 'Protocols documentation', 'Research', 'Resources', 'Source', 'Standardization', 'Supervision', 'System', 'Techniques', 'Technology', 'Time', 'Training', 'Translating', 'Update', 'Vendor', 'Work', 'artificial neural network', 'base', 'data integration', 'deep learning', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'interest', 'learning strategy', 'model design', 'novel', 'research and development', 'software systems', 'systematic review', 'text searching', 'tool']",NIEHS,"SCIOME, LLC",R43,2018,225000,0.26751090031908487
"Capturing Health Information from Research Ontologies (CHIRON) Project Summary/Abstract The volume of published evidence in biomedicine is growing at a rapid pace. Doctors and researchers must keep pace with the rapid generation of new evidence to provide up-to-date care. The review and aggregation of knowledge is a time-intensive process that must be repeated to assimilate the latest evidence. Furthermore, published evaluations of evidence (e.g., literature reviews, meta-analyses, systematic reviews) are not in a format that is able to leverage modern data analytics. Charles River Analytics proposes to design and demonstrate a toolkit for Capturing Health Information from Research Ontologies (CHIRON). CHIRON uses human-in-the-loop approaches to build models of research designs and attributes based on an ontological framework. It leverages a Systemic Functional Grammar (SFG) toolkit for rapidly capturing model data and intuitive user interfaces ensuring accuracy. The format of the research models facilitates their sharing and reuse. This sharable format enables doctors and researchers to rapidly aggregate models and assimilate new representations of evidence into an existing body of research using semantic queries. Furthermore, they facilitate qualitative analyses to obtain summaries and insights about the aggregated literature. The proposed workflow, encompassing capture, analysis, and visualization of research models will decrease the amount of time required to find and aggregate relevant research. Project Narrative  Doctors and researchers require novel tools for finding and evaluating medical evidence to keep pace with the rapidly growing volume of research and provide up-to date care. A principal barrier to the timely review of medical literature is the time it takes to locate relevant studies, examine the research design, and then aggregate the data across a body of literature. To increase the efficiency of the review process, the research and medical community would benefit from tools that allow reviewers to capture models of published research that can be aggregated and combined across a research domain, perform qualitative analytics on the aggregated research, and visualize the results to obtain insights that guide the evaluation of evidence.",Capturing Health Information from Research Ontologies (CHIRON),9885889,R43DA050154,"['Algorithms', 'Attention', 'Automation', 'Biomedical Research', 'Books', 'Caring', 'Communities', 'Computer software', 'Data', 'Data Aggregation', 'Data Analytics', 'Engineering', 'Ensure', 'Evaluation', 'Evidence based practice', 'Feedback', 'Generations', 'Government', 'Graph', 'Guidelines', 'Health', 'Human', 'Imagery', 'Insurance Carriers', 'Intuition', 'Knowledge', 'Literature', 'Medical', 'Medical Research', 'Meta-Analysis', 'Methods', 'Modeling', 'Modernization', 'Ontology', 'Outcome', 'Periodicity', 'Persons', 'Pharmacologic Substance', 'Phase', 'Population', 'Process', 'Publications', 'Publishing', 'Readability', 'Reader', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Review Literature', 'Rivers', 'Sample Size', 'Semantics', 'Social Sciences', 'Structure', 'System', 'Time', 'Update', 'Visualization software', 'automated analysis', 'base', 'cognitive system', 'community based practice', 'data modeling', 'design', 'empowered', 'evidence base', 'heuristics', 'human-in-the-loop', 'informatics\xa0tool', 'information organization', 'insight', 'knowledge base', 'novel', 'social science research', 'sociolinguistics', 'sound', 'systematic review', 'theories', 'tool', 'user-friendly']",NIDA,"CHARLES RIVER ANALYTICS, INC.",R43,2019,148719,0.12558809874459897
"Analyzing Online Reviews to Evaluate Quality of Care at Substance Use Disorder Treatment Facilities In the United States (US), an estimated 20 million adults have been diagnosed with substance use disorder (SUD). A major challenge for patients is to identify the most appropriate treatment with the best outcomes. Patients receive care in outpatient and inpatient facilities yet quality metrics at the level of these individual facilities are not publicly available.  In the era of digital data, the Internet and peer-to-peer resources are often the first place where individuals look to find information about healthcare resources. Online reviews on sites like Google and Yelp provide narratives about healthcare facilities and assign easily interpretable star ratings ranging from one to five stars. These reviews of healthcare facilities provide information about patient experience, structure (e.g. physical education) facility, organizational characteristics, payment methods), process (e.g. diagnosis, treatment, patient and outcomes (e.g. knowledge, health-related quality of life morbidity, mortality).  Prior work has demonstrated that online ratings of hospitals correlate with ratings from the national Hospital Consumer Assessment of Healthcare Providers and Systems (HCAHPS) survey. Little work however has evaluated the utility of unvalidated, spontaneously generated, but widely accessible online reviews of SUD treatment facilities. These reviews could fill a niche with providing patients and their family members with useful information about the questions and experiences that are most important to them. Or these reviews could be limited and provide misinformation or only biased perspectives. To date, less is known about the potential value or usefulness of this emerging data source.  For this proposal we aim to study online reviews of SUD treatment facilities in the US to identify the areas of care that are reported as most important to patients and family members. For Aim 1, we will first extract approximately 50,000 online reviews of SUD treatment facilities and code them for themes using qualitative methodologies that involve manual coding and big data analytics using machine learning and natural language processing. We hypothesize that these online narratives will include qualitative data about patient experience and the type and quality of care (e.g. evidence-based treatments) provided at SUD treatment facilities. For Aim 2, we will then assess how star ratings differentiate facilities relative to structure and process measures reported in the National Survey of Substance Abuse Treatment Services.  Overall, emerging online data resources have the potential to provide new information about a critically vulnerable population affected by the opioid and more broadly the SUD crisis. Our project seeks to rigorously study these new patient-centric data sources viewed by millions of individuals for the purposes of better understanding the needs of patients and family members. These efforts can lay the groundwork for future work in developing measures of quality for a critically important healthcare resource, SUD treatment facilities. Narrative We aim to evaluate narratives and star ratings from online reviews to assess how this consumer generated, data can inform our understanding of care delivery for substance use disorder (SUD). Identifying the themes and measures of quality revealed in these reviews and how they relate to traditional measures of quality could inform future development of patient centric performance measures and ratings for SUD treatment facilities.",Analyzing Online Reviews to Evaluate Quality of Care at Substance Use Disorder Treatment Facilities,9956035,R21DA050761,"['Adult', 'Affect', 'Alcohol or Other Drugs use', 'Ambulatory Care', 'Area', 'Big Data', 'Big Data Methods', 'Caring', 'Characteristics', 'Code', 'Communication', 'Data', 'Data Sources', 'Development', 'Diagnosis', 'Drug Addiction', 'Equilibrium', 'Evidence based practice', 'Evidence based treatment', 'Family', 'Family member', 'Future', 'Goals', 'Government', 'Health', 'Health Personnel', 'Health care facility', 'Healthcare', 'Healthcare Systems', 'Hospitals', 'Individual', 'Inpatients', 'Internet', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Misinformation', 'Modeling', 'Morbidity - disease rate', 'Morphologic artifacts', 'National Institute of Drug Abuse', 'Natural Language Processing', 'Nursing Homes', 'Opioid', 'Outcome', 'Outcome Measure', 'Outpatients', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Perception', 'Performance', 'Physical Education', 'Prevention', 'Process', 'Process Measure', 'Provider', 'Qualitative Methods', 'Quality of Care', 'Recovery', 'Reporting', 'Research', 'Resources', 'Signal Transduction', 'Site', 'Source', 'Structure', 'Substance Use Disorder', 'Surveys', 'Symptoms', 'Time', 'United States', 'Visit', 'Vulnerable Populations', 'Work', 'base', 'care delivery', 'cost', 'data resource', 'digital', 'empowered', 'evidence base', 'experience', 'health related quality of life', 'improved', 'insight', 'medication-assisted treatment', 'mortality', 'opioid abuse', 'opioid epidemic', 'opioid mortality', 'opioid use disorder', 'patient engagement', 'payment', 'peer', 'satisfaction', 'substance abuse treatment', 'treatment center', 'treatment program', 'treatment services', 'urgent care', 'web site']",NIDA,UNIVERSITY OF PENNSYLVANIA,R21,2020,243000,0.19868513688359987
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6495949,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2000,286664,0.20365060284130063
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6181086,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,2000,44870,0.20365060284130063
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6019369,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,1999,318834,0.20365060284130063
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,2771106,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,1998,318557,0.20365060284130063
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,2418025,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,1997,279257,0.20365060284130063
"Understanding and addressing challenges to informed consent and research compliance during Covid-19 research PROJECT SUMMARY Within all US and international codes of research ethics, informed consent serves as a cornerstone for the ethical conduct of research. The application to our current parent R01, “Implementing Evidence-based Informed Consent Practices to Address the Risk of Alzheimer's Dementia and Cognitive Impairment in Clinical Trials” (AG058254) reviewed a large body of literature and concluded that there are several evidence-based practices that improve the consent process: using plain language and optimizing consent document layout; assessing understanding of information using a validated instrument; reviewing with participants any information that was misunderstood; and involving surrogate decision-makers as necessary. Our current parent R01 has 3 specific aims—including development of a web-based toolkit and a social media push—that collectively aim to increase the implementation of these evidence-based consent practices (EBCPs). Implementing EBCPs is made more difficult in times of pandemic. Severely ill patients may be unable to consent, yet quarantine prevents access to surrogates. Electronic consent forms may need to be used, because researchers may not be allowed to enter rooms with patients. These electronic consent forms may need to be produced very quickly, with little attention given to plain language and formatting. Consent information may be incomplete as information about risks may be very limited. Finally, research during a pandemic may lead people to embrace a research ethic that is more strongly focused on the common good (public health) than individual rights. This leads us to propose the following aims in an administrative supplement that examines research on Covid19. 1. Analyze qualitative (open-ended) survey data from IRB members and Covid19 researchers on how current  ethical, regulatory, and institutional requirements might pose barriers to urgent Covid19 research, and what  accommodations would facilitate such research. Relevant to the current R01, we will engage in text mining  and model building to explore the issues arising from the need for informed consent, waivers of consent, or  permissions for use of human subject data and biospecimens. 2. Review findings from the survey of IRB members and Covid19 researchers to guide adaptation and  expansion of the parent R01's EBCP toolkit and implementation trial. We anticipate that this will entail  providing guidance on electronic informed consent processes and alternatives to legally authorized  representatives when patients lack the ability to consent to research. 3. Explore the policy implications of the project, and broadly disseminate data and findings. We will publish a  special issue of the journal, Narrative Inquiry in Bioethics, publish our findings in peer reviewed journals,  and anonymize and deposit data in the ICPSR data repository at the University of Michigan. Project Narrative Informed consent is a cornerstone of research ethics—a key requirement of every national and international code of human research ethics. This project proposes to analyze survey data from Covid-19 researchers and Institutional Review Board personnel who review Covid-19 research to understand how informed consent and related ethical, regulatory and oversight requirements may present challenges or barriers to Covid-19 research. The project will share original data with other researchers, and publish a collection of 25 curated stories by participants together with recommendations for policies and practices to optimize ethical research during a pandemic.",Understanding and addressing challenges to informed consent and research compliance during Covid-19 research,10164294,R01AG058254,"['Address', 'Administrative Supplement', 'Advocate', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease risk', 'Attention', 'Bioethics', 'COVID-19', 'Categories', 'Clinical Trials', 'Code', 'Collection', 'Common Good', 'Computer software', 'Consent', 'Consent Forms', 'Data', 'Data Collection', 'Data Set', 'Deposition', 'Development', 'Ethicists', 'Ethics', 'Evidence based practice', 'Human Research Ethics', 'Human Resources', 'Impaired cognition', 'Individual', 'Informed Consent', 'Institutional Review Boards', 'International', 'Journals', 'Language', 'Lead', 'Legal', 'Literature', 'MeSH Thesaurus', 'Michigan', 'Modeling', 'Online Systems', 'Ontology', 'Parents', 'Participant', 'Patients', 'Peer Review', 'Policies', 'Process', 'Professional Organizations', 'Public Health', 'Published Comment', 'Publishing', 'Quarantine', 'Recommendation', 'Regulation', 'Research', 'Research Ethics', 'Research Personnel', 'Rights', 'Risk', 'Surveys', 'Time', 'Universities', 'data warehouse', 'evidence base', 'human subject', 'implementation trial', 'improved', 'instrument', 'member', 'model building', 'novel', 'pandemic disease', 'policy implication', 'prevent', 'response', 'social media', 'surrogate decision maker', 'symposium', 'text searching', 'waiver']",NIA,WASHINGTON UNIVERSITY,R01,2020,235401,0.08416572563280093
"Machine-Learning Approach to Label-free Detection of new Bacterial Pathogens    DESCRIPTION (provided by applicant): We appreciate the time and effort spent by all the reviewers, and we are grateful for the useful comments and provided suggestions. We have carefully reviewed the critiques and we are happy to see that the panel was receptive to our proposal. The reviewers expressed three major concerns in the summary statement: (1) although the investigating team is well qualified our history of collaboration is short; (2) details regarding the practical constraints of the BARDOT system are lacking; (3) the machine learning techniques employed in the project are considered fairly standard.  Below we briefly discuss the reviewers comments and indicate how we have changed our revised application to address the critique.  (1) Dr. Dundar moved from industry to academia in the fall of 2008, at which point Dr. Rajwa (one of the original inventors of BARDOT) and Dr. Dundar began their collaboration on new approaches to the problem of non-exhaustively defined classes in phenotypic screening. This scientific partnership immediately produced interesting results, and at the time of submission of the original application, Dr. Dundar and Dr. Rajwa had their first manuscript under review. The approach presented in the original proposal was tested and the results were submitted to the ACM 15th Annual SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'09), which is the largest and one of the most respected conferences in this field. The manuscript was accepted after a full peer review as one of the 50 regular papers selected from 551 submissions [20]. Following the proposal submission, research efforts continued and produced yet another approach to the problem described in this grant application. The preliminary findings are reported in a new manuscript which is currently under review [4].  (2) We rewrote the background and research methods sections of our proposal to include information re- quested by the reviewers regarding practical aspects of the BARDOT system, such as accuracy issues (Section D.3.2), frequency of encountering new, unknown classes (Section B.3.1), and validation (Section D.3.1).  (3) The problem of phenotypic screening and classification of bacteria can be defined within exhaustive (stan- dard) or non-exhaustive learning frameworks. Although we agree that the implementation of an exhaustive clas- sification approach for BARDOT does require only fairly standard tools, the problem of the non-exhaustive nature of training libraries cannot be addressed by straightforward use of any textbook-level technique. In fact, the presence of non-exhaustively defined set of classes violates basic assumptions for most supervised learning systems. The issue of non-exhaustively defined classes is the major obstacle for application of machine learning in phenotypic analysis since the number of possible phenotypes may be infinite. In our original proposal we argued that learning with a non-exhaustively defined set of classes remains a very challenging problem, and presented evidence demonstrating that simple extensions of standard techniques cannot provide an acceptable solution. Subsequently, we proposed a new approach based on Bayesian simulation of classes and showed that preliminary results outperformed benchmark techniques [4].  Although these initial results looked promising, we did not consider the described preliminary algorithms final and definitive, and we do not believe that at this point we are able to provide an exact algorithmic solution to this complex problem. If we were able to do that, it would mean that we had already accomplished all the grant goals. The very essence of the proposed research is finding the answer to the defined problem, and the answer will remain unknown until after the work has been done. However, positive reviews and an acceptance of our work by KDD'09 conference judges, tell us that we are heading in the right direction.  In the amended version of this application we propose a modified Bayesian approach based on Wishart priors (Section D.2.3). The algorithm creates new classes on the fly and evaluates maximum likelihood with the updated set of classes, gradually improving detection accuracy for future samples. We believe that this offers a substantial improvement over the previous method. Consequently, the preliminary results in Section C are updated to reflect our progress. Since the modified technique allows for classification with non-exhaustive and exhaustive sets using the same algorithm, we consolidated the previous specific aims 3 and 5 into one in the revised application.      PUBLIC HEALTH RELEVANCE: A Machine Learning Approach to Label-free Detection of Bacterial Pathogens  using Laser Light Scattering  PIs: Dr. M. Murat Dundar and Dr. Bartek Rajwa Successful implementation of this project will allow for a label-free detection and identification of food pathogens and their mutated subclasses not yet seen earlier. This will reduce the number of food related outbreaks and will help secure public food supply.           Public Health Relevance Title: A Machine Learning Approach to Label-free Detection of Bacterial Pathogens  using Laser Light Scattering  PIs: Dr. M. Murat Dundar and Dr. Bartek Rajwa Successful implementation of this project will allow for a label-free detection and identification of food pathogens and their mutated subclasses not yet seen earlier. This will reduce the number of food related outbreaks and will help secure public food supply.",Machine-Learning Approach to Label-free Detection of new Bacterial Pathogens,7896355,R21AI085531,"['Academia', 'Accounting', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Bacteria', 'Benchmarking', 'Biochemical Process', 'Centers for Disease Control and Prevention (U.S.)', 'Characteristics', 'Classification', 'Collaborations', 'Complex', 'Computer Vision Systems', 'Critiques', 'Data Set', 'Detection', 'Disease', 'Disease Outbreaks', 'Escherichia coli', 'Food', 'Food Supply', 'Frequencies', 'Future', 'Genus staphylococcus', 'Goals', 'Grant', 'Head', 'Industry', 'Infection', 'International', 'Knowledge', 'Label', 'Lasers', 'Learning', 'Libraries', 'Listeria', 'Machine Learning', 'Manuscripts', 'Medical', 'Methods', 'Modeling', 'Mutate', 'Mutation', 'Nature', 'Optics', 'Paper', 'Pathogenicity', 'Pattern', 'Pattern Recognition', 'Peer Review', 'Phenotype', 'Process', 'Productivity', 'Public Health', 'Published Comment', 'Qualifying', 'Reagent', 'Recording of previous events', 'Reporting', 'Research', 'Research Methodology', 'Safety', 'Salmonella', 'Sampling', 'Screening procedure', 'Secure', 'Serotyping', 'Solutions', 'Suggestion', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Textbooks', 'Time', 'Training', 'Update', 'Validation', 'Vibrio', 'Work', 'base', 'cost', 'data mining', 'falls', 'fly', 'foodborne', 'foodborne pathogen', 'image processing', 'improved', 'interest', 'light scattering', 'new technology', 'novel strategies', 'optical sensor', 'pathogen', 'pathogenic bacteria', 'public health relevance', 'rapid detection', 'sensor', 'simulation', 'symposium', 'tool']",NIAID,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R21,2010,234831,0.0923756955250051
"Machine-Learning Approach to Label-free Detection of new Bacterial Pathogens DESCRIPTION (provided by applicant): We appreciate the time and effort spent by all the reviewers, and we are grateful for the useful comments and provided suggestions. We have carefully reviewed the critiques and we are happy to see that the panel was receptive to our proposal. The reviewers expressed three major concerns in the summary statement: (1) although the investigating team is well qualified our history of collaboration is short; (2) details regarding the practical constraints of the BARDOT system are lacking; (3) the machine learning techniques employed in the project are considered fairly standard.  Below we briefly discuss the reviewers comments and indicate how we have changed our revised application to address the critique.  (1) Dr. Dundar moved from industry to academia in the fall of 2008, at which point Dr. Rajwa (one of the original inventors of BARDOT) and Dr. Dundar began their collaboration on new approaches to the problem of non-exhaustively defined classes in phenotypic screening. This scientific partnership immediately produced interesting results, and at the time of submission of the original application, Dr. Dundar and Dr. Rajwa had their first manuscript under review. The approach presented in the original proposal was tested and the results were submitted to the ACM 15th Annual SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'09), which is the largest and one of the most respected conferences in this field. The manuscript was accepted after a full peer review as one of the 50 regular papers selected from 551 submissions [20]. Following the proposal submission, research efforts continued and produced yet another approach to the problem described in this grant application. The preliminary findings are reported in a new manuscript which is currently under review [4].  (2) We rewrote the background and research methods sections of our proposal to include information re- quested by the reviewers regarding practical aspects of the BARDOT system, such as accuracy issues (Section D.3.2), frequency of encountering new, unknown classes (Section B.3.1), and validation (Section D.3.1).  (3) The problem of phenotypic screening and classification of bacteria can be defined within exhaustive (stan- dard) or non-exhaustive learning frameworks. Although we agree that the implementation of an exhaustive clas- sification approach for BARDOT does require only fairly standard tools, the problem of the non-exhaustive nature of training libraries cannot be addressed by straightforward use of any textbook-level technique. In fact, the presence of non-exhaustively defined set of classes violates basic assumptions for most supervised learning systems. The issue of non-exhaustively defined classes is the major obstacle for application of machine learning in phenotypic analysis since the number of possible phenotypes may be infinite. In our original proposal we argued that learning with a non-exhaustively defined set of classes remains a very challenging problem, and presented evidence demonstrating that simple extensions of standard techniques cannot provide an acceptable solution. Subsequently, we proposed a new approach based on Bayesian simulation of classes and showed that preliminary results outperformed benchmark techniques [4].  Although these initial results looked promising, we did not consider the described preliminary algorithms final and definitive, and we do not believe that at this point we are able to provide an exact algorithmic solution to this complex problem. If we were able to do that, it would mean that we had already accomplished all the grant goals. The very essence of the proposed research is finding the answer to the defined problem, and the answer will remain unknown until after the work has been done. However, positive reviews and an acceptance of our work by KDD'09 conference judges, tell us that we are heading in the right direction.  In the amended version of this application we propose a modified Bayesian approach based on Wishart priors (Section D.2.3). The algorithm creates new classes on the fly and evaluates maximum likelihood with the updated set of classes, gradually improving detection accuracy for future samples. We believe that this offers a substantial improvement over the previous method. Consequently, the preliminary results in Section C are updated to reflect our progress. Since the modified technique allows for classification with non-exhaustive and exhaustive sets using the same algorithm, we consolidated the previous specific aims 3 and 5 into one in the revised application. PUBLIC HEALTH RELEVANCE: A Machine Learning Approach to Label-free Detection of Bacterial Pathogens using Laser Light Scattering PIs: Dr. M. Murat Dundar and Dr. Bartek Rajwa Successful implementation of this project will allow for a label-free detection and identification of food pathogens and their mutated subclasses not yet seen earlier. This will reduce the number of food related outbreaks and will help secure public food supply.",Machine-Learning Approach to Label-free Detection of new Bacterial Pathogens,8070004,R21AI085531,"['Academia', 'Accounting', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Bacteria', 'Benchmarking', 'Biochemical Process', 'Centers for Disease Control and Prevention (U.S.)', 'Characteristics', 'Classification', 'Collaborations', 'Complex', 'Computer Vision Systems', 'Critiques', 'Data Set', 'Detection', 'Disease', 'Disease Outbreaks', 'Escherichia coli', 'Food', 'Food Supply', 'Frequencies', 'Future', 'Genus staphylococcus', 'Goals', 'Grant', 'Head', 'Health', 'Industry', 'Infection', 'International', 'Knowledge Discovery', 'Label', 'Lasers', 'Learning', 'Libraries', 'Listeria', 'Machine Learning', 'Manuscripts', 'Medical', 'Methods', 'Modeling', 'Mutate', 'Mutation', 'Nature', 'Optics', 'Paper', 'Pathogenicity', 'Pattern', 'Pattern Recognition', 'Peer Review', 'Phenotype', 'Process', 'Productivity', 'Public Health', 'Published Comment', 'Qualifying', 'Reagent', 'Recording of previous events', 'Reporting', 'Research', 'Research Methodology', 'Safety', 'Salmonella', 'Sampling', 'Screening procedure', 'Secure', 'Serotyping', 'Solutions', 'Suggestion', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Textbooks', 'Time', 'Training', 'Update', 'Validation', 'Vibrio', 'Work', 'base', 'cost', 'data mining', 'disorder prevention', 'falls', 'foodborne', 'foodborne pathogen', 'image processing', 'improved', 'interest', 'light scattering', 'new technology', 'novel strategies', 'optical sensor', 'pathogen', 'pathogenic bacteria', 'rapid detection', 'sensor', 'simulation', 'symposium', 'text searching', 'tool']",NIAID,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R21,2011,147939,0.09404631559703273
"Gene Wiki: expanding the ecosystem of community-intelligence resources DESCRIPTION (provided by applicant): This proposal describes the continued maintenance and development of the Gene Wiki, the goal of which is to create a continuously-updated, community-reviewed, and collaboratively-written review article for every human gene. The Gene Wiki was created directly within Wikipedia as an informal collection of 10,646 gene-specific articles. In the first funding period, the infrastructure to keep Gene Wiki ""infoboxes"" in sync wit the source databases in the genomics community was developed. Next, methods to assess and quantify the trustworthiness of each word of a Gene Wiki article were developed and implemented in a system called WikiTrust. And finally, simple text-mining applied to Gene Wiki was able to identify thousands of novel gene annotations. During the next project period, the Gene Wiki is poised to make further strides. First, the scope of the Gene Wiki will be expanded to also include review articles on diseases and drugs. Thousands of articles will either be created or maintained through this initiative with a particular emphasis on rare diseases. Second, a dedicated outreach component will ensure that the community of editors is poised to grow. This outreach effort will engage both faculty members who are experts on specific genes of interest, as well as classroom instructors at all levels who want to design curriculum based on the Gene Wiki for a class project. Third, a ""Centralized Model Organism Database"" will be constructed in the Wikidata environment, which will serve as a clearing house of microbial gene and genome annotation data. And fourth an entirely new crowdsourcing application will be created that taps into crowds of patient-aligned individuals and their desire to advance research. These individuals will be a novel crowd that will be applied to the challenge of systematically annotating the biomedical literature. In summary, the Gene Wiki is a useful tool for biomedical research. Successful completion of this proposal will result in more efficient knowledge management and dissemination through crowdsourcing. PUBLIC HEALTH RELEVANCE: The Gene Wiki is a highly used resource for understanding gene function. The goal of this project is to create a collaboratively-written, community-reviewed, and continuously-updated review article for every human gene, drug, and disease.",Gene Wiki: expanding the ecosystem of community-intelligence resources,9265870,R01GM089820,"['Address', 'Biological', 'Biomedical Research', 'Collection', 'Communities', 'Crowding', 'Data', 'Data Display', 'Databases', 'Development', 'Disease', 'Ecosystem', 'Educational Curriculum', 'Ensure', 'Environment', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Harvest', 'Human', 'Incentives', 'Individual', 'Information Resources Management', 'Intelligence', 'Internet', 'Journals', 'Knowledge', 'Learning', 'Life', 'Link', 'Literature', 'Maintenance', 'Methods', 'Microbiology', 'Mining', 'Modeling', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Population', 'Progress Reports', 'Property', 'Publications', 'Rare Diseases', 'Reader', 'Research', 'Research Infrastructure', 'Resources', 'Solid', 'Source', 'Students', 'System', 'Text', 'Time', 'Trees', 'Update', 'Wit', 'Work', 'base', 'crowdsourcing', 'database structure', 'design', 'file format', 'gene function', 'genetic resource', 'genome annotation', 'genome browser', 'genome sequencing', 'genomic data', 'improved', 'instructor', 'interest', 'member', 'microbial', 'model organisms databases', 'novel', 'outreach', 'public health relevance', 'text searching', 'tool', 'web interface', 'whole genome', 'wiki']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2017,491537,0.19077704501802528
"Gene Wiki: expanding the ecosystem of community-intelligence resources DESCRIPTION (provided by applicant): This proposal describes the continued maintenance and development of the Gene Wiki, the goal of which is to create a continuously-updated, community-reviewed, and collaboratively-written review article for every human gene. The Gene Wiki was created directly within Wikipedia as an informal collection of 10,646 gene-specific articles. In the first funding period, the infrastructure to keep Gene Wiki ""infoboxes"" in sync wit the source databases in the genomics community was developed. Next, methods to assess and quantify the trustworthiness of each word of a Gene Wiki article were developed and implemented in a system called WikiTrust. And finally, simple text-mining applied to Gene Wiki was able to identify thousands of novel gene annotations. During the next project period, the Gene Wiki is poised to make further strides. First, the scope of the Gene Wiki will be expanded to also include review articles on diseases and drugs. Thousands of articles will either be created or maintained through this initiative with a particular emphasis on rare diseases. Second, a dedicated outreach component will ensure that the community of editors is poised to grow. This outreach effort will engage both faculty members who are experts on specific genes of interest, as well as classroom instructors at all levels who want to design curriculum based on the Gene Wiki for a class project. Third, a ""Centralized Model Organism Database"" will be constructed in the Wikidata environment, which will serve as a clearing house of microbial gene and genome annotation data. And fourth an entirely new crowdsourcing application will be created that taps into crowds of patient-aligned individuals and their desire to advance research. These individuals will be a novel crowd that will be applied to the challenge of systematically annotating the biomedical literature. In summary, the Gene Wiki is a useful tool for biomedical research. Successful completion of this proposal will result in more efficient knowledge management and dissemination through crowdsourcing. PUBLIC HEALTH RELEVANCE: The Gene Wiki is a highly used resource for understanding gene function. The goal of this project is to create a collaboratively-written, community-reviewed, and continuously-updated review article for every human gene, drug, and disease.",Gene Wiki: expanding the ecosystem of community-intelligence resources,9063441,R01GM089820,"['Address', 'Biological', 'Biomedical Research', 'Collection', 'Communities', 'Crowding', 'Data', 'Data Display', 'Databases', 'Development', 'Disease', 'Ecosystem', 'Educational Curriculum', 'Ensure', 'Environment', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'Genes', 'Genomics', 'Goals', 'Growth', 'Harvest', 'Health', 'Housing', 'Human', 'Incentives', 'Individual', 'Information Resources Management', 'Intelligence', 'Internet', 'Journals', 'Knowledge', 'Learning', 'Life', 'Link', 'Literature', 'Maintenance', 'Methods', 'Microbiology', 'Mining', 'Modeling', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Population', 'Progress Reports', 'Property', 'Publications', 'Rare Diseases', 'Reader', 'Research', 'Research Infrastructure', 'Resources', 'Solid', 'Source', 'Students', 'System', 'Text', 'Time', 'Trees', 'Update', 'Wit', 'Work', 'Writing', 'base', 'crowdsourcing', 'database structure', 'design', 'file format', 'gene function', 'genetic resource', 'genome annotation', 'genome browser', 'genome sequencing', 'genomic data', 'improved', 'instructor', 'interest', 'member', 'microbial', 'model organisms databases', 'novel', 'outreach', 'text searching', 'tool', 'web interface', 'whole genome', 'wiki']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2016,491537,0.19077704501802528
"Gene Wiki: expanding the ecosystem of community-intelligence resources DESCRIPTION (provided by applicant): This proposal describes the continued maintenance and development of the Gene Wiki, the goal of which is to create a continuously-updated, community-reviewed, and collaboratively-written review article for every human gene. The Gene Wiki was created directly within Wikipedia as an informal collection of 10,646 gene-specific articles. In the first funding period, the infrastructure to keep Gene Wiki ""infoboxes"" in sync wit the source databases in the genomics community was developed. Next, methods to assess and quantify the trustworthiness of each word of a Gene Wiki article were developed and implemented in a system called WikiTrust. And finally, simple text-mining applied to Gene Wiki was able to identify thousands of novel gene annotations. During the next project period, the Gene Wiki is poised to make further strides. First, the scope of the Gene Wiki will be expanded to also include review articles on diseases and drugs. Thousands of articles will either be created or maintained through this initiative with a particular emphasis on rare diseases. Second, a dedicated outreach component will ensure that the community of editors is poised to grow. This outreach effort will engage both faculty members who are experts on specific genes of interest, as well as classroom instructors at all levels who want to design curriculum based on the Gene Wiki for a class project. Third, a ""Centralized Model Organism Database"" will be constructed in the Wikidata environment, which will serve as a clearing house of microbial gene and genome annotation data. And fourth an entirely new crowdsourcing application will be created that taps into crowds of patient-aligned individuals and their desire to advance research. These individuals will be a novel crowd that will be applied to the challenge of systematically annotating the biomedical literature. In summary, the Gene Wiki is a useful tool for biomedical research. Successful completion of this proposal will result in more efficient knowledge management and dissemination through crowdsourcing. PUBLIC HEALTH RELEVANCE: The Gene Wiki is a highly used resource for understanding gene function. The goal of this project is to create a collaboratively-written, community-reviewed, and continuously-updated review article for every human gene, drug, and disease.",Gene Wiki: expanding the ecosystem of community-intelligence resources,8891444,R01GM089820,"['Address', 'Biological', 'Biomedical Research', 'Collection', 'Communities', 'Crowding', 'Data', 'Data Display', 'Databases', 'Development', 'Disease', 'Ecosystem', 'Educational Curriculum', 'Ensure', 'Environment', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Harvest', 'Health', 'Housing', 'Human', 'Incentives', 'Individual', 'Information Resources Management', 'Intelligence', 'Internet', 'Journals', 'Knowledge', 'Learning', 'Life', 'Link', 'Literature', 'Maintenance', 'Methods', 'Microbiology', 'Mining', 'Modeling', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Population', 'Progress Reports', 'Property', 'Publications', 'Rare Diseases', 'Reader', 'Research', 'Research Infrastructure', 'Resources', 'Solid', 'Source', 'Students', 'System', 'Text', 'Time', 'Trees', 'Update', 'Wit', 'Work', 'Writing', 'base', 'database structure', 'design', 'file format', 'gene function', 'genetic resource', 'genome annotation', 'genome sequencing', 'improved', 'instructor', 'interest', 'member', 'microbial', 'model organisms databases', 'novel', 'outreach', 'text searching', 'tool', 'web interface', 'wiki']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2015,485310,0.19077704501802528
"Gene Wiki: expanding the ecosystem of community-intelligence resources DESCRIPTION (provided by applicant): This proposal describes the continued maintenance and development of the Gene Wiki, the goal of which is to create a continuously-updated, community-reviewed, and collaboratively-written review article for every human gene. The Gene Wiki was created directly within Wikipedia as an informal collection of 10,646 gene-specific articles. In the first funding period, the infrastructure to keep Gene Wiki ""infoboxes"" in sync wit the source databases in the genomics community was developed. Next, methods to assess and quantify the trustworthiness of each word of a Gene Wiki article were developed and implemented in a system called WikiTrust. And finally, simple text-mining applied to Gene Wiki was able to identify thousands of novel gene annotations. During the next project period, the Gene Wiki is poised to make further strides. First, the scope of the Gene Wiki will be expanded to also include review articles on diseases and drugs. Thousands of articles will either be created or maintained through this initiative with a particular emphasis on rare diseases. Second, a dedicated outreach component will ensure that the community of editors is poised to grow. This outreach effort will engage both faculty members who are experts on specific genes of interest, as well as classroom instructors at all levels who want to design curriculum based on the Gene Wiki for a class project. Third, a ""Centralized Model Organism Database"" will be constructed in the Wikidata environment, which will serve as a clearing house of microbial gene and genome annotation data. And fourth an entirely new crowdsourcing application will be created that taps into crowds of patient-aligned individuals and their desire to advance research. These individuals will be a novel crowd that will be applied to the challenge of systematically annotating the biomedical literature. In summary, the Gene Wiki is a useful tool for biomedical research. Successful completion of this proposal will result in more efficient knowledge management and dissemination through crowdsourcing. PUBLIC HEALTH RELEVANCE: The Gene Wiki is a highly used resource for understanding gene function. The goal of this project is to create a collaboratively-written, community-reviewed, and continuously-updated review article for every human gene, drug, and disease.",Gene Wiki: expanding the ecosystem of community-intelligence resources,8696695,R01GM089820,"['Address', 'Biological', 'Biomedical Research', 'Collection', 'Communities', 'Crowding', 'Data', 'Data Display', 'Databases', 'Development', 'Disease', 'Ecosystem', 'Educational Curriculum', 'Ensure', 'Environment', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Harvest', 'Health', 'Housing', 'Human', 'Incentives', 'Individual', 'Information Resources Management', 'Intelligence', 'Internet', 'Journals', 'Knowledge', 'Learning', 'Life', 'Link', 'Literature', 'Maintenance', 'Methods', 'Microbiology', 'Mining', 'Modeling', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Population', 'Progress Reports', 'Property', 'Publications', 'Rare Diseases', 'Reader', 'Research', 'Research Infrastructure', 'Resources', 'Solid', 'Source', 'Students', 'System', 'Text', 'Time', 'Trees', 'Update', 'Wit', 'Work', 'Writing', 'base', 'database structure', 'design', 'file format', 'gene function', 'genetic resource', 'genome annotation', 'genome sequencing', 'improved', 'instructor', 'interest', 'member', 'microbial', 'microbial alkaline proteinase inhibitor', 'model organisms databases', 'novel', 'outreach', 'text searching', 'tool', 'web interface', 'wiki']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2014,413289,0.19077704501802528
"Systematic Review and Meta-Analysis of COPD Genetic Studies    DESCRIPTION (provided by applicant): COPD is the fourth leading cause of the death in the United States and the fifth most common cause of death worldwide. The rapidly increasing knowledge of human genetic variation offers hope for new treatments and more accurate risk prediction tools for COPD. With the large volume of genetic data available for COPD association testing, it is important to develop a publicly accessible research infrastructure for accessing, organizing, and synthesizing genetic association data. As one means of accomplishing this task, The Human Genome Epidemiology Network has emphasized the need for disease specific, regularly updated systematic reviews and meta-analyses of genetic association studies. This proposal will conduct a comprehensive search of the literature in order to: 1.) Conduct a detailed descriptive assessment of all published studies of common genetic variants and COPD. 2.) Perform a quantitative meta-analysis of genetic loci studied in three or more independent study populations. 3.) Produce visual maps of the human genome representing the amount of research performed and the number of positive associations reported for particular genomic areas. Using data gathered from the descriptive assessment, we will quantify between-study heterogeneity in genetic effect size and identify causes of this heterogeneity. We will also incorporate primary data from a pending genome-wide association study (one of the first such studies in COPD) into the meta-analysis and visual maps. This project will accomplish the initial work required to establish an online, publicly available compendium of COPD genetic associations similar to preexisting websites for other complex diseases such as AlzGene and PDGene. The development of an online, publicly available database to compile and synthesize information from COPD genetic studies will be a major benefit to the COPD research community, and will help to fulfill one of the missions of the NHLBI to promote research leading to improved prevention, diagnosis, and treatment of COPD. PUBLIC HEALTH RELEVANCE: The proposed research will provide needed infrastructure for the research field of COPD genetics. It will aid current efforts to identify the genetic causes of COPD. Understanding these genetic causes will lead to better treatments for COPD.             n/a",Systematic Review and Meta-Analysis of COPD Genetic Studies,7545112,F32HL094035,"['Alzheimer&apos', 's Disease', 'Area', 'Biomass', 'Case-Control Studies', 'Cause of Death', 'Chronic', 'Chronic Obstructive Airway Disease', 'Collaborations', 'Communities', 'Complex', 'Confidence Intervals', 'Data', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Association', 'Disease regression', 'Environment', 'Environmental Exposure', 'Epidemiology', 'Future', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genomics', 'Hereditary Disease', 'Heterogeneity', 'Human Genetics', 'Human Genome', 'Individual', 'Knowledge', 'Lead', 'Literature', 'Lung diseases', 'Maps', 'Mentors', 'Meta-Analysis', 'Methods', 'Mission', 'Morbidity - disease rate', 'Numbers', 'Odds Ratio', 'Online Systems', 'Parkinson Disease', 'Population Study', 'Prevention', 'Public Health', 'Publishing', 'Relative (related person)', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Respiratory physiology', 'Review, Systematic (PT)', 'Risk', 'Scanning', 'Smoke', 'Smoker', 'Staging', 'Statistically Significant', 'Testing', 'Tobacco', 'United States', 'Update', 'Variant', 'Visual', 'Work', 'concept', 'disability', 'genetic association', 'genetic variant', 'genome wide association study', 'improved', 'mortality', 'prevent', 'size', 'smoking cessation', 'text searching', 'tool', 'visual map']",NHLBI,TUFTS MEDICAL CENTER,F32,2008,71006,0.0770541641393622
"Epi25 Clinical Phenotyping R03 PROJECT SUMMARY Clinical genetic data suggests that specific categories of epilepsy have genetic contributors, and there may be some overlap between categories. The Epi25 Collaborative was formed among more than 40 cohorts from around the world to sequence as many as 25,000 genomes or exomes. As of 2017, the collaborative has sequenced more than 13,000 exomes and clinical data has been collected for more than 8,000 cases. This project will complete the collection and review of the clinical data for each sample in the Epi25 collection to facilitate the translation of genomic and clinical discoveries into improved care for patients. The clinical and genomic data from Epi25 will be a global resource, shared with the research community for years to come. Epi25's governance structure, membership, and other information are available online at www.epi-25.org. In this project, clinical data is entered by contributors into Red Cap forms or uploaded directly into the Epi25 database. The clinical data is then checked by a computer algorithm that looks for key eligibility criteria for each participant. Errors and missing data are sent to the Phenotyping Coordinator to review and resolve, with the help of the contributing site. PROJECT NARRATIVE In 2014, collaborators from around the world created the Epi25 Collaborative to exome sequence as many as 25,000 patients with epilepsy. The collaborative has more than 6,200 exomes generated in year 2016, an additional 7,500 on sequencers in 2017, and more than 1,000 ready for sequencing in 2018. This project will review and correct errors for the descriptive epilepsy data for each sample sequenced in Epi25, to reveal the genetic underpinnings of common epilepsies.",Epi25 Clinical Phenotyping R03,9584612,R03NS108145,"['Absence Epilepsy', 'Artificial Intelligence', 'Autosomal Dominant Partial Epilepsy with Auditory Features', 'Categories', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Computational algorithm', 'Data', 'Data Discovery', 'Databases', 'Eligibility Determination', 'Epilepsy', 'Ethnic Origin', 'Family', 'Frontal Lobe Epilepsy', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Determinism', 'Genetic Translation', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Hand', 'Informatics', 'International', 'Juvenile Myoclonic Epilepsy', 'Major Depressive Disorder', 'Medical Genetics', 'Methods', 'Neurodevelopmental Disorder', 'Partial Epilepsies', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Phenotype', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Site', 'Standardization', 'Structure', 'Syndrome', 'Temporal Lobe Epilepsy', 'Testing', 'Translations', 'Twin Studies', 'Variant', 'autism spectrum disorder', 'clinical phenotype', 'cohort', 'dravet syndrome', 'exome', 'genomic data', 'improved', 'phenotypic data', 'rare variant', 'sample collection', 'tool']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R03,2018,75911,0.07248521137724623
"Epi25 Clinical Phenotyping R03 PROJECT SUMMARY Clinical genetic data suggests that specific categories of epilepsy have genetic contributors, and there may be some overlap between categories. The Epi25 Collaborative was formed among more than 40 cohorts from around the world to sequence as many as 25,000 genomes or exomes. As of 2017, the collaborative has sequenced more than 13,000 exomes and clinical data has been collected for more than 8,000 cases. This project will complete the collection and review of the clinical data for each sample in the Epi25 collection to facilitate the translation of genomic and clinical discoveries into improved care for patients. The clinical and genomic data from Epi25 will be a global resource, shared with the research community for years to come. Epi25's governance structure, membership, and other information are available online at www.epi-25.org. In this project, clinical data is entered by contributors into Red Cap forms or uploaded directly into the Epi25 database. The clinical data is then checked by a computer algorithm that looks for key eligibility criteria for each participant. Errors and missing data are sent to the Phenotyping Coordinator to review and resolve, with the help of the contributing site. PROJECT NARRATIVE In 2014, collaborators from around the world created the Epi25 Collaborative to exome sequence as many as 25,000 patients with epilepsy. The collaborative has more than 6,200 exomes generated in year 2016, an additional 7,500 on sequencers in 2017, and more than 1,000 ready for sequencing in 2018. This project will review and correct errors for the descriptive epilepsy data for each sample sequenced in Epi25, to reveal the genetic underpinnings of common epilepsies.",Epi25 Clinical Phenotyping R03,9753389,R03NS108145,"['Absence Epilepsy', 'Artificial Intelligence', 'Autosomal Dominant Partial Epilepsy with Auditory Features', 'Autosomal dominant nocturnal frontal lobe epilepsy\xa0', 'Categories', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Computational algorithm', 'Data', 'Data Discovery', 'Databases', 'Eligibility Determination', 'Epilepsy', 'Ethnic Origin', 'Family', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Determinism', 'Genetic Translation', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Hand', 'International', 'Juvenile Myoclonic Epilepsy', 'Major Depressive Disorder', 'Medical Genetics', 'Methods', 'Neurodevelopmental Disorder', 'Partial Epilepsies', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Phenotype', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Site', 'Standardization', 'Structure', 'Syndrome', 'Temporal Lobe Epilepsy', 'Testing', 'Translations', 'Twin Studies', 'Variant', 'autism spectrum disorder', 'clinical phenotype', 'cohort', 'dravet syndrome', 'exome', 'genomic data', 'improved', 'informatics\xa0tool', 'phenotypic data', 'rare variant', 'sample collection']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R03,2019,60925,0.07248521137724623
"Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research ﻿    DESCRIPTION (provided by applicant): Supervised machine learning is a popular method that uses labeled training examples to predict future outcomes.  Unfortunately, supervised machine learning for biomedical research is often limited by a lack of labeled data.  Current methods to produce labeled data involve manual chart reviews that are laborious and do not scale with data creation rates.  This project aims to develop a framework to crowd source labeled data sets from electronic medical records by forming a crowd of clinical personnel labelers.  The construction of these labeled data sets will allow for new biomedical research studies that were previously infeasible to conduct.  There are numerous practical and theoretical challenges of developing a crowd sourcing platform for clinical data.  First, popular, public crowd sourcing platforms such as Amazon's Mechanical Turk are not suitable for medical record labeling as HIPAA makes clinical data sharing risky.  Second, the types of clinical questions that are amenable for crowd sourcing are not well understood.  Third, it is unclear if the clinical crowd can produce labels quickly and accurately.  Each of these challenges will be addressed in a separate Aim. As the first Aim of this project, the team will evaluate different clinical crowd sourcing architectures.  The architecture must leverage the scale of the crowd, while minimizing patient information exposure.  De-identification tools will be considered to scrub clinical notes t reduce information leakage.  Using this design, the team will extend a popular open source crowd sourcing tool, Pybossa, and release it to the public.  As the second Aim, the team will study the type, structure, topic and specificity of clinical prediction questions, and how these characteristics impact labeler quality.  Lastly, the team will evaluate the quality and accuracy of collected clinical crowd sourced data on two existing chart review problems to determine the platform's utility.         PUBLIC HEALTH RELEVANCE: Traditionally, clinical prediction models rely on supervised machine learning algorithms to probabilistically predict clinical events using labeled medical records.  When data sets are small, manual chart reviews performed by clinical staff are sufficient to label each outcome; however, as data sets have scaled up and researchers aim to study larger cohorts, current manual approaches become intractable.  The goal of this proposal is to develop a framework to crowd source labeled data sets from electronic medical records to support prediction model development.        ",Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research,9076555,UH2CA203708,"['Accident and Emergency department', 'Address', 'Algorithms', 'Architecture', 'Area', 'Asthma', 'Biomedical Research', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Data', 'Collection', 'Computerized Medical Record', 'Crowding', 'Data', 'Data Set', 'Development', 'Disclosure', 'Ensure', 'Evaluation', 'Event', 'Extravasation', 'Future', 'Goals', 'Health', 'Health Insurance Portability and Accountability Act', 'Human Resources', 'Incentives', 'Interview', 'Label', 'Machine Learning', 'Management Audit', 'Manuals', 'Measures', 'Mechanics', 'Medical Records', 'Medical Research', 'Medical Students', 'Medical center', 'Methods', 'Modeling', 'Nurses', 'Outcome', 'Patients', 'Privacy', 'Productivity', 'Receiver Operating Characteristics', 'Relapse', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Security', 'Specificity', 'Structure', 'System', 'Time', 'Training', 'cohort', 'computer science', 'crowdsourcing', 'design', 'member', 'model development', 'open source', 'public health relevance', 'research study', 'response', 'scale up', 'tool']",NCI,VANDERBILT UNIVERSITY MEDICAL CENTER,UH2,2016,316000,0.09693921111420284
"Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research ﻿    DESCRIPTION (provided by applicant): Supervised machine learning is a popular method that uses labeled training examples to predict future outcomes.  Unfortunately, supervised machine learning for biomedical research is often limited by a lack of labeled data.  Current methods to produce labeled data involve manual chart reviews that are laborious and do not scale with data creation rates.  This project aims to develop a framework to crowd source labeled data sets from electronic medical records by forming a crowd of clinical personnel labelers.  The construction of these labeled data sets will allow for new biomedical research studies that were previously infeasible to conduct.  There are numerous practical and theoretical challenges of developing a crowd sourcing platform for clinical data.  First, popular, public crowd sourcing platforms such as Amazon's Mechanical Turk are not suitable for medical record labeling as HIPAA makes clinical data sharing risky.  Second, the types of clinical questions that are amenable for crowd sourcing are not well understood.  Third, it is unclear if the clinical crowd can produce labels quickly and accurately.  Each of these challenges will be addressed in a separate Aim. As the first Aim of this project, the team will evaluate different clinical crowd sourcing architectures.  The architecture must leverage the scale of the crowd, while minimizing patient information exposure.  De-identification tools will be considered to scrub clinical notes t reduce information leakage.  Using this design, the team will extend a popular open source crowd sourcing tool, Pybossa, and release it to the public.  As the second Aim, the team will study the type, structure, topic and specificity of clinical prediction questions, and how these characteristics impact labeler quality.  Lastly, the team will evaluate the quality and accuracy of collected clinical crowd sourced data on two existing chart review problems to determine the platform's utility. PUBLIC HEALTH RELEVANCE: Traditionally, clinical prediction models rely on supervised machine learning algorithms to probabilistically predict clinical events using labeled medical records.  When data sets are small, manual chart reviews performed by clinical staff are sufficient to label each outcome; however, as data sets have scaled up and researchers aim to study larger cohorts, current manual approaches become intractable.  The goal of this proposal is to develop a framework to crowd source labeled data sets from electronic medical records to support prediction model development.",Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research,9270528,UH2CA203708,"['Accident and Emergency department', 'Address', 'Algorithms', 'Architecture', 'Area', 'Asthma', 'Biomedical Research', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Data', 'Collection', 'Computerized Medical Record', 'Crowding', 'Data', 'Data Set', 'Data Sources', 'Development', 'Disclosure', 'Ensure', 'Evaluation', 'Event', 'Extravasation', 'Future', 'Goals', 'Health', 'Health Insurance Portability and Accountability Act', 'Human Resources', 'Incentives', 'Interview', 'Label', 'Machine Learning', 'Management Audit', 'Manuals', 'Measures', 'Mechanics', 'Medical Records', 'Medical Research', 'Medical Students', 'Medical center', 'Methods', 'Modeling', 'Nurses', 'Outcome', 'Patients', 'Privacy', 'Productivity', 'Receiver Operating Characteristics', 'Relapse', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Security', 'Specificity', 'Structure', 'Supervision', 'System', 'Time', 'Training', 'clinical predictors', 'cohort', 'computer science', 'crowdsourcing', 'data sharing', 'design', 'member', 'model development', 'open source', 'public health relevance', 'research study', 'response', 'scale up', 'tool']",NCI,VANDERBILT UNIVERSITY MEDICAL CENTER,UH2,2017,316000,0.09693921111420284
"Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) promises to transform the way that physicians treat their patients, resulting in better quality and more consistent care informed directly by the totality of relevant evidence. However, clinicians do not have the time to keep up to date with the vast medical literature. Systematic reviews, which provide rigorous, comprehensive and transparent assessments of the evidence pertaining to specific clinical questions, promise to mitigate this problem by concisely summarizing all pertinent evidence. But producing such reviews has become increasingly burdensome (and hence expensive) due in part to the exponential expansion of the biomedical literature base, hampering our ability to provide evidence-based care.  If we are to scale EBM to meet the demands imposed by the rapidly growing volume of published evidence, then we must modernize EBM tools and methods. More specifically, if we are to continue generating up-to-date evidence syntheses, then we must optimize the systematic review process. Toward this end, we propose developing new methods that combine crowdsourcing and machine learning to facilitate efficient annotation of the full-texts of articles describing clinical trials. These annotations will comprise mark-up of sections of text that discuss clinically relevant fields of importance in EBM, such as discussion of patient characteristics, interventions studied and potential sources of bias. Such annotations would make literature search and data extraction much easier for systematic reviewers, thus reducing their workload and freeing more time for them to conduct thoughtful evidence synthesis.  This will be the first in-depth exploration of crowdsourcing for EBM. We will collect annotations from workers with varying levels of expertise and cost, ranging from medical students to workers recruited via Amazon Mechanical Turk. We will develop and evaluate novel methods of aggregating annotations from such heterogeneous sources. And we will use the acquired manual annotations to train machine learning models that automate this markup process. Models capable of automatically identifying clinically salient text snippets in full-text articles describing clinical trials would be broadly useful for biomedical literature retrieval tasks and would have impact beyond our immediate application of EBM. PUBLIC HEALTH RELEVANCE: We propose to develop crowdsourcing and machine learning methods to annotate clinically important sentences in full-text articles describing clinical trials Ultimately, we aim to automate such annotation, thereby enabling more efficient practice of evidence- based medicine (EBM).",Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities,9275458,UH2CA203711,"['Artificial Intelligence', 'Automated Annotation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Trials', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Development', 'Effectiveness', 'Eligibility Determination', 'Environment', 'Evidence Based Medicine', 'Evidence based practice', 'Health', 'Human', 'Hybrids', 'Individual', 'Information Retrieval', 'Intervention', 'Intervention Studies', 'Literature', 'Machine Learning', 'Manuals', 'Mechanics', 'Medical', 'Medical Students', 'Medicine', 'Methods', 'Modeling', 'Modernization', 'Outcome', 'Paper', 'Participant', 'Patients', 'Physicians', 'Population', 'Population Characteristics', 'Preparation', 'Process', 'Publishing', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Retrieval', 'Rewards', 'Source', 'System', 'Text', 'Time', 'Training', 'Work', 'Workload', 'base', 'clinically relevant', 'collaborative environment', 'cost', 'crowdsourcing', 'demographics', 'detector', 'dosage', 'evidence base', 'learning strategy', 'novel', 'phrases', 'public health relevance', 'scale up', 'skills', 'systematic review', 'tool']",NCI,NORTHEASTERN UNIVERSITY,UH2,2017,210198,0.0865021235433373
"Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) promises to transform the way that physicians treat their patients, resulting in better quality and more consistent care informed directly by the totality of relevant evidence. However, clinicians do not have the time to keep up to date with the vast medical literature. Systematic reviews, which provide rigorous, comprehensive and transparent assessments of the evidence pertaining to specific clinical questions, promise to mitigate this problem by concisely summarizing all pertinent evidence. But producing such reviews has become increasingly burdensome (and hence expensive) due in part to the exponential expansion of the biomedical literature base, hampering our ability to provide evidence-based care.  If we are to scale EBM to meet the demands imposed by the rapidly growing volume of published evidence, then we must modernize EBM tools and methods. More specifically, if we are to continue generating up-to-date evidence syntheses, then we must optimize the systematic review process. Toward this end, we propose developing new methods that combine crowdsourcing and machine learning to facilitate efficient annotation of the full-texts of articles describing clinical trials. These annotations will comprise mark-up of sections of text that discuss clinically relevant fields of importance in EBM, such as discussion of patient characteristics, interventions studied and potential sources of bias. Such annotations would make literature search and data extraction much easier for systematic reviewers, thus reducing their workload and freeing more time for them to conduct thoughtful evidence synthesis.  This will be the first in-depth exploration of crowdsourcing for EBM. We will collect annotations from workers with varying levels of expertise and cost, ranging from medical students to workers recruited via Amazon Mechanical Turk. We will develop and evaluate novel methods of aggregating annotations from such heterogeneous sources. And we will use the acquired manual annotations to train machine learning models that automate this markup process. Models capable of automatically identifying clinically salient text snippets in full-text articles describing clinical trials would be broadly useful for biomedical literature retrieval tasks and would have impact beyond our immediate application of EBM.         PUBLIC HEALTH RELEVANCE: We propose to develop crowdsourcing and machine learning methods to annotate clinically important sentences in full-text articles describing clinical trials Ultimately, we aim to automate such annotation, thereby enabling more efficient practice of evidence- based medicine (EBM).         ",Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities,9076888,UH2CA203711,"['Artificial Intelligence', 'Automated Annotation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Trials', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Development', 'Effectiveness', 'Eligibility Determination', 'Environment', 'Evidence Based Medicine', 'Health', 'Human', 'Hybrids', 'Individual', 'Information Retrieval', 'Intervention', 'Intervention Studies', 'Literature', 'Machine Learning', 'Manuals', 'Mechanics', 'Medical', 'Medical Students', 'Methods', 'Modeling', 'Outcome', 'Paper', 'Participant', 'Patients', 'Physicians', 'Population', 'Population Characteristics', 'Preparation', 'Process', 'Publishing', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Retrieval', 'Rewards', 'Source', 'System', 'Text', 'Time', 'Training', 'Work', 'Workload', 'abstracting', 'base', 'clinically relevant', 'collaborative environment', 'cost', 'crowdsourcing', 'demographics', 'detector', 'dosage', 'evidence base', 'learning strategy', 'meetings', 'novel', 'phrases', 'public health relevance', 'scale up', 'skills', 'systematic review', 'text searching', 'tool']",NCI,NORTHEASTERN UNIVERSITY,UH2,2016,240650,0.0865021235433373
"Scientific Information Management and Literature-Based Evaluations for the National Toxicology Program (NTP) – Support for Scoping Activities Scoping reviews involve the summarization and categorization of literature prepared to rapidly map the key concepts, types of evidence, and gaps in research related to a defined research area by systematically searching, selecting and presenting existing knowledge. A scoping review can be undertaken as stand-alone product or in support of decision making and may involve varying degrees of synthesis of the existing knowledge. Systematic review methodology and the OHAT Approach to Systematic Review and Evidence Integration also includes the terms scoping and problem formulation as part of the decision making and planning process for a systematic review. Scoping is the process of taking an initial background knowledge on a topic under consideration and seeking input from stakeholders and clients to understand the extent of interest in an evaluation topic or nomination, assess the potential impact of conducting an evaluation, and identify related activities that may be underway. Problem formulation refers to the first step in the systematic-review process in which an explicit definition or statement is reached on what is to be evaluated in the assessment and how it is to be evaluated. A number of scoping activities are in progress, focused on Parkinson’s Disease, neonicotinoid pesticides, progestins, hypertensive disorders/pregnancy, personal care products and endocrine disruption or fetal growth, PFAS, pesticides, flame retardants and cannabis. Keywords: scoping, toxicology, epidemiology, exposure, cancer, non-cancer effects n/a",Scientific Information Management and Literature-Based Evaluations for the National Toxicology Program (NTP) – Support for Scoping Activities,10148592,73201600015U,"['Area', 'Cannabis', 'Client', 'Decision Making', 'Endocrine disruption', 'Epidemiology', 'Evaluation', 'Fetal Growth', 'Flame Retardants', 'Information Management', 'Knowledge', 'Literature', 'Malignant Neoplasms', 'Maps', 'Methodology', 'National Toxicology Program', 'Parkinson Disease', 'Pesticides', 'Problem Formulations', 'Process', 'Progestins', 'Research', 'Toxicology', 'base', 'interest', 'personal care products', 'pregnancy disorder', 'screening', 'systematic review', 'text searching']",NIEHS,"ICF, INC., LLC",N01,2020,500488,0.29844787610416046
NIST Assistance with NTP SR Automation NIEHS seeks advice from NIST in the areas of human language technology and natural language processing component evaluations that support the measurement of systems that automatically extract toxicology information from publications to support the complex human task of systematic review of literature. NIST is positioned to assist NIEHS building upon existing test and evaluation infrastructure through its Text Analysis Conference (TAC) program. NIST is coordinating the 2019 Systematic Review Information Extraction evaluation (SRIE 2019) task for NIEHS as part of the Retrieval Group’s Text Analysis Conference (TAC) program. This coordination includes advising NIEHS on developing annotation guidelines; advising NIEHS on dataset construction and distribution; writing guidelines for the evaluation task; developing scoring methods and supporting software; including the evaluation task as part of the TAC program and call for participation; accepting participant submissions in the evaluation; evaluating those submissions; and reporting results of the evaluation. NIST and NIH will design an evaluation task in this domain. n/a,NIST Assistance with NTP SR Automation,9794240,ES18001002,"['Advertisements', 'Area', 'Automation', 'Complex', 'Computer software', 'Data Set', 'Development', 'Evaluation', 'Guidelines', 'Human', 'Language', 'Measurement', 'National Institute of Environmental Health Sciences', 'Natural Language Processing', 'Participant', 'Positioning Attribute', 'Preparation', 'Publications', 'Reporting', 'Research', 'Research Infrastructure', 'Retrieval', 'Review Literature', 'Scoring Method', 'System', 'Technology', 'Testing', 'Text', 'Toxicology', 'United States National Institutes of Health', 'Writing', 'biomedical informatics', 'design', 'programs', 'symposium', 'systematic review']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2018,200000,0.10905878572506829
"Scientific Information Management and Literature-Based Evaluations for the National Toxicology Program (NTP) – Support for Scoping Activities Scoping reviews involve the summarization and categorization of literature prepared to rapidly map the key concepts, types of evidence, and gaps in research related to a defined research area by systematically searching, selecting and presenting existing knowledge. A scoping review can be undertaken as stand-alone product or in support of decision making and may involve varying degrees of synthesis of the existing knowledge. Systematic review methodology and the OHAT Approach to Systematic Review and Evidence Integration also includes the terms scoping and problem formulation as part of the decision making and planning process for a systematic review. Scoping is the process of taking an initial background knowledge on a topic under consideration and seeking input from stakeholders and clients to understand the extent of interest in an evaluation topic or nomination, assess the potential impact of conducting an evaluation, and identify related activities that may be underway. Problem formulation refers to the first step in the systematic-review process in which an explicit definition or statement is reached on what is to be evaluated in the assessment and how it is to be evaluated. A number of scoping activities are in progress, focused on Parkinson’s Disease, neonicotinoid pesticides, progestins, epigenetics, biocides, PFAS, pesticides, flame retardants.  Keywords: scoping, toxicology, epidemiology, exposure, cancer, non-cancer effects n/a",Scientific Information Management and Literature-Based Evaluations for the National Toxicology Program (NTP) – Support for Scoping Activities,9915763,73201600015U,"['Area', 'Biocide', 'Client', 'Decision Making', 'Epidemiology', 'Epigenetic Process', 'Evaluation', 'Flame Retardants', 'Information Management', 'Knowledge', 'Literature', 'Malignant Neoplasms', 'Maps', 'Methodology', 'National Toxicology Program', 'Parkinson Disease', 'Pesticides', 'Problem Formulations', 'Process', 'Progestins', 'Research', 'Toxicology', 'base', 'interest', 'screening', 'systematic review', 'text searching']",NIEHS,"ICF, INC., LLC",N01,2019,500000,0.3128879833724743
"Scientific Information Management and Literature-Based Evaluations for the National Toxicology Program (NTP) – Support for Scoping Activities Scoping reviews involve the summarization and categorization of literature prepared to rapidly map the key concepts, types of evidence, and gaps in research related to a defined research area by systematically searching, selecting and presenting existing knowledge. A scoping review can be undertaken as stand-alone product or in support of decision making and may involve varying degrees of synthesis of the existing knowledge. Systematic review methodology and the OHAT Approach to Systematic Review and Evidence Integration also includes the terms scoping and problem formulation as part of the decision making and planning process for a systematic review. Scoping is the process of taking an initial background knowledge on a topic under consideration and seeking input from stakeholders and clients to understand the extent of interest in an evaluation topic or nomination, assess the potential impact of conducting an evaluation, and identify related activities that may be underway. Problem formulation refers to the first step in the systematic-review process in which an explicit definition or statement is reached on what is to be evaluated in the assessment and how it is to be evaluated. A number of scoping activities are in progress, focused on Parkinson’s Disease, mammary gland development, neonicotinoid pesticides, transgenerational inheritance, progestins, epigenetics, biocides, PFAS, polycyclic aromatic hydrocarbons, p-chloro-alpha, alpha, alpha-trifluorotoluene, e-cigarettes and hookah, hair dyes, and radio frequency radiation.  Keywords: scoping, toxicology, epidemiology, exposure, cancer, non-cancer effects n/a",Scientific Information Management and Literature-Based Evaluations for the National Toxicology Program (NTP) – Support for Scoping Activities,9616090,73201600015U,"['Area', 'Aromatic Polycyclic Hydrocarbons', 'Biocide', 'Client', 'Decision Making', 'Electronic cigarette', 'Epidemiology', 'Epigenetic Process', 'Evaluation', 'Hair Dyes', 'Information Management', 'Knowledge', 'Literature', 'Malignant Neoplasms', 'Maps', 'Methodology', 'National Toxicology Program', 'Parkinson Disease', 'Pesticides', 'Problem Formulations', 'Process', 'Progestins', 'Radiation', 'Research', 'Toxicology', 'base', 'hookah', 'interest', 'mammary gland development', 'radio frequency', 'screening', 'systematic review', 'text searching', 'transgenerational epigenetic inheritance']",NIEHS,"ICF, INC., LLC",N01,2018,1352686,0.3070199613426895
"NICEATM Support Contract: Base Contract This is a Research and Development (R&D) contract to provide scientific and administrative support for the National Toxicology Program (NTP) Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM). NICEATM research supports activities of NTP in general, and specifically, NTP’s Biomolecular Screening Branch (BSB), the Tox21 consortium, and the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM). NICEATM is responsible for ensuring compliance with the duties and provisions of the ICCVAM Authorization Act of 2000 (42 U.S.C. 285l-3) - to promote the research, development, validation, evaluation, acceptance, and use of new and alternative testing methods and strategies that are more predictive of human health and ecological effects than currently available methods and strategies. NICEATM carries out its mission by performing independent R&D activities, reviewing proposed test methods, organizing workshops and meetings, and facilitating peer reviews.  The Contractor is required to acquire and apply new and existing scientific knowledge to develop, evaluate, and validate novel computational approaches that can be used for chemical hazard identification and risk assessment with direct relevance to human health. Data development, analysis, and evaluation represents a large portion of the work of this requirement. These approaches include, but are not limited to: exposure modeling, physiologically based pharmacokinetic/pharmacodynamic (PBPK/PD) modeling, reverse toxicokinetic (R-TK) modeling, Quantitative Structure-Activity Relationship (QSAR) modeling, analysis of quantitative high throughput screening (qHTS) and high content (HC) data, and development of novel Integrated Testing and Decision Strategies (ITDS) using in vivo, in vitro and/or in silico systems. The Contractor routinely utilizes information from diverse data types and multiple databases (e.g., ToxRef DB, ToxCastDB, ExpoCastDB, DSSTox, CEBS, etc.) to develop and evaluate the above listed approaches. The Contractor develops Adverse Outcome Pathways (AOPs), in accordance with guidelines proposed by the Organisation for Economic Co-operation and Development (OECD)3, for novel AOPs of interest to Federal agencies. The Contractor analyzes the performance characteristics of the proposed AOPs in the context of emerging scientific literature and novel computational approaches.  To explore the utility of alternative methods, the Contractor identifies, retrieves, and compares data generated from novel methods with extant data from traditional methods found in the published literature. Using public and proprietary databases, the Contractor gathers all relevant production, use, exposure, and toxicological information on selected chemicals and mixtures currently included in or inder consideration for inclusion in Tox21-related efforts or of interest to the NTP. The Contractor reviews and evaluates the data and information gathered from the literature searches and prepares comprehensive written reports, as needed. In order to promote the research, development, validation, acceptance, and use of new and alternative testing methods and strategies, NICEATM supports ICCVAM-coordinates evaluations of submitted and nominated test methods by drafting supporting documentation for ICCVAM review, comment, and approval. In addition, NICEATM may be called upon to coordinate validation studies for emerging alternative approaches. The contractor has the flexibility to subcontract for expertise not resident within the contract organization or to subcontract for necessary validation efforts. Flexibility to engage subcontractors and consultants when needed expertise is not resident within the contract organization is included under the base contract. n/a",NICEATM Support Contract: Base Contract,9201242,73201500010C,"['Animals', 'Authorization documentation', 'Characteristics', 'Chemicals', 'Computer Simulation', 'Contractor', 'Contracts', 'Data', 'Databases', 'Development', 'Documentation', 'Drug Kinetics', 'Economics', 'Educational workshop', 'Ensure', 'Evaluation', 'Exposure to', 'Guidelines', 'Hazard Identification', 'Health', 'Human', 'In Vitro', 'Interagency Coordinating Committee on the Validation of Alternative Methods', 'Knowledge', 'Literature', 'Methods', 'Mission', 'Modeling', 'National Toxicology Program', 'Pathway interactions', 'Peer Review', 'Performance', 'Production', 'Published Comment', 'Publishing', 'Quantitative Structure-Activity Relationship', 'Reporting', 'Research Support', 'Risk Assessment', 'Support Contracts', 'System', 'Testing', 'Toxicokinetics', 'Validation', 'Work', 'Writing', 'adverse outcome', 'base', 'flexibility', 'high throughput screening', 'in vivo', 'information gathering', 'interest', 'meetings', 'novel', 'operation', 'pharmacodynamic model', 'research and development', 'safety testing', 'screening', 'text searching', 'validation studies']",NIEHS,"INTEGRATED LABORATORY SYSTEMS, LLC",N01,2016,2526300,0.1045269194794446
"NICEATM Support Contract: Meeting Support This is a Research and Development (R&D) contract to provide scientific and administrative support for the National Toxicology Program (NTP) Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM). NICEATM research supports activities of NTP in general, and specifically, NTP’s Biomolecular Screening Branch (BSB), the Tox21 consortium, and the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM). NICEATM is responsible for ensuring compliance with the duties and provisions of the ICCVAM Authorization Act of 2000 (42 U.S.C. 285l-3) - to promote the research, development, validation, evaluation, acceptance, and use of new and alternative testing methods and strategies that are more predictive of human health and ecological effects than currently available methods and strategies. NICEATM carries out its mission by performing independent R&D activities, reviewing proposed test methods, organizing workshops and meetings, and facilitating peer reviews.  The Contractor is required to acquire and apply new and existing scientific knowledge to develop, evaluate, and validate novel computational approaches that can be used for chemical hazard identification and risk assessment with direct relevance to human health. Data development, analysis, and evaluation represents a large portion of the work of this requirement. These approaches include, but are not limited to: exposure modeling, physiologically based pharmacokinetic/pharmacodynamic (PBPK/PD) modeling, reverse toxicokinetic (R-TK) modeling, Quantitative Structure-Activity Relationship (QSAR) modeling, analysis of quantitative high throughput screening (qHTS) and high content (HC) data, and development of novel Integrated Testing and Decision Strategies (ITDS) using in vivo, in vitro and/or in silico systems. The Contractor routinely utilizes information from diverse data types and multiple databases (e.g., ToxRef DB, ToxCastDB, ExpoCastDB, DSSTox, CEBS, etc.) to develop and evaluate the above listed approaches. The Contractor develops Adverse Outcome Pathways (AOPs), in accordance with guidelines proposed by the Organisation for Economic Co-operation and Development (OECD)3, for novel AOPs of interest to Federal agencies. The Contractor analyzes the performance characteristics of the proposed AOPs in the context of emerging scientific literature and novel computational approaches.  To explore the utility of alternative methods, the Contractor identifies, retrieves, and compares data generated from novel methods with extant data from traditional methods found in the published literature. Using public and proprietary databases, the Contractor gathers all relevant production, use, exposure, and toxicological information on selected chemicals and mixtures currently included in or inder consideration for inclusion in Tox21-related efforts or of interest to the NTP. The Contractor reviews and evaluates the data and information gathered from the literature searches and prepares comprehensive written reports, as needed. In order to promote the research, development, validation, acceptance, and use of new and alternative testing methods and strategies, NICEATM supports ICCVAM-coordinates evaluations of submitted and nominated test methods by drafting supporting documentation for ICCVAM review, comment, and approval. In addition, NICEATM may be called upon to coordinate validation studies for emerging alternative approaches. The contractor has the flexibility to subcontract for expertise not resident within the contract organization or to subcontract for necessary validation efforts. n/a",NICEATM Support Contract: Meeting Support,9201244,73201500010C,"['Animals', 'Area', 'Authorization documentation', 'Characteristics', 'Chemicals', 'Computer Simulation', 'Contractor', 'Contracts', 'Data', 'Databases', 'Development', 'Documentation', 'Drug Kinetics', 'Economics', 'Educational workshop', 'Ensure', 'Evaluation', 'Exposure to', 'Guidelines', 'Hazard Identification', 'Health', 'Human', 'In Vitro', 'Interagency Coordinating Committee on the Validation of Alternative Methods', 'Knowledge', 'Letters', 'Literature', 'Methods', 'Mission', 'Modeling', 'National Toxicology Program', 'Pathway interactions', 'Peer Review', 'Performance', 'Production', 'Published Comment', 'Publishing', 'Quantitative Structure-Activity Relationship', 'Reporting', 'Research Support', 'Risk Assessment', 'Support Contracts', 'System', 'Testing', 'Toxicokinetics', 'Travel', 'Validation', 'Work', 'Writing', 'adverse outcome', 'base', 'flexibility', 'high throughput screening', 'in vivo', 'information gathering', 'interest', 'meetings', 'novel', 'operation', 'pharmacodynamic model', 'research and development', 'safety testing', 'screening', 'text searching', 'validation studies', 'web page']",NIEHS,"INTEGRATED LABORATORY SYSTEMS, LLC",N01,2016,27590,0.10868811036986309
"NICEATM Support Contract: Contractor Travel This is a Research and Development (R&D) contract to provide scientific and administrative support for the National Toxicology Program (NTP) Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM). NICEATM research supports activities of NTP in general, and specifically, NTP’s Biomolecular Screening Branch (BSB), the Tox21 consortium, and the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM). NICEATM is responsible for ensuring compliance with the duties and provisions of the ICCVAM Authorization Act of 2000 (42 U.S.C. 285l-3) - to promote the research, development, validation, evaluation, acceptance, and use of new and alternative testing methods and strategies that are more predictive of human health and ecological effects than currently available methods and strategies. NICEATM carries out its mission by performing independent R&D activities, reviewing proposed test methods, organizing workshops and meetings, and facilitating peer reviews.  The Contractor is required to acquire and apply new and existing scientific knowledge to develop, evaluate, and validate novel computational approaches that can be used for chemical hazard identification and risk assessment with direct relevance to human health. Data development, analysis, and evaluation represents a large portion of the work of this requirement. These approaches include, but are not limited to: exposure modeling, physiologically based pharmacokinetic/pharmacodynamic (PBPK/PD) modeling, reverse toxicokinetic (R-TK) modeling, Quantitative Structure-Activity Relationship (QSAR) modeling, analysis of quantitative high throughput screening (qHTS) and high content (HC) data, and development of novel Integrated Testing and Decision Strategies (ITDS) using in vivo, in vitro and/or in silico systems. The Contractor routinely utilizes information from diverse data types and multiple databases (e.g., ToxRef DB, ToxCastDB, ExpoCastDB, DSSTox, CEBS, etc.) to develop and evaluate the above listed approaches. The Contractor develops Adverse Outcome Pathways (AOPs), in accordance with guidelines proposed by the Organisation for Economic Co-operation and Development (OECD)3, for novel AOPs of interest to Federal agencies. The Contractor analyzes the performance characteristics of the proposed AOPs in the context of emerging scientific literature and novel computational approaches.  To explore the utility of alternative methods, the Contractor identifies, retrieves, and compares data generated from novel methods with extant data from traditional methods found in the published literature. Using public and proprietary databases, the Contractor gathers all relevant production, use, exposure, and toxicological information on selected chemicals and mixtures currently included in or inder consideration for inclusion in Tox21-related efforts or of interest to the NTP. The Contractor reviews and evaluates the data and information gathered from the literature searches and prepares comprehensive written reports, as needed. In order to promote the research, development, validation, acceptance, and use of new and alternative testing methods and strategies, NICEATM supports ICCVAM-coordinates evaluations of submitted and nominated test methods by drafting supporting documentation for ICCVAM review, comment, and approval. In addition, NICEATM may be called upon to coordinate validation studies for emerging alternative approaches. The contractor has the flexibility to subcontract for expertise not resident within the contract organization or to subcontract for necessary validation efforts. n/a",NICEATM Support Contract: Contractor Travel,9201243,73201500010C,"['Animals', 'Authorization documentation', 'Characteristics', 'Chemicals', 'Computer Simulation', 'Contractor', 'Contracts', 'Data', 'Databases', 'Development', 'Documentation', 'Drug Kinetics', 'Economics', 'Educational workshop', 'Ensure', 'Evaluation', 'Exposure to', 'Guidelines', 'Hazard Identification', 'Health', 'Human', 'In Vitro', 'Interagency Coordinating Committee on the Validation of Alternative Methods', 'International', 'Knowledge', 'Literature', 'Methods', 'Mission', 'Modeling', 'National Toxicology Program', 'Pathway interactions', 'Peer Review', 'Performance', 'Production', 'Published Comment', 'Publishing', 'Quantitative Structure-Activity Relationship', 'Reporting', 'Research Support', 'Risk Assessment', 'Support Contracts', 'System', 'Testing', 'Toxicokinetics', 'Travel', 'Validation', 'Work', 'Writing', 'adverse outcome', 'base', 'flexibility', 'high throughput screening', 'in vivo', 'information gathering', 'interest', 'meetings', 'member', 'novel', 'operation', 'outreach', 'pharmacodynamic model', 'research and development', 'safety testing', 'screening', 'text searching', 'validation studies']",NIEHS,"INTEGRATED LABORATORY SYSTEMS, LLC",N01,2016,34210,0.10868811036986309
"NICEATM Support Contract: Contractor Travel This is a Research and Development (R&D) contract to provide scientific and administrative support for the National Toxicology Program (NTP) Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM). NICEATM research supports activities of NTP in general, and specifically, NTP’s Biomolecular Screening Branch (BSB), the Tox21 consortium, and the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM). NICEATM is responsible for ensuring compliance with the duties and provisions of the ICCVAM Authorization Act of 2000 (42 U.S.C. 285l-3) - to promote the research, development, validation, evaluation, acceptance, and use of new and alternative testing methods and strategies that are more predictive of human health and ecological effects than currently available methods and strategies. NICEATM carries out its mission by performing independent R&D activities, reviewing proposed test methods, organizing workshops and meetings, and facilitating peer reviews.  The Contractor is required to acquire and apply new and existing scientific knowledge to develop, evaluate, and validate novel computational approaches that can be used for chemical hazard identification and risk assessment with direct relevance to human health. Data development, analysis, and evaluation represents a large portion of the work of this requirement. These approaches include, but are not limited to: exposure modeling, physiologically based pharmacokinetic/pharmacodynamic (PBPK/PD) modeling, reverse toxicokinetic (R-TK) modeling, Quantitative Structure-Activity Relationship (QSAR) modeling, analysis of quantitative high throughput screening (qHTS) and high content (HC) data, and development of novel Integrated Testing and Decision Strategies (ITDS) using in vivo, in vitro and/or in silico systems. The Contractor routinely utilizes information from diverse data types and multiple databases (e.g., ToxRef DB, ToxCastDB, ExpoCastDB, DSSTox, CEBS, etc.) to develop and evaluate the above listed approaches. The Contractor develops Adverse Outcome Pathways (AOPs), in accordance with guidelines proposed by the Organisation for Economic Co-operation and Development (OECD)3, for novel AOPs of interest to Federal agencies. The Contractor analyzes the performance characteristics of the proposed AOPs in the context of emerging scientific literature and novel computational approaches.   To explore the utility of alternative methods, the Contractor identifies, retrieves, and compares data generated from novel methods with extant data from traditional methods found in the published literature. Using public and proprietary databases, the Contractor gathers all relevant production, use, exposure, and toxicological information on selected chemicals and mixtures currently included in or inder consideration for inclusion in Tox21-related efforts or of interest to the NTP. The Contractor reviews and evaluates the data and information gathered from the literature searches and prepares comprehensive written reports, as needed. In order to promote the research, development, validation, acceptance, and use of new and alternative testing methods and strategies, NICEATM supports ICCVAM-coordinates evaluations of submitted and nominated test methods by drafting supporting documentation for ICCVAM review, comment, and approval. In addition, NICEATM may be called upon to coordinate validation studies for emerging alternative approaches. The contractor has the flexibility to subcontract for expertise not resident within the contract organization or to subcontract for necessary validation efforts. n/a",NICEATM Support Contract: Contractor Travel,9156446,73201500010C,"['Animals', 'Authorization documentation', 'Characteristics', 'Chemicals', 'Computer Simulation', 'Contractor', 'Contracts', 'Data', 'Databases', 'Development', 'Documentation', 'Drug Kinetics', 'Economics', 'Educational workshop', 'Ensure', 'Evaluation', 'Exposure to', 'Guidelines', 'Hazard Identification', 'Health', 'Human', 'In Vitro', 'Interagency Coordinating Committee on the Validation of Alternative Methods', 'International', 'Knowledge', 'Literature', 'Methods', 'Mission', 'Modeling', 'National Toxicology Program', 'Pathway interactions', 'Peer Review', 'Performance', 'Production', 'Published Comment', 'Publishing', 'Quantitative Structure-Activity Relationship', 'Reporting', 'Research Support', 'Risk Assessment', 'Support Contracts', 'System', 'Testing', 'Toxicokinetics', 'Travel', 'Validation', 'Work', 'Writing', 'adverse outcome', 'base', 'flexibility', 'high throughput screening', 'in vivo', 'information gathering', 'interest', 'meetings', 'member', 'novel', 'operation', 'outreach', 'pharmacodynamic model', 'research and development', 'safety testing', 'screening', 'text searching', 'validation studies']",NIEHS,"INTEGRATED LABORATORY SYSTEMS, LLC",N01,2015,17107,0.10868811036986309
"NICEATM Support Contract: Base Contract This is a Research and Development (R&D) contract to provide scientific and administrative support for the National Toxicology Program (NTP) Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM). NICEATM research supports activities of NTP in general, and specifically, NTP’s Biomolecular Screening Branch (BSB), the Tox21 consortium, and the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM). NICEATM is responsible for ensuring compliance with the duties and provisions of the ICCVAM Authorization Act of 2000 (42 U.S.C. 285l-3) - to promote the research, development, validation, evaluation, acceptance, and use of new and alternative testing methods and strategies that are more predictive of human health and ecological effects than currently available methods and strategies. NICEATM carries out its mission by performing independent R&D activities, reviewing proposed test methods, organizing workshops and meetings, and facilitating peer reviews.  The Contractor is required to acquire and apply new and existing scientific knowledge to develop, evaluate, and validate novel computational approaches that can be used for chemical hazard identification and risk assessment with direct relevance to human health. Data development, analysis, and evaluation represents a large portion of the work of this requirement. These approaches include, but are not limited to: exposure modeling, physiologically based pharmacokinetic/pharmacodynamic (PBPK/PD) modeling, reverse toxicokinetic (R-TK) modeling, Quantitative Structure-Activity Relationship (QSAR) modeling, analysis of quantitative high throughput screening (qHTS) and high content (HC) data, and development of novel Integrated Testing and Decision Strategies (ITDS) using in vivo, in vitro and/or in silico systems. The Contractor routinely utilizes information from diverse data types and multiple databases (e.g., ToxRef DB, ToxCastDB, ExpoCastDB, DSSTox, CEBS, etc.) to develop and evaluate the above listed approaches. The Contractor develops Adverse Outcome Pathways (AOPs), in accordance with guidelines proposed by the Organisation for Economic Co-operation and Development (OECD)3, for novel AOPs of interest to Federal agencies. The Contractor analyzes the performance characteristics of the proposed AOPs in the context of emerging scientific literature and novel computational approaches.   To explore the utility of alternative methods, the Contractor identifies, retrieves, and compares data generated from novel methods with extant data from traditional methods found in the published literature. Using public and proprietary databases, the Contractor gathers all relevant production, use, exposure, and toxicological information on selected chemicals and mixtures currently included in or inder consideration for inclusion in Tox21-related efforts or of interest to the NTP. The Contractor reviews and evaluates the data and information gathered from the literature searches and prepares comprehensive written reports, as needed. In order to promote the research, development, validation, acceptance, and use of new and alternative testing methods and strategies, NICEATM supports ICCVAM-coordinates evaluations of submitted and nominated test methods by drafting supporting documentation for ICCVAM review, comment, and approval. In addition, NICEATM may be called upon to coordinate validation studies for emerging alternative approaches. The contractor has the flexibility to subcontract for expertise not resident within the contract organization or to subcontract for necessary validation efforts. Flexibility to engage subcontractors and consultants when needed expertise is not resident within the contract organization is included under the base contract. n/a",NICEATM Support Contract: Base Contract,9156444,73201500010C,"['Animals', 'Authorization documentation', 'Characteristics', 'Chemicals', 'Computer Simulation', 'Contractor', 'Contracts', 'Data', 'Databases', 'Development', 'Documentation', 'Drug Kinetics', 'Economics', 'Educational workshop', 'Ensure', 'Evaluation', 'Exposure to', 'Guidelines', 'Hazard Identification', 'Health', 'Human', 'In Vitro', 'Interagency Coordinating Committee on the Validation of Alternative Methods', 'Knowledge', 'Literature', 'Methods', 'Mission', 'Modeling', 'National Toxicology Program', 'Pathway interactions', 'Peer Review', 'Performance', 'Production', 'Published Comment', 'Publishing', 'Quantitative Structure-Activity Relationship', 'Reporting', 'Research Support', 'Risk Assessment', 'Support Contracts', 'System', 'Testing', 'Toxicokinetics', 'Validation', 'Work', 'Writing', 'adverse outcome', 'base', 'flexibility', 'high throughput screening', 'in vivo', 'information gathering', 'interest', 'meetings', 'novel', 'operation', 'pharmacodynamic model', 'research and development', 'safety testing', 'screening', 'text searching', 'validation studies']",NIEHS,"INTEGRATED LABORATORY SYSTEMS, LLC",N01,2015,1171246,0.1045269194794446
"NICEATM Support Contract: Meeting Support This is a Research and Development (R&D) contract to provide scientific and administrative support for the National Toxicology Program (NTP) Interagency Center for the Evaluation of Alternative Toxicological Methods (NICEATM). NICEATM research supports activities of NTP in general, and specifically, NTP’s Biomolecular Screening Branch (BSB), the Tox21 consortium, and the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM). NICEATM is responsible for ensuring compliance with the duties and provisions of the ICCVAM Authorization Act of 2000 (42 U.S.C. 285l-3) - to promote the research, development, validation, evaluation, acceptance, and use of new and alternative testing methods and strategies that are more predictive of human health and ecological effects than currently available methods and strategies. NICEATM carries out its mission by performing independent R&D activities, reviewing proposed test methods, organizing workshops and meetings, and facilitating peer reviews.  The Contractor is required to acquire and apply new and existing scientific knowledge to develop, evaluate, and validate novel computational approaches that can be used for chemical hazard identification and risk assessment with direct relevance to human health. Data development, analysis, and evaluation represents a large portion of the work of this requirement. These approaches include, but are not limited to: exposure modeling, physiologically based pharmacokinetic/pharmacodynamic (PBPK/PD) modeling, reverse toxicokinetic (R-TK) modeling, Quantitative Structure-Activity Relationship (QSAR) modeling, analysis of quantitative high throughput screening (qHTS) and high content (HC) data, and development of novel Integrated Testing and Decision Strategies (ITDS) using in vivo, in vitro and/or in silico systems. The Contractor routinely utilizes information from diverse data types and multiple databases (e.g., ToxRef DB, ToxCastDB, ExpoCastDB, DSSTox, CEBS, etc.) to develop and evaluate the above listed approaches. The Contractor develops Adverse Outcome Pathways (AOPs), in accordance with guidelines proposed by the Organisation for Economic Co-operation and Development (OECD)3, for novel AOPs of interest to Federal agencies. The Contractor analyzes the performance characteristics of the proposed AOPs in the context of emerging scientific literature and novel computational approaches.   To explore the utility of alternative methods, the Contractor identifies, retrieves, and compares data generated from novel methods with extant data from traditional methods found in the published literature. Using public and proprietary databases, the Contractor gathers all relevant production, use, exposure, and toxicological information on selected chemicals and mixtures currently included in or inder consideration for inclusion in Tox21-related efforts or of interest to the NTP. The Contractor reviews and evaluates the data and information gathered from the literature searches and prepares comprehensive written reports, as needed. In order to promote the research, development, validation, acceptance, and use of new and alternative testing methods and strategies, NICEATM supports ICCVAM-coordinates evaluations of submitted and nominated test methods by drafting supporting documentation for ICCVAM review, comment, and approval. In addition, NICEATM may be called upon to coordinate validation studies for emerging alternative approaches. The contractor has the flexibility to subcontract for expertise not resident within the contract organization or to subcontract for necessary validation efforts. n/a",NICEATM Support Contract: Meeting Support,9156447,73201500010C,"['Animals', 'Area', 'Authorization documentation', 'Characteristics', 'Chemicals', 'Computer Simulation', 'Contractor', 'Contracts', 'Data', 'Databases', 'Development', 'Documentation', 'Drug Kinetics', 'Economics', 'Educational workshop', 'Ensure', 'Evaluation', 'Exposure to', 'Guidelines', 'Hazard Identification', 'Health', 'Human', 'In Vitro', 'Interagency Coordinating Committee on the Validation of Alternative Methods', 'Knowledge', 'Letters', 'Literature', 'Methods', 'Mission', 'Modeling', 'National Toxicology Program', 'Pathway interactions', 'Peer Review', 'Performance', 'Production', 'Published Comment', 'Publishing', 'Quantitative Structure-Activity Relationship', 'Reporting', 'Research Support', 'Risk Assessment', 'Support Contracts', 'System', 'Testing', 'Toxicokinetics', 'Travel', 'Validation', 'Work', 'Writing', 'adverse outcome', 'base', 'flexibility', 'high throughput screening', 'in vivo', 'information gathering', 'interest', 'meetings', 'novel', 'operation', 'pharmacodynamic model', 'research and development', 'safety testing', 'screening', 'text searching', 'validation studies', 'web page']",NIEHS,"INTEGRATED LABORATORY SYSTEMS, LLC",N01,2015,12631,0.10868811036986309

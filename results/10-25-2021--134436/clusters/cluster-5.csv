text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Individualized Training and Practice in Career Navigation Skills    DESCRIPTION (provided by applicant): As emphasized by the NIH Roadmap and NCRR's Strategic Priorities, NIH's broad mission to improve health outcomes for all persons depends on the ability to train and retain future generations of clinical and translational researchers who can quickly and effectively translate research findings into clinical practice. However, the past two decades have witnessed a significant decline in the clinical scientist workforce, which threatens our nation's ability to leverage advances in basic biomedical and behavioral sciences into improvements in public health. The transition to independent scientist is a particularly high-risk period for attrition from the research career path. Early career researchers face a variety of challenges when beginning their career and many of the skills needed to successfully establish and maintain a research career are not well taught during formal training. The need for and value of training in career navigation skills, such as planning, negotiation, and management, has been increasingly recognized. However, current attempts to bridge this educational gap tend to be either generic, static written materials which are limited in their utility for a given researcher or in-person workshops that are available to only a limited number of select trainees. The primary goal of this SBIR project is to create an interactive software product that provides individualized training and practice in career navigation skills to clinical and translational scientists on a broad scale. Through this software, recent advances in intelligent tutoring systems will be integrated so that researchers can actively participate in specific career challenges (e.g., negotiating salary) within a private, computer-based environment. This Phase I application will accomplish four specific aims: (1) gather recommendations through literature searches and interviews with highly experienced scientists who are actively engaged in training researchers in order to identify specific challenges faced by early career clinical and translational researchers as they transition to independent scientist, specific skills associated with success in career planning, negotiation, and management, and current best practices in career development training; (2) create the software prototype based on these recommendations and best practices; (3) conduct stakeholder feasibility test with independent researchers and early career researchers; and (4) use feasibility data to generate the Phase II development plan with a complete list of training modules and challenge areas to be included in the full software product. Phase I research is expected to demonstrate strong support for the interactive software product across stakeholders and essential feedback to guide Phase II development and testing.      PUBLIC HEALTH RELEVANCE: NIH's broad mission to improve health outcomes for all people emphasizes the need to accelerate and strengthen clinical and translational research. The significant decline in the clinical scientist workforce over the past two decades threatens our nation's ability to quickly and effectively translate advances in biomedical and behavioral sciences into public healthcare improvements. NIH's success depends upon the ability to train and retain future generations of clinical and translational researchers. In response, NCRR's Strategic Priorities stress the need for innovative methods to enhance the training, advancement, and retention of clinical and translational scientists. This SBIR project addresses this high priority through development and testing of an innovative interactive software product to provide individualized training and practice in key career navigation skills within a private, computer-based environment. If successful, the resulting tool could significantly impact public health by broadly disseminating vital training and practice opportunities to foster retention and career advancement of clinical and translational scientists.           NIH's broad mission to improve health outcomes for all people emphasizes the need to accelerate and strengthen clinical and translational research. The significant decline in the clinical scientist workforce over the past two decades threatens our nation's ability to quickly and effectively translate advances in biomedical and behavioral sciences into public healthcare improvements. NIH's success depends upon the ability to train and retain future generations of clinical and translational researchers. In response, NCRR's Strategic Priorities stress the need for innovative methods to enhance the training, advancement, and retention of clinical and translational scientists. This SBIR project addresses this high priority through development and testing of an innovative interactive software product to provide individualized training and practice in key career navigation skills within a private, computer-based environment. If successful, the resulting tool could significantly impact public health by broadly disseminating vital training and practice opportunities to foster retention and career advancement of clinical and translational scientists.         ",Individualized Training and Practice in Career Navigation Skills,8121175,R43TR000256,"['Address', 'Area', 'Attention', 'Behavioral Sciences', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Computer software', 'Computers', 'Data', 'Data Analyses', 'Decision Making', 'Development', 'Development Plans', 'Educational process of instructing', 'Educational workshop', 'Environment', 'Face', 'Faculty', 'Feedback', 'Fostering', 'Future Generations', 'Generic Drugs', 'Goals', 'Head', 'Health', 'Healthcare', 'Internet', 'Interview', 'Mediation', 'Methods', 'Mission', 'Nature', 'Outcome', 'Participant', 'Patient Care', 'Persons', 'Phase', 'Preventive', 'Public Health', 'Recommendation', 'Research', 'Research Personnel', 'Residencies', 'Scientist', 'Small Business Innovation Research Grant', 'Solutions', 'Specific qualifier value', 'Stress', 'Surveys', 'System', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Training Programs', 'Translating', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Wages', 'Writing', 'base', 'career', 'career development', 'clinical practice', 'design', 'experience', 'high risk', 'improved', 'innovation', 'post-doctoral training', 'programs', 'prototype', 'response', 'skills', 'success', 'symposium', 'text searching', 'tool', 'usability']",NCATS,3-C INSTITUTE FOR SOCIAL DEVELOPMENT,R43,2012,294927,0.04350977935874815
"Bayesian Statistics and Algorithms for Homology Modeling   DESCRIPTION (provided by applicant): To improve human health, a goal of the          human genome project is to translate the genome sequence into an understanding       of human biology. An important step in this process is knowledge of the              structure of human proteins and the effects of sequence polymorphisms on             structure and function. Currently, the structures of only 1000 human proteins        are known, but the structures of up to one third or so of human proteins can be      modeled based on the structures of homologous proteins in the Protein Data           Bank. This fraction will increase rapidly due to structural genomics efforts.        Unfortunately, general principles of what works in homology modeling and what        does not have remained elusive. The reasons for this are several: 1)                 insufficient benchmarking of most prediction methods; 2) reliance on                 out-of-date statistical analysis of protein structures, performed without modem      methods of statistics: 3) most modeling methods assume a relatively high level       of sequence identity (>35 percent) between template structure and sequence to        be modeled, when most proteins of unknown structure are only distantly related       to proteins of known structure. The PI proposes benchmarking, new statistical        analysis, and new algorithms for each of the three major aspects of homology         modeling: alignment, building backbone coordinates for insertiondeletion             regions, and sidechain placement. The primary tools will be Bayesian                 statistical analysis, including hierarchical models and non-parametric methods       based on the Dirichlet process. The increase in size of the sequence and             structure databases makes the new statistical analysis timely, both because of       the increased power the new data provide, and the numerous applications              afforded by more sequences and structures.                                                                                                                                n/a",Bayesian Statistics and Algorithms for Homology Modeling,6990509,R01HG002302,"['artificial intelligence', 'computer assisted sequence analysis', 'computer simulation', 'functional /structural genomics', 'mathematical model', 'model design /development', 'molecular biology information system', 'molecular dynamics', 'protein sequence', 'protein structure function', 'statistics /biometry', 'transposon /insertion element']",NHGRI,INSTITUTE FOR CANCER RESEARCH,R01,2006,246078,0.06664202972219559
"Bayesian Statistics and Algorithms for Homology Modeling   DESCRIPTION (provided by applicant): To improve human health, a goal of the          human genome project is to translate the genome sequence into an understanding       of human biology. An important step in this process is knowledge of the              structure of human proteins and the effects of sequence polymorphisms on             structure and function. Currently, the structures of only 1000 human proteins        are known, but the structures of up to one third or so of human proteins can be      modeled based on the structures of homologous proteins in the Protein Data           Bank. This fraction will increase rapidly due to structural genomics efforts.        Unfortunately, general principles of what works in homology modeling and what        does not have remained elusive. The reasons for this are several: 1)                 insufficient benchmarking of most prediction methods; 2) reliance on                 out-of-date statistical analysis of protein structures, performed without modem      methods of statistics: 3) most modeling methods assume a relatively high level       of sequence identity (>35 percent) between template structure and sequence to        be modeled, when most proteins of unknown structure are only distantly related       to proteins of known structure. The PI proposes benchmarking, new statistical        analysis, and new algorithms for each of the three major aspects of homology         modeling: alignment, building backbone coordinates for insertiondeletion             regions, and sidechain placement. The primary tools will be Bayesian                 statistical analysis, including hierarchical models and non-parametric methods       based on the Dirichlet process. The increase in size of the sequence and             structure databases makes the new statistical analysis timely, both because of       the increased power the new data provide, and the numerous applications              afforded by more sequences and structures.                                                                                                                                n/a",Bayesian Statistics and Algorithms for Homology Modeling,6830736,R01HG002302,"['artificial intelligence', 'computer assisted sequence analysis', 'computer simulation', 'functional /structural genomics', 'mathematical model', 'model design /development', 'molecular biology information system', 'molecular dynamics', 'protein sequence', 'protein structure function', 'statistics /biometry', 'transposon /insertion element']",NHGRI,INSTITUTE FOR CANCER RESEARCH,R01,2005,252000,0.06664202972219559
"Bayesian Statistics and Algorithms for Homology Modeling   DESCRIPTION (provided by applicant): To improve human health, a goal of the          human genome project is to translate the genome sequence into an understanding       of human biology. An important step in this process is knowledge of the              structure of human proteins and the effects of sequence polymorphisms on             structure and function. Currently, the structures of only 1000 human proteins        are known, but the structures of up to one third or so of human proteins can be      modeled based on the structures of homologous proteins in the Protein Data           Bank. This fraction will increase rapidly due to structural genomics efforts.        Unfortunately, general principles of what works in homology modeling and what        does not have remained elusive. The reasons for this are several: 1)                 insufficient benchmarking of most prediction methods; 2) reliance on                 out-of-date statistical analysis of protein structures, performed without modem      methods of statistics: 3) most modeling methods assume a relatively high level       of sequence identity (>35 percent) between template structure and sequence to        be modeled, when most proteins of unknown structure are only distantly related       to proteins of known structure. The PI proposes benchmarking, new statistical        analysis, and new algorithms for each of the three major aspects of homology         modeling: alignment, building backbone coordinates for insertiondeletion             regions, and sidechain placement. The primary tools will be Bayesian                 statistical analysis, including hierarchical models and non-parametric methods       based on the Dirichlet process. The increase in size of the sequence and             structure databases makes the new statistical analysis timely, both because of       the increased power the new data provide, and the numerous applications              afforded by more sequences and structures.                                                                                                                                n/a",Bayesian Statistics and Algorithms for Homology Modeling,6683607,R01HG002302,"['artificial intelligence', 'computer assisted sequence analysis', 'computer simulation', 'functional /structural genomics', 'mathematical model', 'model design /development', 'molecular biology information system', 'molecular dynamics', 'protein sequence', 'protein structure function', 'statistics /biometry', 'transposon /insertion element']",NHGRI,INSTITUTE FOR CANCER RESEARCH,R01,2004,252000,0.06664202972219559
"Bayesian Statistics and Algorithms for Homology Modeling   DESCRIPTION (provided by applicant): To improve human health, a goal of the          human genome project is to translate the genome sequence into an understanding       of human biology. An important step in this process is knowledge of the              structure of human proteins and the effects of sequence polymorphisms on             structure and function. Currently, the structures of only 1000 human proteins        are known, but the structures of up to one third or so of human proteins can be      modeled based on the structures of homologous proteins in the Protein Data           Bank. This fraction will increase rapidly due to structural genomics efforts.        Unfortunately, general principles of what works in homology modeling and what        does not have remained elusive. The reasons for this are several: 1)                 insufficient benchmarking of most prediction methods; 2) reliance on                 out-of-date statistical analysis of protein structures, performed without modem      methods of statistics: 3) most modeling methods assume a relatively high level       of sequence identity (>35 percent) between template structure and sequence to        be modeled, when most proteins of unknown structure are only distantly related       to proteins of known structure. The PI proposes benchmarking, new statistical        analysis, and new algorithms for each of the three major aspects of homology         modeling: alignment, building backbone coordinates for insertiondeletion             regions, and sidechain placement. The primary tools will be Bayesian                 statistical analysis, including hierarchical models and non-parametric methods       based on the Dirichlet process. The increase in size of the sequence and             structure databases makes the new statistical analysis timely, both because of       the increased power the new data provide, and the numerous applications              afforded by more sequences and structures.                                                                                                                                n/a",Bayesian Statistics and Algorithms for Homology Modeling,6622146,R01HG002302,"['artificial intelligence', ' computer assisted sequence analysis', ' computer simulation', ' functional /structural genomics', ' mathematical model', ' model design /development', ' molecular biology information system', ' molecular dynamics', ' protein sequence', ' protein structure function', ' statistics /biometry', ' transposon /insertion element']",NHGRI,INSTITUTE FOR CANCER RESEARCH,R01,2003,252000,0.06664202972219559
"Bayesian Statistics and Algorithms for Homology Modeling   DESCRIPTION (provided by applicant): To improve human health, a goal of the          human genome project is to translate the genome sequence into an understanding       of human biology. An important step in this process is knowledge of the              structure of human proteins and the effects of sequence polymorphisms on             structure and function. Currently, the structures of only 1000 human proteins        are known, but the structures of up to one third or so of human proteins can be      modeled based on the structures of homologous proteins in the Protein Data           Bank. This fraction will increase rapidly due to structural genomics efforts.        Unfortunately, general principles of what works in homology modeling and what        does not have remained elusive. The reasons for this are several: 1)                 insufficient benchmarking of most prediction methods; 2) reliance on                 out-of-date statistical analysis of protein structures, performed without modem      methods of statistics: 3) most modeling methods assume a relatively high level       of sequence identity (>35 percent) between template structure and sequence to        be modeled, when most proteins of unknown structure are only distantly related       to proteins of known structure. The PI proposes benchmarking, new statistical        analysis, and new algorithms for each of the three major aspects of homology         modeling: alignment, building backbone coordinates for insertiondeletion             regions, and sidechain placement. The primary tools will be Bayesian                 statistical analysis, including hierarchical models and non-parametric methods       based on the Dirichlet process. The increase in size of the sequence and             structure databases makes the new statistical analysis timely, both because of       the increased power the new data provide, and the numerous applications              afforded by more sequences and structures.                                                                                                                                n/a",Bayesian Statistics and Algorithms for Homology Modeling,6440018,R01HG002302,"['artificial intelligence', ' computer assisted sequence analysis', ' computer simulation', ' functional /structural genomics', ' mathematical model', ' model design /development', ' molecular biology information system', ' molecular dynamics', ' protein sequence', ' protein structure function', ' statistics /biometry', ' transposon /insertion element']",NHGRI,INSTITUTE FOR CANCER RESEARCH,R01,2002,284369,0.06664202972219559
"AUTOMATED EXPERT INTERPRETATION OF STATISTICAL RESEARCH The aim of the proposed project is to investigate the feasibility of developing a microcomputer-based expert system to assist scientists in selecting, performing, interpreting, and reporting statistical procedures. Often researchers in the health sciences are highly competent in their specific areas of expertise but inadequately trained in choosing and interpreting the statistics that are appropriate for the research they would like to do.  Also, many competent researchers are so pressed for time that they fail to prepare detailed reports of their research for the larger scientific community.  In Phase One, the feasibility of a computerized expert system will be demonstrated by developing and evaluating the automated design-consultation component of the system and by developing the report-generation facility for one complex statistical interpretation problem.  In Phase Two, the design and implementation of the complete computerized expert statistical system will be accomplished.  It appears that there is a great commercial opportunity for a microcomputer-based expert system to assist scientist in selecting, performing, interpreting, and reporting statistical procedures.  In addition to being commercially feasible, the system could have a significant impact upon the amount and quality of the research produced by professionals in many fields.  n/a",AUTOMATED EXPERT INTERPRETATION OF STATISTICAL RESEARCH,3505017,R43RR002564,"['computer data analysis', ' computer human interaction', ' computers', ' health science research analysis /evaluation', ' mathematical model', ' statistics /biometry']",NCRR,ASSESSMENT SYSTEMS CORPORATION,R43,1985,49216,0.04164280240634169
"Shifting Conceptions of Human Identity DESCRIPTION (provided by applicant):  . One of the most important questions raised by the ongoing achievements of the Human Genome Project is how this new biological knowledge - and the powers it confers - will affect our identity and self-understanding as human beings. This book project focuses on one key aspect of this complex issue: exploring the extent to which human identity can be reconciled with deliberate design or partial redesign. The author proposes to shed new light on this question by comparing the debates surrounding two areas of scientific innovation that are not normally associated with each other, but that are in fact deeply related: the enterprise of human genetic intervention and the enterprise of building intelligent machines. Both these enterprises entail ""pushing the limits"" of traditional concepts of what it means to be human; and both ultimately confront their makers with the same core ""family"" of questions: What are the defining features of human personhood? To what extent can those features be modified or extended, before human personhood begins to break down? Can some (or all) of those features find embodiment in an entity other than a human being? These kinds of questions are no longer the sole province of science fiction writers, but have been taken up with increasing seriousness by mainstream scientists and technologists, as well as by a wide array of ""science watchers"" in academia, legislative circles, and the news media.   . Through documentary research and interviews, this project aims to deepen our understanding of the history and sociology of the debates surrounding these powerful new technologies, electro-mechanical and biological, that are perceived as destabilizing human identity. The intended audience for the book is a broad one: scientists and technological practitioners interested in the social and cultural reception of their research; legislators and other policymakers with a stake in the governance of science; general educated readers who are concerned about the role of science and technology in shaping our collective future. n/a",Shifting Conceptions of Human Identity,6915830,R03HG003298,"['adult human (21+)', 'artificial intelligence', 'behavioral /social science research tag', 'biotechnology', 'books', 'clinical research', 'ethics', 'genetic manipulation', 'history of life science', 'human subject', 'identity', 'interview', 'robotics', 'self concept', 'sociology /anthropology']",NHGRI,VANDERBILT UNIVERSITY,R03,2005,75833,0.025709706867734532
"INTELLIGENT BIOMEDICAL ASSISTANT The rapid explosion of information and data on biologically important molecules requires powerful computer software to aid scientists with integrating and understanding that information.  As the number, size, and types of on-line information and databases increase, it becomes unreasonable to assume that working scientists will have a full understanding of the organization and contents of these information resources.  The hypothesis of our work in the design and development of an intelligent assistant for molecular biologists is that the technology of knowledge-based systems is sufficiently mature to assist in the discovery and interpretation of information relevant to molecular biological research.  Our long-term goal is to increase the productivity of biomedical scientists by designing, implementing, and providing computer-based intelligent assistants for them.  There are limitless numbers of such assistants that can be built, some of which already exist in some form, ranging from assistants that manage routine activities intelligently (e.g., data collection) to those that assist in creative tasks (such as explaining anomalies and planning experiments).  Creating a uniform, easy to use environment for this broad collection of assistants becomes an essential part of the goal since we can only save scientists' time if they can use the software we provide.  Thus we envision a biochemist's workstation with which scientists can exercise a wide variety of computer-based tools, including specialized assistants, matching algorithms, simulations, and databases.  Specifically, we propose to design and develop a workstation which will function as an Intelligent Biomedical Assistant (IBA) for researchers in molecular biology and genetics.  This IBA will be a highly adaptive and extensible system which will employ techniques from Artificial Intelligence (particularly expert systems and concept learning) which will assist the researcher in model building, experiment planning and hypothesis testing. The IBA will be assembled to assist with structural and functional questions about proteins that bind to DNA.  It will have knowledge of some of the existing software and databases that currently exist to help researchers, and will include basic knowledge of molecular biology in which questions are framed.  n/a",INTELLIGENT BIOMEDICAL ASSISTANT,3374197,R01LM005104,"['artificial intelligence', ' computer system design /evaluation', ' genetics', ' health science research potential', ' molecular biology']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,1991,142902,0.10510219406684428
"INTELLIGENT BIOMEDICAL ASSISTANT The rapid explosion of information and data on biologically important molecules requires powerful computer software to aid scientists with integrating and understanding that information.  As the number, size, and types of on-line information and databases increase, it becomes unreasonable to assume that working scientists will have a full understanding of the organization and contents of these information resources.  The hypothesis of our work in the design and development of an intelligent assistant for molecular biologists is that the technology of knowledge-based systems is sufficiently mature to assist in the discovery and interpretation of information relevant to molecular biological research.  Our long-term goal is to increase the productivity of biomedical scientists by designing, implementing, and providing computer-based intelligent assistants for them.  There are limitless numbers of such assistants that can be built, some of which already exist in some form, ranging from assistants that manage routine activities intelligently (e.g., data collection) to those that assist in creative tasks (such as explaining anomalies and planning experiments).  Creating a uniform, easy to use environment for this broad collection of assistants becomes an essential part of the goal since we can only save scientists' time if they can use the software we provide.  Thus we envision a biochemist's workstation with which scientists can exercise a wide variety of computer-based tools, including specialized assistants, matching algorithms, simulations, and databases.  Specifically, we propose to design and develop a workstation which will function as an Intelligent Biomedical Assistant (IBA) for researchers in molecular biology and genetics.  This IBA will be a highly adaptive and extensible system which will employ techniques from Artificial Intelligence (particularly expert systems and concept learning) which will assist the researcher in model building, experiment planning and hypothesis testing. The IBA will be assembled to assist with structural and functional questions about proteins that bind to DNA.  It will have knowledge of some of the existing software and databases that currently exist to help researchers, and will include basic knowledge of molecular biology in which questions are framed.  n/a",INTELLIGENT BIOMEDICAL ASSISTANT,3374196,R01LM005104,"['artificial intelligence', ' computer system design /evaluation', ' genetics', ' health science research potential', ' molecular biology']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,1990,131916,0.10510219406684428
"INTELLIGENT BIOMEDICAL ASSISTANT The rapid explosion of information and data on biologically important molecules requires powerful computer software to aid scientists with integrating and understanding that information.  As the number, size, and types of on-line information and databases increase, it becomes unreasonable to assume that working scientists will have a full understanding of the organization and contents of these information resources.  The hypothesis of our work in the design and development of an intelligent assistant for molecular biologists is that the technology of knowledge-based systems is sufficiently mature to assist in the discovery and interpretation of information relevant to molecular biological research.  Our long-term goal is to increase the productivity of biomedical scientists by designing, implementing, and providing computer-based intelligent assistants for them.  There are limitless numbers of such assistants that can be built, some of which already exist in some form, ranging from assistants that manage routine activities intelligently (e.g., data collection) to those that assist in creative tasks (such as explaining anomalies and planning experiments).  Creating a uniform, easy to use environment for this broad collection of assistants becomes an essential part of the goal since we can only save scientists' time if they can use the software we provide.  Thus we envision a biochemist's workstation with which scientists can exercise a wide variety of computer-based tools, including specialized assistants, matching algorithms, simulations, and databases.  Specifically, we propose to design and develop a workstation which will function as an Intelligent Biomedical Assistant (IBA) for researchers in molecular biology and genetics.  This IBA will be a highly adaptive and extensible system which will employ techniques from Artificial Intelligence (particularly expert systems and concept learning) which will assist the researcher in model building, experiment planning and hypothesis testing. The IBA will be assembled to assist with structural and functional questions about proteins that bind to DNA.  It will have knowledge of some of the existing software and databases that currently exist to help researchers, and will include basic knowledge of molecular biology in which questions are framed.  n/a",INTELLIGENT BIOMEDICAL ASSISTANT,3374195,R01LM005104,"['artificial intelligence', ' computer system design /evaluation', ' genetics', ' health science research potential', ' molecular biology']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,1989,136833,0.10510219406684428
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,6351629,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,2001,106893,0.04449003604362789
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN DESCRIPTION (Taken from application abstract):  Reminder systems are expert       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,6151393,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,2000,103781,0.04449003604362789
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN DESCRIPTION (Taken from application abstract):  Reminder systems are expert       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,2872989,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,1999,100760,0.04449003604362789
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN DESCRIPTION (Taken from application abstract):  Reminder systems are expert       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,2655310,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,1998,98015,0.04449003604362789
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN DESCRIPTION (Taken from application abstract):  Reminder systems are expert       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,2032395,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,1997,95423,0.04449003604362789
"Pattern Discovery for comparative epigenomics    DESCRIPTION (provided by applicant): We propose a training program that will prepare an effective independent investigator in computational genomics. The candidate has a PhD in biology from the University of Cambridge and will extend his skills in both computational and wet-lab methods through a two-year program of organized mentorship and training, and a structured five-year research program.  This program will promote the command of machine learning as applied to functional genomics data. Dr. William Noble will mentor the candidate's scientific development. Dr. Noble is a recognized leader in computational biology and machine learning. He holds a dual appointment as Associate Professor in Genome Sciences and Com- puter Science and Engineering, and has trained numerous postdoctoral fellows and graduate students. Dr. Jeff Bilmes, Associate Professor of Electrical Engineering, will contribute to the mentoring effort, and a committee of experienced genome and computational biologists will advise on science and the candidate's career goals.  Research will focus on the analysis of multiple tracks of data from high-throughput sequencing assays, such as the ChIP-seq data produced by the ENCODE Project. These experiments allow us to obtain a more complete picture of the structure of human chromatin, revealing the behavior of transcription factors, the organization of epigenetic modifications, and the locations of accessible DNA across the entire genome at up to single-base resolution. A current challenge is to discover joint patterns across multiple tracks of these functional genomics results simultaneously. This project will (1) develop computational methods for identifying such patterns, providing new ways of finding both well-understood genomic features and novel functional elements, (2) apply those methods to characterize the similarities and differences among different biological samples, establishing a better understanding of chromatin, the bounds of its variation, and its role in human disease, and (3) validate computational findings with laboratory experiments. The project will use a dynamic Bayesian network (DBN), a type of probabilistic graphical model, to represent the statistical dependencies between observed data, such as sequencing tag density, on an inferred hidden state sequence.  The Department of Genome Sciences of the University Of Washington School Of Medicine provides an ideal setting for training a new independent investigator with an extensive program of formal and informal education for postdoctoral scientists, opportunities for collaboration with researchers with expertise in diverse areas, and modern computational and laboratory resources. This environment maximizes the potential for the candidate to obtain the training and perform the research necessary to establish himself as a skilled investigator with an independent research program.         The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.            ",Pattern Discovery for comparative epigenomics,8331495,K99HG006259,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Appointment', 'Area', 'Base Pairing', 'Behavior', 'Biological', 'Biological Assay', 'Biology', 'Cell physiology', 'Cells', 'Censuses', 'Chromatin', 'Chromatin Structure', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Methylation', 'DNA Sequence', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Data Sources', 'Deoxyribonucleases', 'Dependency', 'Development', 'Digestion', 'Disease', 'Doctor of Philosophy', 'Education', 'Electrical Engineering', 'Elements', 'Engineering', 'Enhancers', 'Environment', 'Epigenetic Process', 'Event', 'Exposure to', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Human', 'Individual', 'Joints', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Length', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Outcome', 'Pattern', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Probability', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sampling', 'Schools', 'Science', 'Scientist', 'Seeds', 'Signal Transduction', 'Signaling Molecule', 'Structure', 'Techniques', 'Time', 'Training', 'Training Programs', 'Transfection', 'Transgenic Mice', 'Universities', 'Variant', 'Washington', 'Work', 'base', 'career', 'cell type', 'chromatin immunoprecipitation', 'clinically relevant', 'comparative', 'computer based statistical methods', 'computer science', 'cytokine', 'density', 'disorder control', 'empowered', 'epigenomics', 'experience', 'follow-up', 'functional genomics', 'genome-wide', 'graduate student', 'histone modification', 'human disease', 'human tissue', 'improved', 'network models', 'new technology', 'novel', 'professor', 'programs', 'promoter', 'research study', 'skills', 'speech processing', 'transcription factor']",NHGRI,UNIVERSITY OF WASHINGTON,K99,2012,103535,0.0005699732971661247
"Pattern Discovery for comparative epigenomics    DESCRIPTION (provided by applicant): We propose a training program that will prepare an effective independent investigator in computational genomics. The candidate has a PhD in biology from the University of Cambridge and will extend his skills in both computational and wet-lab methods through a two-year program of organized mentorship and training, and a structured five-year research program.  This program will promote the command of machine learning as applied to functional genomics data. Dr. William Noble will mentor the candidate's scientific development. Dr. Noble is a recognized leader in computational biology and machine learning. He holds a dual appointment as Associate Professor in Genome Sciences and Com- puter Science and Engineering, and has trained numerous postdoctoral fellows and graduate students. Dr. Jeff Bilmes, Associate Professor of Electrical Engineering, will contribute to the mentoring effort, and a committee of experienced genome and computational biologists will advise on science and the candidate's career goals.  Research will focus on the analysis of multiple tracks of data from high-throughput sequencing assays, such as the ChIP-seq data produced by the ENCODE Project. These experiments allow us to obtain a more complete picture of the structure of human chromatin, revealing the behavior of transcription factors, the organization of epigenetic modifications, and the locations of accessible DNA across the entire genome at up to single-base resolution. A current challenge is to discover joint patterns across multiple tracks of these functional genomics results simultaneously. This project will (1) develop computational methods for identifying such patterns, providing new ways of finding both well-understood genomic features and novel functional elements, (2) apply those methods to characterize the similarities and differences among different biological samples, establishing a better understanding of chromatin, the bounds of its variation, and its role in human disease, and (3) validate computational findings with laboratory experiments. The project will use a dynamic Bayesian network (DBN), a type of probabilistic graphical model, to represent the statistical dependencies between observed data, such as sequencing tag density, on an inferred hidden state sequence.  The Department of Genome Sciences of the University Of Washington School Of Medicine provides an ideal setting for training a new independent investigator with an extensive program of formal and informal education for postdoctoral scientists, opportunities for collaboration with researchers with expertise in diverse areas, and modern computational and laboratory resources. This environment maximizes the potential for the candidate to obtain the training and perform the research necessary to establish himself as a skilled investigator with an independent research program.       PUBLIC HEALTH RELEVANCE: The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.              The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.            ",Pattern Discovery for comparative epigenomics,8164533,K99HG006259,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Appointment', 'Area', 'Base Pairing', 'Behavior', 'Biological', 'Biological Assay', 'Biology', 'Cell physiology', 'Cells', 'Censuses', 'Chromatin', 'Chromatin Structure', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Methylation', 'DNA Sequence', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Data Sources', 'Deoxyribonucleases', 'Dependency', 'Development', 'Digestion', 'Disease', 'Doctor of Philosophy', 'Education', 'Electrical Engineering', 'Elements', 'Engineering', 'Enhancers', 'Environment', 'Epigenetic Process', 'Event', 'Exposure to', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Human', 'Individual', 'Joints', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Length', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Outcome', 'Pattern', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Probability', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sampling', 'Schools', 'Science', 'Scientist', 'Seeds', 'Signal Transduction', 'Signaling Molecule', 'Structure', 'Techniques', 'Time', 'Training', 'Training Programs', 'Transfection', 'Transgenic Mice', 'Universities', 'Variant', 'Washington', 'Work', 'base', 'career', 'cell type', 'chromatin immunoprecipitation', 'clinically relevant', 'comparative', 'computer based statistical methods', 'computer science', 'cytokine', 'density', 'disorder control', 'empowered', 'epigenomics', 'experience', 'follow-up', 'functional genomics', 'genome-wide', 'graduate student', 'histone modification', 'human disease', 'human tissue', 'improved', 'network models', 'new technology', 'novel', 'professor', 'programs', 'promoter', 'research study', 'skills', 'speech processing', 'transcription factor']",NHGRI,UNIVERSITY OF WASHINGTON,K99,2011,102709,0.015752343973218456
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9983144,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Information Retrieval', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'machine learning method', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'structured data', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,264232,0.0698574468840755
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9772541,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,264255,0.0698574468840755
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9543557,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,264277,0.0698574468840755
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9365558,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Cereals', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Models', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'molecular modeling', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,264299,0.0698574468840755
"Academy of Aphasia Research and Training Symposium PROJECT SUMMARY/ABSTRACT The annual Academy of Aphasia meeting is the premier conference for researchers in the field of language processing and aphasia. Since the first meeting in 1963, this international meeting has brought together an interdisciplinary group of linguists, psychologists, neurologists, and speech-language pathologists to discuss the latest research in the field of aphasia, including theoretical, clinical, and rehabilitation aspects of this language disorder. The topics at the conference range widely but almost always cover all aspects of language processing including phonological processing, lexical-semantic processing, syntactic processing, orthographic processing, bilingualism, computational modeling, non-invasive and invasive brain imaging, language recovery, neuroplasticity, and rehabilitation. In this proposal, we aim to include two special initiatives that will take place during the annual academy of aphasia conference. The first initiative involves a formal mentoring program for young investigators entering the field of aphasia research. In this program, selected student/post-doctoral fellows from interdisciplinary backgrounds who are first authors at the conference are paired with a mentor. This mentor will provide specific feedback about the fellow's presentation and general mentorship to the fellow about research and academic careers. This program is currently occurring as part of the conference and has been growing at the annual meeting with very positive feedback. Additionally, a formal mentoring meeting will allow a structured format for discussion about a career in aphasia research. The second initiative will be a three hour seminar (New Frontiers in Aphasia Research) that covers the background and approach of a state of the art methodology (e.g., fNIRS, graph theoretical metric, machine learning approaches) that has an application to the study of aphasia. These workshops will be recorded and, consequently, uploaded to the academy website/youtube channel for dissemination to aphasia researchers and the public. This workshop will allow conference attendees to understand the conceptual and methodological aspect of a particular scientific approach that can be implemented in their study of aphasia. Given the highly interdisciplinary nature of aphasia research, these workshops will bridge the communication between aphasia researchers and scientists and experts who have developed new approaches to study the brain. This meeting already allows a valuable opportunity for cross-pollination of research ideas and will now provide a platform for the training the next generation of scientists interested in pursuing the nature of aphasia and associated language disorders in adults. PROJECT NARRATIVE Approximately 100,000 individuals suffer from aphasia each year. The academy of aphasia is an organization of clinicians, scientists and practitioners who study this communication disorder and develop interventions to treat individuals with aphasia. The members comprise a very interdisciplinary group of linguists, psychologists, speech and language clinicians, neurologists and neuroscientists. The annual academy of aphasia meeting is the premier venue to share state of the art methodologies for application in the study of aphasia to improve the research in the development of diagnosis and treatment approaches to alleviate aphasia. This conference is also the ideal venue to educate and train the next generation of aphasia researchers who are well trained from a theoretical, technical and clinical standpoint and are committed to expand the impact of research in aphasia.",Academy of Aphasia Research and Training Symposium,9944489,R13DC017375,"['Academy', 'Adult', 'Aphasia', 'Brain', 'Brain imaging', 'Chicago', 'Clinical', 'Collaborations', 'Committee Membership', 'Communication', 'Communication impairment', 'Computer Models', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Discipline', 'Educational workshop', 'Feedback', 'Fellowship', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Governing Board', 'Grant', 'Graph', 'Hour', 'Image', 'Individual', 'International', 'Intervention', 'Language', 'Language Disorders', 'Learning', 'Lesion', 'Linguistics', 'Machine Learning', 'Manuscripts', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Mission', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Neurologic', 'Neurologist', 'Neuronal Plasticity', 'Neurosciences', 'Non-aphasic', 'Orthography', 'Outcome', 'Pathologist', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychologist', 'Psychology', 'Publishing', 'Reading', 'Recovery', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Support', 'Research Training', 'Rest', 'Retrieval', 'Scientist', 'Secure', 'Speech', 'Speech Pathologist', 'Speech Perception', 'Stroke', 'Structure', 'Students', 'Symptoms', 'Techniques', 'Testing', 'Time', 'Training', 'Training and Education', 'Transcranial magnetic stimulation', 'Travel', 'Videotape', 'Work', 'Writing', 'base', 'bilingualism', 'career', 'career networking', 'experience', 'frontier', 'improved', 'improved outcome', 'interest', 'language processing', 'lexical', 'meetings', 'member', 'neuroimaging', 'next generation', 'novel', 'novel strategies', 'outcome forecast', 'phonology', 'programs', 'relating to nervous system', 'semantic processing', 'success', 'symposium', 'syntax', 'tenure track', 'web site']",NIDCD,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R13,2020,39939,-0.00015722620100079107
"Academy of Aphasia Research and Training Symposium PROJECT SUMMARY/ABSTRACT The annual Academy of Aphasia meeting is the premier conference for researchers in the field of language processing and aphasia. Since the first meeting in 1963, this international meeting has brought together an interdisciplinary group of linguists, psychologists, neurologists, and speech-language pathologists to discuss the latest research in the field of aphasia, including theoretical, clinical, and rehabilitation aspects of this language disorder. The topics at the conference range widely but almost always cover all aspects of language processing including phonological processing, lexical-semantic processing, syntactic processing, orthographic processing, bilingualism, computational modeling, non-invasive and invasive brain imaging, language recovery, neuroplasticity, and rehabilitation. In this proposal, we aim to include two special initiatives that will take place during the annual academy of aphasia conference. The first initiative involves a formal mentoring program for young investigators entering the field of aphasia research. In this program, selected student/post-doctoral fellows from interdisciplinary backgrounds who are first authors at the conference are paired with a mentor. This mentor will provide specific feedback about the fellow's presentation and general mentorship to the fellow about research and academic careers. This program is currently occurring as part of the conference and has been growing at the annual meeting with very positive feedback. Additionally, a formal mentoring meeting will allow a structured format for discussion about a career in aphasia research. The second initiative will be a three hour seminar (New Frontiers in Aphasia Research) that covers the background and approach of a state of the art methodology (e.g., fNIRS, graph theoretical metric, machine learning approaches) that has an application to the study of aphasia. These workshops will be recorded and, consequently, uploaded to the academy website/youtube channel for dissemination to aphasia researchers and the public. This workshop will allow conference attendees to understand the conceptual and methodological aspect of a particular scientific approach that can be implemented in their study of aphasia. Given the highly interdisciplinary nature of aphasia research, these workshops will bridge the communication between aphasia researchers and scientists and experts who have developed new approaches to study the brain. This meeting already allows a valuable opportunity for cross-pollination of research ideas and will now provide a platform for the training the next generation of scientists interested in pursuing the nature of aphasia and associated language disorders in adults. PROJECT NARRATIVE Approximately 100,000 individuals suffer from aphasia each year. The academy of aphasia is an organization of clinicians, scientists and practitioners who study this communication disorder and develop interventions to treat individuals with aphasia. The members comprise a very interdisciplinary group of linguists, psychologists, speech and language clinicians, neurologists and neuroscientists. The annual academy of aphasia meeting is the premier venue to share state of the art methodologies for application in the study of aphasia to improve the research in the development of diagnosis and treatment approaches to alleviate aphasia. This conference is also the ideal venue to educate and train the next generation of aphasia researchers who are well trained from a theoretical, technical and clinical standpoint and are committed to expand the impact of research in aphasia.",Academy of Aphasia Research and Training Symposium,9731439,R13DC017375,"['Academy', 'Adult', 'Aphasia', 'Brain', 'Brain imaging', 'Chicago', 'Clinical', 'Collaborations', 'Committee Membership', 'Communication', 'Communication impairment', 'Computer Simulation', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Discipline', 'Educational workshop', 'Feedback', 'Fellowship', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Governing Board', 'Grant', 'Graph', 'Hour', 'Image Analysis', 'Individual', 'International', 'Intervention', 'Language', 'Language Disorders', 'Learning', 'Lesion', 'Linguistics', 'Machine Learning', 'Manuscripts', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Mission', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Neurologic', 'Neurologist', 'Neuronal Plasticity', 'Neurosciences', 'Non-aphasic', 'Orthography', 'Outcome', 'Pathologist', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychologist', 'Psychology', 'Publishing', 'Reading', 'Recovery', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Support', 'Research Training', 'Rest', 'Retrieval', 'Scientist', 'Secure', 'Speech', 'Speech Pathologist', 'Speech Perception', 'Stroke', 'Structure', 'Students', 'Symptoms', 'Techniques', 'Testing', 'Time', 'Training', 'Training and Education', 'Transcranial magnetic stimulation', 'Travel', 'Videotape', 'Work', 'Writing', 'base', 'bilingualism', 'career', 'career networking', 'experience', 'frontier', 'improved', 'improved outcome', 'interest', 'language processing', 'lexical', 'meetings', 'member', 'neuroimaging', 'next generation', 'novel', 'novel strategies', 'outcome forecast', 'phonology', 'programs', 'relating to nervous system', 'semantic processing', 'success', 'symposium', 'syntax', 'tenure track', 'web site']",NIDCD,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R13,2019,39939,-0.00015722620100079107
"Academy of Aphasia Research and Training Symposium PROJECT SUMMARY/ABSTRACT The annual Academy of Aphasia meeting is the premier conference for researchers in the field of language processing and aphasia. Since the first meeting in 1963, this international meeting has brought together an interdisciplinary group of linguists, psychologists, neurologists, and speech-language pathologists to discuss the latest research in the field of aphasia, including theoretical, clinical, and rehabilitation aspects of this language disorder. The topics at the conference range widely but almost always cover all aspects of language processing including phonological processing, lexical-semantic processing, syntactic processing, orthographic processing, bilingualism, computational modeling, non-invasive and invasive brain imaging, language recovery, neuroplasticity, and rehabilitation. In this proposal, we aim to include two special initiatives that will take place during the annual academy of aphasia conference. The first initiative involves a formal mentoring program for young investigators entering the field of aphasia research. In this program, selected student/post-doctoral fellows from interdisciplinary backgrounds who are first authors at the conference are paired with a mentor. This mentor will provide specific feedback about the fellow's presentation and general mentorship to the fellow about research and academic careers. This program is currently occurring as part of the conference and has been growing at the annual meeting with very positive feedback. Additionally, a formal mentoring meeting will allow a structured format for discussion about a career in aphasia research. The second initiative will be a three hour seminar (New Frontiers in Aphasia Research) that covers the background and approach of a state of the art methodology (e.g., fNIRS, graph theoretical metric, machine learning approaches) that has an application to the study of aphasia. These workshops will be recorded and, consequently, uploaded to the academy website/youtube channel for dissemination to aphasia researchers and the public. This workshop will allow conference attendees to understand the conceptual and methodological aspect of a particular scientific approach that can be implemented in their study of aphasia. Given the highly interdisciplinary nature of aphasia research, these workshops will bridge the communication between aphasia researchers and scientists and experts who have developed new approaches to study the brain. This meeting already allows a valuable opportunity for cross-pollination of research ideas and will now provide a platform for the training the next generation of scientists interested in pursuing the nature of aphasia and associated language disorders in adults. PROJECT NARRATIVE Approximately 100,000 individuals suffer from aphasia each year. The academy of aphasia is an organization of clinicians, scientists and practitioners who study this communication disorder and develop interventions to treat individuals with aphasia. The members comprise a very interdisciplinary group of linguists, psychologists, speech and language clinicians, neurologists and neuroscientists. The annual academy of aphasia meeting is the premier venue to share state of the art methodologies for application in the study of aphasia to improve the research in the development of diagnosis and treatment approaches to alleviate aphasia. This conference is also the ideal venue to educate and train the next generation of aphasia researchers who are well trained from a theoretical, technical and clinical standpoint and are committed to expand the impact of research in aphasia.",Academy of Aphasia Research and Training Symposium,9612777,R13DC017375,"['Academy', 'Adult', 'Aphasia', 'Brain', 'Brain imaging', 'Chicago', 'Clinical', 'Collaborations', 'Committee Membership', 'Communication', 'Communication impairment', 'Computer Simulation', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Discipline', 'Educational workshop', 'Feedback', 'Fellowship', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Governing Board', 'Grant', 'Graph', 'Hour', 'Image Analysis', 'Individual', 'International', 'Intervention', 'Language', 'Language Disorders', 'Learning', 'Lesion', 'Linguistics', 'Machine Learning', 'Manuscripts', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Mission', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Neurologic', 'Neurologist', 'Neuronal Plasticity', 'Neurosciences', 'Orthography', 'Outcome', 'Pathologist', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychologist', 'Psychology', 'Publishing', 'Reading', 'Recovery', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Support', 'Research Training', 'Rest', 'Retrieval', 'Scientist', 'Secure', 'Speech', 'Speech Pathologist', 'Speech Perception', 'Stroke', 'Structure', 'Students', 'Symptoms', 'Techniques', 'Testing', 'Time', 'Training', 'Training and Education', 'Transcranial magnetic stimulation', 'Travel', 'Videotape', 'Work', 'Writing', 'base', 'bilingualism', 'career', 'career networking', 'experience', 'frontier', 'improved', 'improved outcome', 'interest', 'language processing', 'lexical', 'meetings', 'member', 'neuroimaging', 'next generation', 'novel', 'novel strategies', 'outcome forecast', 'phonology', 'programs', 'relating to nervous system', 'semantic processing', 'success', 'symposium', 'syntax', 'tenure track', 'web site']",NIDCD,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R13,2018,39939,-0.00015722620100079107
"CRCNS: Common algorithmic strategies used by the brain for labeling points in high-dimensional space The first major goal of this work is to learn how certain brain regions (olfactory system,  hippocampus, and cerebellum) learn very complex stimuli that employ a combinatorial code to  identify stimuli as points in a high-dimensional space. For example, the simple fruit fly  olfactory system uses the firing rates of 50 different types of odorant receptors to  identify each odor by placing it at a point in a SO-dimensional space. Although the fly olfactory  system is well understood, less is known about analogous regions in vertebrate brains, and our  goal is to begin to learn about these other regions. The first step is to start with the mouse olfactory system that is similar to the fly in some ways but has complexities that are  absent in insects. These complexities include an enhanced ability to handle noise in the  odor and to learn over time to discriminate between very similar odors (e.g., two types of  red wines). Preliminary evidence shows that it should be possible to learn the role of  these complexities in vertebrate olfaction. The research design involves studying the  anatomy and recording the firing rates of different types of neurons at different levels of the  mouse olfactory system and in applying computational methods and algorithms that have  proved successful in earlier work to describe these complexities. The second major goal is to use insights into how these brain regions operate to improve the  function of computer algorithms. For a long time, a dream of many neuroscientists and computer  scientists has been to understand how the brain works well enough that we could translate insights  from the brain to improve machine computation. Indeed, experience has shown that the brain   has evolved novel variations of information processing algorithms used by computer  scientists to solve general computational problems. With sufficient insight into algorithms  used by the brain, these insights may provide unexpected ways to improve the function computer  science algorithms. Further, understanding the circuit mechanisms involved in olfactory processing can help illuminate the basis of a variety of smell disorders, and may  in the future lead to the construction of artificial smelling devices. Our main goal is to understand what computations are used by the mammalian olfactory system; this  has two types of public relevance. First, we hope to learn what circuit mechanisms are used in  this brain region, which may help identify how disruption of these mechanisms causes  brain and olfactory malfunction. Second, we hope to improve computer performance by using insights into how the brain computes.",CRCNS: Common algorithmic strategies used by the brain for labeling points in high-dimensional space,9994975,R01DC017695,"['Algorithm Design', 'Algorithms', 'Anatomy', 'Benchmarking', 'Biological', 'Brain', 'Brain region', 'Burn injury', 'Cells', 'Cerebellum', 'Chemicals', 'Code', 'Coffee', 'Complex', 'Computational algorithm', 'Computers', 'Computing Methodologies', 'Data Set', 'Devices', 'Dimensions', 'Discrimination', 'Dreams', 'Drosophila genus', 'Face', 'Feedback', 'Fire - disasters', 'Fruit', 'Future', 'Goals', 'Hippocampus (Brain)', 'Human', 'Individual', 'Insecta', 'Label', 'Lead', 'Learning', 'Literature', 'Machine Learning', 'Mammals', 'Modification', 'Mus', 'Neurons', 'Neurophysiology - biologic function', 'Noise', 'Odorant Receptors', 'Odors', 'Olfaction Disorders', 'Olfactory Pathways', 'Pattern', 'Performance', 'Population', 'Procedures', 'Property', 'Psychological reinforcement', 'Research Design', 'Role', 'Science', 'Scientist', 'Series', 'Smell Perception', 'Stimulus', 'System', 'Techniques', 'Time', 'Translating', 'Variant', 'Work', 'base', 'behavioral response', 'combinatorial', 'computational basis', 'computer science', 'experience', 'experimental study', 'fly', 'high dimensionality', 'improved', 'improved functioning', 'information processing', 'insight', 'learning community', 'neural circuit', 'novel', 'novel strategies', 'preservation', 'red wine', 'response', 'theories']",NIDCD,COLD SPRING HARBOR LABORATORY,R01,2020,446764,0.009735566480982522
"CRCNS: Common algorithmic strategies used by the brain for labeling points in high-dimensional space The first major goal of this work is to learn how certain brain regions (olfactory system,  hippocampus, and cerebellum) learn very complex stimuli that employ a combinatorial code to  identify stimuli as points in a high-dimensional space. For example, the simple fruit fly  olfactory system uses the firing rates of 50 different types of odorant receptors to  identify each odor by placing it at a point in a SO-dimensional space. Although the fly olfactory  system is well understood, less is known about analogous regions in vertebrate brains, and our  goal is to begin to learn about these other regions. The first step is to start with the mouse olfactory system that is similar to the fly in some ways but has complexities that are  absent in insects. These complexities include an enhanced ability to handle noise in the  odor and to learn over time to discriminate between very similar odors (e.g., two types of  red wines). Preliminary evidence shows that it should be possible to learn the role of  these complexities in vertebrate olfaction. The research design involves studying the  anatomy and recording the firing rates of different types of neurons at different levels of the  mouse olfactory system and in applying computational methods and algorithms that have  proved successful in earlier work to describe these complexities. The second major goal is to use insights into how these brain regions operate to improve the  function of computer algorithms. For a long time, a dream of many neuroscientists and computer  scientists has been to understand how the brain works well enough that we could translate insights  from the brain to improve machine computation. Indeed, experience has shown that the brain   has evolved novel variations of information processing algorithms used by computer  scientists to solve general computational problems. With sufficient insight into algorithms  used by the brain, these insights may provide unexpected ways to improve the function computer  science algorithms. Further, understanding the circuit mechanisms involved in olfactory processing can help illuminate the basis of a variety of smell disorders, and may  in the future lead to the construction of artificial smelling devices. Our main goal is to understand what computations are used by the mammalian olfactory system; this  has two types of public relevance. First, we hope to learn what circuit mechanisms are used in  this brain region, which may help identify how disruption of these mechanisms causes  brain and olfactory malfunction. Second, we hope to improve computer performance by using insights into how the brain computes.",CRCNS: Common algorithmic strategies used by the brain for labeling points in high-dimensional space,9692942,R01DC017695,"['Algorithm Design', 'Algorithms', 'Anatomy', 'Benchmarking', 'Biological', 'Brain', 'Brain region', 'Burn injury', 'Cells', 'Cerebellum', 'Chemicals', 'Code', 'Coffee', 'Complex', 'Computational algorithm', 'Computers', 'Computing Methodologies', 'Data Set', 'Devices', 'Dimensions', 'Discrimination', 'Dreams', 'Drosophila genus', 'Face', 'Feedback', 'Fire - disasters', 'Fruit', 'Future', 'Goals', 'Hippocampus (Brain)', 'Human', 'Individual', 'Insecta', 'Label', 'Lead', 'Learning', 'Literature', 'Machine Learning', 'Mammals', 'Modification', 'Mus', 'Neurons', 'Neurophysiology - biologic function', 'Noise', 'Odorant Receptors', 'Odors', 'Olfaction Disorders', 'Olfactory Pathways', 'Pattern', 'Performance', 'Population', 'Procedures', 'Property', 'Psychological reinforcement', 'Research Design', 'Role', 'Science', 'Scientist', 'Series', 'Smell Perception', 'Stimulus', 'System', 'Techniques', 'Time', 'Translating', 'Variant', 'Work', 'base', 'behavioral response', 'combinatorial', 'computer science', 'experience', 'experimental study', 'fly', 'high dimensionality', 'improved', 'improved functioning', 'information processing', 'insight', 'learning community', 'neural circuit', 'novel', 'novel strategies', 'red wine', 'response', 'theories']",NIDCD,SALK INSTITUTE FOR BIOLOGICAL STUDIES,R01,2018,481000,0.009735566480982522
"An Integrative Bioinformatics Approach to Study Single Cancer Cell Heterogeneity DESCRIPTION:  My long term career goal is to become a leading expert in translational bioinformatics who creates, develops and applies computational and statistical methods to reveal landscapes of cancers and to identify strategies to cure cancers. Human cancers are highly heterogeneous. Such heterogeneity is the major source of the ultimate failure of most cancer agents. However, due to the limit of technologies, the intercellular heterogeneity has not been investigated genome wide, at single-cell level until recently. New technologies such as single-cell transcriptome sequencing (RNA-Seq) and exome have revealed new insights and more profound complexity than was previously thought. However, so far these technologies are limited to one assay per cell. It remains a grand challenge to perform multiple, integrative assays from the same single tumor cell, in particular, from those derived from small tumor biopsies. Given the stochasticity at the single cell resolution, reproducibility and sensitivity ar daunting tasks. To overcome this challenge, I have started the single cancer cell sequencing analysis project, in collaboration with Dr. Sherman Weissman at Yale University, who is also my co-mentor of this K01 proposal. My immediate career goal is to identify genome-wide heterogeneity among single cancer cells, using the erythroleukemia K562 cell line. Towards this, I am proposing a research project on an integrative bioinformatics approach to analyze multiple types of genomics data generated from the same single leukemia cells, a timely and critical topic. Specifically, I am interested in studying the following specific aims: (1) buildinga bioinformatics pipeline to study heterogeneity of single-cell RNA-Seq, (2) building a bioinformatics pipeline to study CpG methylome of single cells, (3) building a bioinformatics pipeline to study single-cell Exome-Seq, and (4) integrate the RNA-Seq, methylome and Exome-Seq data generated from the same single cells. These single cells genomic data are provided by Dr. Sherman Weissman's lab from 30 single K562 erythroleukemia cells. I will first construct and validate in parallel, the RNA-Seq, methylome, and Exome-Seq bioinformatics pipelines optimized for single-cell analysis, and then develop and validate an integrative platform to analyze these multiple types of high-throughput data. To accomplish the research project, and to successfully transit from a junior faculty to an expert of the field, I have developed a career plan with my mentoring committee composed of four world-class experts in different fields relevant to Big Data Science: Primary Mentor Dr. Jason Moore in Bioinformatics from Dartmouth College, Co-mentor Dr. Sherman Weissman in Single-cell Genomics and Genetics from Yale University, Co-mentor Dr. Herbert Yu in Cancer Epidemiology from University of Hawaii Cancer Center and Co-mentor Dr. Jason Leigh in Big Data Visualization from the Information and Computer Science Department of University of Hawaii Manoa. I will primarily work with my four co-mentors for planning the development of my career during this award. PUBLIC HEALTH RELEVANCE: The goal of this K01 proposal is to integrate multiple types of high-throughput data, in particular, the transcriptome, exome-sequencing and CpG methylome data generated from single cancer cells. The proposed project is designed to address the urgent need for an integrative bioinformatics platform for mega-data generated from next-generation sequencing applications. It is also aimed to study the fundamental sources of tumor heterogeneity.",An Integrative Bioinformatics Approach to Study Single Cancer Cell Heterogeneity,9520067,K01ES025434,"['Acute Erythroblastic Leukemia', 'Address', 'Alternative Splicing', 'Award', 'Big Data', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biopsy', 'Cancer Center', 'Cell Line', 'Cells', 'Clinical', 'Collaborations', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Development', 'Development Plans', 'Diagnosis', 'Event', 'Evolution', 'Excision', 'Faculty', 'Failure', 'Galaxy', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genomics', 'Goals', 'Hawaii', 'Heterogeneity', 'Human', 'Individual', 'Information Sciences', 'K-562', 'K562 Cells', 'Laboratories', 'Machine Learning', 'Malignant Neoplasms', 'Mentors', 'Messenger RNA', 'Methodology', 'Methods', 'Methylation', 'Microfluidic Microchips', 'Modeling', 'Molecular', 'Noise', 'Nucleotides', 'Pattern', 'Play', 'Population', 'Process', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Role', 'Sample Size', 'Source', 'Statistical Methods', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Universities', 'Variant', 'Work', 'base', 'cancer cell', 'cancer epidemiology', 'cancer heterogeneity', 'career', 'college', 'computer science', 'computerized data processing', 'data integration', 'data visualization', 'design', 'exome', 'exome sequencing', 'experimental study', 'genome-wide', 'genomic data', 'genomic variation', 'improved', 'innovation', 'insertion/deletion mutation', 'insight', 'interest', 'learning strategy', 'leukemia', 'mRNA sequencing', 'methylome', 'neoplastic cell', 'new technology', 'next generation sequencing', 'novel', 'open source', 'outcome forecast', 'public health relevance', 'restriction enzyme', 'single cell analysis', 'single cell sequencing', 'single-cell RNA sequencing', 'success', 'tool', 'transcriptome', 'transcriptome sequencing', 'tumor', 'tumor heterogeneity', 'user-friendly']",NIEHS,UNIVERSITY OF HAWAII AT MANOA,K01,2018,24211,-0.01687097362917322
"An Integrative Bioinformatics Approach to Study Single Cancer Cell Heterogeneity DESCRIPTION:  My long term career goal is to become a leading expert in translational bioinformatics who creates, develops and applies computational and statistical methods to reveal landscapes of cancers and to identify strategies to cure cancers. Human cancers are highly heterogeneous. Such heterogeneity is the major source of the ultimate failure of most cancer agents. However, due to the limit of technologies, the intercellular heterogeneity has not been investigated genome wide, at single-cell level until recently. New technologies such as single-cell transcriptome sequencing (RNA-Seq) and exome have revealed new insights and more profound complexity than was previously thought. However, so far these technologies are limited to one assay per cell. It remains a grand challenge to perform multiple, integrative assays from the same single tumor cell, in particular, from those derived from small tumor biopsies. Given the stochasticity at the single cell resolution, reproducibility and sensitivity ar daunting tasks. To overcome this challenge, I have started the single cancer cell sequencing analysis project, in collaboration with Dr. Sherman Weissman at Yale University, who is also my co-mentor of this K01 proposal. My immediate career goal is to identify genome-wide heterogeneity among single cancer cells, using the erythroleukemia K562 cell line. Towards this, I am proposing a research project on an integrative bioinformatics approach to analyze multiple types of genomics data generated from the same single leukemia cells, a timely and critical topic. Specifically, I am interested in studying the following specific aims: (1) buildinga bioinformatics pipeline to study heterogeneity of single-cell RNA-Seq, (2) building a bioinformatics pipeline to study CpG methylome of single cells, (3) building a bioinformatics pipeline to study single-cell Exome-Seq, and (4) integrate the RNA-Seq, methylome and Exome-Seq data generated from the same single cells. These single cells genomic data are provided by Dr. Sherman Weissman's lab from 30 single K562 erythroleukemia cells. I will first construct and validate in parallel, the RNA-Seq, methylome, and Exome-Seq bioinformatics pipelines optimized for single-cell analysis, and then develop and validate an integrative platform to analyze these multiple types of high-throughput data. To accomplish the research project, and to successfully transit from a junior faculty to an expert of the field, I have developed a career plan with my mentoring committee composed of four world-class experts in different fields relevant to Big Data Science: Primary Mentor Dr. Jason Moore in Bioinformatics from Dartmouth College, Co-mentor Dr. Sherman Weissman in Single-cell Genomics and Genetics from Yale University, Co-mentor Dr. Herbert Yu in Cancer Epidemiology from University of Hawaii Cancer Center and Co-mentor Dr. Jason Leigh in Big Data Visualization from the Information and Computer Science Department of University of Hawaii Manoa. I will primarily work with my four co-mentors for planning the development of my career during this award. PUBLIC HEALTH RELEVANCE: The goal of this K01 proposal is to integrate multiple types of high-throughput data, in particular, the transcriptome, exome-sequencing and CpG methylome data generated from single cancer cells. The proposed project is designed to address the urgent need for an integrative bioinformatics platform for mega-data generated from next-generation sequencing applications. It is also aimed to study the fundamental sources of tumor heterogeneity.",An Integrative Bioinformatics Approach to Study Single Cancer Cell Heterogeneity,9309999,K01ES025434,"['Acute Erythroblastic Leukemia', 'Address', 'Alternative Splicing', 'Award', 'Big Data', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biopsy', 'Cancer Center', 'Cell Line', 'Cells', 'Clinical', 'Collaborations', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Development', 'Development Plans', 'Diagnosis', 'Event', 'Evolution', 'Excision', 'Faculty', 'Failure', 'Galaxy', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genomics', 'Goals', 'Hawaii', 'Heterogeneity', 'Human', 'Individual', 'Information Sciences', 'K-562', 'K562 Cells', 'Laboratories', 'Machine Learning', 'Malignant Neoplasms', 'Mentors', 'Messenger RNA', 'Methodology', 'Methods', 'Methylation', 'Microfluidic Microchips', 'Modeling', 'Molecular', 'Noise', 'Nucleotides', 'Pattern', 'Play', 'Population', 'Process', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Role', 'Sample Size', 'Source', 'Statistical Methods', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Universities', 'Variant', 'Work', 'base', 'cancer cell', 'cancer epidemiology', 'cancer heterogeneity', 'career', 'college', 'computer science', 'computerized data processing', 'data integration', 'data visualization', 'design', 'exome', 'exome sequencing', 'experimental study', 'genome-wide', 'genomic data', 'genomic variation', 'improved', 'innovation', 'insertion/deletion mutation', 'insight', 'interest', 'learning strategy', 'leukemia', 'methylome', 'neoplastic cell', 'new technology', 'next generation sequencing', 'novel', 'open source', 'outcome forecast', 'public health relevance', 'restriction enzyme', 'single cell analysis', 'single cell sequencing', 'success', 'tool', 'transcriptome', 'transcriptome sequencing', 'tumor', 'tumor heterogeneity', 'user-friendly']",NIEHS,UNIVERSITY OF HAWAII AT MANOA,K01,2017,180368,-0.01687097362917322
"An Integrative Bioinformatics Approach to Study Single Cancer Cell Heterogeneity DESCRIPTION:  My long term career goal is to become a leading expert in translational bioinformatics who creates, develops and applies computational and statistical methods to reveal landscapes of cancers and to identify strategies to cure cancers. Human cancers are highly heterogeneous. Such heterogeneity is the major source of the ultimate failure of most cancer agents. However, due to the limit of technologies, the intercellular heterogeneity has not been investigated genome wide, at single-cell level until recently. New technologies such as single-cell transcriptome sequencing (RNA-Seq) and exome have revealed new insights and more profound complexity than was previously thought. However, so far these technologies are limited to one assay per cell. It remains a grand challenge to perform multiple, integrative assays from the same single tumor cell, in particular, from those derived from small tumor biopsies. Given the stochasticity at the single cell resolution, reproducibility and sensitivity ar daunting tasks. To overcome this challenge, I have started the single cancer cell sequencing analysis project, in collaboration with Dr. Sherman Weissman at Yale University, who is also my co-mentor of this K01 proposal. My immediate career goal is to identify genome-wide heterogeneity among single cancer cells, using the erythroleukemia K562 cell line. Towards this, I am proposing a research project on an integrative bioinformatics approach to analyze multiple types of genomics data generated from the same single leukemia cells, a timely and critical topic. Specifically, I am interested in studying the following specific aims: (1) buildinga bioinformatics pipeline to study heterogeneity of single-cell RNA-Seq, (2) building a bioinformatics pipeline to study CpG methylome of single cells, (3) building a bioinformatics pipeline to study single-cell Exome-Seq, and (4) integrate the RNA-Seq, methylome and Exome-Seq data generated from the same single cells. These single cells genomic data are provided by Dr. Sherman Weissman's lab from 30 single K562 erythroleukemia cells. I will first construct and validate in parallel, the RNA-Seq, methylome, and Exome-Seq bioinformatics pipelines optimized for single-cell analysis, and then develop and validate an integrative platform to analyze these multiple types of high-throughput data. To accomplish the research project, and to successfully transit from a junior faculty to an expert of the field, I have developed a career plan with my mentoring committee composed of four world-class experts in different fields relevant to Big Data Science: Primary Mentor Dr. Jason Moore in Bioinformatics from Dartmouth College, Co-mentor Dr. Sherman Weissman in Single-cell Genomics and Genetics from Yale University, Co-mentor Dr. Herbert Yu in Cancer Epidemiology from University of Hawaii Cancer Center and Co-mentor Dr. Jason Leigh in Big Data Visualization from the Information and Computer Science Department of University of Hawaii Manoa. I will primarily work with my four co-mentors for planning the development of my career during this award. PUBLIC HEALTH RELEVANCE: The goal of this K01 proposal is to integrate multiple types of high-throughput data, in particular, the transcriptome, exome-sequencing and CpG methylome data generated from single cancer cells. The proposed project is designed to address the urgent need for an integrative bioinformatics platform for mega-data generated from next-generation sequencing applications. It is also aimed to study the fundamental sources of tumor heterogeneity.",An Integrative Bioinformatics Approach to Study Single Cancer Cell Heterogeneity,9095313,K01ES025434,"['Acute Erythroblastic Leukemia', 'Address', 'Alternative Splicing', 'Award', 'Big Data', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biopsy', 'Cancer Center', 'Cell Line', 'Cells', 'Clinical', 'Collaborations', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Development', 'Development Plans', 'Diagnosis', 'Event', 'Evolution', 'Excision', 'Faculty', 'Failure', 'Galaxy', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genomics', 'Goals', 'Hawaii', 'Health', 'Heterogeneity', 'Human', 'Individual', 'Information Sciences', 'K-562', 'K562 Cells', 'Laboratories', 'Machine Learning', 'Malignant Neoplasms', 'Mentors', 'Messenger RNA', 'Methodology', 'Methods', 'Methylation', 'Microfluidic Microchips', 'Modeling', 'Molecular', 'Noise', 'Nucleotides', 'Pattern', 'Play', 'Population', 'Process', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Role', 'Sample Size', 'Sequence Analysis', 'Source', 'Statistical Methods', 'Technology', 'Testing', 'Therapeutic', 'Tissues', 'Universities', 'Variant', 'Work', 'base', 'cancer cell', 'cancer epidemiology', 'cancer heterogeneity', 'career', 'college', 'computer science', 'computerized data processing', 'data integration', 'data visualization', 'design', 'exome', 'exome sequencing', 'genome-wide', 'genomic data', 'genomic variation', 'improved', 'innovation', 'insertion/deletion mutation', 'insight', 'interest', 'learning strategy', 'leukemia', 'methylome', 'neoplastic cell', 'new technology', 'next generation sequencing', 'novel', 'open source', 'outcome forecast', 'research study', 'restriction enzyme', 'single cell analysis', 'single cell sequencing', 'success', 'tool', 'transcriptome', 'transcriptome sequencing', 'tumor', 'tumor heterogeneity', 'user-friendly']",NIEHS,UNIVERSITY OF HAWAII AT MANOA,K01,2016,180368,-0.01687097362917322
"An Integrative Bioinformatics Approach to Study Single Cancer Cell Heterogeneity DESCRIPTION:  My long term career goal is to become a leading expert in translational bioinformatics who creates, develops and applies computational and statistical methods to reveal landscapes of cancers and to identify strategies to cure cancers. Human cancers are highly heterogeneous. Such heterogeneity is the major source of the ultimate failure of most cancer agents. However, due to the limit of technologies, the intercellular heterogeneity has not been investigated genome wide, at single-cell level until recently. New technologies such as single-cell transcriptome sequencing (RNA-Seq) and exome have revealed new insights and more profound complexity than was previously thought. However, so far these technologies are limited to one assay per cell. It remains a grand challenge to perform multiple, integrative assays from the same single tumor cell, in particular, from those derived from small tumor biopsies. Given the stochasticity at the single cell resolution, reproducibility and sensitivity ar daunting tasks. To overcome this challenge, I have started the single cancer cell sequencing analysis project, in collaboration with Dr. Sherman Weissman at Yale University, who is also my co-mentor of this K01 proposal. My immediate career goal is to identify genome-wide heterogeneity among single cancer cells, using the erythroleukemia K562 cell line. Towards this, I am proposing a research project on an integrative bioinformatics approach to analyze multiple types of genomics data generated from the same single leukemia cells, a timely and critical topic. Specifically, I am interested in studying the following specific aims: (1) buildinga bioinformatics pipeline to study heterogeneity of single-cell RNA-Seq, (2) building a bioinformatics pipeline to study CpG methylome of single cells, (3) building a bioinformatics pipeline to study single-cell Exome-Seq, and (4) integrate the RNA-Seq, methylome and Exome-Seq data generated from the same single cells. These single cells genomic data are provided by Dr. Sherman Weissman's lab from 30 single K562 erythroleukemia cells. I will first construct and validate in parallel, the RNA-Seq, methylome, and Exome-Seq bioinformatics pipelines optimized for single-cell analysis, and then develop and validate an integrative platform to analyze these multiple types of high-throughput data. To accomplish the research project, and to successfully transit from a junior faculty to an expert of the field, I have developed a career plan with my mentoring committee composed of four world-class experts in different fields relevant to Big Data Science: Primary Mentor Dr. Jason Moore in Bioinformatics from Dartmouth College, Co-mentor Dr. Sherman Weissman in Single-cell Genomics and Genetics from Yale University, Co-mentor Dr. Herbert Yu in Cancer Epidemiology from University of Hawaii Cancer Center and Co-mentor Dr. Jason Leigh in Big Data Visualization from the Information and Computer Science Department of University of Hawaii Manoa. I will primarily work with my four co-mentors for planning the development of my career during this award. PUBLIC HEALTH RELEVANCE: The goal of this K01 proposal is to integrate multiple types of high-throughput data, in particular, the transcriptome, exome-sequencing and CpG methylome data generated from single cancer cells. The proposed project is designed to address the urgent need for an integrative bioinformatics platform for mega-data generated from next-generation sequencing applications. It is also aimed to study the fundamental sources of tumor heterogeneity.",An Integrative Bioinformatics Approach to Study Single Cancer Cell Heterogeneity,8935815,K01ES025434,"['Acute Erythroblastic Leukemia', 'Address', 'Alternative Splicing', 'Award', 'Big Data', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biopsy', 'Cancer Center', 'Cell Line', 'Cells', 'Clinical', 'Collaborations', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Development Plans', 'Diagnosis', 'Event', 'Evolution', 'Excision', 'Faculty', 'Failure', 'Galaxy', 'Gene Expression Profile', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genomics', 'Goals', 'Hawaii', 'Health', 'Heterogeneity', 'Human', 'Individual', 'Information Sciences', 'K-562', 'K562 Cells', 'Laboratories', 'Machine Learning', 'Malignant Neoplasms', 'Mentors', 'Messenger RNA', 'Methodology', 'Methods', 'Methylation', 'Microfluidic Microchips', 'Modeling', 'Molecular', 'Noise', 'Nucleotides', 'Pattern', 'Play', 'Population', 'Process', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Role', 'Sample Size', 'Science', 'Sequence Analysis', 'Source', 'Statistical Methods', 'Technology', 'Testing', 'Therapeutic', 'Tissues', 'Universities', 'Variant', 'Work', 'base', 'cancer cell', 'cancer epidemiology', 'career', 'college', 'computer science', 'computerized data processing', 'data integration', 'data visualization', 'design', 'exome', 'exome sequencing', 'genome-wide', 'genomic variation', 'improved', 'innovation', 'insertion/deletion mutation', 'insight', 'interest', 'leukemia', 'methylome', 'neoplastic cell', 'new technology', 'next generation sequencing', 'novel', 'open source', 'outcome forecast', 'research study', 'restriction enzyme', 'single cell analysis', 'single cell sequencing', 'success', 'tool', 'transcriptome sequencing', 'tumor', 'user-friendly']",NIEHS,UNIVERSITY OF HAWAII AT MANOA,K01,2015,180368,-0.01687097362917322
"An Integrative Bioinformatics Approach to Study Single Cancer Cell Heterogeneity     DESCRIPTION:  My long term career goal is to become a leading expert in translational bioinformatics who creates, develops and applies computational and statistical methods to reveal landscapes of cancers and to identify strategies to cure cancers. Human cancers are highly heterogeneous. Such heterogeneity is the major source of the ultimate failure of most cancer agents. However, due to the limit of technologies, the intercellular heterogeneity has not been investigated genome wide, at single-cell level until recently. New technologies such as single-cell transcriptome sequencing (RNA-Seq) and exome have revealed new insights and more profound complexity than was previously thought. However, so far these technologies are limited to one assay per cell. It remains a grand challenge to perform multiple, integrative assays from the same single tumor cell, in particular, from those derived from small tumor biopsies. Given the stochasticity at the single cell resolution, reproducibility and sensitivity ar daunting tasks. To overcome this challenge, I have started the single cancer cell sequencing analysis project, in collaboration with Dr. Sherman Weissman at Yale University, who is also my co-mentor of this K01 proposal. My immediate career goal is to identify genome-wide heterogeneity among single cancer cells, using the erythroleukemia K562 cell line. Towards this, I am proposing a research project on an integrative bioinformatics approach to analyze multiple types of genomics data generated from the same single leukemia cells, a timely and critical topic. Specifically, I am interested in studying the following specific aims: (1) buildinga bioinformatics pipeline to study heterogeneity of single-cell RNA-Seq, (2) building a bioinformatics pipeline to study CpG methylome of single cells, (3) building a bioinformatics pipeline to study single-cell Exome-Seq, and (4) integrate the RNA-Seq, methylome and Exome-Seq data generated from the same single cells. These single cells genomic data are provided by Dr. Sherman Weissman's lab from 30 single K562 erythroleukemia cells. I will first construct and validate in parallel, the RNA-Seq, methylome, and Exome-Seq bioinformatics pipelines optimized for single-cell analysis, and then develop and validate an integrative platform to analyze these multiple types of high-throughput data. To accomplish the research project, and to successfully transit from a junior faculty to an expert of the field, I have developed a career plan with my mentoring committee composed of four world-class experts in different fields relevant to Big Data Science: Primary Mentor Dr. Jason Moore in Bioinformatics from Dartmouth College, Co-mentor Dr. Sherman Weissman in Single-cell Genomics and Genetics from Yale University, Co-mentor Dr. Herbert Yu in Cancer Epidemiology from University of Hawaii Cancer Center and Co-mentor Dr. Jason Leigh in Big Data Visualization from the Information and Computer Science Department of University of Hawaii Manoa. I will primarily work with my four co-mentors for planning the development of my career during this award.         PUBLIC HEALTH RELEVANCE: The goal of this K01 proposal is to integrate multiple types of high-throughput data, in particular, the transcriptome, exome-sequencing and CpG methylome data generated from single cancer cells. The proposed project is designed to address the urgent need for an integrative bioinformatics platform for mega-data generated from next-generation sequencing applications. It is also aimed to study the fundamental sources of tumor heterogeneity.            ",An Integrative Bioinformatics Approach to Study Single Cancer Cell Heterogeneity,8830386,K01ES025434,"['Acute Erythroblastic Leukemia', 'Address', 'Alternative Splicing', 'Award', 'Big Data', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biopsy', 'Cancer Center', 'Cell Line', 'Cells', 'Clinical', 'Collaborations', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Development Plans', 'Diagnosis', 'Event', 'Evolution', 'Excision', 'Faculty', 'Failure', 'Galaxy', 'Gene Expression Profile', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genome', 'Genomics', 'Goals', 'Hawaii', 'Heterogeneity', 'Human', 'Imagery', 'Individual', 'Information Sciences', 'K-562', 'K562 Cells', 'Laboratories', 'Machine Learning', 'Malignant Neoplasms', 'Mentors', 'Messenger RNA', 'Methodology', 'Methods', 'Methylation', 'Microfluidic Microchips', 'Modeling', 'Molecular', 'Noise', 'Nucleotides', 'Pattern', 'Play', 'Population', 'Process', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Role', 'Sample Size', 'Science', 'Sequence Analysis', 'Source', 'Statistical Methods', 'Technology', 'Testing', 'Therapeutic', 'Tissues', 'Universities', 'Variant', 'Work', 'base', 'cancer cell', 'cancer epidemiology', 'career', 'college', 'computer science', 'computerized data processing', 'data integration', 'design', 'exome', 'exome sequencing', 'genome-wide', 'improved', 'innovation', 'insertion/deletion mutation', 'insight', 'interest', 'leukemia', 'methylome', 'neoplastic cell', 'new technology', 'next generation sequencing', 'novel', 'open source', 'outcome forecast', 'public health relevance', 'research study', 'restriction enzyme', 'single cell analysis', 'success', 'tool', 'transcriptome sequencing', 'tumor', 'user-friendly']",NIEHS,UNIVERSITY OF HAWAII AT MANOA,K01,2014,183608,-0.01687097362917322
"2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics. Discussing New Research in Biomedical Imaging and Bioengineering. FOA: PA-16-294 Opportunity Title: NIH Support for Conferences and Scientific Meetings (R13/U13) Agency: NIH - NIBIB Proposal Title: 2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and  Microscopy, Histopathology and Analytics. Discussing New Research in  Biomedical Imaging and Bioengineering Principal Investigator: Gregory J. Quarles, Ph.D., Chief Scientist, The Optical Society  2010 Massachusetts Ave, NW, Washington, DC  gquarles@osa.org, 202-416-1954 Project Summary /Abstract:  The 2018 OSA Biophotonics Congress: Biomedical Optics, 3-6 April 2018, Hollywood, FL, consists of four topical meetings. Two of these meetings, Optical Tomography and Spectroscopy (OTS) and Microscopy, Histopathology and Analytics (Microscopy) provide broad exposure to a very active multidisciplinary field in biomedical imaging and bioengineering focused on illness treatment and health enhancement. The interdisciplinary nature of the co- located meetings will provide cross-fertilization of concepts and techniques between fields with the resulting synergies obtained from such interactions. This proposal is to provide registration and travel support for students and early career professionals presenting at one of these topical meetings.  OTS will focus on new developments in diffuse optics, spectroscopy and other non- invasive tomographic imaging approaches, including the fields of diffuse optical tomography (DOT), photoacoustic tomography (PAT), optical coherence tomography (OCT), wavefront engineering to overcome scattering, as well as new developments in spectroscopic technologies.  Microscopy will include topics central to the development of optical microscopy and in vitro optical sensing for the clinic. Areas such as novel optical approaches, including computational optics, new image processing and segmentation techniques, development of decision-assistance algorithms via machine-learning and other strategies, testing technologies in pre-clinical models, applications to clinical samples, and validation in the clinic will be discussed. Optically enabled microfluidics are included in this track as well. The goal of these efforts should be towards clinical translation.  The general purpose of these meetings is to create an inclusive, open forum for the presentation of high-quality scientific research through plenary and technical sessions, short courses, panels, networking and special events. This method of face-to-face information sharing allows researchers to learn what others in their field and related disciplines are doing and to efficiently learn about new research, tools, and techniques that might be relevant to their work. It allows conversations with colleagues from different institutions around the world and engenders far reaching scientific collaborations – both domestic and international. FOA: PA-16-294 Opportunity Title: NIH Support for Conferences and Scientific Meetings (R13/U13) Agency: NIH - NIBIB Proposal Title: 2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and  Microscopy, Histopathology and Analytics. Discussing New Research in  Biomedical Imaging and Bioengineering. Principal Investigator: Gregory J. Quarles, Ph.D., Chief Scientist, The Optical Society  2010 Massachusetts Ave, NW, Washington, DC  gquarles@osa.org, 202-416-1954 Project Narrative The 2018 OSA Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics Topical Meetings will discuss important, highly interdisciplinary areas that focus on technological solutions to medical challenges and medical applications, and will cover a diversity of cutting-edge research and innovative new tools and techniques, especially in biomedical imaging and bioengineering. These Topical Meetings will bring together researchers working in all aspects of this field and will serve as a forum for discussion of existing and emerging techniques as well as future directions.","2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics. Discussing New Research in Biomedical Imaging and Bioengineering.",9543795,R13EB026325,"['Academic Training', 'Algorithms', 'Area', 'Biomedical Engineering', 'Biophotonics', 'Birds', 'Career Mobility', 'Clinic', 'Clinical', 'Collaborations', 'Congresses', 'Development', 'Diffuse', 'Digital Libraries', 'Discipline', 'Doctor of Philosophy', 'Engineering', 'Ensure', 'Event', 'Exhibits', 'Exposure to', 'Fertilization', 'Fostering', 'Future', 'Goals', 'Grant', 'Health', 'Hearing', 'Histopathology', 'In Vitro', 'Industry', 'Institution', 'International', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Massachusetts', 'Medical', 'Methods', 'Microfluidics', 'Microscopy', 'National Institute of Biomedical Imaging and Bioengineering', 'Nature', 'Optical Coherence Tomography', 'Optical Tomography', 'Optics', 'Outcome', 'Paper', 'Participant', 'Peer Review', 'Physicians', 'Pre-Clinical Model', 'Principal Investigator', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scientist', 'Services', 'Societies', 'Special Event', 'Spectrum Analysis', 'Students', 'Techniques', 'Technology', 'Testing', 'Time', 'Translating', 'Travel', 'Underrepresented Minority', 'United States National Institutes of Health', 'Validation', 'Washington', 'Work', 'academic standard', 'base', 'bioimaging', 'career', 'clinical application', 'clinical translation', 'diffuse optical tomography', 'graduate student', 'image processing', 'imaging Segmentation', 'imaging approach', 'indexing', 'innovation', 'meetings', 'multidisciplinary', 'novel', 'optoacoustic tomography', 'posters', 'programs', 'symposium', 'synergism', 'technique development', 'tomography', 'tool']",NIBIB,OPTICAL SOCIETY OF AMERICA,R13,2018,10000,-0.007242284468410177
"Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics Abstract Support is requested for a Keystone Symposia conference entitled Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics, organized by Drs. Jose M. Lora and Timothy K. Lu. The conference will be held in Breckenridge, Colorado from March 29- April 1, 2019. Synthetic Biology tools and principles have matured tremendously over the last decade and have reached extraordinary levels of sophistication, both in eukaryotic and prokaryotic systems. Synthetic biology as a therapeutic modality is starting to enter multiple clinical studies and has the potential to have a significant impact on medicine across a wide range of diseases (e.g., metabolic, immune-mediated, cancer, and neurologic diseases). This Keystone Symposia conference will delve into the field of synthetic biology with a special emphasis on its applications to medicine. While there are conferences that capture synthetic biology in only a few talks mixed in among other various topics, there is a paucity of conferences focused on synthetic biology as drugs to treat disease. However, due to the rapid pace of fundamental scientific advances along with an expanding number of biotechnology companies and emerging clinical studies with synthetic biology at their core, this conference will be highly relevant for a wide audience of scientists both from academia and industry. In addition, other meetings in this field have a highly technology-driven focus on synthetic biology techniques with relatively little attention given to biological and medical context. Ultimately, this Keystone Symposia conference should inspire researchers from diverse backgrounds to discuss synthetic biology via many new angles. PROJECT NARRATIVE Over the past two decades, tremendous advances have been made in the use of biological parts to engineer systems that can effectively direct living cells for a vast variety of purposes (a.k.a. synthetic biology). Synthetic biology is being used to construct more effective therapies in diseases such as cancer, but there are remaining obstacles to the clinical translation of these therapies. This Keystone Symposia conference will delve into the field of synthetic biology with a special emphasis on its applications to medicine.",Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics,9913772,R13EB029305,"['Academia', 'Address', 'Area', 'Attention', 'Biological', 'Biomedical Research', 'Biotechnology', 'Cells', 'Clinical Research', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collaborations', 'Colorado', 'Computers', 'Disease', 'Educational workshop', 'Engineering', 'Future', 'Genetic Engineering', 'Genetic Screening', 'Human', 'Immune', 'Industrialization', 'Industry', 'Knowledge', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Medical', 'Medicine', 'Metabolic', 'Methodology', 'Modality', 'Neurologic', 'Outcome', 'Participant', 'Pharmaceutical Preparations', 'Postdoctoral Fellow', 'Preventive', 'Process', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Scientific Advances and Accomplishments', 'Scientist', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Work', 'clinical application', 'clinical practice', 'clinical translation', 'combinatorial', 'design', 'effective therapy', 'graduate student', 'meetings', 'nervous system disorder', 'next generation', 'novel diagnostics', 'posters', 'symposium', 'synthetic biology', 'targeted treatment', 'tool']",NIBIB,KEYSTONE SYMPOSIA,R13,2020,10000,-0.021262278196075846
"Continued Maintenance and Development of Software: Integrated Genome Browser and    DESCRIPTION (provided by applicant): Biomedical research increasingly involves examination of huge data sets. In recent years, introduction of new high-throughput sequencing technologies have led to production of new genome sequence for dozens of plants and animals and prompted major efforts to sequence individual human genomes. To take full advantage of these new large-scale data sets, researchers require flexible, user-friendly software that supports interactive, in-depth exploration of data at different levels of detail, ranging from chromosomes to individual base pairs. The Integrated Genome Browser (IGB), used by more than 3,500 scientists worldwide, implements innovative yet practical visualization techniques designed to help scientists explore genome-scale data sets, ultimately leading to better treatments for disease as scientists use IGB to achieve deeper insight into genes and genomes. This project will continue to develop the IGB software, adding new data integration and display features users have requested and which the IGB developers foresee will become increasingly important in the face of on-going and rapid technological change in genomics. Second, the project will implement improvements designed to enhance the productivity of IGB power users, researchers who typically spend many hours per day using the software to explore data sets they or their collaborators have created or obtained from public repositories. These improvements include: enhancing user's ability to invoke IGB from command line ""scripts,"" supporting IGB interoperability with external analysis tools, saving and restoring sessions, and improving techniques for invoking IGB from Web pages via IGB links. The project will also continue development and maintenance of the Genoviz Software Development Kit (SDK), the library of software components that underlies the advanced visualization capabilities such as animated zooming that IGB users most appreciate in the software. By continuing to maintain and develop the Genoviz SDK, the project will make IGB a more robust and flexible software application while also giving developers powerful new tools for building advanced visualization software for scientists.      PUBLIC HEALTH RELEVANCE: Methods for generating genome-scale data sets are becoming cheaper and more accessible. We are rapidly approaching a time when public health professionals will be able to survey the genomes of large numbers of people and relate genotype with disease susceptibility, overall health, and outcomes from specific treatments. However, to take advantage of these data, scientists need good visualization software that makes exploring and analyzing the data easier to accomplish. This project will develop user friendly yet innovative visualization and data sharing software that will accelerate the pace of discovery in the biosciences and ultimately lead to critical discoveries about the human genes and genomes and the roles they play in disease processes and maintaining human health.                          Methods for generating genome-scale data sets are becoming cheaper and more accessible. We are rapidly approaching a time when public health professionals will be able to survey the genomes of large numbers of people and relate genotype with disease susceptibility, overall health, and outcomes from specific treatments. However, to take advantage of these data, scientists need good visualization software that makes exploring and analyzing the data easier to accomplish. This project will develop user friendly yet innovative visualization and data sharing software that will accelerate the pace of discovery in the biosciences and ultimately lead to critical discoveries about the human genes and genomes and the roles they play in disease processes and maintaining human health.                        ",Continued Maintenance and Development of Software: Integrated Genome Browser and,8077112,R01RR032048,"['Animals', 'Base Pairing', 'Bioinformatics', 'Biomedical Research', 'Chromosomes', 'Code', 'Communities', 'Computer software', 'Consult', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Disease susceptibility', 'Electronic Mail', 'Ensure', 'Environment', 'Flying body movement', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Graph', 'Health', 'Health Professional', 'Hour', 'Human', 'Human Genome', 'Image', 'Imagery', 'Individual', 'Institution', 'Internet', 'Java', 'Language', 'Lead', 'Libraries', 'Link', 'Maintenance', 'Methods', 'Outcome', 'Plants', 'Play', 'Process', 'Production', 'Productivity', 'Public Health', 'Publishing', 'Pythons', 'Reading', 'Recovery', 'Research Personnel', 'Role', 'Scientist', 'Software Tools', 'Source', 'Specialist', 'Students', 'Surveys', 'Techniques', 'Technology', 'Time', 'Training', 'Update', 'Visualization software', 'base', 'cluster computing', 'computing resources', 'data integration', 'data sharing', 'design', 'flexibility', 'genome sequencing', 'impression', 'improved', 'innovation', 'insight', 'interest', 'interoperability', 'microbial alkaline proteinase inhibitor', 'open source', 'outreach', 'repository', 'research study', 'software development', 'task analysis', 'text searching', 'tool', 'user friendly software', 'user-friendly', 'web page', 'web services', 'wiki']",NCRR,UNIVERSITY OF NORTH CAROLINA CHARLOTTE,R01,2011,289807,0.11472383006480366
"COMPUTERIZED ELECTROCARDIOLOGY Funds are requested to support publication of the Proceedings of the 1989 Conference on Computerized Interpretation of the Electrocardiogram in the Journal of Electrocardiology.  This will be the 14th Annual Conference which in past years has been co- sponsored by the Engineering Foundation and ISCE.  The topic area is particularly important as it is estimated that about 51 million electrocardiograms processed annually by computer in the United States, and that there are over 15,000 devices in the field with analysis capabilities.  The overall objective of this Conference is to exchange the most recent information and advances in computer analyses in electrocardiology among biomedical engineers, computer scientists, electrophysiologists, clinical researchers, and epidemiologists. Most of the participants come from academic organizations and public health agencies; many are scientists and engineers from companies with a heavy commitment to research and development activities in these areas.  The multi-disciplinary nature of the speakers will allow in depth exploration and discussion of technical/ scientific issues such as mapping endocardial, epicardial, and body surface potential distributions, sampling and data compression methods, and high resolution, low level ECG recordings.  Eight sessions are planned over a 4 day period.  The philosophy of these Conferences, like Gordon Conferences, is to maximize opportunities for participants to discuss issues on the frontiers of electrocardiology.  Formal presentations with considerable time for discussion are held mornings and evenings. Afternoons are available for free discussion in small study groups or workshops; poster sessions are also held in the afternoons.  The scientific areas addressed ideally match the interests of the clinical electrocardiologist, electrophysiologist, and bioengineer readership of the Journal of Electrocardiology (1500 subscribers).  n/a",COMPUTERIZED ELECTROCARDIOLOGY,3435660,R13HL042521,"['United States', ' artificial intelligence', ' computer assisted medical decision making', ' electrocardiography', ' health science research', ' mathematical model', ' travel']",NHLBI,LOS ANGELES COUNTY HARBOR-UCLA MED CTR,R13,1989,14404,0.03041504181284933
"TACTILE COMPUTER DISPLAY Knowledge representation in our information age is not in a useful display for the visually impaired.  The blind person is especially disadvantaged when pursuing a high level skill, and is required to learn from an ever- changing, expanding, dynamic technology driven culture.  The blind scientist cannot scan large amounts of current published articles.  A sighted person must read and describe the information.  The goal of this research is to find a combination of piezoelectric materials and surface mount electronics that will, in a unique geometry, produce a reliable Large Tactile Computer Controlled Array. The tactile pin density must meet the National Library of Congress specification for Braille.  An Array size of 25 lines x 40 characters/ line, 6 pins/character, could replace Braille books.  A single prime mover will provide the up motion for all pins. A piezoelectric latch, each pin controlled by the computer, holds the pin up or releases the pin to follow the prime mover down.  A Modular geometry combines the computer interface piezoelectric driver surface mount components with each tactile pin latch.  The volume needed is included in a surface size of 10 x 13 inches, with a depth of several inches.  n/a",TACTILE COMPUTER DISPLAY,3503502,R43MH051972,"['blind aid', ' computer human interaction', ' computer system design /evaluation', ' electrical property', ' electrotactile communication', ' information display', ' information dissemination']",NIMH,"PIEZO SYSTEMS, INC.",R43,1993,50000,-0.0026531893887921923
"AIDS FOR GEOGRAPHIC PATTERN RECOGNITION IN CANCER DATA The primary goal of this research is to develop anew software tool to help epidemiologists, biostatisticians, and other scientists recognized patterns in multi-dimensional spatial data. The focus of the Phase I project was on the exploration of geographic patterns in cancer data, including specifically correlations between different types of cancer and correlations with environmental variables. Major accomplishments in Phase I were to identify key requirements for effective visualization and analysis of spatially-related disease data, and to prototype and approach and a set of techniques that address these requirements. The project used object-oriented methodologies to create a customizable software framework. The proposed Phase II work will build on this base to develop a full software system that can be evaluated and refined through collaboration with scientists at several ""beta test"" sites.  The major technical innovation in this project is the development of new object-oriented software to explore geographically-linked statistical data. The major health-related contribution is an enhanced ability to find clues to etiology through better tools for detecting patterns in the geographic distribution of disease incidence and correlating them with environmental, demographic, and other factors.  n/a",AIDS FOR GEOGRAPHIC PATTERN RECOGNITION IN CANCER DATA,2098807,R44CA058117,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' geographic site', ' human data', ' neoplasm /cancer epidemiology']",NCI,"BELMONT RESEARCH, INC.",R44,1994,250000,0.0027533795551974833
"NEURAL NETWORK ANALYSIS OF FLOW CYTOMETRIC DATA Multidimensional flow cytometry allows the discrimination of neoplastic cells from their normal counterparts.  Using technology for the simultaneous measurement of 5 parameters, the abnormal expression of normal antigens can be used to identify leukemic populations. Distinction between normal and abnormal cells can be made by a highly trained scientist using a computer program which permits visualization of multidimensional data.  This proposal will demonstrate the utility of artificial neural network technology to reproduce an identification made by a trained scientist.  We will focus on two forms of acute leukemia, acute lymphoblastic leukemia (ALL) and acute myeloblastic leukemia (AML) as test specimens to be compared with normal bone marrow specimens. Using this technology we will characterize acute leukemias and report their relative positions in a multidimensional data space.  We will also investigate whether the neural network can extract more information from the data set than the expert scientist.  n/a",NEURAL NETWORK ANALYSIS OF FLOW CYTOMETRIC DATA,3493230,R43CA058147,"['acute lymphocytic leukemia', ' acute myelogenous leukemia', ' artificial intelligence', ' bone marrow', ' cell population study', ' computer assisted diagnosis', ' computer data analysis', ' computer system design /evaluation', ' flow cytometry', ' fluorescent dye /probe', ' human tissue', ' monoclonal antibody', ' neoplasm /cancer classification /staging', ' neoplastic cell']",NCI,"CYTOMETRY ASSOCIATES, INC.",R43,1993,47200,0.046571365600609406
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6646557,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2003,237000,0.045491273028123724
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6526274,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2002,462420,0.045491273028123724
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6401728,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2001,237000,0.045491273028123724
"Systems Biology of Aging: Data-science meets Gero-science PROJECT SUMMARY / ABSTRACT The funds requested in this R13 application are for partial support of “Systems Biology of Aging: Data-science meets Gero-science” annual meetings to be offered each August/September from 2019 through 2022 at The Jackson Laboratory for Genomic Medicine (JAX-GM) in Farmington, Connecticut. This meeting will bring together up to 150 interdisciplinary scientists including molecular biologists, immunologists, computational biologists, and geriatricians, who share a common interest in understanding aging and aging-associated disease at the systems level. Many aging-associated diseases, such as cancer and cardiovascular disease, are influenced by dysfunctions in the immune system. Recent advances in genomic profiling techniques (e.g., single cell transcriptomics) provide an opportunity to uncover aging-related changes in human cells/tissues and to link these changes to health and lifespan. The wealth and complexity of data produced using these technologies is ever increasing, as is the need to develop advanced computational methods to mine and integrate these data. Despite this need, there are currently no formal venues at which scientists, specifically those in the aging field, can be trained in the basics and application of data mining techniques (i.e., machine learning algorithms). Furthermore, current conferences on aging are not aimed at specifically bringing together computational biologists, immunologists and basic and clinical aging researchers. Therefore, the objectives of this meeting are: (1) to recognize and emphasize the highly interdisciplinary nature of the aging field and to promote and accelerate collaborations and cross-pollination of ideas across the three disciplines: aging, immunology, and computational biology; (2) to provide trainees (students and postdoctoral fellows) an opportunity to closely interact with, and gain feedback from, more senior investigators to advance their projects and establish connections to help build their careers; and (3) to provide an opportunity for researchers in the field of aging to learn the basics of machine learning techniques, which they will be able to immediately apply to their own research upon return to their home institutions. We will reach these objectives through carrying out the following Aims. In Aim 1, we will organize an interdisciplinary meeting and hands-on workshop focused on aging and aging-related diseases. The meeting will include a 2-day seminar session featuring talks by leading scientists, followed by a 1-day hands-on workshop on the basics of machine learning. In Aim 2, we will promote interactions to foster collaborative research and career advancement, including through a poster session. In Aim 3, we will recruit diverse attendees. Our proposed speaker list features several female scientists, and we will use our partnership networks to specifically recruit attendees from nationally underrepresented racial and ethnic groups. The ultimate goal of the meeting is to advance the aging research field through expediting collaborations and the understanding of aging-related genomic data via application of advanced data mining approaches. PROJECT NARRATIVE / RELEVANCE TO PUBLIC HEALTH Aging and aging-associated diseases, such as Alzheimer's, cancer and cardiovascular disease, represent a significant and growing health and economic burden, with the elderly population of the US projected to double by 2030. Herein, we propose to organize an interdisciplinary conference with a hands-on computational training component that will bring together scientists from the fields of aging, immunology, and computational biology, which will enable creative collaborations and train early career scientists in the aging research field on the basics of advanced computational techniques to mine aging-related genomic data. This is ultimately expected to lead to a better molecular understanding of the aging process and to novel approaches for the improvement of human healthspan and/or lifespan.",Systems Biology of Aging: Data-science meets Gero-science,9912317,R13AG064968,"['Academia', 'Address', 'Affect', 'Aging', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Biology of Aging', 'Cardiovascular Diseases', 'Career Mobility', 'Cell physiology', 'Cells', 'Cities', 'Clinical', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Technique', 'Computing Methodologies', 'Connecticut', 'Data', 'Data Analyses', 'Data Science', 'Data Scientist', 'Development', 'Discipline', 'Disease', 'Economic Burden', 'Educational workshop', 'Elderly', 'Ethnic group', 'Etiology', 'Feedback', 'Female', 'Fostering', 'Functional disorder', 'Funding', 'Genomic medicine', 'Genomics', 'Geroscience', 'Goals', 'Health', 'Home environment', 'Human', 'Immune', 'Immune system', 'Immunologist', 'Immunology', 'Impaired cognition', 'Industry', 'Institution', 'Lead', 'Learning', 'Link', 'Location', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Molecular', 'Mus', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Organism', 'Outcome', 'Participant', 'Phenotype', 'Play', 'Population', 'Postdoctoral Fellow', 'Process', 'Public Health', 'Pythons', 'Race', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scholarship', 'Science', 'Scientist', 'Series', 'Shock', 'Societies', 'Students', 'Support System', 'System', 'Systems Biology', 'Techniques', 'Technology', 'The Jackson Laboratory', 'Time', 'Tissues', 'Training', 'Underrepresented Minority', 'Universities', 'Work', 'aging population', 'cancer type', 'career', 'clinical biomarkers', 'clinically significant', 'data mining', 'epigenome', 'epigenomics', 'genomic biomarker', 'genomic data', 'genomic profiles', 'graduate student', 'health economics', 'healthspan', 'innovation', 'interdisciplinary approach', 'interest', 'machine learning algorithm', 'meetings', 'next generation', 'novel strategies', 'posters', 'programs', 'recruit', 'response', 'senescence', 'skills', 'symposium', 'technology development', 'transcriptome', 'transcriptomics', 'translational approach']",NIA,JACKSON LABORATORY,R13,2019,38566,-0.0009748875352371206
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6929696,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2005,206378,0.08058887302114025
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6783420,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2004,200515,0.08058887302114025
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6658916,R33GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2003,200824,0.08058887302114025
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6549345,R21GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R21,2002,99510,0.08058887302114025
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6931476,P20GM067650,"['animal tissue', 'artificial intelligence', 'bioinformatics', 'computational biology', 'computer data analysis', 'computer human interaction', 'computer simulation', 'computer system design /evaluation', 'data management', 'disease /disorder etiology', 'epidemiology', 'functional /structural genomics', 'gene expression', 'human subject', 'interdisciplinary collaboration', 'mathematical model', 'pharmacokinetics', 'science education', 'statistics /biometry', 'technology /technique development', 'therapy', 'training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2005,392500,0.005104246303071567
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6787778,P20GM067650,"['animal tissue', 'artificial intelligence', 'bioinformatics', 'computational biology', 'computer data analysis', 'computer human interaction', 'computer simulation', 'computer system design /evaluation', 'data management', 'disease /disorder etiology', 'epidemiology', 'functional /structural genomics', 'gene expression', 'human subject', 'interdisciplinary collaboration', 'mathematical model', 'pharmacokinetics', 'science education', 'statistics /biometry', 'technology /technique development', 'therapy', 'training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2004,392500,0.005104246303071567
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6690235,P20GM067650,"['animal tissue', ' artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer simulation', ' computer system design /evaluation', ' data management', ' disease /disorder etiology', ' epidemiology', ' functional /structural genomics', ' gene expression', ' human subject', ' informatics', ' interdisciplinary collaboration', ' mathematical model', ' pharmacokinetics', ' science education', ' statistics /biometry', ' technology /technique development', ' therapy', ' training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2003,392500,0.005104246303071567
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7426246,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Depth', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Facility Construction Funding Category', 'Funding', 'GDF15 gene', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Numbers', 'Ontology', 'PLAB Protein', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2008,266213,0.02349391887472534
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,8055527,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Traumatic Stress Disorders', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,248610,0.02349391887472534
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7799875,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,250650,0.02349391887472534
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7591237,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,272818,0.02349391887472534
"Statistical Models and Inference in the Single-Molecule Approach to Biology    DESCRIPTION (provided by applicant): The rapid advance of nanotechnology has generated much excitement in the scientific and engineering community. Its application to the biological front created the new area of single-molecule biology: Scientists were able to investigate biological processes on a molecule-by-molecule basis, opening the door to addressing many problems that were inaccessible just a few decades ago. The new frontier also raises many statistical challenges, calling upon an urgent need for new statistical inference tools and new stochastic models because of the stochastic nature of single-molecule experiments and because many classical models derived from oversimplified assumptions are no longer valid for single-molecule experiments. The current proposal focuses on the statistical challenges in the single-molecule approach to biology. The proposed research consists of three projects: (1) Using stochastic networks to model enzymatic reaction kinetics. The goal is to provide models not only biologically meaningful, but also capable of explaining the recent single-molecule experimental discoveries that contradict the classical Michaelis-Menten model. (2) Using the kernel method to infer biochemical properties from doubly stochastic Poisson process data, in particular, photon arrival data from single-molecule experiments. The goal is to develop nonparametric inference tools to recover the dependence structure, such as the autocorrelation function, from the doubly stochastic Poisson data. (3) Using bootstrap moment estimates to infer the helical diffusion of DNA-binding proteins. The goal is to elucidate how DNA-binding proteins interact with DNA, and estimate the associated energy landscape. The proposed research aims to provide essential statistical models and inference tools to study biological processes at the single-molecule level and to advance significantly our understanding of how important biological processes such as enzymatic reactions and protein-DNA interactions actually occur in our cells. The single-molecule approach to biology presents many opportunities for interdisciplinary research, calling upon collective efforts from mathematical, biological and physical scientists. The proposed research seeks to meet a high academic standard and aims to reach out to the general scientific community to collaborate and cross-fertilize the interdisciplinary field. PUBLIC HEALTH RELEVANCE: The proposed research is relevant to human health because both enzymatic reactions and protein-DNA interactions play fundamental roles in the healthy function of our cells. For example, deficiency of the beta- galactosidase, an enzyme studied in the proposal, can result in galactosialidosis or Morquio B syndrome; the misfunction of hOgg1, a DNA-repair protein studied in this proposal, can result in harmful genetic mutations.       	       n/a",Statistical Models and Inference in the Single-Molecule Approach to Biology,8100430,R01GM090202,"['Address', 'Area', 'Biochemical', 'Biochemical Reaction', 'Biological', 'Biological Process', 'Biology', 'Cells', 'Communities', 'DNA', 'DNA repair protein', 'DNA-Binding Proteins', 'DNA-Protein Interaction', 'Data', 'Dependence', 'Diffusion', 'Engineering', 'Enzymes', 'Gangliosidosis GM1', 'Gene Mutation', 'Goals', 'Health', 'Human', 'Interdisciplinary Study', 'Kinetics', 'Machine Learning', 'Modeling', 'Nanotechnology', 'Nature', 'Photons', 'Play', 'Property', 'Research', 'Role', 'Scientist', 'Statistical Models', 'Structure', 'Syndrome', 'base', 'computerized data processing', 'frontier', 'meetings', 'research study', 'single molecule', 'tool']",NIGMS,HARVARD UNIVERSITY,R01,2011,288150,0.03688237240176722
"Statistical Models and Inference in the Single-Molecule Approach to Biology    DESCRIPTION (provided by applicant): The rapid advance of nanotechnology has generated much excitement in the scientific and engineering community. Its application to the biological front created the new area of single-molecule biology: Scientists were able to investigate biological processes on a molecule-by-molecule basis, opening the door to addressing many problems that were inaccessible just a few decades ago. The new frontier also raises many statistical challenges, calling upon an urgent need for new statistical inference tools and new stochastic models because of the stochastic nature of single-molecule experiments and because many classical models derived from oversimplified assumptions are no longer valid for single-molecule experiments. The current proposal focuses on the statistical challenges in the single-molecule approach to biology. The proposed research consists of three projects: (1) Using stochastic networks to model enzymatic reaction kinetics. The goal is to provide models not only biologically meaningful, but also capable of explaining the recent single-molecule experimental discoveries that contradict the classical Michaelis-Menten model. (2) Using the kernel method to infer biochemical properties from doubly stochastic Poisson process data, in particular, photon arrival data from single-molecule experiments. The goal is to develop nonparametric inference tools to recover the dependence structure, such as the autocorrelation function, from the doubly stochastic Poisson data. (3) Using bootstrap moment estimates to infer the helical diffusion of DNA-binding proteins. The goal is to elucidate how DNA-binding proteins interact with DNA, and estimate the associated energy landscape. The proposed research aims to provide essential statistical models and inference tools to study biological processes at the single-molecule level and to advance significantly our understanding of how important biological processes such as enzymatic reactions and protein-DNA interactions actually occur in our cells. The single-molecule approach to biology presents many opportunities for interdisciplinary research, calling upon collective efforts from mathematical, biological and physical scientists. The proposed research seeks to meet a high academic standard and aims to reach out to the general scientific community to collaborate and cross-fertilize the interdisciplinary field. PUBLIC HEALTH RELEVANCE: The proposed research is relevant to human health because both enzymatic reactions and protein-DNA interactions play fundamental roles in the healthy function of our cells. For example, deficiency of the beta- galactosidase, an enzyme studied in the proposal, can result in galactosialidosis or Morquio B syndrome; the misfunction of hOgg1, a DNA-repair protein studied in this proposal, can result in harmful genetic mutations.       	       n/a",Statistical Models and Inference in the Single-Molecule Approach to Biology,7895064,R01GM090202,"['Address', 'Area', 'Biochemical', 'Biochemical Reaction', 'Biological', 'Biological Process', 'Biology', 'Cells', 'Communities', 'DNA', 'DNA repair protein', 'DNA-Binding Proteins', 'DNA-Protein Interaction', 'Data', 'Dependence', 'Diffusion', 'Engineering', 'Enzymes', 'Gangliosidosis GM1', 'Gene Mutation', 'Goals', 'Health', 'Human', 'Interdisciplinary Study', 'Kinetics', 'Machine Learning', 'Modeling', 'Nanotechnology', 'Nature', 'Photons', 'Play', 'Property', 'Research', 'Role', 'Scientist', 'Statistical Models', 'Structure', 'Syndrome', 'base', 'computerized data processing', 'frontier', 'meetings', 'public health relevance', 'research study', 'single molecule', 'tool']",NIGMS,HARVARD UNIVERSITY,R01,2010,291060,0.03688237240176722
"Statistical Models and Inference in the Single-Molecule Approach to Biology    DESCRIPTION (provided by applicant): The rapid advance of nanotechnology has generated much excitement in the scientific and engineering community. Its application to the biological front created the new area of single-molecule biology: Scientists were able to investigate biological processes on a molecule-by-molecule basis, opening the door to addressing many problems that were inaccessible just a few decades ago. The new frontier also raises many statistical challenges, calling upon an urgent need for new statistical inference tools and new stochastic models because of the stochastic nature of single-molecule experiments and because many classical models derived from oversimplified assumptions are no longer valid for single-molecule experiments. The current proposal focuses on the statistical challenges in the single-molecule approach to biology. The proposed research consists of three projects: (1) Using stochastic networks to model enzymatic reaction kinetics. The goal is to provide models not only biologically meaningful, but also capable of explaining the recent single-molecule experimental discoveries that contradict the classical Michaelis-Menten model. (2) Using the kernel method to infer biochemical properties from doubly stochastic Poisson process data, in particular, photon arrival data from single-molecule experiments. The goal is to develop nonparametric inference tools to recover the dependence structure, such as the autocorrelation function, from the doubly stochastic Poisson data. (3) Using bootstrap moment estimates to infer the helical diffusion of DNA-binding proteins. The goal is to elucidate how DNA-binding proteins interact with DNA, and estimate the associated energy landscape. The proposed research aims to provide essential statistical models and inference tools to study biological processes at the single-molecule level and to advance significantly our understanding of how important biological processes such as enzymatic reactions and protein-DNA interactions actually occur in our cells. The single-molecule approach to biology presents many opportunities for interdisciplinary research, calling upon collective efforts from mathematical, biological and physical scientists. The proposed research seeks to meet a high academic standard and aims to reach out to the general scientific community to collaborate and cross-fertilize the interdisciplinary field. PUBLIC HEALTH RELEVANCE: The proposed research is relevant to human health because both enzymatic reactions and protein-DNA interactions play fundamental roles in the healthy function of our cells. For example, deficiency of the beta- galactosidase, an enzyme studied in the proposal, can result in galactosialidosis or Morquio B syndrome; the misfunction of hOgg1, a DNA-repair protein studied in this proposal, can result in harmful genetic mutations.       	       n/a",Statistical Models and Inference in the Single-Molecule Approach to Biology,7787322,R01GM090202,"['Address', 'Area', 'Biochemical', 'Biochemical Reaction', 'Biological', 'Biological Process', 'Biology', 'Cells', 'Communities', 'DNA', 'DNA repair protein', 'DNA-Binding Proteins', 'DNA-Protein Interaction', 'Data', 'Dependence', 'Diffusion', 'Engineering', 'Enzymes', 'Gangliosidosis GM1', 'Gene Mutation', 'Goals', 'Health', 'Human', 'Interdisciplinary Study', 'Kinetics', 'Machine Learning', 'Modeling', 'Nanotechnology', 'Nature', 'Photons', 'Play', 'Property', 'Research', 'Role', 'Scientist', 'Statistical Models', 'Structure', 'Syndrome', 'base', 'computerized data processing', 'frontier', 'meetings', 'public health relevance', 'research study', 'single molecule', 'tool']",NIGMS,HARVARD UNIVERSITY,R01,2009,294000,0.03688237240176722
"Statistical Models and Inference in the Single-Molecule Approach to Biology DESCRIPTION (provided by applicant): The rapid advance of nanotechnology has generated much excitement in the scientific and engineering community. Its application to the biological front created the new area of single-molecule biology: Scientists were able to investigate biological processes on a molecule-by-molecule basis, opening the door to addressing many problems that were inaccessible just a few decades ago. The new frontier also raises many statistical challenges, calling upon an urgent need for new statistical inference tools and new stochastic models because of the stochastic nature of single-molecule experiments and because many classical models derived from oversimplified assumptions are no longer valid for single-molecule experiments. The current proposal focuses on the statistical challenges in the single-molecule approach to biology. The proposed research consists of three projects: (1) Using stochastic networks to model enzymatic reaction kinetics. The goal is to provide models not only biologically meaningful, but also capable of explaining the recent single-molecule experimental discoveries that contradict the classical Michaelis-Menten model. (2) Using the kernel method to infer biochemical properties from doubly stochastic Poisson process data, in particular, photon arrival data from single-molecule experiments. The goal is to develop nonparametric inference tools to recover the dependence structure, such as the autocorrelation function, from the doubly stochastic Poisson data. (3) Using bootstrap moment estimates to infer the helical diffusion of DNA-binding proteins. The goal is to elucidate how DNA-binding proteins interact with DNA, and estimate the associated energy landscape. The proposed research aims to provide essential statistical models and inference tools to study biological processes at the single-molecule level and to advance significantly our understanding of how important biological processes such as enzymatic reactions and protein-DNA interactions actually occur in our cells. The single-molecule approach to biology presents many opportunities for interdisciplinary research, calling upon collective efforts from mathematical, biological and physical scientists. The proposed research seeks to meet a high academic standard and aims to reach out to the general scientific community to collaborate and cross-fertilize the interdisciplinary field. PUBLIC HEALTH RELEVANCE: The proposed research is relevant to human health because both enzymatic reactions and protein-DNA interactions play fundamental roles in the healthy function of our cells. For example, deficiency of the beta- galactosidase, an enzyme studied in the proposal, can result in galactosialidosis or Morquio B syndrome; the misfunction of hOgg1, a DNA-repair protein studied in this proposal, can result in harmful genetic mutations.",Statistical Models and Inference in the Single-Molecule Approach to Biology,8280335,R01GM090202,"['Address', 'Area', 'Biochemical', 'Biochemical Reaction', 'Biological', 'Biological Process', 'Biology', 'Cells', 'Communities', 'DNA', 'DNA repair protein', 'DNA-Binding Proteins', 'DNA-Protein Interaction', 'Data', 'Dependence', 'Diffusion', 'Educational Activities', 'Educational process of instructing', 'Engineering', 'Enzymes', 'Gangliosidosis GM1', 'Gene Mutation', 'Goals', 'Health', 'Human', 'Interdisciplinary Study', 'Kinetics', 'Machine Learning', 'Modeling', 'Nanotechnology', 'Nature', 'Photons', 'Play', 'Postdoctoral Fellow', 'Property', 'Research', 'Role', 'Scientist', 'Statistical Models', 'Structure', 'Students', 'Syndrome', 'Training', 'base', 'career', 'computerized data processing', 'frontier', 'graduate student', 'meetings', 'research study', 'single molecule', 'statistics', 'tool']",NIGMS,HARVARD UNIVERSITY,R01,2012,288150,0.03688237240176722
"Structural bioinformatics software for epitope selection and antibody engineering    DESCRIPTION (provided by applicant): Human health has benefited tremendously from the therapeutic application of monoclonal antibodies (mAb), treating painful and devastating diseases such as rheumatoid arthritis and cancer, among others. However, mAb development is a laborious and time consuming process. The health benefits gained from faster mAb development are clear, creating a great need for tools to guide scientists toward discovering the most promising antigenic targets-particularly with regard to B-cell epitopes (the part of an antigen recognized by an antibody). The critical barrier to progress in this domain is the inability to deduce the conformational characteristics of protein sequence in the absence of known structure for predicting linear B-cell epitopes-the largest, most diverse, and pharmaceutically valuable class of known epitopes. The general criticism of existing prediction methods is that they are inaccurate and do not address the conformational nature of B-cell epitopes.  DNASTAR proposes to create a software pipeline that guides the prediction of B-cell epitopes, models the dynamic structural interface between a monoclonal antibody and its experimentally identified antigen, and screens in silico site-directed mutations to engineer more potent antibodies with enhanced binding affinity. The Phase I goal is to improve the prediction of antigenic peptides from target protein sequences and experimental or predicted structures. Toward this goal, DNASTAR has established collaborations with experts in monoclonal antibody production, 3D structure prediction, and protein structure and dynamics, including access to their experimental methods, data, and software tools. Our predictive models will benefit from three key innovations: 1) a superior data set and professional insights into monoclonal antibody production, 2) the introduction of state of the art 3D structure prediction for training our epitope predictors, and 3) the first use of structure-based protein dynamics in B-cell epitope prediction.  At the conclusion of Phase I, we will deliver an enhanced sequence-only B-cell epitope prediction model when compared to current top prediction methods (Aim 1) and a superior sequence and structure-based epitope prediction model using 3D structure prediction and protein dynamics (Aim 2). In creating these models, we will account for the chemical and physical properties of a protein sequence and the biophysics that mediate protein-protein interactions, including solvent accessibility, hydrogen bonding, residue flexibility, binding nuclei, and geometric contours of the molecular surface. The proposed software pipeline will be built upon Protean 3D, our new molecular structure and simulation viewer, and will elevate the technical capability of a broad range of experimental scientists to estimate key antigenic structural properties from proteins without known structure-all on their desktop computer. Upon achieving these aims, scientists will recognize that it is no longer adequate to describe B-cell epitopes using amino acid frequencies or propensity scales alone.      PUBLIC HEALTH RELEVANCE: Monoclonal antibodies are invaluable tools for diagnosing and treating human diseases. Unfortunately, the experimental methods used today to identify the most promising immunogenic targets are time consuming and less than totally effective. By taking the novel approach of incorporating both protein sequence information and structural features derived from high quality 3D structure predictions within our desktop computer software product, we propose to advance the ability of a broad range of life scientists to properly predict B-cell epitopes (the part of an antigen recognized by an antibody) applicable to their area of interest. This will accelerate the discovery of new monoclonal antibody pharmaceuticals, leading to improved human health across many diseases.           Monoclonal antibodies are invaluable tools for diagnosing and treating human diseases. Unfortunately, the experimental methods used today to identify the most promising immunogenic targets are time consuming and less than totally effective. By taking the novel approach of incorporating both protein sequence information and structural features derived from high quality 3D structure predictions within our desktop computer software product, we propose to advance the ability of a broad range of life scientists to properly predict B-cell epitopes (the part of an antigen recognized by an antibody) applicable to their area of interest. This will accelerate the discovery of new monoclonal antibody pharmaceuticals, leading to improved human health across many diseases.         ",Structural bioinformatics software for epitope selection and antibody engineering,8251785,R43GM100520,"['Accounting', 'Address', 'Affinity', 'Amino Acid Sequence', 'Amino Acids', 'Antibodies', 'Antibody Affinity', 'Antibody Binding Sites', 'Antibody Formation', 'Antigens', 'Area', 'Autoimmune Diseases', 'B-Lymphocyte Epitopes', 'Base Sequence', 'Binding', 'Bioinformatics', 'Biological', 'Biophysics', 'Cell Nucleus', 'Characteristics', 'Chemicals', 'Collaborations', 'Complement', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Engineering', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Health Benefit', 'Human', 'Hydrogen Bonding', 'Immunology', 'Life', 'Machine Learning', 'Malignant Neoplasms', 'Marketing', 'Measures', 'Mediating', 'Methods', 'Metric', 'Modeling', 'Molecular', 'Molecular Structure', 'Monoclonal Antibodies', 'Mutation', 'Nature', 'Pain', 'Peptide Sequence Determination', 'Peptides', 'Performance', 'Pharmacologic Substance', 'Phase', 'Post-Translational Protein Processing', 'Process', 'Property', 'Protein Dynamics', 'Protein Engineering', 'Proteins', 'Receiver Operating Characteristics', 'Rheumatoid Arthritis', 'Scientist', 'Site', 'Software Tools', 'Solvents', 'Structure', 'Surface', 'Therapeutic', 'Time', 'Training', 'antibody engineering', 'base', 'chemical property', 'design', 'flexibility', 'human disease', 'immunogenic', 'improved', 'innovation', 'insight', 'interest', 'monoclonal antibody production', 'novel', 'novel strategies', 'physical property', 'predictive modeling', 'programs', 'protein protein interaction', 'protein structure prediction', 'research study', 'simulation', 'success', 'three dimensional structure', 'three-dimensional modeling', 'tool']",NIGMS,"DNASTAR, INC.",R43,2012,156708,-0.001006570232005337
"Continued Maintenance and Development of Software: Integrated Genome Browser and DESCRIPTION (provided by applicant): Biomedical research increasingly involves examination of huge data sets. In recent years, introduction of new high-throughput sequencing technologies have led to production of new genome sequence for dozens of plants and animals and prompted major efforts to sequence individual human genomes. To take full advantage of these new large-scale data sets, researchers require flexible, user-friendly software that supports interactive, in-depth exploration of data at different levels of detail, ranging from chromosomes to individual base pairs. The Integrated Genome Browser (IGB), used by more than 3,500 scientists worldwide, implements innovative yet practical visualization techniques designed to help scientists explore genome-scale data sets, ultimately leading to better treatments for disease as scientists use IGB to achieve deeper insight into genes and genomes. This project will continue to develop the IGB software, adding new data integration and display features users have requested and which the IGB developers foresee will become increasingly important in the face of on-going and rapid technological change in genomics. Second, the project will implement improvements designed to enhance the productivity of IGB power users, researchers who typically spend many hours per day using the software to explore data sets they or their collaborators have created or obtained from public repositories. These improvements include: enhancing user's ability to invoke IGB from command line ""scripts,"" supporting IGB interoperability with external analysis tools, saving and restoring sessions, and improving techniques for invoking IGB from Web pages via IGB links. The project will also continue development and maintenance of the Genoviz Software Development Kit (SDK), the library of software components that underlies the advanced visualization capabilities such as animated zooming that IGB users most appreciate in the software. By continuing to maintain and develop the Genoviz SDK, the project will make IGB a more robust and flexible software application while also giving developers powerful new tools for building advanced visualization software for scientists. Methods for generating genome-scale data sets are becoming cheaper and more accessible. We are rapidly approaching a time when public health professionals will be able to survey the genomes of large numbers of people and relate genotype with disease susceptibility, overall health, and outcomes from specific treatments. However, to take advantage of these data, scientists need good visualization software that makes exploring and analyzing the data easier to accomplish. This project will develop user friendly yet innovative visualization and data sharing software that will accelerate the pace of discovery in the biosciences and ultimately lead to critical discoveries about the human genes and genomes and the roles they play in disease processes and maintaining human health.",Continued Maintenance and Development of Software: Integrated Genome Browser and,8881220,R01GM103463,"['Animals', 'Base Pairing', 'Bioinformatics', 'Biomedical Research', 'Chromosomes', 'Code', 'Communities', 'Computer software', 'Consult', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Disease susceptibility', 'Electronic Mail', 'Ensure', 'Environment', 'Flying body movement', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Graph', 'Health', 'Health Professional', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Institution', 'Internet', 'Java', 'Language', 'Lead', 'Libraries', 'Link', 'Maintenance', 'Methods', 'Outcome', 'Plants', 'Play', 'Process', 'Production', 'Productivity', 'Public Health', 'Publishing', 'Pythons', 'Reading', 'Recovery', 'Research Personnel', 'Role', 'Scientist', 'Software Tools', 'Source', 'Specialist', 'Students', 'Surveys', 'Techniques', 'Technology', 'Time', 'Training', 'Update', 'Visualization software', 'base', 'cluster computing', 'computing resources', 'data integration', 'data sharing', 'data visualization', 'design', 'flexibility', 'genome sequencing', 'genome-wide', 'imaging software', 'impression', 'improved', 'innovation', 'insight', 'interest', 'interoperability', 'open source', 'outreach', 'repository', 'research study', 'software development', 'task analysis', 'text searching', 'tool', 'user friendly software', 'user-friendly', 'web page', 'web services', 'wiki']",NIGMS,UNIVERSITY OF NORTH CAROLINA CHARLOTTE,R01,2015,293987,0.1188514325086204
"Continued Maintenance and Development of Software: Integrated Genome Browser and    DESCRIPTION (provided by applicant): Biomedical research increasingly involves examination of huge data sets. In recent years, introduction of new high-throughput sequencing technologies have led to production of new genome sequence for dozens of plants and animals and prompted major efforts to sequence individual human genomes. To take full advantage of these new large-scale data sets, researchers require flexible, user-friendly software that supports interactive, in-depth exploration of data at different levels of detail, ranging from chromosomes to individual base pairs. The Integrated Genome Browser (IGB), used by more than 3,500 scientists worldwide, implements innovative yet practical visualization techniques designed to help scientists explore genome-scale data sets, ultimately leading to better treatments for disease as scientists use IGB to achieve deeper insight into genes and genomes. This project will continue to develop the IGB software, adding new data integration and display features users have requested and which the IGB developers foresee will become increasingly important in the face of on-going and rapid technological change in genomics. Second, the project will implement improvements designed to enhance the productivity of IGB power users, researchers who typically spend many hours per day using the software to explore data sets they or their collaborators have created or obtained from public repositories. These improvements include: enhancing user's ability to invoke IGB from command line ""scripts,"" supporting IGB interoperability with external analysis tools, saving and restoring sessions, and improving techniques for invoking IGB from Web pages via IGB links. The project will also continue development and maintenance of the Genoviz Software Development Kit (SDK), the library of software components that underlies the advanced visualization capabilities such as animated zooming that IGB users most appreciate in the software. By continuing to maintain and develop the Genoviz SDK, the project will make IGB a more robust and flexible software application while also giving developers powerful new tools for building advanced visualization software for scientists.        Methods for generating genome-scale data sets are becoming cheaper and more accessible. We are rapidly approaching a time when public health professionals will be able to survey the genomes of large numbers of people and relate genotype with disease susceptibility, overall health, and outcomes from specific treatments. However, to take advantage of these data, scientists need good visualization software that makes exploring and analyzing the data easier to accomplish. This project will develop user friendly yet innovative visualization and data sharing software that will accelerate the pace of discovery in the biosciences and ultimately lead to critical discoveries about the human genes and genomes and the roles they play in disease processes and maintaining human health.                        ",Continued Maintenance and Development of Software: Integrated Genome Browser and,8704959,R01GM103463,"['Animals', 'Base Pairing', 'Bioinformatics', 'Biomedical Research', 'Chromosomes', 'Code', 'Communities', 'Computer software', 'Consult', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Disease susceptibility', 'Electronic Mail', 'Ensure', 'Environment', 'Flying body movement', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Graph', 'Health', 'Health Professional', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Human Genome', 'Image', 'Imagery', 'Individual', 'Institution', 'Internet', 'Java', 'Language', 'Lead', 'Libraries', 'Link', 'Maintenance', 'Methods', 'Outcome', 'Plants', 'Play', 'Process', 'Production', 'Productivity', 'Public Health', 'Publishing', 'Pythons', 'Reading', 'Recovery', 'Research Personnel', 'Role', 'Scientist', 'Software Tools', 'Source', 'Specialist', 'Students', 'Surveys', 'Techniques', 'Technology', 'Time', 'Training', 'Update', 'Visualization software', 'base', 'cluster computing', 'computing resources', 'data integration', 'data sharing', 'design', 'flexibility', 'genome sequencing', 'impression', 'improved', 'innovation', 'insight', 'interest', 'interoperability', 'microbial alkaline proteinase inhibitor', 'open source', 'outreach', 'repository', 'research study', 'software development', 'task analysis', 'text searching', 'tool', 'user friendly software', 'user-friendly', 'web page', 'web services', 'wiki']",NIGMS,UNIVERSITY OF NORTH CAROLINA CHARLOTTE,R01,2014,294171,0.1188514325086204
"Continued Maintenance and Development of Software: Integrated Genome Browser and    DESCRIPTION (provided by applicant): Biomedical research increasingly involves examination of huge data sets. In recent years, introduction of new high-throughput sequencing technologies have led to production of new genome sequence for dozens of plants and animals and prompted major efforts to sequence individual human genomes. To take full advantage of these new large-scale data sets, researchers require flexible, user-friendly software that supports interactive, in-depth exploration of data at different levels of detail, ranging from chromosomes to individual base pairs. The Integrated Genome Browser (IGB), used by more than 3,500 scientists worldwide, implements innovative yet practical visualization techniques designed to help scientists explore genome-scale data sets, ultimately leading to better treatments for disease as scientists use IGB to achieve deeper insight into genes and genomes. This project will continue to develop the IGB software, adding new data integration and display features users have requested and which the IGB developers foresee will become increasingly important in the face of on-going and rapid technological change in genomics. Second, the project will implement improvements designed to enhance the productivity of IGB power users, researchers who typically spend many hours per day using the software to explore data sets they or their collaborators have created or obtained from public repositories. These improvements include: enhancing user's ability to invoke IGB from command line ""scripts,"" supporting IGB interoperability with external analysis tools, saving and restoring sessions, and improving techniques for invoking IGB from Web pages via IGB links. The project will also continue development and maintenance of the Genoviz Software Development Kit (SDK), the library of software components that underlies the advanced visualization capabilities such as animated zooming that IGB users most appreciate in the software. By continuing to maintain and develop the Genoviz SDK, the project will make IGB a more robust and flexible software application while also giving developers powerful new tools for building advanced visualization software for scientists.        Methods for generating genome-scale data sets are becoming cheaper and more accessible. We are rapidly approaching a time when public health professionals will be able to survey the genomes of large numbers of people and relate genotype with disease susceptibility, overall health, and outcomes from specific treatments. However, to take advantage of these data, scientists need good visualization software that makes exploring and analyzing the data easier to accomplish. This project will develop user friendly yet innovative visualization and data sharing software that will accelerate the pace of discovery in the biosciences and ultimately lead to critical discoveries about the human genes and genomes and the roles they play in disease processes and maintaining human health.                        ",Continued Maintenance and Development of Software: Integrated Genome Browser and,8512744,R01GM103463,"['Animals', 'Base Pairing', 'Bioinformatics', 'Biomedical Research', 'Chromosomes', 'Code', 'Communities', 'Computer software', 'Consult', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Disease susceptibility', 'Electronic Mail', 'Ensure', 'Environment', 'Flying body movement', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Graph', 'Health', 'Health Professional', 'Hour', 'Human', 'Human Genome', 'Image', 'Imagery', 'Individual', 'Institution', 'Internet', 'Java', 'Language', 'Lead', 'Libraries', 'Link', 'Maintenance', 'Methods', 'Outcome', 'Plants', 'Play', 'Process', 'Production', 'Productivity', 'Public Health', 'Publishing', 'Pythons', 'Reading', 'Recovery', 'Research Personnel', 'Role', 'Scientist', 'Software Tools', 'Source', 'Specialist', 'Students', 'Surveys', 'Techniques', 'Technology', 'Time', 'Training', 'Update', 'Visualization software', 'base', 'cluster computing', 'computing resources', 'data integration', 'data sharing', 'design', 'flexibility', 'genome sequencing', 'impression', 'improved', 'innovation', 'insight', 'interest', 'interoperability', 'microbial alkaline proteinase inhibitor', 'open source', 'outreach', 'repository', 'research study', 'software development', 'task analysis', 'text searching', 'tool', 'user friendly software', 'user-friendly', 'web page', 'web services', 'wiki']",NIGMS,UNIVERSITY OF NORTH CAROLINA CHARLOTTE,R01,2013,284041,0.1188514325086204
"Continued Maintenance and Development of Software: Integrated Genome Browser and    DESCRIPTION (provided by applicant): Biomedical research increasingly involves examination of huge data sets. In recent years, introduction of new high-throughput sequencing technologies have led to production of new genome sequence for dozens of plants and animals and prompted major efforts to sequence individual human genomes. To take full advantage of these new large-scale data sets, researchers require flexible, user-friendly software that supports interactive, in-depth exploration of data at different levels of detail, ranging from chromosomes to individual base pairs. The Integrated Genome Browser (IGB), used by more than 3,500 scientists worldwide, implements innovative yet practical visualization techniques designed to help scientists explore genome-scale data sets, ultimately leading to better treatments for disease as scientists use IGB to achieve deeper insight into genes and genomes. This project will continue to develop the IGB software, adding new data integration and display features users have requested and which the IGB developers foresee will become increasingly important in the face of on-going and rapid technological change in genomics. Second, the project will implement improvements designed to enhance the productivity of IGB power users, researchers who typically spend many hours per day using the software to explore data sets they or their collaborators have created or obtained from public repositories. These improvements include: enhancing user's ability to invoke IGB from command line ""scripts,"" supporting IGB interoperability with external analysis tools, saving and restoring sessions, and improving techniques for invoking IGB from Web pages via IGB links. The project will also continue development and maintenance of the Genoviz Software Development Kit (SDK), the library of software components that underlies the advanced visualization capabilities such as animated zooming that IGB users most appreciate in the software. By continuing to maintain and develop the Genoviz SDK, the project will make IGB a more robust and flexible software application while also giving developers powerful new tools for building advanced visualization software for scientists.        Methods for generating genome-scale data sets are becoming cheaper and more accessible. We are rapidly approaching a time when public health professionals will be able to survey the genomes of large numbers of people and relate genotype with disease susceptibility, overall health, and outcomes from specific treatments. However, to take advantage of these data, scientists need good visualization software that makes exploring and analyzing the data easier to accomplish. This project will develop user friendly yet innovative visualization and data sharing software that will accelerate the pace of discovery in the biosciences and ultimately lead to critical discoveries about the human genes and genomes and the roles they play in disease processes and maintaining human health.                        ",Continued Maintenance and Development of Software: Integrated Genome Browser and,8286882,R01GM103463,"['Animals', 'Base Pairing', 'Bioinformatics', 'Biomedical Research', 'Chromosomes', 'Code', 'Communities', 'Computer software', 'Consult', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Disease susceptibility', 'Electronic Mail', 'Ensure', 'Environment', 'Flying body movement', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Graph', 'Health', 'Health Professional', 'Hour', 'Human', 'Human Genome', 'Image', 'Imagery', 'Individual', 'Institution', 'Internet', 'Java', 'Language', 'Lead', 'Libraries', 'Link', 'Maintenance', 'Methods', 'Outcome', 'Plants', 'Play', 'Process', 'Production', 'Productivity', 'Public Health', 'Publishing', 'Pythons', 'Reading', 'Recovery', 'Research Personnel', 'Role', 'Scientist', 'Software Tools', 'Source', 'Specialist', 'Students', 'Surveys', 'Techniques', 'Technology', 'Time', 'Training', 'Update', 'Visualization software', 'base', 'cluster computing', 'computing resources', 'data integration', 'data sharing', 'design', 'flexibility', 'genome sequencing', 'impression', 'improved', 'innovation', 'insight', 'interest', 'interoperability', 'microbial alkaline proteinase inhibitor', 'open source', 'outreach', 'repository', 'research study', 'software development', 'task analysis', 'text searching', 'tool', 'user friendly software', 'user-friendly', 'web page', 'web services', 'wiki']",NIGMS,UNIVERSITY OF NORTH CAROLINA CHARLOTTE,R01,2012,294505,0.1188514325086204
"MultiDimensional Expert System for Flow Cytometry DESCRIPTION (provided by applicant):  Multidimensional flow cytometry allows the discrimination of neoplastic cells from their normal counterparts. Using technology for the simultaneous measurement of 6 parameters, the abnormal expression of normal antigens can be used to identify leukemic populations. Distinction between normal and abnormal cells can be made by a highly trained scientist using a computer program which permits visualization of multidimensional data. This proposal will demonstrate the utility of a software program to reproduce the identification made by a trained scientist. We will focus on antigen expression on B lymphoid cells in bone marrow specimens that exhibit normal phenotypes. The software will follow the biological rules established by the trained scientist to identify the locations of normal cells in multidimensional space. Using this technology we will characterize acute lymphoblastic leukemias and report their relative positions in a multidimensional data space. We will mimic residual disease detection by combining normal and abnormal data to assess the value of the software technology in facilitating the identification of low levels of tumor cells in a background of normal cells. n/a",MultiDimensional Expert System for Flow Cytometry,6783233,R43CA107747,"['B lymphocyte', 'acute lymphocytic leukemia', 'antigens', 'bioengineering /biomedical engineering', 'bone marrow', 'cell population study', 'clinical research', 'computer assisted diagnosis', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'flow cytometry', 'gene expression', 'human tissue', 'lymphoblast', 'neoplasm /cancer diagnosis', 'neoplastic cell', 'patient oriented research']",NCI,"HEMATOLOGICS, INC.",R43,2004,96750,0.014143451844008455
"Towards Accurate Protein Structure Predictions with SAXS Technology (TAPESTRY) Project Summary There is an unmet need in medicine and basic sciences for accurate atomic structures of proteins. This need surpasses the capabilities of traditional high-resolution experimental methods. With machine learning advances, structure prediction algorithms are poised to provide atomic models for these areas in the near future. Yet, the gaps in prediction algorithms limit accuracy and reliability, particularly for large multi-domain proteins, protein complexes, and flexible proteins. Our proposal, Towards Accurate protein structure Predictions with SAXS TechnologY (TAPESTRY), will create technology to increase reliability and improve accuracy of protein structure predictions through experimental validation, particularly for difficult proteins. TAPESTRY is innovative by combining our strengths in high-throughput synchrotron SAXS (Small Angle X-ray Scattering) data collection and analysis with the Critical Assessment of protein Structure Prediction (CASP), which assesses structure predictions against “gold standard”, not-yet-released crystal structures every two years. Through CASP, we take advantage of the collective protein folding knowledge of the global community of structure prediction scientists. Our approach is strategic. We provide SAXS data for CASP, giving prediction scientists access to experimental data. We develop analytical and experimental tools, designed for prediction scientists to overcome current gaps that limit the use of SAXS data. We test these tools against our TAPESTRY databases of standard proteins, with corresponding crystal structures, SAXS data, and predicted models. Finally, we evaluate the robustness of our technology through CASP and obtain an unbiased assessment of our tools and the state of the field. As a first step, we target well-folded proteins (Aim 1) and proteins with disordered tails (Aim 2) in this proposal. The feasibility of our technology proposal is supported by our current data and proofs-in-concepts, our beamline capabilities, and proven experience in SAXS analysis. We show that experimental SAXS data, which contains distance information that can act as restraints in protein structure prediction algorithms, match crystal structures of well-folded proteins and score predictions based on topological accuracy. We show cases in CASP13 (2018) when SAXS data improved the fold of predicted models. SAXS data collection is rapid (10 seconds), does not require labeling or crystallization, and is available at no cost to the scientific community. We have proven experience in developing informative and effective SAXS analytical tools. Our long-term goal is to enable biomedical researchers to input an amino acid sequence and rapidly obtain an experimentally validated and accurate atomic model(s) that reflects the protein conformation(s) in solution. If TAPESTRY is successful, the increased availability of such atomic models will have strong and broad potential to advance biomedical research and impact all areas of biology in which proteins are involved. PROJECT NARRATIVE Atomic structures are critical for medical science from understanding how proteins function to drug design. Structure prediction algorithms could provide atomic models for these purposes in the near future. This proposal will leverage the structure-based technology, Small Angle X-ray Scattering (SAXS), to develop methods to experimentally validate these atomic models and increase their accuracy.",Towards Accurate Protein Structure Predictions with SAXS Technology (TAPESTRY),9946349,R01GM137021,"['Algorithms', 'Amino Acid Sequence', 'Area', 'Basic Science', 'Biology', 'Biomedical Research', 'Bypass', 'Communities', 'Complex', 'Crystallization', 'Data', 'Data Analyses', 'Data Collection', 'Databases', 'Discipline', 'Disease', 'Drug Design', 'Effectiveness', 'Engineering', 'Feedback', 'Funding', 'Future', 'Goals', 'Gold', 'Knowledge', 'Label', 'Lead', 'Light', 'Machine Learning', 'Measures', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Molecular Conformation', 'Nuclear Magnetic Resonance', 'Pathogenesis', 'Patient-Focused Outcomes', 'Positioning Attribute', 'Protein Conformation', 'Protein Region', 'Proteins', 'Provider', 'Publishing', 'Research Personnel', 'Resolution', 'Risk', 'Roentgen Rays', 'Sampling', 'Science', 'Scientist', 'Shapes', 'Signal Transduction', 'Source', 'Structural Protein', 'Structure', 'Synchrotrons', 'Tail', 'Technology', 'Tertiary Protein Structure', 'Testing', 'Validation', 'Work', 'analytical tool', 'base', 'beamline', 'cost', 'data modeling', 'design', 'drug discovery', 'experience', 'experimental study', 'flexibility', 'improved', 'innovation', 'insight', 'prediction algorithm', 'predictive modeling', 'protein complex', 'protein folding', 'protein function', 'protein structure', 'protein structure prediction', 'restraint', 'tool']",NIGMS,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,R01,2020,404480,0.05814033093634478
"Developing Machine Learning-Driven Prediction Models and Therapeutic Strategies for Circulatory Shock in Critically-ill Patients K23 Abstract This application is for a K23 Mentored Clinical Scientist Research Career Development Award entitled “Developing Machine Learning-Driven Prediction Models and Therapeutic Strategies for Circulatory Shock in Critically-ill Patients”. I am a pulmonary and critical care physician at the University of Pittsburgh. This award will facilitate my acquisition of advanced training in clinical research methods, clinical informatics, and computer science to develop my career as a physician-scientist focused on data-driven studies of dynamic physiology in critically-ill patients. The main objective of this proposal is to develop individualized prediction models and treatment strategies for shock among critically-ill patients. The aims of this study are: 1) To build machine learning-based prediction models of tachycardia and hypotension following blood donation using non-invasive waveform data in healthy blood donor volunteers, and create baseline features to compare with circulatory shock 2) To provide an operational definition, prediction models, differentiation of physiologic evolution towards shock, and personalized treatment of circulatory shock in ICU patients. Through this proposal, I will develop advanced skills in machine-learning, clinical bioinformatics, and clinical research. I will complete a Master of Science in Biomedical Informatics to learn advanced data- driven research methodologies to strengthen my technical training. This award will be a critical step towards my long-term goal, being an independent physician scientist, with expertise in prediction analytics in critical care medicine through clinical trials. I have committed mentors Dr. Michael Pinsky (physiology, functional hemodynamics) and Dr. Gilles Clermont (critical care, algorithms, data science) who will ensure successful completion of my proposed aims. My mentoring committee also includes an advisor, Dr. Milos Hauskrecht - a renowned computer scientist in the Computer Science and Information Sciences at the University of Pittsburgh. My work will be completed within the Division of Pulmonary, Allergy, and Critical Care Medicine at the University of Pittsburgh, which has an extensive track record of committing to the development of physician scientists. PROJECT NARRATIVE Circulatory shock is a major cause of morbidity and mortality among critically ill patients. Early identification or forecasting of shock would be clinically actionable and improve prognosis, but no personalized prediction model for shock has been presented yet. We propose to develop physiologically and clinically relevant machine learning-based prediction models and therapeutic strategies that could guide personalized diagnosis and treatment of circulatory shock.",Developing Machine Learning-Driven Prediction Models and Therapeutic Strategies for Circulatory Shock in Critically-ill Patients,10039613,K23GM138984,"['Acute', 'Adverse event', 'Algorithms', 'Artificial Intelligence', 'Award', 'Bioinformatics', 'Blood Donations', 'Blood Pressure', 'Blood donor', 'Blood flow', 'Cardiovascular system', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Clinical Trials', 'Complex', 'Computerized Medical Record', 'Computers', 'Critical Care', 'Critical Illness', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Databases', 'Detection', 'Development', 'Distress', 'Early Diagnosis', 'Early identification', 'Electrocardiogram', 'Electroconvulsive Therapy', 'Ensure', 'Environment', 'Evolution', 'Failure', 'Family suidae', 'Financial compensation', 'Goals', 'Gold', 'Grant', 'Health Care Costs', 'Hemoglobin concentration result', 'Hemorrhage', 'Heterogeneity', 'Hypersensitivity', 'Hypotension', 'Hypovolemia', 'Hypovolemics', 'Individual', 'Information Sciences', 'Intensive Care Units', 'Interdisciplinary Study', 'K-Series Research Career Programs', 'Learning', 'Life', 'Link', 'Liquid substance', 'Lung', 'Machine Learning', 'Master of Science', 'Measures', 'Medicine', 'Mentors', 'Metabolic', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Organ', 'Outcome', 'Oxygen', 'Patients', 'Pattern', 'Phenotype', 'Photoplethysmography', 'Physicians', 'Physiological', 'Physiology', 'Plethysmography', 'Predictive Analytics', 'Probability', 'Psychological reinforcement', 'Reaction', 'Reporting', 'Research', 'Research Methodology', 'Resuscitation', 'Risk', 'Savings', 'Scientist', 'Series', 'Severities', 'Severity of illness', 'Shock', 'Tachycardia', 'Therapeutic', 'Time', 'Training', 'Treatment Protocols', 'Universities', 'Vasoconstrictor Agents', 'Work', 'adverse event risk', 'adverse outcome', 'alternative treatment', 'analog', 'base', 'biomedical informatics', 'career', 'causal model', 'clinically actionable', 'clinically relevant', 'cohort', 'computer science', 'computerized data processing', 'data acquisition', 'deep learning', 'density', 'hemodynamics', 'high risk', 'human subject', 'improved', 'improved outcome', 'individualized medicine', 'machine learning algorithm', 'mortality', 'neurotensin mimic 2', 'outcome forecast', 'personalized diagnostics', 'personalized medicine', 'personalized predictions', 'predictive modeling', 'response', 'signal processing', 'skills', 'supervised learning', 'treatment strategy', 'unsupervised learning', 'volunteer']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K23,2020,189861,0.058844013322664666
"30th Annual MR Angiography Meeting Project Summary The objective of the 30th Annual Workshop on Magnetic Resonance Angiography is to provide a forum for scientists and scientist-clinicians, clinical staff and industry interested in MR angiography techniques. The Workshop is the annual meeting of the Society for Magnetic Resonance Angiography (SMRA). At this meeting, emerging techniques and exciting new applications to visualize the vascular system, measure and display blood flow and improve patient outcomes will be presented. MR angiography is an important clinical tool that is applied to millions of patients annually and accounts for an estimated 10% of all MR procedures. Recent advances in time-resolved imaging, non-contrast imaging, post-processing techniques, flow measurements, and flow visualization, as well other innovations, continue to make MRA a dynamic, cutting-edge area of interest for scientific investigation. A major goal of this SMRA Workshop is to provide scientists, clinicians, and students with the opportunity to build connections, pool their knowledge, and educate each other in order to accelerate the refinement of MRA technology and critically how to apply it in clinical practice. Topics for the MRA Workshop will include: vascular disease mechanisms, vessel wall and plaque imaging, quantification of blood flow dynamics, machine learning including deep learning plus `Big Data', vessel lumen imaging, MRA of the brain, heart, abdomen, and extremities; contrast agents, cardiac MR, assessment of cardiac structure & function, clinical study design, new MRA techniques, interventional MRI, MRI of implanted devices, technology assessment, and comparing MRI with other medical imaging modalities. The 3-day workshop will be preceded by an informative one-day educational program that will include both fundamental and advanced lectures from international experts in the field. These topics and educational objectives of the 30th Annual Workshop on Magnetic Resonance Angiography are directly related to the NHLBI mission to provide global leadership for research, training, and education to promote the prevention and treatment of heart and blood diseases. The scientific presentations will include new discoveries about the causes of disease and as such contribute to the translation of basic discoveries into clinical practice. In addition, the proposed educational activities as well as discussion among participants will foster training and mentoring of emerging scientists and physicians. In this context, the workshop will support a collaborative research infrastructure, including participants from academic institutions and industry. Project Narrative This proposed “30th Annual Workshop on Magnetic Resonance Angiography” will provide a forum in which researchers and clinicians interested in MRA can build connections, pool their knowledge, and educate students and fellow scientists in order to further develop MRA technology and translate it into clinical practice.",30th Annual MR Angiography Meeting,9613125,R13HL144016,"['Abdomen', 'Angiography', 'Area', 'Award', 'Big Data', 'Biology', 'Blood flow', 'Brain', 'Cardiac', 'Catheters', 'Clinical', 'Clinical Research', 'Contrast Media', 'Development', 'Diagnostic Imaging', 'Disease', 'Educational Activities', 'Educational workshop', 'Engineering', 'Female', 'Fertilization', 'Fostering', 'Funding', 'Genetic Medicine', 'Genetics and Medicine', 'Goals', 'Growth', 'Heart', 'Heart Diseases', 'Hematological Disease', 'Image', 'Imagery', 'Industry', 'Institution', 'International', 'Intervention', 'Investigation', 'Ionizing radiation', 'Knowledge', 'Leadership', 'Limb structure', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Angiography', 'Magnetic Resonance Imaging', 'Mathematics', 'Measurement', 'Measures', 'Medical Imaging', 'Mentors', 'Minority', 'Mission', 'Morphologic artifacts', 'National Heart, Lung, and Blood Institute', 'Oral', 'Organ', 'Participant', 'Patient-Focused Outcomes', 'Patients', 'Physicians', 'Physics', 'Postdoctoral Fellow', 'Prevention', 'Procedures', 'Protocols documentation', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Training', 'Scientist', 'Scotland', 'Secure', 'Societies', 'Standardization', 'Structure', 'Students', 'Techniques', 'Technology', 'Technology Assessment', 'Time', 'Tissue Viability', 'Training', 'Training and Education', 'Translating', 'Translational Research', 'Translations', 'Travel', 'Vascular Diseases', 'Vascular System', 'Venous', 'Woman', 'Work', 'X-Ray Computed Tomography', 'base', 'career', 'clinical application', 'clinical practice', 'collaborative environment', 'computer science', 'deep learning', 'imaging modality', 'implantable device', 'improved', 'innovation', 'interest', 'lectures', 'meetings', 'minority trainee', 'posters', 'programs', 'research and development', 'student participation', 'success', 'tool']",NHLBI,NORTHWESTERN UNIVERSITY AT CHICAGO,R13,2018,10000,-0.016441501089680597
"31st Annual International MR Angiography Conference Project Summary The objective of the 31st Annual Workshop on Magnetic Resonance Angiography is to provide a forum for scientists and scientist-clinicians, clinical staff and industry interested in MR angiography techniques. The Workshop is the annual meeting of the Society for Magnetic Resonance Angiography (SMRA). At this meeting, emerging techniques and exciting new applications to visualize the vascular system, measure and display blood flow and improve patient outcomes will be presented. MR angiography is an important clinical tool that is applied to millions of patients annually and accounts for an estimated 10% of all MR procedures. Recent advances in time-resolved imaging, non-contrast imaging, post-processing techniques, flow measurements, and flow visualization, as well other innovations, continue to make MRA a dynamic, cutting-edge area of interest for scientific investigation. A major goal of this SMRA Workshop is to provide scientists, clinicians, and students with the opportunity to build connections, pool their knowledge, and educate each other in order to accelerate the refinement of MRA technology and critically how to apply it in clinical practice. Topics for the MRA Workshop will include: vascular disease mechanisms, vessel wall and plaque imaging, quantification of blood flow dynamics, applications of artificial intelligence (AI) and deep learning, MRA of the brain, heart, abdomen, and extremities; contrast agents, cardiac MR, assessment of cardiac structure and function, clinical study design, new MRA techniques, MRI of implanted devices, technology assessment, comparing MRI with other imaging modalities and critically translating advanced MRA techniques into day-to- day clinical practice. The 3-day workshop will be preceded by an informative one-day educational program that will include both fundamental and advanced lectures from international experts in the field. These topics and educational objectives of the 31st Annual Workshop on Magnetic Resonance Angiography are directly related to the NHLBI mission to provide global leadership for research, training, and education to promote the prevention and treatment of heart and blood diseases. The scientific presentations will include new discoveries about the causes of disease and as such contribute to the translation of basic discoveries into clinical practice. In addition, the proposed educational activities as well as discussion among participants will foster training and mentoring of emerging scientists and physicians. In this context, the workshop will support a collaborative research infrastructure, including participants from academic institutions and industry. Project Narrative This proposed “31st Annual Workshop on Magnetic Resonance Angiography” will provide a forum in which researchers and clinicians interested in MRA can build connections, pool their knowledge, and educate students and fellow scientists in order to further develop MRA technology and translate it into clinical practice.",31st Annual International MR Angiography Conference,9837090,R13HL149441,"['Abdomen', 'Angiography', 'Area', 'Artificial Intelligence', 'Award', 'Biology', 'Blood flow', 'Brain', 'Cardiac', 'Catheters', 'Clinical', 'Clinical Research', 'Contrast Media', 'Development', 'Diagnostic Imaging', 'Disease', 'Educational Activities', 'Educational workshop', 'Engineering', 'Female', 'Fertilization', 'Fostering', 'France', 'Funding', 'Genetic Medicine', 'Genetics and Medicine', 'Goals', 'Growth', 'Heart', 'Heart Diseases', 'Hematological Disease', 'Image', 'Imagery', 'Industry', 'Institution', 'International', 'Investigation', 'Ionizing radiation', 'Knowledge', 'Leadership', 'Limb structure', 'Magnetic Resonance', 'Magnetic Resonance Angiography', 'Magnetic Resonance Imaging', 'Mathematics', 'Measurement', 'Measures', 'Medical Imaging', 'Mentors', 'Minority', 'Mission', 'Morphologic artifacts', 'National Heart, Lung, and Blood Institute', 'Oral', 'Organ', 'Participant', 'Patient-Focused Outcomes', 'Patients', 'Physicians', 'Physics', 'Postdoctoral Fellow', 'Prevention', 'Procedures', 'Protocols documentation', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Training', 'Scientist', 'Secure', 'Societies', 'Standardization', 'Structure', 'Students', 'Techniques', 'Technology', 'Technology Assessment', 'Time', 'Tissue Viability', 'Training', 'Training and Education', 'Translating', 'Translational Research', 'Translations', 'Travel', 'Vascular Diseases', 'Vascular System', 'Venous', 'Woman', 'Work', 'X-Ray Computed Tomography', 'base', 'career', 'clinical application', 'clinical practice', 'computer science', 'deep learning', 'imaging modality', 'implantable device', 'improved', 'innovation', 'interest', 'lectures', 'meetings', 'minority trainee', 'posters', 'programs', 'research and development', 'student participation', 'success', 'supportive environment', 'symposium', 'tool']",NHLBI,NORTHWESTERN UNIVERSITY AT CHICAGO,R13,2019,10000,-0.016822496349363083
"Society for Magnetic Resonance Angiography (SMRA) 32nd Annual International Conference Project Summary The objective of the 32nd Annual Workshop on Magnetic Resonance Angiography is to provide a forum for basic scientists, clinician scientists, clinical staff, and industry interested in MR angiography techniques to exchange ideas and share the latest research and clinical developments. The Workshop is the annual meeting of the Society for Magnetic Resonance Angiography (SMRA). At this meeting, emerging techniques and exciting new applications to visualize the vascular system, measure and display blood flow and improve patient outcomes will be presented. MR angiography is an important clinical tool that is applied to millions of patients annually and accounts for an estimated 10% of all MR procedures. Recent advances in time-resolved imaging, low-contrast and non-contrast imaging, novel contrast agents, post-processing and display techniques, flow measurements and flow visualization, artificial intelligence, as well other innovations, continue to make MRA a dynamic, cutting-edge area of interest for scientific investigation. A major goal of this SMRA Workshop is to provide scientists, clinicians, and particularly trainees with diverse background with the opportunity to build connections, pool their knowledge, and educate each other in order to accelerate the refinement of MRA technology and critically how to apply it in clinical practice. Topics for the MRA Workshop will include: vascular disease mechanisms, vessel wall and plaque imaging, quantification of blood flow dynamics, applications of artificial intelligence (AI) and deep learning, MRA of the brain, heart, abdomen, and extremities; contrast agents, cardiac MR, assessment of cardiac structure and function, clinical study design, new MRA techniques, MRI of implanted devices, technology assessment, comparing MRI with other imaging modalities, values added by MRA, and critically translating advanced MRA techniques into day-to-day clinical practice. The 3-day workshop will be preceded by an informative one-day educational program that will include both fundamental and advanced lectures from international experts in the field. These topics and educational objectives of the 32nd Annual Workshop on Magnetic Resonance Angiography are directly related to the NHLBI mission to provide global leadership for research, training, and education to promote the prevention and treatment of heart and blood diseases. The scientific presentations will include new discoveries about the causes of disease and as such contribute to the translation of basic discoveries into clinical practice. In addition, the proposed educational activities as well as discussion among participants will foster training and mentoring of emerging scientists and physicians. In this context, the workshop will support a collaborative research infrastructure, including participants from academic institutions and industry. Project Narrative This proposed “32nd Annual Workshop on Magnetic Resonance Angiography” will provide a forum in which researchers and clinicians interested in MRA can build connections, pool their knowledge, and educate students and fellow scientists in order to further develop MRA technology and translate it into clinical practice.",Society for Magnetic Resonance Angiography (SMRA) 32nd Annual International Conference,10071098,R13HL154799,"['Abdomen', 'Angiography', 'Area', 'Artificial Intelligence', 'Award', 'Biology', 'Blood flow', 'Brain', 'Cardiac', 'Catheters', 'Clinical', 'Clinical Research', 'Contrast Media', 'Development', 'Diagnostic Imaging', 'Disease', 'Education', 'Educational Activities', 'Educational workshop', 'Engineering', 'Female', 'Fertilization', 'Fostering', 'Funding', 'Genetics and Medicine', 'Goals', 'Growth', 'Heart', 'Heart Diseases', 'Hematological Disease', 'Image', 'Industry', 'Institution', 'International', 'Investigation', 'Ionizing radiation', 'Knowledge', 'Leadership', 'Limb structure', 'Los Angeles', 'Magnetic Resonance', 'Magnetic Resonance Angiography', 'Magnetic Resonance Imaging', 'Mathematics', 'Measurement', 'Measures', 'Mentors', 'Mission', 'Modality', 'Morphologic artifacts', 'National Heart, Lung, and Blood Institute', 'Oral', 'Organ', 'Participant', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Physicians', 'Physics', 'Postdoctoral Fellow', 'Prevention', 'Procedures', 'Protocols documentation', 'Publications', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Training', 'Rest', 'Safety', 'Scientist', 'Series', 'Societies', 'Standardization', 'Structure', 'Students', 'Techniques', 'Technology', 'Technology Assessment', 'Time', 'Tissue Viability', 'Training', 'Training and Education', 'Translating', 'Translational Research', 'Translations', 'Travel', 'Underrepresented Minority', 'Vascular Diseases', 'Vascular System', 'Venous', 'Visualization', 'Work', 'X-Ray Computed Tomography', 'base', 'career', 'clinical application', 'clinical development', 'clinical practice', 'computer science', 'cost', 'deep learning', 'imaging modality', 'implantable device', 'improved', 'innovation', 'interest', 'lectures', 'meetings', 'novel', 'posters', 'programs', 'research and development', 'success', 'supportive environment', 'symposium', 'tool']",NHLBI,CEDARS-SINAI MEDICAL CENTER,R13,2020,20000,-0.002614563545230693
"NDEx - the Network Data Exchange A Network Commons for Biologists Summary Knowledge of the hallmark networks of cancer has been key to interpreting the molecular heterogeneity seen within and across tumors. NDEx, the Network Data Exchange (www.ndexbio.org), is an online commons where scientists can upload, share, and publicly distribute molecular networks and pathway models. We are currently two years into a three-year grant from the National Cancer Institute (U24 CA184427A), during which time we have created the NDEx open-source platform; released NDEx as a public website; and begun to link NDEx to applications such as Cytoscape. Having built this initial system, our goals for the next funding period are to rapidly develop NDEx network content and communities and the means by which these resources impact cancer research and treatment. In particular, we will (1) Connect NDEx networks to next-generation cancer genome analysis; (2) Enable the direct publishing of interactive networks on journal websites; (3) Develop hallmark cancer pathway content via social organization and automation; and (4) Scale the NDEx framework to support a growing community of users and analysis tools. Through these aims, the emphasis of the NDEx Project transitions from core infrastructure to genome analysis workflows, cancer-relevant network content, publishing, and community building. These developments address a strong and immediate need in cancer research for a cyberinfrastructure that can analyze genome-scale information in the context of molecular networks. Narrative NDEx (www.ndexbio.org) is an online commons in which biological networks can be shared, published and used by cancer researchers. Informatics applications for analysis and visualization can use NDEx as a hub to exchange networks, facilitating modular workflows. NDEx is a step towards new forms of scientific publication and collaboration in which networks bearing data, hypotheses, and findings flow easily between scientists.",NDEx - the Network Data Exchange A Network Commons for Biologists,9483261,U24CA184427,"['Address', 'Algorithms', 'Automation', 'Biological', 'Biology', 'Cancer Biology', 'Charge', 'Collaborations', 'Collection', 'Communities', 'Community Developments', 'Computer software', 'Data', 'Databases', 'Development', 'Fostering', 'Funding', 'Genes', 'Genomics', 'Goals', 'Grant', 'Heterogeneity', 'Image', 'Imagery', 'Informatics', 'Journals', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Maps', 'Methods', 'Mining', 'Modeling', 'Molecular', 'National Cancer Institute', 'Network-based', 'Pathway Analysis', 'Pathway interactions', 'Phase', 'Process', 'Provider', 'Publications', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Services', 'Software Framework', 'Stratification', 'System', 'Time', 'Variant', 'Work', 'anticancer research', 'cancer genome', 'cancer therapy', 'community building', 'cyber infrastructure', 'data exchange', 'genome analysis', 'genome-wide', 'improved', 'interest', 'journal article', 'member', 'next generation', 'online resource', 'open source', 'outreach', 'patient oriented', 'repository', 'social', 'social organization', 'text searching', 'tool', 'tumor', 'web site']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2018,753354,0.03876091636146349
"NDEx - the Network Data Exchange A Network Commons for Biologists Summary Knowledge of the hallmark networks of cancer has been key to interpreting the molecular heterogeneity seen within and across tumors. NDEx, the Network Data Exchange (www.ndexbio.org), is an online commons where scientists can upload, share, and publicly distribute molecular networks and pathway models. We are currently two years into a three-year grant from the National Cancer Institute (U24 CA184427A), during which time we have created the NDEx open-source platform; released NDEx as a public website; and begun to link NDEx to applications such as Cytoscape. Having built this initial system, our goals for the next funding period are to rapidly develop NDEx network content and communities and the means by which these resources impact cancer research and treatment. In particular, we will (1) Connect NDEx networks to next-generation cancer genome analysis; (2) Enable the direct publishing of interactive networks on journal websites; (3) Develop hallmark cancer pathway content via social organization and automation; and (4) Scale the NDEx framework to support a growing community of users and analysis tools. Through these aims, the emphasis of the NDEx Project transitions from core infrastructure to genome analysis workflows, cancer-relevant network content, publishing, and community building. These developments address a strong and immediate need in cancer research for a cyberinfrastructure that can analyze genome-scale information in the context of molecular networks. Narrative NDEx (www.ndexbio.org) is an online commons in which biological networks can be shared, published and used by cancer researchers. Informatics applications for analysis and visualization can use NDEx as a hub to exchange networks, facilitating modular workflows. NDEx is a step towards new forms of scientific publication and collaboration in which networks bearing data, hypotheses, and findings flow easily between scientists.",NDEx - the Network Data Exchange A Network Commons for Biologists,9296906,U24CA184427,"['Address', 'Algorithms', 'Automation', 'Biological', 'Biology', 'Cancer Biology', 'Charge', 'Collaborations', 'Collection', 'Communities', 'Community Developments', 'Computer software', 'Data', 'Databases', 'Development', 'Fostering', 'Funding', 'Genes', 'Genomics', 'Goals', 'Grant', 'Heterogeneity', 'Image', 'Imagery', 'Informatics', 'Journals', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Maps', 'Methods', 'Mining', 'Modeling', 'Molecular', 'National Cancer Institute', 'Network-based', 'Pathway Analysis', 'Pathway interactions', 'Phase', 'Process', 'Provider', 'Publications', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Services', 'Software Framework', 'Stratification', 'System', 'Time', 'Variant', 'Work', 'anticancer research', 'cancer genome', 'cancer therapy', 'community building', 'cyber infrastructure', 'data exchange', 'genome analysis', 'genome-wide', 'improved', 'interest', 'journal article', 'member', 'next generation', 'online resource', 'open source', 'outreach', 'patient oriented', 'repository', 'social', 'social organization', 'text searching', 'tool', 'tumor', 'web site']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2017,771433,0.03876091636146349
"NDEx - the Network Data Exchange A Network Commons for Biologists Summary Knowledge of the hallmark networks of cancer has been key to interpreting the molecular heterogeneity seen within and across tumors. NDEx, the Network Data Exchange (www.ndexbio.org), is an online commons where scientists can upload, share, and publicly distribute molecular networks and pathway models. We are currently two years into a three-year grant from the National Cancer Institute (U24 CA184427A), during which time we have created the NDEx open-source platform; released NDEx as a public website; and begun to link NDEx to applications such as Cytoscape. Having built this initial system, our goals for the next funding period are to rapidly develop NDEx network content and communities and the means by which these resources impact cancer research and treatment. In particular, we will (1) Connect NDEx networks to next-generation cancer genome analysis; (2) Enable the direct publishing of interactive networks on journal websites; (3) Develop hallmark cancer pathway content via social organization and automation; and (4) Scale the NDEx framework to support a growing community of users and analysis tools. Through these aims, the emphasis of the NDEx Project transitions from core infrastructure to genome analysis workflows, cancer-relevant network content, publishing, and community building. These developments address a strong and immediate need in cancer research for a cyberinfrastructure that can analyze genome-scale information in the context of molecular networks. Narrative NDEx (www.ndexbio.org) is an online commons in which biological networks can be shared, published and used by cancer researchers. Informatics applications for analysis and visualization can use NDEx as a hub to exchange networks, facilitating modular workflows. NDEx is a step towards new forms of scientific publication and collaboration in which networks bearing data, hypotheses, and findings flow easily between scientists.",NDEx - the Network Data Exchange A Network Commons for Biologists,9918260,U24CA184427,"['Address', 'Algorithms', 'Automation', 'Biological', 'Biology', 'Cancer Biology', 'Charge', 'Collaborations', 'Collection', 'Communities', 'Community Developments', 'Computer software', 'Data', 'Databases', 'Development', 'Facebook', 'Fostering', 'Funding', 'Genes', 'Genomics', 'Goals', 'Grant', 'Heterogeneity', 'Image', 'Informatics', 'Infrastructure', 'Journals', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Maps', 'Methods', 'Mining', 'Modeling', 'Molecular', 'National Cancer Institute', 'Network-based', 'Pathway Analysis', 'Pathway interactions', 'Phase', 'Process', 'Provider', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Services', 'Software Framework', 'Stratification', 'System', 'Time', 'Variant', 'Visualization', 'Work', 'anticancer research', 'cancer genome', 'cancer therapy', 'community building', 'cyber infrastructure', 'data exchange', 'genome analysis', 'genome-wide', 'improved', 'interest', 'journal article', 'member', 'next generation', 'online resource', 'open source', 'outreach', 'patient oriented', 'repository', 'social', 'social organization', 'text searching', 'tool', 'tumor', 'web site']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2020,745589,0.03876091636146349
"NDEx - the Network Data Exchange A Network Commons for Biologists Summary Knowledge of the hallmark networks of cancer has been key to interpreting the molecular heterogeneity seen within and across tumors. NDEx, the Network Data Exchange (www.ndexbio.org), is an online commons where scientists can upload, share, and publicly distribute molecular networks and pathway models. We are currently two years into a three-year grant from the National Cancer Institute (U24 CA184427A), during which time we have created the NDEx open-source platform; released NDEx as a public website; and begun to link NDEx to applications such as Cytoscape. Having built this initial system, our goals for the next funding period are to rapidly develop NDEx network content and communities and the means by which these resources impact cancer research and treatment. In particular, we will (1) Connect NDEx networks to next-generation cancer genome analysis; (2) Enable the direct publishing of interactive networks on journal websites; (3) Develop hallmark cancer pathway content via social organization and automation; and (4) Scale the NDEx framework to support a growing community of users and analysis tools. Through these aims, the emphasis of the NDEx Project transitions from core infrastructure to genome analysis workflows, cancer-relevant network content, publishing, and community building. These developments address a strong and immediate need in cancer research for a cyberinfrastructure that can analyze genome-scale information in the context of molecular networks. Narrative NDEx (www.ndexbio.org) is an online commons in which biological networks can be shared, published and used by cancer researchers. Informatics applications for analysis and visualization can use NDEx as a hub to exchange networks, facilitating modular workflows. NDEx is a step towards new forms of scientific publication and collaboration in which networks bearing data, hypotheses, and findings flow easily between scientists.",NDEx - the Network Data Exchange A Network Commons for Biologists,9692560,U24CA184427,"['Address', 'Algorithms', 'Automation', 'Biological', 'Biology', 'Cancer Biology', 'Charge', 'Collaborations', 'Collection', 'Communities', 'Community Developments', 'Computer software', 'Data', 'Databases', 'Development', 'Facebook', 'Fostering', 'Funding', 'Genes', 'Genomics', 'Goals', 'Grant', 'Heterogeneity', 'Image', 'Imagery', 'Informatics', 'Infrastructure', 'Journals', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Maps', 'Methods', 'Mining', 'Modeling', 'Molecular', 'National Cancer Institute', 'Network-based', 'Pathway Analysis', 'Pathway interactions', 'Phase', 'Process', 'Provider', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Services', 'Software Framework', 'Stratification', 'System', 'Time', 'Variant', 'Work', 'anticancer research', 'cancer genome', 'cancer therapy', 'community building', 'cyber infrastructure', 'data exchange', 'genome analysis', 'genome-wide', 'improved', 'interest', 'journal article', 'member', 'next generation', 'online resource', 'open source', 'outreach', 'patient oriented', 'repository', 'social', 'social organization', 'text searching', 'tool', 'tumor', 'web site']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2019,728373,0.03876091636146349
"Tools for Visualization of Geographic Structure in Population Genomic Data ﻿    DESCRIPTION (provided by applicant): Large samples sizes are increasingly common in genetics/genomics, particularly in human genetics where sample sizes must be large (>1,000s of individuals) to detect variant associations with complex disease traits. A common feature of data from large samples is that the individuals within the study have varying levels of similarity with one another that can become problematic for downstream analyses (e.g. causing spurious associations) if not understood. Thus uncovering population structure and dissecting it to understand its source is a common and important practice in large-scale studies. Here, we aim to solve challenges for visualizing population structure that regularly arise when researchers interact with large-scale population genomic data sets. In Aim 1 we will develop a software tool for visualizing population structure using principal components analysis (PCA). This tool will make straightforward several steps that are commonly reinvented by data scientists as they analyze PCA outputs from genetic data. It will also make more clear whether PCA analyses may be returning anomalous results. In Aim 2 we will develop a tool for producing geographic allele frequency maps of publicly available or user-generated allele frequency data. In Aim 3 we will develop a visualization approach for displaying geographic regions where populations show unexpectedly high or low levels of differentiation. In Aim 4 we will integrate these pieces of software into a single suite and link them to externally generated data sources and existing genome browsers. By developing these sets of tools we help to remove the need for unnecessary script generation by independent researchers and increase the pace of genomics research. Throughout the project we will pay special attention to developing user-friendly interactive data displays such as those generated by the Data Driven Documents (d3) JavaScript visualization libraries. Where possible we will use simple, yet flexible python backends and provide complementary R libraries to facilitate customizations and integration with existing analysis pipelines. While population genetic applications will motivate our work, the tools we are generating will be generally applicable to other forms of structured biomedical data. PUBLIC HEALTH RELEVANCE: This project will provide tools for visualizing large-scale genetic data with population structure. While numerous advanced algorithms for summarizing population structure exist, the human interface to the outputs of these methods is lacking and has become a time sink during the analysis of large samples. In this project we will provide user-friendly tools that lower the barrier to understanding genetic variation datasets. In particulr we will develop tools for visualizing compressed representations of genetic variation (i.e. PCA results) and how genetic diversity is distributed across geographic space in a sample.",Tools for Visualization of Geographic Structure in Population Genomic Data,9272843,U01CA198933,"['Address', 'Algorithms', 'Attention', 'Awareness', 'Big Data to Knowledge', 'Biological', 'Biological Process', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Display', 'Data Science', 'Data Set', 'Data Sources', 'Disease', 'Ensure', 'Exhibits', 'Frequencies', 'Funding', 'Gene Frequency', 'Generations', 'Genes', 'Genetic', 'Genetic Drift', 'Genetic Variation', 'Genetic study', 'Genome', 'Genomics', 'Genotype', 'Geographic Distribution', 'Geographic Locations', 'Geography', 'Goals', 'Grant', 'Human', 'Human Genetics', 'Imagery', 'Individual', 'Knowledge', 'Label', 'Libraries', 'Link', 'Manuscripts', 'Maps', 'Methods', 'Output', 'Paper', 'Pattern', 'Pharmacogenomics', 'Phenotype', 'Play', 'Population', 'Population Genetics', 'Preparation', 'Principal Component Analysis', 'Publications', 'Publishing', 'Pythons', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Sample Size', 'Sampling', 'Sampling Studies', 'Software Tools', 'Source', 'Statistical Data Interpretation', 'Structure', 'Surface', 'Techniques', 'Time', 'Tissues', 'Uncertainty', 'Variant', 'Visualization software', 'Work', 'base', 'data visualization', 'digital', 'expectation', 'flexibility', 'genetic variant', 'genome browser', 'genome wide association study', 'genomic data', 'interest', 'migration', 'population based', 'public health relevance', 'tool', 'trait', 'transcriptome sequencing', 'user-friendly']",NCI,UNIVERSITY OF CHICAGO,U01,2017,409700,-0.010678328727359644
"Tools for Visualization of Geographic Structure in Population Genomic Data ﻿    DESCRIPTION (provided by applicant): Large samples sizes are increasingly common in genetics/genomics, particularly in human genetics where sample sizes must be large (>1,000s of individuals) to detect variant associations with complex disease traits. A common feature of data from large samples is that the individuals within the study have varying levels of similarity with one another that can become problematic for downstream analyses (e.g. causing spurious associations) if not understood. Thus uncovering population structure and dissecting it to understand its source is a common and important practice in large-scale studies. Here, we aim to solve challenges for visualizing population structure that regularly arise when researchers interact with large-scale population genomic data sets. In Aim 1 we will develop a software tool for visualizing population structure using principal components analysis (PCA). This tool will make straightforward several steps that are commonly reinvented by data scientists as they analyze PCA outputs from genetic data. It will also make more clear whether PCA analyses may be returning anomalous results. In Aim 2 we will develop a tool for producing geographic allele frequency maps of publicly available or user-generated allele frequency data. In Aim 3 we will develop a visualization approach for displaying geographic regions where populations show unexpectedly high or low levels of differentiation. In Aim 4 we will integrate these pieces of software into a single suite and link them to externally generated data sources and existing genome browsers. By developing these sets of tools we help to remove the need for unnecessary script generation by independent researchers and increase the pace of genomics research. Throughout the project we will pay special attention to developing user-friendly interactive data displays such as those generated by the Data Driven Documents (d3) JavaScript visualization libraries. Where possible we will use simple, yet flexible python backends and provide complementary R libraries to facilitate customizations and integration with existing analysis pipelines. While population genetic applications will motivate our work, the tools we are generating will be generally applicable to other forms of structured biomedical data. PUBLIC HEALTH RELEVANCE: This project will provide tools for visualizing large-scale genetic data with population structure. While numerous advanced algorithms for summarizing population structure exist, the human interface to the outputs of these methods is lacking and has become a time sink during the analysis of large samples. In this project we will provide user-friendly tools that lower the barrier to understanding genetic variation datasets. In particulr we will develop tools for visualizing compressed representations of genetic variation (i.e. PCA results) and how genetic diversity is distributed across geographic space in a sample.",Tools for Visualization of Geographic Structure in Population Genomic Data,9070657,U01CA198933,"['Address', 'Algorithms', 'Alleles', 'Attention', 'Base Sequence', 'Big Data to Knowledge', 'Biological', 'Biological Process', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Display', 'Data Science', 'Data Set', 'Data Sources', 'Disease', 'Ensure', 'Exhibits', 'Frequencies', 'Funding', 'Gene Frequency', 'Generations', 'Genes', 'Genetic', 'Genetic Drift', 'Genetic Variation', 'Genetic study', 'Genomics', 'Genotype', 'Geographic Distribution', 'Geographic Locations', 'Geography', 'Goals', 'Grant', 'Health', 'Human', 'Human Genetics', 'Imagery', 'Individual', 'Knowledge', 'Label', 'Libraries', 'Link', 'Manuscripts', 'Maps', 'Methods', 'Output', 'Paper', 'Pattern', 'Pharmacogenomics', 'Phenotype', 'Play', 'Population', 'Population Genetics', 'Preparation', 'Principal Component Analysis', 'Printing', 'Publications', 'Publishing', 'Pythons', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Sample Size', 'Sampling', 'Sampling Studies', 'Shapes', 'Software Tools', 'Source', 'Staging', 'Structure', 'Surface', 'Techniques', 'Time', 'Tissues', 'Uncertainty', 'Variant', 'Work', 'data visualization', 'digital', 'expectation', 'flexibility', 'genetic variant', 'genome browser', 'genome sequencing', 'genome wide association study', 'genomic data', 'interest', 'migration', 'population based', 'tool', 'trait', 'transcriptome sequencing', 'user-friendly']",NCI,UNIVERSITY OF CHICAGO,U01,2016,409700,-0.010678328727359644
"Tools for Visualization of Geographic Structure in Population Genomic Data ﻿    DESCRIPTION (provided by applicant): Large samples sizes are increasingly common in genetics/genomics, particularly in human genetics where sample sizes must be large (>1,000s of individuals) to detect variant associations with complex disease traits. A common feature of data from large samples is that the individuals within the study have varying levels of similarity with one another that can become problematic for downstream analyses (e.g. causing spurious associations) if not understood. Thus uncovering population structure and dissecting it to understand its source is a common and important practice in large-scale studies. Here, we aim to solve challenges for visualizing population structure that regularly arise when researchers interact with large-scale population genomic data sets. In Aim 1 we will develop a software tool for visualizing population structure using principal components analysis (PCA). This tool will make straightforward several steps that are commonly reinvented by data scientists as they analyze PCA outputs from genetic data. It will also make more clear whether PCA analyses may be returning anomalous results. In Aim 2 we will develop a tool for producing geographic allele frequency maps of publicly available or user-generated allele frequency data. In Aim 3 we will develop a visualization approach for displaying geographic regions where populations show unexpectedly high or low levels of differentiation. In Aim 4 we will integrate these pieces of software into a single suite and link them to externally generated data sources and existing genome browsers. By developing these sets of tools we help to remove the need for unnecessary script generation by independent researchers and increase the pace of genomics research. Throughout the project we will pay special attention to developing user-friendly interactive data displays such as those generated by the Data Driven Documents (d3) JavaScript visualization libraries. Where possible we will use simple, yet flexible python backends and provide complementary R libraries to facilitate customizations and integration with existing analysis pipelines. While population genetic applications will motivate our work, the tools we are generating will be generally applicable to other forms of structured biomedical data.        PUBLIC HEALTH RELEVANCE: This project will provide tools for visualizing large-scale genetic data with population structure. While numerous advanced algorithms for summarizing population structure exist, the human interface to the outputs of these methods is lacking and has become a time sink during the analysis of large samples. In this project we will provide user-friendly tools that lower the barrier to understanding genetic variation datasets. In particulr we will develop tools for visualizing compressed representations of genetic variation (i.e. PCA results) and how genetic diversity is distributed across geographic space in a sample.            ",Tools for Visualization of Geographic Structure in Population Genomic Data,8876141,U01CA198933,"['Address', 'Algorithms', 'Alleles', 'Attention', 'Base Sequence', 'Biological', 'Biological Process', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Display', 'Data Set', 'Data Sources', 'Disease', 'Ensure', 'Exhibits', 'Frequencies', 'Funding', 'Gene Frequency', 'Generations', 'Genes', 'Genetic', 'Genetic Drift', 'Genetic Variation', 'Genetic study', 'Genome', 'Genomics', 'Genotype', 'Geographic Distribution', 'Geographic Locations', 'Geography', 'Goals', 'Grant', 'Human', 'Human Genetics', 'Imagery', 'Individual', 'Knowledge', 'Label', 'Libraries', 'Link', 'Manuscripts', 'Maps', 'Methods', 'Output', 'Paper', 'Pattern', 'Pharmacogenomics', 'Phenotype', 'Play', 'Population', 'Population Genetics', 'Preparation', 'Principal Component Analysis', 'Printing', 'Publications', 'Publishing', 'Pythons', 'Quality Control', 'RNA Sequences', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Sample Size', 'Sampling', 'Sampling Studies', 'Scientist', 'Shapes', 'Software Tools', 'Source', 'Staging', 'Structure', 'Surface', 'Techniques', 'Time', 'Tissues', 'Uncertainty', 'Variant', 'Work', 'data visualization', 'digital', 'expectation', 'flexibility', 'genetic variant', 'genome sequencing', 'genome wide association study', 'interest', 'migration', 'population based', 'public health relevance', 'tool', 'trait', 'transcriptome sequencing', 'user-friendly']",NCI,UNIVERSITY OF CHICAGO,U01,2015,360853,-0.010678328727359644

text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"Puerto Rico Testsite for Exploring Contamination Threats (PROTECT) Project Abstract Karst terrains show distinctive surface and subsurface features associated with sinkholes, springs, and caves. Many of the hydrological and hydraulic characteristics associated with these features make groundwater systems in karst areas a significant freshwater resource for human consumption and ecological integrity. Unfortunately, these characteristics also make karst aquifers highly vulnerable to contamination. Karst springs, specifically, provide researchers insight to the interaction between input precipitation, karst aquifers as the transmission medium, and discharge as the output response. A karst catchment area or spring catchment is a mapping unit defined by the surface area and the subsurface drain system that contributes to spring discharge. Therefore, being able to determine this catchment area, and understand how hydrologic conditions in karst environments affect contaminant fate and transport in springs, is crucial to establishing effective source water protection areas (SWPAs) to safeguard groundwater and public health. However, due to the inherent complexity of karst systems, it has been challenging for the PROTECT team to (1) forecast spring response, and (2) infer subsurface properties from surface features in karst environments, since both require the application of advanced computational techniques, such as big data analytics and machine learning (ML) algorithms. Hence, the proposed project for the KC Donnelly Externship award will allow me to train along with Mr. Sean Griffin and his team on the core concepts and models behind geospatial artificial intelligence to automate object detection, object classification, and feature extraction from remotely sensed imagery. Mastering the application of ML algorithms will allow me to discover patterns in my research data, and to construct predictive mathematical models using these discoveries. Per the FOA Project Narrative is not required",Puerto Rico Testsite for Exploring Contamination Threats (PROTECT),10382038,P42ES017198,"['Affect', 'Area', 'Artificial Intelligence', 'Award', 'Big Data Methods', 'Catchment Area', 'Characteristics', 'Classification', 'Computational Technique', 'Consumption', 'Data', 'Detection', 'Environment', 'Fresh Water', 'Human Resources', 'Imagery', 'Modeling', 'Output', 'Pattern', 'Precipitation', 'Public Health', 'Puerto Rico', 'Research', 'Research Personnel', 'Source', 'Surface', 'Surface Properties', 'System', 'Training', 'Water', 'externship', 'feature extraction', 'ground water', 'insight', 'machine learning algorithm', 'mathematical model', 'remote sensing', 'response', 'transmission process']",NIEHS,NORTHEASTERN UNIVERSITY,P42,2021,15000
"Access to parietal action representations after stroke lesions in visual cortex PROJECT SUMMARY  The ability to recognize and use objects according to their function (e.g., fork, hammer, pencil) requires integration of visual, semantic and action knowledge across occipital, temporal and parietal areas. Left parietal regions support critical aspects of object-directed action, such as grasping and object manipulation. This research activity uses a combination of fMRI and behavioral measures in patients with ischemic strokes to early and extrastriate visual areas to test the following hypotheses: Aim 1: There is a visual pathway to the parietal grasp region (aIPS) that bypasses processing in primary visual cortex. Aim 2: Left ventral extrastriate cortex is necessary to access manipulation information for visually presented objects. Aim 3A: Ballistic grasping actions to objects in the hemianopic field are influenced by volumetric properties (size, orientation) of targets. Aim 3B: Left ventral extrastriate lesions impair object function (e.g., `scissors used to cut') and disrupt access to manipulation knowledge from visual input. The research leverages strengths of fMRI (whole brain correlational measure) and neuropsychology (causal inference) to test new hypotheses about vision and action.  `Tools' (i.e., small manipulable objects) are an excellent domain in which to address broader questions about the integration of sensory, motor and cognitive processing. This is because tool recognition and tool use require the integration of distinct sensory, motor and cognitive representations, and the neural substrates of tool processing are well described. The research program emphasizes fresh perspectives on longstanding ideas about the dorsal and ventral visual pathways, by a) undertaking the first systematic investigation of the types of information about objects that are extracted by visual pathways that bypass primary visual cortex, and by b) studying how some parietal areas depend on inputs from the ventral stream in order to access the correct action for a given object. The research activity innovates by testing hypotheses about how lesions at different stages in the cortical visual hierarchy affect downstream processing in parietal cortex, combining neural and behavioral measures to study brain damaged patients (generating causal evidence), and by combining univariate and multivariate measures to `read out' the information content of brain regions (parietal cortex) that are anatomically remote from a lesion. The research advances understanding of how lesions in one brain region disrupt computations in other parts of the brain that depend on the damaged region for their inputs, a phenomenon (`dynamic diaschisis') that applies to brain injury generally. Advancing understanding of these basic issues using causal data has broad implications for understanding how the brain selects the correct action for the correct object, and more generally for theories of conceptual organization and causal reasoning. Understanding how the brain accesses actions from visual input has implications for related fields, such as robotics, neuroprosthetics, and evidenced based approaches for rehabilitating function after brain injury. Project Narrative Functional MRI and behavioral testing are used to test hypotheses about how strokes affecting occipital and temporal cortex disrupt access to action representations in parietal cortex. This research will advance understanding of how the brain processes visual information in support of everyday actions.",Access to parietal action representations after stroke lesions in visual cortex,10130533,R01EY028535,"['Address', 'Affect', 'Alexia', 'Anatomy', 'Area', 'Ballistics', 'Behavioral Assay', 'Brain', 'Brain Injuries', 'Brain region', 'Bypass', 'Cognitive', 'Data', 'Dorsal', 'Eating', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grain', 'Imaging Device', 'Impairment', 'Investigation', 'Ischemic Stroke', 'Knowledge', 'Left', 'Lesion', 'Literature', 'Location', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Motor', 'Neural Pathways', 'Neuropsychology', 'Occipital lobe', 'Parahippocampal Gyrus', 'Parietal', 'Parietal Lobe', 'Participant', 'Pathway interactions', 'Patients', 'Population', 'Process', 'Property', 'Prosopagnosia', 'Reading', 'Research', 'Research Activity', 'Robotics', 'Role', 'Semantics', 'Sensory', 'Stimulus', 'Stream', 'Stroke', 'Structure of supramarginal gyrus', 'Temporal Lobe', 'Testing', 'Vision', 'Visual', 'Visual Cortex', 'Visual Fields', 'Visual Pathways', 'Visual system structure', 'Work', 'area striata', 'base', 'behavior measurement', 'behavior test', 'blind', 'cognitive development', 'evidence base', 'extrastriate', 'extrastriate visual cortex', 'fovea centralis', 'grasp', 'information processing', 'innovation', 'lens', 'multisensory', 'neuroprosthesis', 'post stroke', 'programs', 'relating to nervous system', 'sensory integration', 'theories', 'tool', 'visual information', 'visual motor', 'visual object processing', 'visual process', 'visual processing']",NEI,CARNEGIE-MELLON UNIVERSITY,R01,2021,410316
"Early representation of 3D volumetric shape in visual object processing Project Summary The goal of this project is to test a novel theoretical framework for understanding how the ventral pathway subserves object vision. In the standard framework, a series of neural operations on 2D image data through many intermediate cortical stages, including area V4, leads to high-level perceptual representations, including representation of object identity, at the final stages of the ventral pathway. However, our preliminary microelectrode data from a fixating monkey show that many neurons in V4 represent volumetric (volume- enclosing) 3D shape, not 2D image patterns. These neurons respond to many different 2D images that convey the same 3D shape with different shape-in-depth cues, including shading, reflection, and refraction. They even respond preferentially to random dot stereograms that convey 3D volumetric shape with no 2D cues whatsoever. Moreover, our preliminary results with 2-photon functional imaging in anesthetized monkey V4 show that 3D shape signals are grouped by their similarity, and also group with isomorphic (same outline) 2D shape signals (which could contribute evidence to corresponding 3D shape inferences). We propose to capitalize on these preliminary data by demonstrating the prevalence of 3D shape tuning in area V4, analyzing the 3D shape coding strategies used by these neurons, and measuring how 2D and 3D shape signals are arranged at a microscopic level across the surface of V4. We expect these results to provide strong evidence that extraction of 3D shape fragments is a primary goal of V4 processing. This early extraction of 3D shape information, just two synapses beyond primary visual cortex, would suggest a competing framework for understanding the ventral pathway, in which the initial goal is to represent 3D physical structure, independent of the various 2D image cues used to infer it. In this framework, object recognition would be based on preceding information about 3D physical structure, which would explain why human object recognition is so robust to image changes, in a way that the best computational vision systems are not. The scientific impact of this work would be to divert vision experiments toward understanding representation of real world 3D structure (rather than 2D planar stimuli) and to encourage computational vision scientists to incorporate early 3D shape processing into the deep convolutional network models that are the current state of the art. Narrative This study will help explain how the human brain achieves such remarkably robust perception of visual objects. The results will help guide future rehabilitative and prosthetic strategies for patients with visual impairments, by elucidating neural strategies that could be used to bring computer vision prosthetics up to human performance levels, and by discovering specific neural signals for object information that could be replicated by electrode array implants in higher-level visual cortex.",Early representation of 3D volumetric shape in visual object processing,10173788,R01EY029420,"['3-Dimensional', '3D world', 'Area', 'Biological', 'Brain', 'Calcium Signaling', 'Code', 'Computer Vision Systems', 'Cues', 'Data', 'Electrodes', 'Functional Imaging', 'Future', 'Genetic Programming', 'Glass', 'Goals', 'Human', 'Image', 'Implant', 'Lead', 'Learning', 'Measures', 'Medial', 'Microelectrodes', 'Microscopic', 'Microscopy', 'Modeling', 'Monkeys', 'Network-based', 'Neurons', 'Ocular Prosthesis', 'Pathway interactions', 'Patients', 'Pattern', 'Performance', 'Population', 'Prevalence', 'Prosthesis', 'Rehabilitation therapy', 'Scientist', 'Series', 'Shapes', 'Signal Transduction', 'Stimulus', 'Structure', 'Surface', 'Synapses', 'System', 'Testing', 'Texture', 'Three-dimensional analysis', 'Time', 'V4 neuron', 'Vision', 'Visual Cortex', 'Visual Perception', 'Visual impairment', 'Work', 'area V4', 'area striata', 'base', 'convolutional neural network', 'experimental study', 'individual response', 'network models', 'neurotransmission', 'nonhuman primate', 'novel', 'object perception', 'object recognition', 'operation', 'relating to nervous system', 'response', 'three dimensional structure', 'two-photon', 'virtual reality', 'visual object processing']",NEI,JOHNS HOPKINS UNIVERSITY,R01,2021,532112
"Towards a Compositional Generative Model of Human Vision Understanding object recognition has long been a central problem in vision science, because of its applied utility and computational difficulty. Progress has been slow, because of an inability to process complex natural images, where the largest challenges arise. Recently, advances in Deep Convolutional Neural Networks (DCNNs) spurred unprecedented success in natural image recognition. The general goal of this proposal is to leverage this success to test computational theories of human object recognition in natural images. However, DCNNs still markedly underperform humans when challenged with high levels of ambiguity, occlusion, and articulation. We hypothesize that humans' superior performance arises from the use of knowledge about how images and objects are structured. Preliminary evidence for this claim comes from the success of hybrid models, that combine DCNNS for identifying features and parts in images, with explicit knowledge of object and image structure. These computations occur within a hierarchy, which includes both top-down and bottom- up processing. The specific goal of the work proposed here is to strongly test whether these computational strategies, structured, hierarchical representations and bidirectional processing, are used to recognize objects in natural images. Human bodies are composed of hierarchically organized configurable parts, making them an ideal test domain. We examine the complete recognition process, from parts, to pairs of parts, to whole bodies, each in its own aim. Each aim also tests important sub-hypotheses about when and how the computational strategies are used. Aim 1 examines recognition of individual body parts, testing whether it is dependent on parsing images into more basic features and relationships, for example edges and materials. Aim 2 examines pairs of parts, testing the importance of knowledge of body connectedness relationships. Aim 3 examines perception of entire bodies, testing whether knowledge of global body structure guides bidirectional processing. In each aim, we first develop nested computer vision models that either do or do not make use of structural knowledge, to test whether it aids recognition. We then test whether human performance can be accounted for by the availability of that structural knowledge. We next measure neural activity with functional MRI to identify where and how it is used in cortex. Finally, we integrate these results to produce even stronger tests, using the nested models to predict human performance and confusion matrices as well as fMRI activity levels and confusion matrices. Altogether, this work will strongly test key theoretical accounts of object recognition in the most important domain, perception of natural images. The work, based on extensive preliminary data, measures and models the entire body recognition system. The models developed and tested here should surpass the state-of-the-art, and be useful for many real-world recognition tasks. The proposal will also lay the groundwork for future studies of recognition impaired by disease. This research uses computational, behavioral, and brain imaging methods to investigate how the visual system represents and processes information about human bodies. The studies will reveal how and when people can accurately recognize objects in natural images, how the brain supports this function, and how loss of information, similar to that that accompanies visual disease, may affect the ability to interpret everyday scenes.",Towards a Compositional Generative Model of Human Vision,10228003,R01EY029700,"['Affect', 'Area', 'Articulation', 'Behavioral', 'Body Image', 'Body part', 'Brain', 'Brain imaging', 'Complex', 'Computer Vision Systems', 'Confusion', 'Cues', 'Data', 'Development', 'Disease', 'Elbow', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Human', 'Human body', 'Hybrids', 'Image', 'Impairment', 'Individual', 'Knowledge', 'Link', 'Measures', 'Modeling', 'Perception', 'Performance', 'Predictive Value', 'Process', 'Psychophysics', 'Published Comment', 'Research', 'Structure', 'System', 'Testing', 'Training', 'Vision', 'Visual', 'Visual system structure', 'Work', 'Wrist', 'base', 'convolutional neural network', 'crowdsourcing', 'human model', 'imaging modality', 'improved', 'object recognition', 'relating to nervous system', 'spatial relationship', 'success', 'theories', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2021,324460
"Infants' self-generated visual statistics support object and category learning PROJECT SUMMARY Human visual object recognition is remarkable in its ability to recognize individual objects in challenging circumstances and to rapidly recognize even novel instances of tens of thousands of everyday categories. Although a great deal is known about these processes at maturity, very little is known about their development especially with respect to common everyday objects and the experiences that support robust object recognition and categorization. This gap is critical because object recognition and categorization support early word learning, physical problem solving, and the later learning of orthographies and mathematical symbols. This research projects focuses on visual object learning in 1 year old toddlers, a developmental period that at the front end of marked advances in visual object recognition and a period in which children with multiple risk factors begin to fall behind the normative developmental trajectory. The approach focuses on the properties of real- world visual experiences that support learning to recognize individual objects in challenging visual contexts and generalizing that learning to same category members. The method uses head-mounted eye-trackers to capture field-of-view images from 100 infants 17 to 22 months of age as they spontaneously interact and play with objects. Through active interactions with objects infants generates their own packets of visual data for learning. Multiple visual properties relevant to object perception will be algorithmically measured and quantified. Toddlers’ recognition of the actively-engaged object and a novel object from the same category will be measured in challenging benchmark contexts including clutter, occlusion, and different views. Category generalization will be measured in a name generalization task. Advanced statistics and machine learning will determine the visual properties of self-generated experiences that support infants object recognition and categorization. The research will provide the first characterization of the natural visual statistics of toddlers’ active interactions with objects and potentially transformative evidence that the developmental foundation for human prowess in visual object categorization lies not in experiences with many different instances of a single category, the standard assumption, but in active visual experiences with individual objects. Moreover, infants at risk for Developmental Language Delay and Autism show disruptions in early object name learning that have been recently linked to disruptions in visual learning about objects. The project includes preliminary analyses of infants at risk in preparation for the next step in the long-term research program. PROJECT NARRATIVE The proposed research is relevant to public health because visual object recognition and learning is implicated in broad array of cognitive tasks in early the learning of first words, reading and learning the symbol systems of mathematic; the continued risks of children treated for amblyopia and strabismus in perceptual, cognitive and school achievements and the nexus of disrupted visual learning and lexical development in children with Developmental Language Delay and Autism. Understanding infants self-generated active visual experiences is expected to give rise to new diagnostics and behavioral interventions for at-risk infants. Thus, this proposal is relevant to NIH’s mission that pertains to fostering fundamental discoveries and innovative research strategies as a basis for ultimately protecting child health.",Infants' self-generated visual statistics support object and category learning,10368173,R01HD104624,"['1 year old', 'Achievement', 'Age-Months', 'Algorithms', 'Amblyopia', 'Behavior', 'Behavior Therapy', 'Benchmarking', 'Canis familiaris', 'Categories', 'Child', 'Child Health', 'Cognitive', 'Data', 'Development', 'Eye', 'Flowers', 'Fostering', 'Foundations', 'Free Will', 'Gap Junctions', 'Geometry', 'Goals', 'Head', 'Health', 'Human', 'Image', 'Individual', 'Infant', 'Language Delays', 'Learning', 'Link', 'Literature', 'Machine Learning', 'Mathematics', 'Measures', 'Methods', 'Mission', 'Motor', 'Movement', 'Names', 'Nature', 'Orthography', 'Pathway interactions', 'Perception', 'Play', 'Preparation', 'Problem Solving', 'Process', 'Property', 'Public Health', 'Reading', 'Reporting', 'Research', 'Research Project Grants', 'Risk', 'Risk Factors', 'Role', 'Schools', 'Sensory', 'Shapes', 'Specific qualifier value', 'Strabismus', 'System', 'Testing', 'Time', 'Toddler', 'United States National Institutes of Health', 'Variant', 'Vision', 'Visual', 'Visual system structure', 'Work', 'aged', 'autism spectrum disorder', 'cognitive task', 'experience', 'falls', 'infancy', 'innovation', 'interest', 'lexical', 'member', 'novel', 'novel diagnostics', 'object perception', 'object recognition', 'programs', 'sensory input', 'statistics', 'success', 'visual cognition', 'visual information', 'visual learning', 'visual object processing', 'word learning']",NICHD,INDIANA UNIVERSITY BLOOMINGTON,R01,2021,645832
"Sensory mechanisms of manual dexterity and their application to neuroprosthetics PROJECT SUMMARY Manual behavior requires sensory signals from the hand, both tactile and proprioceptive, as evidenced by the severe deficits that result from somatosensory deafferentation. Three aspects of the sensory component of hand sensory function are poorly understood. First, the neural basis of touch has been studied almost exclusively with stimuli delivered passively to the skin, precluding any understanding of how tactile signals are modulated by and interact with motor commands. Second, proprioceptive signals carry information not only about the time-varying conformation of the hand, but also about manually applied forces, but proprioceptive representations of force are poorly understood. Third, stereognosis – the sense of the three-dimensional shape of objects acquired from sensory signals arising from the hand – implies the integration of tactile and proprioceptive signals, a process about which little is known. The study of active touch, hand proprioception, and stereognosis has been hindered by technical obstacles. Indeed, characterizing self-generated contact with objects has been difficult or impossible, as has tracking hand movements with sufficient precision. To overcome these obstacles, my team has developed an apparatus that allows us to measure contact events – with a sensor sheet covering the object’s surface – and track time-varying hand postures – using deep learning-based computer vision – with unprecedented precision as animals interact with objects. We then characterize the responses at every stage along the somatosensory neuraxis, from peripheral nerve through cortex. This novel experimental set up will allow us to study the neural basis of somatosensation – particularly as it relates to manual dexterity – under ecologically valid conditions. In a related line of inquiry, we leverage what we learn about sensory processing to restore the sense of touch to bionic hands. In brief, we develop algorithms to convert the output of sensors on the bionic hand into patterns of electrical stimulation of the peripheral nerve (for amputees) or of somatosensory cortex (for people with tetraplegia) to evoke meaningful tactile percepts. I am one of the principal architects of the biomimetic approach to artificial touch, which posits that encoding algorithms that mimic natural neural signals will give rise to more intuitive tactile percepts, thereby endowing bionic hands with greater dexterity. Our work on artificial touch comprises three components: evaluation of the perceptual correlates of electrical stimulation, development of sensory encoding algorithms, and assessment of the benefits of artificial touch to manual behavior. The interplay of the basic scientific results and neural engineering efforts will result in more naturalistic artificial touch for brain- controlled bionic hands. PROJECT NARRATIVE We seek to understand the neural basis of somatosensation – touch and proprioception – during manual interactions with objects. To this end, we measure time-varying limb postures, forces exerted on sensorized objects, and neuronal responses across the entire somatosensory neuraxis, as monkeys grasp objects whose size, shape, and location is under experimenter control. We then leverage what we have learned about somatosensory coding under active conditions to develop ever more intuitive ways to convey sensory feedback to bionic hands via electrical interfaces with somatosensory cortex.",Sensory mechanisms of manual dexterity and their application to neuroprosthetics,10240106,R35NS122333,"['3-Dimensional', 'Algorithms', 'Amputees', 'Animals', 'Behavior', 'Biomimetics', 'Bionics', 'Brain', 'Code', 'Computer Vision Systems', 'Deafferentation procedure', 'Development', 'E-learning', 'Electric Stimulation', 'Engineering', 'Evaluation', 'Event', 'Hand', 'Intuition', 'Learning', 'Limb structure', 'Location', 'Manuals', 'Measures', 'Molecular Conformation', 'Monkeys', 'Motor', 'Movement', 'Neuraxis', 'Neurons', 'Output', 'Pattern', 'Peripheral Nerve Stimulation', 'Peripheral Nerves', 'Posture', 'Process', 'Proprioception', 'Quadriplegia', 'Sensory', 'Shapes', 'Signal Transduction', 'Skin', 'Somatosensory Cortex', 'Stereognosis', 'Stimulus', 'Surface', 'Tactile', 'Time', 'Touch sensation', 'Work', 'deep learning', 'dexterity', 'grasp', 'neuroprosthesis', 'neurotransmission', 'novel', 'object shape', 'relating to nervous system', 'response', 'sensor', 'sensory feedback', 'sensory mechanism', 'somatosensory']",NINDS,UNIVERSITY OF CHICAGO,R35,2021,1094647
"Models and methods for automatically measuring disease body-wide and staging disease via FDG-PET/CT in Lymphoma Quantitative Radiology holds great promise to transform our ability to diagnose, monitor, stage, prognosticate, and detect diseases as well as to plan and guide patient therapeutic interventions. However, the process of locating and delineating anatomic organs and pathologic regions in medical images, known as image segmentation, at a high level of automation has remained a major hurdle to these advances. Most developments on image segmentation have focused on a specific organ or a small group of objects in a specific body region. A new method or a major adaptation of an existing method is engineered when any of these parameters changed. Such an approach is not sustainable and becomes a stumbling block when dealing with whole-body systemic diseases where body-wide image analytics is required. A critical advance is needed in this field to overcome two main challenges: (1) Although prior information about normal anatomy is deemed vital for image segmentation and analysis, its creation and utilization body-wide on a massive scale have not been attempted and are sorely lacking. (2) Techniques to employ such information and methods for body-wide disease quantification at high levels of automation do not exist. The overarching goal is to overcome these challenges by developing a body-wide and generalizable anatomy-guided deep learning image segmentation methodology and demonstrate its application in the study of patients with diffuse large B cell lymphoma (DLBCL) for which PET-based staging and response assessment are of paramount importance. The project has three specific aims. Aim1: To develop a family of body-wide anatomy models representing the entire human adult age spectrum. Existing FDG PET/CT scans of 600 patients from two institutions (Penn and New York Proton Center) covering 10 age groups will be utilized to build anatomy models involving 50 organs and 50 lymph node zones in the extended body torso including neck, thorax, abdomen, and pelvis. A family of 40 anatomy models representing the 4 body regions and 10 age groups will be created from roughly 60,000 3D object samples. Aim2: To develop, implement, and validate a methodology for localizing objects and to quantify disease without explicitly delineating organs and lesions. Gender- and age-specific anatomy models will be utilized for automatically locating the above 100 objects in any given patient PET/CT image and to quantify disease in each body region, organ, and lymph node zone. The methods will be tested on 400 PET/CT images of DLBCL patients. Aim3: To develop and validate an automated method of DLBCL disease staging and prognosis. The disease quantity information will be utilized to develop automated staging and outcome prediction algorithms which will be tested on the above 400 cases in comparison to current clinical methods. Two key outcomes of this project will be: an unprecedented well-curated database of body-wide images, segmented objects, and family of models; and a validated methodology for automatic body-wide disease quantification and disease staging in DLBCL. Most current methods for the fundamental image analysis task of object segmentation have focused on a small group of objects and were designed to operate on a specific imaging modality - an approach that is not sustainable and becomes a stumbling block when dealing with whole-body systemic diseases where body- wide image analysis is called for. The main goal of this project is to develop a body-wide and generalizable image segmentation and analytics methodology that is based on creating a comprehensive family of body-wide anatomic models covering the entire adult human age spectrum and performing disease quantification without explicitly delineating objects. The project will apply this methodology to the study of patients with diffuse large B cell lymphoma (DLBCL), a common subtype of lymphoma, for which PET-based staging and response assessment/ prediction is of paramount importance clinically.",Models and methods for automatically measuring disease body-wide and staging disease via FDG-PET/CT in Lymphoma,10296059,R01CA255748,"['3-Dimensional', 'Abdomen', 'Adult', 'Affect', 'Age', 'Aging', 'Agreement', 'Anatomic Models', 'Anatomy', 'Automation', 'Body Burden', 'Body Regions', 'Chest', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Engineering', 'Family', 'Female', 'Gender', 'Geography', 'Goals', 'Image', 'Image Analysis', 'Individual', 'Institution', 'Knowledge', 'Lesion', 'Lymphoma', 'Manuals', 'Measurement', 'Measures', 'Medical Imaging', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Names', 'Neck', 'New York', 'Normalcy', 'Organ', 'Outcome', 'PET/CT scan', 'Pathologic', 'Pathology', 'Patients', 'Pelvis', 'Performance', 'Population', 'Positron-Emission Tomography', 'Process', 'Prognosis', 'Protons', 'Radiology Specialty', 'Reproducibility', 'Sampling', 'Scanning', 'Specificity', 'Staging', 'Standardization', 'Systemic disease', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'X-Ray Computed Tomography', 'age group', 'base', 'burden of illness', 'deep learning', 'design', 'fluorodeoxyglucose positron emission tomography', 'imaging Segmentation', 'imaging modality', 'improved', 'innovation', 'large cell Diffuse non-Hodgkin&apos', 's lymphoma', 'learning strategy', 'lymph nodes', 'male', 'object recognition', 'outcome prediction', 'predict clinical outcome', 'predicting response', 'prediction algorithm', 'quantitative imaging', 'response', 'task analysis', 'theories', 'virtual']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2021,647411

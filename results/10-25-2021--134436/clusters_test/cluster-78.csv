text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes The trained ear of the speech-language pathologist is the gold standard assessment tool for clinical practice in motor speech disorders. However, perceptual judgments are vulnerable to bias and their relationship with estimates of listener intelligibility – the final arbiter of speech goodness – is indeterminate. Interpretable, objective, and robust outcome measures that provide targets for treatment are urgently needed in order to provide more precise care and reliably monitor patient progress. Based on theoretical models of speech perception, in our previous grants we have developed a novel set of outcome measures that provide a multi- dimensional intelligibility profile (MIP) by using custom speech stimuli and a new coding strategy that allows us to capture the types of errors that listeners make when listening to dysarthric speech. This has led to a more complete intelligibility profile that codifies these errors at different levels of granularity, from global to discrete. Simultaneously, we have also developed a computational model for evaluation of dysarthric speech capable of reliably estimating a limited set of intelligibility measures directly from the speech acoustics. To date, both the outcome measures and the objective model have been evaluated on cross-sectional data only. In this renewal application, our principal goal is to evaluate specific hypotheses regarding expected changes in this multidimensional intelligibility profile as a result of different intervention instruction conditions (loud, clear, slow). A secondary goal of the proposal is to further refine our objective model to predict the complete intelligibility profile and to evaluate its ability to detect intelligibility changes within individual speakers. This is critical for clinicians who currently have no objective ways to assess the value of their interventions. With the aim of improving the standard of care through technology, the long-term goal of this proposal is to develop stand-alone objective outcome measures for dysarthria that can provide clinicians with reliable treatment targets. Such applications have the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury. Furthermore, these applications also have the potential to reduce health disparities by partially automating clinical intervention and providing easier access to these services to those in remote areas or in underdeveloped countries.   There is an urgent need in the field of speech-language pathology for objective outcome measures of speech intelligibility that provide clinicians with actionable information regarding treatment targets. This proposal seeks to leverage theoretical advances in speech intelligibility to evaluate the sensitivity of a novel multidimensional intelligibility profile that quantifies the perceptual effects of speech change. Using listener transcriptions of dysarthric speech, along with a suite of automated acoustic metrics, the predictive model uses machine-learning algorithms to learn the relationship between speech acoustics and listener percepts. Ultimately, this model will allow clinicians to predict the outcomes of an intervention strategy to assess its utility for a patient. This has the potential to dramatically alter the current standard of care in speech pathology for patients with neurological disease or injury.",Perception of dysarthric speech: An objective model of dysarthric speech evaluation with actionable outcomes,10087511,R01DC006859,"['Acoustics', 'Adopted', 'Affect', 'Area', 'Attention', 'Caring', 'Clinical', 'Clinical Assessment Tool', 'Code', 'Cognitive', 'Communication impairment', 'Complex', 'Computer Models', 'Country', 'Cues', 'Custom', 'Data', 'Dimensions', 'Disease Progression', 'Dysarthria', 'Ear', 'Educational Intervention', 'Evaluation', 'Frequencies', 'Genetic Transcription', 'Goals', 'Gold', 'Grant', 'Health Services Accessibility', 'Individual', 'Instruction', 'Intervention', 'Judgment', 'Knowledge', 'Language', 'Learning', 'Loudness', 'Measures', 'Modeling', 'Motor', 'Nervous System Trauma', 'Noise', 'Outcome', 'Outcome Measure', 'Participant', 'Pathologist', 'Patient Monitoring', 'Patients', 'Pattern', 'Perception', 'Periodicity', 'Population', 'Process', 'Research', 'Sampling', 'Severities', 'Signal Transduction', 'Source', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Intelligibility', 'Speech Pathology', 'Speech Perception', 'Speech-Language Pathology', 'Stimulus', 'Stream', 'Technology', 'Testing', 'Theoretical model', 'Time', 'Training', 'Update', 'Validation', 'Work', 'base', 'clinical practice', 'health disparity', 'improved', 'lexical', 'machine learning algorithm', 'nervous system disorder', 'novel', 'optimal treatments', 'outcome prediction', 'phrases', 'predictive modeling', 'recruit', 'signal processing', 'speech in noise', 'standard of care', 'tool']",NIDCD,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2021,305399
"Speech segregation to improve intelligility of reverberant-noisy speech Project Summary Hearing loss is one of the most prevalent chronic conditions, affecting 37.5 million Americans. Although signal amplification in modern hearing aids makes sound more audible to hearing impaired listeners, speech understanding in background interference remains the biggest challenge by hearing aid wearers. The proposed research seeks a monaural (one-microphone) solution to this challenge by developing supervised speech segregation based on deep learning. Unlike traditional speech enhancement, deep learning based speech segregation is driven by training data, and three components of a deep neural network (DNN) model are features, training targets, and network architectures. Recently, deep learning has achieved tremendous successes in a variety of real world applications. Our approach builds on the progress made in the PI's previous R01 project which demonstrated, for the first time, substantial speech intelligibility improvements for hearing-impaired listeners in noise. A main focus of the proposed work in this cycle is to combat room reverberation in addition to background interference. The proposed work is designed to achieve three specific aims. The first aim is to improve intelligibility of reverberant-noisy speech for hearing- impaired listeners. To achieve this aim, we will train DNNs to perform time-frequency masking. The second aim is to improve intelligibility of reverberant speech in the presence of competing speech. To achieve this aim, we will perform DNN training to estimate two ideal masks, one for the target talker and the other for the interfering talker. The third aim is to improve intelligibility of reverberant speech in combined speech and nonspeech interference. To achieve this aim, we will develop a two-stage DNN model where the first stage will be trained to remove nonspeech interference and the second stage to remove interfering speech. Eight speech intelligibility experiments involving both hearing-impaired and normal-hearing listeners will be conducted to systematically evaluate the developed system. The proposed project is expected to substantially close the speech intelligibility gap between hearing-impaired and normal-hearing listeners in daily conditions, with the ultimate goal of removing the gap altogether. Relevance A widely acknowledged deficit of hearing loss is reduced intelligibility of reverberant-noisy speech. How to improve speech intelligibility of hearing impaired listeners in everyday environments is a major technical challenge. This project directly addresses this challenge and the results from the project are expected to yield technical methods that can be translated to hearing prosthesis, potentially benefiting millions of individuals with hearing loss.",Speech segregation to improve intelligility of reverberant-noisy speech,10064139,R01DC012048,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'American', 'Auditory', 'Auditory Prosthesis', 'Chronic', 'Complex', 'Data', 'Environment', 'Formulation', 'Frequencies', 'Goals', 'Hearing', 'Hearing Aids', 'Individual', 'Investigation', 'Laboratories', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Network-based', 'Neural Network Simulation', 'Noise', 'Recurrence', 'Research', 'Signal Transduction', 'Source', 'Speech', 'Speech Intelligibility', 'Structure', 'Supervision', 'Surface', 'Symptoms', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'combat', 'deep learning', 'deep neural network', 'design', 'digital', 'experimental study', 'hearing impairment', 'improved', 'innovation', 'microphone', 'network architecture', 'normal hearing', 'real world application', 'segregation', 'signal processing', 'sound', 'speech in noise', 'success', 'supervised learning']",NIDCD,OHIO STATE UNIVERSITY,R01,2021,301954
"Functional and computational characterization of the human auditory cortex According to NIDCD, 6 to 8 million people in the United States have some form of speech or communication disorder. Speech perception requires a listener to map variable acoustic signals onto a finite set of phonological categories known as phonemes, and to integrate those categories over time to form larger linguistic units such as syllables and words. It remains speculative where these different speech features are encoded and what cortical computations are needed for their calculation from an acoustic signal. A better understanding of what neural circuits are involved, how they are organized, and what computations they perform to support speech comprehension is critical for developing a detailed neurobiological model of speech perception. The major aim of this proposal is to use a joint framework to study the encoding of acoustic and linguistic features and the computational underpinnings of natural speech processing, using invasive surface and depth electrodes implanted in human neurosurgical patients. To study the cortical organization of acoustic features, we will characterize the encoding and anatomical organization of acoustic features in auditory cortical regions. To study the cortical organization of linguistic features, we will measure the encoding of phonetic, phonotactic, and semantic information using multivariate linear regression. To understand the underlying computational mechanisms, we will train convolutional neural network models to predict the neural responses to speech and use a novel method to express their computation as a set of linear transforms. By interpreting these models, we will uncover nonlinear computations used in different auditory areas and relate them to the encoding of acoustic and linguistic features. These complementary analyses will extend our knowledge of speech processing in the human auditory cortex and lead to new hypotheses about the mechanisms of various speech and language disorders. Together, the proposed research will greatly improve the current models of cortical speech processing, which are of great interest in many disciplines including neurolinguistics, speech pathology, speech prostheses, and speech technologies. Speech and language disorders are major health issues. Speech perception requires a listener to compute linguistic units from variable acoustic signals, and where and how these computations happen in the human auditory cortex remains speculative. Using invasive human electrophysiology, we propose to study the neural encoding and computational underpinnings of the acoustic and linguistic features that enable speech perception; investigating this process in various auditory areas at high resolution will extend our knowledge of human speech perception and produce new insights into the mechanisms of speech and language disorders.",Functional and computational characterization of the human auditory cortex,10225600,R01DC014279,"['Acoustics', 'Address', 'Anatomy', 'Animal Model', 'Aphasia', 'Area', 'Auditory', 'Auditory Perception', 'Auditory area', 'Brain', 'Categories', 'Characteristics', 'Communication impairment', 'Comprehension', 'Computer Models', 'Discipline', 'Dyslexia', 'Electrocorticogram', 'Electroencephalography', 'Electrophysiology (science)', 'Frequencies', 'Health', 'Human', 'Impairment', 'Implanted Electrodes', 'Intuition', 'Joints', 'Knowledge', 'Language Development', 'Language Disorders', 'Lead', 'Linear Regressions', 'Linguistics', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Neural Network Simulation', 'Neurobiology', 'Patients', 'Process', 'Property', 'Prosthesis', 'Research', 'Resolution', 'Response Latencies', 'Semantics', 'Signal Transduction', 'Site', 'Specificity', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Pathology', 'Speech Perception', 'Stimulus', 'Superior temporal gyrus', 'Surface', 'Technology', 'Time', 'To specify', 'Training', 'United States', 'Work', 'auditory processing', 'auditory stimulus', 'convolutional neural network', 'cortex mapping', 'improved', 'innovation', 'insight', 'interest', 'neural circuit', 'neural network', 'neurophysiology', 'neurotransmission', 'nonhuman primate', 'novel', 'phonology', 'predicting response', 'receptive field', 'relating to nervous system', 'response', 'sound', 'speech processing', 'temporal measurement']",NIDCD,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2021,540899
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases ﻿    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand. PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9841303,R01DC014498,"['3-Dimensional', 'Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'American Sign Language', 'Articulation', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Databases', 'Detection', 'Devices', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Human', 'Image', 'Individual', 'Intervention', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deaf', 'deafness', 'design', 'experience', 'experimental study', 'face perception', 'innovation', 'instructor', 'interest', 'machine learning algorithm', 'prevent', 'public health relevance', 'reconstruction', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2021,317844
"Identifying the neural structures and dynamics that regulate phonological structure The systematic patterning of language is a fundamental property of cognition. One aspect of this patterning, constraints on the combination of speech sounds to form words (phonotactic structure), has been implicated in constraining diverse processes related to language acquisition, perception, and production, bilingual language use, memory and even influences non-linguistic processes including the memorability of novel words and consumer reaction to novel brand names. This patterning changes in a variety of common acquired and developmental communication disorders. We will explore two explanations for these effects. One, developed in linguistic theory, argues that language users discover a set of abstract, language-specific rules or constraints that shape language use. The other view, developed in connectionist and dynamic systems theory, argues that phonotactic constraints emerge from top-down lexical influences on speech perception. Discriminating between these approaches is difficult because both explain behavioral data well. It is essential to discriminate between these accounts for two reasons. This question offers an excellent opportunity to resolve the debate over whether abstract linguistic rules/constraints create or simply describe the patterning of language. The resolution of this question has fundamental implications for the way linguistic formalism and connectionist simulations relate to human processing. At a more immediate level, this research offers the opportunity to identify a common core mechanism (either the leveraged use of abstract linguistic rules or top-down lexical influences) that explains and unites diverse linguistic and cognitive phenomena. Past efforts to resolve these issues have failed because of fundamental inferential limitations of behavioral and BOLD imaging paradigms. Accordingly, we have developed new tools and research strategies that allow us to identify patterns of directed interaction between brain regions (effective connectivity), and use these analyses to draw much stronger inferences about the dynamic processes that shape cognition. Observers in the field have argued that our methods have already provided “definitive” evidence to resolve the decades old debate over the role of top- down processes in speech processing. This proposal would extend those methods, and introduce innovative neural decoding analyses that we will use to characterize the categories (e.g. rules, words, abstract phonological representations needed to support rule application) that are encoded in localized brain activity. Using these methods, we will determine whether top-down lexical processes that we have shown produce phonotactic phenomena related to the processing of patterns that occur in speaker's language generalize to unfamiliar patterns. We will also use them to identify the substrates of rule- versus word-mediated processing, to provide a baseline for interpreted the representations and dynamic processes that support phonotactic effects in natural language processing. The human ability to learn and use language is the result of a complex set of dynamic brain processes that can break down as the result of disease, injury or developmental disorders. This work uses new non-invasive techniques to observe and understand some of the most fundamental brain processes that shape language learning and use so that we may better understand how they break down. This understanding should one day guide the development of better ways to treat diverse language disorders.",Identifying the neural structures and dynamics that regulate phonological structure,10146334,R01DC015455,"['Address', 'Affect', 'Algebra', 'Behavioral', 'Brain', 'Brain region', 'Categories', 'Cognition', 'Cognitive', 'Common Core', 'Communication impairment', 'Competence', 'Complex', 'Data', 'Development', 'Developmental Communication Disorders', 'Disease', 'Evaluation', 'Frequencies', 'Goals', 'Heart', 'Human', 'Image', 'Imaging Techniques', 'Injury', 'Intuition', 'Judgment', 'Knowledge', 'Laboratories', 'Language', 'Language Development', 'Language Disorders', 'Learning', 'Linguistics', 'Mediating', 'Mediation', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Neighborhoods', 'Pathology', 'Pattern', 'Perception', 'Performance', 'Process', 'Production', 'Property', 'Reaction', 'Research', 'Resolution', 'Role', 'Shapes', 'Speech', 'Speech Perception', 'Speech Sound', 'Structure', 'Structure of supramarginal gyrus', 'Systems Theory', 'Techniques', 'Testing', 'Work', 'bilingualism', 'developmental disease', 'dynamic system', 'experience', 'innovation', 'lexical', 'lexical processing', 'models and simulation', 'novel', 'phonology', 'relating to nervous system', 'simulation', 'sound', 'spatiotemporal', 'speech processing', 'theories', 'tool', 'word learning']",NIDCD,MASSACHUSETTS GENERAL HOSPITAL,R01,2021,427761
"Training program in the neurology of language and neurodegenerative aphasias This K24 grant renewal is requested to support the mentoring activities of Dr. Maria Luisa Gorno Tempini, a behavioral neurologist and cognitive neuroscientist. Dr. Gorno Tempini is a leading researcher and clinician in the fields of neurology and the neuroscience of language, with specific expertise in atypical neurodegenerative diseases that present as speech and language disorders. She mentors a talented group of clinicians and researchers to study and treat Primary Progressive Aphasia (PPA) and investigate the neural basis of language. Dr. Gorno Tempini has a proven track record of success in mentoring both clinicians and scientists in patient- oriented research (POR) related to speech and language disorders. The goals of this proposal are to support time for Dr. Gorno Tempini to mentor a growing number of increasingly diverse clinicians and researchers interested in neurodegenerative language disorders, to extend her impact as a mentor at UCSF, and to pursue novel lines of research in PPA. Leveraging the resources available at the UCSF Memory and Aging Center (MAC), the Global Brain Health Institute, and the UCSF Clinical and Translational Science Institute, she will mentor clinicians, faculty, postdoctoral scholars, and scientists with particular attention to cultivating diversity, equity, and inclusion in her laboratory, the university, and the profession at large. She will sustain her comprehensive mentoring system that features one-on-one mentoring, supervision of clinicians in patient diagnosis and care, and group mentoring activities. She will update and extend that system with opportunities for training with colleagues in speech and data science and a focus on mentoring more advanced junior colleagues as emerging mentors themselves. Dr. Gorno Tempini's experience in basic cognitive neuroscience and neurological methodologies and her increasingly nuanced leadership expertise uniquely position her to benefit a diverse group of trainees. The research project proposed here will develop automatic, efficient, and objective methods to measure speech production in PPA. The study will apply acoustic analysis scripts and automatic speech recognition to five minutes of audio recordings collected from 500 well-characterized PPA patients and employ dynamic speech MRI to identify anatomical biomarkers of vocal tract movements corresponding to speech articulation errors. This will lead to earlier detection of speech deficits; improved characterization of clinical syndromes; and more detailed, objective, and efficient outcome measures to track progression and, in the future, response to treatment. Machine learning analyses of acoustic measures and imaging data collected through other MAC projects will provide novel tools for prediction of post-mortem pathology and new knowledge on the neural basis of speech. This grant will be instrumental in supporting the pursuit of Dr. Gorno Tempini's goals to mentor the next generation of clinician scientists in the neurology of language and neurodegenerative aphasias. In our previous K24 cycle, Dr Gorno Tempini expanded and formalized her POR mentoring and undertook novel research into the susceptibility of the language system to neurodegeneration. This proposed renewal will train the next generation of clinician scientists in language disorders and neurodegenerative disease through didactics, group, and one on one mentoring sessions. The research project will develop novel automated and imaging measures of speech production impairments in PPA.",Training program in the neurology of language and neurodegenerative aphasias,10216095,K24DC015544,"['Acoustics', 'Aging', 'Air', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anomia', 'Anterior', 'Aphasia', 'Area', 'Articulation', 'Atrophic', 'Attention', 'Autopsy', 'Basic Science', 'Behavior Therapy', 'Behavioral', 'Biological Markers', 'Biomechanics', 'Brain', 'Caring', 'Cessation of life', 'Characteristics', 'Clinical', 'Clinical Sciences', 'Cognitive', 'Communication', 'Corpus striatum structure', 'Data', 'Data Science', 'Dementia', 'Diagnosis', 'Differential Diagnosis', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Enrollment', 'Evaluation', 'Faculty', 'Frontotemporal Lobar Degenerations', 'Functional Imaging', 'Future', 'Goals', 'Grant', 'Image', 'Impairment', 'Institutes', 'Knowledge', 'Laboratories', 'Language', 'Language Disorders', 'Leadership', 'Learning', 'Left', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measurement', 'Measures', 'Memory', 'Mentors', 'Methodology', 'Methods', 'Molecular', 'Monitor', 'Motor', 'Movement', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Neurologic', 'Neurologist', 'Neurology', 'Neurosciences', 'Outcome Measure', 'Parietal', 'Pathologic', 'Pathologist', 'Pathology', 'Patients', 'Pattern', 'Peer Review', 'Pharmacologic Substance', 'Pharmacological Treatment', 'Phenotype', 'Positioning Attribute', 'Predisposition', 'Primary Progressive Aphasia', 'Production', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Sampling', 'Scientist', 'Semantics', 'Severities', 'Shapes', 'Site', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Sound', 'Structure', 'Supervision', 'Syndrome', 'System', 'Talents', 'Techniques', 'Technology', 'Time', 'Tissues', 'Training', 'Training Programs', 'Translational Research', 'Universities', 'Update', 'Variant', 'automated analysis', 'automated speech recognition', 'brain circuitry', 'brain health', 'cognitive neuroscience', 'cohort', 'constriction', 'diversity and equity', 'experience', 'improved', 'innovation', 'interest', 'machine learning algorithm', 'molecular pathology', 'neuroimaging', 'next generation', 'novel', 'patient oriented research', 'phonology', 'real-time images', 'relating to nervous system', 'response', 'shape analysis', 'success', 'tau Proteins', 'tool', 'training opportunity', 'treatment response', 'treatment trial']",NIDCD,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",K24,2021,188829
"Algorithmic Classification of Paraphasias Project Summary This application’s parent grant, R01DC015999, is focused on the development of automated systems for identifying and categorizing paraphasic speech errors in language samples from individuals with post-stroke aphasia, both in the context of confrontation naming tests as well as in connected speech. Current approaches require that language samples be manually transcribed, which is both time-consuming and error-prone, and limits the clinical applicability of the technology. Since the parent grant was written, there have been major improvements in automatic speech recognition (ASR) technology, and it may soon be possible to automate this transcription step. This would open many new avenues for applying automated systems of the sort developed under the parent grant, both in clinical and research settings. However, these promising new ASR techniques depend on large and carefully-annotated datasets, of the sort that do not exist currently for aphasic speech. Under this administrative supplement, we propose to address this issue by performing an extensive campaign of transcription and detailed annotation of an already-existing publicly-available library of audio recordings of aphasic speech, including both structured naming tests and discourse samples. In addition to phonemic transcription of utterances themselves, we will annotate other features of aphasic speech (false starts, disfluencies, etc.) so as to support the development of automated algorithms for analyzing such speech. Our interdisciplinary team of machine learning researchers and aphasiologists will collaborate closely to produce a curated dataset of the sort needed to develop, train, and evaluate modern machine learning techniques for speech recognition. Importantly, the resulting dataset will be documented and organized in a similar manner to other large-scale ASR datasets, and will be released publicly to both the clinical and machine learning communities. In order to raise awareness of the dataset (and of this problem space in general) within the machine learning community, we further propose to organize a shared evaluation task, in which participating teams will make use of our final dataset to build automated transcription systems for naming tests, which will be compared in a “bakeoff” setting. Project Narrative Patients who have experienced a stroke or other brain injury frequently experience anomia- a condition in which they are unable to produce words when speaking. Our parent R01's goal is to create a computerized system for detecting and characterizing an individual’s anomia, which will open the door to new ways of treating anomia and reduce clinical workload. In this administrative supplement, we will prepare a new dataset for use in training these computerized systems, and organize a shared task evaluation within the speech recognition community that will use this dataset.",Algorithmic Classification of Paraphasias,10411534,R01DC015999,"['Address', 'Administrative Supplement', 'Algorithmic Analysis', 'Anomia', 'Aphasia', 'Awareness', 'Brain Injuries', 'Clinical', 'Clinical Research', 'Communities', 'Consumption', 'Data Set', 'Development', 'Evaluation', 'Genetic Transcription', 'Goals', 'Individual', 'Language', 'Libraries', 'Machine Learning', 'Manuals', 'Modernization', 'Names', 'Parents', 'Patients', 'Research Personnel', 'Sampling', 'Speech', 'Stroke', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Workload', 'automated algorithm', 'automated speech recognition', 'classification algorithm', 'clinical application', 'computerized', 'experience', 'learning community', 'parent grant', 'post stroke', 'speech recognition', 'stroke-induced aphasia']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2021,294197
"Prolonging Functional Speech in Persons with Amyotrophic Lateral Sclerosis: A Real-Time Virtual Vocal Tract People with ALS eventually and inevitably experience serious speech impairment due to progressive deterioration of brain cells that control movements of the tongue, lips and jaw. Despite the devastating consequences of this speech impairment on quality of life and survival, few options are available to assist impaired oral communication, and many existing speech-generating technologies are slow to operate and cost prohibitive. This project seeks to improve quality of life for persons with impaired speech due to ALS by testing the effectiveness of a low-cost, speech-generating device (a virtual vocal tract) that could significantly prolong the ability of these patients to communicate orally. If successful, these techniques could be extended for use by patients' with a broad range of speech motor impairments. The virtual vocal track uses machine learning algorithms to predict what a person is attempting to say, in real-time, based solely on lip movements. Users of the device are able to trigger the playback of a number of predetermined phrases by simply attempting to articulate what they want to say. Our previous work has shown the feasibility of this approach using cost-prohibitive laboratory systems such as electromagnetic articulography. Recent advances in 3D depth mapping camera technology allow these techniques to be tested for the first time using technologies, which are low-cost, portable and already being integrated into consumer devices such as laptops and cellphones. To this end, the system under development will be tested in 60 patients with ALS, representing a range of speech impairment from normal to severe speech intelligibility (15 normal, 15 mild, 15 moderate, 15 severe). During testing, participants will be cued to articulate the phrases in a random order as fast as is comfortable for them. The entire session will be recorded and the following variables will be measured offline: recognition accuracy, recognition latency, task time, % completion, and communication rate (words per minute). Users will rate the usability and acceptability of the virtual vocal tract immediately following device testing, using the System Usability Scale. Results of this testing will be used to address the following specific aims: (1) Determine the accuracy and latency of real-time phrase synthesis based on dysarthric speech using the virtual vocal tract, (2) Determine the usability and acceptability of real-time phrases produced using the virtual vocal tract, and (3) Identify the articulatory and speech factors that degrade recognition accuracy. People with ALS eventually and inevitably experience serious speech impairment due to progressive bulbar motor deterioration. Despite the devastating consequences of this impairment to quality of life, few options are available to assist or prolong impaired oral communication. The goal of the proposed work is to lay the groundwork for development of a low-cost speech-generating device—a virtual vocal tract—that can prolong functional oral communication for people with bulbar motor deterioration. This project seeks to testing the efficacy of a low-cost, speech-generating device (a virtual vocal tract) that records lip movements in real-time and triggers the playback of prerecorded phrases as users articulate what they want to say. If successful, the virtual device could provide an alternative means of oral communication for the large number of persons with unintelligible speech but still able to move their oral structures.",Prolonging Functional Speech in Persons with Amyotrophic Lateral Sclerosis: A Real-Time Virtual Vocal Tract,10201558,K24DC016312,"['3-Dimensional', 'Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Articulation', 'Articulators', 'Bypass', 'Cellular Phone', 'Cerebral Palsy', 'Characteristics', 'Communication', 'Complex', 'Cues', 'Data', 'Deterioration', 'Development', 'Devices', 'Disease', 'Dysarthria', 'Effectiveness', 'Electromagnetics', 'Ensure', 'Future', 'Generations', 'Goals', 'Impairment', 'Individual', 'Jaw', 'Laboratories', 'Learning', 'Lip structure', 'Machine Learning', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Modeling', 'Modification', 'Motion', 'Motor', 'Movement', 'Multiple Sclerosis', 'Oral', 'Output', 'Parkinson Disease', 'Participant', 'Patients', 'Performance', 'Persons', 'Play', 'Quality of life', 'Questionnaires', 'Records', 'Research', 'Research Personnel', 'Running', 'Severities', 'Speech', 'Speech Intelligibility', 'Speech Sound', 'Speed', 'Stroke', 'Structure', 'Surveys', 'System', 'Tablet Computer', 'Tablets', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Time', 'Tongue', 'Traumatic Brain Injury', 'Voice', 'Work', 'base', 'brain cell', 'clear speech', 'cost', 'effectiveness testing', 'efficacy testing', 'experience', 'experimental study', 'improved', 'innovation', 'jaw movement', 'laptop', 'machine learning algorithm', 'motor impairment', 'novel', 'oral communication', 'orofacial', 'phrases', 'portability', 'spatiotemporal', 'time use', 'usability', 'virtual', 'virtual vocal tract']",NIDCD,MGH INSTITUTE OF HEALTH PROFESSIONS,K24,2021,189937
"Wearable silent speech technology to enhance impaired oral communication Project Summary/Abstract The long-term objectives of this project are to obtain a deeper understanding how articulatory movement patterns are mapped to speech particularly when there is no vocal fold vibration (silent speech) and then to develop a novel, wearable assistive technology called silent speech interface (SSI) to assist the impaired oral communication for individuals in need (e.g., individuals after laryngectomy, surgical removal of larynx to treat advanced laryngeal cancer). Designed for daily use, the SSI contains a wearable magnetic device and a small camera for tongue and lip motion tracking, respectively, and an articulation-to-speech synthesizer to output natural sounding speech that preserves the speaker’s voice characteristics. Specific Aims of the proposal include to (1) determine the articulatory patterns of normal (vocalized) and silent speech, produced by both healthy talkers and people after laryngectomy, (2) develop a wearable, wireless magnetic device for real-time tongue and lip motion tracking, and (3) synthesize speech from articulation directly. There are currently limited alternative communication options for people who have undergone laryngectomy. These options include esophageal speech, tracheo-esophageal speech, and use of an artificial larynx (or electrolarynx). These solutions are either invasive or difficult to use, and all of them result in a hoarse or mechanical/robotic sounding voice, which can be difficult to understand. In contrast, the SSI in this application is non-invasive, easy-to-use, and produces natural sounding speech and may even preserve the patient’s voice identity. We have exciting preliminary results that support the feasibility of the project including that (1) we have recently developed a wireless magnetic device for tongue motion, and (2) we have demonstrated real- time articulation-to-speech synthesis with a 90% word accuracy (judged by a human listener). In this project, we will further reduce the size of the wireless device and make it wearable and conduct articulation-to-speech algorithms by studying 30 participants after laryngectomy and 30 age- and gender-matched healthy controls. If successful, the proposed research will enhance human health by making an impact on individuals after laryngectomy and potentially to a broader range of other speech and voice disorders. In addition, the technology will have an impact to the speech science field by providing a fist-time-ever tool for potential large- scale tongue motion data collection and have a variety of broader implications including visual feedback-based secondary language training and speech therapy, which may benefit millions of people with motor speech deficits in the United States. Project Narrative Silent speech interfaces (SSI) is a novel assistive technology for enhancing the oral communication for people who are unable to produce speech sounds (e.g., individuals who undergo laryngectomy, removal of larynx to treat advanced laryngeal cancer). The proposed SSI is a wearable device for tongue motion tracking and produces synthesized, natural sounding speech that preserves the patient’s voice characteristics in real-time, which holds potential to enhance the speech health and quality of life of laryngectomees. The technology also has potential for a variety of broader applications including visual feedback-based secondary language training and speech therapy.",Wearable silent speech technology to enhance impaired oral communication,10218134,R01DC016621,"['Acoustics', 'Age', 'Alaryngeal Speech', 'Algorithms', 'Articular Range of Motion', 'Articulation', 'Articulators', 'Characteristics', 'Communication', 'Data', 'Data Collection', 'Development', 'Devices', 'Electrolarynx', 'Electromagnetics', 'Enhancement Technology', 'Esophageal Speech', 'Excision', 'Gender', 'Goals', 'Gold', 'Health', 'Hoarseness', 'Human', 'Impairment', 'Individual', 'Knowledge', 'Laryngeal Prosthesis', 'Laryngectomee', 'Laryngectomy', 'Larynx', 'Life', 'Lip structure', 'Machine Learning', 'Magnetism', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Mechanics', 'Mental Depression', 'Motion', 'Motor', 'Movement', 'Output', 'Participant', 'Patients', 'Pattern', 'Performance', 'Population', 'Positioning Attribute', 'Production', 'Quality of life', 'Research', 'Robotics', 'Science', 'Self-Help Devices', 'Speech', 'Speech Disorders', 'Speech Sound', 'Speech Synthesizers', 'Speech Therapy', 'Speed', 'Technology', 'Testing', 'Time', 'Tongue', 'Tracer', 'Tracheoesophageal Speech', 'United States', 'Voice', 'Voice Disorders', 'Voice Quality', 'Wireless Technology', 'alternative communication', 'auditory feedback', 'base', 'design', 'experimental study', 'improved', 'innovation', 'kinematics', 'language training', 'machine learning algorithm', 'millisecond', 'new technology', 'novel', 'oral communication', 'preservation', 'prototype', 'social', 'social exclusion', 'sound', 'speech synthesis', 'tool', 'vibration', 'visual feedback', 'vocal cord', 'wearable device']",NIDCD,"UNIVERSITY OF TEXAS, AUSTIN",R01,2021,581235
"Wearable silent speech technology to enhance impaired oral communication Project Summary/Abstract The long-term objectives of the parent R01 project (R01DC016621, 08/15/2019 – 06/31/2024) are to obtain a deeper understanding how articulatory movement patterns are mapped to speech particularly when there is no vocal fold vibration (silent speech) and then to develop a novel, wearable assistive technology called silent speech interface (SSI) to assist the impaired oral communication for individuals in need (e.g., individuals after laryngectomy, surgical removal of larynx to treat advanced laryngeal cancer). The parent R01 project aims to (1) determine the articulatory patterns of normal (vocalized) and silent speech, produced by both healthy talkers and people after laryngectomy, (2) develop a wearable device for real-time tongue and lip motion tracking, and (3) synthesize speech from articulation directly. If successful, the proposed research will enhance human health by making an impact on individuals after laryngectomy and potentially to a broader range of other speech and voice disorders as well as visual feedback-based secondary language training and speech therapy. Through the parent R01 project, we are collecting a unique multi-modal speech dataset from patients following laryngectomy and healthy controls. This data set includes speech kinematics from multiple tracers attached on the tongue and the lips, and speech acoustics. Each tracer comprises multiple sensors that measure inertial and magnetic information that can provide additional information to assist the speech acoustic and the articulation-to-speech algorithm. Making such data ML ready for others to consume was out of scope due to required effort and complexity needed to pre-process and synchronize sampling between kinematic and acoustic data. The specific goal of this supplemental project is to make data AI/ML ready by developing the pre-processing algorithms needed to generate a set of features, along with proper formatting and labeling, that can be more easily shared through repositories and used by others to evaluate different ML algorithms. New ML models that will be tested on these ML-ready shared datasets will significantly advance our capabilities to translate articulatory motion into speech sounds, which will not only improve the quality of life for people affected by laryngectomy but also for the millions of individuals living with speech sound disorders such as Parkinson’s disease, and amyotrophic lateral sclerosis. Project Narrative The parent R01 project is to develop a wearable silent speech interface (SSI), a novel assistive technology for enhancing the oral communication for people who are unable to produce speech sounds (e.g., individuals who undergo laryngectomy, removal of larynx to treat advanced laryngeal cancer) by converting their tongue and lip motion into intelligible speech. Lack of such a publicly available AI/ML ready data set is a critical gap in this field, thus the proposed supplemental project is to make the unique, multi-modal data collected in the parent R01 project AI/ML ready. If successful, this supplement project will have high impact on the speech production affected by laryngectomy.",Wearable silent speech technology to enhance impaired oral communication,10412276,R01DC016621,"['Acceleration', 'Acoustics', 'Address', 'Affect', 'Alaryngeal Speech', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Articulation', 'Articulators', 'Benchmarking', 'California', 'Characteristics', 'Communication', 'Communities', 'Consumption', 'Data', 'Data Set', 'Devices', 'Disease', 'Documentation', 'Engineering', 'Enhancement Technology', 'Ensure', 'Excision', 'Feedback', 'Felis catus', 'Goals', 'Health', 'Hoarseness', 'Human', 'Impairment', 'Individual', 'Label', 'Laryngectomy', 'Larynx', 'Lip structure', 'Machine Learning', 'Magnetism', 'Malignant neoplasm of larynx', 'Maps', 'Measures', 'Mechanics', 'Mental Depression', 'Modeling', 'Motion', 'Motor', 'Movement', 'Nature', 'Output', 'Parents', 'Parkinson Disease', 'Patients', 'Pattern', 'Physiological', 'Positioning Attribute', 'Process', 'Production', 'Publishing', 'Quality of life', 'Readiness', 'Research', 'Research Personnel', 'Robotics', 'Rotation', 'Route', 'Sampling', 'Science', 'Self-Help Devices', 'Series', 'Signal Transduction', 'Site', 'Speech', 'Speech Acoustics', 'Speech Disorders', 'Speech Intelligibility', 'Speech Sound', 'Speech Therapy', 'Standardization', 'Technology', 'Testing', 'Time', 'Tongue', 'Tracer', 'Translating', 'United States', 'Universities', 'Voice', 'Voice Disorders', 'Wireless Technology', 'archive data', 'archived data', 'base', 'community planning', 'data archive', 'data mining', 'improved', 'innovation', 'interest', 'kinematics', 'language training', 'multimodal data', 'multimodality', 'new technology', 'novel', 'oral communication', 'public repository', 'recruit', 'repository', 'sensor', 'social exclusion', 'sound', 'tool', 'vibration', 'visual feedback', 'vocal cord', 'wearable device']",NIDCD,"UNIVERSITY OF TEXAS, AUSTIN",R01,2021,292804
"Speech markers of cognitive impairment in Parkinson's disease ABSTRACT Dr. Kara Smith is a Movement Disorders neurologist at the University of Massachusetts Medical School (UMMS) whose goal is to become an independent investigator focused on early cognitive impairment in Parkinson disease (PD). Her long-term goal is to develop speech markers of cognitive impairment in PD. Cognitive impairment occurs in the majority of PD patients, leading to increased mortality and decreased quality of life. The current diagnostic tools are resource-intensive and have limited sensitivity. Treatments are often offered late in the course of cognitive decline and do not provide optimal benefit. Speech markers could improve detection, monitoring and treatment of cognitive impairment in PD. Speech markers could be monitored frequently and remotely via mobile technology, capturing sensitive, quantitative data about cognitive function in the context of patients’ daily life and in response to therapeutics. Dr. Smith’s role as a clinical movement disorders specialist ideally positions her to lead the application of advanced speech and language research to feasible, patient-oriented tools for real-life clinical practice and clinical trials. Dr. Smith has assembled an expert interdisciplinary mentorship team ideally suited for the goals of this innovative proposal. Dr. Smith and her team have previously shown that a) speech acoustic markers are associated with cognitive function in non-demented PD patients, and b) PD patients with mild cognitive impairment had linguistic deficits including pauses within utterances and grammaticality. Building on these results, Dr. Smith proposes to study speech and language more comprehensively in PD patients with and without mild cognitive impairment and controls to confirm these preliminary results and identify additional biomarkers. The aims of this study will be 1) to develop algorithms using speech acoustic markers to categorize by cognitive status, 2) to identify linguistic markers associated with mild cognitive impairment in PD, and 3) to assess on-line syntactic processing in PD subjects with mild cognitive impairment. The overall goal of the proposal is to identify speech and language markers of early cognitive dysfunction that can be further refined, validated and implemented using mobile technology into a larger scale, longitudinal R01 proposal. Further work will also address the underlying neurobiological mechanisms of these speech markers. Dr. Smith’s rigorous training plan includes a Master’s degree, linguistics and speech motor physiology courses, and experience in signal processing and speech acoustic analysis. Through her training goals, she will advance her knowledge and skills in patient-centered outcomes measures and instrument validation. She will gain experience in research leadership, presentation and dissemination of scientific work, and in grant writing, culminating in an R01 proposal. This K23 award will be critical for Dr. Smith to establish an independent career as a PD clinician-scientist at the unique intersection of speech and language science and cognitive impairment. PUBLIC HEALTH RELEVANCE: Speech markers have the potential to improve diagnosis, monitoring and treatment of cognitive impairment in Parkinson’s disease (PD). Although the majority of PD patients will develop cognitive impairment, the tools available to assess and treat this disabling complication are fraught with limitations. As a detailed and quantitative assessment tool, speech markers may increase sensitivity to early cognitive dysfunction and to changes over time compared with current measures. They may be automated and then implemented through mobile technology to increase patients’ access to cognitive symptom monitoring outside of the clinic setting. Dr. Smith’s proposed career development plan has potential to fill a major gap in PD research by making inexpensive and easy-to-use cognitive assessment tools accessible to patients in rural and international settings, and by fueling clinical trials to discover new therapeutics capable of slowing cognitive decline in PD.",Speech markers of cognitive impairment in Parkinson's disease,10115019,K23DC016656,"['Acoustics', 'Address', 'Algorithms', 'American', 'Area', 'Articulation', 'Assessment tool', 'Award', 'Biological Markers', 'Biomedical Engineering', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Cognitive Therapy', 'Cognitive deficits', 'Complication', 'Comprehension', 'Data', 'Data Analyses', 'Dementia', 'Detection', 'Development', 'Development Plans', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Foundations', 'Future', 'Goals', 'Grant', 'Health Services Accessibility', 'Impaired cognition', 'Impairment', 'Individual', 'International', 'Knowledge', 'Language', 'Language Disorders', 'Lead', 'Leadership', 'Life', 'Linguistics', 'Location', 'Longitudinal Studies', 'Machine Learning', 'Massachusetts', 'Master&apos', 's Degree', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement Disorders', 'Nature', 'Nerve Degeneration', 'Neurobehavioral Manifestations', 'Neurobiology', 'Neurologist', 'Neuropsychological Tests', 'Outcome', 'Outcome Measure', 'Outcomes Research', 'Parkinson Disease', 'Parkinson&apos', 's Dementia', 'Participant', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physiology', 'Population', 'Positioning Attribute', 'Production', 'Proxy', 'Quality of Care', 'Quality of life', 'Research', 'Research Personnel', 'Resources', 'Role', 'Rural', 'Science', 'Scientist', 'Severities', 'Specialist', 'Speech', 'Speech Acoustics', 'Symptoms', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'Universities', 'Validation', 'Work', 'Writing', 'career', 'career development', 'clinical movement disorder', 'clinical practice', 'cognitive change', 'cognitive control', 'cognitive function', 'cognitive impairment in Parkinson&apos', 's', 'cognitive performance', 'cognitive testing', 'common symptom', 'experience', 'handheld mobile device', 'improved', 'innovation', 'instrument', 'language processing', 'large scale data', 'lexical retrieval', 'machine learning method', 'medical schools', 'mild cognitive impairment', 'mobile computing', 'mortality', 'motor deficit', 'neurobiological mechanism', 'non-demented', 'novel', 'novel therapeutics', 'patient oriented', 'public health relevance', 'recruit', 'response', 'screening', 'signal processing', 'skills', 'syntax', 'tool']",NIDCD,UNIV OF MASSACHUSETTS MED SCH WORCESTER,K23,2021,189216
"Biofeedback-Enhanced Treatment for Speech Sound Disorder: Randomized Controlled Trial and Delineation of Sensorimotor Subtypes Project Summary/Abstract Children with speech sound disorder show diminished accuracy and intelligibility in spoken communication and may thus be perceived as less capable or intelligent than peers, with negative consequences for both socio- emotional and socioeconomic outcomes [1]–[3]. While most speech errors resolve by the late school-age years, between 2-5% of speakers exhibit residual speech sound disorder (RSSD) that persists through adoles- cence or even adulthood [4], [5], reflecting about 6 million cases in the US. In a series of experimental studies since 2013, our research team has demonstrated that treatment incorporating technologically-enhanced feed- back can improve speech production in individuals with RSSD who have not responded to previous interven- tion [6]–[10]. The primary objective of the parent award (R01 DC017476, “Biofeedback-Enhanced Treatment for Speech Sound Disorder”) is to conduct the first well-powered randomized controlled trial comparing tradi- tional vs biofeedback intervention for the most common type of RSSD, misarticulation of the English /r/ sound.  Treatment of RSSD could also be enhanced through the development of tools incorporating artificial intelligence/machine learning (AI/ML). Applications with automated scoring of speech sounds could in principle be used to augment clinician services and achieve higher-intensity practice for faster progress. However, no computerized treatment to date has demonstrated sufficient accuracy for clinical use with children [11]. Existing systems are limited primarily by the fact that publicly available speech corpora have very little representation of either children or individuals with speech impairments. This data scarcity represents a fundamental issue hin- dering advances in clinical applications of automatic speech recognition (ASR) [12]. The proposed sup- plement will address this barrier by modifying and augmenting PERCEPT (Perceptual Error Rating for the Clin- ical Evaluation of Phonetic Targets), an existing corpus of acoustic recordings of child speech assembled through the parent award and the investigators’ previous NIH-funded research since 2013. AI/ML applications of the augmented database are expected to have a twofold scientific impact. First, we anticipate direct benefits for children with RSSD affecting /r/, whose speech samples make up the majority of the current corpus. We will use our database to train a neural network to classify novel child productions containing /r/ as correct or incor- rect. This classifier could then be incorporated into AI tools to increase the efficacy of intervention for children with RSSD. Second, we anticipate that engineers working on the broader problem of ASR for child or clinical speech will be interested in using PERCEPT for model training, especially after the corpus is augmented with more diverse data, as proposed here. We expect to show that acoustic models generated with PERCEPT can improve the performance of currently available open-access speech recognition systems (e.g., Kaldi [13]) in recognition of novel child speech stimuli. The PERCEPT database will be made publicly available through part- nership with PhonBank, an NIH-funded data-sharing platform for speech research (e.g., [14]–[16]). Project Narrative Speech sound disorder in childhood poses a barrier to academic and social participation, with potentially life- long consequences for educational and occupational outcomes. The parent award aims to meet a public health need by conducting the first randomized controlled trial comparing the efficacy and efficiency of speech inter- vention with and without real-time visual biofeedback. The proposed supplement will lay groundwork for the development of automated speech recognition tools for speech sound disorder by modifying and augmenting an existing corpus of acoustic recordings of child speech in preparation for sharing with researchers in artificial intelligence and machine learning (AI/ML).",Biofeedback-Enhanced Treatment for Speech Sound Disorder: Randomized Controlled Trial and Delineation of Sensorimotor Subtypes,10412492,R01DC017476,"['Acoustics', 'Address', 'Adolescence', 'Adult', 'Affect', 'Age-Years', 'Articulation Disorders', 'Articulators', 'Artificial Intelligence', 'Auditory', 'Award', 'Biofeedback', 'Biofeedback Training', 'Child', 'Childhood', 'Clinical', 'Clinical Research', 'Communication', 'Data', 'Databases', 'Decision Making', 'Development', 'Disease', 'Emotional', 'Engineering', 'Enrollment', 'Exhibits', 'Feedback', 'Foundations', 'Funding', 'Impairment', 'Individual', 'Individual Differences', 'Intelligence', 'Intervention', 'Lead', 'Learning', 'Life', 'Link', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Motor', 'Occupational', 'Oral', 'Outcome', 'Parents', 'Participant', 'Perception', 'Performance', 'Population', 'Prediction of Response to Therapy', 'Preparation', 'Prevalence', 'Procedures', 'Production', 'Public Health', 'Publishing', 'Randomized', 'Randomized Controlled Trials', 'Recording of previous events', 'Research', 'Research Personnel', 'Residual state', 'Sampling', 'School-Age Population', 'Sensory', 'Series', 'Services', 'Social isolation', 'Speech', 'Speech Sound', 'Stimulus', 'Surveys', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Treatment Efficacy', 'Tweens', 'Ultrasonography', 'United States National Institutes of Health', 'Visual', 'Work', 'automated speech recognition', 'base', 'bullying', 'clinical application', 'comparative efficacy', 'computerized', 'diverse data', 'evidence base', 'experience', 'experimental study', 'improved', 'insight', 'interest', 'neural network', 'novel', 'peer', 'personalized learning', 'predicting response', 'predictive modeling', 'preference', 'recruit', 'research clinical testing', 'responders and non-responders', 'response', 'sensory feedback', 'sharing platform', 'social engagement', 'socioeconomics', 'somatosensory', 'sound', 'speech accuracy', 'speech recognition', 'tool', 'tool development', 'trait', 'trial comparing', 'visual information']",NIDCD,NEW YORK UNIVERSITY,R01,2021,268779
"Studying the Laryngeal Mechanisms Underlying Dysphonia in Connected Speech Project Summary/Abstract  This proposal aims to employ the recent advancement of coupling fiberoptic endoscopes with high-speed videoendoscopy (HSV) systems to obtain HSV recordings during connected speech. The goal is to study vocal mechanisms underlying dysphonia in patients with neurogenic voice disorders. The long-term goal of this line of research is to create clinically applicable quantitative methods for functional measurement of vocal fold vibration in connected speech using innovative laryngeal imaging, an approach that could advance clinical voice assessment and treatment practice. In Aim 1, HSV-based measures of vocal fold kinematics will be developed and the influence of these measures on voice audio-perceptual qualities in the patients will be determined. Image processing techniques will be developed to extract such measures from the HSV data in connected speech. The extracted measures will be given as inputs to the statistical models to determine the source of the differences between the normal controls and the patients for different speech phonetic contexts and words. This aim provides an unbiased HSV-based method to predict voice quality. Developing such HSV-based methodology for functional laryngeal examination in connected speech can enhance clinical voice assessment. In addition, better understanding the influence of phonetic context would lead to optimizing the protocols for functional voice assessment through laryngeal imaging in connected speech. In Aim 2, machine learning approaches will be employed to discover hidden physics and unknown laryngeal mechanisms of voice production in the dysphonic patients. The findings of this project will help make necessary adjustments in biomechanical or physiological characteristics of vocal folds to enhance voice quality in patients with neurogenic voice disorders. Therefore, the outcome of this research will aid clinicians in properly selecting, and developing new treatment strategies (therapeutic, medicinal, or surgical), which are based on the gained knowledge of laryngeal mechanisms of dysphonia. The proposed research is in harmony with multiple priority areas of the NIDCD, described in the 2017-2021 Strategic Plan. Both aims support Priority 3 (improve methods of diagnosis, treatment, and prevention) through developing objective HSV-based measures and predicting the voice quality. Comparing laryngeal mechanisms in normal and disordered voices addresses Priority 1 (deepen our understanding of the normal function of the systems of human communication). Both aims propose to study laryngeal mechanisms in patients with neurogenic and functional voice disorders, which addresses Priority 2 (increase our knowledge about conditions that alter or diminish communication and health). Project Narrative  The goal of this proposal is to determine laryngeal mechanisms underlying dysphonia in connected speech, which will lead to development of clinically applicable quantitative methods for functional laryngeal examination in connected speech using laryngeal imaging. This can potentially result in enhancement of clinical voice assessment and development of new clinical voice management strategies to better help people with voice disorders.",Studying the Laryngeal Mechanisms Underlying Dysphonia in Connected Speech,10134315,K01DC017751,"['Acoustics', 'Address', 'Age', 'Area', 'Auditory', 'Behavior', 'Biomechanics', 'Categories', 'Characteristics', 'Clinical', 'Communication', 'Coupling', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Dysphonia', 'Endoscopes', 'Evaluation', 'Functional disorder', 'Goals', 'Gold', 'Health', 'Human', 'Image', 'Knowledge', 'Larynx', 'Lead', 'Machine Learning', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mining', 'Modeling', 'National Institute on Deafness and Other Communication Disorders', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Paralysed', 'Patients', 'Physics', 'Physiological', 'Prevention', 'Production', 'Protocols documentation', 'Research', 'Series', 'Severities', 'Source', 'Spastic Dysphonias', 'Speech', 'Speed', 'Statistical Data Interpretation', 'Statistical Models', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Time', 'Tremor', 'Visual', 'Voice', 'Voice Disorders', 'Voice Disturbances', 'Voice Quality', 'base', 'clinical application', 'clinical development', 'clinical practice', 'clinically relevant', 'cohort', 'flexibility', 'image processing', 'imaging approach', 'improved', 'innovation', 'kinematics', 'sex', 'temporal measurement', 'time use', 'tool', 'treatment strategy', 'vibration', 'vocal cord', 'vocalization']",NIDCD,MICHIGAN STATE UNIVERSITY,K01,2021,137795
"Revealing the organization and functional significance of neural timescales in auditory cortex Project Summary People are remarkably adept at making sense of the world through sound: understanding speech in a noisy restaurant, picking out the voice of a family member, or recognizing a familiar melody. Although we take these abilities for granted, they reflect impressive computational feats of biological engineering that are remarkably difficult to replicate in machine systems. The long-term goal of my research program is to develop computational and experimental methods to reverse-engineer how the brain codes natural sounds like speech and to exploit these advances to understand and aid in the treatment of hearing impairment. One of the central challenges of coding natural sounds is that they are structured at many different timescales from milliseconds to seconds and even minutes. How does the brain integrate across these diverse timescales to derive meaning from sound? Answering this question has been challenging because there are no general-purpose methods for measuring neural timescales in the brain. As a consequence, we know relatively little about how neural timescales are organized in auditory cortex and how this organization enables the coding of natural sounds. To overcome these limitations, we develop a simple experimental paradigm (the “temporal context invariance” or TCI paradigm) for estimating the temporal integration period of any sensory response: the time window during which stimuli alter the response. We apply the TCI method to human electrocorticography (ECoG) and animal physiology recordings to reveal the organization of neural timescales at both the region and single-cell level (Aim I). Pilot data from our analyses reveal that timescales are organized hierarchically, with higher-order regions showing substantially longer integration periods. To explore the functional significance of this timescale hierarchy, we couple TCI with computational techniques well-suited for characterizing natural sounds (Aim II). We test whether increased integration periods enable a more noise-robust representation of speech (Aim IIA), whether regions with longer integration periods code higher-order properties of natural sounds (Aim IIB&IIC), whether there are dedicated integration periods for important sounds categories like speech or music (Aim IID), and whether cortical integration periods can be explained by the duration of the features they respond to (Aim IIE). In the process of conducting this research, I will be trained in two critical areas: (1) ECoG, which is the only method with the spatial and temporal precision to understand how neural timescales are organized in the human brain (2) deep neural networks (DNN) which are the only models able to perform challenging perceptual tasks at human levels and predict neural responses in higher-order cortical regions. After completing this training, I will have a unique set of experimental (fMRI, ECoG, psychophysics) and computational skills (data-driven statistical modeling and hypothesis-driven DNN modeling), which will facilitate my transition to an independent investigator. Project Narrative Natural sounds like speech contain information at many different timescales (e.g. phonemes, syllables, words), but how the human brain extracts this information remains unclear. Understanding this process is critical to understanding how hearing impairment degrades speech perception. The proposed research will reveal the organization of neural timescales in the brain, and how this organization facilitates the coding of natural sounds like speech, which is a critical first step in understanding how this code is impaired by hearing loss.",Revealing the organization and functional significance of neural timescales in auditory cortex,10169404,K99DC018051,"['Address', 'Animals', 'Area', 'Auditory area', 'Biological', 'Brain', 'Categories', 'Cells', 'Code', 'Collaborations', 'Communication', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Data', 'Electrocorticogram', 'Engineering', 'Family member', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grant', 'Hearing', 'Human', 'Learning', 'Measures', 'Methods', 'Modeling', 'Music', 'Neural Network Simulation', 'Neurosciences', 'Noise', 'Physiology', 'Population', 'Process', 'Property', 'Psychophysics', 'Reaction Time', 'Research', 'Research Personnel', 'Research Training', 'Restaurants', 'Sensory', 'Speech', 'Speech Perception', 'Statistical Models', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Training', 'Voice', 'Work', 'clinically significant', 'deep neural network', 'experience', 'experimental study', 'hearing impairment', 'millisecond', 'neuromechanism', 'programs', 'relating to nervous system', 'response', 'skills', 'sound', 'theories']",NIDCD,COLUMBIA UNIV NEW YORK MORNINGSIDE,K99,2021,125442
"The role of amplitude modulation in perceiving speech and music Project Summary/Abstract  My career goal is to become a leading researcher on cognitive neuroscience, with a special focus on the neural mechanisms underlying auditory perception, including how humans track and perceive the fleeting audi- tory information in speech and music. In this proposal, I outline a research program to investigate the acoustic and neural distinctions between speech and music, two specialized forms of auditory signals that are closely tied to the human mind. Despite our increasingly rich understanding of the perceptual and neural mechanisms for processing speech or music, surprisingly little is known about why and how they are treated as different au- ditory signals by the human mind and brain in the first place. Investigating these distinctions is foundational for a thorough understanding of how acoustic waveforms are transformed into meaningful information. The work will provide a more solid basis for understanding cognition and communication as well as treating people with communicative deficits, such as people with autism, Alzheimer's disease, and aphasia.  I hypothesize that the temporal structure reflected in the amplitude modulation (AM) of speech and music signals is a critical distinctive feature for the brain and engages to different processing pathways, as speech and music are known to have distinct AM rates. A series of studies, combining psychophysics, MEG (magne- toencephalography), fMRI (functional magnetic resonance imaging), and machine learning approaches, will use stimuli with AM rates across the modulation frequency ranges of speech and music to address this topic at the computational (the goals), algorithmic (the representations and operations), and implementational (neural mechanism) levels. (1) Does the AM rate of a sound affect whether it will be perceived as speech or music? (2) Does the AM rate of a stimulus optimize speech and music perceptual performance at different frequencies? (3) What are the underlying neural mechanisms and the associated brain regions implementing the differentiation of speech and music? Aim 1 investigates whether the AM rate of a sound conditions it to be processed as speech or music. By manipulating the AM rate of noise-vocoded speech and music recordings, I hypothesize that the sounds with slower or faster AM rates will likely to be perceived as music or speech, respectively, the perceptual judgment will be biased by the higher or lower spectral energy of neural oscillatory activity (meas- ured by MEG) while listening to the sounds, respectively, and the associated brain regions will be revealed by fMRI with machine learning decoding approaches. Aim 2 investigates whether the AM rate of stimuli optimizes speech and music perceptual performances at different rates. I hypothesize that the music perceptual perfor- mance is optimal at slower AM rates while the speech perceptual performance is optimal at faster AM rates, and the neural oscillatory entrainment at lower or higher frequency band has domain-specific function facilitat- ing speech or music perceptual performance. Project Narrative Speech and music are two specialized forms of auditory signal that are closely tied to human mind; however, despite our increasingly rich understanding of the perceptual and neural mechanisms of human processing of speech or music, surprisingly little is known how they are treated as different auditory signals by the human mind and brain at the first place. The current proposal aims to investigate the fundamental differences between speech and music at the acoustic, perceptual, and neural levels, by combining psychophysics, neuroimaging, and machine learning approaches. Investigating their distinctions is crucial for understanding how acoustic waveforms are transformed into meaningful information, and it will provide the basis for understanding and treating people with communicative deficits, such as people with autism, Alzheimer's disease, and aphasia.",The role of amplitude modulation in perceiving speech and music,10118008,F32DC018205,"['Acoustics', 'Address', 'Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Aphasia', 'Auditory', 'Auditory Perception', 'Auditory area', 'Behavioral', 'Brain', 'Brain region', 'Cognition', 'Communication', 'Data', 'Foundations', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Human', 'Judgment', 'Linguistics', 'Machine Learning', 'Magnetoencephalography', 'Measures', 'Mind', 'Music', 'Noise', 'Participant', 'Pathway interactions', 'Perception', 'Performance', 'Periodicity', 'Process', 'Psychophysics', 'Records', 'Research', 'Research Personnel', 'Role', 'Series', 'Signal Transduction', 'Solid', 'Speech', 'Speech Perception', 'Stimulus', 'Structure', 'System', 'Testing', 'Work', 'auditory processing', 'career', 'cognitive neuroscience', 'experimental study', 'individuals with autism spectrum disorder', 'insight', 'neuroimaging', 'neuromechanism', 'non-invasive imaging', 'operation', 'programs', 'relating to nervous system', 'sound', 'spectral energy', 'speech processing']",NIDCD,NEW YORK UNIVERSITY,F32,2021,65994
"A holistic approach to identifying functional units of tongue motion during speech PROJECT SUMMARY  Oral cancers have the seventh highest incidence, with roughly 51,540 new cases and 10,030 cancer- related deaths expected to occur in 2018. Although a variety of treatment methods are available, the death rate is higher than that for most cancers with five-year rates of about 50 percent. The most frequently used treatment method, glossectomy surgery, involves the surgical removal of tumors and surrounding tissues, and the addition of grafted tissues, often followed by radiotherapy. Although tongue cancer and its treatment have debilitating effects on speech, the impact of varying degrees of resection and reconstruction on the formation of functional units in speech has remained poorly understood. In order to produce intelligible speech, a variety of local muscle groupings of the tongue—i.e., functional units—emerge and recede rapidly and nimbly in a highly coordinated fashion. Therefore, understanding the formation of functional units that are critical for speech production can provide substantial insights into normal, pathological, and adapted motor control strategies in controls and patients with tongue cancer for novel therapeutic, surgical, and rehabilitative strategies. One of the critical challenges in pre-operative surgical and treatment planning, as well as in post- operative evaluation for tongue cancer is the difficulty in developing objective and quantitative measures and in evaluating their functional outcome predictability. To address this, in this proposal, three integrated approaches will be used in in vivo tongue motion during speech to seamlessly identify the functional units and associated quantitative measures: multimodal MRI methods, multimodal deep learning, and biomechanical simulations. This will provide a convergent approach, thereby allowing us to (1) test hypotheses about the spatiotemporal basis of muscle coordination in a consilient way, and (2) develop objective quantitative measures that are required for understanding the complex biomechanical system as well as for predicting the functional outcomes after various reconstruction methods. The first proof of concept study published by the PI and the team identified the functional units of speech tasks using the sparse non-negative matrix factorization framework, in which the magnitude and angle of displacements from tagged MRI were used as our input quantities. With these advances in place, we will further incorporate muscle fiber anatomy from diffusion MRI and motion tracking from tagged MRI into our framework to yield physiologically and anatomically meaningful functional units. In addition, we will create a completely novel and integrated way of directly relating the functional units to tongue muscle anatomy, learning joint representation via a multimodal deep learning technique, and linking them to biomechanical simulations. Furthermore, 3D and 4D atlases will be utilized to identify objective and quantitative measures based on our functional units analysis. Taken together, the successful implementation of our integrated framework will identify functional units that can be used for research on tongue motion, for surgical planning, and for diagnosis, prognosis, and rehabilitation in a range of speech-related disorders. PROJECT NARRATIVE  Tongue cancer and its treatment affect tongue structure and function, yet little is known about how the changes in tongue structure due to varying degrees of resection and reconstruction affect the formation of functional units of tongue motion during speech. We propose to use novel integrated platform tools to identify functional units seamlessly with unprecedented resolution and precision. Upon success of this proposal, our integrated framework has the potential to aid in an increased understanding of speech motor control strategies in healthy controls and the patient group, thereby benefiting patients through improved diagnosis, treatment, and rehabilitative strategies.",A holistic approach to identifying functional units of tongue motion during speech,10147030,R01DC018511,"['3-Dimensional', 'Acoustics', 'Address', 'Affect', 'Aftercare', 'Anatomy', 'Atlases', 'Behavior', 'Biomechanics', 'Cessation of life', 'Clinical', 'Complex', 'Computing Methodologies', 'Data', 'Death Rate', 'Deglutition', 'Diagnosis', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Evaluation', 'Excision', 'Exhibits', 'Fiber', 'Geometry', 'Glossectomy', 'Goals', 'Grouping', 'Impairment', 'Incidence', 'Joints', 'Knowledge', 'Learning', 'Link', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Motion', 'Muscle', 'Muscle Fibers', 'Operative Surgical Procedures', 'Outcome', 'Pathologic', 'Patients', 'Physiological', 'Postoperative Period', 'Predictive Value', 'Procedures', 'Production', 'Prognosis', 'Proxy', 'Publishing', 'Radiation therapy', 'Rehabilitation therapy', 'Research', 'Resolution', 'Speech', 'Speech Intelligibility', 'Standardization', 'Structure', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Tissue Grafts', 'Tissues', 'Tongue', 'Weight', 'Work', 'base', 'biomechanical model', 'clinical practice', 'deep learning', 'functional outcomes', 'holistic approach', 'improved', 'in vivo', 'insight', 'malignant mouth neoplasm', 'malignant tongue neoplasm', 'motor control', 'multimodality', 'muscular structure', 'novel', 'novel therapeutics', 'outcome prediction', 'reconstruction', 'rehabilitation strategy', 'signal processing', 'spatiotemporal', 'success', 'tool', 'treatment planning', 'treatment strategy', 'tumor']",NIDCD,MASSACHUSETTS GENERAL HOSPITAL,R01,2021,565453
"Characterizing the temporal processing of speech in the human auditory cortex Project Summary Time is the fundamental dimension of sound, and temporal integration is thus fundamental to speech perception. To recognize a complex structure such as a word in fluent speech, the brain must integrate across many different timescales spanning tens to hundreds of milliseconds. These timescales are considerably longer than the duration of responses at the auditory nerve. Therefore, the auditory cortex must integrate acoustic information over long and varied timescales to encode linguistic units. On the other hand, the nature of the intermediate units of representation between sound and meaning remains debated. Focal brain injuries have shown selective impairment at all levels of linguistic processing (phonemic, phonotactic, and semantic) but current models of spoken word recognition disagree on the existence and type of these representational levels. The neural basis of temporal and linguistic processing remains speculative partly due to the limited spatiotemporal resolution of noninvasive human neuroimaging techniques which is needed to study the encoding of fluent speech. Our multi- PI proposal overcomes these challenges by assembling a team of researchers and clinicians with complementary expertise at NYU and Columbia University. We propose to record invasively from a large number of neurosurgical patients, which provides a rare and unique opportunity to collect direct cortical recordings across several auditory regions. We propose novel experimental paradigms and analysis methods to investigate where, when, and how acoustic features of speech are integrated over time to encode linguistic units. Our experimental paradigms will determine the functional and anatomical organization of stimulus integration periods in primary and nonprimary auditory cortical regions and relate the temporal processing in these regions to the emergence of phonemic-, phonotactic-, and semantic-level representations. Finally, we will determine the nonlinear computational mechanisms that enable the auditory cortex to integrate fast features over long durations, which is essential in speech recognition. Understanding of the temporal processing of speech in primary and nonprimary auditory cortex is critical for developing complete models of speech perception in the human brain, which is essential to understanding of how these processes break down in speech and communication disorders. Project Narrative The process of mapping acoustic signals onto units of syllables, words, and phrases is essential to human communication and is also remarkably difficult to replicate in machine systems. We propose an approach that leverages novel speech paradigms, computational modeling and invasive neural measures in neurosurgical patients in order to characterize human cortical speech perception. Using these complementary approaches in tandem with high resolution recording, we will greatly advance our understanding of how speech is processed and temporal information is integrated over time in various parts of cortex in in the hopes that our findings will inform new therapeutic strategies for a range of speech communication impairments.",Characterizing the temporal processing of speech in the human auditory cortex,10211535,R01DC018805,"['Acoustic Nerve', 'Acoustics', 'Address', 'Anatomy', 'Area', 'Auditory', 'Auditory area', 'Behavioral', 'Behavioral Research', 'Brain', 'Categories', 'Communication', 'Communication impairment', 'Communities', 'Complex', 'Computer Models', 'Custom', 'Data', 'Dimensions', 'Discipline', 'Early identification', 'Focal Brain Injuries', 'Heterogeneity', 'Human', 'Impairment', 'Implanted Electrodes', 'Language Disorders', 'Light', 'Linguistics', 'Link', 'Measures', 'Methods', 'Modeling', 'Nature', 'Neural Network Simulation', 'Neurobiology', 'New York', 'Patients', 'Pattern', 'Process', 'Prosthesis', 'Research', 'Research Personnel', 'Resolution', 'Route', 'Scheme', 'Semantics', 'Signal Transduction', 'Speech', 'Speech Disorders', 'Speech Pathology', 'Speech Perception', 'Speech Sound', 'Stimulus', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Universities', 'auditory pathway', 'behavior observation', 'cohort', 'deep neural network', 'density', 'design', 'experimental study', 'human model', 'interest', 'lexical', 'lexical processing', 'millisecond', 'neuroimaging', 'neurophysiology', 'novel', 'novel therapeutic intervention', 'pandemic disease', 'phonology', 'phrases', 'receptive field', 'relating to nervous system', 'response', 'sound', 'spatiotemporal', 'speech processing', 'speech recognition', 'syntax', 'temporal measurement']",NIDCD,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2021,681122
"Real-time deep learning to improve speech intelligibility in noise Project Summary/Abstract  One in eight Americans has hearing loss, and this constitutes a major health and economic burden (Blackwell et al., 2014). The primary complaint of hearing-impaired (HI) listeners is difficulty understanding speech when background noise is present (see Dillon, 2012). While hearing aids (HAs) have improved in recent years, they still provide little benefit in noisy environments. For decades, a means of improving the ability to understand speech in background noise appeared unattainable, despite substantial amounts of research by both universities and HA companies. This changed when deep learning provided the first demonstration of a single-microphone algorithm that improves intelligibly in noise for HI listeners (Healy et al., 2013, 2014, 2015). Although this algorithm provides massive intelligibility improvements (even allowing listeners to improve intelligibility from floor to ceiling levels), it is currently not implemented to operate in real time and is therefore not suitable for implementation into HAs and cochlear implants (CIs). What is needed, therefore, is a highly effective noise-reduction algorithm that is capable of operating in real time. This project aims to address this critical need.  The long-term goal of the currently proposed project is to alleviate HI listeners’ predominant hearing handicap, which is difficulty understanding speech in background noise. The first aim introduces a new algorithm, based on a novel foundational scheme, that is designed to provide substantial benefit for any HI listener in real time. This algorithm will be well suited for implementation into HAs, CIs, and other face-to-face communication applications. The effectiveness of this new algorithm will be quantified using both HI and normal-hearing (NH) listeners. The second aim expands upon this new algorithm by modifying it to accept a small amount of future time-frame information, which could improve its noise-reduction performance but will introduce a brief processing delay. The rationale is that different devices have different allowable latencies. Face-to-face communication devices (HAs, CIs, etc.) have strict low-latency requirements, but other important communication systems (e.g., telephones) have different requirements. It is possible that the addition of future time-frame information within these requirements (up to 150 ms) will result in even better speech intelligibility. But the magnitude of any potential benefit is unknown. This critical information will be established currently. Using both HI and NH listeners, we will measure intelligibility for noisy sentences that have been processed using various amounts of future time information.  This comprehensive fellowship training plan will provide individualized, mentored research training from world-class faculty in a highly supportive and productive environment. The proposed work will endow the applicant with the skills needed to transition to the next stage of his research career, transform our treatment of hearing loss, and substantially impact quality of life for millions of Americans. Project Narrative An estimated 37.5 million Americans have hearing loss, which commonly leads to difficulty understanding speech in background noise. The proposed study will test a new noise-reduction system and improve our treatment of hearing loss.",Real-time deep learning to improve speech intelligibility in noise,10268203,F32DC019314,"['Address', 'Algorithms', 'American', 'Area', 'Auditory', 'Cellular Phone', 'Characteristics', 'Cochlear Implants', 'Communication', 'Complex', 'Data', 'Devices', 'Diagnosis', 'Economic Burden', 'Effectiveness', 'Environment', 'Equilibrium', 'Etiology', 'Faculty', 'Fellowship', 'Floor', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Hearing', 'Hearing Aids', 'Human', 'Implant', 'Measures', 'Mentors', 'Mission', 'National Institute on Deafness and Other Communication Disorders', 'Noise', 'Performance', 'Phase', 'Prevention', 'Process', 'Quality of life', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Scheme', 'Seminal', 'Signal Transduction', 'Speech', 'Speech Intelligibility', 'Strategic Planning', 'System', 'Telephone', 'Testing', 'Time', 'Training', 'Translating', 'Universities', 'Videoconferencing', 'Work', 'artificial neural network', 'base', 'career', 'communication device', 'deep learning', 'deep neural network', 'design', 'experimental study', 'health economics', 'hearing impairment', 'hearing loss treatment', 'improved', 'microphone', 'network architecture', 'neural network', 'normal hearing', 'novel', 'novel strategies', 'operation', 'skills', 'speech in noise', 'wearable device']",NIDCD,OHIO STATE UNIVERSITY,F32,2021,79852
"Single-neuron population dynamics in human speech motor cortex for a speech prosthesis PROJECT SUMMARY Augmentative and alternative communication (AAC) technology for people with severe speech and motor impairment (SSMI) continues to improve, with recent advances being made in the neural control of communication devices. In prior NIDCD-supported research, our research team developed a high-performance intracortical brain-computer interface (iBCI) that decodes arm movement intentions directly from brain activity. This technology has allowed people with SSMI to control a computer cursor with sufficient speed and accuracy to type at up to 8 words/min and has enabled full control of unmodified consumer devices using only decoded motor cortical activity. In the proposed U01 clinical research, performed as part of the multi-site BrainGate consortium, we will build upon decades of experience in studying the motor system in humans and non-human primates, with the end goal of advancing iBCI technology. The goals of this project are to study how speech is prepared and produced at the level of ensembles of single neurons in speech-related motor areas of the brain in people with amyotrophic lateral sclerosis (ALS), and to create a speech prosthesis that will allow communication at rates approaching conversational speech (120-150 words per minute). We will approach these investigations with a suite of advanced methods, including (1) newly-developed dynamical systems computational approaches that have provided fundamental insights into the function of the motor system, and (2) machine learning algorithms for decoding of movement intention and language modeling that have formed the basis of the fastest communication prosthesis yet reported. Finally, we will continue to evaluate the safety profile of Utah-array based iBCIs through the ongoing BrainGate2 pilot clinical trial. Upon completion, this project will advance both the capabilities of iBCIs for communication and our understanding of the detailed neural mechanisms of speech production. PROJECT NARRATIVE People with brainstem stroke, advanced amyotrophic lateral sclerosis (ALS, also known as Lou Gehrig’s disease), or other disorders can become unable to speak despite being awake and alert. In this project, we seek a fundamental understanding of how the brain produces speech, and to develop an intracortical brain-computer interface (iBCI) to restore communication at conversational speeds by people with paralysis.",Single-neuron population dynamics in human speech motor cortex for a speech prosthesis,10200463,U01DC019430,"['Address', 'Amyotrophic Lateral Sclerosis', 'Area', 'Articulators', 'Augmentative and Alternative Communication', 'Brain', 'Brain Stem Infarctions', 'Broca&apos', 's area', 'Chronic', 'Clinical Research', 'Clinical Trials', 'Communication', 'Computers', 'Custom', 'Data', 'Devices', 'Dimensions', 'Disease', 'Dorsal', 'Electrocorticogram', 'Electrodes', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hand', 'Handwriting', 'Human', 'Impairment', 'Implant', 'Individual', 'Inferior frontal gyrus', 'Intention', 'Investigation', 'Language', 'Locked-In Syndrome', 'Machine Learning', 'Methods', 'Microelectrodes', 'Modeling', 'Modernization', 'Motor', 'Motor Cortex', 'Movement', 'Muscle', 'National Institute on Deafness and Other Communication Disorders', 'Neurodegenerative Disorders', 'Neurons', 'Neurosciences', 'Output', 'Paralysed', 'Participant', 'Performance', 'Population', 'Population Dynamics', 'Precentral gyrus', 'Preparation', 'Production', 'Prosthesis', 'Quality of life', 'Reporting', 'Research', 'Research Support', 'Safety', 'Signal Transduction', 'Site', 'Speech', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Utah', 'Voice', 'arm', 'arm movement', 'automated speech recognition', 'awake', 'base', 'brain computer interface', 'communication device', 'deep learning', 'design', 'dynamic system', 'experience', 'improved', 'insight', 'kinematics', 'machine learning algorithm', 'motor impairment', 'nervous system disorder', 'neural correlate', 'neuromechanism', 'neuroregulation', 'nonhuman primate', 'orofacial', 'recurrent neural network', 'relating to nervous system', 'safety assessment', 'speech accuracy']",NIDCD,STANFORD UNIVERSITY,U01,2021,1050840
"A Wireless micro-ECoG Prosthesis for Speech Project Summary / Abstract Patients who suffer from debilitating neuromuscular disorders (e.g. amyotrophic lateral sclerosis – ALS, Locked- in-Syndrome – LIS, and muscular dystrophies/myopathies) have difficulty or an inability to communicate through speech leading to a detrimental loss in quality of life. Current technology using eye movements and signals/spellers from electroencephalography (EEG) are slow and inconsistent. Neural prostheses offer an opportunity to produce fast and accurate communication for patients suffering from neuromuscular disorders, but success for regaining speech has been limited due to technological limitations: there is an inability to capture the high dimensionality of the brain and an inability to record in naturalistic conditions using fully implanted, wireless electrode arrays. To solve these challenges, we develop and optimize custom wireless micro- electrocorticographic (µECoG) arrays with over 1,000 channels to decode speech directly from the human brain. We will accomplish this by 1) Testing and optimizing the spatial resolution of µECoG to capture neural signals, 2) Fine-tune our machine learning algorithms to decode speech directly from the brain and 3) developing wireless technology to enable neural prosthetic usage in naturalistic settings. High-density, high channel-count neural interfaces will offer an unprecedented ability to decode speech from the human brain. This ability combined with wireless technology, will allow for a new generation of speech neural prostheses. Project Narrative This project seeks to design, test, and optimize a novel wireless recording array for a neural speech prosthetic. Patients with various types of motor disorders such as Amyotrophic lateral sclerosis (ALS) and Locked-in- Syndrome (LOS) can have difficulty or an inability to communicate. We aim to restore this ability by reading signals directly from the brain and translating them into speech. We will design and test novel high density, high channel-count electrodes to read from the brain at an unprecedented spatial scale. We will also develop wireless technology to enable this speech translation during everyday usage. With our advanced technology for reading the brain, we aim to literally give a voice to the voiceless.",A Wireless micro-ECoG Prosthesis for Speech,10375951,R01DC019498,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Area', 'Auditory', 'Auditory area', 'Brain', 'Clinical', 'Communication', 'Consumption', 'Custom', 'Data', 'Development', 'Electrodes', 'Electroencephalography', 'Environment', 'Eye Movements', 'Family', 'Friends', 'Generations', 'Health Personnel', 'Human', 'Implant', 'Infection prevention', 'Life', 'Locked-In Syndrome', 'Machine Learning', 'Maps', 'Measures', 'Modeling', 'Motor Cortex', 'Muscular Dystrophies', 'Myopathy', 'Neuromuscular Diseases', 'Patients', 'Performance', 'Prosthesis', 'Quality of life', 'Reading', 'Research', 'Resolution', 'Rogaine', 'Sampling', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Voice', 'Wireless Technology', 'Work', 'brain surgery', 'density', 'design', 'high dimensionality', 'improved', 'infection risk', 'liquid crystal polymer', 'machine learning algorithm', 'motor disorder', 'neural prosthesis', 'neurotransmission', 'novel', 'reading ability', 'reconstruction', 'relating to nervous system', 'signal processing', 'success']",NIDCD,DUKE UNIVERSITY,R01,2021,655596
"International Conference on Advances in Quantitative Laryngology,  Voice and Speech Research (AQL) Project Abstract This proposal requests support to convene the next three International Conferences on Advances in Quantitative Laryngology, Voice and Speech Research (AQL). During the 5 years of requested support, the 14th, 15th, and 16th AQL (2021, 2023, and 2025 respectively) will be assembled. The 14th AQL will be held in Bogota, Colombia, South America, June 9 and 10, 2021 (pre-conference workshops June 7 and 8), the 15th will be in Phoenix, AZ, USA, in April 2023, while location of the 16th AQL conference will be determined during the 2021 conference (potential locations in South Korea or Germany). AQL is a valuable scientific meeting in the field of voice research and is regarded by voice and speech science specialists as a high quality international scientific meeting. AQL fosters the exchange of theoretical, experimental, and methodological advances, thereby progressing the translational and clinical aspects of voice and speech science. This multidisciplinary conference brings together scientists, clinicians, and students from around the world in various disciplines including Engineering, Biology, Physics, Otolaryngology and Speech Pathology. This proposal will allow for conference development and continuity to ensure an equitable conference that maintains its cutting-edge focus and provide support for students and early career investigators. The continuity of support will also ensure metrics and evaluations from previous conferences to be used to guide future AQL planning. The goals of this proposal are: to promote and support the education and development of young and underrepresented investigators in the voice and speech research community, organize special sessions on emerging research areas specific to quantitative laryngology, voice and speech research, to develop and foster digital networking strategies to provide a more inclusive and equitable conference, and to establish a Steering Committee. NIH funds are requested to provide support for child/family care, conference support for students and keynote speakers, students to assist Chair/Co-Chairs with various logistic needs, before, during and after the AQL conferences, website design and management, and streaming support during the meeting. In the off- conference years, necessary activities will include: student research trainee support, maintaining AQL website, Steering Committee meetings for conference evaluation and planning. Project Narrative This proposal seeks funding for the upcoming 14th, 15th, and 16th International Conference on Advances in Quantitative Laryngology, Voice and Speech Research (AQL) and the associated workshops and pre- conference, taking place in 2021, 2023, and 2025. AQL is an important international scientific meeting specialized in translational research and methods for measurement and modeling of voice and speech. Special emphasis will be given to promoting young investigators and underrepresented populations, digital networks to support a more inclusive and equitable conference, and to develop a permanent AQL website to help disseminate AQL information.","International Conference on Advances in Quantitative Laryngology,  Voice and Speech Research (AQL)",10237766,R13DC019564,"['Area', 'Award', 'Biology', 'COVID-19 pandemic', 'Carbon', 'Caring', 'Child', 'Child Support', 'Clinical', 'Collection', 'Colombia', 'Communities', 'Development', 'Disabled Persons', 'Discipline', 'Education', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Event', 'Family', 'Fees', 'Fostering', 'Funding', 'Future', 'Germany', 'Goals', 'Health', 'International', 'Leadership', 'Location', 'Logistics', 'Machine Learning', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Modeling', 'Occupations', 'Online Systems', 'Otolaryngology', 'Persons', 'Physics', 'Recording of previous events', 'Request for Proposals', 'Research', 'Research Methodology', 'Research Personnel', 'Safety', 'Science', 'Scientist', 'South America', 'South Korea', 'Specialist', 'Speech', 'Speech Pathology', 'Stream', 'Students', 'Translational Research', 'Travel', 'Underrepresented Populations', 'United States National Institutes of Health', 'Update', 'Voice', 'Woman', 'career', 'cost', 'design', 'digital', 'ethnic minority population', 'expectation', 'improved', 'interest', 'meeting abstracts', 'meetings', 'member', 'minority student', 'multidisciplinary', 'outreach', 'posters', 'preservation', 'programs', 'racial and ethnic', 'recruit', 'social', 'symposium', 'virtual', 'web site']",NIDCD,MAYO CLINIC ARIZONA,R13,2021,40000
"Exploring the brain basis of rhythm in individuals with aphasia PROJECT SUMMARY The goal of this proposal is to characterize the neural substrates underlying rhythm abilities and the relationship with language profiles in individuals with aphasia. Aphasia is an acquired communication disorder resulting from damage to language regions of the brain, with stroke as the leading cause. Currently, over 2 million individuals in the United States are living with aphasia. Aphasia is notoriously difficult to treat and patients exhibit significant individual variability in recovery trajectories and in what therapeutic elements work best in aiding such recovery. Speech-language pathologists frequently use rhythmic elements (e.g., tapping to a beat) in the clinic in order to facilitate speech output. However, there is a lack of a deep and systematic empirical assessment of rhythm in aphasia at both a behavioral and neural level. Our first aim is to characterize the neural basis of individual differences in rhythm abilities in individuals with chronic, post-stroke aphasia. To do this, we will administer a comprehensive battery of rhythm perception and production tasks to a large cohort of individuals with aphasia and age-matched controls. We will then employ multivariate lesion-symptom mapping, a machine-learning methodology for identifying brain-behavior relationships, to determine which brain regions are associated with rhythm processing in aphasia. We hypothesize that individuals who have damage to brain regions important for rhythm, including the basal ganglia or the left inferior frontal gyrus (LIFG), will exhibit the greatest impairments in rhythm. Critically, the LIFG is a core language region typically damaged in post-stroke, non-fluent aphasia. Motivated by robust evidence for associations between rhythm and language across cognitive, neural, and behavioral domains, we will assess the relationship between rhythm and language measures in aphasia in our second aim. We predict that individuals with higher rhythm abilities will have higher language scores, particularly on measures of connected speech. This mentored training award will provide the applicant with training in advanced neural and behavioral data analysis techniques and expertise in large-scale project management with a patient cohort. With significant and timely clinical relevance, our proposal will address vital gaps in the literature by taking an individual differences approach to understanding the relationship between rhythm, the brain, and language in aphasia. PROJECT NARRATIVE Rhythmic elements (e.g., tapping to a beat) are used frequently by speech-language pathologists to facilitate speech output in individuals with aphasia, despite very little empirical and mechanistic groundwork in this area. This proposal aims to characterize the neural substrates underlying rhythm abilities, and the relationship with language profiles, in individuals with chronic, post-stroke aphasia. The proposed experiments, which make critical links between rhythm, the brain, and language, will provide new knowledge for understanding individual differences in rhythm in aphasia and informing future precision medicine approaches.",Exploring the brain basis of rhythm in individuals with aphasia,10386555,F31DC020112,"['Acquired Communication Disorders', 'Address', 'Affect', 'Age', 'Alpha Rhythm', 'Aphasia', 'Area', 'Auditory', 'Basal Ganglia', 'Basic Science', 'Behavioral', 'Brain', 'Brain Injuries', 'Brain region', 'Brain scan', 'Chronic', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Cognitive', 'Communication impairment', 'Conflict (Psychology)', 'Control Groups', 'Data Analyses', 'Discrimination', 'Disease', 'Elements', 'Empirical Research', 'Exhibits', 'Future', 'Goals', 'Heterogeneity', 'Impairment', 'Individual', 'Individual Differences', 'Inferior frontal gyrus', 'Knowledge', 'Language', 'Lead', 'Left', 'Lesion', 'Linguistics', 'Link', 'Literature', 'Location', 'Measures', 'Mentored Research Scientist Development Award', 'Methodology', 'Motor', 'Music', 'Neurobiology', 'Output', 'Participant', 'Pathologist', 'Patients', 'Pattern', 'Perception', 'Performance', 'Periodicity', 'Population', 'Production', 'Recovery', 'Rehabilitation therapy', 'Role', 'Speech', 'Stimulus', 'Stroke', 'Structure', 'Symptoms', 'Techniques', 'Therapeutic', 'Time', 'Tissues', 'Training', 'United States', 'Work', 'base', 'behavioral study', 'brain behavior', 'clinically relevant', 'cohort', 'experimental study', 'improved', 'individual variation', 'infancy', 'knowledge base', 'machine learning method', 'melodic intonation therapy', 'neuromechanism', 'post stroke', 'precision medicine', 'preservation', 'relating to nervous system', 'skills', 'stem', 'stroke-induced aphasia', 'therapy development', 'therapy outcome', 'trait']",NIDCD,VANDERBILT UNIVERSITY,F31,2021,30787
"A Specialized Automatic Speech Recognition and Conversational Platform to Enable Socially Assistive Robots for Persons with Mild-to-Moderate Alzheimer's Disease and Related Dementia Abstract 1 in 3 seniors in the United States dies with dementia, of which Alzheimer’s disease (AD) is the most common form. AD patients suffer from decreased ability to meaningfully communicate and interact, which causes significant stress and burden for both professional caregivers and family members. Socially assistive robots (SARs) have been designed to promote therapeutic interaction and communication. Unfortunately, artificial intelligence (AI) has long been challenged by the speech of elderly persons, who exhibit age-related voice tremors, hesitations, imprecise production of consonants, increased variability of fundamental frequency, and other barriers that can be exacerbated by the neurological changes associated with AD, further complicated by common environmental noises such as the ceiling fan, television, etc. Because of the resulting poor real-world speech and language understanding by available SAR technologies, scarce human caregivers are often required to guide AD patients through SAR interactions, limiting SARs to small deployments, mostly as part of research studies. Unlike existing approaches relying purely on AI, care.coach™ is developing a SAR-like avatar that converses with elderly and AD patients through truly natural speech. Each avatar is controlled by a 24x7 team of trained human staff who can cost-effectively monitor and engage 12 or more patients sequentially (2 simultaneously) through the audio/visual feeds from the patient’s avatar device. The staff communicate with each patient by sending text commands which are converted into the avatar’s voice through a speech synthesis engine. The staff contribute to the system their human abilities for speech and natural language processing (NLP) and for generating free-form conversational responses to help patients build personal relationships with the avatar. The staff are guided by a software-driven expert system embedded into their work interface, which is programmed with evidence-based prompting and protocols to support healthy behaviors and self-care. This SBIR Fast-Track project will leverage the unique data generated by our human- in-the-loop platform to develop new ASR capabilities, enabling fully automatic conversational protocols to engage and support AD patients without human intervention. We aim in Phase I to leverage our unique prior work dataset to train an automatic speech recognition (ASR) engine to enable the understanding of certain types of elderly and AD patient speech more successfully than any currently available engine. We aim in Phase II to incorporate this new engine along with an NLP module into our existing human-in-the-loop avatar system, recruiting a population of AD patients to further train and validate with during a 2-year human subjects study so that we can demonstrate full automation of a significant portion of our avatar conversations with mild- to-moderate level AD patients. Thus, we will improve the commercial scalability of our avatars, while validating our new ASR/NLP engine as the most accurate platform for enabling the next generation of AD-focused SARs. Narrative Artificial intelligence (AI) has long been challenged by the speech of elderly persons, and especially persons with dementia, due to age-related voice tremors, hesitations, imprecise production of consonants, increased variability of fundamental frequency, and other barriers. Unlike existing approaches to socially assistive robots (SARs) relying purely on limited AI for conversation, care.coach™ has been commercializing a SAR-like avatar that converses with elderly and AD patients through truly natural speech, powered by a 24x7 team of trained human staff. The unique data sets that our solution enables us to gather at commercial scale will be leveraged in this SBIR project to develop an automatic speech recognition (ASR) and natural language processing (NLP) engine that is best-in-class for AD applications, improving the commercial scalability of our avatars by reducing our dependence on human staff, while serving as a new AI platform for enabling the next generation of AD- focused, conversational SARs.",A Specialized Automatic Speech Recognition and Conversational Platform to Enable Socially Assistive Robots for Persons with Mild-to-Moderate Alzheimer's Disease and Related Dementia,10263325,R44AG062014,"['Age', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease patient', 'Alzheimer&apos', 's disease related dementia', 'Artificial Intelligence', 'Automation', 'Behavior', 'Caregivers', 'Caring', 'Clinical Research', 'Communication', 'Community Hospitals', 'Computer software', 'Computers', 'Contracts', 'Data', 'Data Set', 'Delirium', 'Dementia', 'Dependence', 'Devices', 'Disease', 'Elderly', 'Exhibits', 'Expert Systems', 'Family', 'Family member', 'Feeds', 'Frequencies', 'Generations', 'Genetic Transcription', 'Goals', 'Home', 'Hospitals', 'Hour', 'Human', 'Hybrids', 'Individual', 'Institutes', 'Intervention', 'Jamaica', 'Label', 'Language', 'Licensing', 'Loneliness', 'Manuals', 'Measures', 'Medical center', 'Modeling', 'Monitor', 'Natural Language Processing', 'Network-based', 'Neural Network Simulation', 'Neurologic', 'Noise', 'Patients', 'Persons', 'Phase', 'Population', 'Price', 'Production', 'Protocols documentation', 'Reaction', 'Self Care', 'Semantics', 'Small Business Innovation Research Grant', 'Social Interaction', 'Social support', 'Speech', 'Stress', 'Study Subject', 'System', 'Techniques', 'Technology', 'Television', 'Text', 'Therapeutic', 'Training', 'Tremor', 'United States', 'Universities', 'Validation', 'Visual', 'Voice', 'Washington', 'Work', 'World Health Organization', 'age related', 'aging population', 'automated speech recognition', 'care providers', 'cost', 'deep neural network', 'depressive symptoms', 'design', 'evidence base', 'falls', 'health plan', 'human subject', 'human-in-the-loop', 'improved', 'next generation', 'older patient', 'patient engagement', 'patient response', 'phrases', 'recruit', 'research study', 'response', 'restraint', 'satisfaction', 'skills', 'social assistive robot', 'speech recognition', 'speech synthesis', 'success', 'usability']",NIA,CARE.COACH CORPORATION,R44,2021,1386676
"Computational Speech Analysis in Alzheimer's Disease and Other Neurocognitive Disorders Abstract: Early and accurate diagnosis of neurocognitive disorders (NCDs) is critical for planning, treatment, and research referral, but demands time and expertise often unavailable to primary care providers. Speech and language are often impaired early in the disease course of several NCDs. Previous research has demonstrated the diagnostic potential of computer speech analysis (CSA), with differences between healthy controls and disorders such as mild cognitive impairment (MCI) and Alzheimer's disease. However, there are several additional steps that must be taken to make CSA a diagnostically viable screening tool. This proposal includes a career development plan providing the applicant with training, mentorship, and experience in the following areas in order to bring CSA techniques into clinical practice: 1) computational linguistics and paralinguistics, 2) longitudinal markers of disease, and 3) design of novel technology for dissemination. As part of this training, academic and professional skills, including ethics in research, will also be expanded. Uniquely qualified mentorship and advisory teams have been selected to ensure the success of the proposed training and research. The proposed study is a prospective, longitudinal, observational, cohort investigation of two distinct research groups. The first group is a highly selected and well-characterized research cohort of healthy control, Alzheimer's disease, and MCI subjects (Group A). In Group A, the performance and reproducibility of a machine learning algorithm will be improved to distinguish Alzheimer's disease and MCI from healthy controls using CSA. Multiple regression and voxel-based morphometry will be used to better understand what may drive group differences in CSA measures in Group A as well. Clinical applications of this algorithm will then be assessed in a clinic-based cohort of patients with different NCDs (Group B) in order reduce spectrum bias likely present in prior studies. As sub-aims in both groups, possible further improvement of the algorithmic outcomes with longitudinal CSA measures will also be examined. The overall objective is to develop intuitive, reliable and reproducible CSA-based clinical measures by correlating them with established neuropsychiatric and imaging markers, determining their efficacy in clinical populations, and determining how they change over time. As a result, this research will validate specific speech traits as useful diagnostic markers of neurocognitive disease and explain why those markers differ between patient groups, both of which are major steps towards the design of novel and easily implemented tools in the screening of NCDs such as Alzheimer's disease. PROJECT NARRATIVE Computational speech analysis (CSA) has shown promise as a cost-effective, rapid screening for patients with neurocognitive disorders (NCDs) by objectively and automatically quantifying speech and language use; however, critical steps must be taken before these measures can become clinically useful. I have training and experience in the neurology of speech and language, but require additional training in computational linguistics and paralinguistics, longitudinal markers of disease including neuroimaging and neuropsychological measures, and design of novel technology for dissemination in order to bring CSA into clinical practice. In this project, we propose to investigate the utility of using CSA measures in two distinct patient groups, including a highly characterized group of research participants that includes healthy controls, Alzheimer's disease patients, and mild cognitive impairment patients (Group A), and a group of consented clinic patients with different NCDs (Group B) and to follow these two groups in prospective, longitudinal studies to correlate spontaneous speech measures with standardized linguistic, neuropsychological, and biological measures.",Computational Speech Analysis in Alzheimer's Disease and Other Neurocognitive Disorders,10145566,K23AG063900,"['Accent', 'Address', 'Adult', 'Advisory Committees', 'Algorithms', 'Alzheimer disease screening', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease diagnosis', 'Alzheimer&apos', 's disease pathology', 'Alzheimer&apos', 's disease patient', 'Area', 'Biological', 'Brain', 'Caregivers', 'Characteristics', 'Classification', 'Clinic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Communities', 'Computational Linguistics', 'Computers', 'Consent', 'Cross-Sectional Studies', 'Dementia', 'Dementia with Lewy Bodies', 'Development', 'Development Plans', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Diagnostic Sensitivity', 'Disease', 'Disease Marker', 'Early Diagnosis', 'Effectiveness', 'Enrollment', 'Ensure', 'Ethics', 'Evaluation', 'Fostering', 'Frontotemporal Dementia', 'Goals', 'Image', 'Impaired cognition', 'Impairment', 'Individual', 'Intuition', 'Investigation', 'Knowledge', 'Language', 'Language Tests', 'Lead', 'Linguistics', 'Liquid substance', 'Longitudinal Studies', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Memory', 'Mentorship', 'Neurocognitive', 'Neurology', 'Neuropsychological Tests', 'Neuropsychology', 'Outcome', 'Participant', 'Patients', 'Performance', 'Population', 'Positioning Attribute', 'Preparation', 'Prevalence', 'Primary Health Care', 'Quality of life', 'Rapid screening', 'Reproducibility', 'Research', 'Screening procedure', 'Speech', 'Standardization', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Variant', 'accurate diagnosis', 'aging population', 'base', 'care providers', 'career development', 'clinical application', 'clinical practice', 'cohort', 'cost effective', 'design', 'diagnostic biomarker', 'experience', 'healthy aging', 'imaging biomarker', 'improved', 'machine learning algorithm', 'mild cognitive impairment', 'morphometry', 'neurocognitive disorder', 'neuroimaging', 'neuropsychiatry', 'new technology', 'novel', 'novel diagnostics', 'novel therapeutics', 'patient screening', 'primary outcome', 'prospective', 'recruit', 'screening', 'skills', 'success', 'technology development', 'tool', 'trait']",NIA,UNIVERSITY OF COLORADO DENVER,K23,2021,188298
"SCH: INT: Collaborative Research: Exploiting Voice Assistant Systems for Early Detection of Cognitive Decline Early detection of Alzheimer’s Disease and Related Dementias (ADRD) in older adults living alone is essential for developing, planning, and initiating interventions and support systems to improve patients’ everyday function and quality of life. Conventional, clinic-based methods for early diagnosis are expensive, impractical, and time-consuming. This project aims to develop a low-cost, passive, and practical home-based assessment method using Voice Assistant Systems (VAS) for early detection of ADRD, including a set of novel data mining techniques for sparse time-series speech. The project has three specific aims: 1. Using a recurrent neural network (RNN) and a softmax regression model, we will develop a transfer learning technique to investigate the link between the speech from in-lab VAS tasks and cognitive decline and discover ADRD-related voice biomarkers. The Pitt Corpus speech database will be used to optimize the RNN parameters and thereby overcome the limited data problem of VAS. The softmax regression model will allow us to align the feature distributions from the previous speech data and in-lab VAS speech; 2. We will develop a novel “many-to- difference” prediction model with a symmetric RNN structure to predict the ADRD-related cognitive differences at two ends of a time period from the sparse time-series data. The proposed model is different from previous ones as the learning focus is shifted from the short-term pattern differences across users to the pattern difference over time for an individual user. The proposed model accommodates well for the highly dynamic nature of the inputs and maximally removes individual characteristics from the prediction result. To analyze the sparse time-series speech, a new data sampling technique will be used to address the imbalanced data problem, and a data quality metric will be developed for the proposed model; 3. The team will conduct an 18- month in-lab evaluation and a 28-month in-home evaluation with a focus on whether the VAS tasks and features from the in-lab evaluation and the repetition features of the in-home VAS data can measure and predict ADRD-related cognitive decline in the in-home participants over time. The proposed methods will be integrated into an interactive system to enable efficient communication on ADRD status among patients, caregivers, and clinicians. If successful, the outcomes of this project will provide an opportunity to provide supportive evidence to clinicians for the early detection of ADRD outside of a clinic-based setting. Project Relevance This project aims to develop a low-cost, passive, and practical cognitive assessment method using Voice Assistant Systems (VAS) for early detection of ADRD-related cognitive decline. If successful, the proposed system may be widely disseminated for the early diagnosis of ADRD to complement existing diagnostic modalities that could ultimately enable long-term patient and caregiver planning to maintain individual’s independence at home. This project aims to develop a low-cost, passive, and practical cognitive assessment method using Voice  Assistant Systems (VAS) for early detection of cognitive decline. If successful, the proposed system may  be widely disseminated for the early diagnosis of cognitive impairment to complement existing diagnostic  modalities that could ultimately enable long-term patient and caregiver planning to maintain individual's  independence at home.",SCH: INT: Collaborative Research: Exploiting Voice Assistant Systems for Early Detection of Cognitive Decline,10190783,R01AG067416,"['Accelerometer', 'Address', 'Age', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease diagnosis', 'Alzheimer&apos', 's disease related dementia', 'Biological Markers', 'Caregivers', 'Cellular Phone', 'Cerebrospinal Fluid', 'Characteristics', 'Clinic', 'Clinical', 'Cognition', 'Cognitive', 'Communication', 'Complement', 'Complex', 'Consumption', 'Custom', 'Data', 'Databases', 'Dementia', 'Devices', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Elderly', 'Evaluation', 'Health', 'Healthcare Systems', 'Home', 'Human', 'Image', 'Impaired cognition', 'Individual', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Link', 'Magnetic Resonance Imaging', 'Measures', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Nature', 'Neural Network Simulation', 'Neuropsychology', 'Outcome', 'Participant', 'Patients', 'Pattern', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Population', 'Privacy', 'Process', 'Psychological Transfer', 'Public Health', 'Quality of life', 'Research', 'Research Personnel', 'Sampling', 'Science', 'Series', 'Services', 'Speech', 'Stress', 'Structure', 'Support System', 'System', 'Tablet Computer', 'Techniques', 'Therapeutic Intervention', 'Time', 'Visit', 'Voice', 'Wrist', 'actigraphy', 'aging in place', 'base', 'burden of illness', 'cognitive change', 'cognitive testing', 'cost', 'data mining', 'data quality', 'deep learning', 'handheld mobile device', 'improved', 'microphone', 'novel', 'predictive modeling', 'recurrent neural network', 'screening', 'sensor', 'therapeutically effective', 'touchscreen', 'usability']",NIA,UNIVERSITY OF MASSACHUSETTS BOSTON,R01,2021,292596
"Digital biomarkers in narrative and conversational speech in Frontotemporal Dementia Today clinical criteria are the gold standard to identify variants of Frontotemporal Dementia (FTD). Though well established and useful clinically, these criteria rely on lengthy procedures that are expensive and invasive. There is urgent need for reliable, objective, and valid measures to screen for likely pathology and monitor disease longitudinally in disease-modifying treatment trials. While preparing the PI to become an independent translational researcher, this research program addresses clinical objectives to develop novel, reproducible, inexpensive, non-invasive, validated biomarkers derived from automated analyses of digitized speech in patients with Frontotemporal Lobar Degeneration (FTLD) pathology. This program is also designed with the scientific objectives to enrich neurobiologic models of language with essential but missing speech components and to improve our pathophysiologic understanding of spreading pathology. In Aim 1 of the K99 mentored phase, the PI will identify informative digitized acoustic and lexical markers in antemortem speech and relate these directly to regional pathologic burden in FTLD-Tau and FTLD-TDP, and uniquely study these speech markers and associated MRI cortical thinning in presymptomatic and symptomatic mutation carriers. In Aim 2, she will examine acoustic and lexical markers longitudinally in FTLD-Tau and FTLD-TDP and in mutation carriers. The PI’s preliminary data show that automated algorithms such as automated speech recognition (ASR) and natural language processing (NLP) tools extract novel digital markers from speech that are sensitive and specific to FTD phenotypes, are associated with imaging and biofluid markers of underlying FTLD pathology, and are related directly to regional pathologic burden in cross-sectional and longitudinal studies of autopsied FTLD-Tau and FTLD-TDP as well as carriers of mutations related to FTLD. During the R00 independent phase, Aim 3 will discover novel markers in the highly elusive domain of natural conversational speech, the most common form of human-to-human communication, and relate these to biomarkers of FTD. The PI will accomplish these aims with the guidance of a world-class mentoring team (Drs. Murray Grossman, Mark Liberman, James Gee, and David Irwin) who have expertise in computational linguistics, neuroimaging, and experimental neuropathology. The PI has available a unique database of 1500 digitized speech samples in deeply endophenotyped autopsied cases and mutation carriers. The mentoring team has a very strong track record of training postdoctoral fellows for independent research careers. A detailed training plan includes regular one-on-one meetings, seminars, coursework, grant writing, and responsible conduct of research. The new skills acquired during this period, combined with the PI’s prior expertise in cognitive neurology and leadership experience as a Major in the military, will ensure a strong foundation to launch an independent translational research career that will synergistically address vital scientific and clinical issues. This proposal aims to develop novel, inexpensive and reproducible digital speech markers of aphasic and non- aphasic variants of Frontotemporal Dementia (FTD) using automated computational linguistic algorithms, and validate them using quantitative, regional Frontotemporal Lobar Degeneration (FTLD) pathology and carriers of mutations associated with FTLD spectrum pathology. This will advance neurobiologic models of speech and provide validated, longitudinal in vivo models reflecting spreading pathology while synergistically developing in vivo screening measures for specific FTLD pathologies and providing validated longitudinal markers for disease-modifying treatment trials. The research strategies outlined in this proposal will give me the necessary training to advance the study of neurodegeneration and improve the lives of these patients, their caregivers and families while I develop the skill set to become an independent, NIH-funded translational investigator.",Digital biomarkers in narrative and conversational speech in Frontotemporal Dementia,10284370,K99AG073510,"['Acoustics', 'Address', 'Algorithms', 'Anatomy', 'Anterior', 'Aphasia', 'Atrophic', 'Autopsy', 'Behavior', 'Biological Markers', 'Caregivers', 'Caring', 'Clinical', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Cognitive', 'Communication', 'Complex', 'Computational Linguistics', 'Consumption', 'Cross-Sectional Studies', 'Data', 'Databases', 'Dementia', 'Development', 'Disease', 'Disease Marker', 'Ensure', 'Family', 'Foundations', 'Frontotemporal Dementia', 'Frontotemporal Lobar Degenerations', 'Funding', 'Goals', 'Gold', 'Grant', 'Human', 'Image', 'Impairment', 'Inferior', 'Knowledge', 'Language', 'Leadership', 'Life', 'Linguistics', 'Link', 'Longitudinal Studies', 'Magnetic Resonance Imaging', 'Manuals', 'Measures', 'Mentors', 'Methods', 'Military Personnel', 'Modeling', 'Monitor', 'Mutation', 'Natural Language Processing', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Neuropsychology', 'Non-aphasic', 'Pathogenicity', 'Pathologic', 'Pathology', 'Pathway interactions', 'Patients', 'Pattern', 'Personal Satisfaction', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Procedures', 'Production', 'Regional Disease', 'Reproducibility', 'Research', 'Sampling', 'Speech', 'Speech Acoustics', 'Thick', 'Thinness', 'Time', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'Writing', 'automated algorithm', 'automated analysis', 'automated speech recognition', 'career', 'design', 'digital', 'endophenotype', 'experience', 'frontal lobe', 'frontotemporal degeneration', 'impression', 'improved', 'in vivo', 'in vivo Model', 'lexical', 'meetings', 'minimally invasive', 'mutation carrier', 'neuroimaging', 'neuropathology', 'novel', 'novel marker', 'post-doctoral training', 'programs', 'responsible research conduct', 'screening', 'skills', 'social communication', 'tau Proteins', 'tool', 'translational scientist', 'treatment trial']",NIA,UNIVERSITY OF PENNSYLVANIA,K99,2021,129465
"Longitudinal neuroimaging and neurocognitive assessment of risk and protective factors across the schizophrenia spectrum ABSTRACT  The parent R01 project is a longitudinal study examining risk and protective factors in the schizophrenia (SZ) spectrum—from healthy controls (HCs) to individuals with schizotypal personality disorder (SPD) to recent-onset SZ patients (80 per group)—using MRI and neurocognitive approaches. It tests a neurobiological model which posits that individuals with SPD—an intermediate phenotype—have protective factors against developing threshold psychosis, such as preservation of frontal lobe and less severe temporal lobe abnormalities compared to SZ that lead to milder cognitive and social impairments. Examining natural language processing (NLP) as proposed in this supplement is in line with the scope and aims of the parent R01 project and may inform the key neurobiological model being tested. Moreover, examining NLP using novel measures of semantics and syntax in association with measures from the parent R01 of frontal and temporal white matter integrity/connectivity assessed with diffusion tensor imaging and cognitive domains such as processing speed and working memory is innovative.  Speech and language provide a rich source of data on human thought, including semantic and emotional content, semantic coherence (i.e. flow of meaning), and syntactic structure and complexity (i.e. usage of parts of speech). There is a critical gap in our understanding of the linguistic mechanisms that underlie thought disorder in SZ spectrum. The use of automated linguistic analytic methods has been limited to only a few studies focused on discriminating SZ patients from HCs and predicting psychosis.  Together with our colleagues with expertise in NLP at Icahn School of Medicine at Mount Sinai, we will use advanced computational speech analytic approaches to identify the linguistic basis of language production along a spectrum from normal to thought disordered. We will use optimal interviewing techniques1 to obtain open-ended 30-45 minute narratives from the large (N = 240) English-speaking sample in the parent R01 study, with a range of language disturbances across the spectrum ranging from none/subtle to severe. NLP techniques including Latent Semantic Analysis2 (LSA) and part-of-speech (POS) tagging3,4 will be conducted using artificial intelligence to examine semantic and syntactic language features to include in our overall neurobiological model. These analyses yield fine-grained indices of speech and language that may more accurately capture thought disorder.  Three specific aims will assess (1) semantic coherence in language production using LSA2 and examine its association with positive symptoms and functional impairment across the spectrum; (2) syntactic complexity in language production using POS tagging3,4 and measure acoustic features to examine their association with negative symptoms and functional impairment; and (3) the relationship between language and speech features (semantic, syntactic, and acoustic) with putative white matter integrity assessed using diffusion tensor imaging. PROJECT NARRATIVE Speech and language provide a rich source of data on human thought, including semantic and emotional content, semantic coherence (i.e. flow of meaning), and syntactic structure and complexity (i.e. usage of parts of speech). This supplement is in line with the scope and aims of the parent R01 study and in collaboration with our colleagues with expertise in natural language processing at ISMMS, we will: (a) leverage the parent R01—a study of risk and protective factors in the schizophrenia (SZ) spectrum using MRI and neurocognitive measures in healthy controls, individuals with schizotypal personality disorder (SPD), and SZ patients (80 per group); and (b) use advanced computational speech analytic approaches to identify the linguistic basis– semantics and syntax–that underlies language production along a spectrum from normal to gradations of thought disorder across the SZ spectrum. No study has rigorously examined this in individuals with SPD as proposed in this application.",Longitudinal neuroimaging and neurocognitive assessment of risk and protective factors across the schizophrenia spectrum,10381940,R01MH121411,"['Acoustics', 'Activities of Daily Living', 'Administrative Supplement', 'Anisotropy', 'Artificial Intelligence', 'Clinical', 'Cognitive', 'Collaborations', 'Computers', 'Data', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Emotional', 'Goals', 'Grain', 'Grant', 'Graph', 'Human', 'Impairment', 'Individual', 'Inferior', 'Interview', 'Language', 'Lead', 'Linguistics', 'Longitudinal Studies', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Metaphor', 'Methods', 'Modeling', 'Multimodal Imaging', 'Natural Language Processing', 'Neurobiology', 'Neurocognitive', 'Parents', 'Patients', 'Performance', 'Phenotype', 'Production', 'Psychoses', 'Quality of life', 'Risk', 'Risk Assessment', 'Risk Factors', 'Sampling', 'Schizophrenia', 'Schizotypal Personality Disorder', 'Scientist', 'Semantics', 'Short-Term Memory', 'Social Adjustment', 'Source', 'Speech', 'Structure', 'Symptoms', 'Techniques', 'Temporal Lobe', 'Testing', 'Thick', 'Time', 'analytical method', 'blood oxygenation level dependent response', 'brain abnormalities', 'cohort', 'frontal lobe', 'functional disability', 'functional outcomes', 'indexing', 'innovation', 'longitudinal course', 'medical schools', 'multimodality', 'neuroimaging', 'novel', 'preservation', 'processing speed', 'protective factors', 'schizophrenia-spectrum disorder', 'social', 'social cognition', 'syntax', 'white matter']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2021,206753

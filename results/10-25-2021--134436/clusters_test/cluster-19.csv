text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"High Performance Text Mining for Translator We propose to build a knowledge provider that will seek out, integrate and provide AI-ready, BioLink-compatible models via high-performance text-mining of the biomedical literature. Problems with Translator’s current mining of the biomedical literature that we intend to solve include: (1) weaknesses in framework extensibility and benchmarking that make integrating and validating new text-mining approaches difficult; (2) problematic licensing of software, terminologies and other resources that do not adequately support FAIR (and TLC) best practices; (3) processing only PubMed titles and abstracts, not full text publications; (4) Translator’s use of older NLP technology with relatively poor performance; (5) lack of a mechanism for community feedback regarding errors and other problems; (6) lack of continuous updates to add knowledge from new publications; (7) output knowledge representation that is simplistic and vague, failing to reflect the richness of what is expressed in scientific documents. Plan for implementation: Our team has a long history of productive NLP research, successful open source software projects, effective benchmarking and broad community engagement. We will build on the results of NLM-funded work in information extraction, our gold-standard Colorado Richly Annotated Full Text (CRAFT) corpus, a recent BioNLP Open Shared Task (BioNLP-OST) that we organized, and recent advances in state-of-the-art NLP. For Segment 1, we will: (1) Demonstrate BioStacks, an extensible, cloud-based text-mining framework that produces knowledge graphs grounded in the Open Biomedical Ontologies (OBOs). This BioStacks demo will include a state-of-the-art OBO concept recognizer for multiple ontologies, a state-of-the-art semantic relationship prediction tool, and a state-of-the-art structural analysis tool. All generated assertions will have provenance metadata linking the assertion to a particular text span in a document specified by PMCID. (2) Demonstrate CRAFTST, a cloud-based text-mining evaluation system that evaluates the performance of text-mining systems against the CRAFT gold standard. (3) Demonstrate an adaptive machine learning process illustrating how to efficiently create tools to extract BioLink association types. For Segment 2, we propose to extend the text-mining and evaluation frameworks to align with BioLink and the Translator community, improve text-mining quality and expand the collection of source documents mined. Specifically, we propose to target 10 long term milestones: (1) Align CRAFT to BioLink. (2) Develop new tools for extracting associations from text. (3) Develop and manage a community engagement process on text-mining for Translator. (4) Extend benchmarking. (5) Improve recall. (6) Improve precision. (7) Improve computational efficiency. (8) Expand BioStacks to include all available full text biomedical journal articles. (9) Expand document collections to include Patents & Regulatory filings. (10) Develop a scientist-based movement to improve document access for text-mining from non-open publishers. The types of questions the resulting knowledge graph can be used to address are extremely broad, as it is generated by mining a large part of the biomedical literature. Questions that can be answered include those about specific assertions (e.g. is this drug an agonist-activator of this protein?), general relations (are these two proteins often mentioned together?), and documents (which publications mention this gene, mutation and drug?). Integration: We are long-time contributors to the open-science community and have longstanding collaborations with existing awardees; we were participants in the NIH Data Commons Pilot. We propose to align the output of text-mining tools to the BioLink model via OBO terms. We propose to implement our frameworks in NIH Cloud Computing environments. We propose to adopt the CD2H Contributor Attribution Model to foreground community contributions. We plan to coordinate with the NLM’s nascent benchmarking activities and the SmartAPI effort to build Translator standard interfaces. Challenges and gaps: High-performance mining of rich, contextualized knowledge from the literature remains a difficult task, and is unlikely to be solved in the next five years. Many important publications remain inaccessible to text-mining due to restrictive licensing. n/a",High Performance Text Mining for Translator,10334356,OT2TR003422,"['Address', 'Adopted', 'Agonist', 'Benchmarking', 'Cloud Computing', 'Collaborations', 'Collection', 'Colorado', 'Communities', 'Computer software', 'Data Commons', 'Environment', 'Evaluation', 'Feedback', 'Funding', 'Gene Mutation', 'Gold', 'Information Retrieval', 'Knowledge', 'Legal patent', 'Licensing', 'Link', 'Literature', 'Machine Learning', 'Metadata', 'Mining', 'Modeling', 'Movement', 'Ontology', 'Output', 'Participant', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Provider', 'PubMed', 'Publications', 'Recording of previous events', 'Research', 'Resources', 'Scientist', 'Semantics', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Terminology', 'Text', 'Time', 'United States National Institutes of Health', 'Update', 'Work', 'base', 'biomedical ontology', 'cloud based', 'community engagement', 'improved', 'information organization', 'journal article', 'knowledge graph', 'knowledge of results', 'open data', 'open source', 'text searching', 'tool']",NCATS,UNIVERSITY OF COLORADO DENVER,OT2,2021,471239
"Audio Generation and Optimization from Existing Resources for Patient Education Project Summary/Abstract Health literacy is vital to achieving and maintaining good health. Several national programs have emphasized this goal and its importance. Text is generally much more efficient and cost-effective for presenting healthcare information on a large scale than interactive tools and videos. Over the past decade, therefore, most medical information has been provided as text, e.g., via printed pamphlets or on websites. We are entering a new era where a new similarly effective mode of information dissemination is becoming increasingly available: audio accessed with mobile devices. Millions of households have and use smart speakers and virtual assistants and they are increasingly used by patients and consumers to gather information. Hospitals also plan to gradually integrate them among their tools. However, there exist few if any guidelines on optimal generation and use of audio. The overall goal of this project is to discover how to support the creation of optimal audio from existing text sources for consumer and patient education. To accomplish this, four aims are proposed. The first aim is to identify audio features that affect information comprehension and retention. Here, features in audio content and style (e.g., word frequency or grammatical complexity) of the underlying information will be tested for impact. In addition, two groups of features specific to the audio medium will be tested: the delivery features (e.g., speed and pauses) as well as meta-features (e.g., speaker characteristics such as gender or accent and bias in listeners). This first aim will rely on large-scale datasets, semi-automatically generated and augmented with user scores for comprehension gathered using Amazon Mechanical Turk (MTurk). Statistical and machine learning approaches will be used to tease out the best features and combinations. The second aim focuses on discovering how to augment text for audio and finding the optimal combination of text and audio for information comprehension and retention. Different combinations will be tested online with MTurk participants using controlled user studies. The third aim is to update, test and provide the existing online free text editor to generate optimized audio. We will also start dissemination of the tool to potential users including API access to components. The project will conclude with a summative evaluation with representative consumers recruited at a local community health center and further dissemination of preferences, practical obstacles, and best practices for the medical community to help increase health literacy through this new, popular audio medium. If successful, this project will generate best practices for the medical community in using audio as an additional method for bringing healthcare information to the general public; it will provide an online, free tool to generate audio leveraging these best practices and will include API access so that other researchers can easily integrate tool components into their research and tools; and it will provide immediate practical lessons from working with consumers relevant for clinical practice. Project Narrative Improving health literacy is an important national goal and necessary for achieving and maintaining health. The increased adoption of smart speakers and virtual assistants by consumers and in medical settings has created novel opportunities to educate the population by delivering health information through audio or audio combined with text. Because few (if any) tools exist to improve content and generate optimal audio, we aim to discover supportive and adverse features of audio/text information provision and create a free, online software tool for optimizing health-related text with demonstrated impact on information comprehension and retention.",Audio Generation and Optimization from Existing Resources for Patient Education,10295641,R01LM011975,"['Accent', 'Adoption', 'Affect', 'Affordable Care Act', 'Age', 'Arizona', 'Characteristics', 'Collaborations', 'Communication', 'Communities', 'Comprehension', 'Computer software', 'Computers', 'Data', 'Development', 'Effectiveness', 'Evaluation', 'Frequencies', 'Gender', 'General Population', 'Generations', 'Goals', 'Government', 'Growth', 'Guidelines', 'Health', 'Healthcare', 'Hospitals', 'Household', 'Information Dissemination', 'Machine Learning', 'Measures', 'Mechanics', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Neighborhood Health Center', 'Operative Surgical Procedures', 'Outcome', 'Pamphlets', 'Participant', 'Patient Education', 'Patients', 'Pilot Projects', 'Population', 'Research', 'Research Personnel', 'Resources', 'Software Tools', 'Source', 'Specific qualifier value', 'Speed', 'Technology', 'Testing', 'Text', 'Update', 'Voice', 'Work', 'Work Simplification', 'Writing', 'application programming interface', 'clinical encounter', 'clinical practice', 'clinically relevant', 'cost effective', 'design', 'digital', 'experience', 'handheld mobile device', 'health literacy', 'improved', 'information processing', 'innovation', 'intelligent personal assistant', 'interactive tool', 'large scale data', 'novel', 'open source tool', 'preference', 'programs', 'real world application', 'recruit', 'skills', 'statistical and machine learning', 'symposium', 'tool', 'web site']",NLM,UNIVERSITY OF ARIZONA,R01,2021,257905
"Image-guided Biocuration of Disease Pathways From Scientific Literature Realization of precision medicine ideas requires an unprecedented rapid pace of translation of biomedical discoveries into clinical practice. However, while many non-canonical disease pathways and uncommon drug actions, which are of vital importance for understanding individual patient-specific disease pathways, are accumulated in the literature, most are not organized in databases. Currently, such knowledge is curated manually or semi-automatically in a very limited scope. Meanwhile, the volume of biomedical information in PubMed (currently 28 million publications) keeps growing by more than a million articles per year, which demands more efficient and effective biocuration approaches.  To address this challenge, a novel biocuration method for automatic extraction of disease pathways from figures and text of biomedical articles will be developed.  Specific Aim 1: To develop focused benchmark sets of articles to assess the performance of the biocuration pipeline.  Specific Aim 2: To develop a method for extraction of components of disease pathways from articles’ figures based on deep-learning techniques.  Specific Aim 3: To develop a method for reconstruction of disease-specific pathways through enrichment and through graph neural network (GNN) approaches.  Specific Aim 4: To conduct a comprehensive evaluation of the pipeline.  The overarching goal of this project is to develop a computer-based automatic biocuration ecosystem for rapid transformation of free-text biomedical literature into a machine-processable format for medical applications.  The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. It will especially benefit cancer patients for which up-to-date knowledge of newly discovered molecular mechanisms and drug actions is critical. The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. In this project, a novel biocuration method for an automatic extraction of disease mechanisms from figures and text in scientific literature will be developed. These mechanisms will be stored in a database for further querying to assist in medical diagnosis and treatment.",Image-guided Biocuration of Disease Pathways From Scientific Literature,10149399,R01LM013392,"['Address', 'Architecture', 'Benchmarking', 'Biological', 'Cancer Patient', 'Communities', 'Computers', 'Databases', 'Deposition', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Pathway', 'Ecosystem', 'Elements', 'Evaluation', 'Feedback', 'Genes', 'Goals', 'Graph', 'Health', 'Image', 'Informatics', 'Knowledge', 'Label', 'Language', 'Link', 'Literature', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Measures', 'Medical', 'Methods', 'Molecular', 'Molecular Analysis', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Oxidative Stress', 'Pathway interactions', 'Patients', 'Performance', 'Phenotype', 'PubMed', 'Publications', 'Regulation', 'Reporting', 'Research', 'Retrieval', 'Selection Criteria', 'Signal Pathway', 'Source', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Translations', 'Visual', 'Work', 'base', 'clinical practice', 'deep learning', 'design', 'detector', 'drug action', 'image guided', 'improved', 'individual patient', 'knowledge base', 'knowledge curation', 'multimodality', 'neural network', 'neural network architecture', 'novel', 'precision medicine', 'reconstruction', 'success', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2021,313018
"Curation at scale: Integrating AI into community curation Project Summary Biological knowledgebases are a critical resource for researchers and accelerate scientific discoveries by providing manually curated, machine-readable data collections. However, the aggregation and manual curation of biological data is a labor-intensive process that relies almost entirely on professional biocurators. Two approaches have been advanced to help with this problem: natural language processing (NLP; text mining (TM) and machine learning (ML)) and engagement of researchers (community curation). However, neither of these approaches alone is sufficient to address the critical need for increased efficiency in the biocuration process. Our solution to these challenges is an NLP-enhanced community curation portal, Author Curation to Knowledgebase (ACKnowledge). The ACKnowledge system, currently implemented for the C. elegans literature, couples statistical methods and text mining algorithms to enhance community curation of research articles. We propose to strengthen and expand ACKnowledge by including other species into our pipeline, incorporating more sophisticated machine learning models, and presenting sentence-level entity and concept extraction for more detailed author curation. In addition, we will develop an Author Curation Portal (ACP) to allow authors to easily upload and curate their own documents. Taken together, these enhancements will allow us to maximize community curation efforts by leveraging author expertise in multiple areas of biology, while at the same time supporting authors with as much AI-assisted curation as possible. This reciprocal interaction will improve not only the content of knowledgebases, but the AI methods themselves, as we will receive valuable feedback on our models. By developing an Author Curation Portal, we will further empower authors to participate in the curation process and alert knowledgebases to key information that can, and should, be readily discoverable in accordance with FAIR (Findable, Accessible, Interoperable, and Reusable) data principles. Project Narrative The pace at which biological knowledge is published in the scientific literature far surpasses the ability of professional biocurators to collate and annotate such knowledge in a timely fashion. However, knowledgebases are a critical component of biological research whose utility depends on prompt entry of new data. Combining the dual benefits of text mining and community curation, the proposed ACKnowledge system, readily generalized, presents a novel approach to aiding curation by guiding authors to efficiently and accurately annotate their papers, thus quickly and reliably adding new information to knowledgebases.",Curation at scale: Integrating AI into community curation,10344771,R01LM013871,"['Address', 'Algorithms', 'Area', 'Benchmarking', 'Biological', 'Biology', 'Bypass', 'Caenorhabditis elegans', 'Classification', 'Communities', 'Computer Assisted', 'Couples', 'Coupling', 'Data', 'Data Collection', 'Databases', 'FAIR principles', 'Feedback', 'Generations', 'Gold', 'Infrastructure', 'Ingestion', 'Knowledge', 'Literature', 'Machine Learning', 'Manuals', 'Methods', 'Microbiology', 'Modeling', 'Names', 'Natural Language Processing', 'Paper', 'Process', 'Publications', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Resources', 'Speed', 'Statistical Methods', 'System', 'Text', 'Time', 'Training', 'Validation', 'WormBase', 'biological research', 'dashboard', 'data submission', 'design', 'genome resource', 'improved', 'innovation', 'knowledge base', 'learning engagement', 'machine learning algorithm', 'machine learning method', 'member', 'novel strategies', 'repository', 'text searching', 'tool']",NLM,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2021,355938
"Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing The adoption of Electronic Health Record (EHR) systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format with tremendous potential but an equally large concern for patient confidentiality breaches. Secondary use of clinical data is essential to fulfill the potential for high quality healthcare, improved healthcare management, and effective clinical research. NIH expects that larger research projects share their research data in a way that protects the confidentiality of research subjects. De-identification of patient data has been proposed as a solution to both facilitate secondary use of clinical data and protect patient data confidentiality. The majority of clinical data found in the EHR is represented as narrative text clinical notes, and de-identification of clinical text is a tedious and costly manual endeavor. Automated approaches based on Natural Language Processing have been implemented and evaluated, allowing for higher accuracy and much faster de- identification than manual approaches. Clinacuity, Inc. proposes to advance a text de-identification system from a prototype to an accurate, adaptable, and robust system, integrated into the research infrastructure at our implementation and testing site (Medical University of South Carolina, Charleston, SC), and ready for commercialization efforts. To accomplish this undertaking, we will focus on the following specific aims and related objectives, while continuing to prepare the commercialization of the integrated system, with detailed market analysis, commercial roadmap development, and modern media communication: 1) Enhance the text de-identification system performance, scalability, and quality to produce an enterprise-grade solution ready for deployment; 2) Enable use of structured data for enhanced text de-identification (when structured PII is available) and for complete patient records de-identification (i.e., records combining structured and unstructured data). This aim also includes implementing “one-way” pseudo-identifier cryptographic hashing to enable securely linking already de-identified patient records; 3) Integrate the text de-identification system with a research data capture and management system. This includes implementation of the de-identification system as a secure web service, with standards-based access and integration. This de-identification system has potential commercial applications in clinical research and in healthcare settings. It will improve access to richer, more detailed, and more accurate clinical data (in clinical text) for clinical researchers. It will ease research data sharing (as expected for larger NIH-funded research projects) and help healthcare organizations protect patient data confidentiality. Significant time-savings will also be offered, with a process at least 200-1000 times faster than manual de-identification. The adoption of Electronic Health Record systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format, with tremendous potential, but also equally growing concern for patient confidentiality breaches. De-identification of patient data has been proposed as a solution to both facilitate secondary uses of clinical data and protect patient data confidentiality. This project will advance a text de-identification system from a prototype to an accurate, adaptable and robust system allowing for complete patient records de-identification, integrated in the research infrastructure at our implementation and testing site and ready for commercialization efforts. It will improve access to richer, more detailed, and more accurate clinical data for clinical researchers, ease research data sharing and help healthcare organizations protect patient data confidentiality. !",Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing,10098325,R42GM116479,"['Adoption', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Communications Media', 'Confidentiality of Patient Information', 'Data', 'Deimplementation', 'Development', 'Electronic Health Record', 'Environment', 'Fast Healthcare Interoperability Resources', 'Funding', 'Growth', 'Health Insurance Portability and Accountability Act', 'Improve Access', 'Link', 'Manuals', 'Medical', 'Modernization', 'National Institute of General Medical Sciences', 'Natural Language Processing', 'Patient Care', 'Patient Data Privacy', 'Patients', 'Performance', 'Personally Identifiable Information', 'Phase', 'Privacy', 'Process', 'Records', 'Reference Standards', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Risk', 'Savings', 'Secure', 'Site', 'South Carolina', 'Speed', 'Structure', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'Universities', 'Visualization', 'base', 'commercial application', 'commercialization', 'cost', 'cryptography', 'data reuse', 'data sharing', 'health care quality', 'health care service organization', 'health care settings', 'health management', 'improved', 'large scale data', 'participant enrollment', 'prototype', 'software development', 'standard measure', 'structured data', 'systems research', 'unstructured data', 'web services']",NIGMS,"CLINACUITY,INC.",R42,2021,725232
"Protein Knowledge Networks and Semantic Computing for Disease Discovery Protein Knowledge Networks and Semantic Computing for Disease Discovery The growing volume and breadth of information from the scientific literature and biomedical databases pose challenges to the research community to exploit the content for discovery. This MIRA grant application will advance our knowledge mining and semantic computing system to accelerate data-driven discovery for understanding of gene-disease-drug relationships. We have employed natural language processing and machine learning approaches in a generalizable framework for bioentity and relation extraction from large-scale text. Our Protein Ontology supports protein-centric semantic integration of biomedical data for both human understanding and computational reasoning. We have also developed a resource to support functional interpretation and analysis of protein post-translational modifications (PTMs) across modification types and organisms. Building on our computational algorithms, bioinformatics infrastructure and community interactions, we will further develop literature mining tools to support automated information extraction across the bibliome and open linked data models for semantic integration of biomedical data from heterogeneous resources. Our text mining tools will be trained for different use cases using deep learning methods. We will develop RDF (Resource Description Framework) semantic models in an increasingly computable, inferable and explainable knowledge system to assist in hypothesis generation. We will present evidence in the form of textual artifacts and semantic models to ensure unbiased analysis and interpretation of results to promote rigorous and reproducible research. We will develop scientific case studies to drive the system development. Examples include PTM disease variant and enrichment analyses for drug target identification, genotype- phenotype knowledge mining for Alzheimer's Disease understanding, and gene-disease-drug knowledge network construction for COVID-19 drug repurposing. To foster community engagement, we will host workshops and hackathons to address critical fundamental research questions and emerging disease scenarios. We have fully adopted the FAIR (Findable, Accessible, Interoperable, Reusable) principles for resource sharing. All data, tools and research results will be broadly disseminated from the project website, accessible programmatically via RESTful API, queryable via SPARQL endpoints, and dockerized for community code reuse. The successful completion of this research will thus support scalable, integrative and collaborative knowledge discovery to accelerate disease understanding and drug target discovery. PROJECT NARRATIVE The accelerated growth of research data of increasing volume and complexity is profoundly transforming biomedical research, allowing scientists to ask new research questions and address emerging disease scenarios. We will further develop our literature mining system with an automated workflow for extraction of scientific knowledge across the bibliome, along with semantic integration of biomedical data for computational reasoning in an inferable and explainable knowledge system. The combined effort will aid in improving understanding of human disease and identification of potential drug targets for major health conditions like Alzheimer's Disease or urgent pandemics such as COVID-19.",Protein Knowledge Networks and Semantic Computing for Disease Discovery,10207002,R35GM141873,"['Address', 'Adopted', 'Alzheimer&apos', 's Disease', 'Applications Grants', 'Biomedical Research', 'COVID-19', 'Case Study', 'Code', 'Communities', 'Computational algorithm', 'Computer Systems', 'Data', 'Databases', 'Disease', 'Drug Targeting', 'Educational workshop', 'Ensure', 'FAIR principles', 'Fostering', 'Generations', 'Genes', 'Genotype', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Knowledge', 'Knowledge Discovery', 'Link', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Modification', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Organism', 'Pharmaceutical Preparations', 'Phenotype', 'Post-Translational Protein Processing', 'Protein Analysis', 'Proteins', 'Reproducibility', 'Research', 'Resource Description Framework', 'Resource Sharing', 'Resources', 'Scientist', 'Semantics', 'System', 'Systems Development', 'Text', 'Training', 'Variant', 'bioinformatics infrastructure', 'community engagement', 'computational reasoning', 'data modeling', 'data tools', 'deep learning', 'drug repurposing', 'fundamental research', 'hackathon', 'human disease', 'improved', 'learning strategy', 'pandemic disease', 'text searching', 'tool', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,R35,2021,433379
"Personalized Patient data and behavioral nudges to improve adherence to chronic cardiovascular medications Abstract Our objective in this project is to employ population level pharmacy data and delivery of nudges via cell phone text messaging and artificially intelligent (AI) interactive chat bot to improve medication adherence and patient outcomes in 3 integrated healthcare delivery systems (HCS): University of Colorado Health System (UCHealth), VA Eastern Colorado Health Care System (VA), and Denver Health Medical Center (DH). We will identify patients with chronic cardiovascular (CV) conditions taking medications to treat hypertension, atrial fibrillation, coronary artery disease, diabetes and/or hyperlipidemia. We will leverage pharmacy refill data to identify episodes of non-adherence through gaps in medication refills and randomize individuals to 1 of 4 study arms when they have a first refill gap: 1) usual care; 2) generic text message reminder; 3) tailored and engaging text messages optimized to facilitate behavior change; or 4) optimized text messages plus a pre- programmed AI interactive chat bot designed to support identification and resolution of barriers to medication refill and adherence. In the UG3 phase (year 1), we will develop and program a theoretically informed technology-based (a) text message library and (b) chat bot content library using multiple and iterative N of 1 within subject studies to optimize content for a range of diverse patients. These outcomes will inform a pilot intervention to demonstrate feasibility of delivering the intervention and preliminary effects in all 3 HCS. We will also engage patient, provider and health systems stakeholders in designing, refining, and implementing the pilot intervention. In the UH3 phase (years 2-5), we will conduct a pragmatic patient-level randomized intervention across 3 HCS to improve adherence to chronic CV medications. We will evaluate the intervention using a mixed methods approach and apply the RE-AIM (reach, effectiveness, adoption, implementation, and maintenance) framework. In addition, we will assess the context and implementation processes to inform local tailoring, adaptations and modifications, and eventual expansion of the intervention. Project Narrative Given the increasing prevalence of chronic diseases that require ongoing treatment with medications, the problem of medication non-adherence is unlikely to go away and will grow. This study will employ population level pharmacy data to identify non-adherent patients and utilize ubiquitous cell phone technology to send them tailored, engaging and motivating text messages and text message based chat through an artificially intelligent (AI) interactive chat bot to improve cardiac medication adherence and patient outcomes in 3 integrated healthcare delivery systems. This study will advance medication adherence research and the intervention could be applied to multiple clinical conditions requiring medications for long term treatment and other NIH institutes.!",Personalized Patient data and behavioral nudges to improve adherence to chronic cardiovascular medications,10200136,UH3HL144163,"['Acute myocardial infarction', 'Adherence', 'Artificial Intelligence', 'Atrial Fibrillation', 'Attention', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Sciences', 'Body Weight decreased', 'Cardiac', 'Cardiovascular system', 'Cellular Phone', 'Chronic', 'Chronic Disease', 'Clinical', 'Colorado', 'Coronary Arteriosclerosis', 'Data', 'Decision Making', 'Diabetes Mellitus', 'Disease', 'Event', 'Goals', 'Health', 'Health Care Costs', 'Health Promotion', 'Health Technology', 'Health behavior', 'Health system', 'Healthcare Systems', 'Hospitalization', 'Hospitals', 'Human', 'Hyperlipidemia', 'Hypertension', 'Individual', 'Institutes', 'Integrated Delivery of Health Care', 'Intervention', 'Intervention Studies', 'Libraries', 'Measures', 'Medical center', 'Methods', 'Modification', 'Morbidity - disease rate', 'Outcome', 'Patient Education', 'Patient Noncompliance', 'Patient-Focused Outcomes', 'Patients', 'Pharmaceutical Preparations', 'Pharmacists', 'Pharmacy facility', 'Phase', 'Pilot Projects', 'Population', 'Prevalence', 'Provider', 'Randomized', 'Reach, Effectiveness, Adoption, Implementation, and Maintenance', 'Recommendation', 'Research', 'Resolution', 'Resources', 'Self Management', 'Study Subject', 'System', 'Technology', 'Testing', 'Text Messaging', 'United States National Institutes of Health', 'Universities', 'base', 'behavior change', 'behavioral economics', 'blood pressure regulation', 'chatbot', 'copayment', 'cost', 'design', 'digital', 'evidence base', 'financial incentive', 'four-arm study', 'health care delivery', 'health care service utilization', 'implementation process', 'improved', 'individual response', 'medication compliance', 'medication nonadherence', 'mortality', 'primary outcome', 'programs', 'randomized trial', 'response', 'safety net', 'secondary outcome', 'smoking cessation', 'text messaging intervention', 'theories', 'treatment as usual']",NHLBI,UNIVERSITY OF COLORADO DENVER,UH3,2021,1417404
"Enriching SARS-CoV-2 sequence data in public repositories with information extracted from full text articles Project Summary In response to the COVID-19 pandemic, scientists have published over one hundred thousand research articles and made available over eight hundred thousand virus genome sequences. These sequences, along with their metadata, can be used to understand virus evolution and spread and their implications for public health, a field of study called genomic epidemiology. However, these sequence records do not typically contain patient metadata such as demographics, clinical severity, or comorbidities, preventing researchers from uncovering trends in population health. To understand the severity of the problem, we analyzed nearly 748 thousand SARS-CoV-2 records from GISAID and 60 thousand from GenBank for the presence of patient metadata finding age and gender were represented in < 1% of GenBank records and in GISAID, 26% included sex, and 24% had age. For other fields, the amount of missing data is even more pronounced, with neither resource providing information on a patient's race and only GISAID specifying severity (i.e. ICU) in less than 5% of records. To address missing virus metadata, researchers could utilize the publication associated with the new sequences, however, the virus sequence record is often never updated with a link to the publication. From the set of records that we analyzed, 3.4% (of 748K) in GISAID and < 1% (of 117K) in GenBank had a link to a publication. This greatly hinders secondary data analysis of these sequences and limits the ability to use them at scale to uncover associations between the viral genome, transmission risk, and health outcomes. The goal of this proposal is to enhance genomic epidemiology and population health of COVID-19 with a framework to continuously and automatically enrich SARS-CoV-2 nucleic acid sequence metadata in public databases such as GenBank and GISAID with metadata in associated published articles. We will incorporate input from clinicians at the front-line of patient care during the pandemic and build on our NIH funded work (R01AI117011), which used Natural Language Processing (NLP) to enrich the geographic metadata of a sequence record using its corresponding published article. We have used these data in virus phylogeographic models and shown the benefit of using enriched metadata for modeling virus evolution and spread. Theavailability of SARS-CoV-2 sequences, paired withfull- text COVID-19 articles and preprints, presents an opportunity for metadata enrichment and scientific discovery beyond our prior work. Our specific aims are to: (1) enrich SARS-CoV-2 sequence metadata using text extracted from publications and (2) derive key epidemiologic insights for different patient demographics using our enriched SARS-CoV-2 sequence dataset. We will leverage our prior joint work funded by the NIH to enable the secondary use of enriched metadata for genomic epidemiology to improve our understanding of SARS-CoV-2 evolution and spread among different population groups. We will disseminate the enriched data through our GeoBoost2 data dashboard, GenBank LinkOut and the i2b2 platform. The latter will more immediately allow integration with COVID-specific clinical data shared by the 4CE Consortium. Project In Narrative response to the COVID-19 pandemic, scientists around the world have publishedover one hundred thousand research articles and made available over eight hundred thousand virus genome sequences, these comorbidities, journal corresponding however, many of sequence records l ack even the most basic patient metadata such as demographics, clinical severity, or preventing researchers f rom uncovering trends in population health. Researchers can utilize the article t hat describes the study, however, virus records are usually not updated with a link to the online publication, greatly hinderingthe ability to use them at scale for genomic epidemiology to uncover associations between the viral genome, transmission risk, and health outcomes. propose 2 to SARS-CoV-2 In this work, we to build on our prior work utilizing Natural Language Processing (NLP) t o further enrich SARS-CoV-  sequence metadata by analyzing text from preprint and peer-reviewed articles and use his enriched dataset derive key epidemiologic insights for different patient demographics to i mprove our understanding of evolution and spread among different population groups and we will t disseminate the enriched data through our GeoBoost2 data dashboard, GenBank LinkOut and the i2b2 platform.",Enriching SARS-CoV-2 sequence data in public repositories with information extracted from full text articles,10390667,R01AI164481,"['2019-nCoV', 'Address', 'Age', 'Agreement', 'Algorithms', 'Base Sequence', 'COVID-19', 'COVID-19 pandemic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communicable Diseases', 'Coronavirus', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Epidemiology', 'Evolution', 'Funding', 'Genbank', 'Gender', 'Genetic', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Gold', 'Health', 'International', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Link', 'Location', 'Manuals', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Ontology', 'Outcome', 'Patient Care', 'Patients', 'Peer Review', 'Performance', 'Phylogenetic Analysis', 'Population', 'Population Group', 'Populations at Risk', 'Probability', 'Public Health', 'Publications', 'Publishing', 'Race', 'Records', 'Relative Risks', 'Reporting', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'SARS coronavirus', 'Scientist', 'Sequence Analysis', 'Severities', 'Specific qualifier value', 'System', 'Testing', 'Text', 'Unified Medical Language System', 'United States National Institutes of Health', 'Update', 'Viral', 'Viral Genome', 'Virus', 'Work', 'clinical phenotype', 'cohort', 'comorbidity', 'coronavirus disease', 'dashboard', 'data sharing', 'deep learning', 'demographics', 'field study', 'genomic epidemiology', 'heuristics', 'improved', 'insight', 'novel', 'pandemic disease', 'population health', 'prevent', 'public repository', 'residence', 'response', 'secondary analysis', 'sex', 'text searching', 'transmission process', 'trend', 'virus characteristic']",NIAID,UNIVERSITY OF PENNSYLVANIA,R01,2021,757140
"Knowledge Management Center for Illuminating the Druggable Genome SUMMARY The understudied protein targets that are the focus of the implementation phase of the Illuminating the Druggable Genome (IDG) project need to be placed in the contexts of gene-sets/pathways, drugs/small-molecules, diseases/phenotypes, and cells/tissues. By extending our previous methods, we will impute knowledge about the understudied potential target protein kinases, GPCRs, and ion channels listed in the RFA using machine learning strategies. To establish this classification system, we will organize data from many omics- and literature- based resources into attribute tables where genes are the rows and their attributes are the columns. Examples of such attribute tables include gene or protein expression in cancer cell lines (CCLE) or human tissues (GTEx), changes in expression in response to drug perturbations or single-gene knockdowns (LINCS), regulation by transcription factors based on ChIP-seq data (ENCODE), and phenotypes in mice observed when single genes are knocked out (KOMP). In total, we will process and abstract data from over 100 resources. We will then predict target functions, target association with pathways, small-molecules/drugs that modulate the activity and expression of the target, and target relevance to human disease. To further validate such predictions, we will employ text mining to identify knowledge that corroborates with the data mining predictions, perform molecular docking of predicted small molecules using homology modeling, and seek associations between variants and human diseases by mining electronic medical records (EMR) together with genomic profiling of thousands of patients. In addition, we will develop innovative data visualization tools to allow users to interact with all the collected data, and develop social networking software to build communities centered around proteins/genes/targets as well as biological topics including pathways, cell types, drugs/small-molecules, and diseases. Overall, we will develop an invaluable resource that will accelerate target and drug discovery. NARRATIVE The Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) project will facilitate translational research by integrating and mining data about understudied druggable targets from numerous repositories and other resources. The KMC for IDG team will develop novel tools to analyze these data for the purpose of finding connections between genes/proteins/targets and diseases/phenotypes, cells/tissues, pathways/gene-sets, and drugs/small-molecules in order to identify potential applications to treat diseases and for other biological contexts of clinical relevance.",Knowledge Management Center for Illuminating the Druggable Genome,10057365,U24CA224260,"['Achievement', 'Award', 'Biological', 'Cancer cell line', 'Cells', 'ChIP-seq', 'Classification', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Disease', 'Docking', 'G-Protein-Coupled Receptors', 'Gene Expression', 'Gene Proteins', 'Gene Targeting', 'Generations', 'Genes', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Grant', 'Graph', 'Homology Modeling', 'Human', 'Information Resources Management', 'Internet', 'Ion Channel', 'Knock-out', 'Knockout Mice', 'Knowledge', 'Learning', 'Link', 'Literature', 'Machine Learning', 'Methods', 'Mining', 'Molecular', 'Mus', 'Online Systems', 'Paper', 'Pathway interactions', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Phosphotransferases', 'Process', 'Protein Kinase', 'Proteins', 'Public Domains', 'Publications', 'Publishing', 'Reproducibility', 'Research Personnel', 'Resources', 'Social Network', 'Supervision', 'System', 'Time trend', 'Tissues', 'Transcriptional Regulation', 'Translational Research', 'Update', 'Variant', 'Visit', 'Visualization software', 'Work', 'base', 'biobank', 'cell type', 'clinically relevant', 'community center', 'computerized data processing', 'data integration', 'data mining', 'data resource', 'data visualization', 'disease phenotype', 'drug discovery', 'druggable target', 'human disease', 'human tissue', 'in silico', 'innovation', 'knock-down', 'learning strategy', 'machine learning method', 'novel', 'online community', 'outreach', 'programs', 'protein expression', 'repository', 'response', 'side effect', 'small molecule', 'success', 'text searching', 'tool', 'transcription factor', 'unsupervised learning', 'web site']",NCI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U24,2021,249999
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations for the Division of the National Toxicology Program (DNTP). These research and development activities focus on tools to advance and adopt the use of Artificial Intelligence (AI) and Machine Learning (ML) techniques in the area of literature extraction. Projects focus on increasing efficiency in extracting and summarizing an increasing volume of digital information that is stored in unstructured text, tables, figures, and images. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and unique publication mining capability to enable automated evaluation of scientific publications. DNTP is working with experts in AL/ML at ORNL to develop accurate natural language processing models and image processing models that can augment or replace human effort. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,10460094,ES16002001,"['Adopted', 'Area', 'Artificial Intelligence', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Evaluation Research', 'Human', 'Image', 'Internet', 'Laboratories', 'Literature', 'Machine Learning', 'Methods', 'Mining', 'Modeling', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Text', 'Visual', 'base', 'digital', 'experience', 'image processing', 'machine learning method', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2021,600000

text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Fall Detection and Prevention for Memory Care through Real-Time Artificial Intelligence Applied to Video Abstract In the US, Alzheimer’s disease (AD) is the single most expensive disease, the only one in the top six for which the number of deaths is increasing. The greatest costs are hospitalizations, where falls are the largest culprit, and frequent need for assistance with daily life activities. A fall safety system shows the potential to reduce costs and increase quality of care by reducing the likelihood of emergency events (e.g., detecting falls before a fracture occurs, reducing the number of repeat falls). Unfortunately, no fall detection and prevention technology has been developed specifically for the needs of dementia care where individuals (1) fall more frequently and (2) often cannot tell care staff how they fell, leading to increased use of Emergency Medical Services (EMS) when falls are unwitnessed to ensure affected individuals are safe. Our goal is to perform a randomized wait-list control clinical trial (n=460) of SafelyYou Guardian, an online fall detection system with wall-mounted cameras to automatically detect falls for residents with AD and related dementias (ADRD). The automation is based on algorithms that push the frontier of deep learning, a subfield of Artificial Intelligence (AI), with a human-in-the- loop (HIL). SafelyYou Guardian is designed to primarily operate in memory care facilities (defined herein as assisted living and skilled nursing facilities providing ADRD care). Deep learning has already revolutionized several fields: robotics, self-driving cars, social networks in particular. Our approach is anchored in novel algorithms developed at the Berkeley AI Research Lab (BAIR) and extended by SafelyYou for real-time detection of rare events in video. The HIL is operating from a call center, confirms the fall detection alerts provided by our artificial intelligence algorithms, and places a call to the communities, so an intervention can happen within minutes of the fall detection. Subsequently, an Occupational Therapist (OT) working from our office in San Francisco reviews the fall videos with the front-line staff over video conference and using our web portal to make recommendations on how to re-organize the resident space (intervention) to prevent future falls. We leverage our HIL paradigm, in which our deep learning approach identifies and pre-filters falls with high sensitivity followed by a human who confirms the fall with high specificity and calls the communities in case of detected fall. This project leverages past small scale clinical and technical pilots including 87 residents from 11 partner communities, and our experience with paid commitments for 480 residents from three partner networks. Past pilots leading to this NIH Phase II proposal include:  · Pilot 1: Technical proof of concept with healthy subjects (200 acted falls).  · Pilot 2: We demonstrated acceptance of privacy/safety tradeoffs by residents, family and  staff, through the collection of 3 months of video data at WindChime of Marin, our first  partner facility; we identified 4 total hours of fall data. This led to clinical benefits  including an 80% fall reduction through the intervention of OT. · Pilot 3: We demonstrated scalability and acceptance by deploying the system in 11  communities, for 87 residents monitored by our system (offline, no HIL intervention). · Pilot 4: Small scale NIH Phase I clinical trial. We demonstrated the ability to perform real-time fall detection, with real-time intervention of the HIL through our partner company Magellan-Solutions which provides the 24/7 monitoring service for the facilities. We demonstrated that 93% of 89 falls were detected, that time on the ground was reduced by 42%, that the likelihood of EMS use was 50% lower with video available, and the that total facility falls including participants and non-participants decreased by 38%. The trial proposed for this NIH SBIR Phase II will provide clinical evidence that the preliminary trends observed experimentally (pilot 2) and at small scale (pilot 4) are true phenomena. It will use a wait-list control population (230 residents) to be compared to the population monitored with SafelyYou Guardian (230 residents). After crossover, the wait-list population will also benefit from the technology and be compared to itself before crossover. Narrative The goal of this project is to perform a randomized, wait-list controlled trial (n=460) of SafelyYou Guardian, an online fall detection and prevention system for memory care facilities (defined here as skilled nursing and assisted living facilities providing dementia care). The technology applies breakthroughs in artificial intelligence to video data collected from off- the-shelf, wall-mounted cameras to automatically detect falls from video for residents with Alzheimer’s disease and related dementias (ADRD); it enables care staff (1) to know about falls right away without requiring residents wear a device, (2) to use video review to quickly assess need for emergency medical services (EMS) after unwitnessed falls, and (3) to perform accurate incident review with support from a remote occupational therapist (OT) to assess how to reduce the risk of repeat falls. The Phase I project goal of launching this service at small scale was achieved and demonstrated with 11 memory care facilities (1) 93% of 89 falls detected, generating 1 alarm per camera per 15 days, (2) 37% reduction in EMS use through better understanding of risk when residents with dementia were found on the floor, and (3) 38% reduction in falls through reduced risk of repeat falls following OT video review.",Fall Detection and Prevention for Memory Care through Real-Time Artificial Intelligence Applied to Video,9776678,R44AG058354,"['Accidents', 'Address', 'Adult', 'Affect', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease related dementia', 'Artificial Intelligence', 'Assisted Living Facilities', 'Automation', 'Automobile Driving', 'Awareness', 'Beds', 'Caregivers', 'Caring', 'Cessation of life', 'Clinical', 'Clinical Trials', 'Cognitive', 'Collection', 'Communities', 'Control Groups', 'Data', 'Dementia', 'Detection', 'Devices', 'Discipline of Nursing', 'Disease', 'Emergency Situation', 'Emergency department visit', 'Emergency medical service', 'Ensure', 'Event', 'Family', 'Floor', 'Fracture', 'Future', 'Goals', 'Health Care Costs', 'Health care facility', 'Hospital Costs', 'Hospitalization', 'Hour', 'Human', 'Individual', 'Intervention', 'Lead', 'Letters', 'Life', 'Measures', 'Medical', 'Memory', 'Monitor', 'Morbidity - disease rate', 'Notification', 'Occupational Therapist', 'Outcome', 'Participant', 'Persons', 'Phase', 'Phase I Clinical Trials', 'Population', 'Population Control', 'Prevention', 'Privacy', 'Quality of Care', 'Quality of life', 'Randomized', 'Recommendation', 'Research', 'Risk', 'Risk Factors', 'Robotics', 'Safety', 'Sample Size', 'San Francisco', 'Series', 'Services', 'Skilled Nursing Facilities', 'Small Business Innovation Research Grant', 'Social Network', 'Specificity', 'Statistical Data Interpretation', 'Stream', 'System', 'Technology', 'Time', 'United States National Institutes of Health', 'Visit', 'Waiting Lists', 'base', 'care costs', 'cohort', 'cost', 'deep learning', 'dementia care', 'design', 'experience', 'falls', 'frontier', 'human-in-the-loop', 'improved', 'mortality', 'novel', 'phase 1 study', 'prevent', 'sensor', 'standard of care', 'symposium', 'trend', 'web portal']",NIA,"SAFELYYOU, INC.",R44,2019,493633,-0.008239318836446448
"Ethical Considerations for Language Modeling within Brain-Computer Interfaces Project Summary Machine learning (ML) and Natural Language Processing (NLP) have the potential to transform communication for patients with neurodegenerative disease through personalized and real-time augmentative and alternative communication (AAC) devices. Individuals with severe communication impairments who can no longer control their daily conversations or participate in previous life roles want AAC devices. And they want them to work – to be reliable, effective, and fast. ML and NLP are emerging as promising tools to bridge current technology and next generation devices for individuals with the most severe speech and physical impairments, like the RSVP Keyboard™, a brain-computer interface (BCI) being developed by the parent grant. BCI systems for communication are referred to as AAC-BCIs. NLP efforts to combine large public data sets with private data sets, such as personal email messages, promise to give individuals with communication impairments their own personalized language models, models that are sufficiently robust to get closer to real-time communication. The focus on getting AAC-BCIs to work with machine learning, however, has led to a critical oversight in the field: an inadequate understanding of why individuals want next-generation devices and what trade-offs they are willing to make for faster and more personalized communication. The turn to ML brings this oversight into sharp relief. Individuals should provide input about the data sets used to construct their personal language models, but this raises important ethical questions about what individuals value, how they understand their identity, and what trade-offs they are willing to make relative to their personalized communication data. The goal of this supplement is to fill this gap in understanding so that researchers can implement ML into next generation AAC-BCI systems in a way that is sensitive to the ethical concerns of future users. There are four components to this ethics supplement: (1) to design a toolbox of ethics vignettes tailored to ethical concerns raised by both BCI communication and ML; (2) to administer monthly vignette-based online ethics surveys to individuals with severe communication impairments due to motor neuron disease (e.g., ALS) (n=25) or movement disorders (e.g., Parkinson's disease) (n=25); (3) to conduct semi-structured vignette-based interviews with individuals with pre- clinical or mild communication impairment due to motor neuron disease (n=10) or movement disorder (n=10). Components (2) and (3) will employ an iterative, parallel mixed-method approach. Trends in Likert-style online responses to ethics vignettes in the severe communication impairment cohort will be used to inform and modify the semi-structured interview prompts asked of the pre-clinical or mild impairment cohort. In parallel, themes emerging from direct content analysis of interviews will be used to refine online survey questions. Results of this iterative, mix-methods approach will be used (4) to outline a framework of core ethical domains and preliminary tools (vignettes and discussion prompts) that AAC-BCI researchers can use to assess ethical concerns while developing and iteratively refining communication technology for personalized language models. Project Narrative The populations of US citizens with severe speech and physical impairments secondary to neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means that adapt to their best access methods in communication technologies. Bioethical issues about privacy, agency and identity must be included in technology development and implementation as the parent grant implements the translation of basic computer science and engineering into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Ethical Considerations for Language Modeling within Brain-Computer Interfaces,9929337,R01DC009834,"['Address', 'Administrative Supplement', 'Affect', 'Attention', 'Attitude', 'Augmentative and Alternative Communication', 'Award', 'Bioethical Issues', 'Bioethics', 'Clinical', 'Code', 'Cognitive', 'Communication', 'Communication impairment', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Development', 'Devices', 'Disease', 'Electroencephalography', 'Electronic Mail', 'Encapsulated', 'Engineering', 'Ensure', 'Ethical Analysis', 'Ethical Issues', 'Ethics', 'Foundations', 'Future', 'Goals', 'Home environment', 'Impairment', 'Individual', 'Informed Consent', 'Interview', 'Language', 'Letters', 'Life', 'Link', 'Literature', 'Locked-In Syndrome', 'Machine Learning', 'Medical', 'Medical Technology', 'Methods', 'Modeling', 'Monkeys', 'Motor Neuron Disease', 'Movement', 'Movement Disorders', 'Natural Language Processing', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Oregon', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Patient advocacy', 'Patients', 'Population', 'Privacy', 'Privatization', 'Public Health', 'Reporting', 'Research Personnel', 'Review Literature', 'Role', 'Secondary to', 'Self-Help Devices', 'Source', 'Speech', 'Structure', 'Surveys', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'User-Computer Interface', 'Voice', 'Work', 'advocacy organizations', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'cohort', 'communication device', 'computer science', 'design', 'expectation', 'informant', 'neurophysiology', 'next generation', 'novel', 'parent grant', 'pre-clinical', 'recruit', 'research and development', 'response', 'signal processing', 'skills', 'spelling', 'technology development', 'technology validation', 'tool', 'trend', 'uptake']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2019,153834,-0.038390760039257045
"CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision   To understand and navigate the environment, sensory systems must solve simultaneously two competing and challenging tasks: the segmentation of a sensory scene into individual objects and the grouping of elementary sensory features to build these objects. Understanding perceptual grouping and segmentation is therefore a major goal of sensory neuroscience, and it is central to advancing artificial perceptual systems that can help restore impaired vision. To make progress in understanding image segmentation and improving algorithms, this project combines two key components. First, a new experimental paradigm that allows for well-controlled measurements of perceptual segmentation of natural images. This addresses a major limitation of existing data that are either restricted to artificial stimuli, or, for natural images, rely on manual labeling and conflate perceptual, motor, and cognitive factors. Second, this project involves developing and testing a computational framework that accommodates bottom-up information about image statistics and top-down information about objects and behavioral goals. This is in contrast with the paradigmatic view of visual processing as a feedforward cascade of feature detectors, that has long dominated computer vision algorithms and our understanding of visual processing. The proposed approach builds instead on the influential theory that perception requires probabilistic inference to extract meaning from ambiguous sensory inputs. Segmentation is a prime example of inference on ambiguous inputs: the pixels of an image often cannot be labeled with certainty as grouped or segmented. This project will test the hypothesis that human visual segmentation is a process of hierarchical probabilistic inference. Specific Aim 1 will determine whether the measured variability of human segmentations reflects the uncertainty predicted by the model, as required for well-calibrated probabilistic inference. Specific Aim 2 addresses how feedforward and feedback processing in human segmentation contribute to efficient integration of visual features across different levels of complexity, from small contours to object parts. Specific Aim 3 will determine reciprocal interactions between perceptual segmentation and top-down influences including: semantic scene content; visual texture discrimination; and expectations reflecting environmental statistics. The proposed approach models these influences as Bayesian priors, and thus, if supported by the proposed experiments, will offer a unified framework to understand the integration of bottom-up and top- down influences in human segmentation of natural inputs. RELEVANCE (See instructions): This project aims to provide a unified understanding of perceptual segmentation and grouping of visual inputs encountered in the natural environment, through correct integration of the information contained in the visual inputs with top-down information about objects and behavioral goals. This understanding is central to advancing artificial perceptual systems that can help restore impaired vision in patient populations. n/a",CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision  ,9916219,R01EY031166,"['Address', 'Algorithms', 'Behavioral', 'Cognitive', 'Computer Vision Systems', 'Cues', 'Data', 'Data Set', 'Discrimination', 'Environment', 'Experimental Designs', 'Feedback', 'Goals', 'Grouping', 'Human', 'Image', 'Impairment', 'Individual', 'Influentials', 'Instruction', 'Label', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Measures', 'Mental disorders', 'Modeling', 'Motor', 'Neurodevelopmental Disorder', 'Neurons', 'Participant', 'Perception', 'Process', 'Protocols documentation', 'Recurrence', 'Semantics', 'Sensory', 'Statistical Models', 'Stimulus', 'System', 'Testing', 'Texture', 'Uncertainty', 'Vision', 'Visual', 'Visual Cortex', 'Visual impairment', 'Work', 'base', 'behavior influence', 'computer framework', 'deep learning', 'detector', 'expectation', 'experimental study', 'flexibility', 'imaging Segmentation', 'improved', 'object recognition', 'patient population', 'predictive modeling', 'sensory input', 'sensory integration', 'sensory neuroscience', 'sensory system', 'statistics', 'theories', 'vision science', 'visual processing']",NEI,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2019,195245,-0.012353076463514391
"Virtual prototyping for retinal prosthesis patients Project Summary/Abstract Retinal dystrophies such as retinitis pigmentosa and macular degeneration induce progressive loss of photoreceptors, resulting in profound visual impairment in more than ten million people worldwide. Visual neuroprostheses (‘bionic eyes’) aim to restore functional vision by electrically stimulating remaining cells in the retina, analogous to cochlear implants. A wide variety of neuroprostheses are either in development (e.g. optogenetics, cortical) or are being implanted in patients (e.g. subretinal or epiretinal electrical). A limiting factor that affects all device types are perceptual distortions and subsequent loss of information, caused by interactions between the implant technology and the underlying neurophysiology. Understanding the causes of these distortions and finding ways to alleviate them is critically important to the success of current and future sight restoration technologies. In this proposal, human visual psychophysics, computational modeling, data-driven approaches, and virtual reality (VR) will be combined to develop and experimentally validate optimized stimulation protocols for epiretinal prostheses. This approach is analogous to virtual prototyping for airplanes and other complex systems: to use a high-quality model of both the implant electronics and the visual system in order to generate a ‘virtual patient’. Retinal electrophysiological and visual behavioral data will be used to develop and validate a computational model of the expected visual experience of patients when electrically stimulated. One way of using this model will be to generate simulations of the expected perceptual outcome of electrical stimulation across a wide variety of electrical stimulation patterns. These will be used as a training set for machine learning algorithms that will invert the input-output function of the model to find the electrical stimulation protocol that best replicates any desired perceptual experience. The model can also be used to simulate the expected perceptual experience of real patients by using sighted subjects in a VR environment – ‘VR virtual patients’. These virtual patients will be used to discover preprocessing methods (e.g., edge enhancement, retargeting, decluttering) that improve behavioral performance in VR. Although current retinal prostheses have been implanted in over 250 patients worldwide, experimentation with improved stimulation protocols remains challenging and expensive. Implementing ‘virtual patients’ in VR offers an affordable and practical alternative for high-throughput experiments to test new stimulation protocols. Stimulation protocols that result in good VR performance will be experimentally validated in real prosthesis patients in collaboration with Second Sight Medical Products Inc. and Pixium Vision, two leading device manufacturers in the field. This work has the potential to significantly improve the effectiveness of visual neuroprostheses as a treatment option for individuals suffering from blinding retinal diseases. Project Narrative Inadequate stimulation paradigms are currently one of the main factors limiting the effectiveness of visual prostheses as a treatment option for individuals suffering from blinding retinal diseases. My goal is to develop and validate novel stimulation protocols for visual prosthesis patients that minimize perceptual distortions and thereby improve behavioral performance. Developing methods for generating better stimulation protocols through a combination of behavioral testing, virtual reality, computational modeling, and machine learning, has the potential to provide a transformative improvement of this device technology.",Virtual prototyping for retinal prosthesis patients,9756406,K99EY029329,"['Affect', 'Behavioral', 'Bionics', 'Cells', 'Clinical Trials', 'Cochlear Implants', 'Collaborations', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Devices', 'Effectiveness', 'Electric Stimulation', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Eye', 'Eye Movements', 'Family', 'Financial compensation', 'Future', 'Goals', 'Head', 'Human', 'Imagery', 'Implant', 'In Vitro', 'Individual', 'Knowledge', 'Learning', 'Letters', 'Machine Learning', 'Macular degeneration', 'Manufacturer Name', 'Medical', 'Medicare', 'Methods', 'Modeling', 'Motion', 'Neurons', 'Ocular Prosthesis', 'Online Systems', 'Outcome', 'Output', 'Patients', 'Pattern', 'Perceptual distortions', 'Performance', 'Photoreceptors', 'Prosthesis', 'Prosthesis Design', 'Protocols documentation', 'Psychophysics', 'Rehabilitation therapy', 'Reporting', 'Retina', 'Retinal', 'Retinal Diseases', 'Retinal Dystrophy', 'Retinitis Pigmentosa', 'Schedule', 'Severities', 'Shapes', 'Specialist', 'Stimulus', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Psychophysics', 'Visual impairment', 'Visual system structure', 'Work', 'base', 'behavior measurement', 'behavior test', 'deep neural network', 'design', 'experience', 'experimental study', 'gaze', 'implantation', 'improved', 'machine learning algorithm', 'neurophysiology', 'neuroprosthesis', 'novel', 'object recognition', 'optogenetics', 'predictive modeling', 'prototype', 'regression algorithm', 'restoration', 'retinal prosthesis', 'simulation', 'spatiotemporal', 'success', 'virtual', 'virtual reality']",NEI,UNIVERSITY OF WASHINGTON,K99,2019,122039,0.0026468985072768338
"User-driven Retrospectively Supervised Classification Updating (RESCU) systemfor robust upper limb prosthesis control ABSTRACT Approximately 41,000 individuals live with upper-limb loss (loss of at least one hand) in the US. Fortunately, prosthetic devices have advanced considerably in the past decades with the development of dexterous, anthropomorphic hands. However, potentially the most promising used control strategy, myoelectric control, lacks a correspondingly high-level of performance and hence the use of dexterous hands remains highly limited. The need for a complete overhaul in upper limb prosthesis control is well highlighted by the abandonment rates of myoelectric devices, which can reach up to 40% in the case of trans-humeral amputees. The area of research that has received the most focus over the past decade has been “pattern recognition,” which is a signal processing based control method that uses multi-channel surface electromyography as the control input. While pattern recognition provides intuitive operation of multiple prosthetic degrees of freedom, it lacks robustness and requires frequent, often daily calibration. Thus, it has not yet achieved the desired clinical acceptance. Our team proposes clinical translation of a novel highly adaptive upper limb prosthesis control system that incorporates two major advances: 1) machine learning (robust classification by implementing a non-boundary based algorithm), and 2) training by retrospectively incorporating user data from activities of daily living (ADL). The proposed system will enable machine intelligence with user input for prosthesis control. Our work is organized as follows: Phase I: (a) First, we will implement a fundamentally new machine intelligence technique, Extreme Learning Machine with Adaptive Sparse Representation Classification (EASRC), that is more resilient to untrained noisy conditions that users may encounter in the real-world and requires less data than traditional myoelectric signal processing. (b) In parallel, we will implement an adaptive learning algorithm, Nessa, which allows users to relabel misclassified data recorded during use and then update the EASRC classifier to adapt to any major extrinsic or intrinsic changes in the signals. Taken together, EASRC and Nessa comprise the Retrospectively Supervised Classification Updating (RESCU) system. Once, the RESCU implementation is complete, we will optimize the system through a joint effort with Johns Hopkins University, and complete an iterative benchtop RESCU evaluation with a focus group of 3 amputee subjects and their prosthetists. Our milestones for Phase I are as follows:  Milestone 1.1: Extreme Learning Machine with Adaptively Sparse Representation (EASRC) algorithm  successfully implemented and verified  Milestone 1.2: Implementation of Nessa adaptive learning algorithm and smartwatch interface  Milestone 1.3: User Needs and Design Inputs locked as a result of Focus Group testing  Milestone 1.4: Hold a pre-submission meeting with FDA for feedback on device classification and  planned product performance testing  Milestone 1.5: Hold a Scientific Steering Group (SSG) meeting  Milestone 1.6: Convene a Study Monitoring Committee (SMC) and hold an initial meeting to review  clinical plans.  Milestone 1.7: Develop Clinical Study Protocol  Milestone 1.8: Register the study on www.clinicaltrials.gov. PROJECT NARRATIVE In this project, we aim to empower the user by bringing them into the control loop of their prosthesis and improve the stability of their control strategy over time. Specifically, we implement to a robust classifier, an adaptive learning algorithm, and a smartwatch interface, which allows the user to teach their device when it misunderstands the commands that the user is sending to control the prosthesis. This will result in improved control without cumbersome or time-consuming effort on the part of the user and, more importantly, we hope that it will give the user a greater sense of empowerment and ownership over their prosthesis.",User-driven Retrospectively Supervised Classification Updating (RESCU) systemfor robust upper limb prosthesis control,9779227,U44NS108894,"['Activities of Daily Living', 'Algorithms', 'Amputees', 'Area', 'Artificial Intelligence', 'Calibration', 'Classification', 'Clinical', 'Clinical Research', 'Consumption', 'Data', 'Development', 'Devices', 'Electromyography', 'Evaluation', 'Feedback', 'Focus Groups', 'Freedom', 'Group Meetings', 'Hand', 'Individual', 'Intuition', 'Joints', 'Machine Learning', 'Methods', 'Monitor', 'Ownership', 'Pattern Recognition', 'Performance', 'Phase', 'Prosthesis', 'Protocols documentation', 'Research', 'Signal Transduction', 'Supervision', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Update', 'Upper Extremity', 'Work', 'adaptive learning', 'base', 'clinical translation', 'design', 'empowerment', 'improved', 'learning algorithm', 'meetings', 'myoelectric control', 'novel', 'operation', 'performance tests', 'prosthesis control', 'signal processing', 'smart watch']",NINDS,"INFINITE BIOMEDICAL TECHNOLOGIES, LLC",U44,2019,79250,0.00288346229730316
"User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control ABSTRACT Approximately 41,000 individuals live with upper-limb loss (loss of at least one hand) in the US. Fortunately, prosthetic devices have advanced considerably in the past decades with the development of dexterous, anthropomorphic hands. However, potentially the most promising used control strategy, myoelectric control, lacks a correspondingly high-level of performance and hence the use of dexterous hands remains highly limited. The need for a complete overhaul in upper limb prosthesis control is well highlighted by the abandonment rates of myoelectric devices, which can reach up to 40% in the case of trans-humeral amputees. The area of research that has received the most focus over the past decade has been “pattern recognition,” which is a signal processing based control method that uses multi-channel surface electromyography as the control input. While pattern recognition provides intuitive operation of multiple prosthetic degrees of freedom, it lacks robustness and requires frequent, often daily calibration. Thus, it has not yet achieved the desired clinical acceptance. Our team proposes clinical translation of a novel highly adaptive upper limb prosthesis control system that incorporates two major advances: 1) machine learning (robust classification by implementing a non-boundary based algorithm), and 2) training by retrospectively incorporating user data from activities of daily living (ADL). The proposed system will enable machine intelligence with user input for prosthesis control. Our work is organized as follows: Phase I: (a) First, we will implement a fundamentally new machine intelligence technique, Extreme Learning Machine with Adaptive Sparse Representation Classification (EASRC), that is more resilient to untrained noisy conditions that users may encounter in the real-world and requires less data than traditional myoelectric signal processing. (b) In parallel, we will implement an adaptive learning algorithm, Nessa, which allows users to relabel misclassified data recorded during use and then update the EASRC classifier to adapt to any major extrinsic or intrinsic changes in the signals. Taken together, EASRC and Nessa comprise the Retrospectively Supervised Classification Updating (RESCU) system. Once, the RESCU implementation is complete, we will optimize the system through a joint effort with Johns Hopkins University, and complete an iterative benchtop RESCU evaluation with a focus group of 3 amputee subjects and their prosthetists. Phase II: Verification and validation of RESCU will be completed, culminating in third-party validation testing and certification. Finally, we will complete a clinical assessment including self-reporting subjective measures, and real-world usage metrics in a long-term clinical study. PROJECT NARRATIVE In this project, we aim to empower the user by bringing them into the control loop of their prosthesis and improve the stability of their control strategy over time. Specifically, we implement to a robust classifier, an adaptive learning algorithm, and a smartwatch interface, which allows the user to teach their device when it misunderstands the commands that the user is sending to control the prosthesis. This will result in improved control without cumbersome or time-consuming effort on the part of the user and, more importantly, we hope that it will give the user a greater sense of empowerment and ownership over their prosthesis.",User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control,10013405,U44NS108894,"['Activities of Daily Living', 'Adoption', 'Algorithms', 'Amputees', 'Area', 'Artificial Intelligence', 'Calibration', 'Certification', 'Classification', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Communication', 'Consumption', 'Data', 'Development', 'Devices', 'Electromyography', 'Evaluation', 'Focus Groups', 'Freedom', 'Goals', 'Hand', 'Individual', 'Intuition', 'Joints', 'Label', 'Limb Prosthesis', 'Machine Learning', 'Measures', 'Methods', 'Outcome', 'Ownership', 'Patient Self-Report', 'Pattern Recognition', 'Performance', 'Phase', 'Prosthesis', 'Research', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Supervision', 'Surface', 'Surveys', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Update', 'Upper Extremity', 'Validation', 'Work', 'adaptive learning', 'base', 'clinical translation', 'empowerment', 'functional improvement', 'improved', 'innovation', 'learning algorithm', 'myoelectric control', 'novel', 'operation', 'programs', 'prospective', 'prosthesis control', 'satisfaction', 'signal processing', 'smart watch', 'verification and validation']",NINDS,"INFINITE BIOMEDICAL TECHNOLOGIES, LLC",U44,2019,735600,0.0008360341991336726
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user's location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user's location by recognizing standard informational signs present in the environment, tracking the user's trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9934891,R01EY029033,"['Adoption', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Environment', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Medical center', 'Process', 'Research', 'Schools', 'System', 'Tactile', 'Time', 'Travel', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'interest', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2019,105337,0.06385403908050247
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9663319,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2019,416374,0.06415479102337215
"Improved AD/ADRD Assessment Sensitivities Using a Novel In-Situ Sensor System Project Summary/Abstract  Accurate assessment of daily functions for individuals at risk for and with AD/ADRD, is fundamental to detection, diagnosis, and characterization of its progression and prescribed treatments. Current assessment techniques typically rely on non- continuous, discreet observations provided from a third party and covering single or limited performance domains. With significantly larger portions of American’s choosing to age in place, any assessment technology must be able to be in-situ (low-cost, ubiquitous) and operate without user interface (autonomous) to provide objective, cross-domain, and continuous daily function measurements and reporting.  The primary objective of this fast track SBIR project is to demonstrate the feasibility and effectiveness of using the Birkeland Current Sovrin IoT system to continuously and accurately assess daily functions, ADLs, and IADLs, for persons experiencing cognitive decline in a home or assisted care settings. This includes direct comparison with an accepted assessment technique, ADCS-ADL/23. Machine learning and artificial intelligent techniques will be employed to identify novel subfactors for improved sensitivities from available sensor data combinations. Secondary objectives include establishing a significant data set of detailed daily actions (<10 sec resolution) for 100+ individuals with AD/ADRD. Long-term goals support future intervention studies through improved assessment tools with enhanced sensitivity to early and mid-stage decline.  The Birkeland Current Sovrin IoT system makes use of patented proximity-based energy monitoring and control sensors, data analytics and change detection algorithms to continuously monitor activities of individuals in a home or assisted care environment. Intelligent power-strips and battery-based sensors located throughout the home or facility, monitor real time absolute location of individuals, caregivers, and devices they interact with. Correlation of high-fidelity data allows accurate determination of activities, attribution to a specific individual, mobility measurement, and behavior assessment across traditional and novel ADL/IADL categories. Birkeland Current is teamed with Texas A&M Center for Population Health and Aging, Georgia, Tech Institute for People and Technology, Baylor Scott and White Division of Gerontology, and multiple home-care and assisted-care facilities, in the development of the study approach, implementation plan, analytics tools, and applications to aging populations and future intervention studies. Project Narrative  The proposed research would utilize novel, ubiquitous Internet-of-Things sensors and automated analytics to demonstrate enhanced sensitivity and future utility of continuous in-situ IADL/ADL data for dementia research and its effectiveness in characterizing interventions for Alzheimer’s and related dementias of aging populations in support of NIA stated priorities.",Improved AD/ADRD Assessment Sensitivities Using a Novel In-Situ Sensor System,9846881,R44AG065118,"['Address', 'Adoption', 'Aging', 'Algorithms', 'Alzheimer&apos', 's disease related dementia', 'American', 'Artificial Intelligence', 'Assessment tool', 'Behavior assessment', 'Behavioral Symptoms', 'Caregivers', 'Caring', 'Categories', 'Centers for Population Health', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Dementia', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Documentation', 'Early Diagnosis', 'Early identification', 'Effectiveness', 'Environment', 'Future', 'Gerontology', 'Goals', 'Grouping', 'Health care facility', 'Home environment', 'Impaired cognition', 'In Situ', 'Individual', 'Industry', 'Institutes', 'Intelligence', 'Internet of Things', 'Intervention', 'Intervention Studies', 'Legal patent', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Monitor', 'Outcome', 'Participant', 'Patients', 'Performance', 'Persons', 'Phase', 'Population', 'Problem Solving', 'Protocols documentation', 'Publishing', 'Recommendation', 'Reporting', 'Research', 'Resolution', 'Resources', 'Risk', 'Series', 'Small Business Innovation Research Grant', 'System', 'Techniques', 'Technology', 'Technology Assessment', 'Testing', 'Texas', 'Time', 'Training', 'United States National Institutes of Health', 'Use Effectiveness', 'aging in place', 'aging population', 'analytical tool', 'base', 'cost', 'daily functioning', 'data acquisition', 'data integration', 'database structure', 'design', 'experience', 'improved', 'insight', 'instrumental activity of daily living', 'learning algorithm', 'novel', 'off-patent', 'patient home care', 'personalized care', 'real time monitoring', 'sensor', 'symposium', 'tool']",NIA,BIRKELAND CURRENT LLC,R44,2019,350000,-0.015483852213078375
"Detecting Middle Ear Fluid Using Smartphones PROJECT SUMMARY Otitis media is one of the most common childhood diseases in developing countries; many of its complications are preventable if middle ear fluid is detected early. We propose an accessible and accurate smartphone-based screening tool that (i) sends a soft acoustic chirp into the ear canal using the smartphone speaker, (ii) detects reflected sound from the eardrum using the smartphone microphone, and (iii) employs a machine learning model to classify these reflections and predict middle ear fluid status in realtime. Given the ubiquity of smartphones and the inaccuracy of visual otoscopy, the system we propose has the potential to be the default screening tool used in developing countries by healthcare providers and caregivers at home. PROJECT NARRATIVE Otitis media is one of the most common childhood diseases in developing countries affecting over 1.23 billion people in 2013 and can lead to complications such as hearing loss, developmental delay, meningitis, mastoiditis, and death. Many of these complications are preventable if middle ear fluid is detected early. However, the absence of an accurate and accessible method to detect middle ear fluid has led to high misdiagnosis rates. The consequence is associated hearing and speech impairment rates greater than any other pediatric condition and growing microbial resistance as a result of antibiotic over-prescription. Currently, the technique of choice for detecting middle ear fluid by primary care providers is visual otoscopy, which has a diagnostic accuracy as low as 51%. Although more accurate methods like tympanometry and pneumatic otoscopy exist, they require significant expertise and referral to a specialist. Commercial acoustic reflectometers and smartphone-mounted otoscopes require specialized hardware. Thus, there is an urgent, unmet need for an accurate, rapid and easily accessible method for resource-limited healthcare providers and caregivers to detect middle ear fluid. This project aims to demonstrate the feasibility of using the speakers and microphones on existing smartphones to detect middle ear fluid by assessing eardrum mobility. Our proposed system would operate by (i) sending a soft acoustic chirp into the ear canal using the smartphone speaker, (ii) detecting reflected sound from the eardrum using the smartphone microphone, and (iii) employing a machine learning model to classify these reflections and predict middle ear fluid status. No additional attachments would be required beyond a paper funnel, which acts as a speculum to reduce waveform variability and can be constructed with printer paper, scissors, and tape. This technique is the first software-based screening tool for middle ear fluid detection that uses off-the-shelf smartphones which does not require hardware attachments or visual interpretation. Using data from our existing preliminary clinical study we aim to develop signal processing and machine learning algorithms to optimize sensitivity and specificity. We plan to develop a bench testing technique that enables previously unsupported smartphones to to run our test and prospectively validate our optimized algorithm clinically in parallel testing with an acoustic reflectometer. Further we aim to develop a user interface and improved funnel design. These new designs will undergo usability testing in physician and parent populations. Given the ubiquity of smartphones, our app has the potential to be the default screening tool used in developing countries by healthcare providers and caregivers at home.",Detecting Middle Ear Fluid Using Smartphones,9906782,R43DC018434,"['Acoustics', 'Affect', 'Agreement', 'Algorithms', 'Antibiotics', 'Caregivers', 'Cellular Phone', 'Cessation of life', 'Childhood', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Computer software', 'Data', 'Detection', 'Developing Countries', 'Development', 'Developmental Delay Disorders', 'Diagnosis', 'Disease', 'Ear', 'Earwax', 'Environment', 'External auditory canal', 'FDA approved', 'Feedback', 'Future', 'Galaxy', 'Health', 'Health Personnel', 'Hearing', 'Home environment', 'Impairment', 'Industry Standard', 'Lead', 'Liquid substance', 'Machine Learning', 'Mastoiditis', 'Measures', 'Meningitis', 'Methods', 'Modeling', 'Obstruction', 'Otitis Media', 'Otoscopes', 'Otoscopy', 'Outcome', 'Output', 'Paper', 'Parents', 'Patients', 'Performance', 'Peripheral', 'Phase', 'Physicians', 'Population', 'Preparation', 'Publishing', 'Research Personnel', 'Resistance', 'Resources', 'Running', 'Screening procedure', 'Sensitivity and Specificity', 'Small Business Innovation Research Grant', 'Specialist', 'Specificity', 'Speculums', 'Speech', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Tympanic membrane', 'Tympanometry', 'Visual', 'base', 'care providers', 'clinical care', 'clinical practice', 'design', 'diagnostic accuracy', 'experience', 'hearing impairment', 'improved', 'machine learning algorithm', 'meetings', 'microbial', 'microphone', 'middle ear', 'prospective', 'screening', 'signal processing', 'sound', 'telehealth', 'tool', 'urgent care', 'usability']",NIDCD,"WAVELY DIAGNOSTICS, INC.",R43,2019,157842,-0.0015461729773714621
"Evaluating and Improving Assistive Robotic Devices Continuously and in Real-time Project Summary Lower limb assistive robotic devices, such as active prosthesis, orthoses, and exoskeletons have the potential to restore function for the millions of Americans who experience mobility challenges due to injury and disability. Since individuals with mobility challenges have an increased energetic cost of transport, the benefit of such assistive devices is commonly assessed via the reduction in the metabolic work rate of the individual who is using the device. Currently, metabolic work rate can only be obtained in a laboratory environment, using breath-by-breath measurements of respiratory gas analysis. To obtain a single steady state data point of metabolic work rate, multiple minutes of data must be collected, since the signals are noisy, sparsely sampled, and dynamically delayed. In addition, the user has to wear a mask and bulky equipment, further restricting the applicability of the method on a larger scale. We propose an improved way to obtain such estimates of metabolic work rate in real-time. Aim 1 will determine salient signal features and characterize the dynamics of sensing metabolic work rate from a variety of physiological sensor signals. Aim 2 will use advanced sensor fusion and machine learning techniques to accurately predict instantaneous energy cost in real-time from multiple physiological signals without relying on a metabolic mask. Aim 3 will use the obtained real-time estimates to optimize push-off timing for an active robotic prosthesis. The resulting methods will enable an automated and continuous evaluation of assistive robotic devices that can be realized outside the laboratory and with simple wearable sensors. This automated evaluation will enable devices, such as active prostheses, orthoses, or exoskeletons, that can self-monitor their performance, optimize their own behavior, and continuously adapt to changing circumstances. This will open up a radically new way of human-robot- interaction for assistive devices. It will greatly increase their clinical viability and enable novel advanced controllers and algorithms that can improve device performance on a subject specific basis. Project Narrative A common way of evaluating assistive robotic devices, such as active prostheses or exoskeletons, is by measuring the reduction in effort that they bring to an individual walking in them. The proposed project will develop ways to perform this evaluation automatically and in real-time by the device itself, which will be used in the future to develop prostheses and exoskeletons that automatically adapt themselves to their users. This project supports the NIH's stated mission of reducing disability by improving patient outcomes with new prosthetic and orthotic devices.",Evaluating and Improving Assistive Robotic Devices Continuously and in Real-time,9668174,R03HD092639,"['Algorithms', 'American', 'Amputation', 'Amputees', 'Ankle', 'Behavior', 'Clinical', 'Data', 'Devices', 'Electromyography', 'Energy Metabolism', 'Environment', 'Equipment', 'Evaluation', 'Future', 'Goals', 'Heart Rate', 'Indirect Calorimetry', 'Individual', 'Injury', 'Laboratories', 'Linear Regressions', 'Lower Extremity', 'Machine Learning', 'Masks', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Mission', 'Modality', 'Modeling', 'Monitor', 'Noise', 'Orthotic Devices', 'Patient-Focused Outcomes', 'Performance', 'Persons', 'Physiological', 'Population', 'Prosthesis', 'Reference Values', 'Robotics', 'Sampling', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Techniques', 'Time', 'Training', 'United States National Institutes of Health', 'Walking', 'Work', 'base', 'cost', 'disability', 'exoskeleton', 'experience', 'functional restoration', 'human-robot interaction', 'improved', 'light weight', 'neural network', 'novel', 'respiratory gas', 'robotic device', 'sensor', 'wearable device']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R03,2019,78000,0.0322710837369709
"BlueBox: A Complete Code Blue Data Recorder, Phase II “Code blue” is the signal used in hospitals to call for an immediate cardiopulmonary resuscitation (CPR) following a cardiac or respiratory arrest. Reviewing the performance of the “code blue team” is a cornerstone for improving outcomes. The current standard of using handwritten records on a paper “code sheet” does not allow measurement of key quality indicators and is subject to human error. In the Phase I STTR project, we developed an electronic device for complete recording of code blue events, called BlueBox. The BlueBox is a small electronic recorder on an adhesive patch to be placed on the left chest next to the mid-sternum. The prototype we developed in Phase I was successfully tested on high fidelity mannequins and on pigs. In Phase II, our goal is to complete the product development and testing and prepare the BlueBox for regulatory clearance and market launch. To achieve this goal, we propose 3 Specific Aims. Aim 1 is to complete the product development of the BlueBox device and the software user interface (UI) for the “electronic code sheet.” We will turn the engineering prototype we developed in Phase I into a product ready for commercialization through rigorous product development processes. We will develop a mobile app for iPads with a software UI for the “electronic code sheet.” Aim 2 is to conduct human factors and usability engineering (HF/UE) testing and prepare for regulatory submission. The alpha prototype will undergo HF/UE testing in the Simulation Center. We will establish and maintain quality management records and conduct a pilot production run of 200 units of BlueBox. Aim 3 is to validate the BlueBox system in a clinical study of code blues in the hospital. We will first conduct a pilot study of 5 code blue patients in the CCU and Cath Lab. Once the pilot study is successful, we plan to conduct a full clinical study of 100 patients recruited from the Harbor-UCLA ICU/CCU and emergency departments. The objectives of the clinical study are: 1) to establish equivalence of the electronic code sheet to the current standard of paper code sheet; 2) to demonstrate the effectiveness of the electronic code sheet in identifying key CPR quality indicators. The criteria for successful development of the product will be that it passes all required regulatory testing and is validated in the clinical study for its equivalence and effectiveness in code blue recording. There will be two major milestones in this project: (1) finalizing product development with successful test production of 200 units; and (2) completing the clinical study and preparing for a 510(k) submission. Achieving the aims will result in a validated BlueBox system ready for submission to the FDA and commercialization. We intend to first introduce the BlueBox system to hospitals as a tool for staff training and quality improvement. We will continue the technology development with machine learning to provide instant feedback in the second generation BlueBox. Our ultimate goal is to minimize human error and improve patient outcomes through the BlueBox system's better documentation and continuous feedback mechanism. PROJECT NARRATIVE Debriefings and detailed reviews of the performance of the “code blue team” in cardiopulmonary resuscitation (CPR) can improve quality of care and patient outcomes. In Phase I, we developed and successfully tested an electronic device, the BlueBox, for recording all CPR events and enabling full displays of code blue resuscitations in an “electronic code sheet.” We will turn the engineering prototype into a product ready for regulatory submission and commercialization in the proposed Phase II project.","BlueBox: A Complete Code Blue Data Recorder, Phase II",9680137,R42GM113463,"['Accident and Emergency department', 'Adhesives', 'American Heart Association', 'Animals', 'Cardiac', 'Cardiopulmonary Resuscitation', 'Chest', 'Clinical', 'Clinical Research', 'Code', 'Code Blue', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Documentation', 'Effectiveness', 'Electric Countershock', 'Electronics', 'Emergency Situation', 'Engineering', 'Event', 'Family suidae', 'Feedback', 'Generations', 'Goals', 'Guidelines', 'Hospital Administrators', 'Hospitals', 'Human', 'Industrialization', 'Left', 'Machine Learning', 'Manikins', 'Measurement', 'Mechanics', 'Medical', 'Medical Errors', 'Miniaturization', 'Modeling', 'Outcome', 'Paper', 'Patient Recruitments', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Pilot Projects', 'Preparation', 'Procedures', 'Process', 'Production', 'Quality Indicator', 'Quality of Care', 'Records', 'Resuscitation', 'Running', 'Signal Transduction', 'Small Business Technology Transfer Research', 'Specialist', 'Specific qualifier value', 'Sternum', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Validity and Reliability', 'base', 'care outcomes', 'commercialization', 'design', 'heart rhythm', 'human error', 'improved', 'improved outcome', 'machine learning algorithm', 'meetings', 'member', 'mobile application', 'patient safety', 'product development', 'prototype', 'respiratory', 'sensor', 'simulation', 'technology development', 'tool', 'usability', 'validation studies']",NIGMS,"NEOVATIVE, INC.",R42,2019,723986,-0.0017551362348056665
"Just-In-Time Fall Prevention: Development of an mHealth Intervention for Persons with Multiple Sclerosis One out of every two of the 2.3 million persons with Multiple Sclerosis (PwMS) report a fall in any given three- month period. The onset of MS is often during early or middle adulthood, making MS-induced falls a significant, long-term problem in need of new preventative interventions. Here we propose to advance several components of Just-In-Time Fall Prevention – a novel mHealth (mobile health) approach for preventing falls and to demonstrate each component in a sample of PwMS. The proposed mHealth system will be composed of wireless, wearable sensors and a mobile phone application. The wearable sensors will capture patient biomechanics and a network of statistical models, created using machine learning and deployed on the mobile phone, will predict fall risk based upon these measurements. Fall risk predictions will inform personalized interventions, delivered through the mobile application, that leverage strategies from social psychology designed to induce biomechanical and behavioral changes to immediately reduce fall risk. This approach enables real- time assessment and intervention to prevent future falls. Data to develop and pilot each component of this mHealth system will be collected in a study of N=50 PwMS that will a) capture concurrent measurements from wearable sensors, optical motion capture, force platforms, and an instrumented treadmill during functional assessments and simulated daily activities, b) track falls and objective biomechanical and behavioral measures during a 3-month in-home study, and c) assess the efficacy of point-of-choice prompts for altering fall-related biomechanics during balance-challenging daily activities. These data will be used to accomplish the following specific aims: 1) Validate wearable sensor algorithms for capturing biomechanics and behavior of PwMS, 2) Identify digital biomarkers for quantifying fall risk in real time during daily life in PwMS, and 3) Pilot point-of- choice prompts for inducing biomechanical changes in PwMS under realistic cognitive load. These aims are the first step toward our goal of developing a paradigm-shifting mHealth system for fall prevention and lead naturally to future R01 support for a randomized, controlled clinical trial testing the efficacy of this approach. This project will advance several components of a novel mobile health intervention for preventing falls. If successful, this approach has the potential to shift fall preventions efforts from reactive and physician centered to preventative and patient centered improving long-term health outcomes and quality of life for those with balance and mobility impairment.",Just-In-Time Fall Prevention: Development of an mHealth Intervention for Persons with Multiple Sclerosis,9724645,R21EB027852,"['Acute', 'Adult', 'Affect', 'Algorithms', 'Behavior', 'Behavioral', 'Biological Markers', 'Biomechanics', 'Car Phone', 'Cognitive', 'Data', 'Development', 'Equilibrium', 'Fall prevention', 'Future', 'Goals', 'Gold', 'Health', 'Health Personnel', 'Healthcare Systems', 'Heart', 'Home environment', 'Impairment', 'Injury', 'Intervention', 'Lead', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical Care Costs', 'Modality', 'Motion', 'Multiple Sclerosis', 'Optics', 'Outcome', 'Patient Self-Report', 'Patients', 'Persons', 'Physicians', 'Preventive Intervention', 'Quality of life', 'Randomized Controlled Clinical Trials', 'Reporting', 'Research Design', 'Sampling', 'Science', 'Social Psychology', 'Statistical Models', 'Study Subject', 'Symptoms', 'System', 'Techniques', 'Time', 'Wireless Technology', 'Work', 'base', 'behavior measurement', 'cognitive load', 'design', 'digital', 'efficacy testing', 'fall risk', 'falls', 'improved', 'instrument', 'mHealth', 'mobile application', 'nervous system disorder', 'novel', 'patient oriented', 'personalized intervention', 'prevent', 'prospective', 'standard measure', 'treadmill', 'wearable device']",NIBIB,UNIVERSITY OF VERMONT & ST AGRIC COLLEGE,R21,2019,190538,-0.0039126834176078
"Inferential methods for functional data from wearable devices Project Summary/Abstract This is a project to develop new statistical methods for comparing groups of subjects in terms of health outcomes that are assessed using data from wearable devices. Inexpensive wearable sensors for health monitoring are now capable of generating massive amounts of data collected longitudinally, up to months at a time. The project will develop inferential methods that can deal with the complexity of such data. A serious challenge is the presence of unmeasured time-dependent confounders (e.g., circadian and dietary patterns), making direct comparisons or borrowing strength across subjects untenable unless the studies are carried out in controlled experimental con- ditions. Generic data mining and machine learning tools have been widely used to provide predictions of health status from such data. However, such tools cannot be used for signiﬁcance testing of covariate effects, which is necessary for designing precision medicine interventions, for example, without taking the inherent model selection or the presence of the unmeasured confounders into account. To overcome these difﬁculties, a systematic de- velopment of inferential methods for functional outcome data obtained from wearable devices will be carried out. There are three speciﬁc aims: 1) Develop metrics for functional outcome data from wearable devices, 2) Develop nonparametric estimation and testing methods for activity proﬁles and a screening method for predictors of activity proﬁles, 3) Implement the methods in an R package and carry out two case studies using accelerometer data. For Aim 1, the approach is to reduce the sensor data to occupation time proﬁles (e.g., as a function of activity level), and formulate the statistical modeling in terms of these proﬁles using survival and functional data analytic meth- ods. This will have a number of advantages, the principal one being that time-dependent confounders become less problematic because the effect of differences in temporal alignment across subjects is mitigated. In addition, survival analysis methods can be applied by viewing the occupation time as a time-to-event outcome indexed by activity level. For Aim 2, nonparametric methods will be used to compare and order occupation time distributions between groups of subjects that are speciﬁed in terms of baseline covariate levels or treatment groups. Further, a new method of post-selection inference based on marginal screening for function-on-scalar regression will be developed to identify and formally test whether covariates are signiﬁcantly associated with activity proﬁles. Aim 3 will develop an R-package implementation, and as a test-bed for the proposed methods they will be applied to two Columbia-based clinical studies: to the study of physical activity in children enrolled in New York City Head Start, and to the study of experimental drugs for the treatment of mitochondrial depletion syndrome. Project Narrative The relevance of the project to public health is that it will develop statistical methods for the physiological eval- uation of patients on the basis of data collected by inexpensive wearable sensors (e.g., accelerometers). By introducing methods for the rigorous comparison of healthcare status among groups of patients observed longi- tudinally over time using such devices, treatment decisions that can beneﬁt targeted populations of patients in terms of continuously-assessed health outcomes will become possible.",Inferential methods for functional data from wearable devices,9658873,R01AG062401,"['Acceleration', 'Accelerometer', 'Beds', 'Bypass', 'Case Study', 'Characteristics', 'Child', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Dietary Practices', 'Drug Combinations', 'Enrollment', 'Evaluation', 'Event', 'Grant', 'Head Start Program', 'Health', 'Health Status', 'Healthcare', 'Intervention', 'Lead', 'Machine Learning', 'Measures', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Monitor', 'Motivation', 'Nature', 'New York City', 'Obesity', 'Occupations', 'Outcome', 'Outcome Measure', 'Patients', 'Pharmacotherapy', 'Physical activity', 'Physiological', 'Preschool Child', 'Process', 'Proxy', 'Public Health', 'Recording of previous events', 'Regimen', 'Signal Transduction', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Stochastic Processes', 'Survival Analysis', 'Syndrome', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Work', 'analytical method', 'base', 'circadian', 'data mining', 'design', 'experimental study', 'functional outcomes', 'indexing', 'interest', 'lower income families', 'novel', 'patient population', 'precision medicine', 'screening', 'sensor', 'theories', 'time use', 'tool', 'treatment group', 'wearable device']",NIA,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,317858,-0.034260751299422416
"Device to control circadian-effective light in Alzheimer's disease environments Project Summary This proposed project will develop and field-test a device that accurately monitors and controls the circadian stimulus (CS) for Alzheimer disease (AD) and Alzheimer-disease-related dementia (ADRD) patients in nursing homes. Human biology has evolved to have two distinct optical systems: the visual system, by which we see and process images, and the circadian system, which regulates our biological clock and associated biological systems. These two systems have significantly different spectral and temporal responses to optical input. Specifically, circadian stimulation peaks at 460 nm and responds after several minutes of optical activation, while the visual system peaks at 555 nm and responds nearly instantaneously to inputs. All lighting systems are designed and installed in buildings with consideration only given to the photopic (visual) system and all light meters used to characterize lighting buildings are calibrated to measure photopic light, not CS. While a broad and growing body of research has documented the impacts of the circadian system on human health, including regulating sleep and improving cognition in AD/ADRD patients, research on the CS experienced by AD/ADRD patients is extremely limited. Researchers at the Lighting Research Center at Rensselaer Polytechnic Institute developed the Daysimeter, a calibrated light meter that measures circadian light and circadian stimulus. In Phase I of this project, researchers modified an existing workstation-based lighting control system they previously developed for the visual system to include Daysimeter technology, allowing this control system to record CS measurements. The accuracy of these CS measurements was confirmed in the laboratory and field-testing of 20 of devices is currently ongoing in AD/ADRD nursing homes. In this Phase II application, researchers propose adding control features to this device so that lighting can be controlled to optimize CS dosages in AD/ADRD patient environments. Machine learning-based lighting control algorithms will be driven by continuous light level and spectrum measurements as well as periodic (e.g., daily) patient health data. Data from these devices would be wirelessly transmitted to researchers via an Internet gateway and associated cloud-based data management systems. These data would be of immediate value for gaining a better understanding of AD/ADRD patients' CS exposure and could ultimately result in new lighting systems and/or building codes that consider both our visual and circadian systems. Following the development phase, 30 CS-enabled lighting control systems will be field tested over a 22-week test period. Researchers aim to commercialize this CS-enabled lighting control system shortly after the completion of this field test and the Phase II project specifically targeting AD/ADRD nursing home applications. Project Narrative A growing body of research has demonstrated how light impacts human circadian systems and how these impacts can affect sleep, alertness, cognition and agitation in people with Alzheimer's disease (AD) and Alzheimer's-disease-related dementia (ADRD). Still, significant knowledge gaps exist in determining how much circadian stimulation is typically provided to AD/ADRD patients and there are no commercial products designed to control lighting in AD/ADRD environments in ways that promote circadian-related health. This project aims to fill in these gaps by developing and testing a device specifically designed to measure and control the circadian stimulation experienced by AD/ADRD patients in nursing homes.",Device to control circadian-effective light in Alzheimer's disease environments,9907480,R44AG060857,"['Affect', 'Agitation', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease related dementia', 'Back', 'Behavior', 'Biological Clocks', 'Building Codes', 'Characteristics', 'Clinical Trials', 'Cognition', 'Data', 'Database Management Systems', 'Development', 'Device or Instrument Development', 'Devices', 'Dose', 'Effectiveness', 'Elderly', 'Environment', 'Feeds', 'Health', 'Hour', 'Human', 'Human Biology', 'Image', 'Institutes', 'Internet', 'Intervention', 'Knowledge', 'Laboratories', 'Light', 'Lighting', 'Machine Learning', 'Measurement', 'Measures', 'Monitor', 'Moods', 'Nursing Homes', 'Optics', 'Patients', 'Pattern', 'Performance', 'Periodicity', 'Phase', 'Phototherapy', 'Planet Earth', 'Population', 'Process', 'Reporting', 'Research', 'Research Personnel', 'Retina', 'Rotation', 'Running', 'Sleep', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Vision', 'Visual', 'Visual system structure', 'Wakefulness', 'Wireless Technology', 'Work', 'active control', 'alertness', 'appropriate dose', 'awake', 'base', 'biological systems', 'circadian', 'circadian pacemaker', 'cloud based', 'commercialization', 'design', 'dosage', 'experience', 'falls', 'feeding', 'field study', 'health data', 'improved', 'interest', 'meter', 'next generation', 'novel', 'prototype', 'residence', 'response', 'success', 'therapy design']",NIA,"ERIK PAGE AND ASSOCIATES, INC.",R44,2019,1240470,-0.010740698434711392
"Graphene-based Nanosensor Device for Rapid, Onsite Detection of Total Lead in Tap Water PROJECT SUMMARY Detrimental health impacts of lead are largely attributed to long-term exposures to undetected lead, which are particularly troublesome and problematic because of the neurological damage to children, a situation that should not be tolerated by an advanced society like the U.S. The Flint Water Crisis and many other water catastrophes could have been avoided if early warning can be made possible through timely detection of lead in drinking water at the point of use. Our extensive customer interviews unambiguously suggest that current options for lead detection are unsatisfactory for on-site testing, as they represent two extremes: one being accurate but expensive, slow, and hard to use; and the other being low-cost, fast, and easy to use but inaccurate. NanoAffix Science LLC (NAFX) proposes to address the above unmet need and niche market product gap by empowering water users (particularly those in economically disadvantaged communities) and water service providers with a low-cost, easy-to-use, and accurate handheld tester for rapid detection of total lead in the tap water, right from the kitchen sink. The handheld lead tester combines a novel proprietary micro-sized sensor chip embedded in a proprietary test cell with a portable digital meter for direct readout of testing results. The Phase I project has successfully established the feasibility for detection of soluble lead in the tap water using an earlier version of the prototype handheld tester. The Phase II project will continue to develop the handheld tester toward total lead detection, better device uniformity, pilot scale-up manufacturing, and accurate calibration. At the end of the Phase II project, NAFX plans to produce 20 beta units of the handheld lead tester meeting all performance specifications for field validation by 10 initial customers (e.g., schools/daycares, end water users, and well water drillers). Major innovations of the proposed approach include accurate prediction of the particulate lead through partial digestion based on lead digestion kinetics, and strategic and synergistic improvement of the ultimate sensor prediction accuracy by (1) improving the physical sensor device uniformity (both intra-wafer and inter-wafer) through innovative device configuration and rigorous quality control; and (2) improving the calibration accuracy through innovative theoretical equilibrium chemistry modeling and machine learning data analytics. The NAFX handheld lead tester is the first of its kind to (1) offer all three features sought by customers: accurate, cheap, and fast; and (2) to simultaneously report all three types of lead: total lead (indicative of overall toxicity), soluble lead (indicative of slow leaching of lead), and particulate lead (indicative of sporadic flaking of lead), which thus can not only alert customers to the lead hazard in their drinking water but also enable customers to identify possible causes and most effective solutions to mitigate the lead contamination. Therefore, the project will result in not only considerable economic impact but also immense societal impact. The regular use of NAFX handheld tester - even if intermittently - will virtually eliminate the chance of chronic exposure to undetected lead, thereby accruing significant and predictable public health impact, especially in locations with the highest risk. PROJECT NARRATIVE The NanoAffix Phase II project aims to continue the development of a handheld lead tester for accurate and low- cost onsite detection of total lead in tap water by untrained users, based on the success of the Phase I project. The project will contribute to enhancing the public health by offering an accessible tool for quantitative monitoring of all three types of lead: total lead (indicative of overall toxicity), soluble lead (indicative of slow leaching of lead), and particulate lead (indicative of sporadic flaking of lead) in tap water. The regular use of NanoAffix handheld tester - even if intermittently - will virtually eliminate the chance of chronic exposure to undetected lead, thereby accruing significant and predictable public health impact, especially in locations with the highest risk.","Graphene-based Nanosensor Device for Rapid, Onsite Detection of Total Lead in Tap Water",9847052,R44ES028656,"['Address', 'Algorithms', 'Calibration', 'Cations', 'Cells', 'Chemistry', 'Child', 'Chronic', 'Communication', 'Communities', 'Complex', 'Contracts', 'Data', 'Data Analytics', 'Detection', 'Development', 'Devices', 'Digestion', 'Disinfection', 'Economically Deprived Population', 'Equilibrium', 'Equipment', 'Exposure to', 'Goals', 'Gold', 'Health', 'International', 'Interview', 'Kinetics', 'Laboratories', 'Lead', 'Lead Poisoning', 'Location', 'Machine Learning', 'Measurement', 'Michigan', 'Modeling', 'Monitor', 'Nervous System Trauma', 'Paper', 'Particulate', 'Performance', 'Phase', 'Procedures', 'Process', 'Public Health', 'Quality Control', 'Reporting', 'Research', 'Schools', 'Science', 'Site', 'Societies', 'Specialist', 'System', 'Test Result', 'Testing', 'Time', 'Toxic effect', 'Training', 'Uncertainty', 'Validation', 'Variant', 'Water', 'Water Supply', 'Wireless Technology', 'aqueous', 'base', 'cost', 'digital', 'drinking water', 'economic impact', 'empowered', 'graphene', 'hazard', 'high risk', 'improved', 'innovation', 'lead concentration', 'lead contamination', 'manufacturing scale-up', 'meetings', 'meter', 'nanosensors', 'novel', 'operation', 'portability', 'prototype', 'rapid detection', 'real time monitoring', 'response', 'sample collection', 'sensor', 'service providers', 'success', 'tool', 'virtual', 'water quality', 'well water']",NIEHS,"NANOAFFIX SCIENCE, LLC",R44,2019,565059,0.011070473748861831
"SCH: INT: Conversations for Vision: Human-Computer Synergies in Prosthetic Interactions  The project will investigate prosthetic support for people with visual impairment (PVI) that integrates computer vision-based prosthetics with video-mediated human-in-the-loop prosthetics. Computer vision- based (CV) prosthetics construe the fundamental technical challenge for visual prosthetics as one of parsing and identifying objects across scales, distances, and orientations. Visual prosthetic applications have been central drivers in the development of computer vision technology through the past 50 years. Video-mediated remote sighted assistance (RSA) prosthetics are more recent, enabled by different technologies, and construe the orienting technical challenge for visual prosthetics as one of effective helping interactions. RSA services are commercially available now, and have evoked much excitement in the PVI community. The two approaches, CV and RSA, will be successively integrated through a series of increasingly refined Wizard of Oz simulations, and investigate possible synergies between the two approaches. We will employ a human-centered design approach, identifying a set of key assistive interaction scenarios that represent authentic needs and concerns of PVIs, by leveraging our 6-year relationship working directly with our local chapter of the National Federation of the Blind. RELEVANCE (See Instructions): 23.7 million American adults have vision loss; 1.3 million people in US are legally blind. This project addresses a transformational opportunity to enhance human performance and experience, to diversify workplace participation, and to enhance economic and social well-being. n/a",SCH: INT: Conversations for Vision: Human-Computer Synergies in Prosthetic Interactions ,9928587,R01LM013330,"['Address', 'Adult', 'American', 'Articulation', 'Back', 'Blindness', 'Communities', 'Computer Vision Systems', 'Computers', 'Data Set', 'Development', 'Economics', 'Emotional', 'Female', 'Goals', 'Human', 'Information Sciences', 'Instruction', 'Mediating', 'Modeling', 'Ocular Prosthesis', 'Performance', 'Prosthesis', 'Route', 'Self-Help Devices', 'Series', 'Services', 'Social Well-Being', 'Technology', 'Time', 'Underrepresented Students', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'design', 'experience', 'graduate student', 'human-in-the-loop', 'learning materials', 'legally blind', 'outreach', 'prototype', 'simulation', 'synergism', 'undergraduate student']",NLM,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R01,2019,225147,0.03222135392768901
"Healthcare Impact of Consumer-Driven Atrial Fibrillation Detection PROJECT SUMMARY/ABSTRACT  Companies are increasingly marketing mobile technologies as FDA-cleared medical devices, yet we do not know the consequences of these devices on healthcare utilization, cost, and outcomes. Recently, Apple released the Apple Watch Series 4 as an FDA-cleared medical device. The device includes an alert for the presence of atrial fibrillation (AF) and allows anyone to monitor their heart rhythm for the presence of AF. Apple has an enormous global audience, and the number of people who will use this (and other similar devices) to self-diagnose or monitor AF will be substantial. On one hand, the device may allow new diagnoses that result in treatment, improved quality of life, fewer AF related complications. On the other hand, the device may result in false positives in otherwise healthy people, resulting in more testing and treatments with associated harms. In fact, the U.S. Preventative Task Force recommends against routine surveillance for AF in the general population, citing lack of evidence and possible harm. We have an urgent need for a population-based infrastructure to ensure that technologies entering the market as medical devices are beneficial and safe.  The overall goal of this project is to measure the uptake and effect of the Apple Watch 4 release on healthcare utilization among first-time and known AF patients. Dr. Shah is an early stage investigator with a K08 Career Development Award from the NHLBI. As part of the K08, she has developed a detailed cohort of contemporary AF patients, including clinical notes. Along with a team, she will use real world data, as proposed by the FDA, to generate evidence about risks and benefits of consumer-driven AF detection. She will use natural language processing to leverage the notes and identify AF patients who seek care due to the medical device, and evaluate downstream healthcare utilization, such as additional clinic visits, cardioversions, additional remote monitoring, and cost. The goals of this project will be accomplished through the following Specific Aims: 1) Estimate the proportion of first-time AF patient visits attributable to a mobile device before and after FDA clearance of the Apple Watch 4, and characterize device accuracy and downstream healthcare utilization in this population; and 2) Evaluate healthcare utilization patterns among prevalent AF patients who use mobile devices with AF alerts.  In 2017, Apple sold 17.7 million smart watches, in a device market that continues to grow. Extrapolating from prior annual sales and conservatively assuming a 5% increase in users each year, almost 60 million people will have an Apple Watch by the end of 2020 (not accounting for non-Apple devices with similar functionality). Thus, even in this short period of time, uptake will be substantial and warrant immediate feedback. The results of this project will provide preliminary data for a long-term, multicenter study that evaluates the benefits (improved quality of life, fewer strokes) and harms (increased treatment complications, increased cost) of consumer-driven AF detection. PROJECT NARRATIVE The consequence of mobile technologies marketed as medical devices are unknown, including devices that provide alerts for the presence of atrial fibrillation. The goal of this project is to evaluate the benefits and harm associated with consumer-driven atrial fibrillation detection.",Healthcare Impact of Consumer-Driven Atrial Fibrillation Detection,9809717,R03HL148372,"['Adult', 'Advisory Committees', 'Affect', 'Apple', 'Apple watch', 'Arrhythmia', 'Atrial Fibrillation', 'Benefits and Risks', 'Cardiovascular system', 'Caring', 'Case-Control Studies', 'Clinic Visits', 'Clinical', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Electric Countershock', 'Ensure', 'Feedback', 'General Population', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Hemorrhage', 'Holter Electrocardiography', 'Infrastructure', 'Interruption', 'Intervention', 'K-Series Research Career Programs', 'Lead', 'Marketing', 'Measures', 'Medical Device', 'Monitor', 'Multicenter Studies', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Outcome', 'Patients', 'Pattern', 'Population', 'Prevalence', 'Preventive', 'Quality of life', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Sales', 'Series', 'Sinus', 'Stroke', 'Technology', 'Testing', 'Text', 'Time', 'United States Food and Drug Administration', 'Universities', 'Utah', 'Visit', 'base', 'care seeking', 'cohort', 'cost', 'cost outcomes', 'cryptogenic stroke', 'design', 'follow-up', 'handheld mobile device', 'health care service utilization', 'heart rhythm', 'improved', 'mobile computing', 'population based', 'routine screening', 'self diagnosis', 'smart watch', 'uptake']",NHLBI,UNIVERSITY OF UTAH,R03,2019,76250,0.02309047624067382
" SBIR Topic 379: DigiBioMarC: Digital BioMarkers for Clinical Impact- Moonshot Project(Fast Track) The overall objective of this Fast-Track SBIR contract project is to develop DigiBioMarCTM (Digital BioMarkers for Clinical Impact), a scalable and flexible cloud-based platform to capture and analyze wearable, implantable, or external device data. This platform also provides an informatics tool for automated data aggregation, integration, and machine learning algorithms. It is based on the scalable user-centered Medable platform, which implements standardization and normalization of patient-generated data to drive health insights. DigiBioMarCTM will compare and combine disparate data streams to understand contextualized patient physiology in real time in order to identify disease and/or detect changes in disease/health status. It also will support cohort and clinical studies, particularly those testing digital biomarkers from wearable sensor technologies. This Fast-Track project will focus on product development with an ultimate aim of a product that improves cancer research data and clinical trials, enhances clinical care, and that can be used to engage patients in preventive health behaviors and treatment adherence. The Phase I goal is to develop a data-agnostic DigiBioMarCTM prototype for validation in Phase II. The Phase 1 Go/No-Go decision point to proceed to Phase II will be a working prototype with specified features for further development and validation in Phase II. n/a", SBIR Topic 379: DigiBioMarC: Digital BioMarkers for Clinical Impact- Moonshot Project(Fast Track),9952269,61201800010C,"['Biological Markers', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Cohort Studies', 'Computer software', 'Contracts', 'Data', 'Data Aggregation', 'Development', 'Devices', 'Disease', 'Documentation', 'Goals', 'Health', 'Health Status', 'Health behavior', 'Patients', 'Phase', 'Physiology', 'Preventive', 'Publishing', 'Reporting', 'Small Business Innovation Research Grant', 'Specific qualifier value', 'Standardization', 'Stream', 'System', 'Testing', 'Time', 'Validation', 'anticancer research', 'base', 'behavioral adherence', 'clinical care', 'cloud based', 'digital', 'flexibility', 'graphical user interface', 'improved', 'informatics\xa0tool', 'insight', 'knowledge base', 'machine learning algorithm', 'product development', 'prototype', 'sensor', 'skills', 'tool', 'treatment adherence', 'wearable sensor technology']",NCI,"MEDABLE, INC.",N43,2019,1499800,0.011594337050583893
"Vision in Natural Tasks Summary/Abstract  In the context of natural behavior, humans make continuous sequences of sensory-motor decisions to satisfy current behavioral goals, and vision must provide the information needed to achieve those goals. The proposed work examines gaze and walking decisions in locomotion in outdoor environments, taking advantage of our novel system for measuring combined eye and body movements in these contexts. Currently we have only limited understanding of the constituent tasks in natural locomotion, or the requisite information, and the proposal attempts to specify these.  in the context of natural gait, the patterns of optic flow are unexpectedly complex, raising questions about its role. The patterns of motion on the retina during locomotion depend critically on both eye and body motion, and these in turn depend on behavioral goals. Our first Aim is therefore to comprehensively describe the statistics of retinal motion patterns in a variety of terrains and task contexts. We will measure binocular eye and body movements while walking in outdoor terrains of varying roughness, crossing a busy intersection, and making coffee. These contexts will induce different gaze patterns. We will provide a comprehensive description of the motion stimulus in natural locomotion and help separate out self-motion signals from externally generated motion. These data will allow a more precise specification of the response patterns in cortical motion sensitive areas. Because of the complexity of natural motion patterns, we will re-examine the influence of optic flow on walking direction in a virtual reality environment and test alternative explanations for the role of flow.  A central task in walking is foot placement, and we will focus on identifying the image properties that make a good foothold. Stereo, structure from motion, and spatial image structure are all likely contenders. We directly investigate the role of stereo in foothold selection by examining gait patterns in stereo-deficient subjects in terrains with varying degrees of roughness. Using a different strategy, we will attempt to predict gaze locations and footholds in rough terrain using convolution neural nets (CNN’s) to identify potential search templates for footholds in rough terrain. We will describe fixation patterns from crosswalk and sidewalk navigation and attempt to make inferences about their purpose, and use Modular Inverse Reinforcement Learning (MIRL) to predict direction decisions and decompose the behavior into sub-tasks.  The collection of integrated gaze, body kinematics, and scene images in a range of natural environments is innovative, as little comparable data exists The work will be strengthened by the investigation of stereo- deficient subjects for whom there is almost no integrated eye and body data. Since much of the work in robotics has no visual input at all this should help in development of visual guidance for robots and also help better define the necessary information for individuals with impaired vision. The data set will be made publicly available. Project Narrative  The central goal of this work is to understand vision in its natural context. This is very important information in order to devise suitable vision aids and rehabilitation strategies for individuals with visual impairments, and it is becoming increasingly accessible because of developments in technology for monitoring eye and body movements. The proposed work examines gaze and walking decisions in locomotion in outdoor environments, taking advantage of our novel system for measuring combined eye and body movements in these contexts. Currently we have only limited understanding of the constituent tasks and requisite information in natural locomotion, and the proposal attempts to specify these. The collection of integrated gaze, body kinematics, and scene images in a range of natural environments is innovative, as little comparable data exists. The work will be strengthened by the investigation of stereo-deficient subjects for whom there is almost no integrated eye and body data. Since much of the work in robotics has no visual input at all this should help in development of visual guidance for robots and also help better define the necessary information for individuals with impaired vision. The data set will be made publicly available.",Vision in Natural Tasks,9830977,R01EY005729,"['Affect', 'Area', 'Behavior', 'Behavioral', 'Binocular Vision', 'Cells', 'Characteristics', 'Coffee', 'Collection', 'Complex', 'Cues', 'Data', 'Data Set', 'Development', 'Distant', 'Environment', 'Eye', 'Eye Movements', 'Gait', 'Goals', 'Grant', 'Head', 'Human', 'Image', 'Individual', 'Investigation', 'Knowledge', 'Learning', 'Link', 'Location', 'Locomotion', 'Machine Learning', 'Measures', 'Monitor', 'Motion', 'Motor', 'Movement', 'Pattern', 'Psychological reinforcement', 'Retina', 'Retinal', 'Rewards', 'Robot', 'Robotics', 'Role', 'Sampling', 'Seminal', 'Sensory', 'Signal Transduction', 'Speed', 'Stimulus', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'To specify', 'Uncertainty', 'Vision', 'Visit', 'Visual', 'Visual Fields', 'Visual impairment', 'Walkers', 'Walking', 'Work', 'base', 'convolutional neural network', 'cost', 'experimental study', 'foot', 'gaze', 'imaging properties', 'innovation', 'kinematics', 'novel', 'optic flow', 'rehabilitation strategy', 'response', 'sample fixation', 'statistics', 'virtual reality', 'vision aid', 'vision development', 'vision rehabilitation', 'visual information']",NEI,"UNIVERSITY OF TEXAS, AUSTIN",R01,2019,369009,-0.0037439143325081795
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9750520,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Personal Satisfaction', 'Persons', 'Phase', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'manufacturability', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2019,507856,0.04698193430233425
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9653180,R01EY025332,"['3-Dimensional', 'Access to Information', 'Adoption', 'Algorithms', 'American', 'Architecture', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image Enhancement', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'experimental study', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2019,416574,0.02138415910852644
SCH: INT: A Context-aware Cuff-less Wearable Ambulatory Blood Pressure Monitor using a Bio-Impedance Sensor Array No abstract provided n/a,SCH: INT: A Context-aware Cuff-less Wearable Ambulatory Blood Pressure Monitor using a Bio-Impedance Sensor Array,9791169,R01EB028106,"['Address', 'Adult', 'Affect', 'Aging', 'Algorithms', 'Ambulatory Blood Pressure Monitoring', 'American', 'American Heart Association', 'Arteries', 'Awareness', 'Biological Markers', 'Blood Pressure', 'Blood Pressure Monitors', 'Blood Vessels', 'Calibration', 'Cardiology', 'Cardiovascular Diseases', 'Cardiovascular system', 'Caregivers', 'Caring', 'Cessation of life', 'Characteristics', 'Clinic', 'Clinic Visits', 'Clinical', 'Clinical Trials', 'Data', 'Devices', 'Diagnosis', 'Disease', 'Disease Management', 'Environment', 'Frequencies', 'Funding', 'Grain', 'Guidelines', 'Health Sciences', 'Home environment', 'Hour', 'Human Resources', 'Hypertension', 'Institutes', 'Institution', 'International', 'Intervention Trial', 'Investigation', 'Laboratories', 'Left ventricular structure', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Monitor', 'Noise', 'Outcome', 'Patients', 'Pattern', 'Phenotype', 'Physicians', 'Physiologic pulse', 'Physiological', 'Play', 'Positioning Attribute', 'Posture', 'Preventive Intervention', 'Reading', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Role', 'Site', 'Skin', 'Source', 'Specific qualifier value', 'Speed', 'Sphygmomanometers', 'System', 'Technology', 'Testing', 'Texas', 'Time', 'Tissues', 'Travel', 'United States National Institutes of Health', 'Universities', 'Validation', 'Wearable Computer', 'Wrist', 'base', 'blood perfusion', 'cardiovascular disorder risk', 'clinical practice', 'cohesion', 'college', 'cost', 'design', 'disability-adjusted life years', 'disorder prevention', 'electric impedance', 'health disparity', 'hypertension control', 'learning strategy', 'minority health', 'novel', 'novel strategies', 'patient population', 'sensor', 'signal processing', 'therapy development', 'validation studies', 'wearable device']",NIBIB,TEXAS ENGINEERING EXPERIMENT STATION,R01,2019,289428,0.015609540521454544
"Annual meeting of the Vision Sciences Society: Travel grants for junior investigators PROJECT SUMMARY/ABSTRACT The Vision Sciences Society is a nonprofit membership organization of nearly 2000 scientists interested in the functional aspects of vision. VSS was founded in 2001 with the purpose of bringing together scientists from a broad range of disciplines including visual psychophysics, visual neuroscience, computational vision and visual cognition. The scientific content of the meetings reflects the breadth of topics and interconnected ideas and approaches in modern vision science, from visual coding to perception, recognition and the visual control of action, as well as recent developments in cognitive psychology, computer vision and neuroimaging. Since its founding, VSS has provided a forum and framework for communicating advances in vision science, and VSS has become a flagship conference for the field. The interdisciplinary nature of VSS is reflected in the deliberately diverse membership of the Board of Directors and Abstract Review Committee, and by its formal relationship with the more clinically-oriented Association for Research in Vision and Ophthalmology. Many of the faculty from institutions in the United States who attend VSS are principal investigators of National Eye Institute grants; hence, the research objectives of the programs of the National Institutes of Health and of the National Eye Institute are well-represented in the program planning and individual presentations. Over 60% of participants are predoctoral and postdoctoral trainees. Of these 55% are US citizens. VSS provides multiple career development opportunities: (1) the platform and poster presentations provide a forum for trainees to showcase their work and receive feedback, (2) career-development workshops cover topics such as “Getting that Faculty Job”, “Reviewing and Responding to Reviews”, “The Public Face of your Science”, “Careers in Industry and Government”, “Faculty Careers at Primarily Undergraduate Institutions”, and include panel discussions with journal editors, NIH and NSF grant officers, and academic and industry representatives, (3) a “Meet the Professors” event in which trainees meet in small groups with members of the VSS Board and other professors for free-wheeling, open-ended discussions, and (4), a partnership with ARVO through which trainees from one society can carry out research or attend the meeting of the other. Informally, VSS provides opportunities for networking with peers and senior colleagues in a comfortable and engaging setting. The large contingent of early-stage investigators at VSS is a sign of the strong health of the field and the opportunity VSS provides for advancing the field. Our goal is to facilitate access and participation for this next generation of vision scientists. The purpose of this grant is to provide 35 travel awards (5 for ARVO affiliates) for early-career investigators to attend the 2019 meeting, with the focus on attracting and supporting a diverse pool of pre- doctoral students, postdoctoral trainees, and pre-tenure faculty who demonstrate potential for future success as vision researchers and whose research findings will be presented at the meeting. Funds are also requested to support childcare services for awardees and workshops and social events for all early-career participants. PROJECT NARRATIVE The Vision Science Society (VSS) organizes an annual meeting for presenting cutting edge research on the mechanisms and principles of normal and abnormal visual perception. The aging American population will experience an increasing rate of visual loss due to diseases and disorders of the eye and central visual pathway. Understanding the perceptual sequelae of and developing effective therapies for visual disorders requires knowledge in the diverse research domains sponsored by the annual meeting of VSS.",Annual meeting of the Vision Sciences Society: Travel grants for junior investigators,9763082,R13EY030356,"['3-Dimensional', 'Address', 'Aging', 'American', 'Area', 'Attention', 'Award', 'Binocular Vision', 'Blindness', 'Child Care', 'Clinical', 'Coffee', 'Cognitive Science', 'Color', 'Complement', 'Computer Simulation', 'Computer Vision Systems', 'Development', 'Discipline', 'Disease', 'Educational workshop', 'Event', 'Exhibits', 'Eye Movements', 'Eye diseases', 'Face', 'Faculty', 'Feedback', 'Female', 'Florida', 'Funding', 'Future', 'Goals', 'Government', 'Grant', 'Health', 'Hour', 'Human', 'Individual', 'Industry', 'Institution', 'Island', 'Journals', 'Knowledge', 'Light', 'Medal', 'Mediation', 'Mentors', 'Mentorship', 'Modernization', 'Motion Perception', 'Music', 'National Eye Institute', 'Nature', 'Occupations', 'Ophthalmology', 'Participant', 'Perception', 'Perceptual learning', 'Population', 'Principal Investigator', 'Psychophysics', 'Publishing', 'Research', 'Research Personnel', 'Resort', 'Review Committee', 'Science', 'Scientist', 'Services', 'Societies', 'Students', 'Time', 'Training', 'Travel', 'United States', 'United States National Institutes of Health', 'Vendor', 'Vision', 'Vision Disorders', 'Visual Illusions', 'Visual Pathways', 'Visual Perception', 'Visual Psychophysics', 'Work', 'care systems', 'career', 'career development', 'design', 'doctoral student', 'effective therapy', 'experience', 'face perception', 'fovea centralis', 'graduate student', 'interest', 'mathematical model', 'meetings', 'member', 'multisensory', 'neuroimaging', 'next generation', 'object recognition', 'peer', 'perceptual organization', 'posters', 'pre-doctoral', 'professor', 'programs', 'science and society', 'social', 'spatial vision', 'success', 'symposium', 'undergraduate student', 'vision science', 'visual coding', 'visual cognition', 'visual control', 'visual memory', 'visual neuroscience', 'visual processing', 'visual search']",NEI,UNIVERSITY OF NEVADA RENO,R13,2019,47210,0.027147840805032054
"Adaptive & Individualized AAC The heterogeneity of the more than 1.3% of Americans who suffer from severe physical impairments (SPIs) preclude the use of common augmentative or alternative communication (AAC) solutions such as manual signs, gestures or dexterous interaction with a touchscreen for communication. While efforts to develop alternative access methods through eye or head tracking have provided some communication advancements for these individuals, all current technologies suffer from the same fundamental limitation: existing AAC devices require patients to conform to generic communication access methods and interfaces rather than the device conforming to the user. Consequently, AAC users are forced to settle for interventions that require excessive training and cognitive workload only to deliver extremely slow information transfer rates (ITRs) and recurrent communication errors that ultimately deprive them of the fundamental human right of communication. To meet this health need, we propose the first smart-AAC system designed using individually adaptive access methods and AAC interfaces to accommodate the unique manifestations of motor impairments specific to each user. Preliminary research by our team of speech researchers at Madonna Rehabilitation Hospital (Communication Center Lab) and Boston University (STEPP Lab), utilizing wearable sensors developed by our group (Altec, Inc) have already demonstrated that metrics based on surface electromyographic (sEMG) and accelerometer measures of muscle activity and movement for head-mediated control can be combined with optimizable AAC interfaces to improve ITRs when compared with traditional unoptimized AAC devices. Leveraging this pilot work, our team is now proposing a Phase I project to demonstrate the proof-of-concept that a single sEMG/IMU hybrid sensor worn on the forehead can provide improvements in ITR and communication accuracy when integrated with an AAC interface that is optimized through machine learning algorithms. The prototype system will be tested and compared to a conventional (non-adaptable) interface in subjects with SPI at a collaborative clinical site. Assistance by our speech and expert-AAC collaborators will ensure that all phases of technology development are patient-centric and usable in the context of clinical care. In Phase II we will build upon this proof-of-concept to design a smart-AAC system with automated optimization software that achieves dynamic learning which adapts to intra-individual changes in function through disease progression or training as well as inter-individual differences in motor impairments for a diverse set of users with spinal cord injury, traumatic brain injury, cerebral palsy, ALS, and other SPIs. The innovation is the first and only AAC technology that combines advancements in wearable-sensor access with interfaces that are autonomously optimized to the user, thereby reducing the resources and training needed to achieve effective person-centric communication in SPI, through improved HMI performance and reduced workload. This project addresses the fundamental mission of NIDCD (National Institute for Deafness and Communication Disorders) to provide a direct means of assisting communication for people with severe physical impairments caused by stroke, high level spinal cord injury, neural degeneration, or neuromuscular disease. Leveraging wearable access technology (which has barely been explored for AAC users), we will develop a first-of-its-kind adaptive tablet interface tailored to individual users through advanced movement classification algorithms. Through these efforts, we aim to provide an improved Human Machine Interface (HMI) that is able to accommodate varying degrees of inter- and intra-subject residual motor function and context dependent impairments to provide individuals with SPI the opportunity for improved societal integration and quality of life.",Adaptive & Individualized AAC,9907832,R43DC018437,"['Accelerometer', 'Address', 'American', 'Boston', 'Cerebral Palsy', 'Child', 'Cognitive', 'Communication', 'Communication Methods', 'Communication impairment', 'Computer software', 'Custom', 'Development', 'Devices', 'Diagnosis', 'Disease Progression', 'Ensure', 'Eye', 'Facial Muscles', 'Fatigue', 'Forehead', 'Gestures', 'Goals', 'Head', 'Head Movements', 'Health', 'Heterogeneity', 'Hospitals', 'Human Rights', 'Hybrids', 'Image', 'Imaging problem', 'Impairment', 'Individual', 'Individual Differences', 'Institutes', 'Intervention', 'Intuition', 'Learning', 'Linguistics', 'Manuals', 'Measures', 'Mediating', 'Methods', 'Mission', 'Motor', 'Motor Manifestations', 'Movement', 'Muscle', 'National Institute on Deafness and Other Communication Disorders', 'Nerve Degeneration', 'Neuromuscular Diseases', 'Patients', 'Pattern', 'Performance', 'Persons', 'Phase', 'Population Heterogeneity', 'Quality of life', 'Recurrence', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Residual state', 'Resources', 'Series', 'Signal Transduction', 'Speech', 'Spinal cord injury', 'Stroke', 'Surface', 'System', 'Tablets', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Traumatic Brain Injury', 'United States National Aeronautics and Space Administration', 'Universities', 'User-Computer Interface', 'Variant', 'Work', 'Workload', 'alternative communication', 'base', 'classification algorithm', 'clinical care', 'clinical research site', 'communication device', 'deafness', 'design', 'experimental study', 'improved', 'innovation', 'kinematics', 'machine learning algorithm', 'mathematical model', 'motor impairment', 'novel', 'prototype', 'rehabilitation engineering', 'rehabilitation science', 'sensor', 'sensor technology', 'signal processing', 'technology development', 'touchscreen', 'two-dimensional', 'wearable device']",NIDCD,"ALTEC, INC.",R43,2019,224701,-0.010825500970362462
"Dense life-log health analytics from wearable senors using functional analysis and Riemannian geometry The growth and acceptance of wearable devices (e.g., accelerometers) and personal technologies (e.g., smartphones), coupled with larger storage capacities, waterproofing, and more unobtrusive wear locations, has made long-term monitoring of behaviors throughout the 24-hour spectrum more feasible. Wearable devices relevant for human activity (e.g., GENEActiv accelerometer) contain several complementary sensors (accelerometers, gyro, heart- rate monitor etc.) and sample at high rates (e.g., 100Hz for accelerometer). These high-sampling rates and the long duration of capture result in life-log data that truly qualifies as multimodal and big time-series data. The challenges and opportunities involved in fully harvesting these types of data, for widely applicable interventions, suggest that an interdisciplinary approach spanning mathematical sciences, signal processing, and health is needed. Our innovation includes the use of functional-data analysis tools to represent and process the dense time-series data. Functional data analysis is then integrated into machine learning and pattern discovery algorithms for activity classification, prediction of attributes, and discovery of new activity classes. We anticipate that the proposed framework will lead to new insights about human activity and its impact on health outcomes. This interdisciplinary project builds on several research activities of the team. Our past work includes: a) new mathematical developments for computing statistics on time-series data viewed as elements of a function-spaces, b) algorithms for activity recognition that integrate the function-space techniques, and c) data from long-term observational studies of human activity from multimodal sensors. The new work we propose addresses the unique mathematical and computational challenges posed by densely multimodal, long-term, densely-sampled Iifelog big-data in a comprehensive framework. The fusion of ideas from human activity modeling, functional-analysis, geometric metrics, and algorithmic machine learning, present unique opportunities for fundamental advancement of the state-of-the-art in objective measurement and quantification of behavioral markers from wearable devices. The proposed approach also brings to fore: a) new mathematical developments of elastic metrics over multi-modal time-series data, b) comparing sequences evolving on different feature manifolds, c) estimation of quasi- periodicities, d) and a new generation of machine-learning and pattern discovery algorithms. The mathematical and algorithmic tools proposed have the potential to significantly advance how wearable data from contemporary devices with high-sampling rates and large storage capabilities are represented, processed, and transformed into accurate inferences about human activity. Wearable devices are becoming more widely adopted in recent years for general health and recreational uses by the broad populace. This research will result in improved algorithms to process the data available from such wearable devices. The long-term goal of the research is to enable personalized home-based physical activity regimens for conditions such as stroke and diabetes. n/a",Dense life-log health analytics from wearable senors using functional analysis and Riemannian geometry,9903672,R01GM135927,"['Accelerometer', 'Activity Cycles', 'Algorithms', 'Arrhythmia', 'Awareness', 'Behavior', 'Behavior monitoring', 'Big Data', 'Body Temperature', 'Cellular Phone', 'Chemotherapy-Oncologic Procedure', 'Collection', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Devices', 'Dust', 'Encapsulated', 'Energy Metabolism', 'Geometry', 'Glean', 'Goals', 'Growth', 'Health', 'Health Status', 'Healthcare', 'Heart', 'Heart Rate', 'Home environment', 'Hour', 'Human', 'Individual', 'Lead', 'Life', 'Location', 'Mathematics', 'Medical', 'Methods', 'Modeling', 'Monitor', 'Movement', 'National Institute of General Medical Sciences', 'Outcome', 'Participant', 'Pattern', 'Periodicity', 'Physical activity', 'Population', 'Research', 'Research Project Grants', 'Rest', 'Sampling', 'Series', 'Signal Transduction', 'Swimming', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'Vision', 'Walking', 'Water', 'Work', 'base', 'circadian', 'design', 'diaries', 'health related quality of life', 'heart rate monitor', 'insight', 'interest', 'mathematical methods', 'metastatic colorectal', 'multimodality', 'personalized intervention', 'post stroke', 'programs', 'sensor', 'statistics', 'tool', 'wearable device']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2019,316425,-0.04022099869133258
"CANNABIS IMPAIRMENT DETECTION APPLICATION (CIDA) (T163). SBIR PHASE II. POP: 9/20/2019-9/19/2021. N44DA-19-1218. Under this Small Business Innovation Research (SBIR) Phase I project, Research Topic 163, the Contractor will develop a portable, easily applied system incorporating a neuropsychological test protocol to detect cannabis impairment. n/a",CANNABIS IMPAIRMENT DETECTION APPLICATION (CIDA) (T163). SBIR PHASE II. POP: 9/20/2019-9/19/2021. N44DA-19-1218.,10044153,5N95019C00052,"['Algorithm Design', 'Apple', 'Biological Markers', 'Cannabis', 'Clinical Research', 'Contractor', 'Data', 'Databases', 'Detection', 'Goals', 'Health Technology', 'Impairment', 'Law Enforcement', 'Neuropsychological Tests', 'Phase', 'Procedures', 'Protocols documentation', 'Research', 'Small Business Innovation Research Grant', 'Study Subject', 'System', 'digital', 'human subject', 'machine learning algorithm', 'novel', 'novel therapeutics', 'portability', 'potential biomarker']",NIDA,"ADVANCED BRAIN MONITORING, INC.",N44,2019,1499722,0.007591354971097855
"Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility Innovative Design Labs (IDL) proposes to create a system to improve the mobility and control of exoskeletons. Recent research has found that 3.86 million Americans require wheelchairs and the number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk, thus providing a way to more fully reintegrate these individuals into society. Our proposal seeks to address one of the hurdles limiting the widespread adoption of exoskeletons in the home and community—the inability of the user to dynamically control gait parameters. This concept has the potential to significantly change the way exoskeletons work and facilitate their adoption into the market. Hypothesis: We hypothesize that the proposed solution will provide users a practical way to adjust their suit’s gait to precisely achieve their navigational goals. Specific Aims: Phase I: 1) Build a prototype and Perform Preliminary Laboratory Testing; 2) Develop and Benchmark Algorithms; and 3) Perform Pilot Human Study of Prototype with Exoskeleton Subjects. Phase II: 1) Develop Customized, Production-Ready Hardware and Firmware 2) Integrate with Exoskeleton Control System; and 3) Perform an evaluation of the system through human study testing. Recent research has found that 3.86 million Americans require wheelchairs and that number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk thereby providing a way to more fully reintegrate these individuals into society.",Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility,9570624,R44AG053890,"['3-Dimensional', 'Address', 'Adoption', 'Algorithm Design', 'Algorithms', 'American', 'Benchmarking', 'Bionics', 'Caregivers', 'Chicago', 'Clinical', 'Collaborations', 'Communities', 'Community Participation', 'Computational algorithm', 'Computer Vision Systems', 'Crutches', 'Custom', 'Dependence', 'Devices', 'Electrical Engineering', 'Emotional', 'Environment', 'Evaluation', 'Exercise', 'Eye', 'Family', 'Feedback', 'Freedom', 'Friends', 'Gait', 'Goals', 'Health', 'Height', 'Home environment', 'Hospitals', 'Human', 'Image', 'Impairment', 'Individual', 'Industry', 'Institutes', 'Laboratories', 'Length', 'Location', 'Medical', 'Methods', 'Motion', 'Patients', 'Performance', 'Phase', 'Population', 'Process', 'Production', 'Quality of life', 'Ramp', 'Rehabilitation Centers', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Safety', 'Small Business Innovation Research Grant', 'Social isolation', 'Societies', 'Software Engineering', 'System', 'Technology', 'Testing', 'Uncertainty', 'Vision', 'Walking', 'Wheelchairs', 'Work', 'commercialization', 'design', 'exoskeleton', 'experience', 'human study', 'image processing', 'improved', 'improved mobility', 'innovation', 'insight', 'member', 'product development', 'prototype', 'rehabilitation technology', 'robot exoskeleton', 'usability']",NIA,"INNOVATIVE DESIGN LABS, INC.",R44,2019,814735,0.0006945255712555243
"Environmental Localization Mapping and Guidance for Visual Prosthesis Users Project Summary About 1.3 million Americans aged 40 and older are legally blind, a majority because of diseases with onset later in life, such as glaucoma and age-related macular degeneration. Second Sight has developed the world's first FDA approved retinal implant, Argus II, intended to restore some functional vision for people suffering from retinitis pigmentosa (RP). In this era of smart devices, generic navigation technology, such as GPS mapping apps for smartphones, can provide directions to help guide a blind user from point A to point B. However, these navigational aids do little to enable blind users to form an egocentric understanding of the surroundings, are not suited to navigation indoors, and do nothing to assist in avoiding obstacles to mobility. The Argus II, on the other hand, provides blind users with a limited visual representation of their surroundings that improves users' ability to orient themselves and traverse obstacles, yet lacks features for high-level navigation and semantic interpretation of the surroundings. The proposed research aims to address these limitations of the Argus II through a synergy of state-of-the-art stimultaneous localization and mapping (SLAM) and object recognition technologies. For the past three years, JHU/APL has collaborated with Second Sight to develop similar advanced vision-based capabilities for the Argus II, including capabilities for object recognition and obstacle detection by stereo vision. This proposal is driven by the hypothesis that navigation for users of retinal prosthetics can be greatly improved by incorporating SLAM and object recognition technology conveying environmental information via a retinal prosthesis and auditory feedback. SLAM enables the visual prosthesis system to construct a map of the user's environment and locate the user within that map. The system then provides object location and navigational cues via appropriate sensory modalities enabling the user to mentally form an egocentric map of the environment. We propose to develop and test a visual prosthesis system which 1) constructs a map of unfamiliar environments and localizes the user using SLAM technology 2) automatically identifies navigationally-relevant objects and landmarks using object recognition and 3) provides sensory feedback for navigation, obstacle avoidance, and object/landmark identification. Project Narrative The proposed system, when realized, will use advanced simultaneous localization and mapping, and object recognition techniques, to enable visual prosthesis users with unprecedented abilities to autonomously navigate and identify objects/landmarks in unfamiliar environments.",Environmental Localization Mapping and Guidance for Visual Prosthesis Users,9818350,R01EY029741,"['3-Dimensional', 'Address', 'Age related macular degeneration', 'Algorithms', 'American', 'Competence', 'Complex', 'Computer Vision Systems', 'Cues', 'Data', 'Dependence', 'Detection', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Environment', 'Evaluation', 'FDA approved', 'Feedback', 'Glaucoma', 'Goals', 'Image', 'Implant', 'Late-Onset Disorder', 'Lead', 'Learning', 'Life', 'Location', 'Maps', 'Medical Device', 'Modality', 'Motion', 'Ocular Prosthesis', 'Patients', 'Performance', 'Psyche structure', 'Research', 'Retinitis Pigmentosa', 'Running', 'Semantics', 'Sensory', 'Societies', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Update', 'Vision', 'Visual', 'Volition', 'aged', 'auditory feedback', 'base', 'behavior test', 'blind', 'cognitive load', 'falls', 'human subject', 'improved', 'innovation', 'legally blind', 'navigation aid', 'object recognition', 'portability', 'prosthesis wearer', 'prototype', 'research and development', 'retina implantation', 'retinal prosthesis', 'sensory feedback', 'smartphone Application', 'synergism', 'visual feedback', 'visual information']",NEI,JOHNS HOPKINS UNIVERSITY,R01,2019,641060,0.02669711505429278
"Development/Commercialization of a Sensing Device to Detect Vaping Development/Commercialization of a Sensing Device to Detect Vaping Summary The use of e-cigarettes or vaping has been steadily increasing since its introduction. While potentially a tool to wean cigarette smokers from combustible tobacco, one consequence of the introduction of these devices has been the adoption of vaping by adolescents. While companies that offer vaping instruments for sale note that their material is directed to adults and intended as an aid for smoking cessation, recent reports have demonstrated that middle school and high school students in many countries, some as young as thirteen, have taken to vaping. Data analysis from a 2015 study in the U.S. indicated that 16% of high school students and 5% of middle school students reported vaping in the past thirty days. Most researchers speculated that the number of users would increase from these baselines and evidence indicates that this prediction is correct. Anecdotal evidence indicates that vaping in middle school and high school bathrooms is a major problem. FreshAir Sensor currently sells tobacco and marijuana smoking sensors along with 24/7 monitoring of the devices. The company has leveraged the knowledge of sensor development to produce preliminary components of an early stage sensing system capable of detecting vaping. Preliminary data to demonstrate this accomplishment is provided. The fast track research described in this proposal will enable the optimization of the sensor as well as commercialization of the resulting instrument in minimal time. The need to reduce and eventually eliminate adolescent vaping is urgent. The deployment of the proposed device in schools and other educational institutions will eliminate vaping during school hours and will, therefore, contribute to improvements in the overall health of adolescents by curtailing nicotine intake. Narrative Vaping has become a problem in schools with students, in steadily increasing numbers, using bathrooms and other less monitored spaces to indulge in the use of the newest vaping hardware. FreshAir Sensor is developing a sensor to detect vaping in otherwise unmonitored spaces. The use of this sensing system has the potential to reduce and, eventually, eliminate vaping behavior in schools, thereby reducing the harmful effects of nicotine in adolescents.",Development/Commercialization of a Sensing Device to Detect Vaping,9838650,R44DA049595,"['Adolescent', 'Adoption', 'Adult', 'Air', 'Algorithms', 'Behavior', 'Chemicals', 'Cigarette Smoker', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Dose', 'Effectiveness', 'Electronic cigarette', 'Electronics', 'Engineering', 'Environmental Risk Factor', 'Event', 'Exposure to', 'Fatigue', 'Film', 'Goals', 'High School Student', 'Hour', 'Humidity', 'Institution', 'Intake', 'Knowledge', 'Laboratories', 'Longevity', 'Marijuana', 'Marijuana Smoking', 'Methods', 'Middle School Student', 'Minor', 'Modality', 'Monitor', 'Morphology', 'Neurotoxins', 'Nicotine', 'Phase', 'Polymers', 'Production', 'Property', 'Public Housing', 'Reporting', 'Research', 'Research Personnel', 'Sales', 'Schools', 'Science', 'Smoking', 'Specificity', 'Students', 'System', 'Temperature', 'Testing', 'Time', 'Tobacco', 'Tobacco smoking behavior', 'Weaning', 'adolescent health', 'base', 'commercialization', 'design', 'detector', 'electronic cigarette use', 'high school', 'instrument', 'junior high school', 'machine learning algorithm', 'monitoring device', 'prototype', 'research and development', 'response', 'sensor', 'sensor technology', 'smoking cessation', 'tool', 'vaping', 'vapor']",NIDA,FRESHAIR SENSOR CORPORATION,R44,2019,225000,0.01018520045033583
"Independent Exoskeleton-Use through Robust Stand-to-Sit Safety Project Summary/Abstract Innovative Design Labs (IDL) proposes to create a system for the sensing and control of stand-to-sit motions of a wearable bionics suit. Currently 5.6 million people in the US have impaired mobility from a number of different causes. The primary means of mobility for many of these patients is the wheelchair as it has been for most of the last 50 years. Despite all the benefits introduced by widespread use of the wheelchair, it remains a less than ideal mobility solution. Exoskeleton suits have the potential to empower individuals with impaired mobility with an alternative to wheelchairs that allows them to stand up and walk independently within their home and community has the potential to more fully reintegrate these individuals into society while also further improving their health and quality of life. For exoskeletons to gain acceptance in every-day independent home and community use, many control and safety related functionalities still need to be addressed. Our proposal seeks to address one of the gaps in allowing for independent use of exoskeletons in the home and community, namely, functionality to transition from standing to sitting in a safe manner. The proposed work will provide exoskeleton users with the new ability to independently sit down without assistance and confidence in being able to do so without falling and risking possible injuries. It aims to significantly change the way exoskeletons work thereby facilitating their adoption into the market and directly impacting the lives of individuals with disabilities. Project Narrative Exoskeletons can provide patients with SCI, stroke, and other types of impaired mobility access to extended duration, gravity dependent ambulation that can directly combat the risks associated with physical deconditioning. There are benefits for exoskeleton-use to gain widespread acceptance in every-day, independent home and community settings. Currently, exoskeletons are not approved for independent-use because functionalities like transferring from standing-to-sitting requires continuous assistance from caregivers.",Independent Exoskeleton-Use through Robust Stand-to-Sit Safety,9560683,R44AG057267,"['3-Dimensional', 'Activities of Daily Living', 'Address', 'Adoption', 'Algorithmic Software', 'Algorithms', 'Area', 'Bionics', 'Caliber', 'Caregivers', 'Chicago', 'Collaborations', 'Communities', 'Computer Vision Systems', 'Country', 'Custom', 'Development', 'Disabled Persons', 'Emerging Technologies', 'Engineering', 'Environment', 'Evaluation', 'Fall injury', 'Feedback', 'Force of Gravity', 'Gait', 'Health', 'Height', 'Home environment', 'Hospitals', 'Imaging technology', 'Impairment', 'Individual', 'Injury', 'Institutes', 'Letters', 'Mechanics', 'Metric System', 'Motion', 'Patients', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Production', 'Quality of life', 'Rehabilitation Centers', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Rest', 'Risk', 'Robotics', 'Safety', 'Scientist', 'Small Business Innovation Research Grant', 'Societies', 'Spinal cord injury patients', 'Stroke', 'Surface', 'System', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Validation', 'Walking', 'Wheelchairs', 'Work', 'blind', 'combat', 'community setting', 'critical period', 'deconditioning', 'design', 'exoskeleton', 'experience', 'fall risk', 'falls', 'human study', 'improved', 'innovation', 'member', 'prototype', 'rehabilitation technology', 'robot exoskeleton', 'stroke patient']",NIA,"INNOVATIVE DESIGN LABS, INC.",R44,2019,806285,-0.00922891422129273
"Developing and Evaluating In-Home Supportive Technology for Dementia Caregivers Abstract  Caring for a loved one with Alzheimer’s disease, frontotemporal dementia, or another neurodegenerative disease is a highly meaningful part of family life. However, the associated burden and strain can have adverse effects on caregivers including mental and physical health problems, reduced well-being, and increased mortality. These effects, in turn, can compromise care quality and shorten survival times for people with dementia (PWD). Research has consistently found that behavioral symptoms in PWD are most strongly associated with adverse caregiver effects, even more so than cognitive and functional symptoms. Empirically- supported interventions are needed that: (a) target mechanisms/pathways shown to connect behavioral symptoms in PWD with adverse effects in caregivers, and (b) can be disseminated successfully into larger community settings. In this SBIR Fast Track application, we will develop, refine, and evaluate People Power Caregiver (PPCg), a flexible and expandable hardware/software system designed to integrate in-home sensors and devices, emergency responding, social networking, and Internet-of-Things (i.e., devices that can be controlled and communicated with via the internet) technologies to create a more supportive and safe home environment for caregivers and PWD. PPCg monitors troublesome behaviors in PWD (e.g., wandering), and targets mechanisms (e.g., worry, social isolation) thought to link behavioral symptoms in PWD with adverse caregiver outcomes. PPCg is also designed to minimize demands on caregivers’ limited time and energy and to provide a platform for data collection that can be used by researchers and care professionals.  This application is an innovative partnership between People Power (CEO: Gene Wang, www.peoplepowerco.com) in Redwood City, California and the Berkeley Psychophysiology Laboratory (Director: Robert W. Levenson) at the University of California, Berkeley. People Power is an award-winning, established leader in home monitoring and Internet-of-Things technology and has recently started developing assistive technologies for the elderly. The Berkeley Psychophysiology Laboratory has been engaged in basic and applied research with PWD and other neurodegenerative diseases and their caregivers for the past 15 years. The proposal addresses three specific aims: Aim 1: Focus groups. In Phase I of the project, a preliminary version of PPCg will be developed and refined with input from focus groups of caregivers and in- home testing (Study 1). Aim 2: Efficacy. In the first year of Phase II of the project, the first production version of PPCg will be installed by the research team in 80 homes and evaluated in a randomized controlled efficacy trial that includes careful diagnosis and assessment of emotional functioning in PWD and caregivers (Study 2). Aim 3: Effectiveness. In the second year of Phase II of the project, working with energy industry partners, a refined and expanded second production version of PPCg will be provided to 400 homes with familial caregivers for self-installation and evaluation in a community-based effectiveness trial (Study 3). Relevance  Dementias cause profound cognitive, emotional, and functional deficits. As the disease progresses, people with dementia become increasingly dependent on caregivers, who are at heightened risk for mental and physical health problems. Applying assistive technology to monitor worrisome behaviors, improve safety, and reduce social isolation in the home environment can reduce caregiver burden and improve care in ways that have major public health benefits.",Developing and Evaluating In-Home Supportive Technology for Dementia Caregivers,9838988,R44AG059458,"['Address', 'Adverse effects', 'Age', 'Aggressive behavior', 'Alzheimer&apos', 's Disease', 'Applied Research', 'Artificial Intelligence', 'Award', 'Basic Science', 'Behavior', 'Behavior monitoring', 'Behavioral Symptoms', 'California', 'Caregiver Burden', 'Caregivers', 'Caring', 'Cities', 'Clinical Trials', 'Cognitive', 'Communities', 'Computer software', 'Data Collection', 'Dementia', 'Dementia caregivers', 'Devices', 'Diagnosis', 'Disease', 'Effectiveness', 'Elderly', 'Emergency Situation', 'Emotional', 'Evaluation', 'Family', 'Family member', 'Fire - disasters', 'Floods', 'Focus Groups', 'Friends', 'Fright', 'Frontotemporal Dementia', 'Future', 'GTP-Binding Protein alpha Subunits, Gs', 'Genes', 'Health', 'Health Benefit', 'Home environment', 'Impairment', 'Internet', 'Internet of Things', 'Intervention', 'Laboratories', 'Lead', 'Life', 'Link', 'Loneliness', 'Mental Health', 'Monitor', 'Neurodegenerative Disorders', 'Outcome', 'Pathway interactions', 'Pattern', 'Personal Satisfaction', 'Phase', 'Population', 'Production', 'Progressive Disease', 'Psychophysiology', 'Public Health', 'Quality of Care', 'Randomized', 'Redwood', 'Research', 'Research Personnel', 'Risk', 'Safety', 'Self-Help Devices', 'Small Business Innovation Research Grant', 'Social Behavior', 'Social Network', 'Social isolation', 'Symptoms', 'Technology', 'Testing', 'Time', 'Universities', 'Voice', 'adverse outcome', 'base', 'caregiver interventions', 'community setting', 'dementia caregiving', 'design', 'effectiveness trial', 'efficacy trial', 'emotional behavior', 'flexibility', 'hazard', 'improved', 'industry partner', 'innovation', 'learning algorithm', 'loved ones', 'mortality', 'physical conditioning', 'prevent', 'psychosocial', 'response', 'sensor', 'service providers', 'social media', 'software systems', 'theories', 'user-friendly']",NIA,PEOPLE POWER COMPANY,R44,2019,1705298,-0.01779854857399466
"The nGoggle: A portable brain-based device for assessment of visual function deficits PROJECT SUMMARY Assessment of loss of visual function outside the foveal area is an essential component of the management of numerous conditions, including glaucoma, retinal and neurological disorders. Despite the significant progress achieved with the development of standard automated perimetry (SAP) many decades ago, assessment of visual field loss with SAP still has significant drawbacks. SAP testing is limited by subjectivity of patient responses and high test-retest variability, frequently requiring many tests for effective detection of change over time. Moreover, as these tests are generally conducted in clinic-based settings, limited patient availability and health care resources often result in an insufficient number of tests acquired over time, with delayed diagnosis and detection of disease progression. The requirement for highly trained technicians, cost, complexity, and lack of portability of SAP also preclude its use for screening of visual field loss in underserved populations. To address shortcomings of current methods to assess visual function, we have developed the nGoggle, a wearable device that uses a head-mounted display (HMD) integrated with wireless electroencephalography (EEG), capable of objectively assessing visual field deficits using multifocal steady-state visual-evoked potentials (mfSSVEP). As part of the funded NEI SBIR Phase I, we developed the nGoggle prototype using a modified smartphone-based HMD display and non-disposable electrodes. In our Phase I studies, we conducted benchmarking tests on signal quality of EEG acquisition, developed methods for EEG data extraction and analysis, and conducted a pilot study demonstrating the ability of the device to detect visual field loss in glaucoma, a progressive neuropathy that results in characteristic damage to the optic nerve and resulting visual field defects. We also identified limitations of current existing displays and electrodes, as well as potential avenues for enhancing test reliability and improving user interface. Based on the encouraging results from Phase I and a clear delineation of the steps needed to bring the device into its final commercial product form, we now propose a series of Phase II studies. We hypothesize that optimization of nGoggle's accuracy and repeatability in detecting visual function loss can be achieved through the development of a customized head-mounted display with front-view eye/pupil tracking cameras and disposable no-prep electrodes, as well as enhancement of the visual stimulation protocol and data analytics. The specific aims of this proposal are: 1) To develop a customized head-mounted display and enhanced no-prep electrodes for improving nGoggle's ability to acquire users' mfSSVEP with high signal-to- noise ratios (SNR) in response to visual stimulation; 2) To optimize and validate mfSSVEP stimuli design and data analytics to enhance the accuracy and repeatability of assessing visual function loss with the nGoggle. 3) Complete pivotal clinical studies to support FDA approval. PROJECT NARRATIVE NGoggle Inc. has developed the nGoggle, a wearable device that uses a head-mounted display integrated with wireless electroencephalography, capable of objectively assessing visual field deficits using multifocal steady- state visual-evoked potentials. NGoggle Inc is now proposing to optimize nGoggle's accuracy and repeatability in detecting visual function loss with the use of a customized display, adherent no-prep electrodes, optimized visual stimuli and data analytics. It will also complete pivotal clinical studies to support FDA approval.",The nGoggle: A portable brain-based device for assessment of visual function deficits,9772484,R42EY027651,"['Address', 'Area', 'Base of the Brain', 'Benchmarking', 'Blindness', 'Brain', 'Cellular Phone', 'Characteristics', 'Client satisfaction', 'Clinic', 'Clinical Research', 'Custom', 'Data', 'Data Analytics', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Disease', 'Disease Progression', 'Elastomers', 'Electrodes', 'Electroencephalography', 'Electrooculogram', 'Exhibits', 'Eye', 'Funding', 'Glaucoma', 'Head', 'Healthcare', 'Methods', 'Neuropathy', 'Noise', 'Optic Nerve', 'Optical Coherence Tomography', 'Optics', 'Patients', 'Perimetry', 'Phase', 'Photic Stimulation', 'Pilot Projects', 'Protocols documentation', 'Pupil', 'Resources', 'Retinal Diseases', 'Scotoma', 'Series', 'Signal Transduction', 'Skin', 'Small Business Innovation Research Grant', 'Source', 'Stimulus', 'Testing', 'Time', 'Training', 'Underserved Population', 'Vision', 'Visual Fields', 'Visual evoked cortical potential', 'Wireless Technology', 'base', 'cost', 'design', 'field study', 'improved', 'loss of function', 'machine learning algorithm', 'nervous system disorder', 'patient response', 'phase 1 study', 'phase 2 study', 'portability', 'prototype', 'real world application', 'relating to nervous system', 'response', 'sample fixation', 'screening', 'virtual reality', 'visual stimulus', 'wearable device']",NEI,"NGOGGLE, INC.",R42,2019,648557,-0.0015229135514489346
"Lightweight optoimpedance sensors enabling early detection of the physiological response to injury and illness Abstract Early detection of ongoing hemorrhage (OH) before onset of shock is a universally acknowledged great unmet need, and particularly important after trauma. Delays in the detection of OH are associated with a “failure to rescue” and a dramatic deterioration in prognosis once the onset of clinically frank shock has occurred. While uniplex noninvasive technologies have failed to detect or diagnose complex disease states, we have demonstrated the superiority of multiplex approaches in silico. The goal of this STTR project is to develop a commercially viable optoimpedance sensor-based system that combines state-of-the-art noninvasive sensing technologies and advanced multivariable statistical algorithms. Phase I will involve three Aims: 1) D​esign, Fabricate and Test Opto-Impedance oPiic sensors, 2) Develop of Mobile App, Data and ML Pipeline on Secure Cloud, and 3) Evaluate oPiics on an Unanesthetized Upright Porcine Hemorrhage Model. By derisking the hardware challenges, we will be well-positioned for a Phase II application to optimize oPiic design and manufacturing, fold-in predictive algorithms under current development with DOD support, and validate with a clinical trial in critical care setting. Project Narrative We have demonstrated that a multiplex approach is superior to predicting shock compared to single clinical devices alone. During a mass-casualty event, a predictive tool would need to be deployed widely, since only a fraction of individuals will have ongoing hemorrhage that will progress to decompensated shock. Optical and bioimpedance signals are critical indicators of muscle hemodynamics and electrolyte balance likely to be modified in the period leading up to shock. No commercial device exists that provides continuous, low-power, low-cost monitoring of these signals with characteristics suitable for integration with the multiplexing approach. This STTR application seeks Phase I funding to commercialize an opto-impedance sensor, called the ‘oPiic’, that will address this unmet need.",Lightweight optoimpedance sensors enabling early detection of the physiological response to injury and illness,9909081,R41EB029284,"['Address', 'Adhesives', 'Algorithms', 'Animals', 'Back', 'Benchmarking', 'Cardiovascular Physiology', 'Caring', 'Characteristics', 'Classification', 'Clinical', 'Clinical Trials', 'Complex', 'Computer Simulation', 'Computer software', 'Conscious', 'Critical Care', 'Data', 'Data Set', 'Detection', 'Deterioration', 'Development', 'Devices', 'Diagnosis', 'Dimensions', 'Disasters', 'Disease', 'Early Diagnosis', 'Electrolyte Balance', 'Event', 'Faculty', 'Failure', 'Family suidae', 'Fiber Optics', 'Funding', 'General anesthetic drugs', 'Goals', 'Hemorrhage', 'Human', 'Hydration status', 'Individual', 'Injury', 'Intensive Care', 'Learning', 'Life', 'Location', 'Measurement', 'Medical', 'Medical Device', 'Metabolism', 'Military Personnel', 'Modality', 'Modeling', 'Monitor', 'Muscle', 'Noise', 'Operative Surgical Procedures', 'Optics', 'Outcome', 'Patients', 'Phase', 'Physiological', 'Positioning Attribute', 'Postoperative Care', 'Protocols documentation', 'Resolution', 'Risk', 'Sampling', 'Secure', 'Shock', 'Signal Transduction', 'Small Business Technology Transfer Research', 'Specialist', 'Spectrum Analysis', 'Statistical Algorithm', 'Stream', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Trauma', 'Triage', 'Trust', 'Validation', 'Vertebrates', 'analog', 'base', 'clinical development', 'clinically relevant', 'college', 'cost', 'deep learning', 'design', 'effective intervention', 'electric impedance', 'electrical property', 'hemodynamics', 'instrument', 'light weight', 'mass casualty', 'member', 'miniaturize', 'mobile application', 'optical sensor', 'optimal treatments', 'outcome forecast', 'performance tests', 'portability', 'prediction algorithm', 'predictive tools', 'preservation', 'programs', 'response', 'response to injury', 'sensor', 'sensor technology', 'tissue oxygenation', 'wearable device']",NIBIB,"MULTIVARIATE SYSTEMS, INC.",R41,2019,155282,0.010734853341765386
"Naturalistic Data Collection In The SmartPlayroom PROJECT SUMMARY The aims of this proposal are to fully develop and validate the SmartPlayroom as a powerful automated data collection and analysis tool in developmental research. This room looks like any playroom in a home or school but is designed to naturalistically collect data in real time and simultaneously on all aspects of children's behavior. Behaviors include movement kinematics, language, eye movements, and social interaction while a child performs naturalistic tasks, plays and explores without instruction, walks or crawls, and interacts with a caregiver. The space is equipped with mobile eye tracking, wireless physiological heart rate and galvanic skin response sensors, audio and video recording, and depth sensor technologies. Funding is requested to demonstrate the scientific advantage of naturalistic measurement using an example from visual attention research (Aim 1), and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use in 4-9 year-old children (Aim 2). By combining fine-grained sensor data with high-throughput automated computer vision and machine learning tools, we will be able to automate quantitative data collection and analysis in the SmartPlayroom for use in addressing myriad developmental questions. The SmartPlayroom approach overcomes completely the limitations of task-based experimentation in developmental research, offering quantitative precision in the collection of ecologically valid data. It has the power to magnify both construct validity and measurement reliability in developmental research. The investigators are committed to making freely available our data, computer vision algorithms, and discoveries so that we might move the field forward quickly. NARRATIVE We focus this work on developing and validating a novel and innovative data collection space called the SmartPlayroom, designed to pair naturalistic exploration and action with the precision of computerized automated data collection and analysis. This proposal aims to demonstrate the scientific advantage of naturalistic measurement, and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use with 4-9 year-old children in the SmartPlayroom. !",Naturalistic Data Collection In The SmartPlayroom,9507909,R21MH113870,"['9 year old', 'Address', 'Adult', 'Age', 'Algorithms', 'Attention', 'Automated Annotation', 'Behavior', 'Behavior assessment', 'Behavioral', 'Benchmarking', 'Caregivers', 'Child', 'Child Behavior', 'Child Development', 'Child Rearing', 'Childhood', 'Cognitive', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Developmental Process', 'Discipline', 'Education', 'Environment', 'Event', 'Eye', 'Eye Movements', 'Face', 'Funding', 'Galvanic Skin Response', 'Goals', 'Grain', 'Heart Rate', 'Home environment', 'Human', 'Instruction', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Memory', 'Methods', 'Modernization', 'Monitor', 'Movement', 'Neurodevelopmental Disorder', 'Performance', 'Physiological', 'Play', 'Policies', 'Process', 'Research', 'Research Personnel', 'Schools', 'Social Interaction', 'Societies', 'Time', 'Time Study', 'Training', 'Video Recording', 'Vision', 'Visual attention', 'Walking', 'Wireless Technology', 'Work', 'base', 'cognitive development', 'computerized', 'cost', 'deep learning', 'design', 'eye hand coordination', 'flexibility', 'frontier', 'grasp', 'indexing', 'innovation', 'kinematics', 'novel', 'research and development', 'sample fixation', 'sensor', 'sensor technology', 'skills', 'tool']",NIMH,BROWN UNIVERSITY,R21,2018,203125,0.012989092279013527
"Smartphone phenotype collection for diagnostic screening of mild cognitive impairment Project Summary This project addresses a critical need for early detection of mild cognitive impairment (MCI) and other Alzheimer's-related dementias (ADRD). Advances in smartphone hardware, computer vision, and machine learning have enabled the possibility of producing smartphone-based cognitive testing applications able to collect electronic sensor data and transform it into highly informative phenotypes that can serve as early indicators of future disease progression. In this project, we aim to develop a revolutionary new smartphone- based cognitive testing platform, called CTX, that will enable the rapid development and deployment of smartphone-based tests that can capture raw sensor streams in a synchronized fashion, subsample and compress the combined streams, and transmit them to a cloud server for subsequent analysis and modeling. CTX will provide a high-level application development framework that will significantly reduce the time and technical knowledge required to produce a smartphone-based cognitive testing application by providing an application programming interface (API) that enables developers to simply declare what sensor data should be collected and when. The framework will handle all the details of collecting the sensor data, synchronizing it, and transmitting it to a back-end server. The API will also have a variety of other high-level features to facilitate development of cognitive test apps. To demonstrate the feasibility of our vision for CTX, in Aim 1 of this project we will develop the software framework, back-end server software and a prototype smartphone app to exercise and validate many of the platform's features. For Aim 2, we will develop three different tests for this app to test saccade (eye movement) latency, verbal recall, and wrist mobility, each collecting a different type of sensor data (video, audio, and inertial measurement). These tests were selected because their results have been been shown to be predictive of MCI. We will implement phenotype extraction pipelines that employ advanced signal processing, machine learning, and computer vision algorithms to extract the target phenotypes from the sensor data collected for these tests and demonstrate they operate with sufficient accuracy to replicate published experimental designs. Successful completion of this project will eliminate the need for expensive and cumbersome phenotype collection equipment (e.g., eye tracking stations) and create the possibility of generating data from which MCI onset can be predicted. Data collected in Phase II via these and other such tests will enable us to apply our machine learning expertise to produce models able to predict transition to MCI that are both sensitive and specific, transforming any smartphone into an MCI risk assessment tool available for at-home use by millions of people. Project Narrative This NIH Phase I project will address the critical need for early detection of Alzheimer's Disease (AD) and Alzheimer's-related dementias (ADRD) by developing a revolutionary new smartphone-based cognitive testing platform that will provide individuals with an ongoing status of their cognitive health. Doctors who are given access to the results of these tests will be able to monitor patients more closely and provide more timely diagnoses. By studying test results from many people, researchers may someday be able to identify patterns that can distinguish mild cognitive impairment from normative age-related cognitive decline.",Smartphone phenotype collection for diagnostic screening of mild cognitive impairment,9679400,R43AG062072,"['Achievement', 'Address', 'Adult', 'Age', 'Age-associated memory impairment', 'Algorithms', 'Alzheimer disease detection', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease model', 'Apple', 'Assessment tool', 'Back', 'Big Data', 'Cellular Phone', 'Cognitive', 'Collection', 'Computer Vision Systems', 'Computer software', 'Cyclophosphamide', 'Data', 'Data Set', 'Dementia', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Diagnostic tests', 'Disease Progression', 'Early Diagnosis', 'Elderly', 'Emotional', 'Equipment', 'Exercise', 'Exhibits', 'Experimental Designs', 'Eye', 'Eye Movements', 'Face', 'Facial Expression', 'Forearm', 'Frequencies', 'Future', 'Genetic Risk', 'Genotype', 'Goals', 'Health', 'Healthcare', 'Home environment', 'Image', 'Individual', 'Knowledge', 'Lead', 'Machine Learning', 'Measurement', 'Memory impairment', 'Methods', 'Modeling', 'Monitor', 'Patient Monitoring', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Publishing', 'Reporting', 'Research Infrastructure', 'Research Personnel', 'Risk Assessment', 'Rotation', 'Saccades', 'Scanning', 'Secure', 'Sensitivity and Specificity', 'Small Business Innovation Research Grant', 'Software Framework', 'Software Tools', 'Stream', 'Tablets', 'Telephone', 'Test Result', 'Testing', 'Time', 'United States National Institutes of Health', 'Vision', 'Visuospatial', 'Work', 'Wrist', 'Yang', 'age related', 'age related cognitive change', 'application programming interface', 'base', 'cloud platform', 'cognitive development', 'cognitive task', 'cognitive testing', 'cohort', 'cost', 'crowdsourcing', 'data modeling', 'diagnostic screening', 'interest', 'markov model', 'mild cognitive impairment', 'predictive modeling', 'prototype', 'response', 'screening', 'sensor', 'signal processing', 'smartphone Application', 'software development', 'success']",NIA,"PARABON NANOLABS, INC.",R43,2018,394297,-0.02390882953532517
"User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control ABSTRACT Approximately 41,000 individuals live with upper-limb loss (loss of at least one hand) in the US. Fortunately, prosthetic devices have advanced considerably in the past decades with the development of dexterous, anthropomorphic hands. However, potentially the most promising used control strategy, myoelectric control, lacks a correspondingly high-level of performance and hence the use of dexterous hands remains highly limited. The need for a complete overhaul in upper limb prosthesis control is well highlighted by the abandonment rates of myoelectric devices, which can reach up to 40% in the case of trans-humeral amputees. The area of research that has received the most focus over the past decade has been “pattern recognition,” which is a signal processing based control method that uses multi-channel surface electromyography as the control input. While pattern recognition provides intuitive operation of multiple prosthetic degrees of freedom, it lacks robustness and requires frequent, often daily calibration. Thus, it has not yet achieved the desired clinical acceptance. Our team proposes clinical translation of a novel highly adaptive upper limb prosthesis control system that incorporates two major advances: 1) machine learning (robust classification by implementing a non-boundary based algorithm), and 2) training by retrospectively incorporating user data from activities of daily living (ADL). The proposed system will enable machine intelligence with user input for prosthesis control. Our work is organized as follows: Phase I: (a) First, we will implement a fundamentally new machine intelligence technique, Extreme Learning Machine with Adaptive Sparse Representation Classification (EASRC), that is more resilient to untrained noisy conditions that users may encounter in the real-world and requires less data than traditional myoelectric signal processing. (b) In parallel, we will implement an adaptive learning algorithm, Nessa, which allows users to relabel misclassified data recorded during use and then update the EASRC classifier to adapt to any major extrinsic or intrinsic changes in the signals. Taken together, EASRC and Nessa comprise the Retrospectively Supervised Classification Updating (RESCU) system. Once, the RESCU implementation is complete, we will optimize the system through a joint effort with Johns Hopkins University, and complete an iterative benchtop RESCU evaluation with a focus group of 3 amputee subjects and their prosthetists. Phase II: Verification and validation of RESCU will be completed, culminating in third-party validation testing and certification. Finally, we will complete a clinical assessment including self-reporting subjective measures, and real-world usage metrics in a long-term clinical study. PROJECT NARRATIVE In this project, we aim to empower the user by bringing them into the control loop of their prosthesis and improve the stability of their control strategy over time. Specifically, we implement to a robust classifier, an adaptive learning algorithm, and a smartwatch interface, which allows the user to teach their device when it misunderstands the commands that the user is sending to control the prosthesis. This will result in improved control without cumbersome or time-consuming effort on the part of the user and, more importantly, we hope that it will give the user a greater sense of empowerment and ownership over their prosthesis.",User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control,9622537,U44NS108894,"['Activities of Daily Living', 'Adoption', 'Algorithms', 'Amputees', 'Area', 'Artificial Intelligence', 'Calibration', 'Certification', 'Classification', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Communication', 'Data', 'Development', 'Devices', 'Electromyography', 'Evaluation', 'Focus Groups', 'Freedom', 'Goals', 'Hand', 'Individual', 'Intuition', 'Joints', 'Label', 'Limb Prosthesis', 'Machine Learning', 'Measures', 'Methods', 'Outcome', 'Ownership', 'Patient Self-Report', 'Pattern Recognition', 'Performance', 'Phase', 'Prosthesis', 'Research', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Supervision', 'Surface', 'Surveys', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Update', 'Upper Extremity', 'Validation', 'Work', 'adaptive learning', 'base', 'clinical translation', 'empowerment', 'functional improvement', 'improved', 'innovation', 'myoelectric control', 'novel', 'operation', 'programs', 'prospective', 'prosthesis control', 'satisfaction', 'signal processing', 'verification and validation']",NINDS,"INFINITE BIOMEDICAL TECHNOLOGIES, LLC",U44,2018,216724,0.0008360341991336726
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9499823,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Research Infrastructure', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radiofrequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2018,416374,0.06415479102337215
"IGF::OT::IGF SBIR Topic 379: DigiBioMarC: Digital BioMarkers for Clinical Impact- Moonshot Project(Fast Track) The overall objective of this Fast-Track SBIR contract project is to develop DigiBioMarCTM (Digital BioMarkers for Clinical Impact), a scalable and flexible cloud-based platform to capture and analyze wearable, implantable, or external device data. This platform also provides an informatics tool for automated data aggregation, integration, and machine learning algorithms. It is based on the scalable user-centered Medable platform, which implements standardization and normalization of patient-generated data to drive health insights. DigiBioMarCTM will compare and combine disparate data streams to understand contextualized patient physiology in real time in order to identify disease and/or detect changes in disease/health status. It also will support cohort and clinical studies, particularly those testing digital biomarkers from wearable sensor technologies. This Fast-Track project will focus on product development with an ultimate aim of a product that improves cancer research data and clinical trials, enhances clinical care, and that can be used to engage patients in preventive health behaviors and treatment adherence. The Phase I goal is to develop a data-agnostic DigiBioMarCTM prototype for validation in Phase II. The Phase 1 Go/No-Go decision point to proceed to Phase II will be a working prototype with specified features for further development and validation in Phase II. n/a",IGF::OT::IGF SBIR Topic 379: DigiBioMarC: Digital BioMarkers for Clinical Impact- Moonshot Project(Fast Track),9788845,61201800010C,"['Algorithms', 'Biological Markers', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Cohort Studies', 'Computer software', 'Contracts', 'Data', 'Data Aggregation', 'Development', 'Devices', 'Disease', 'Documentation', 'Goals', 'Health', 'Health Status', 'Health behavior', 'Informatics', 'Machine Learning', 'Patients', 'Phase', 'Physiology', 'Preventive', 'Publishing', 'Reporting', 'Small Business Innovation Research Grant', 'Specific qualifier value', 'Standardization', 'Stream', 'System', 'Testing', 'Time', 'Validation', 'anticancer research', 'base', 'clinical care', 'cloud based', 'digital', 'flexibility', 'graphical user interface', 'improved', 'insight', 'knowledge base', 'product development', 'prototype', 'sensor', 'skills', 'tool', 'treatment adherence', 'wearable sensor technology']",NCI,"MEDABLE, INC.",N01,2018,224294,0.011594337050583893
"A Customizable Real-Time Biosensor for Continuous Monitoring of Water Contaminants Ensuring access to clean water for generations to come will involve developing novel ap- proaches to determining the safety and composition of potable water that are practical and afford- able. Arsenic, mercury, and cadmium are three of the top priorities among hazardous substances commonly found at Superfund sites, as they are linked to health problems in people exposed to them in drinking water, yet the current real-time monitoring methods for these and other contam- inants are either extremely costly or nonexistent, making it difﬁcult to monitor water quality with high spatial or temporal resolution. QBiSci is developing a biosensor that uses synthetic micro- bial sensor strains that ﬂuoresce in response to speciﬁc toxins to continuously monitor water for contamination. The platform will substantially improve upon currently available technologies for toxin detection, making monitoring more affordable, continuous, and ﬁeld-deployable. Speciﬁc Aim 1: To fully characterize three synthetic E. coli strains that speciﬁcally detect ar- senic, mercury, and cadmium in a continuous water stream. For a real-time sensor to be maxi- mally effective, it must be able to report accurate toxin concentrations in real-time. Focusing on three of the highest priority contaminants as a proof of feasibility, comprehensive data will be acquired to train a machine learning algorithm to be able classify real-world samples in real-time. Speciﬁc Aim 2: To develop and train a classiﬁcation algorithm to recognize the type and amount of each contaminant present in a continuous water stream. The ability to analyze and interpret data in real-time from a constantly ﬂuctuating water source will require an extensive classiﬁcation train- ing effort. QBiSci's existing machine learning framework will be trained and tested using many contamination induction scenarios, ranging from sudden pulses to subtly varying concentrations. Speciﬁc Aim 3: To develop a microﬂuidic cartridge system that reduces device complexity and enables sensor deployment with minimal intervention. QBiSci will develop a swappable car- tridge system using devices that are pre-loaded with biologically-stable strains and can simply be “plugged in” to the sensor platform to achieve repeatable results in a user-friendly manner. The development of a method for thermoplastic device fabrication will enable the more precise connections required for a cartridge clamping system that will require little operational expertise.  A successful outcome of this proposal will lead to a biosensor capable of real-time quantiﬁca- tion of arsenic, mercury, and cadmium in a continuous water input. A future Phase II proposal would focus on real-world performance evaluations of our sensors via deployment in areas of con- cern and comparison of our results to standard techniques as well as an expansion of the platform to detect other contaminants quantitatively and continuously. 1 Access to clean, reliable water supplies is critical to our quality of life and our economy, yet across the country over 100,000 hazardous waste sites are so heavily contaminated that the un- derlying groundwater doesn't meet drinking water standards. While there is a wide range of toxins found at these sites, arsenic, mercury, and cadmium are among the most common offend- ers, all of which have been linked to a variety of health problems ranging from cancer to dia- betes as well as behavior and neurological disorders. We are developing a customizable real-time biosensor that will enable contamination monitoring to become more affordable, continuous, and ﬁeld-deployable and will facilitate improved management decisions aimed at reducing toxin con- centrations in the environment, tracking the progression of contamination plums, and targeting investments in remediation efforts. 1",A Customizable Real-Time Biosensor for Continuous Monitoring of Water Contaminants,9467134,R43ES028993,"['Algorithms', 'Area', 'Arsenic', 'Behavior Disorders', 'Biological', 'Biosensor', 'Cadmium', 'Classification', 'Closure by clamp', 'Country', 'Data', 'Detection', 'Development', 'Devices', 'Diabetes Mellitus', 'Ensure', 'Environment', 'Escherichia coli', 'Evaluation', 'Exposure to', 'Fluorescence', 'Future', 'Generations', 'Goals', 'Hazardous Substances', 'Hazardous Waste Sites', 'Health', 'Intervention', 'Investments', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Mercury', 'Methods', 'Microfluidics', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Physiologic pulse', 'Plug-in', 'Plum', 'Quality of life', 'Reporting', 'Research', 'Research Infrastructure', 'Risk', 'Safety', 'Sampling', 'Signal Transduction', 'Site', 'Source', 'Stream', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Toxin', 'Training', 'Water', 'Water Supply', 'contaminated water', 'cost', 'drinking water', 'ground water', 'improved', 'innovation', 'microbial', 'nervous system disorder', 'novel', 'novel strategies', 'operation', 'real time monitoring', 'remediation', 'response', 'sensor', 'simulation', 'superfund site', 'temporal measurement', 'user-friendly', 'water quality']",NIEHS,"QUANTITATIVE BIOSCIENCES, INC.",R43,2018,162205,0.018447665122209397
"Virtual prototyping for retinal prosthesis patients Project Summary/Abstract Retinal dystrophies such as retinitis pigmentosa and macular degeneration induce progressive loss of photoreceptors, resulting in profound visual impairment in more than ten million people worldwide. Visual neuroprostheses (‘bionic eyes’) aim to restore functional vision by electrically stimulating remaining cells in the retina, analogous to cochlear implants. A wide variety of neuroprostheses are either in development (e.g. optogenetics, cortical) or are being implanted in patients (e.g. subretinal or epiretinal electrical). A limiting factor that affects all device types are perceptual distortions and subsequent loss of information, caused by interactions between the implant technology and the underlying neurophysiology. Understanding the causes of these distortions and finding ways to alleviate them is critically important to the success of current and future sight restoration technologies. In this proposal, human visual psychophysics, computational modeling, data-driven approaches, and virtual reality (VR) will be combined to develop and experimentally validate optimized stimulation protocols for epiretinal prostheses. This approach is analogous to virtual prototyping for airplanes and other complex systems: to use a high-quality model of both the implant electronics and the visual system in order to generate a ‘virtual patient’. Retinal electrophysiological and visual behavioral data will be used to develop and validate a computational model of the expected visual experience of patients when electrically stimulated. One way of using this model will be to generate simulations of the expected perceptual outcome of electrical stimulation across a wide variety of electrical stimulation patterns. These will be used as a training set for machine learning algorithms that will invert the input-output function of the model to find the electrical stimulation protocol that best replicates any desired perceptual experience. The model can also be used to simulate the expected perceptual experience of real patients by using sighted subjects in a VR environment – ‘VR virtual patients’. These virtual patients will be used to discover preprocessing methods (e.g., edge enhancement, retargeting, decluttering) that improve behavioral performance in VR. Although current retinal prostheses have been implanted in over 250 patients worldwide, experimentation with improved stimulation protocols remains challenging and expensive. Implementing ‘virtual patients’ in VR offers an affordable and practical alternative for high-throughput experiments to test new stimulation protocols. Stimulation protocols that result in good VR performance will be experimentally validated in real prosthesis patients in collaboration with Second Sight Medical Products Inc. and Pixium Vision, two leading device manufacturers in the field. This work has the potential to significantly improve the effectiveness of visual neuroprostheses as a treatment option for individuals suffering from blinding retinal diseases. Project Narrative Inadequate stimulation paradigms are currently one of the main factors limiting the effectiveness of visual prostheses as a treatment option for individuals suffering from blinding retinal diseases. My goal is to develop and validate novel stimulation protocols for visual prosthesis patients that minimize perceptual distortions and thereby improve behavioral performance. Developing methods for generating better stimulation protocols through a combination of behavioral testing, virtual reality, computational modeling, and machine learning, has the potential to provide a transformative improvement of this device technology.",Virtual prototyping for retinal prosthesis patients,9581681,K99EY029329,"['Affect', 'Algorithms', 'Behavioral', 'Bionics', 'Cells', 'Clinical Trials', 'Cochlear Implants', 'Collaborations', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Devices', 'Effectiveness', 'Electric Stimulation', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Eye', 'Eye Movements', 'Family', 'Financial compensation', 'Future', 'Goals', 'Head', 'Human', 'Imagery', 'Implant', 'In Vitro', 'Individual', 'Knowledge', 'Learning', 'Letters', 'Machine Learning', 'Macular degeneration', 'Manufacturer Name', 'Medical', 'Medicare', 'Methods', 'Modeling', 'Motion', 'Neurons', 'Ocular Prosthesis', 'Online Systems', 'Outcome', 'Output', 'Patients', 'Pattern', 'Perceptual distortions', 'Performance', 'Photoreceptors', 'Prosthesis', 'Prosthesis Design', 'Protocols documentation', 'Psychophysics', 'Rehabilitation therapy', 'Reporting', 'Retina', 'Retinal', 'Retinal Diseases', 'Retinal Dystrophy', 'Retinitis Pigmentosa', 'Schedule', 'Severities', 'Shapes', 'Specialist', 'Stimulus', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Psychophysics', 'Visual impairment', 'Visual system structure', 'Work', 'base', 'behavior measurement', 'behavior test', 'deep neural network', 'design', 'experience', 'experimental study', 'gaze', 'implantation', 'improved', 'neurophysiology', 'neuroprosthesis', 'novel', 'object recognition', 'optogenetics', 'predictive modeling', 'prototype', 'restoration', 'retinal prosthesis', 'simulation', 'spatiotemporal', 'success', 'virtual', 'virtual reality']",NEI,UNIVERSITY OF WASHINGTON,K99,2018,122763,0.0026468985072768338
"Evaluating and Improving Assistive Robotic Devices Continuously and in Real-time Project Summary Lower limb assistive robotic devices, such as active prosthesis, orthoses, and exoskeletons have the potential to restore function for the millions of Americans who experience mobility challenges due to injury and disability. Since individuals with mobility challenges have an increased energetic cost of transport, the benefit of such assistive devices is commonly assessed via the reduction in the metabolic work rate of the individual who is using the device. Currently, metabolic work rate can only be obtained in a laboratory environment, using breath-by-breath measurements of respiratory gas analysis. To obtain a single steady state data point of metabolic work rate, multiple minutes of data must be collected, since the signals are noisy, sparsely sampled, and dynamically delayed. In addition, the user has to wear a mask and bulky equipment, further restricting the applicability of the method on a larger scale. We propose an improved way to obtain such estimates of metabolic work rate in real-time. Aim 1 will determine salient signal features and characterize the dynamics of sensing metabolic work rate from a variety of physiological sensor signals. Aim 2 will use advanced sensor fusion and machine learning techniques to accurately predict instantaneous energy cost in real-time from multiple physiological signals without relying on a metabolic mask. Aim 3 will use the obtained real-time estimates to optimize push-off timing for an active robotic prosthesis. The resulting methods will enable an automated and continuous evaluation of assistive robotic devices that can be realized outside the laboratory and with simple wearable sensors. This automated evaluation will enable devices, such as active prostheses, orthoses, or exoskeletons, that can self-monitor their performance, optimize their own behavior, and continuously adapt to changing circumstances. This will open up a radically new way of human-robot- interaction for assistive devices. It will greatly increase their clinical viability and enable novel advanced controllers and algorithms that can improve device performance on a subject specific basis. Project Narrative A common way of evaluating assistive robotic devices, such as active prostheses or exoskeletons, is by measuring the reduction in effort that they bring to an individual walking in them. The proposed project will develop ways to perform this evaluation automatically and in real-time by the device itself, which will be used in the future to develop prostheses and exoskeletons that automatically adapt themselves to their users. This project supports the NIH's stated mission of reducing disability by improving patient outcomes with new prosthetic and orthotic devices.",Evaluating and Improving Assistive Robotic Devices Continuously and in Real-time,9529742,R03HD092639,"['Algorithms', 'American', 'Amputation', 'Amputees', 'Ankle', 'Behavior', 'Biological Neural Networks', 'Clinical', 'Data', 'Devices', 'Electromyography', 'Energy Metabolism', 'Environment', 'Equipment', 'Evaluation', 'Future', 'Goals', 'Gray unit of radiation dose', 'Heart Rate', 'Indirect Calorimetry', 'Individual', 'Injury', 'Laboratories', 'Linear Regressions', 'Lower Extremity', 'Machine Learning', 'Masks', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Mission', 'Modality', 'Modeling', 'Monitor', 'Noise', 'Orthotic Devices', 'Patient-Focused Outcomes', 'Performance', 'Persons', 'Physiological', 'Population', 'Prosthesis', 'Reference Values', 'Robotics', 'Sampling', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Techniques', 'Time', 'Training', 'United States National Institutes of Health', 'Walking', 'Work', 'base', 'cost', 'disability', 'exoskeleton', 'experience', 'functional restoration', 'human-robot interaction', 'improved', 'light weight', 'novel', 'respiratory gas', 'robotic device', 'sensor', 'wearable device']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R03,2018,77959,0.0322710837369709
"Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor Summary The principal goal of this proposal is to increase the accuracy and precision of a low-cost autorefraction device called the QuickSee, in order to improve access to refractive eye care for underserved populations. Poor vision due to a lack of eyeglasses is highly prevalent in low-resource settings throughout the world and significantly reduces quality of life, education, and productivity. The existing QuickSee only extracts the lower- order aberration information contained within a wavefront profile of the eye, to roughly estimate an eyeglass prescription. This proposal will further improve the accuracy of the QuickSee device by exploiting both the lower- and higher-order aberrations contained within the complete wavefront. To realize this goal, we will enroll 300 subjects (600 eyes) in Baltimore, MD, and will obtain subjective refraction and visual acuity (VA) measurements and will use machine learning on this large dataset of wavefront profiles to optimize the wavefront-to-refraction algorithm of the QuickSee device. The main output of this project will be a robust and improved-accuracy next-generation QuickSee device that will increase efficiency of and decrease the training requirements of eye care professionals, and potentially dispense refractive correction that provides similar or better VA than correction from an eye care professional. Successful completion of this work will be an important step towards dramatically improving eyeglass accessibility for health disparity populations in the USA and internationally in low-resource settings. Upon completion of this proposal, we will apply for a Phase II award proposing to work with Wilmer Eye Institute research faculty to assess widespread deployment of the next-generation QuickSee with minimally-trained personnel in order to accurately and reliably provide thousands of pairs of low-cost corrective eyeglasses to underserved communities. Project Narrative This project proposal seeks to develop a novel technology that will disruptively increase the accessibility of refractive eye care for health disparity populations in low-resource settings. Specifically, sophisticated algorithms will be developed that improve the accuracy of the QuickSee device so that it can improve the efficiency of and reduce the training barriers for eye care professionals, and potentially provide refractive correction without the need for refinement by a trained eye care professional. Our goal is to develop a low- cost, easy-to-use, scalable solution to increase accessibility to vision correction globally.","Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor",9711194,R43EB024299,"['Algorithms', 'Award', 'Baltimore', 'Brazil', 'Businesses', 'Caliber', 'Calibration', 'Caring', 'Communities', 'Country', 'Data', 'Data Set', 'Developed Countries', 'Developing Countries', 'Development', 'Devices', 'Diagnostic', 'Education', 'Educational Status', 'Enrollment', 'Eye', 'Eyeglasses', 'Feedback', 'Geometry', 'Goals', 'Gold', 'Guatemala', 'Hospitals', 'Human Resources', 'Impairment', 'Improve Access', 'Income', 'India', 'Institutes', 'International', 'Machine Learning', 'Mali', 'Measurement', 'Measures', 'Modeling', 'Noise', 'Ophthalmic examination and evaluation', 'Optometrist', 'Output', 'Patient Schedules', 'Patients', 'Phase', 'Population', 'Prevalence', 'Procedures', 'Productivity', 'Pupil', 'Quality of life', 'Refractive Errors', 'Research Institute', 'Resources', 'Spottings', 'Testing', 'Time', 'Training', 'Underserved Population', 'Universities', 'Validation', 'Vision', 'Visual Acuity', 'Work', 'base', 'cost', 'faculty research', 'health care disparity', 'health disparity', 'improved', 'lens', 'new technology', 'next generation', 'novel strategies', 'success', 'vector']",NIBIB,"PLENOPTIKA, INC.",R43,2018,99914,0.008639935472644267
"A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device ABSTRACT This project will create the first objective measurement tool, the VisBox, for the vision subtype of concussion (VSC). This will enable physicians to identify VSC without an eye-care professional, for referral to a vision specialist for personalized vision therapy recommendations. The persistence of concussion symptoms beyond several weeks is often a life-altering situation for affected individuals, and children are particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties. A lack of accessible, objective vision diagnostics are critical barriers to identification of VSC and referral for treatment. The VisBox will be a software product that is used with the OcuTracker, Oculogica’s proprietary eye-tracking hardware platform. The VisBox will input eye movement measurements from the OcuTracker, calculate metrics that correspond to aspects of cranial nerve function affected during a concussion, and use those metrics to calculate a score to predict VSC using an algorithm developed with guided machine learning in the course of this study. The VisBox will be used by non- vision specialists to objectively measure three vision disorders related to concussion: convergence insufficiency (CI), accommodative insufficiency (AI), and saccadic dysfunction (SD) in under 4 minutes, during the clinical visit where the concussion is diagnosed. The long-term goal is to develop an objective assessment of vision characteristics, that will enable physicians that are non-specialists in vision to 1) screen for concussion-related vision disorders; 2) identify VSC; 3) make decisions about the necessity of a referral for a comprehensive vision examination; 4) monitor the effectiveness of vision treatment. Phase I Hypothesis. VisBox can produce an output score that correlates with the presence or absence of TBI- related vision disorder, i.e., VSC, by leveraging the OcuTracker visual stimulus and eye tracking system. Specific Aim I. Generate OcuTracker eye tracking data and the diagnosis of TBI-related vision disorder in 250 pediatric concussion patients. Specific Aim II. Develop and validate VisBox algorithm for assessing CI, AI, and SD using OcuTracker data. Plans for Phase II. The VisBox score will be used to predict responsiveness to vision therapy in a prospective randomized clinical study. Phase II will be a multi-armed study comparing vision therapy with placebo therapy in concussion patients and assessing whether the VisBox software can predict which patients are responsive to vision therapy. Commercial Opportunity. VisBox customers are non-eye care specialists including neurologists, pediatricians, emergency room physicians, sports medicine physicians, and concussion specialists. The total addressable market is $400M, assuming 4M annual scans at $100/scan needed for concussions in the US. PUBLIC HEALTH RELEVANCE STATEMENT At least 4 million concussions occur in the US each year, and up to 30% of these injuries persist beyond 4 weeks in a condition known as persistent post-concussion symptoms, which is often a life-altering situation for affected individuals, with children particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties – with often serious consequences. The lack of an accessible, objective vision diagnostics presents a critical barrier to identification of the vision disorder concussion sub-type (VSC) and referral for treatment. The proposed technology will be the first objective tool that can be used by non-vision-specialists to identify concussion-related vision symptoms that is accessible to a broad range of facilities and will enable non-specialist physicians the ability to refer patients to concussion specialists to improve outcomes, decrease the time it takes patients to return to work or play, and reduce healthcare costs associated with this debilitating condition.",A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device,9465330,R41NS103698,"['Address', 'Affect', 'Algorithms', 'Anxiety', 'Area Under Curve', 'Brain Concussion', 'Caring', 'Characteristics', 'Child', 'Childhood', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Comorbidity', 'Computer software', 'Convergence Insufficiency', 'Cranial Nerves', 'Data', 'Data Analyses', 'Decision Making', 'Devices', 'Diagnosis', 'Diagnostic', 'Economics', 'Effectiveness', 'Emergency Department Physician', 'Evaluation', 'Eye', 'Eye Movements', 'Family', 'Fatigue', 'Fees', 'Functional disorder', 'Goals', 'Health Care Costs', 'Individual', 'Injury', 'Intervention', 'Learning', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Mental Depression', 'Monitor', 'Neurologist', 'Neurosurgeon', 'Optometrist', 'Output', 'Patients', 'Performance', 'Phase', 'Physicians', 'Placebos', 'Play', 'Post-Concussion Syndrome', 'Prevalence', 'Primary Care Physician', 'Randomized', 'Recommendation', 'Research Personnel', 'Resolution', 'Rest', 'Risk', 'Sampling', 'Scanning', 'Small Business Technology Transfer Research', 'Software Tools', 'Specialist', 'Sports Medicine', 'Symptoms', 'System', 'TBI Patients', 'Technology', 'Therapeutic Intervention', 'Time', 'Traumatic Brain Injury recovery', 'Treatment Efficacy', 'Vision', 'Vision Disorders', 'Visit', 'Work', 'associated symptom', 'chronic pain', 'commercial application', 'concussive symptom', 'disabling symptom', 'disorder subtype', 'economic impact', 'handheld equipment', 'improved outcome', 'lens', 'novel', 'patient response', 'pediatrician', 'population based', 'prospective', 'public health relevance', 'recruit', 'skills', 'software development', 'success', 'therapy outcome', 'tool', 'tv watching', 'visual stimulus']",NINDS,"OCULOGICA, INC.",R41,2018,503162,0.018049415882760163
"A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device ABSTRACT This project will create the first objective measurement tool, the VisBox, for the vision subtype of concussion (VSC). This will enable physicians to identify VSC without an eye-care professional, for referral to a vision specialist for personalized vision therapy recommendations. The persistence of concussion symptoms beyond several weeks is often a life-altering situation for affected individuals, and children are particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties. A lack of accessible, objective vision diagnostics are critical barriers to identification of VSC and referral for treatment. The VisBox will be a software product that is used with the OcuTracker, Oculogica’s proprietary eye-tracking hardware platform. The VisBox will input eye movement measurements from the OcuTracker, calculate metrics that correspond to aspects of cranial nerve function affected during a concussion, and use those metrics to calculate a score to predict VSC using an algorithm developed with guided machine learning in the course of this study. The VisBox will be used by non- vision specialists to objectively measure three vision disorders related to concussion: convergence insufficiency (CI), accommodative insufficiency (AI), and saccadic dysfunction (SD) in under 4 minutes, during the clinical visit where the concussion is diagnosed. The long-term goal is to develop an objective assessment of vision characteristics, that will enable physicians that are non-specialists in vision to 1) screen for concussion-related vision disorders; 2) identify VSC; 3) make decisions about the necessity of a referral for a comprehensive vision examination; 4) monitor the effectiveness of vision treatment. Phase I Hypothesis. VisBox can produce an output score that correlates with the presence or absence of TBI- related vision disorder, i.e., VSC, by leveraging the OcuTracker visual stimulus and eye tracking system. Specific Aim I. Generate OcuTracker eye tracking data and the diagnosis of TBI-related vision disorder in 250 pediatric concussion patients. Specific Aim II. Develop and validate VisBox algorithm for assessing CI, AI, and SD using OcuTracker data. Plans for Phase II. The VisBox score will be used to predict responsiveness to vision therapy in a prospective randomized clinical study. Phase II will be a multi-armed study comparing vision therapy with placebo therapy in concussion patients and assessing whether the VisBox software can predict which patients are responsive to vision therapy. Commercial Opportunity. VisBox customers are non-eye care specialists including neurologists, pediatricians, emergency room physicians, sports medicine physicians, and concussion specialists. The total addressable market is $400M, assuming 4M annual scans at $100/scan needed for concussions in the US. PUBLIC HEALTH RELEVANCE STATEMENT At least 4 million concussions occur in the US each year, and up to 30% of these injuries persist beyond 4 weeks in a condition known as persistent post-concussion symptoms, which is often a life-altering situation for affected individuals, with children particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties – with often serious consequences. The lack of an accessible, objective vision diagnostics presents a critical barrier to identification of the vision disorder concussion sub-type (VSC) and referral for treatment. The proposed technology will be the first objective tool that can be used by non-vision-specialists to identify concussion-related vision symptoms that is accessible to a broad range of facilities and will enable non-specialist physicians the ability to refer patients to concussion specialists to improve outcomes, decrease the time it takes patients to return to work or play, and reduce healthcare costs associated with this debilitating condition.",A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device,9698505,R41NS103698,"['Address', 'Affect', 'Algorithms', 'Anxiety', 'Area Under Curve', 'Brain Concussion', 'Caring', 'Characteristics', 'Child', 'Childhood', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Comorbidity', 'Computer software', 'Convergence Insufficiency', 'Cranial Nerves', 'Data', 'Data Analyses', 'Decision Making', 'Devices', 'Diagnosis', 'Diagnostic', 'Economics', 'Effectiveness', 'Emergency Department Physician', 'Evaluation', 'Eye', 'Eye Movements', 'Family', 'Fatigue', 'Fees', 'Functional disorder', 'Goals', 'Health Care Costs', 'Individual', 'Injury', 'Intervention', 'Learning', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Mental Depression', 'Monitor', 'Neurologist', 'Neurosurgeon', 'Optometrist', 'Output', 'Patients', 'Performance', 'Phase', 'Physicians', 'Placebos', 'Play', 'Post-Concussion Syndrome', 'Prevalence', 'Primary Care Physician', 'Randomized', 'Recommendation', 'Research Personnel', 'Resolution', 'Rest', 'Risk', 'Sampling', 'Scanning', 'Small Business Technology Transfer Research', 'Software Tools', 'Specialist', 'Sports Medicine', 'Symptoms', 'System', 'TBI Patients', 'Technology', 'Therapeutic Intervention', 'Time', 'Traumatic Brain Injury recovery', 'Treatment Efficacy', 'Vision', 'Vision Disorders', 'Visit', 'Work', 'associated symptom', 'chronic pain', 'commercial application', 'concussive symptom', 'disabling symptom', 'disorder subtype', 'economic impact', 'handheld equipment', 'improved outcome', 'lens', 'novel', 'patient response', 'pediatrician', 'population based', 'prospective', 'public health relevance', 'recruit', 'skills', 'software development', 'success', 'therapy outcome', 'tool', 'tv watching', 'visual stimulus']",NINDS,"OCULOGICA, INC.",R41,2018,50000,0.018049415882760163
"Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research PROJECT SUMMARY This Phase II project aims to continue development of a commercial quality, innovative cloud hosted information management system, called Climb 2.0™ that will increase laboratory efficiency and provide improved capabilities for research laboratories. Climb is designed to offer integrated laboratory process management modules that include mobile communications tools data monitoring and alert systems, and integrated access to Microsoft Azure Cloud™ Machine Learning and Stream Analytics services. Initially, Climb 2.0 will target animal model research laboratories; however, the core of the platform is designed to be adaptable to nearly any research type or related industry. Current research information management systems are primarily designed as record-keeping tools with little or no direct focus on laboratory efficiency or in enhancing value of the research data. They also do not leverage emergent mobile device technologies, social media frameworks, and data analysis and storage capabilities of cloud computing. Many laboratories still use paper as their primary recording system. Paper data logging is then followed by secondary data entry into a laboratory database. These systems are error prone, time consuming and lead to laboratory databases with significant time lags between data acquisition and data entry. Moreover, they do not recognized cumulative data relationships, which may identify important trends, and researchers often miss windows of opportunity to take action on time-sensitive events. In Phase I, RockStep Solutions demonstrated feasibility of an innovative Cloud Information Management Bundle system, Climb, which will increase efficiency and improve capabilities in animal model data management. During Phase I, a beta version of Climb was successfully developed and tested against strict performance metrics as a proof of concept. We successfully built a prototype with working interfaces that integrates real-time communication technologies with media capabilities of mobile devices. Phase II proposes four specific Aims: 1) Develop the technology infrastructure to support the secure and scalable Software as a Service (SaaS) deployment of Climb for enterprise commercial release; 2) Develop and extend the Phase-I prototype Data Monitoring and Messaging System (DMMS) into a platform ready for production use; 3) Extend Climb’s DMMS adding a Stream Analytics engine to support Internet of Things (IoT) devices and streaming media; 4) Deploy a beta release of Climb at partner research labs, test and refine the product for final commercialization. To ensure Climb is developed with functionality and tools relevant to research organizations, RockStep Solutions has established collaborations with key beta sites to test all of the major functionality developed in this proposal. IMPACT: By leveraging emergent technologies and cloud computing, Climb offers several advantages: 1) enables real-time communications using familiar tools among members of research groups; 2) reduces the risk of experimental setbacks, and 3) enables complex experiments to be conducted efficiently. PROJECT NARRATIVE PUBLIC HEALTH RELEVANCE: The NIH Invests approximately $12 billion each year in animal model research that is central to both understanding basic biological processes and for developing applications directly related to improving human health. More cost-effective husbandry and colony management techniques, equipment, and new approaches to improve laboratory animal welfare and assure efficient and appropriate research use (e.g., through cost savings and reduced animal burden) are high priorities for NIH. RockStep Solutions proposes to meet this need by integrating existing and emergent software and communication technologies to create a novel solution, Climb 2.0, for research animal data management. Climb 2.0 extracts immediate value of data in animal research laboratories and extends the database into a multimedia communication network, thus enabling science that would be impractical to conduct with traditional methods.",Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research,9741597,R44GM112206,"['Address', 'Algorithms', 'Animal Experimentation', 'Animal Model', 'Animal Welfare', 'Animals', 'Biological Process', 'Biomedical Research', 'Clinical Laboratory Information Systems', 'Cloud Computing', 'Collaborations', 'Communication', 'Communication Tools', 'Communications Media', 'Complex', 'Computer software', 'Computers', 'Consumption', 'Cost Savings', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Security', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Devices', 'Emerging Technologies', 'Ensure', 'Equipment', 'Equipment and supply inventories', 'Event', 'Feedback', 'Future', 'Goals', 'Health', 'Health system', 'Human', 'Industry', 'Information Management', 'Internet', 'Internet of Things', 'Investments', 'Laboratories', 'Laboratory Animals', 'Laboratory Research', 'Lead', 'Machine Learning', 'Management Information Systems', 'Methods', 'Modernization', 'Monitor', 'Motion', 'Multimedia', 'Notification', 'Paper', 'Performance', 'Phase', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rest', 'Risk', 'Science', 'Secure', 'Services', 'Site', 'Stream', 'System', 'Techniques', 'Technology', 'Testing', 'The Jackson Laboratory', 'Time', 'Transact', 'United States National Institutes of Health', 'analytical tool', 'base', 'cloud based', 'commercialization', 'computerized data processing', 'cost', 'cost effective', 'data acquisition', 'data management', 'design', 'encryption', 'experimental study', 'good laboratory practice', 'graphical user interface', 'handheld mobile device', 'improved', 'innovation', 'laptop', 'member', 'novel', 'novel strategies', 'predictive modeling', 'programs', 'prototype', 'public health relevance', 'service learning', 'social media', 'software as a service', 'success', 'time interval', 'tool', 'trend', 'usability']",NIGMS,"ROCKSTEP SOLUTIONS, INC.",R44,2018,99999,-0.00048697307623054366
"Advancing a novel portable detection method for cannabis intoxication Intoxication from marijuana (MJ) impairs psychomotor performance and at least doubles the risk of motor vehicle accidents. The ongoing wave of legalization of MJ has brought increasing prevalence of driving while intoxicated with MJ. However, there is no quantitative biologic test that can accurately determine whether an individual is acutely impaired from MJ intoxication. Assays of the primary intoxicating substance in MJ, THC, in body fluids has a high false negative rate as THC is cleared from blood within 15 minutes, long before impairment is resolved. And assays of THC metabolites yield a high false positive rate because clearance of these metabolites can take weeks. Thus there is now no nor is there likely to ever be a test of blood, breath or body fluids that can accurately detect MJ intoxication. In response to this significant knowledge gap, this project aims to develop an accurate, portable method for detection of impairment due to MJ intoxication using functional near-infrared spectroscopy (fNIRS). fNIRS is a non-invasive, safe brain imaging technique that capitalizes on differences in the light absorption spectra of deoxygenated and oxygenated hemoglobin (Hb), that allows the measurement of relative changes in Hb concentration that reflect brain activity. fNIRS can be performed in natural environments at low cost, and thus can be used in real-world settings. In Phase I, we will develop an algorithm for individual-level detection of impairment from THC using fNIRS measurements. To do so, we will assess the effect of oral THC (or placebo) on fNIRS measurements, self-reported intoxication, and impairment as defined by the gold standard field sobriety test conducted by a Drug Recognition Expert (DRE) in 40 healthy MJ users. fNIRS assessments will examine (1) the effect of THC exposure on resting state and task-based activation in the prefrontal cortex, (2) the extent to which impairment in psychomotor functioning with THC administration correlates with THC-induced change in hemodynamic responses detected with fNIRS, and (3) the sensitivity and specificity and area under the ROC curve of fNIRS measurements and field sobriety test determinants of impairment. Milestone: Should machine learning applications to the data generate an algorithm that predicts impairment with >80% accuracy compared with a gold standard field sobriety test, we will proceed to Phase II. In Phase II, we will conduct fNIRS testing in 150 individuals under THC/placebo as in Phase I and in 50 individuals in a THC plus alcohol/placebo condition in order to further refine the algorithm for MJ impairment detection such that fNIRS detection concurs with field sobriety testing with >90% specificity. It is anticipated that this level of specificity could be used in legal definitions of impairment. This will warrant commercialization, which will be followed by prototype development and field testing. An accurate, quantitative, biological test that is user-friendly and enables law enforcement to detect impairment from MJ has the potential to dramatically change practice of law enforcement across the country and the world and thus has enormous commercial potential, as outlined in the Commercialization Plan and in accompanying letters of support. The goal of this project is to develop, test, and refine a method to accurately and reliably detect marijuana (MJ) impairment using a portable, user-friendly, non-invasive, brain-based modality. MJ doubles the chance of motor vehicle accidents, yet, there now exists no valid, biologically based method to detect whether an individual is acutely impaired from MJ. The development of a reliable, quantitative biological marker that enables law enforcement officers to screen individuals whom they suspect are impaired from MJ will have highly significant public health importance and enormous commercial potential.",Advancing a novel portable detection method for cannabis intoxication,9541180,R42DA043977,"['Acute', 'Adult', 'Age', 'Alcohols', 'Algorithms', 'Area', 'Base of the Brain', 'Biochemical', 'Biological', 'Biological Assay', 'Biological Markers', 'Biological Testing', 'Blood', 'Blood Circulation', 'Blood Tests', 'Body Fluids', 'Brain', 'Brain imaging', 'Cannabis', 'Collaborations', 'Comorbidity', 'Country', 'Cross-Over Trials', 'Data', 'Detection', 'Development', 'Devices', 'Dose', 'Double-Blind Method', 'Driving While Intoxicated', 'Drug Kinetics', 'Ensure', 'Environment', 'Equipment', 'Evaluation', 'Formulation', 'Future', 'Goals', 'Gold', 'Hemoglobin', 'Hour', 'Human Resources', 'Imaging Techniques', 'Impairment', 'Individual', 'Intoxication', 'Knowledge', 'Law Enforcement', 'Law Enforcement Officers', 'Legal', 'Letters', 'Licensing', 'Light', 'Machine Learning', 'Marijuana', 'Measurement', 'Methods', 'Modality', 'Near-Infrared Spectroscopy', 'Oral', 'Patient Self-Report', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Placebos', 'Population', 'Prefrontal Cortex', 'Prevalence', 'Property', 'Psychomotor Impairments', 'Psychomotor Performance', 'Public Health', 'ROC Curve', 'Randomized', 'Readiness', 'Rest', 'Risk', 'Sensitivity and Specificity', 'Source', 'Specificity', 'System', 'THC exposure', 'Testing', 'Tetrahydrocannabinol', 'United States', 'Urine', 'Vendor', 'absorption', 'alcohol exposure', 'base', 'behavior test', 'commercialization', 'cost', 'density', 'detector', 'driving under influence', 'drug testing', 'field sobriety tests', 'field study', 'functional disability', 'hemodynamics', 'interest', 'marijuana legalization', 'marijuana use', 'marijuana user', 'novel', 'novel strategies', 'portability', 'prediction algorithm', 'prototype', 'response', 'spectroscopic imaging', 'tool', 'user-friendly', 'vehicular accident']",NIDA,"HIGHLIGHTI, INC",R42,2018,731789,0.008295505591167275
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions Project Summary/Abstract People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. Project Narrative People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9644103,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Personal Satisfaction', 'Persons', 'Phase', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'manufacturability', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2018,519349,0.047117288349963204
"Real-time prediction of marijuana use & effects of use on cognition in the natural environment ABSTRACT Some young adult marijuana (MJ) users report adverse effects of MJ use on cognition that impact daily functioning, with negative consequences such as injury and fatality due to driving while under the influence of MJ. Research on the effects of MJ use on cognition, however, has produced mixed findings. MJ effects on cognition may depend on factors such as history and current severity of marijuana use, time since last MJ use (including possible MJ withdrawal effects), and gender. This R21 aims to address limitations of existing research by (1) starting to develop an algorithm to predict MJ use using smartphone data in regular/heavy MJ users based on “routine” or “habitual use”, and (2) examining effects of MJ use on cognition using smartphone- based cognitive testing in the natural environment. Development of an algorithm to predict MJ use would facilitate systematic assessment of MJ effects on cognitive functioning through more efficient scheduling of smartphone cognitive testing among regular/heavy MJ users in relation to daily routines. Cognitive testing by smartphone in the natural environment is an innovative method that has shown validity, and permits sampling of cognitive functioning within and across days in relation to MJ use. This project will enroll non-treatment seeking young adult (ages 18-25) MJ users from the community, representing “low”, “regular”, and “heavy” MJ use, with 50% female at each level of use. Participants will complete a baseline lab assessment, 30-day data collection using smartphone and wearable devices (e.g., wristband), and a debriefing interview. Piloting will optimize the protocol and methods for compliance. Smartphones will collect continuously sensed data (e.g., geolocation) for input to an algorithm to predict MJ use in regular/heavy MJ users. This R21 will identify which types of data, available through smartphone, provide optimal detection of routines in MJ use among regular/heavy users. Smartphone cognitive testing will be administered at various times during acute MJ intoxication and various naturalistically occurring lengths of MJ abstinence to examine effects of MJ use on selected aspects of cognitive functioning in daily life. Development of an algorithm to predict MJ use in regular/heavy MJ users based on smartphone data could, for example, facilitate real-time assessment of MJ effects on cognition through improved sampling of cognition in relation to acute and non-acute effects of MJ use. This R21 will provide the foundation for a research program that aims to examine MJ effects on cognitive functioning in vivo, and could support the development of just-in-time intervention to reduce MJ use. This R21 aligns with NIDA's strategic goal of determining consequences of drug use, and cross-cutting themes of highlighting real-world relevance of research and leveraging mobile health technologies to reduce drug use. Project Narrative This exploratory project will initiate development of an algorithm to predict marijuana use using data from smartphone and ecological momentary assessment, and will examine effects of marijuana use on cognitive functioning in the natural environment using innovative smartphone-based cognitive tests. Developing an algorithm to predict marijuana use has substantial healthcare applications, specifically for timely intervention to reduce marijuana use. Further, examining effects of marijuana use on cognitive functioning daily life has important implications for determining possible adverse health consequences associated with marijuana use.",Real-time prediction of marijuana use & effects of use on cognition in the natural environment,9456715,R21DA043181,"['Abstinence', 'Acute', 'Address', 'Adverse effects', 'Age', 'Age of Onset', 'Algorithms', 'Attention', 'Automobile Driving', 'Awareness', 'Behavior', 'Cellular Phone', 'Cognition', 'Communities', 'Data', 'Data Collection', 'Detection', 'Development', 'Drug usage', 'Ecological momentary assessment', 'Enrollment', 'Environment', 'Female', 'Foundations', 'Frequencies', 'Gender', 'Geography', 'Goals', 'Health', 'Health Technology', 'Healthcare', 'Heart Rate', 'Hour', 'Injury', 'Intake', 'Intervention', 'Interview', 'Intoxication', 'Length', 'Life', 'Literature', 'Location', 'Machine Learning', 'Marijuana', 'Metadata', 'Methods', 'Modeling', 'Monitor', 'Moods', 'National Institute of Drug Abuse', 'Participant', 'Patient Self-Report', 'Performance', 'Positioning Attribute', 'Procedures', 'Protocols documentation', 'Recording of previous events', 'Relaxation', 'Reporting', 'Research', 'Sampling', 'Schedule', 'Severities', 'Short-Term Memory', 'System', 'Testing', 'Text', 'Time', 'Travel', 'Withdrawal', 'addiction', 'age group', 'base', 'cognitive function', 'cognitive task', 'cognitive testing', 'computer science', 'daily functioning', 'data mining', 'improved', 'in vivo', 'innovation', 'mHealth', 'male', 'marijuana use', 'marijuana use disorder', 'marijuana user', 'marijuana withdrawal', 'mobile computing', 'prediction algorithm', 'predictive modeling', 'programs', 'recruit', 'reduce marijuana use', 'wearable device', 'young adult']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2018,196166,-0.014540759437119195
"The nGoggle: A portable brain-based device for assessment of visual function deficits PROJECT SUMMARY Assessment of loss of visual function outside the foveal area is an essential component of the management of numerous conditions, including glaucoma, retinal and neurological disorders. Despite the significant progress achieved with the development of standard automated perimetry (SAP) many decades ago, assessment of visual field loss with SAP still has significant drawbacks. SAP testing is limited by subjectivity of patient responses and high test-retest variability, frequently requiring many tests for effective detection of change over time. Moreover, as these tests are generally conducted in clinic-based settings, limited patient availability and health care resources often result in an insufficient number of tests acquired over time, with delayed diagnosis and detection of disease progression. The requirement for highly trained technicians, cost, complexity, and lack of portability of SAP also preclude its use for screening of visual field loss in underserved populations. To address shortcomings of current methods to assess visual function, we have developed the nGoggle, a wearable device that uses a head-mounted display (HMD) integrated with wireless electroencephalography (EEG), capable of objectively assessing visual field deficits using multifocal steady-state visual-evoked potentials (mfSSVEP). As part of the funded NEI SBIR Phase I, we developed the nGoggle prototype using a modified smartphone-based HMD display and non-disposable electrodes. In our Phase I studies, we conducted benchmarking tests on signal quality of EEG acquisition, developed methods for EEG data extraction and analysis, and conducted a pilot study demonstrating the ability of the device to detect visual field loss in glaucoma, a progressive neuropathy that results in characteristic damage to the optic nerve and resulting visual field defects. We also identified limitations of current existing displays and electrodes, as well as potential avenues for enhancing test reliability and improving user interface. Based on the encouraging results from Phase I and a clear delineation of the steps needed to bring the device into its final commercial product form, we now propose a series of Phase II studies. We hypothesize that optimization of nGoggle's accuracy and repeatability in detecting visual function loss can be achieved through the development of a customized head-mounted display with front-view eye/pupil tracking cameras and disposable no-prep electrodes, as well as enhancement of the visual stimulation protocol and data analytics. The specific aims of this proposal are: 1) To develop a customized head-mounted display and enhanced no-prep electrodes for improving nGoggle's ability to acquire users' mfSSVEP with high signal-to- noise ratios (SNR) in response to visual stimulation; 2) To optimize and validate mfSSVEP stimuli design and data analytics to enhance the accuracy and repeatability of assessing visual function loss with the nGoggle. 3) Complete pivotal clinical studies to support FDA approval. PROJECT NARRATIVE NGoggle Inc. has developed the nGoggle, a wearable device that uses a head-mounted display integrated with wireless electroencephalography, capable of objectively assessing visual field deficits using multifocal steady- state visual-evoked potentials. NGoggle Inc is now proposing to optimize nGoggle's accuracy and repeatability in detecting visual function loss with the use of a customized display, adherent no-prep electrodes, optimized visual stimuli and data analytics. It will also complete pivotal clinical studies to support FDA approval.",The nGoggle: A portable brain-based device for assessment of visual function deficits,9559052,R42EY027651,"['Address', 'Algorithms', 'Area', 'Base of the Brain', 'Benchmarking', 'Blindness', 'Brain', 'Cellular Phone', 'Characteristics', 'Client satisfaction', 'Clinic', 'Clinical Research', 'Custom', 'Data', 'Data Analytics', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Disease', 'Disease Progression', 'Elastomers', 'Electrodes', 'Electroencephalography', 'Electrooculogram', 'Exhibits', 'Eye', 'Funding', 'Glaucoma', 'Head', 'Healthcare', 'Machine Learning', 'Methods', 'Neuropathy', 'Noise', 'Optic Nerve', 'Optical Coherence Tomography', 'Optics', 'Patients', 'Perimetry', 'Phase', 'Photic Stimulation', 'Pilot Projects', 'Protocols documentation', 'Pupil', 'Resources', 'Retinal Diseases', 'Scotoma', 'Series', 'Signal Transduction', 'Skin', 'Small Business Innovation Research Grant', 'Source', 'Stimulus', 'Testing', 'Time', 'Training', 'Underserved Population', 'Vision', 'Visual Fields', 'Visual evoked cortical potential', 'Wireless Technology', 'base', 'cost', 'design', 'field study', 'improved', 'loss of function', 'nervous system disorder', 'patient response', 'phase 1 study', 'phase 2 study', 'portability', 'prototype', 'real world application', 'relating to nervous system', 'response', 'sample fixation', 'screening', 'virtual reality', 'visual stimulus', 'wearable device']",NEI,"NGOGGLE, INC.",R42,2018,849367,-0.0015229135514489346
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9438535,R01EY025332,"['Access to Information', 'Adoption', 'Algorithms', 'American', 'Architecture', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image Enhancement', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'experimental study', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2018,416574,0.02138415910852644
SCH: INT: A Context-aware Cuff-less Wearable Ambulatory Blood Pressure Monitor using a Bio-Impedance Sensor Array  n/a,SCH: INT: A Context-aware Cuff-less Wearable Ambulatory Blood Pressure Monitor using a Bio-Impedance Sensor Array,9756906,R01EB028106,"['Address', 'Adult', 'Affect', 'Aging', 'Algorithms', 'Ambulatory Blood Pressure Monitoring', 'American', 'American Heart Association', 'Arteries', 'Awareness', 'Biological Markers', 'Blood Pressure', 'Blood Pressure Monitors', 'Blood Vessels', 'Calibration', 'Cardiology', 'Cardiovascular Diseases', 'Cardiovascular system', 'Caregivers', 'Caring', 'Cessation of life', 'Characteristics', 'Clinic', 'Clinic Visits', 'Clinical', 'Clinical Trials', 'Data', 'Devices', 'Diagnosis', 'Disease', 'Disease Management', 'Environment', 'Frequencies', 'Funding', 'Grain', 'Guidelines', 'Health Sciences', 'Home environment', 'Hour', 'Human Resources', 'Hypertension', 'Institutes', 'Institution', 'International', 'Intervention Trial', 'Investigation', 'Laboratories', 'Left ventricular structure', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Monitor', 'Noise', 'Outcome', 'Patients', 'Pattern', 'Phenotype', 'Physicians', 'Physiologic pulse', 'Physiological', 'Play', 'Positioning Attribute', 'Posture', 'Preventive Intervention', 'Reading', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Role', 'Site', 'Skin', 'Source', 'Specific qualifier value', 'Speed', 'Sphygmomanometers', 'System', 'Technology', 'Testing', 'Texas', 'Time', 'Tissues', 'Travel', 'United States National Institutes of Health', 'Universities', 'Validation', 'Wearable Computer', 'Wrist', 'base', 'blood perfusion', 'cardiovascular disorder risk', 'clinical practice', 'cohesion', 'college', 'cost', 'design', 'disability-adjusted life years', 'disorder prevention', 'electric impedance', 'health disparity', 'hypertension control', 'learning strategy', 'minority health', 'novel', 'novel strategies', 'patient population', 'sensor', 'signal processing', 'therapy development', 'validation studies', 'wearable device']",NIBIB,TEXAS ENGINEERING EXPERIMENT STATION,R01,2018,299998,0.015063420829675406
"Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs PROJECT ABSTRACT Above-knee amputees often struggle to perform the varying activities of daily life with conventional prostheses. Emerging powered knee-ankle prostheses have motors that can restore normative biomechanics, but these devices are limited to a small set of pre-defined activities that must be tuned to the user by technical experts over several hours. The overall goal of this project is to model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning. The universal use of different task-specific controllers in current powered legs is a direct consequence of the prevailing paradigm for viewing human locomotion as a discrete set of activities. There is a fundamental gap in knowledge about how to analyze, model, and control continuously varying locomotion, which greatly limits the adaptability and agility of powered prostheses. The central hypothesis of this project is that continuously varying activities can be represented by a single mathematical model based on measureable physical quantities called task variables. The proposed project will be scientifically significant to understanding how humans continuously adapt to varying activities and environments, technologically significant to the design of agile, user-synchronized powered prosthetic legs, and clinically significant to the adoption of powered knee-ankle prostheses for improved community ambulation. The proposed model of human locomotion will enable new prosthetic strategies for controlling and adapting to the environment, which aligns with the missions of the NICHD/NCMRR Devices and Technology Development program area and the NIBIB Mathematical Modeling, Simulation, and Analysis program. The innovation of this work is encompassed in 1) a continuous paradigm for variable locomotor activities that challenges the existing discrete paradigm, 2) a unified task control methodology that drastically improves the agility of powered prosthetic legs, and 3) a partially automated tuning process that significantly reduces the time and technical expertise required to configure powered knee- ankle prostheses. This continuous task paradigm will provide new methods and models for studying human locomotion across tasks and task transitions. This innovation will address a key roadblock in control technology that currently restricts powered legs to a small set of activities that do not generalize well across users. The adaptability of the proposed control paradigm across users and activities will transform the prosthetics field with a new generation of “plug-and-play” powered legs for community ambulation. PROJECT NARRATIVE The proposed research is relevant to public health because the clinical application of variable-activity powered prosthetic legs can significantly improve community mobility and therefore quality of life for nearly a million American amputees. Recently developed powered knee-ankle prostheses are limited to a small set of pre- defined activities that require several hours of expert tuning for each user. This project will model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning, which aligns with the missions of the Devices and Technology Development program area of the NICHD National Center for Medical Rehabilitation Research and the Mathematical Modeling, Simulation, and Analysis program of the NIBIB.",Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs,9596236,R01HD094772,"['Address', 'Adoption', 'American', 'Amputees', 'Ankle', 'Area', 'Artificial Leg', 'Biomechanics', 'Clinical', 'Communities', 'Computer Simulation', 'Data', 'Degree program', 'Device or Instrument Development', 'Devices', 'Doctor of Philosophy', 'Electrical Engineering', 'Environment', 'Gait', 'Gait speed', 'Generations', 'Goals', 'Gray unit of radiation dose', 'Hand', 'Home environment', 'Hour', 'Human', 'Human body', 'Joints', 'Knee', 'Knowledge', 'Lead', 'Leg', 'Life', 'Locomotion', 'Lower Extremity', 'Machine Learning', 'Measurable', 'Measures', 'Mechanics', 'Medical center', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motor', 'Motor Activity', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'Orthotic Devices', 'Outcome', 'Phase', 'Play', 'Process', 'Program Development', 'Prosthesis', 'Public Health', 'Quality of life', 'Rehabilitation Research', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Speed', 'Spinal cord injury', 'Stroke', 'Study models', 'System', 'Technical Expertise', 'Technology', 'Time', 'United States National Institutes of Health', 'Walking', 'Work', 'base', 'clinical application', 'clinically significant', 'design', 'exoskeleton', 'experience', 'human data', 'human model', 'improved', 'innovation', 'kinematics', 'mathematical model', 'multidisciplinary', 'orthotics', 'powered prosthesis', 'programs', 'prosthesis control', 'robot control', 'sensor', 'success', 'technology development', 'temporal measurement', 'trend']",NICHD,UNIVERSITY OF TEXAS DALLAS,R01,2018,474602,-0.02235590852367701
"Psychophysics of Reading - Normal and Low Vision DESCRIPTION (provided by applicant):  Psychophysics of Reading - Normal and Low Vision Abstract Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  Difficulty in accessing print imposes obstacles to education, employment, social interaction and recreation.  The ongoing transition to the production and distribution of digital documents brings about new opportunities for people with visual impairment.  Digital documents on computers and mobile devices permit easy manipulation of print size, contrast polarity, font, page layout and other attributes of text.  In short, we now hae unprecedented opportunities to adapt text format to meet the needs of visually impaired readers.  In recent years, our laboratory and others in the vision-science community have made major strides in understanding the impact of different forms of low vision on reading, and the dependence of reading performance on key text properties such as character size and contrast.  But innovations in reading technology have outstripped our knowledge about low-vision reading.  A major gap still exists in translating these laboratory findings into methods for customizing text displays for people with low vision.  The broad aim of the current proposal is to apply our knowledge about the impact of vision impairment on reading to provide tools and methods for enhancing reading accessibility in the modern world of digital reading technology.  Our research plan has three specific goals:   1) To develop and validate an electronic version of the MNREAD test of reading vision, to extend this technology to important text variables in addition to print size, and to develop methods for customizing the selection of text properties for low-vision readers.  MNREAD is the most widely used test of reading in vision research and was originally developed in our laboratory with NIH support.  2) To investigate the ecology of low-vision reading in order to better understand how modern technologies, such as iPad and Kindle are being used by people with low vision.  We plan to evaluate the feasibility of using internet methods to survey low-vision individuals concerning their reading behavior and goals, and of collecting approximate measures of visual function over the internet.  We also plan to develop an ""accessibility checker"" to help low-vision computer users and their families to evaluate the accessibility of specific text displays.  3) To enhance reading accessibility by developing methods for enlarging the visual span (the number of adjacent letters that can be recognized without moving the eyes).  A reduced visual span is thought to be a major factor limiting reading in low vision, especially for people with central-field loss from macular degeneration.  We have already demonstrated methods for enlarging the visual span in peripheral vision.  We plan to develop a more effective perceptual training method for enlarging the visual span, with the goal of improving reading performance for people with central-vision loss. PUBLIC HEALTH RELEVANCE:  Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  The ongoing transition to the use of digital documents on computers and mobile devices brings about new opportunities for customizing text for people with visual impairment.  We propose to apply findings from basic vision science on low vision and reading to develop tools and methods for enhancing reading accessibility for digital text.",Psychophysics of Reading - Normal and Low Vision,9474120,R01EY002934,"['American', 'Attention', 'Auditory', 'Behavior', 'Blindness', 'Books', 'Caring', 'Central Scotomas', 'Characteristics', 'Clinical Research', 'Color', 'Communities', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Custom', 'Dependence', 'Development', 'Devices', 'Ecology', 'Education', 'Employment', 'Eye', 'Family', 'Galaxy', 'Goals', 'Government', 'Guidelines', 'Habits', 'Health', 'Individual', 'Internet', 'Knowledge', 'Laboratories', 'Laboratory Finding', 'Leg', 'Length', 'Letters', 'Life', 'Macular degeneration', 'Mainstreaming', 'Maps', 'Marshal', 'Measures', 'Methods', 'Modernization', 'Optics', 'Paper', 'Participant', 'Patients', 'Perceptual learning', 'Performance', 'Peripheral', 'Play', 'Policies', 'Printing', 'Production', 'Progress Reports', 'Property', 'Protocols documentation', 'Psychophysics', 'Reader', 'Reading', 'Recreation', 'Reporting', 'Research', 'Resources', 'Role', 'Self-Help Devices', 'Social Interaction', 'Surveys', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Uncertainty', 'United States National Institutes of Health', 'Validation', 'Vision', 'Vision research', 'Visual', 'Visual impairment', 'Work', 'analog', 'base', 'design', 'digital', 'essays', 'handheld mobile device', 'improved', 'innovation', 'invention', 'large print', 'literate', 'public health relevance', 'reading difficulties', 'sound', 'symposium', 'tool', 'vision science', 'web-accessible']",NEI,UNIVERSITY OF MINNESOTA,R01,2018,364257,0.03841125352041422
"Designing Visually Accessible Spaces DESCRIPTION (provided by applicant):  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals, including those with physical disabilities and to enhance safety for older people and others who may need to operate under low lighting and other visually challenging conditions.  We plan to develop a computer-based design tool enabling architectural design professionals to assess the visual accessibility of a wide range of environments (such as a hotel lobby, subway station, or eye-clinic reception area).  This tool will simulate such environments with sufficient accuracy to predict the visibility of key landmarks or hazards, such as steps or benches, for different levels and types of low vision, and for spaces varying in lighting, surface properties and geometric arrangement.  Our project addresses one of the National Eye Institute's program objectives:  ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments."" Our research plan has three specific goals:  1) Empirical:  determine factors that influence low-vision accessibility related to hazard detection and navigation in real-world spaces.  2) Computational:  develop working models to predict low vision visibility and navigability in real-world spaces.  3) Deployment:  translate findings from basic vision science and clinical low vision into much needed industrial usage by producing a set of open source software modules to enhance architectural and lighting design for visual accessibility.  The key scientific personnel in our partnership come from three institutions:  University of Minnesota -- Gordon Legge and Daniel Kersten; University of Utah -- William Thompson and Sarah Creem-Regehr; and Indiana University -- Robert Shakespeare.  This interdisciplinary team has expertise in the necessary areas required for programmatic research on visual accessibility -- empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), computer graphics and photometrically accurate rendering (Thompson & Shakespeare) and architectural lighting design (Shakespeare).  We have collaborative arrangements with additional architectural design professionals who will participate in the translation of our research and development into practice. PUBLIC HEALTH RELEVANCE:  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our BRP partnership, with interdisciplinary expertise in vision science, computer science and lighting design, plans to develop computer-based tools enabling architecture professionals to assess the visual accessibility of their designs.",Designing Visually Accessible Spaces,9440421,R01EY017835,"['Address', 'Age', 'American', 'Architecture', 'Area', 'Biological Models', 'Blindness', 'Characteristics', 'Clinic', 'Clinical', 'Complement', 'Complex', 'Computer Analysis', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Contrast Sensitivity', 'Cues', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Disorientation', 'Distance Perception', 'Effectiveness', 'Environment', 'Evaluation', 'Eye', 'Fall injury', 'Geometry', 'Glare', 'Goals', 'Grant', 'Guidelines', 'Height', 'Human', 'Human Resources', 'Indiana', 'Individual', 'Industrialization', 'Institution', 'Investigation', 'Judgment', 'Learning', 'Light', 'Lighting', 'Location', 'Minnesota', 'Modeling', 'Modification', 'National Eye Institute', 'Nature', 'Optics', 'Perception', 'Peripheral', 'Phase', 'Photometry', 'Physical environment', 'Physically Handicapped', 'Positioning Attribute', 'Process', 'Production', 'Property', 'Research', 'Risk', 'Safety', 'Shapes', 'Source', 'Specific qualifier value', 'Stimulus', 'Structure', 'Subway', 'Surface', 'Surface Properties', 'System', 'Test Result', 'Testing', 'Translating', 'Translations', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual Acuity', 'Visual Fields', 'Visual impairment', 'Work', 'base', 'computer science', 'design', 'disability', 'hazard', 'imaging system', 'improved', 'knowledge base', 'luminance', 'novel strategies', 'open source', 'programs', 'public health relevance', 'research and development', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2018,585564,0.06011487784224445
"Developing and Evaluating In-Home Supportive Technology for Dementia Caregivers Abstract  Caring for a loved one with Alzheimer’s disease, frontotemporal dementia, or another neurodegenerative disease is a highly meaningful part of family life. However, the associated burden and strain can have adverse effects on caregivers including mental and physical health problems, reduced well-being, and increased mortality. These effects, in turn, can compromise care quality and shorten survival times for people with dementia (PWD). Research has consistently found that behavioral symptoms in PWD are most strongly associated with adverse caregiver effects, even more so than cognitive and functional symptoms. Empirically- supported interventions are needed that: (a) target mechanisms/pathways shown to connect behavioral symptoms in PWD with adverse effects in caregivers, and (b) can be disseminated successfully into larger community settings. In this SBIR Fast Track application, we will develop, refine, and evaluate People Power Caregiver (PPCg), a flexible and expandable hardware/software system designed to integrate in-home sensors and devices, emergency responding, social networking, and Internet-of-Things (i.e., devices that can be controlled and communicated with via the internet) technologies to create a more supportive and safe home environment for caregivers and PWD. PPCg monitors troublesome behaviors in PWD (e.g., wandering), and targets mechanisms (e.g., worry, social isolation) thought to link behavioral symptoms in PWD with adverse caregiver outcomes. PPCg is also designed to minimize demands on caregivers’ limited time and energy and to provide a platform for data collection that can be used by researchers and care professionals.  This application is an innovative partnership between People Power (CEO: Gene Wang, www.peoplepowerco.com) in Redwood City, California and the Berkeley Psychophysiology Laboratory (Director: Robert W. Levenson) at the University of California, Berkeley. People Power is an award-winning, established leader in home monitoring and Internet-of-Things technology and has recently started developing assistive technologies for the elderly. The Berkeley Psychophysiology Laboratory has been engaged in basic and applied research with PWD and other neurodegenerative diseases and their caregivers for the past 15 years. The proposal addresses three specific aims: Aim 1: Focus groups. In Phase I of the project, a preliminary version of PPCg will be developed and refined with input from focus groups of caregivers and in- home testing (Study 1). Aim 2: Efficacy. In the first year of Phase II of the project, the first production version of PPCg will be installed by the research team in 80 homes and evaluated in a randomized controlled efficacy trial that includes careful diagnosis and assessment of emotional functioning in PWD and caregivers (Study 2). Aim 3: Effectiveness. In the second year of Phase II of the project, working with energy industry partners, a refined and expanded second production version of PPCg will be provided to 400 homes with familial caregivers for self-installation and evaluation in a community-based effectiveness trial (Study 3). Relevance  Dementias cause profound cognitive, emotional, and functional deficits. As the disease progresses, people with dementia become increasingly dependent on caregivers, who are at heightened risk for mental and physical health problems. Applying assistive technology to monitor worrisome behaviors, improve safety, and reduce social isolation in the home environment can reduce caregiver burden and improve care in ways that have major public health benefits.",Developing and Evaluating In-Home Supportive Technology for Dementia Caregivers,9558429,R44AG059458,"['Address', 'Adverse effects', 'Age', 'Aggressive behavior', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Applied Research', 'Artificial Intelligence', 'Award', 'Basic Science', 'Behavior', 'Behavior monitoring', 'Behavioral Symptoms', 'California', 'Caregiver Burden', 'Caregivers', 'Caring', 'Cities', 'Clinical Trials', 'Cognitive', 'Communities', 'Computer software', 'Data Collection', 'Dementia', 'Dementia caregivers', 'Devices', 'Diagnosis', 'Disease', 'Effectiveness', 'Elderly', 'Emergency Situation', 'Emotional', 'Evaluation', 'Family', 'Family member', 'Fire - disasters', 'Floods', 'Focus Groups', 'Friends', 'Fright', 'Frontotemporal Dementia', 'Future', 'GTP-Binding Protein alpha Subunits, Gs', 'Genes', 'Gray unit of radiation dose', 'Health', 'Health Benefit', 'Home environment', 'Impairment', 'Internet', 'Internet of Things', 'Intervention', 'Laboratories', 'Lead', 'Learning', 'Life', 'Link', 'Loneliness', 'Mental Health', 'Monitor', 'Neurodegenerative Disorders', 'Outcome', 'Pathway interactions', 'Pattern', 'Personal Satisfaction', 'Phase', 'Population', 'Production', 'Progressive Disease', 'Psychophysiology', 'Public Health', 'Quality of Care', 'Randomized', 'Redwood', 'Research', 'Research Personnel', 'Risk', 'Safety', 'Self-Help Devices', 'Small Business Innovation Research Grant', 'Social Behavior', 'Social Network', 'Social isolation', 'Symptoms', 'Technology', 'Testing', 'Time', 'Universities', 'Voice', 'adverse outcome', 'base', 'caregiver interventions', 'community setting', 'dementia caregiving', 'design', 'effectiveness trial', 'efficacy trial', 'emotional behavior', 'flexibility', 'hazard', 'improved', 'industry partner', 'innovation', 'loved ones', 'mortality', 'physical conditioning', 'prevent', 'psychosocial', 'response', 'sensor', 'service providers', 'social media', 'software systems', 'theories', 'user-friendly']",NIA,PEOPLE POWER COMPANY,R44,2018,825561,-0.01779854857399466
"NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair DESCRIPTION (provided by applicant):         The aim of this proposal is to conduct research on the foundational models and algorithms in computer vision and machine learning for an egocentric vision based active learning co-robot wheelchair system to improve the quality of life of elders and disabled who have limited hand functionality or no hand functionality at all, and rely on wheelchairs for mobility. In this co-robt system, the wheelchair users wear a pair of egocentric camera glasses, i.e., the camera is capturing the users' field-of-the-views. This project help reduce the patients' reliance on care-givers. It fits NINR's mission in addressing key issues raised by the Nation's aging population and shortages of healthcare workforces, and in supporting patient-focused research that encourage and enable individuals to become guardians of their own well-beings.  The egocentric camera serves two purposes. On one hand, from vision based motion sensing, the system can capture unique head motion patterns of the users to control the robot wheelchair in a noninvasive way. Secondly, it serves as a unique environment aware vision sensor for the co-robot system as the user will naturally respond to the surroundings by turning their focus of attention, either consciously or subconsciously. Based on the inputs from the egocentric vision sensor and other on-board robotic sensors, an online learning reservoir computing network is exploited, which not only enables the robotic wheelchair system to actively solicit controls from the users when uncertainty is too high for autonomous operation, but also facilitates the robotic wheelchair system to learn from the solicited user controls. This way, the closed- loop co-robot wheelchair system will evolve and be more capable of handling more complicated environment overtime.  The aims ofthe project include: 1) develop an method to harness egocentric computer vision-based sensing of head movements as an alternative method for wheelchair control; 2) develop a method leveraging visual motion from the egocentric camera for category independent moving obstacle detection; and 3) close the loop of the active learning co-robot wheelchair system through uncertainty based active online learning. PUBLIC HEALTH RELEVANCE:          The project will improve the quality of life of those elders and disabled who rely on wheelchairs for mobility, and reduce their reliance on care-givers. It builds an intelligent wheelchair robot system that can adapt itself to the personalized behavior of the user. The project addresses the need of approximately 1 % of our world's population, including millions of Americans, who rely on wheelchair for mobility.",NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair,9133939,R01NR015371,"['Active Learning', 'Address', 'Algorithms', 'American', 'Attention', 'Awareness', 'Behavior', 'Caregivers', 'Categories', 'Computer Vision Systems', 'Conscious', 'Detection', 'Disabled Persons', 'E-learning', 'Elderly', 'Environment', 'Foundations', 'GTP-Binding Protein alpha Subunits, Gs', 'Hand functions', 'Head', 'Head Movements', 'Healthcare', 'Individual', 'Learning', 'Machine Learning', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motivation', 'Patients', 'Pattern', 'Population', 'Principal Investigator', 'Quality of life', 'Research', 'Robot', 'Robotics', 'Subconscious', 'System', 'Uncertainty', 'Vision', 'Visual Motion', 'Wheelchairs', 'aging population', 'base', 'improved', 'operation', 'programs', 'public health relevance', 'robot control', 'sensor']",NINR,STEVENS INSTITUTE OF TECHNOLOGY,R01,2017,133916,0.036111410160356607
"Computational Image Analysis for Cellular and Developmental Biology DESCRIPTION (provided by applicant): This proposal requests support for an intensive ten-day course on Computational Image Analysis for Cellular and Developmental Biology. The course is designed for graduate students and postdoctoral fellows, and takes place at the Marine Biological Laboratory in Woods Hole, MA. The course is the first of its kind, giving students formal training in computer vision for the specific analysis of cell and developmental biology image data. Building strong foundations in this topic is critical for pushing cell and developmental biology forward, as imaging has become more and more an indispensable tool in these fields. The course covers the fundamentals of computer vision, taking the students through the sequence of low-, intermediate-, and high-level computer vision tasks that are required to solve image analysis problems in quantitative cellular and developmental biology. The curriculum starts with filtering, thresholding and edge/line/generic feature detection, followed by more sophisticated detection algorithms that employ model fitting. After this introductory block to low-level computer vision tasks, the course moves on to intermediate and higher-level tasks, including object association in space and time (such as tracking) and machine learning tools for phenotype classification. Each topic is covered first by a lecture, generally taught by one of the four core faculty, followed by a 3-4 hour computer programming session where students immediately implement the concepts they learn. There are usually two lectures + computer labs per day. Most programming exercises are individual, giving each student the opportunity to ""get their hands dirty,"" while two are team projects allowing the students to also learn and practice methods of code sharing. Over the course's ten days there are three guest lectures by leading researchers in the fields of biological imaging and computer vision, each followed by in-depth discussion, as well as research talks given by the students, faculty and teaching assistants. With this, the core lectures and labs teach the students the fundamentals of computer vision in a logical, continuous manner, the guest lecturers introduce the students to exciting new challenges in imaging and image analysis, and the student/faculty research talks encourage communication between all course participants and give especially the students the opportunity to reflect on how the course can help them with their research. PUBLIC HEALTH RELEVANCE:  Imaging has become an indispensable tool in cellular and developmental biology research, but without rigorous, quantitative image analysis it cannot achieve its full potential. This proposal requests support for a new course that will fill the voidof education in computer vision as applied to cellular and developmental biology. The curriculum incorporates a carefully balanced mixture of lectures and associated programming exercises. We have given a pilot course once in October 2010, with very positive reviews from the students and their advisers.",Computational Image Analysis for Cellular and Developmental Biology,9215686,R25GM103792,"['Address', 'Algorithms', 'Biological', 'Cells', 'Cellular biology', 'Classification', 'Code', 'Collection', 'Communication', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Data', 'Detection', 'Development', 'Developmental Biology', 'Education', 'Educational Curriculum', 'Exercise', 'Faculty', 'Foundations', 'Future', 'Generic Drugs', 'Goals', 'Hand', 'Home environment', 'Hour', 'Image', 'Image Analysis', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Marines', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Phenotype', 'Postdoctoral Fellow', 'Request for Proposals', 'Research', 'Research Personnel', 'Seeds', 'Software Design', 'Solid', 'Students', 'Time', 'Training', 'Wood material', 'computer program', 'design', 'faculty research', 'graduate student', 'lecturer', 'lectures', 'personalized approach', 'programs', 'public health relevance', 'quantitative imaging', 'student training', 'teaching assistant', 'tool']",NIGMS,MARINE BIOLOGICAL LABORATORY,R25,2017,59383,0.007972048264027903
"Naturalistic Data Collection In The SmartPlayroom PROJECT SUMMARY The aims of this proposal are to fully develop and validate the SmartPlayroom as a powerful automated data collection and analysis tool in developmental research. This room looks like any playroom in a home or school but is designed to naturalistically collect data in real time and simultaneously on all aspects of children's behavior. Behaviors include movement kinematics, language, eye movements, and social interaction while a child performs naturalistic tasks, plays and explores without instruction, walks or crawls, and interacts with a caregiver. The space is equipped with mobile eye tracking, wireless physiological heart rate and galvanic skin response sensors, audio and video recording, and depth sensor technologies. Funding is requested to demonstrate the scientific advantage of naturalistic measurement using an example from visual attention research (Aim 1), and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use in 4-9 year-old children (Aim 2). By combining fine-grained sensor data with high-throughput automated computer vision and machine learning tools, we will be able to automate quantitative data collection and analysis in the SmartPlayroom for use in addressing myriad developmental questions. The SmartPlayroom approach overcomes completely the limitations of task-based experimentation in developmental research, offering quantitative precision in the collection of ecologically valid data. It has the power to magnify both construct validity and measurement reliability in developmental research. The investigators are committed to making freely available our data, computer vision algorithms, and discoveries so that we might move the field forward quickly. NARRATIVE We focus this work on developing and validating a novel and innovative data collection space called the SmartPlayroom, designed to pair naturalistic exploration and action with the precision of computerized automated data collection and analysis. This proposal aims to demonstrate the scientific advantage of naturalistic measurement, and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use with 4-9 year-old children in the SmartPlayroom. !",Naturalistic Data Collection In The SmartPlayroom,9373088,R21MH113870,"['9 year old', 'Address', 'Adult', 'Age', 'Algorithms', 'Attention', 'Automated Annotation', 'Behavior', 'Behavior assessment', 'Behavioral', 'Benchmarking', 'Caregivers', 'Cereals', 'Child', 'Child Behavior', 'Child Development', 'Child Rearing', 'Childhood', 'Cognitive', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Darkness', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Developmental Process', 'Discipline', 'Education', 'Environment', 'Event', 'Eye', 'Eye Movements', 'Face', 'Funding', 'Galvanic Skin Response', 'Goals', 'Heart Rate', 'Home environment', 'Human', 'Instruction', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Memory', 'Methods', 'Modernization', 'Monitor', 'Movement', 'Neurodevelopmental Disorder', 'Performance', 'Physiological', 'Play', 'Policies', 'Process', 'Research', 'Research Personnel', 'Schools', 'Social Interaction', 'Societies', 'Technology', 'Time', 'Time Study', 'Training', 'Video Recording', 'Vision', 'Visual attention', 'Walking', 'Wireless Technology', 'Work', 'base', 'behavioral study', 'cognitive development', 'computerized', 'cost', 'design', 'eye hand coordination', 'flexibility', 'frontier', 'grasp', 'indexing', 'innovation', 'kinematics', 'novel', 'research and development', 'sample fixation', 'sensor', 'skills', 'tool']",NIMH,BROWN UNIVERSITY,R21,2017,243750,0.012989092279013527
"IGF::OT::IGF Constructed Environments for Successfully Sustaining Abstinence Through Immersive and On-Demand Treatment. Period of Performance: September 22, 2017 - March 21, 2018. N43DA-17-5583.   Charles River Analytics and our partners at Massachusetts General Hospital and Virtual Reality Medical Center propose to design and demonstrate Constructed Environments for Successfully Sustaining Abstinence Through Immersive and On-Demand Treatment (CESSATION). We will build VR environments using our in-house game engine and content (cues) that are diverse, realistic, and well-placed, bolstering traditional psychological therapy delivery, and use research-based principles of narrative and game design to produce immersive and engaging content to motivate continued use. We will use probabilistic models (e.g., Bayesian networks) and machine-learning (e.g., neural nets) to learn from patient data and clinician input to assist in selecting and combining therapies in a way that is tailored to the individual patient. Finally, CESSATION will communicate information to clinicians in a manner that is understandable, and gives them enough meta-information to foster appropriate trust in the system. The anticipated results of the proposed Phase I work are: (1) an initial domain analysis; (2) initial VR content; (3) initial motivational content; (4) prototype sensor capabilities; (5) a prototype Intervention Tailoring Component; (6) a prototype Clinician User Interface and Remote Patient Data Server; and (7) an initial Phase I CESSATION prototype demonstration and evaluation. n/a","IGF::OT::IGF Constructed Environments for Successfully Sustaining Abstinence Through Immersive and On-Demand Treatment. Period of Performance: September 22, 2017 - March 21, 2018. N43DA-17-5583.  ",9582496,71201700022C,"['Abstinence', 'Algorithms', 'Contractor', 'Cues', 'Data', 'Effectiveness', 'Elements', 'Ensure', 'Environment', 'Evaluation', 'Exposure to', 'Feedback', 'Fostering', 'Future', 'General Hospitals', 'Health Insurance Portability and Accountability Act', 'Immersion Investigative Technique', 'Intervention', 'Learning', 'Machine Learning', 'Massachusetts', 'Mathematics', 'Measures', 'Medical center', 'Motivation', 'Output', 'Patients', 'Performance', 'Phase', 'Quick Test for Liver Function', 'Recommendation', 'Regulation', 'Reporting', 'Research', 'Rivers', 'Secure', 'Statistical Models', 'Stimulus', 'Summary Reports', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Trust', 'Visit', 'Work', 'base', 'commercialization', 'computer based statistical methods', 'design', 'graphical user interface', 'individual patient', 'lens', 'prototype', 'psychologic', 'relating to nervous system', 'response', 'sensor', 'simulation', 'smoking cessation', 'therapy design', 'virtual reality']",NIDA,"CHARLES RIVER ANALYTICS, INC.",N43,2017,149987,-0.009815407393261531
"User-driven fitting of hearing aids and other assistive hearing devices Hearing aids are the principal tool today for ameliorating age-related hearing loss and its significant social, cognitive and functional costs to patients and society at large. However, many individuals who are prescribed hearing aids do not use them at all, or use them only occasionally. Most reasons behind the “hearing aid in the drawer” phenomenon relate to the characteristics of the sound produced, and could, in theory, be addressed with the correct signal processing strategy. The problem persists despite the increased complexity and power of new devices, for three reasons: (a) The hearing aid parameters, as set in the clinic, introduce distortion or render audible many sounds that the hearing impaired user had become accustomed to not hearing. The novelty is often so uncomfortable for the user as to discard the device. (b) The optimum parameters vary depending on the listening task and environment. Under some conditions, a device with parameters designed for a different condition will perform worse than no device at all. (c) The clinical fitting is derived from a non-ideal way to assess auditory function (the pure- tone audiogram). The optimum parameters for the actual impairment may be different from those of the prescribed fitting. Although it is true that the physiological mechanisms make it impossible to process sound so as to completely reverse the effect of sensorineural hearing loss, a device that delivers some benefit at all times is likely to be used all the time. The goal is to develop a hearing aid that can adaptively change its parameters to address the problems above, and will be accomplished with a novel fitting approach that rapidly presents a number of parameter settings to the user and lets the user guide the system toward the optimal settings for each listening situation. This requires the development of machine-learning algorithms to effectively search the parameter space and user interface devices and instructions that are easy for the patient to use. The focus of this Phase I proposal is the development of the algorithms and the adaptive user-driven fitting program, and to compare the proposed fitting with the traditional audiogram-based fitting across measures of functional hearing (ability to recognize speech in noise) and subjective preference. A hearing aid user is often dissatisfied with the sound quality of their device, despite its sophistication and adjustment by a trained audiologist. The problem can be mitigated by letting the user fine-tune the device for maximum comfort in everyday use. We will apply modern machine learning methods to develop a program for efficient user-driven fitting of hearing assistive devices.",User-driven fitting of hearing aids and other assistive hearing devices,9409910,R43DC016251,"['Address', 'Algorithms', 'Audiometry', 'Auditory', 'Back', 'Books', 'Cellular Phone', 'Characteristics', 'Clinic', 'Clinical', 'Cognitive', 'Complex', 'Computer software', 'Development', 'Devices', 'Environment', 'Future', 'Goals', 'Hearing', 'Hearing Aids', 'Human', 'Impairment', 'Individual', 'Instruction', 'Intuition', 'Knowledge', 'Likelihood Functions', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Music', 'Noise', 'Outcome', 'Patients', 'Performance', 'Phase', 'Physiological', 'Presbycusis', 'Process', 'Protocols documentation', 'Psychology', 'Psychophysics', 'Relaxation', 'Reproducibility', 'Self-Help Devices', 'Sensorineural Hearing Loss', 'Societies', 'Speech', 'Speed', 'Statistical Models', 'Stimulus', 'System', 'Testing', 'Time', 'Training', 'Update', 'base', 'cohort', 'cost', 'design', 'hearing impairment', 'improved', 'learning strategy', 'models and simulation', 'novel', 'performance tests', 'preference', 'programs', 'response', 'signal processing', 'simulation', 'social', 'sound', 'success', 'theories', 'tool', 'vector']",NIDCD,"CARAWAY SOFTWARE, INC.",R43,2017,224966,-0.0017630964343990677
"Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor Summary The principal goal of this proposal is to increase the accuracy and precision of a low-cost autorefraction device called the QuickSee, in order to improve access to refractive eye care for underserved populations. Poor vision due to a lack of eyeglasses is highly prevalent in low-resource settings throughout the world and significantly reduces quality of life, education, and productivity. The existing QuickSee only extracts the lower- order aberration information contained within a wavefront profile of the eye, to roughly estimate an eyeglass prescription. This proposal will further improve the accuracy of the QuickSee device by exploiting both the lower- and higher-order aberrations contained within the complete wavefront. To realize this goal, we will enroll 300 subjects (600 eyes) in Baltimore, MD, and will obtain subjective refraction and visual acuity (VA) measurements and will use machine learning on this large dataset of wavefront profiles to optimize the wavefront-to-refraction algorithm of the QuickSee device. The main output of this project will be a robust and improved-accuracy next-generation QuickSee device that will increase efficiency of and decrease the training requirements of eye care professionals, and potentially dispense refractive correction that provides similar or better VA than correction from an eye care professional. Successful completion of this work will be an important step towards dramatically improving eyeglass accessibility for health disparity populations in the USA and internationally in low-resource settings. Upon completion of this proposal, we will apply for a Phase II award proposing to work with Wilmer Eye Institute research faculty to assess widespread deployment of the next-generation QuickSee with minimally-trained personnel in order to accurately and reliably provide thousands of pairs of low-cost corrective eyeglasses to underserved communities. Project Narrative This project proposal seeks to develop a novel technology that will disruptively increase the accessibility of refractive eye care for health disparity populations in low-resource settings. Specifically, sophisticated algorithms will be developed that improve the accuracy of the QuickSee device so that it can improve the efficiency of and reduce the training barriers for eye care professionals, and potentially provide refractive correction without the need for refinement by a trained eye care professional. Our goal is to develop a low- cost, easy-to-use, scalable solution to increase accessibility to vision correction globally.","Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor",9312420,R43EB024299,"['Algorithms', 'Award', 'Baltimore', 'Brazil', 'Businesses', 'Caliber', 'Calibration', 'Caring', 'Communities', 'Country', 'Data', 'Data Set', 'Developed Countries', 'Developing Countries', 'Development', 'Devices', 'Diagnostic', 'Education', 'Educational Status', 'Enrollment', 'Eye', 'Eyeglasses', 'Feedback', 'Geometry', 'Goals', 'Gold', 'Guatemala', 'Hospitals', 'Human Resources', 'Impairment', 'Improve Access', 'Income', 'India', 'Institutes', 'International', 'Machine Learning', 'Mali', 'Measurement', 'Measures', 'Modeling', 'Noise', 'Ophthalmic examination and evaluation', 'Optometrist', 'Output', 'Patient Schedules', 'Patients', 'Phase', 'Population', 'Prevalence', 'Procedures', 'Productivity', 'Pupil', 'Quality of life', 'Refractive Errors', 'Research Institute', 'Resources', 'Spottings', 'Testing', 'Time', 'Training', 'Underserved Population', 'Universities', 'Validation', 'Vision', 'Visual Acuity', 'Work', 'base', 'cost', 'faculty research', 'health care disparity', 'health disparity', 'improved', 'lens', 'new technology', 'next generation', 'novel strategies', 'success', 'vector']",NIBIB,"PLENOPTIKA, INC.",R43,2017,200225,0.008639935472644267
"Crowd-Sourced Annotation of Longitudinal Sensor Data to Enhance Data-Driven Precision Medicine for Behavioral Health ﻿    DESCRIPTION (provided by applicant): Longitudinal sensor data collected passively from mobile phones and other wearable sensors will transform behavioral science by allowing researchers to use ""big data,"" but at the person-level, to understand how behavior and related environmental exposures impact health outcomes. Computers will analyze individual-level data streams to permit unprecedented, individual-level precision in research and intervention. This type of precision medicine enables targeting of science and medicine to a particular individual's genetic makeup, past and current situation, and behavioral health exposures. Mobile phones, smartwatches, and common fitness devices are already capable of generating rich data on behavior, but developing algorithms to interpret that raw data using the latest machine learning algorithms requires practical strategies to annotate large datasets. We propose to develop and test the feasibility and usability of a mobile and online crowdsource-based system for cleaning and annotating behavioral data collected from motion sensors, mobile phones, and other mobile devices. Our goal is to demonstrate how individuals playing mobile and online games - the ""crowd"" - can collectively, affordably, and incrementally clean and add important metadata to raw sensor data that has been passively collected from individuals, similar to that from population-scale surveillance studies (e.g., the National Health and Nutrition Examination Survey (NHANES) and UK Biobank) and those planned for studies such as the White House's Precision Medicine Initiative. The game-playing crowd will thereby dramatically improve the utility of the datasets collected for a variety of scientific studies. We will validate our prototye system on datasets collected from motion monitors used to study physical activity, sedentary behavior, and sleep, but we will demonstrate how the system could be extended for use on the increasingly rich datasets that are being collected with mobile devices and that include not only motion data, but also sensor data on location, light, audio, and person-to-person proximity. We will then refine the system, foster a community of crowd game players interested in citizen science, and release the source code to the system as an open source project so that other researchers can adapt the technique for their own work. PUBLIC HEALTH RELEVANCE: Longitudinal sensor data collected passively from wearable activity monitors and mobile phones will transform behavioral science by allowing researchers to use ""big data,"" but at the person-level, to understand how behavior and related environmental exposures impact health outcomes and personalize health intervention and research. We propose to develop and test a system that permits typical mobile application game players to help scientists improve this type of data, by adding additional annotations that enrich the data, making it more useful for behavioral science and more amenable to automatic processing. This will help researchers to better understand how individual-level behaviors relate to health outcomes in current research studies that collect personal-level sensor data such as NHANES and the Women's Health Study, and future big data ventures such as the new Precision Medicine Initiative.",Crowd-Sourced Annotation of Longitudinal Sensor Data to Enhance Data-Driven Precision Medicine for Behavioral Health,9357585,UH2EB024407,"['Accelerometer', 'Algorithms', 'American', 'Behavior', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Car Phone', 'Classification', 'Cohort Studies', 'Communities', 'Complex', 'Computers', 'Crowding', 'Data', 'Data Collection', 'Data Quality', 'Data Science', 'Data Set', 'Devices', 'Environmental Exposure', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Future', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Intervention', 'Interview', 'Label', 'Lead', 'Light', 'Location', 'Machine Learning', 'Manuals', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Monitor', 'Motion', 'National Health and Nutrition Examination Survey', 'Outcome', 'Output', 'Participant', 'Pathway Analysis', 'Pattern', 'Performance', 'Persons', 'Phonation', 'Physical activity', 'Play', 'Population', 'Precision Medicine Initiative', 'Process', 'Proteomics', 'Research', 'Research Personnel', 'Risk Behaviors', 'Sampling', 'Science', 'Scientist', 'Sleep', 'Source Code', 'Stream', 'System', 'Techniques', 'Telephone', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Visualization software', 'Women&apos', 's Health', 'Work', 'annotation  system', 'base', 'behavioral health', 'biobank', 'citizen science', 'crowdsourcing', 'design', 'experimental study', 'fitness', 'genetic makeup', 'handheld mobile device', 'improved', 'innovation', 'insight', 'instrument', 'interest', 'metabolomics', 'mobile application', 'mobile computing', 'novel', 'open source', 'precision medicine', 'prototype', 'public health relevance', 'research study', 'sedentary lifestyle', 'sensor', 'surveillance study', 'temporal measurement', 'tool', 'ubiquitous computing', 'usability']",NIBIB,NORTHEASTERN UNIVERSITY,UH2,2017,300665,-0.01508044226639171
"Capti Screen Reading Assistant for Goal Directed Web Browsing ﻿    DESCRIPTION (provided by applicant): Web browsing with assistive technologies such as screen readers and magnifiers can often be a frustrating and challenging experience for people with vision impairments, because it entails a lot of searching for content, forms, and links that are required for doing online tasks such as shopping, bill-payment, reservations, etc. This SBIR Phase II project will build on Phase I results and will continue the development and eventual deployment of Capti Screen Reading Assistant - a next-generation assistive technology, enabling goal- directed web browsing for people with visual impairments. With Capti Assistant, users will be able to stay focused on their high-level browsing goals that are expressed in natural language (spoken or typed). The Assistant will lead the users step-by-step towards the fulfillment of these goals by offering suggestions on what action to take at every step of the way and automatically executing the chosen action on behalf of the user. Suggested actions will include operations such as form filling, activating controls (e.g., clicking buttons and links), et. Capti Assistant will dramatically reduce the time spent by people with visual impairments on performing tasks online. The Assistant will significantly improve the speed and efficiency with which they can interact with the Web, thereby, making people with disabilities more productive in today's web-based economy. Given a user browsing goal, expressed in a natural-language form, Capti Assistant will utilize a predictive model to guide the user toward the goal. The unique aspect of the Assistant is that its suggestions will be automatically learned from the user's own history of browsing actions and commands, as well as from the user's demonstration of how to accomplish browsing tasks that have not been done before. The Assistant will process user commands and present the suggested browsing actions to the user on demand, giving the user a choice between following the suggestions or continue browsing normally without accepting the suggestions. The functionality offered by the Assistant will go far beyond popular personal assistant applications such as Siri, which have not been designed specifically for people with vision impairments, and which cannot be used for ad hocweb browsing. Capti Assistant will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the Web. For them, the Assistant will usher in a new era of independence and employability in our global web-based economy. Thus, from a broader perspective, goal- directed browsing will exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", i.e. anyone should be able to reap the benefits of the Web without being constrained by any disability. PUBLIC HEALTH RELEVANCE: This SBIR Project seeks to do Research and Development on goal-directed web browsing - the next generation accessible technology that will empower people with vision impairments to stay focused on their high-level browsing goals, while the browser will do low-level operations (such as clicking on links and filling forms) necessary to fulfill these goals. Goal-directed browsing will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the web, thus improving independence and employability of the former in our global Web-based economy. From a broader perspective, goal-directed browsing will facilitate rehabilitation of people with disabilities and exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", enabling anyone to reap the benefits of the Web without being constrained by any disability.",Capti Screen Reading Assistant for Goal Directed Web Browsing,9199231,R44EY021962,"['Automation', 'Blindness', 'Budgets', 'Businesses', 'Communication', 'Computer software', 'Computers', 'Data', 'Development', 'Disabled Persons', 'Ensure', 'Environment', 'Evaluation', 'FarGo', 'Focus Groups', 'Generations', 'Goals', 'Human Resources', 'In Situ', 'Information Retrieval', 'Internet', 'Internships', 'Laboratory Study', 'Lead', 'Legal patent', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Mining', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Pattern', 'Phase', 'Probability', 'Process', 'Productivity', 'Publications', 'Publishing', 'Reader', 'Reading', 'Recording of previous events', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Reservations', 'Resources', 'Schedule', 'Scheme', 'Seasons', 'Self-Help Devices', 'Small Business Innovation Research Grant', 'Speed', 'Suggestion', 'System', 'Technology', 'Time', 'Universities', 'Vision', 'Visual impairment', 'Work', 'base', 'collaborative environment', 'commercial application', 'commercialization', 'computer human interaction', 'design', 'disability', 'educational atmosphere', 'experience', 'improved', 'innovation', 'member', 'natural language', 'next generation', 'novel strategies', 'operation', 'payment', 'predictive modeling', 'public health relevance', 'quality assurance', 'query optimization', 'research and development', 'response', 'success', 'technological innovation', 'tool', 'usability', 'web page', 'web-accessible']",NEI,"CHARMTECH LABS, LLC",R44,2017,500000,0.043926339386031944
"Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research ﻿    DESCRIPTION (provided by applicant): This Phase II project aims to continue development of a commercial quality, innovative cloud hosted information management system, called Climb 2.0(tm) that will increase laboratory efficiency and provide improved capabilities for research laboratories. Climb is designed to offer integrated laboratory process management modules that include mobile communications tools data monitoring and alert systems, and integrated access to Microsoft Azure Cloud(tm) Machine Learning and Stream Analytics services. Initially, Climb 2.0 will target animal model research laboratories; however, the core of the platform is designed to be adaptable to nearly any research type or related industry. Current research information management systems are primarily designed as record-keeping tools with little or no direct focus on laboratory efficiency or in enhancing value of the research data. They also do not leverage emergent mobile device technologies, social media frameworks, and data analysis and storage capabilities of cloud computing. Many laboratories still use paper as their primary recording system. Paper data logging is then followed by secondary data entry into a laboratory database. These systems are error prone, time consuming and lead to laboratory databases with significant time lags between data acquisition and data entry. Moreover, they do not recognized cumulative data relationships, which may identify important trends, and researchers often miss windows of opportunity to take action on time-sensitive events. In Phase I, RockStep Solutions demonstrated feasibility of an innovative Cloud Information Management Bundle system, Climb, which will increase efficiency and improve capabilities in animal model data management. During Phase I, a beta version of Climb was successfully developed and tested against strict performance metrics as a proof of concept. We successfully built a prototype with working interfaces that integrates real-time communication technologies with media capabilities of mobile devices. Phase II proposes four specific Aims: 1) Develop the technology infrastructure to support the secure and scalable Software as a Service (SaaS) deployment of Climb for enterprise commercial release; 2) Develop and extend the Phase-I prototype Data Monitoring and Messaging System (DMMS) into a platform ready for production use; 3) Extend Climb's DMMS adding a Stream Analytics engine to support Internet of Things (IoT) devices and streaming media; 4) Deploy a beta release of Climb at partner research labs, test and refine the product for final commercialization. To ensure Climb is developed with functionality and tools relevant to research organizations, RockStep Solutions has established collaborations with key beta sites to test all of the major functionality developed in this proposal. IMPACT: By leveraging emergent technologies and cloud computing, Climb offers several advantages: 1) enables real-time communications using familiar tools among members of research groups; 2) reduces the risk of experimental setbacks, and 3) enables complex experiments to be conducted efficiently. PUBLIC HEALTH RELEVANCE: The NIH Invests approximately $12 billion each year in animal model research that is central to both understanding basic biological processes and for developing applications directly related to improving human health. More cost-effective husbandry and colony management techniques, equipment, and new approaches to improve laboratory animal welfare and assure efficient and appropriate research use (e.g., through cost savings and reduced animal burden) are high priorities for NIH. RockStep Solutions proposes to meet this need by integrating existing and emergent software and communication technologies to create a novel solution, Climb 2.0, for research animal data management. Climb 2.0 extracts immediate value of data in animal research laboratories and extends the database into a multimedia communication network, thus enabling science that would be impractical to conduct with traditional methods.",Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research,9354497,R44GM112206,"['Address', 'Algorithms', 'Animal Experimentation', 'Animal Model', 'Animal Welfare', 'Animals', 'Biological Process', 'Biomedical Research', 'Clinical Laboratory Information Systems', 'Cloud Computing', 'Collaborations', 'Communication', 'Communication Tools', 'Communications Media', 'Complex', 'Computer software', 'Computers', 'Consumption', 'Cost Savings', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Security', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Devices', 'Emerging Technologies', 'Ensure', 'Equipment', 'Equipment and supply inventories', 'Event', 'Feedback', 'Future', 'Goals', 'Health', 'Health system', 'Human', 'Industry', 'Information Management', 'Internet', 'Internet of Things', 'Investments', 'Laboratories', 'Laboratory Animals', 'Laboratory Research', 'Lead', 'Machine Learning', 'Management Information Systems', 'Methods', 'Modernization', 'Monitor', 'Motion', 'Multimedia', 'Notification', 'Paper', 'Performance', 'Phase', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rest', 'Risk', 'Science', 'Secure', 'Services', 'Site', 'Stream', 'System', 'Techniques', 'Technology', 'Testing', 'The Jackson Laboratory', 'Time', 'Transact', 'United States National Institutes of Health', 'analytical tool', 'base', 'cloud based', 'commercialization', 'computerized data processing', 'cost', 'cost effective', 'data acquisition', 'data management', 'design', 'encryption', 'experimental study', 'good laboratory practice', 'graphical user interface', 'handheld mobile device', 'improved', 'innovation', 'laptop', 'member', 'novel', 'novel strategies', 'predictive modeling', 'programs', 'prototype', 'public health relevance', 'service learning', 'social media', 'software as a service', 'success', 'time interval', 'tool', 'trend', 'usability']",NIGMS,"ROCKSTEP SOLUTIONS, INC.",R44,2017,739402,0.0004033009192586735
"Advancing a novel portable detection method for cannabis intoxication Intoxication from marijuana (MJ) impairs psychomotor performance and at least doubles the risk of motor vehicle accidents. The ongoing wave of legalization of MJ has brought increasing prevalence of driving while intoxicated with MJ. However, there is no quantitative biologic test that can accurately determine whether an individual is acutely impaired from MJ intoxication. Assays of the primary intoxicating substance in MJ, THC, in body fluids has a high false negative rate as THC is cleared from blood within 15 minutes, long before impairment is resolved. And assays of THC metabolites yield a high false positive rate because clearance of these metabolites can take weeks. Thus there is now no nor is there likely to ever be a test of blood, breath or body fluids that can accurately detect MJ intoxication. In response to this significant knowledge gap, this project aims to develop an accurate, portable method for detection of impairment due to MJ intoxication using functional near-infrared spectroscopy (fNIRS). fNIRS is a non-invasive, safe brain imaging technique that capitalizes on differences in the light absorption spectra of deoxygenated and oxygenated hemoglobin (Hb), that allows the measurement of relative changes in Hb concentration that reflect brain activity. fNIRS can be performed in natural environments at low cost, and thus can be used in real-world settings. In Phase I, we will develop an algorithm for individual-level detection of impairment from THC using fNIRS measurements. To do so, we will assess the effect of oral THC (or placebo) on fNIRS measurements, self-reported intoxication, and impairment as defined by the gold standard field sobriety test conducted by a Drug Recognition Expert (DRE) in 40 healthy MJ users. fNIRS assessments will examine (1) the effect of THC exposure on resting state and task-based activation in the prefrontal cortex, (2) the extent to which impairment in psychomotor functioning with THC administration correlates with THC-induced change in hemodynamic responses detected with fNIRS, and (3) the sensitivity and specificity and area under the ROC curve of fNIRS measurements and field sobriety test determinants of impairment. Milestone: Should machine learning applications to the data generate an algorithm that predicts impairment with >80% accuracy compared with a gold standard field sobriety test, we will proceed to Phase II. In Phase II, we will conduct fNIRS testing in 150 individuals under THC/placebo as in Phase I and in 50 individuals in a THC plus alcohol/placebo condition in order to further refine the algorithm for MJ impairment detection such that fNIRS detection concurs with field sobriety testing with >90% specificity. It is anticipated that this level of specificity could be used in legal definitions of impairment. This will warrant commercialization, which will be followed by prototype development and field testing. An accurate, quantitative, biological test that is user-friendly and enables law enforcement to detect impairment from MJ has the potential to dramatically change practice of law enforcement across the country and the world and thus has enormous commercial potential, as outlined in the Commercialization Plan and in accompanying letters of support. The goal of this project is to develop, test, and refine a method to accurately and reliably detect marijuana (MJ) impairment using a portable, user-friendly, non-invasive, brain-based modality. MJ doubles the chance of motor vehicle accidents, yet, there now exists no valid, biologically based method to detect whether an individual is acutely impaired from MJ. The development of a reliable, quantitative biological marker that enables law enforcement officers to screen individuals whom they suspect are impaired from MJ will have highly significant public health importance and enormous commercial potential.",Advancing a novel portable detection method for cannabis intoxication,9334516,R42DA043977,"['Acute', 'Adult', 'Age', 'Alcohols', 'Algorithms', 'Area', 'Base of the Brain', 'Biochemical', 'Biological', 'Biological Assay', 'Biological Markers', 'Biological Testing', 'Blood', 'Blood Circulation', 'Blood Tests', 'Body Fluids', 'Brain', 'Brain imaging', 'Breath Tests', 'Cannabis', 'Collaborations', 'Comorbidity', 'Country', 'Cross-Over Trials', 'Data', 'Detection', 'Development', 'Devices', 'Dose', 'Double-Blind Method', 'Driving While Intoxicated', 'Drug Kinetics', 'Ensure', 'Environment', 'Equipment', 'Evaluation', 'Formulation', 'Future', 'Goals', 'Gold', 'Hemoglobin', 'Hour', 'Human Resources', 'Imaging Techniques', 'Impairment', 'Individual', 'Intoxication', 'Knowledge', 'Law Enforcement', 'Law Enforcement Officers', 'Legal', 'Letters', 'Licensing', 'Light', 'Machine Learning', 'Marijuana', 'Measurement', 'Methods', 'Modality', 'Near-Infrared Spectroscopy', 'Oral', 'Patient Self-Report', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Placebo Control', 'Placebos', 'Population', 'Prefrontal Cortex', 'Prevalence', 'Property', 'Psychomotor Impairments', 'Psychomotor Performance', 'Public Health', 'ROC Curve', 'Randomized', 'Readiness', 'Rest', 'Risk', 'Sensitivity and Specificity', 'Source', 'Specificity', 'System', 'THC exposure', 'Testing', 'Tetrahydrocannabinol', 'United States', 'Urine', 'Vendor', 'absorption', 'alcohol exposure', 'base', 'behavior test', 'commercialization', 'cost', 'density', 'detector', 'driving under influence', 'drug testing', 'field sobriety tests', 'field study', 'functional disability', 'hemodynamics', 'interest', 'marijuana legalization', 'marijuana use', 'marijuana user', 'novel', 'novel strategies', 'portability', 'prediction algorithm', 'prototype', 'response', 'spectroscopic imaging', 'tool', 'user-friendly', 'vehicular accident']",NIDA,"HIGHLIGHTI, INC",R42,2017,224973,0.008295505591167275
"Real-time prediction of marijuana use & effects of use on cognition in the natural environment ABSTRACT Some young adult marijuana (MJ) users report adverse effects of MJ use on cognition that impact daily functioning, with negative consequences such as injury and fatality due to driving while under the influence of MJ. Research on the effects of MJ use on cognition, however, has produced mixed findings. MJ effects on cognition may depend on factors such as history and current severity of marijuana use, time since last MJ use (including possible MJ withdrawal effects), and gender. This R21 aims to address limitations of existing research by (1) starting to develop an algorithm to predict MJ use using smartphone data in regular/heavy MJ users based on “routine” or “habitual use”, and (2) examining effects of MJ use on cognition using smartphone- based cognitive testing in the natural environment. Development of an algorithm to predict MJ use would facilitate systematic assessment of MJ effects on cognitive functioning through more efficient scheduling of smartphone cognitive testing among regular/heavy MJ users in relation to daily routines. Cognitive testing by smartphone in the natural environment is an innovative method that has shown validity, and permits sampling of cognitive functioning within and across days in relation to MJ use. This project will enroll non-treatment seeking young adult (ages 18-25) MJ users from the community, representing “low”, “regular”, and “heavy” MJ use, with 50% female at each level of use. Participants will complete a baseline lab assessment, 30-day data collection using smartphone and wearable devices (e.g., wristband), and a debriefing interview. Piloting will optimize the protocol and methods for compliance. Smartphones will collect continuously sensed data (e.g., geolocation) for input to an algorithm to predict MJ use in regular/heavy MJ users. This R21 will identify which types of data, available through smartphone, provide optimal detection of routines in MJ use among regular/heavy users. Smartphone cognitive testing will be administered at various times during acute MJ intoxication and various naturalistically occurring lengths of MJ abstinence to examine effects of MJ use on selected aspects of cognitive functioning in daily life. Development of an algorithm to predict MJ use in regular/heavy MJ users based on smartphone data could, for example, facilitate real-time assessment of MJ effects on cognition through improved sampling of cognition in relation to acute and non-acute effects of MJ use. This R21 will provide the foundation for a research program that aims to examine MJ effects on cognitive functioning in vivo, and could support the development of just-in-time intervention to reduce MJ use. This R21 aligns with NIDA's strategic goal of determining consequences of drug use, and cross-cutting themes of highlighting real-world relevance of research and leveraging mobile health technologies to reduce drug use. Project Narrative This exploratory project will initiate development of an algorithm to predict marijuana use using data from smartphone and ecological momentary assessment, and will examine effects of marijuana use on cognitive functioning in the natural environment using innovative smartphone-based cognitive tests. Developing an algorithm to predict marijuana use has substantial healthcare applications, specifically for timely intervention to reduce marijuana use. Further, examining effects of marijuana use on cognitive functioning daily life has important implications for determining possible adverse health consequences associated with marijuana use.",Real-time prediction of marijuana use & effects of use on cognition in the natural environment,9329948,R21DA043181,"['Abstinence', 'Acute', 'Address', 'Adverse effects', 'Age', 'Age of Onset', 'Algorithms', 'Attention', 'Automobile Driving', 'Awareness', 'Behavior', 'Cellular Phone', 'Cognition', 'Communities', 'Data', 'Data Collection', 'Detection', 'Development', 'Devices', 'Drug usage', 'Ecological momentary assessment', 'Enrollment', 'Environment', 'Female', 'Foundations', 'Frequencies', 'Gender', 'Geography', 'Goals', 'Health', 'Health Technology', 'Healthcare', 'Heart Rate', 'Hour', 'Injury', 'Intake', 'Intervention', 'Interview', 'Intoxication', 'Length', 'Life', 'Literature', 'Location', 'Machine Learning', 'Marijuana', 'Metadata', 'Methods', 'Modeling', 'Monitor', 'Moods', 'National Institute of Drug Abuse', 'Participant', 'Patient Self-Report', 'Performance', 'Positioning Attribute', 'Procedures', 'Protocols documentation', 'Recording of previous events', 'Recruitment Activity', 'Relaxation', 'Reporting', 'Research', 'Sampling', 'Schedule', 'Severities', 'Short-Term Memory', 'System', 'Testing', 'Text', 'Time', 'Travel', 'Withdrawal', 'addiction', 'age group', 'base', 'cognitive function', 'cognitive task', 'cognitive testing', 'computer science', 'daily functioning', 'data mining', 'improved', 'in vivo', 'innovation', 'mHealth', 'male', 'marijuana use', 'marijuana use disorder', 'marijuana user', 'marijuana withdrawal', 'mobile computing', 'prediction algorithm', 'programs', 'reduce marijuana use', 'young adult']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2017,234230,-0.014540759437119195
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions Project Summary/Abstract People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. Project Narrative People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9407137,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Persons', 'Phase', 'Phonation', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2017,159267,0.047117288349963204
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9238777,R01EY025332,"['Access to Information', 'Adoption', 'Algorithms', 'American', 'Architecture', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image Enhancement', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'experimental study', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2017,416574,0.02138415910852644
"Understanding Real-Life Falls in Amputees using Mobile Phone Technology DESCRIPTION (provided by applicant): Falls are a significant cause of death and serious injury and result in significant health-care costs. Individuals with a lower extremity amputation due to vascular disease are overwhelmingly elderly (at least 65 years of age) and are at especially high risk of falling. Successful fall prevention strategies depend on understanding how, why, when, and where individuals fall, and what types of falls (e.g., trip, slip, or lateral fll) are likely in a given population. Most studies on falls in amputees to date have relied surveys or questionnaires that are often completed a significant time after the fall and thus rely both on the individual's ability to remember the details of their fall and their willingness to be objective abut how and why they fell. Such approaches are susceptible both to inaccurate memories of the fall and to recall bias-for example, due to embarrassment about falling- and are especially unreliable in the elderly amputees. Mobile phones provide a simple, cost-effective method for detection and characterization of falls. Most available smart phones today have a tri-axial accelerometer, which provides highly accurate fall detection in real-time. Other available applications (or apps) can provide data on activity (running, walking etc.) and environment-such as the weather conditions or population density-that may have contributed to the fall and can pin-point the location of the fall-using GPS technology and highly accurate maps. Mobile phones also have inbuilt data storage and transfer capability, allowing for real-time acquisition and transmission of data. Additionally, mobile phones provide a simple means to contact the individual immediately after a suspected fall to confirm details of the fall (and to ascertain the need for medical assistance). Because mobile phone use is so widespread, there is no stigma associated with carrying such a device, which is likely to lead to high compliance. This study aims to use a mobile phone-based fall detection system in dysvascular amputees to detect falls, characterize the type of fall, analyze environmental conditions that may have contributed to the fall, and determine the longer-term consequences of each type of fall. Data acquired may be used to improve rehabilitation protocols or design better prostheses in order to prevent falls. This technology is ultimately transferrable to many populations with a high risk of falling-for example, the elderly, stroke survivors, or those with other musculoskeletal disorders or disabilities-leading to the design of specific fall prevention strategies for those populations. PUBLIC HEALTH RELEVANCE: Successful fall prevention strategies for the elderly dysvascular amputee population-including better prosthesis design and improved rehabilitation/fall prevention strategies-depend on understanding how, why, when, and where such individuals fall, and what types of fall occur (e.g., trip, slip, or lateral fall). Mobile smartphones with built-in triaxial accelerometers can accurately detect and classify falls, in addition to providing other applications (or 'apps') that can identify contributory environmental conditions (e.g., weather or traffic) or activities (e.g. walking or running) that have may contributed to the fall. Combined with server side analysis of wirelessly transmitted phone data-using machine learning techniques-mobile smartphones provide a simple, portable fall-detection system that generates real-time information on the mechanisms, contributory environmental factors, and consequences of falls in elderly amputees-or in any population at risk of falling.",Understanding Real-Life Falls in Amputees using Mobile Phone Technology,9341305,R01EB019406,"['3-Dimensional', 'Accelerometer', 'Age-Years', 'Algorithms', 'Amputation', 'Amputees', 'Car Phone', 'Cause of Death', 'Cellular Phone', 'Classification', 'Communication', 'Communities', 'Crowding', 'Data', 'Data Collection', 'Data Quality', 'Data Set', 'Data Storage and Retrieval', 'Detection', 'Devices', 'Elderly', 'Emergency department visit', 'Environment', 'Environmental Risk Factor', 'Etiology', 'Event', 'Fall prevention', 'Family', 'Fright', 'Geography', 'Goals', 'Health Care Costs', 'Hospitals', 'Individual', 'Injury', 'Interview', 'Knowledge', 'Laboratories', 'Lateral', 'Lead', 'Life', 'Location', 'Longitudinal Studies', 'Lower Extremity', 'Machine Learning', 'Maps', 'Medical Assistance', 'Medical Care Costs', 'Memory', 'Methods', 'Morbidity - disease rate', 'Musculoskeletal Diseases', 'Outcome', 'Patients', 'Persons', 'Phonation', 'Population', 'Population Density', 'Populations at Risk', 'Prevalence', 'Prevention strategy', 'Prospective cohort', 'Prosthesis', 'Prosthesis Design', 'Protocols documentation', 'Publications', 'Quality of life', 'Questionnaires', 'Rain', 'Real-Time Systems', 'Recovery', 'Rehabilitation therapy', 'Reporting', 'Research', 'Running', 'Side', 'Surveys', 'System', 'Techniques', 'Technology', 'Telephone', 'Time', 'Vascular Diseases', 'Visit', 'Walking', 'Weather', 'Wireless Technology', 'Work', 'base', 'cost effective', 'data exchange', 'design', 'diaries', 'disability', 'fall risk', 'falls', 'fear of falling', 'health care quality', 'high risk', 'high risk population', 'improved', 'improved mobility', 'mortality', 'new technology', 'novel', 'portability', 'prospective', 'public health relevance', 'sensor', 'social stigma', 'stroke survivor', 'willingness']",NIBIB,REHABILITATION INSTITUTE OF CHICAGO D/B/A SHIRLEY RYAN ABILITYLAB,R01,2017,166794,0.0028040063003246223
"Active Power Exoskeleton Device for Spinal Cord Injury ! ABSTRACT Significance: Spinal cord injuries (SCI) cause costly and morbid chronic conditions such as lack of voluntary movement, increased chance of pressure sores, problematic spasticity, loss of bowel, bladder, and sexual function, and more physical impairments which result in a lower quality of life and lack of independence. Approximately 285,000 people in the U.S. have SCI with ~17,000 new patients added each year. Current treatment options include an array of electrical stimulation interventions or high-intensity fitness regimens, but all share one common limitation of being episodic in their treatment delivery. There is a compelling need for a treatment option that can be used daily, improves Activities of Daily Living (ADL’s), allows for in- home rehab and most importantly provides patient independence. Enabling technology exists, often proven in other medical device applications, that can facilitate the design of an upper limb orthotic system to deliver the functional performance required by SCI patients. The OlympEX Medical Actively Powered Exoskeleton (APEX) device can meet the functional movement requirements, will be designed for in-home use and be affordable to the user. Hypothesis: We hypothesize that the APEX orthotic system will achieve clinically meaningful improvement in a SCI patient’s range of motion (ROM) capability to perform ADL’s and to perform upper limb rehab in an in-home setting. Preliminary Work: The APEX orthotic system will be the third generation of orthotic device system to be developed by OlympEX Medical. Generations 1 and 2 are passively powered device systems that have provided the OlympEX design engineering team experience with mechanical apparatus requirements to elevate an upper limb. Likewise, the team has developed capabilities in advanced materials for the body chassis for these devices. The APEX orthotic system evolves from these product platforms to introduce system features in device guidance and user control only obtainable with an actively powered system. Specific Aims: This project entails the APEX hardware/electronic architecture development and the acute pre-clinical evaluation of the system on N=3 patients to evaluate patient safety and device feasibility. In Specific Aim 1 we will design, fabricate and evaluate the APEX mechanical design with electronic architecture. A subset of the APEX functional arm movements will be developed to demonstrate feasibility in performing a limited number of high priority ADL’s. In Specific Aim 2 we will evaluate the performance of this subset of ADL’s on 3 SCI patients. Success criteria will be Pass/Fail as evaluated by clinicians. In addition, an initial patient safety assessment will be completed. Safety factors including discomfort or pain before and after the fitting of the device and no uncontrolled arm movements will be evaluated. Together, these studies will demonstrate the feasibility of APEX device to guide arm movements required to complete ADL’s and therapeutic arm movements in an ambulatory setting. ! Project Narrative Those with cervical spinal cord injuries (SCI) have reduced ability to voluntarily move and use their arms and hands which results in the inability to accomplish many activities of daily living. OlympEX Medical proposes to develop and test an active powered upper extremity exoskeleton to help people with SCI and other neuromuscular conditions with their upper-limb function and rehabilitation. This will improve health, rehabilitative technology, quality of life, and independence for those with SCI.",Active Power Exoskeleton Device for Spinal Cord Injury,9465618,R43HD094440,"['Accelerometer', 'Activities of Daily Living', 'Acute', 'Address', 'Appointment', 'Architecture', 'Articular Range of Motion', 'Bladder', 'Cervical spinal cord injury', 'Chronic', 'Clinical', 'Computer software', 'Computers', 'Data', 'Decubitus ulcer', 'Development', 'Devices', 'Distal', 'Eating', 'Elbow', 'Electric Stimulation', 'Evaluation', 'Feedback', 'Generations', 'Grant', 'Hand', 'Head', 'Health', 'Health Care Costs', 'Home environment', 'Impairment', 'Individual', 'Institutes', 'Intervention', 'Intestines', 'Ions', 'Knowledge', 'Lithium', 'Longevity', 'Machine Learning', 'Mechanics', 'Medical', 'Medical Device', 'Monitor', 'Movement', 'Neuromuscular conditions', 'Orthotic Devices', 'Pain', 'Patients', 'Performance', 'Pilot Projects', 'Quality of life', 'Recruitment Activity', 'Regimen', 'Rehabilitation therapy', 'Safety', 'Sex Functioning', 'Shoulder', 'Source', 'Spastic', 'Spinal cord injury', 'Spinal cord injury patients', 'System', 'Technology', 'Testing', 'Textiles', 'Therapeutic', 'Upper Extremity', 'Voice', 'Weight Gain', 'Wheelchairs', 'Work', 'Wrist', 'arm', 'arm movement', 'cost', 'design', 'effective therapy', 'engineering design', 'exoskeleton', 'experience', 'fitness', 'improved', 'light weight', 'motor recovery', 'motor rehabilitation', 'operation', 'orthotics', 'patient safety', 'phase 2 study', 'phrases', 'pre-clinical', 'prevent', 'prototype', 'research clinical testing', 'spasticity', 'success', 'volunteer']",NICHD,"ABILITECH MEDICAL, INC.",R43,2017,239802,-0.017803487380675817
"Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility Innovative Design Labs (IDL) proposes to create a system to improve the mobility and control of exoskeletons. Recent research has found that 3.86 million Americans require wheelchairs and the number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk, thus providing a way to more fully reintegrate these individuals into society. Our proposal seeks to address one of the hurdles limiting the widespread adoption of exoskeletons in the home and community—the inability of the user to dynamically control gait parameters. This concept has the potential to significantly change the way exoskeletons work and facilitate their adoption into the market. Hypothesis: We hypothesize that the proposed solution will provide users a practical way to adjust their suit’s gait to precisely achieve their navigational goals. Specific Aims: Phase I: 1) Build a prototype and Perform Preliminary Laboratory Testing; 2) Develop and Benchmark Algorithms; and 3) Perform Pilot Human Study of Prototype with Exoskeleton Subjects. Phase II: 1) Develop Customized, Production-Ready Hardware and Firmware 2) Integrate with Exoskeleton Control System; and 3) Perform an evaluation of the system through human study testing. Recent research has found that 3.86 million Americans require wheelchairs and that number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk thereby providing a way to more fully reintegrate these individuals into society.",Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility,9474743,R44AG053890,"['Address', 'Adoption', 'Algorithm Design', 'Algorithms', 'American', 'Benchmarking', 'Bionics', 'Caregivers', 'Chicago', 'Clinical', 'Collaborations', 'Communities', 'Community Participation', 'Computational algorithm', 'Computer Vision Systems', 'Crutches', 'Custom', 'Dependence', 'Devices', 'Electrical Engineering', 'Emotional', 'Environment', 'Evaluation', 'Exercise', 'Eye', 'Family', 'Feedback', 'Freedom', 'Friends', 'Gait', 'Goals', 'Health', 'Height', 'Home environment', 'Hospitals', 'Human', 'Image', 'Impairment', 'Individual', 'Industry', 'Institutes', 'Laboratories', 'Length', 'Location', 'Medical', 'Methods', 'Motion', 'Patients', 'Performance', 'Phase', 'Population', 'Process', 'Production', 'Quality of life', 'Ramp', 'Rehabilitation Centers', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Safety', 'Small Business Innovation Research Grant', 'Social isolation', 'Societies', 'Software Engineering', 'System', 'Technology', 'Testing', 'Uncertainty', 'Vision', 'Walking', 'Wheelchairs', 'Work', 'commercialization', 'design', 'exoskeleton', 'experience', 'human study', 'image processing', 'improved', 'improved mobility', 'innovation', 'insight', 'member', 'product development', 'prototype', 'rehabilitation technology', 'robot exoskeleton', 'usability']",NIA,"INNOVATIVE DESIGN LABS, INC.",R44,2017,562410,0.0006945255712555243
"Independent Exoskeleton-Use through Robust Stand-to-Sit Safety Project Summary/Abstract Innovative Design Labs (IDL) proposes to create a system for the sensing and control of stand-to-sit motions of a wearable bionics suit. Currently 5.6 million people in the US have impaired mobility from a number of different causes. The primary means of mobility for many of these patients is the wheelchair as it has been for most of the last 50 years. Despite all the benefits introduced by widespread use of the wheelchair, it remains a less than ideal mobility solution. Exoskeleton suits have the potential to empower individuals with impaired mobility with an alternative to wheelchairs that allows them to stand up and walk independently within their home and community has the potential to more fully reintegrate these individuals into society while also further improving their health and quality of life. For exoskeletons to gain acceptance in every-day independent home and community use, many control and safety related functionalities still need to be addressed. Our proposal seeks to address one of the gaps in allowing for independent use of exoskeletons in the home and community, namely, functionality to transition from standing to sitting in a safe manner. The proposed work will provide exoskeleton users with the new ability to independently sit down without assistance and confidence in being able to do so without falling and risking possible injuries. It aims to significantly change the way exoskeletons work thereby facilitating their adoption into the market and directly impacting the lives of individuals with disabilities. Project Narrative Exoskeletons can provide patients with SCI, stroke, and other types of impaired mobility access to extended duration, gravity dependent ambulation that can directly combat the risks associated with physical deconditioning. There are benefits for exoskeleton-use to gain widespread acceptance in every-day, independent home and community settings. Currently, exoskeletons are not approved for independent-use because functionalities like transferring from standing-to-sitting requires continuous assistance from caregivers.",Independent Exoskeleton-Use through Robust Stand-to-Sit Safety,9409715,R44AG057267,"['Activities of Daily Living', 'Address', 'Adoption', 'Algorithmic Software', 'Algorithms', 'Area', 'Bionics', 'Caliber', 'Caregivers', 'Chicago', 'Collaborations', 'Communities', 'Computer Vision Systems', 'Country', 'Custom', 'Development', 'Disabled Persons', 'Emerging Technologies', 'Engineering', 'Environment', 'Evaluation', 'Fall injury', 'Feedback', 'Force of Gravity', 'Gait', 'Health', 'Height', 'Home environment', 'Hospitals', 'Imaging technology', 'Impairment', 'Individual', 'Injury', 'Institutes', 'Letters', 'Mechanics', 'Metric System', 'Motion', 'Patients', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Production', 'Quality of life', 'Rehabilitation Centers', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Rest', 'Risk', 'Robotics', 'Safety', 'Scientist', 'Small Business Innovation Research Grant', 'Societies', 'Stroke', 'Surface', 'System', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Validation', 'Walking', 'Wheelchairs', 'Work', 'blind', 'combat', 'community setting', 'critical period', 'deconditioning', 'design', 'exoskeleton', 'experience', 'fall risk', 'falls', 'human study', 'improved', 'innovation', 'member', 'prototype', 'rehabilitation technology', 'robot exoskeleton']",NIA,"INNOVATIVE DESIGN LABS, INC.",R44,2017,653843,-0.00922891422129273
"Psychophysics of Reading - Normal and Low Vision DESCRIPTION (provided by applicant):  Psychophysics of Reading - Normal and Low Vision Abstract Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  Difficulty in accessing print imposes obstacles to education, employment, social interaction and recreation.  The ongoing transition to the production and distribution of digital documents brings about new opportunities for people with visual impairment.  Digital documents on computers and mobile devices permit easy manipulation of print size, contrast polarity, font, page layout and other attributes of text.  In short, we now hae unprecedented opportunities to adapt text format to meet the needs of visually impaired readers.  In recent years, our laboratory and others in the vision-science community have made major strides in understanding the impact of different forms of low vision on reading, and the dependence of reading performance on key text properties such as character size and contrast.  But innovations in reading technology have outstripped our knowledge about low-vision reading.  A major gap still exists in translating these laboratory findings into methods for customizing text displays for people with low vision.  The broad aim of the current proposal is to apply our knowledge about the impact of vision impairment on reading to provide tools and methods for enhancing reading accessibility in the modern world of digital reading technology.  Our research plan has three specific goals:   1) To develop and validate an electronic version of the MNREAD test of reading vision, to extend this technology to important text variables in addition to print size, and to develop methods for customizing the selection of text properties for low-vision readers.  MNREAD is the most widely used test of reading in vision research and was originally developed in our laboratory with NIH support.  2) To investigate the ecology of low-vision reading in order to better understand how modern technologies, such as iPad and Kindle are being used by people with low vision.  We plan to evaluate the feasibility of using internet methods to survey low-vision individuals concerning their reading behavior and goals, and of collecting approximate measures of visual function over the internet.  We also plan to develop an ""accessibility checker"" to help low-vision computer users and their families to evaluate the accessibility of specific text displays.  3) To enhance reading accessibility by developing methods for enlarging the visual span (the number of adjacent letters that can be recognized without moving the eyes).  A reduced visual span is thought to be a major factor limiting reading in low vision, especially for people with central-field loss from macular degeneration.  We have already demonstrated methods for enlarging the visual span in peripheral vision.  We plan to develop a more effective perceptual training method for enlarging the visual span, with the goal of improving reading performance for people with central-vision loss. PUBLIC HEALTH RELEVANCE:  Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  The ongoing transition to the use of digital documents on computers and mobile devices brings about new opportunities for customizing text for people with visual impairment.  We propose to apply findings from basic vision science on low vision and reading to develop tools and methods for enhancing reading accessibility for digital text.",Psychophysics of Reading - Normal and Low Vision,9292314,R01EY002934,"['American', 'Attention', 'Auditory', 'Behavior', 'Blindness', 'Books', 'Caring', 'Central Scotomas', 'Characteristics', 'Clinical Research', 'Color', 'Communities', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Custom', 'Dependence', 'Development', 'Devices', 'Ecology', 'Education', 'Employment', 'Eye', 'Family', 'Galaxy', 'Goals', 'Government', 'Guidelines', 'Habits', 'Health', 'Individual', 'Internet', 'Knowledge', 'Laboratories', 'Laboratory Finding', 'Leg', 'Length', 'Letters', 'Life', 'Macular degeneration', 'Mainstreaming', 'Maps', 'Marshal', 'Measures', 'Methods', 'Modernization', 'Optics', 'Paper', 'Participant', 'Patients', 'Perceptual learning', 'Performance', 'Peripheral', 'Play', 'Policies', 'Printing', 'Production', 'Progress Reports', 'Property', 'Protocols documentation', 'Psychophysics', 'Reader', 'Reading', 'Recreation', 'Reporting', 'Research', 'Resources', 'Role', 'Self-Help Devices', 'Social Interaction', 'Surveys', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Uncertainty', 'United States National Institutes of Health', 'Validation', 'Vision', 'Vision research', 'Visual', 'Visual impairment', 'Work', 'analog', 'base', 'design', 'digital', 'essays', 'handheld mobile device', 'improved', 'innovation', 'invention', 'large print', 'literate', 'public health relevance', 'reading difficulties', 'sound', 'symposium', 'tool', 'vision science', 'web-accessible']",NEI,UNIVERSITY OF MINNESOTA,R01,2017,364423,0.03841125352041422
"Designing Visually Accessible Spaces DESCRIPTION (provided by applicant):  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals, including those with physical disabilities and to enhance safety for older people and others who may need to operate under low lighting and other visually challenging conditions.  We plan to develop a computer-based design tool enabling architectural design professionals to assess the visual accessibility of a wide range of environments (such as a hotel lobby, subway station, or eye-clinic reception area).  This tool will simulate such environments with sufficient accuracy to predict the visibility of key landmarks or hazards, such as steps or benches, for different levels and types of low vision, and for spaces varying in lighting, surface properties and geometric arrangement.  Our project addresses one of the National Eye Institute's program objectives:  ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments."" Our research plan has three specific goals:  1) Empirical:  determine factors that influence low-vision accessibility related to hazard detection and navigation in real-world spaces.  2) Computational:  develop working models to predict low vision visibility and navigability in real-world spaces.  3) Deployment:  translate findings from basic vision science and clinical low vision into much needed industrial usage by producing a set of open source software modules to enhance architectural and lighting design for visual accessibility.  The key scientific personnel in our partnership come from three institutions:  University of Minnesota -- Gordon Legge and Daniel Kersten; University of Utah -- William Thompson and Sarah Creem-Regehr; and Indiana University -- Robert Shakespeare.  This interdisciplinary team has expertise in the necessary areas required for programmatic research on visual accessibility -- empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), computer graphics and photometrically accurate rendering (Thompson & Shakespeare) and architectural lighting design (Shakespeare).  We have collaborative arrangements with additional architectural design professionals who will participate in the translation of our research and development into practice. PUBLIC HEALTH RELEVANCE:  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our BRP partnership, with interdisciplinary expertise in vision science, computer science and lighting design, plans to develop computer-based tools enabling architecture professionals to assess the visual accessibility of their designs.",Designing Visually Accessible Spaces,9251284,R01EY017835,"['Address', 'Age', 'American', 'Architecture', 'Area', 'Biological Models', 'Blindness', 'Characteristics', 'Clinic', 'Clinical', 'Complement', 'Complex', 'Computer Analysis', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Contrast Sensitivity', 'Cues', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Disorientation', 'Distance Perception', 'Effectiveness', 'Environment', 'Evaluation', 'Eye', 'Fall injury', 'Geometry', 'Glare', 'Goals', 'Grant', 'Guidelines', 'Height', 'Human', 'Human Resources', 'Indiana', 'Individual', 'Industrialization', 'Institution', 'Investigation', 'Judgment', 'Learning', 'Light', 'Lighting', 'Location', 'Minnesota', 'Modeling', 'Modification', 'National Eye Institute', 'Nature', 'Optics', 'Perception', 'Peripheral', 'Phase', 'Photometry', 'Physical environment', 'Physically Handicapped', 'Positioning Attribute', 'Process', 'Production', 'Property', 'Research', 'Risk', 'Safety', 'Shapes', 'Source', 'Specific qualifier value', 'Stimulus', 'Structure', 'Subway', 'Surface', 'Surface Properties', 'System', 'Test Result', 'Testing', 'Translating', 'Translations', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual Acuity', 'Visual Fields', 'Visual impairment', 'Work', 'base', 'computer science', 'design', 'disability', 'hazard', 'imaging system', 'improved', 'knowledge base', 'luminance', 'novel strategies', 'open source', 'programs', 'public health relevance', 'research and development', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2017,580671,0.06011487784224445
"Computational Image Analysis for Cellular and Developmental Biology DESCRIPTION (provided by applicant): This proposal requests support for an intensive ten-day course on Computational Image Analysis for Cellular and Developmental Biology. The course is designed for graduate students and postdoctoral fellows, and takes place at the Marine Biological Laboratory in Woods Hole, MA. The course is the first of its kind, giving students formal training in computer vision for the specific analysis of cell and developmental biology image data. Building strong foundations in this topic is critical for pushing cell and developmental biology forward, as imaging has become more and more an indispensable tool in these fields. The course covers the fundamentals of computer vision, taking the students through the sequence of low-, intermediate-, and high-level computer vision tasks that are required to solve image analysis problems in quantitative cellular and developmental biology. The curriculum starts with filtering, thresholding and edge/line/generic feature detection, followed by more sophisticated detection algorithms that employ model fitting. After this introductory block to low-level computer vision tasks, the course moves on to intermediate and higher-level tasks, including object association in space and time (such as tracking) and machine learning tools for phenotype classification. Each topic is covered first by a lecture, generally taught by one of the four core faculty, followed by a 3-4 hour computer programming session where students immediately implement the concepts they learn. There are usually two lectures + computer labs per day. Most programming exercises are individual, giving each student the opportunity to ""get their hands dirty,"" while two are team projects allowing the students to also learn and practice methods of code sharing. Over the course's ten days there are three guest lectures by leading researchers in the fields of biological imaging and computer vision, each followed by in-depth discussion, as well as research talks given by the students, faculty and teaching assistants. With this, the core lectures and labs teach the students the fundamentals of computer vision in a logical, continuous manner, the guest lecturers introduce the students to exciting new challenges in imaging and image analysis, and the student/faculty research talks encourage communication between all course participants and give especially the students the opportunity to reflect on how the course can help them with their research. PUBLIC HEALTH RELEVANCE:  Imaging has become an indispensable tool in cellular and developmental biology research, but without rigorous, quantitative image analysis it cannot achieve its full potential. This proposal requests support for a new course that will fill the voidof education in computer vision as applied to cellular and developmental biology. The curriculum incorporates a carefully balanced mixture of lectures and associated programming exercises. We have given a pilot course once in October 2010, with very positive reviews from the students and their advisers.",Computational Image Analysis for Cellular and Developmental Biology,9021663,R25GM103792,"['Address', 'Algorithms', 'Biological', 'Cellular biology', 'Classification', 'Code', 'Collection', 'Communication', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Data', 'Detection', 'Development', 'Developmental Biology', 'Developmental Cell Biology', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Equilibrium', 'Exercise', 'Faculty', 'Foundations', 'Future', 'Generic Drugs', 'Goals', 'Hand', 'Health', 'Home environment', 'Hour', 'Image', 'Image Analysis', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Marines', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Phenotype', 'Postdoctoral Fellow', 'Request for Proposals', 'Research', 'Research Personnel', 'Seeds', 'Software Design', 'Solid', 'Students', 'Time', 'Training', 'Wood material', 'computer program', 'design', 'exercise program', 'faculty research', 'graduate student', 'lecturer', 'lectures', 'personalized approach', 'programs', 'quantitative imaging', 'student training', 'teaching assistant', 'tool']",NIGMS,MARINE BIOLOGICAL LABORATORY,R25,2016,59383,0.007972048264027903
"Micromachined microphones with in-plane and out-of-plane directivity Project Summary We aim to introduce to the hearing-assistive device industry directional microphones with high signal-to-noise ratio, and the first commercialized microphone that combines all three axes of acoustic pressure gradient onto a single silicon chip. We expect the technology to empower the signal-processing community with a new tool which, when used in conjunction with a conventional omnidirectional microphone, will facilitate new features like ultra-sharp directionality adaptable in real-time by the user and/or artificial intelligence algorithms which scan for desired inputs while filtering out unwanted noise. Directional sensing and the ability to filter out undesirable background acoustic noise are important for those with hearing impairments. Hearing impairment is associated with a loss of fidelity to quiet sounds, while the threshold of pain remains the same. As such, hearing impairment causes a loss of dynamic range or “window” of detectable sound amplitudes. Directional sensing enables preferentially amplifying desired sounds without amplifying background noise. As the first step, we aim to accelerate the commercialization of recently introduced biologically- inspired “rocking” style microphones by synthesizing these designs with integrated, robust piezoelectric readout which is ideal for addressing the low-power, small-size, and high levels of integration required of the hearing-aid industry. Previous work in this field using laboratory prototypes and optical readout have demonstrated the merits of the biologically-inspired sensing approach (i.e. a simultaneous 20-dB SNR improvement and 10x reduction size improvement beyond what is achievable with present-day hearing-aid or MEMS microphones). By synthesizing a piezoelectric embodiment as an alternative to optical readout, we aim to accelerate through many of the commercialization challenges so that an impact to the hearing device industry can be made. Further, the proposed readout is better adapted towards integrating multiple microphones in the same silicon chip. We aim to integrate a microphone with both in-plane axis of directivity with an out-of-plane directional design to form a complete  -axis pressure gradient sensor. Project Narrative Studies show that today 2% of Americans wear a hearing aid, whereas at least 10% of Americans could benefit from a hearing assistive device. The major reason for this gap is patient dissatisfaction. Hearing-aid wearers suffer from what is known as the “cocktail party” effect. When the gain is turned up to hear the person speaking across from you, noises in the background are equally amplified – making every scenario sound like a cocktail party. This research aims to make a positive, long-term improvement to hearing-aid patient satisfaction by making commercially available directional microphones with high fidelity.",Micromachined microphones with in-plane and out-of-plane directivity,9098683,R44DC013746,"['Acoustics', 'Address', 'Algorithms', 'American', 'Artificial Intelligence', 'Client satisfaction', 'Communities', 'Devices', 'Environment', 'Hearing', 'Hearing Aids', 'Industry', 'Laboratories', 'Noise', 'Optics', 'Pain Threshold', 'Patients', 'Persons', 'Research', 'Scanning', 'Self-Help Devices', 'Signal Transduction', 'Silicon', 'Small Business Innovation Research Grant', 'Techniques', 'Technology', 'Time', 'Work', 'commercialization', 'design', 'empowered', 'hearing impairment', 'pressure', 'prototype', 'sensor', 'signal processing', 'sound', 'tool']",NIDCD,"SILICON AUDIO, INC.",R44,2016,490980,0.006237534536885459
"Crowd-Sourced Annotation of Longitudinal Sensor Data to Enhance Data-Driven Precision Medicine for Behavioral Health ﻿    DESCRIPTION (provided by applicant): Longitudinal sensor data collected passively from mobile phones and other wearable sensors will transform behavioral science by allowing researchers to use ""big data,"" but at the person-level, to understand how behavior and related environmental exposures impact health outcomes. Computers will analyze individual-level data streams to permit unprecedented, individual-level precision in research and intervention. This type of precision medicine enables targeting of science and medicine to a particular individual's genetic makeup, past and current situation, and behavioral health exposures. Mobile phones, smartwatches, and common fitness devices are already capable of generating rich data on behavior, but developing algorithms to interpret that raw data using the latest machine learning algorithms requires practical strategies to annotate large datasets. We propose to develop and test the feasibility and usability of a mobile and online crowdsource-based system for cleaning and annotating behavioral data collected from motion sensors, mobile phones, and other mobile devices. Our goal is to demonstrate how individuals playing mobile and online games - the ""crowd"" - can collectively, affordably, and incrementally clean and add important metadata to raw sensor data that has been passively collected from individuals, similar to that from population-scale surveillance studies (e.g., the National Health and Nutrition Examination Survey (NHANES) and UK Biobank) and those planned for studies such as the White House's Precision Medicine Initiative. The game-playing crowd will thereby dramatically improve the utility of the datasets collected for a variety of scientific studies. We will validate our prototye system on datasets collected from motion monitors used to study physical activity, sedentary behavior, and sleep, but we will demonstrate how the system could be extended for use on the increasingly rich datasets that are being collected with mobile devices and that include not only motion data, but also sensor data on location, light, audio, and person-to-person proximity. We will then refine the system, foster a community of crowd game players interested in citizen science, and release the source code to the system as an open source project so that other researchers can adapt the technique for their own work.         PUBLIC HEALTH RELEVANCE: Longitudinal sensor data collected passively from wearable activity monitors and mobile phones will transform behavioral science by allowing researchers to use ""big data,"" but at the person-level, to understand how behavior and related environmental exposures impact health outcomes and personalize health intervention and research. We propose to develop and test a system that permits typical mobile application game players to help scientists improve this type of data, by adding additional annotations that enrich the data, making it more useful for behavioral science and more amenable to automatic processing. This will help researchers to better understand how individual-level behaviors relate to health outcomes in current research studies that collect personal-level sensor data such as NHANES and the Women's Health Study, and future big data ventures such as the new Precision Medicine Initiative.        ",Crowd-Sourced Annotation of Longitudinal Sensor Data to Enhance Data-Driven Precision Medicine for Behavioral Health,9078547,UH2EB024407,"['Accelerometer', 'Algorithms', 'American', 'Behavior', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Car Phone', 'Classification', 'Cohort Studies', 'Communities', 'Complex', 'Computers', 'Crowding', 'Data', 'Data Collection', 'Data Quality', 'Data Science', 'Data Set', 'Devices', 'Environmental Exposure', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Future', 'Genomics', 'Goals', 'Health', 'Housing', 'Human', 'Imagery', 'Individual', 'Intervention', 'Intervention Studies', 'Interview', 'Label', 'Lead', 'Light', 'Location', 'Machine Learning', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Monitor', 'Motion', 'National Health and Nutrition Examination Survey', 'Outcome', 'Output', 'Participant', 'Pathway Analysis', 'Pattern', 'Performance', 'Persons', 'Physical activity', 'Play', 'Population', 'Precision Medicine Initiative', 'Process', 'Proteomics', 'Research', 'Research Personnel', 'Risk Behaviors', 'Sampling', 'Science', 'Scientist', 'Sleep', 'Source Code', 'Stream', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Women&apos', 's Health', 'Work', 'base', 'behavioral health', 'biobank', 'citizen science', 'crowdsourcing', 'design', 'fitness', 'genetic makeup', 'handheld mobile device', 'improved', 'innovation', 'insight', 'instrument', 'interest', 'learning network', 'meetings', 'metabolomics', 'mobile application', 'mobile computing', 'novel', 'open source', 'precision medicine', 'prototype', 'public health relevance', 'research study', 'sedentary lifestyle', 'sensor', 'surveillance study', 'temporal measurement', 'tool', 'ubiquitous computing', 'usability']",NIBIB,NORTHEASTERN UNIVERSITY,UH2,2016,299438,-0.01508044226639171
"Capti Screen Reading Assistant for Goal Directed Web Browsing ﻿    DESCRIPTION (provided by applicant): Web browsing with assistive technologies such as screen readers and magnifiers can often be a frustrating and challenging experience for people with vision impairments, because it entails a lot of searching for content, forms, and links that are required for doing online tasks such as shopping, bill-payment, reservations, etc. This SBIR Phase II project will build on Phase I results and will continue the development and eventual deployment of Capti Screen Reading Assistant - a next-generation assistive technology, enabling goal- directed web browsing for people with visual impairments. With Capti Assistant, users will be able to stay focused on their high-level browsing goals that are expressed in natural language (spoken or typed). The Assistant will lead the users step-by-step towards the fulfillment of these goals by offering suggestions on what action to take at every step of the way and automatically executing the chosen action on behalf of the user. Suggested actions will include operations such as form filling, activating controls (e.g., clicking buttons and links), et. Capti Assistant will dramatically reduce the time spent by people with visual impairments on performing tasks online. The Assistant will significantly improve the speed and efficiency with which they can interact with the Web, thereby, making people with disabilities more productive in today's web-based economy. Given a user browsing goal, expressed in a natural-language form, Capti Assistant will utilize a predictive model to guide the user toward the goal. The unique aspect of the Assistant is that its suggestions will be automatically learned from the user's own history of browsing actions and commands, as well as from the user's demonstration of how to accomplish browsing tasks that have not been done before. The Assistant will process user commands and present the suggested browsing actions to the user on demand, giving the user a choice between following the suggestions or continue browsing normally without accepting the suggestions. The functionality offered by the Assistant will go far beyond popular personal assistant applications such as Siri, which have not been designed specifically for people with vision impairments, and which cannot be used for ad hocweb browsing. Capti Assistant will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the Web. For them, the Assistant will usher in a new era of independence and employability in our global web-based economy. Thus, from a broader perspective, goal- directed browsing will exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", i.e. anyone should be able to reap the benefits of the Web without being constrained by any disability.         PUBLIC HEALTH RELEVANCE: This SBIR Project seeks to do Research and Development on goal-directed web browsing - the next generation accessible technology that will empower people with vision impairments to stay focused on their high-level browsing goals, while the browser will do low-level operations (such as clicking on links and filling forms) necessary to fulfill these goals. Goal-directed browsing will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the web, thus improving independence and employability of the former in our global Web-based economy. From a broader perspective, goal-directed browsing will facilitate rehabilitation of people with disabilities and exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", enabling anyone to reap the benefits of the Web without being constrained by any disability.        ",Capti Screen Reading Assistant for Goal Directed Web Browsing,9048176,R44EY021962,"['Automation', 'Blindness', 'Budgets', 'Businesses', 'Communication', 'Computer software', 'Computers', 'Data', 'Development', 'Disabled Persons', 'Ensure', 'Environment', 'Evaluation', 'FarGo', 'Focus Groups', 'Generations', 'Goals', 'Human Resources', 'In Situ', 'Information Retrieval', 'Internet', 'Internships', 'Laboratory Study', 'Lead', 'Learning', 'Legal patent', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Marketing', 'Mining', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Pattern', 'Phase', 'Probability', 'Process', 'Productivity', 'Publications', 'Publishing', 'Reader', 'Reading', 'Recording of previous events', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Reservations', 'Resources', 'Schedule', 'Scheme', 'Seasons', 'Self-Help Devices', 'Small Business Innovation Research Grant', 'Speed', 'Suggestion', 'System', 'Technology', 'Time', 'Universities', 'Vision', 'Visual impairment', 'Work', 'base', 'collaborative environment', 'commercial application', 'commercialization', 'computer human interaction', 'design', 'disability', 'educational atmosphere', 'empowered', 'experience', 'improved', 'innovation', 'member', 'natural language', 'next generation', 'novel strategies', 'operation', 'payment', 'predictive modeling', 'public health relevance', 'quality assurance', 'query optimization', 'research and development', 'response', 'success', 'technological innovation', 'tool', 'usability', 'web page', 'web-accessible']",NEI,"CHARMTECH LABS, LLC",R44,2016,500000,0.043926339386031944
"Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research ﻿    DESCRIPTION (provided by applicant): This Phase II project aims to continue development of a commercial quality, innovative cloud hosted information management system, called Climb 2.0(tm) that will increase laboratory efficiency and provide improved capabilities for research laboratories. Climb is designed to offer integrated laboratory process management modules that include mobile communications tools data monitoring and alert systems, and integrated access to Microsoft Azure Cloud(tm) Machine Learning and Stream Analytics services. Initially, Climb 2.0 will target animal model research laboratories; however, the core of the platform is designed to be adaptable to nearly any research type or related industry. Current research information management systems are primarily designed as record-keeping tools with little or no direct focus on laboratory efficiency or in enhancing value of the research data. They also do not leverage emergent mobile device technologies, social media frameworks, and data analysis and storage capabilities of cloud computing. Many laboratories still use paper as their primary recording system. Paper data logging is then followed by secondary data entry into a laboratory database. These systems are error prone, time consuming and lead to laboratory databases with significant time lags between data acquisition and data entry. Moreover, they do not recognized cumulative data relationships, which may identify important trends, and researchers often miss windows of opportunity to take action on time-sensitive events. In Phase I, RockStep Solutions demonstrated feasibility of an innovative Cloud Information Management Bundle system, Climb, which will increase efficiency and improve capabilities in animal model data management. During Phase I, a beta version of Climb was successfully developed and tested against strict performance metrics as a proof of concept. We successfully built a prototype with working interfaces that integrates real-time communication technologies with media capabilities of mobile devices. Phase II proposes four specific Aims: 1) Develop the technology infrastructure to support the secure and scalable Software as a Service (SaaS) deployment of Climb for enterprise commercial release; 2) Develop and extend the Phase-I prototype Data Monitoring and Messaging System (DMMS) into a platform ready for production use; 3) Extend Climb's DMMS adding a Stream Analytics engine to support Internet of Things (IoT) devices and streaming media; 4) Deploy a beta release of Climb at partner research labs, test and refine the product for final commercialization. To ensure Climb is developed with functionality and tools relevant to research organizations, RockStep Solutions has established collaborations with key beta sites to test all of the major functionality developed in this proposal. IMPACT: By leveraging emergent technologies and cloud computing, Climb offers several advantages: 1) enables real-time communications using familiar tools among members of research groups; 2) reduces the risk of experimental setbacks, and 3) enables complex experiments to be conducted efficiently.         PUBLIC HEALTH RELEVANCE: The NIH Invests approximately $12 billion each year in animal model research that is central to both understanding basic biological processes and for developing applications directly related to improving human health. More cost-effective husbandry and colony management techniques, equipment, and new approaches to improve laboratory animal welfare and assure efficient and appropriate research use (e.g., through cost savings and reduced animal burden) are high priorities for NIH. RockStep Solutions proposes to meet this need by integrating existing and emergent software and communication technologies to create a novel solution, Climb 2.0, for research animal data management. Climb 2.0 extracts immediate value of data in animal research laboratories and extends the database into a multimedia communication network, thus enabling science that would be impractical to conduct with traditional methods.        ",Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research,9138555,R44GM112206,"['Address', 'Algorithms', 'Animal Experimentation', 'Animal Model', 'Animal Welfare', 'Animals', 'Biological Process', 'Biomedical Research', 'Clinical Laboratory Information Systems', 'Cloud Computing', 'Collaborations', 'Communication', 'Communication Tools', 'Complex', 'Computer software', 'Computers', 'Consumption', 'Cost Savings', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Security', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Devices', 'Emerging Technologies', 'Ensure', 'Equipment', 'Equipment and supply inventories', 'Event', 'Feedback', 'Future', 'Goals', 'Health', 'Health system', 'Human', 'Industry', 'Information Management', 'Internet', 'Investments', 'Laboratories', 'Laboratory Animals', 'Laboratory Research', 'Lead', 'Machine Learning', 'Management Information Systems', 'Methods', 'Monitor', 'Motion', 'Multimedia', 'Notification', 'Paper', 'Performance', 'Phase', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rest', 'Risk', 'Science', 'Secure', 'Services', 'Site', 'Stream', 'System', 'Techniques', 'Technology', 'Testing', 'The Jackson Laboratory', 'Time', 'United States National Institutes of Health', 'Work', 'analytical tool', 'animal data', 'base', 'cloud based', 'commercialization', 'computerized data processing', 'cost', 'cost effective', 'data acquisition', 'data management', 'design', 'encryption', 'good laboratory practice', 'graphical user interface', 'handheld mobile device', 'improved', 'innovation', 'laptop', 'meetings', 'member', 'novel', 'novel strategies', 'predictive modeling', 'programs', 'prototype', 'public health relevance', 'research study', 'social media', 'software as a service', 'success', 'time interval', 'tool', 'trend', 'usability']",NIGMS,"ROCKSTEP SOLUTIONS, INC.",R44,2016,750063,0.0004033009192586735
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering. PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,9336584,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Health', 'Healthcare', 'High School Student', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'rehabilitation engineering', 'robot rehabilitation', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2016,28000,0.06226317079937248
"NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired PROJECT SUMMARY (See instructions): The objective of the proposed research is to develop new technology for a Wearable Robotic Object Manipulation Aid (W-ROMA) for the visually impaired. The W-ROMA is a hand-worn assistive device that provides assistance to a visually impaired individual in effectively grasping an object. Thanks to the onboard computer vision methods, the W-ROMA is capable of detecting a target object, determining the hand-object misalignment, and conveying to the wearer, via natural human-device interfaces, the desired hand motion for hand-object alignment. The W-ROMA will contribute to the independent lives of the visually impaired in twofold: First, it helps the visually impaired with independent travel by enabling them to identify a movable obstacle and manipulate the obstacle to make a passage. Second, it assists the visually impaired in effectively grasping an object for non-navigational purpose. The PIs will involve graduate, undergraduate and high school students in the project and use the proposed project activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision and human-robot interaction methods that support accurate and effective object grasping for the visually impaired for their independent daily lives. These methods include: (1) a new real-time object recognition method; (2) an innovative hand-object alignment mechanism; (3) a novel hybrid tactile display system for object shape rendering; and (4) a computationally efficient device localization method. The proposed solutions can be encapsulated in a hand-worn robotic device. The W-ROMA will provide new co-robotic functions for the visually impaired. The PIs have performed proof of concept studies for the computer vision and tactile display methods and the results are promising. The broader impacts include: (1) the research will positively impact the large visually impaired community; (2) the proposed methods can be applied to other small robotic systems that have a wide range of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the PI's university and train graduate and undergraduate students for their future careers in science and engineering. RELEVANCE (See instructions): The project addresses a growing public health care issue--visual impairment. The research fits well into the NEI's Low Vision and Blindness Rehabilitation program that supports development of new technologies for minimizing the impact of visual impairment. The project addresses the NEI's mission by developing new assistive technology that will help the visually impaired to maintain a higher quality of life.",NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired,9136188,R01EY026275,"['Address', 'Blindness', 'Canes', 'Code', 'Communities', 'Companions', 'Computer Vision Systems', 'Data', 'Detection', 'Development', 'Devices', 'Encapsulated', 'Engineering', 'Environment', 'Funding', 'Future', 'Goals', 'Grant', 'Graph', 'Hand', 'Healthcare', 'High School Student', 'Human', 'Hybrids', 'Image', 'Independent Living', 'Individual', 'Instruction', 'Law Enforcement', 'Maps', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'Motion', 'Movement', 'National Institute of Biomedical Imaging and Bioengineering', 'Performance', 'Persons', 'Polymers', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robotics', 'Rotation', 'Scheme', 'Science', 'Self-Help Devices', 'Solid', 'Speech', 'Speed', 'Students', 'System', 'Tactile', 'Testing', 'Thumb structure', 'Time', 'Training', 'Translations', 'Travel', 'United States National Institutes of Health', 'Universities', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'base', 'blind', 'career', 'design', 'experience', 'graduate student', 'grasp', 'human-robot interaction', 'improved', 'innovation', 'new technology', 'novel', 'object recognition', 'object shape', 'programs', 'research study', 'robotic device', 'tactile display', 'undergraduate student']",NEI,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2016,274076,0.04216719998666237
"Development of epidermal, wireless sensor tattoos for non-invasive monitoring of biofluid composition ﻿    DESCRIPTION (provided by applicant): Non-invasive medical devices are fast emerging as powerful tools in providing quantitative, simple to parse data in assessing the health and wellness of users. These devices can provide information feedback to users on their state of health, and can potentially provide valuable real-time insight to doctors on the links between user consumption and activity on their respective health. Recently described epidermal electronics / multifunctional tattoos introduced from our lab and others have demonstrated sensing of biopressure, bioelectricity, analyte concentration, bacteria, and more. These structures potentially represent the evolution of wearable devices as they possess a negligible form factor and conform to any surface (such as skin or teeth), minimizing user impact. These devices also bypass more traditional, ""wearable"" gadgets that often require bulky mechanical fixtures or straps, require complex microfluidic systems or integrated electronics, cannot provide dynamic readout, and cannot be disposed of easily. Dielectric sensors are a class of structures that are able to probe the composition of a biofluid via their impedance spectrum, and can be configured for remote sensing via radio waves. These devices can be composed of isolated, thin film circuits, and are directly amenable to epidermal or tattoo formats. This measurement methodology is inherently tremendously powerful, as it can potentially measure multiple analytes at once by probing multiple resonance peaks, and can potentially be piggybacked onto existing RFID infrastructure for data readout. However, these capabilities have not yet been demonstrated, and under real-world applications dielectric sensors have often proven difficult to use. This is often due to simplistic readout (devices will directly correlate a single metric such s the real value of the impedance at a frequency to a biomarker) and are thus can be sensitive from user to user or to environmental conditions. Herein, the applicant will leverage the expertise and knowledge of the labs of Dr. Omenetto and colleagues to bring practical usability to these dielectric antennas, bridging the gap between laboratory measurement and real-time, and real-world health monitoring applications. The end-goal is to generate disposable, skin and teeth-mounted, dielectric sensors to remotely probe the presence of biomarkers in sweat, saliva, and potentially blood to draw definitive links between user nutrition and their health.         PUBLIC HEALTH RELEVANCE: This proposal seeks to create non-invasive, wireless sensor tattoos to be tagged onto the human body for probing the composition of saliva, sweat, or blood. These wireless sensors would present negligible impediment to the user, and would require next to no upkeep to maintain, potentially facilitating an unprecedented level of subject data collection and dissemination. Such data could yield conclusive links between diet, activity, biomarkers, and public health.            ","Development of epidermal, wireless sensor tattoos for non-invasive monitoring of biofluid composition",9148070,F32EB021159,"['Architecture', 'Bacteria', 'Behavior', 'Biocompatible Materials', 'Biological', 'Biological Markers', 'Blood', 'Bypass', 'Cells', 'Characteristics', 'Classification', 'Complex', 'Consumption', 'Data', 'Data Analyses', 'Data Collection', 'Detection', 'Development', 'Devices', 'Diet', 'Electronics', 'Evolution', 'Feedback', 'Film', 'Fingerprint', 'Frequencies', 'Goals', 'Health', 'Human body', 'Ions', 'Knowledge', 'Laboratories', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Mechanics', 'Medical Device', 'Methodology', 'Microfluidics', 'Monitor', 'Nafion', 'Pattern', 'Performance', 'Principal Component Analysis', 'Public Health', 'Radio', 'Radio Waves', 'Reader', 'Research Infrastructure', 'Running', 'Saliva', 'Silk', 'Skin', 'Sodium', 'Stimulus', 'Structure', 'Surface', 'Sweat', 'Sweating', 'System', 'Tattooing', 'Testing', 'Time', 'Tooth structure', 'Training', 'Urea', 'Validation', 'Wireless Technology', 'World Health', 'bioelectricity', 'design', 'electric impedance', 'enzyme activity', 'flexibility', 'glucose monitor', 'improved', 'insight', 'non-invasive monitor', 'nutrition', 'public health relevance', 'real world application', 'remote sensing', 'response', 'saliva composition', 'sensor', 'tool', 'usability']",NIBIB,TUFTS UNIVERSITY MEDFORD,F32,2016,47190,0.038855736288279596
"Development of a low-cost, portable autorefractor for the measuring of refractive errors in low-resource settings ﻿    DESCRIPTION (provided by applicant): ""Development of a low-cost, extended-range, autorefractor""    This  project  proposal  seeks  to  develop  technologies  that  will  lower  the cost  of  and  increase  the  accessibility  of  refractive  eye  care,  especially  in  low-resoure  settings.  We propose to advance our previously developed low-cost, handheld device capable of automatically measuring the optical properties of the eye based on the technique of wavefront aberrometry.  The  proposed  work  will  specifically  focus  on  the development of optical and software systems will extend the device's measurement  range  to  work  on  a  larger  range  of  refractive  errors.  First,  optical  systems  with  no  moving  parts  will  be  developed  which  enable  the  device  to  work  on  subjects  with  severe  myopia  or  hyperopia  (<-­-6  diopters or  >6  diopters  of  refractive  error).  Second,  algorithms  that  make  the  device  easy  and  intuitive  to  use  will  be  implemented.  Third,  a  handheld  prototype  incorporating  these  improvements  will  be  constructed  and  validated  in  model  eye  systems.  The  output  of  this  project  will  be  a  functional,  extended-range  prototype  that  will  facilitate  the  fild-testing  and  commercialization  of  a  low-cost,  easy-to-use  device  to  dispense  eyeglass  prescriptions. PUBLIC HEALTH RELEVANCE: ""Development of a low-cost, extended-range, autorefractor""    This  project  proposal  seeks  to  develop  technologies  that  will  lower  the  cost  and  increase  the  accessibility  of  refractive  eye  care,  especially  in  low-resource  settings.  Specifically,  optical  and  software  systems  will  be  developed  that  will  extend  the  measurement  range  of  a  low-cost  device  that  automatically  prescribes  eyeglasses  for patients with a large range of refractive errors.","Development of a low-cost, portable autorefractor for the measuring of refractive errors in low-resource settings",9349858,R43EY025452,"['Adoption', 'Algorithms', 'Brazil', 'Caring', 'China', 'Client satisfaction', 'Clinical Trials', 'Communities', 'Databases', 'Detection', 'Development', 'Devices', 'Education', 'Ensure', 'Eye', 'Eyeglasses', 'Feedback', 'Goals', 'Health', 'Hospitals', 'Human', 'Human Resources', 'Hyperopia', 'Image', 'India', 'Laser In Situ Keratomileusis', 'Letters', 'Licensing', 'Lighting', 'Machine Learning', 'Marketing', 'Measurement', 'Measures', 'Methods', 'Mission', 'Modeling', 'Myopia', 'New England', 'Nurses', 'Operative Surgical Procedures', 'Optics', 'Optometrist', 'Optometry', 'Output', 'Patients', 'Performance', 'Pharmacists', 'Phase', 'Population', 'Positioning Attribute', 'Productivity', 'Property', 'Protocols documentation', 'Provider', 'Pupil', 'Quality of life', 'Refractive Errors', 'Resources', 'Rest', 'Retina', 'Source', 'Spottings', 'System', 'Target Populations', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Vision', 'Visual Acuity', 'Work', 'base', 'college', 'commercialization', 'cost', 'design', 'digital', 'disability', 'field study', 'handheld equipment', 'improved', 'lens', 'meetings', 'prototype', 'software systems', 'standard of care', 'success', 'usability']",NEI,"PLENOPTIKA, INC.",R43,2016,25000,0.014172835362485548
"Unassisted Blood Pressure Monitoring Using Arterial Tonometry and Photoplethysmography ﻿    DESCRIPTION (provided by applicant):  We propose a tonometry based solution to the problem of inexpensive, unencumbering and non- invasive measurement of blood pressure. The proposed solution will be useful in clinical as well as ambulatory blood pressure measurements. Specifically, we propose to develop a large (1cm x 5cm) two-dimensional array of pressure sensors with 0.2mm spacing (24x128 pressure-sensing elements), together with robust signal processing algorithms with a feedback controlled band-tightening mechanism in order to measure blood pressure at the wrist. The pressure sensor array together with a photoplethysmogram (PPG) sensor will be embedded in a band, and will provide signals through a multiplexed circuit designed. Signal conditioning techniques will be used to get the large amount of data in the form that can be efficiently processed on a microcontroller. The proposed two-dimensional array of pressure sensors will be built using flexible plastic and micro-fabrication techniques. The increased size of the sensor array will ensure proper contact and pressure application with arteries in the wrist for tonometric measurement of BP. The signals generated from the sensor array will be processed through advanced signal processing and optimization techniques to handle the huge amount of data and to mitigate noise. The PPG signals will be used at the wrist to improve the efficiency and accuracy of the system. The sensor array system will be embodied in the form of a band, which will be integrated in a wearable device such as smartwatch. The capabilities of the smartwatch will be used for data processing and analytics. a) Design, develop, and test a two-dimensional pressure sensor array on a polyimide flexible substrate b) Process large amount of analog data from sensor array using signal conditioning and processing techniques  to generate blood pressure estimates c) Design a user friendly prototype form factor, and perform a clinical trial to validate device performance  The engineering members of our multidisciplinary team have specialized in cardiac instrumentation, signal and image processing for medical applications, pressure and touch sensors, signal processing and robust optimization. The team members with clinical expertise have been engaged in blood pressure trials in US, and have been working with leading institutions in India engaged in cardiovascular research. This proposal brings together their unique expertise and experience to innovatively address this challenging problem through a joint effort. PUBLIC HEALTH RELEVANCE: Unmanaged hypertension is a major problem, and its management based on occasional measurement is known to be suboptimal. The ability to measure blood pressure using non- invasive techniques in an ambulatory setting has many significant benefits. We propose to research and develop a large (1cm x 5cm) two-dimensional array of pressure sensors based device together with robust signal processing algorithms with a feedback controlled to measure blood pressure at the wrist. The device will be test in a clinical trial based on the European Society of Hypertension International Protocol.",Unassisted Blood Pressure Monitoring Using Arterial Tonometry and Photoplethysmography,9145739,U01EB020589,"['Address', 'Algorithms', 'Area', 'Arteries', 'Blood Pressure', 'Blood Pressure Monitors', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Engineering', 'Clinical Trials', 'Data', 'Data Analytics', 'Devices', 'Elements', 'Engineering', 'Ensure', 'Environment', 'European', 'Fatigue', 'Feedback', 'Generations', 'Goals', 'Health', 'Hypertension', 'India', 'Institution', 'International', 'Joints', 'Lead', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Noise', 'Operative Surgical Procedures', 'Output', 'Patients', 'Performance', 'Photoplethysmography', 'Process', 'Protocols documentation', 'Research', 'Signal Transduction', 'Societies', 'Somatotype', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Touch sensation', 'Training', 'Untrained Personnel', 'Work', 'Wrist', 'analog', 'arterial tonometry', 'base', 'computerized data processing', 'conditioning', 'design', 'experience', 'field study', 'flexibility', 'image processing', 'improved', 'instrumentation', 'member', 'multidisciplinary', 'pressure', 'process optimization', 'prototype', 'rural area', 'sensor', 'signal processing', 'tonometry', 'two-dimensional', 'user-friendly']",NIBIB,NORTHWESTERN UNIVERSITY AT CHICAGO,U01,2016,355735,0.017275847091809935
"Enabling access to printed text for blind people via assisted mobile OCR DESCRIPTION (provided by applicant): This application proposes new technology development and user studies aiming to facilitate the use of mobile Optical Character Recognition (OCR) for blind people. Mobile OCR systems, implemented as smartphones apps, have recently appeared on the market. This technology unleashes the power of modern computer vision algorithms to enable a blind person to hear (via synthetic speech) the content of printed text imaged by the smartphone's camera. Unlike traditional OCR, that requires scanning of a document with a flatbed scanner, mobile OCR apps enable access to text anywhere, anytime. Using their own smartphones, blind people can read store receipts, menus, flyers, business cards, utility bills, and many other printed documents of the type normally encountered in everyday life. Unfortunately, current mobile OCR systems suffer from a chicken-and-egg problem, which limits their usability. They require the user to take a well-framed snapshot of the document to be scanned, with the full text in view, and at a close enough distance that each character can be well resolved and thus readable by the machine. However, taking a good picture of a document is difficult without sight, and thus without the ability to look at the scene being imaged by the camera through the smartphone's screen. Anecdotal evidence, supported by results of preliminary studies conducted by the principal investigator's group, confirms that acquisition of an OCR-readable image of a document can indeed by very challenging for some blind users. We plan to address this problem by developing and testing a new technique of assisted mobile OCR. As the user aims the camera at the document, the system analyzes in real time the stream of images acquired by the camera, and determines how the camera position and orientation should be adjusted so that an OCR-readable image of the document can be acquired. This information is conveyed to the user via a specially designed acoustic signal. This acoustic feedback allows users to quickly adjust and reorient the camera or the document, resulting in reduced access time and in more satisfactory user experience. Multiple user studies with blind participants are planned with the purpose of selecting an appropriate acoustic interface and of evaluating the effectiveness of the proposed assisted mobile OCR modality. PUBLIC HEALTH RELEVANCE: This application is concerned with the development of new technology designed to facilitate use of mobile Optical Character Recognition (OCR) systems to access printed text without sight. Specifically, this exploratory research will develop and test a novel system that, by means of a specially designed acoustic interface, will help a blind person take a well-framed, well-resolved image of a document for OCR processing using a smartphone or wearable camera. If successful, this novel approach to assisted mobile OCR will reduce access time and improve user experience of blind mobile OCR users.",Enabling access to printed text for blind people via assisted mobile OCR,8989105,R21EY025077,"['Acoustics', 'Address', 'Algorithms', 'Augmented Reality', 'Businesses', 'Cellular Phone', 'Chest', 'Chickens', 'Clothing', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Disabled Persons', 'Education', 'Effectiveness', 'Employment', 'Environment', 'Eyeglasses', 'Feedback', 'Goals', 'Hand', 'Health', 'Hearing', 'Image', 'Knowledge', 'Life', 'Light', 'Location', 'Marketing', 'Modality', 'Monitor', 'Participant', 'Pattern', 'Positioning Attribute', 'Principal Investigator', 'Printing', 'Process', 'Quality of life', 'Reading', 'Report (document)', 'Research', 'Resolution', 'Restaurants', 'Scanning', 'Series', 'Signal Transduction', 'Speech', 'Stream', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Technology Development Study', 'Testing', 'Text', 'Time', 'Translating', 'Travel', 'Vision', 'Visual', 'Visually Impaired Persons', 'blind', 'design', 'egg', 'experience', 'handicapping condition', 'improved', 'new technology', 'novel', 'novel strategies', 'object recognition', 'optical character recognition', 'research study', 'technology development', 'usability', 'way finding']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2016,230563,0.022555166948246042
"CRCNS: Model-driven single-neuron studies of cortical mapping DESCRIPTION (provided by applicant): During natural vision humans and non-human primates make several saccadic eye movements each second that result in large changes in the retinal input. Despite these often dramatic changes, our visual percept remains remarkably stable and we can readily attend to and direct motor actions towards objects in our visual environment. This project will use a model-driven approach to investigate the neural circuits linking vision, attention and oculomotor planning that stabilize perceptual and attentional representations during natural vision. Experimental data will be collected and used to design a detailed computational model of the visual and oculomotor areas involved in saccade compensation. The proposed collaboration between a computational (DE) and experimental neurophysiological (US) laboratories leverages the power of both disciplines. Biologically accurate models of visually guided behavior and trans-saccadic integration developed in the Hamker lab will guide the design of and interpretation of data obtained from neurophysiological experiments in awake, behaving primates performed in the Mazer lab in an iterative fashion, with experimental results informing model revisions and new model predictions altering experimental designs. The proposed studies will characterize both dorsal and ventral stream visual area contributions to stabilizing visual and attentional representations in the primate brai. Data obtained from these experiments will identify the neural circuits responsible for integrating oculomotor commands, bottom-up visual inputs and top-down attention signals. This approach will yield novel insights into interactions between the dorsal and ventral streams during natural vision and facilitate our understanding of goal-directed, active visual perception, a defining feature of human and non-human primate natural vision. A critical component of this project is the highly collaborative nature of the planned research. We expect great benefits from this interdisciplinary approach, which depends critically on computational models that strictly adhere to the known physiological and anatomical constraints to guide our neurophysiological experiments.     1. Training. The proposal includes a detailed training plan intended to facilitate international training of future modelers and neurophysiologists. Specifically, we will train students and post-doctoral researchers to be experts in both experimental and theoretical approaches in order to advance the field using the hybrid approach outlined in the proposal.    2. Education and Outreach. We plan to organize two in-depth workshops on attention and eye movements. These events (one in Germany and one in the US) will bring together investigators from other institutions and related scientific disciplines to advance the field. In addition investigators will organize and chair 1-2 workshops/symposia at annual meetings (e.g., SFN and COSYNE) during the funding period. Finally, we will participate in science education for underrepresented groups through Yale's STARS program by providing training, research and mentoring opportunities in the Mazer lab.    3. Data Sharing. The software tools generated and behavioral and neurophysiological data collected during this project will be distributed to the neuroscience community to facilitate data mining and secondary analyses of experimental data.    4. Impact in other scientific fields. Efficient allocation of limited sensor resources is also a important problem faced by computer vision and robotics researchers. Understanding how the primate brain efficiently allocates visual resources using an active-sensing approach will guide development of biologically inspired computer vision algorithms and humanoid cognitive robots.    5. Translational Implications. Although the proposed research is not translational, there is a growing body of evidence suggesting that several clinically important conditions, including Autism Spectrum Disorder, Attention Deficit Hyperactivity Disorder and Schizophrenia, are associated with impaired behavioral links between saccade planning and visual attention. The proposed basic science studies could have significant implications for future translational research potentially leading to improved understanding disease etiology, development of early diagnostic tools and possible interventional strategies. n/a",CRCNS: Model-driven single-neuron studies of cortical mapping,9308612,R01EY025103,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Area', 'Attention', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Dorsal', 'Education and Outreach', 'Educational workshop', 'Electrophysiology (science)', 'Environment', 'Etiology', 'Event', 'Experimental Designs', 'Eye Movements', 'Financial compensation', 'Funding', 'Future', 'Germany', 'Goals', 'Human', 'Hybrids', 'Institution', 'International', 'Intervention', 'Laboratories', 'Link', 'Mentors', 'Modeling', 'Monkeys', 'Motor', 'Nature', 'Neurons', 'Neurosciences', 'Performance', 'Physiological', 'Postdoctoral Fellow', 'Primates', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Retinal', 'Robot', 'Robotics', 'Saccades', 'Schizophrenia', 'Signal Transduction', 'Software Tools', 'Stream', 'Testing', 'Training', 'Translational Research', 'Underrepresented Groups', 'Update', 'Vision', 'Visual', 'Visual Perception', 'Visual attention', 'Work', 'area V4', 'autism spectrum disorder', 'awake', 'base', 'cell type', 'computer framework', 'cortex mapping', 'data mining', 'data sharing', 'design', 'extrastriate visual cortex', 'improved', 'insight', 'interdisciplinary approach', 'meetings', 'model building', 'neural circuit', 'neurophysiology', 'nonhuman primate', 'novel', 'oculomotor', 'programs', 'relating to nervous system', 'research study', 'science education', 'sensor', 'simulation', 'spatiotemporal', 'student training', 'symposium', 'tool', 'vector', 'visual motor']",NEI,MONTANA STATE UNIVERSITY - BOZEMAN,R01,2016,180000,-0.006317622179802325
"Understanding Real-Life Falls in Amputees using Mobile Phone Technology DESCRIPTION (provided by applicant): Falls are a significant cause of death and serious injury and result in significant health-care costs. Individuals with a lower extremity amputation due to vascular disease are overwhelmingly elderly (at least 65 years of age) and are at especially high risk of falling. Successful fall prevention strategies depend on understanding how, why, when, and where individuals fall, and what types of falls (e.g., trip, slip, or lateral fll) are likely in a given population. Most studies on falls in amputees to date have relied surveys or questionnaires that are often completed a significant time after the fall and thus rely both on the individual's ability to remember the details of their fall and their willingness to be objective abut how and why they fell. Such approaches are susceptible both to inaccurate memories of the fall and to recall bias-for example, due to embarrassment about falling- and are especially unreliable in the elderly amputees. Mobile phones provide a simple, cost-effective method for detection and characterization of falls. Most available smart phones today have a tri-axial accelerometer, which provides highly accurate fall detection in real-time. Other available applications (or apps) can provide data on activity (running, walking etc.) and environment-such as the weather conditions or population density-that may have contributed to the fall and can pin-point the location of the fall-using GPS technology and highly accurate maps. Mobile phones also have inbuilt data storage and transfer capability, allowing for real-time acquisition and transmission of data. Additionally, mobile phones provide a simple means to contact the individual immediately after a suspected fall to confirm details of the fall (and to ascertain the need for medical assistance). Because mobile phone use is so widespread, there is no stigma associated with carrying such a device, which is likely to lead to high compliance. This study aims to use a mobile phone-based fall detection system in dysvascular amputees to detect falls, characterize the type of fall, analyze environmental conditions that may have contributed to the fall, and determine the longer-term consequences of each type of fall. Data acquired may be used to improve rehabilitation protocols or design better prostheses in order to prevent falls. This technology is ultimately transferrable to many populations with a high risk of falling-for example, the elderly, stroke survivors, or those with other musculoskeletal disorders or disabilities-leading to the design of specific fall prevention strategies for those populations. PUBLIC HEALTH RELEVANCE: Successful fall prevention strategies for the elderly dysvascular amputee population-including better prosthesis design and improved rehabilitation/fall prevention strategies-depend on understanding how, why, when, and where such individuals fall, and what types of fall occur (e.g., trip, slip, or lateral fall). Mobile smartphones with built-in triaxial accelerometers can accurately detect and classify falls, in addition to providing other applications (or 'apps') that can identify contributory environmental conditions (e.g., weather or traffic) or activities (e.g. walking or running) that have may contributed to the fall. Combined with server side analysis of wirelessly transmitted phone data-using machine learning techniques-mobile smartphones provide a simple, portable fall-detection system that generates real-time information on the mechanisms, contributory environmental factors, and consequences of falls in elderly amputees-or in any population at risk of falling.",Understanding Real-Life Falls in Amputees using Mobile Phone Technology,9133378,R01EB019406,"['3-Dimensional', 'Accelerometer', 'Age-Years', 'Algorithms', 'Amputation', 'Amputees', 'Car Phone', 'Cause of Death', 'Cellular Phone', 'Classification', 'Communication', 'Communities', 'Crowding', 'Data', 'Data Collection', 'Data Quality', 'Data Set', 'Data Storage and Retrieval', 'Detection', 'Devices', 'Elderly', 'Emergency department visit', 'Environment', 'Environmental Risk Factor', 'Etiology', 'Event', 'Fall injury', 'Fall prevention', 'Family', 'Fright', 'Goals', 'Health', 'Health Care Costs', 'Hospitals', 'Individual', 'Injury', 'Interview', 'Knowledge', 'Laboratories', 'Lateral', 'Lead', 'Life', 'Location', 'Longitudinal Studies', 'Lower Extremity', 'Machine Learning', 'Maps', 'Medical Assistance', 'Medical Care Costs', 'Memory', 'Methods', 'Morbidity - disease rate', 'Musculoskeletal Diseases', 'Outcome', 'Patients', 'Persons', 'Population', 'Population Density', 'Populations at Risk', 'Prevalence', 'Prevention strategy', 'Prosthesis', 'Prosthesis Design', 'Protocols documentation', 'Publications', 'Quality of life', 'Questionnaires', 'Rain', 'Real-Time Systems', 'Recovery', 'Rehabilitation therapy', 'Reporting', 'Research', 'Running', 'Side', 'Surveys', 'System', 'Techniques', 'Technology', 'Time', 'Vascular Diseases', 'Visit', 'Walking', 'Weather', 'Wireless Technology', 'Work', 'base', 'cohort', 'cost effective', 'data exchange', 'design', 'diaries', 'disability', 'falls', 'fear of falling', 'health care quality', 'high risk', 'improved', 'improved mobility', 'information gathering', 'mortality', 'new technology', 'novel', 'prospective', 'sensor', 'social stigma', 'stroke survivor', 'trafficking', 'willingness']",NIBIB,REHABILITATION INSTITUTE OF CHICAGO D/B/A SHIRLEY RYAN ABILITYLAB,R01,2016,166809,0.0028040063003246223
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software.         PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.                ",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9030162,R01EY025332,"['3D Print', 'Access to Information', 'Adoption', 'Algorithms', 'American', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image', 'Information Systems', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'research study', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2016,416574,0.02138415910852644
"Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility ﻿    DESCRIPTION (provided by applicant): Innovative Design Labs (IDL) proposes to create a system to improve the mobility and control of exoskeletons. Recent research has found that 3.86 million Americans require wheelchairs and the number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk, thus providing a way to more fully reintegrate these individuals into society. Our proposal seeks to address one of the hurdles limiting the widespread adoption of exoskeletons in the home and community-the inability of the user to dynamically control gait parameters. This concept has the potential to significantly change the way exoskeletons work and facilitate their adoption into the market. Hypothesis: We hypothesize that the proposed solution will provide users a practical way to adjust their suit's gait to precisely achieve their navigational goals. Specific Aims: Phase I: 1) Build a prototype and Perform Preliminary Laboratory Testing; 2) Develop and Benchmark Algorithms; and 3) Perform Pilot Human Study of Prototype with Exoskeleton Subjects. Phase II: 1) Develop Customized, Production-Ready Hardware and Firmware 2) Integrate with Exoskeleton Control System; and 3) Perform an evaluation of the system through human study testing.         PUBLIC HEALTH RELEVANCE: Recent research has found that 3.86 million Americans require wheelchairs and that number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk thereby providing a way to more fully reintegrate these individuals into society.        ",Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility,9140712,R44AG053890,"['Address', 'Adoption', 'Algorithm Design', 'Algorithms', 'American', 'Benchmarking', 'Bionics', 'Caregivers', 'Chicago', 'Clinical', 'Collaborations', 'Communities', 'Community Participation', 'Computational algorithm', 'Computer Vision Systems', 'Crutches', 'Dependence', 'Devices', 'Electrical Engineering', 'Environment', 'Evaluation', 'Exercise', 'Eye', 'Family', 'Feedback', 'Freedom', 'Friends', 'Gait', 'Goals', 'Health', 'Height', 'Home environment', 'Hospitals', 'Human', 'Image', 'Individual', 'Industry', 'Institutes', 'Laboratories', 'Length', 'Location', 'Marketing', 'Medical', 'Methods', 'Motion', 'Patients', 'Performance', 'Phase', 'Population', 'Process', 'Production', 'Quality of life', 'Ramp', 'Rehabilitation Centers', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Safety', 'Small Business Innovation Research Grant', 'Social isolation', 'Societies', 'Software Engineering', 'System', 'Technology', 'Testing', 'Uncertainty', 'Walking', 'Wheelchairs', 'Work', 'commercialization', 'design', 'exoskeleton', 'experience', 'human study', 'image processing', 'improved', 'improved mobility', 'innovation', 'insight', 'member', 'product development', 'prototype', 'public health relevance', 'rehabilitation technology', 'robot exoskeleton', 'usability', 'vision aid']",NIA,"INNOVATIVE DESIGN LABS, INC.",R44,2016,224990,0.00093355981094266
"Designing Visually Accessible Spaces DESCRIPTION (provided by applicant):  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals, including those with physical disabilities and to enhance safety for older people and others who may need to operate under low lighting and other visually challenging conditions.  We plan to develop a computer-based design tool enabling architectural design professionals to assess the visual accessibility of a wide range of environments (such as a hotel lobby, subway station, or eye-clinic reception area).  This tool will simulate such environments with sufficient accuracy to predict the visibility of key landmarks or hazards, such as steps or benches, for different levels and types of low vision, and for spaces varying in lighting, surface properties and geometric arrangement.  Our project addresses one of the National Eye Institute's program objectives:  ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments."" Our research plan has three specific goals:  1) Empirical:  determine factors that influence low-vision accessibility related to hazard detection and navigation in real-world spaces.  2) Computational:  develop working models to predict low vision visibility and navigability in real-world spaces.  3) Deployment:  translate findings from basic vision science and clinical low vision into much needed industrial usage by producing a set of open source software modules to enhance architectural and lighting design for visual accessibility.  The key scientific personnel in our partnership come from three institutions:  University of Minnesota -- Gordon Legge and Daniel Kersten; University of Utah -- William Thompson and Sarah Creem-Regehr; and Indiana University -- Robert Shakespeare.  This interdisciplinary team has expertise in the necessary areas required for programmatic research on visual accessibility -- empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), computer graphics and photometrically accurate rendering (Thompson & Shakespeare) and architectural lighting design (Shakespeare).  We have collaborative arrangements with additional architectural design professionals who will participate in the translation of our research and development into practice. PUBLIC HEALTH RELEVANCE:  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our BRP partnership, with interdisciplinary expertise in vision science, computer science and lighting design, plans to develop computer-based tools enabling architecture professionals to assess the visual accessibility of their designs.",Designing Visually Accessible Spaces,9024545,R01EY017835,"['Accounting', 'Address', 'Age', 'American', 'Architecture', 'Area', 'Biological Models', 'Blindness', 'Central Scotomas', 'Characteristics', 'Clinic', 'Clinical', 'Complement', 'Complex', 'Computer Analysis', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Contrast Sensitivity', 'Cues', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Disorientation', 'Distance Perception', 'Effectiveness', 'Environment', 'Evaluation', 'Eye', 'Fall injury', 'Geometry', 'Glare', 'Goals', 'Grant', 'Guidelines', 'Health', 'Height', 'Human', 'Human Resources', 'Indiana', 'Individual', 'Institution', 'Investigation', 'Judgment', 'Learning', 'Light', 'Lighting', 'Lobbying', 'Location', 'Minnesota', 'Modeling', 'Modification', 'National Eye Institute', 'Nature', 'Optics', 'Perception', 'Peripheral', 'Phase', 'Photometry', 'Physical environment', 'Physically Handicapped', 'Positioning Attribute', 'Process', 'Production', 'Property', 'Research', 'Risk', 'Safety', 'Shapes', 'Source', 'Specific qualifier value', 'Stimulus', 'Structure', 'Subway', 'Surface', 'Surface Properties', 'System', 'Test Result', 'Testing', 'Thinking', 'Translating', 'Translations', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual Acuity', 'Visual Fields', 'Visual impairment', 'Work', 'base', 'computer science', 'design', 'disability', 'hazard', 'imaging system', 'improved', 'knowledge base', 'luminance', 'novel strategies', 'open source', 'programs', 'research and development', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2016,576055,0.06011487784224445
"NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair DESCRIPTION (provided by applicant):         The aim of this proposal is to conduct research on the foundational models and algorithms in computer vision and machine learning for an egocentric vision based active learning co-robot wheelchair system to improve the quality of life of elders and disabled who have limited hand functionality or no hand functionality at all, and rely on wheelchairs for mobility. In this co-robt system, the wheelchair users wear a pair of egocentric camera glasses, i.e., the camera is capturing the users' field-of-the-views. This project help reduce the patients' reliance on care-givers. It fits NINR's mission in addressing key issues raised by the Nation's aging population and shortages of healthcare workforces, and in supporting patient-focused research that encourage and enable individuals to become guardians of their own well-beings.  The egocentric camera serves two purposes. On one hand, from vision based motion sensing, the system can capture unique head motion patterns of the users to control the robot wheelchair in a noninvasive way. Secondly, it serves as a unique environment aware vision sensor for the co-robot system as the user will naturally respond to the surroundings by turning their focus of attention, either consciously or subconsciously. Based on the inputs from the egocentric vision sensor and other on-board robotic sensors, an online learning reservoir computing network is exploited, which not only enables the robotic wheelchair system to actively solicit controls from the users when uncertainty is too high for autonomous operation, but also facilitates the robotic wheelchair system to learn from the solicited user controls. This way, the closed- loop co-robot wheelchair system will evolve and be more capable of handling more complicated environment overtime.  The aims ofthe project include: 1) develop an method to harness egocentric computer vision-based sensing of head movements as an alternative method for wheelchair control; 2) develop a method leveraging visual motion from the egocentric camera for category independent moving obstacle detection; and 3) close the loop of the active learning co-robot wheelchair system through uncertainty based active online learning. PUBLIC HEALTH RELEVANCE:          The project will improve the quality of life of those elders and disabled who rely on wheelchairs for mobility, and reduce their reliance on care-givers. It builds an intelligent wheelchair robot system that can adapt itself to the personalized behavior of the user. The project addresses the need of approximately 1 % of our world's population, including millions of Americans, who rely on wheelchair for mobility.",NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair,8914675,R01NR015371,"['Active Learning', 'Address', 'Algorithms', 'American', 'Attention', 'Behavior', 'Caregivers', 'Categories', 'Computer Vision Systems', 'Detection', 'Disabled Persons', 'E-learning', 'Elderly', 'Environment', 'Glass', 'Hand', 'Head', 'Head Movements', 'Health', 'Healthcare', 'Individual', 'Learning', 'Machine Learning', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motivation', 'Patients', 'Pattern', 'Population', 'Principal Investigator', 'Quality of life', 'Research', 'Robot', 'Robotics', 'System', 'Uncertainty', 'Vision', 'Visual Motion', 'Wheelchairs', 'aging population', 'base', 'improved', 'operation', 'programs', 'sensor']",NINR,STEVENS INSTITUTE OF TECHNOLOGY,R01,2015,136355,0.036111410160356607
"Computational Image Analysis for Cellular and Developmental Biology DESCRIPTION (provided by applicant): This proposal requests support for an intensive ten-day course on Computational Image Analysis for Cellular and Developmental Biology. The course is designed for graduate students and postdoctoral fellows, and takes place at the Marine Biological Laboratory in Woods Hole, MA. The course is the first of its kind, giving students formal training in computer vision for the specific analysis of cell and developmental biology image data. Building strong foundations in this topic is critical for pushing cell and developmental biology forward, as imaging has become more and more an indispensable tool in these fields. The course covers the fundamentals of computer vision, taking the students through the sequence of low-, intermediate-, and high-level computer vision tasks that are required to solve image analysis problems in quantitative cellular and developmental biology. The curriculum starts with filtering, thresholding and edge/line/generic feature detection, followed by more sophisticated detection algorithms that employ model fitting. After this introductory block to low-level computer vision tasks, the course moves on to intermediate and higher-level tasks, including object association in space and time (such as tracking) and machine learning tools for phenotype classification. Each topic is covered first by a lecture, generally taught by one of the four core faculty, followed by a 3-4 hour computer programming session where students immediately implement the concepts they learn. There are usually two lectures + computer labs per day. Most programming exercises are individual, giving each student the opportunity to ""get their hands dirty,"" while two are team projects allowing the students to also learn and practice methods of code sharing. Over the course's ten days there are three guest lectures by leading researchers in the fields of biological imaging and computer vision, each followed by in-depth discussion, as well as research talks given by the students, faculty and teaching assistants. With this, the core lectures and labs teach the students the fundamentals of computer vision in a logical, continuous manner, the guest lecturers introduce the students to exciting new challenges in imaging and image analysis, and the student/faculty research talks encourage communication between all course participants and give especially the students the opportunity to reflect on how the course can help them with their research. PUBLIC HEALTH RELEVANCE:  Imaging has become an indispensable tool in cellular and developmental biology research, but without rigorous, quantitative image analysis it cannot achieve its full potential. This proposal requests support for a new course that will fill the voidof education in computer vision as applied to cellular and developmental biology. The curriculum incorporates a carefully balanced mixture of lectures and associated programming exercises. We have given a pilot course once in October 2010, with very positive reviews from the students and their advisers.",Computational Image Analysis for Cellular and Developmental Biology,8813596,R25GM103792,"['Address', 'Algorithms', 'Biological', 'Cellular biology', 'Classification', 'Code', 'Collection', 'Communication', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Data', 'Detection', 'Development', 'Developmental Biology', 'Developmental Cell Biology', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Equilibrium', 'Exercise', 'Faculty', 'Foundations', 'Future', 'Generic Drugs', 'Goals', 'Hand', 'Health', 'Home environment', 'Hour', 'Image', 'Image Analysis', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Marines', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Phenotype', 'Postdoctoral Fellow', 'Request for Proposals', 'Research', 'Research Personnel', 'Seeds', 'Software Design', 'Solid', 'Students', 'Time', 'Training', 'Wood material', 'computer program', 'design', 'graduate student', 'lecturer', 'lectures', 'programs', 'quantitative imaging', 'tool']",NIGMS,MARINE BIOLOGICAL LABORATORY,R25,2015,59383,0.007972048264027903
"Micromachined microphones with in-plane and out-of-plane directivity Project Summary We aim to introduce to the hearing-assistive device industry directional microphones with high signal-to-noise ratio, and the first commercialized microphone that combines all three axes of acoustic pressure gradient onto a single silicon chip. We expect the technology to empower the signal-processing community with a new tool which, when used in conjunction with a conventional omnidirectional microphone, will facilitate new features like ultra-sharp directionality adaptable in real-time by the user and/or artificial intelligence algorithms which scan for desired inputs while filtering out unwanted noise. Directional sensing and the ability to filter out undesirable background acoustic noise are important for those with hearing impairments. Hearing impairment is associated with a loss of fidelity to quiet sounds, while the threshold of pain remains the same. As such, hearing impairment causes a loss of dynamic range or “window” of detectable sound amplitudes. Directional sensing enables preferentially amplifying desired sounds without amplifying background noise. As the first step, we aim to accelerate the commercialization of recently introduced biologically- inspired “rocking” style microphones by synthesizing these designs with integrated, robust piezoelectric readout which is ideal for addressing the low-power, small-size, and high levels of integration required of the hearing-aid industry. Previous work in this field using laboratory prototypes and optical readout have demonstrated the merits of the biologically-inspired sensing approach (i.e. a simultaneous 20-dB SNR improvement and 10x reduction size improvement beyond what is achievable with present-day hearing-aid or MEMS microphones). By synthesizing a piezoelectric embodiment as an alternative to optical readout, we aim to accelerate through many of the commercialization challenges so that an impact to the hearing device industry can be made. Further, the proposed readout is better adapted towards integrating multiple microphones in the same silicon chip. We aim to integrate a microphone with both in-plane axis of directivity with an out-of-plane directional design to form a complete  -axis pressure gradient sensor. Project Narrative Studies show that today 2% of Americans wear a hearing aid, whereas at least 10% of Americans could benefit from a hearing assistive device. The major reason for this gap is patient dissatisfaction. Hearing-aid wearers suffer from what is known as the “cocktail party” effect. When the gain is turned up to hear the person speaking across from you, noises in the background are equally amplified – making every scenario sound like a cocktail party. This research aims to make a positive, long-term improvement to hearing-aid patient satisfaction by making commercially available directional microphones with high fidelity.",Micromachined microphones with in-plane and out-of-plane directivity,8981015,R44DC013746,"['Acoustics', 'Address', 'Algorithms', 'American', 'Artificial Intelligence', 'Client satisfaction', 'Communities', 'Devices', 'Environment', 'Hearing', 'Hearing Aids', 'Industry', 'Laboratories', 'Noise', 'Optics', 'Pain Threshold', 'Patients', 'Persons', 'Research', 'Scanning', 'Self-Help Devices', 'Signal Transduction', 'Silicon', 'Small Business Innovation Research Grant', 'Techniques', 'Technology', 'Time', 'Work', 'commercialization', 'design', 'empowered', 'hearing impairment', 'pressure', 'prototype', 'sensor', 'signal processing', 'sound', 'tool']",NIDCD,"SILICON AUDIO, INC.",R44,2015,508997,0.006237534536885459
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering. PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,8920573,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Health', 'Healthcare', 'High School Student', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Solutions', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'rehabilitation engineering', 'robot rehabilitation', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2015,3412,0.06226317079937248
"NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired PROJECT SUMMARY (See instructions): The objective of the proposed research is to develop new technology for a Wearable Robotic Object Manipulation Aid (W-ROMA) for the visually impaired. The W-ROMA is a hand-worn assistive device that provides assistance to a visually impaired individual in effectively grasping an object. Thanks to the onboard computer vision methods, the W-ROMA is capable of detecting a target object, determining the hand-object misalignment, and conveying to the wearer, via natural human-device interfaces, the desired hand motion for hand-object alignment. The W-ROMA will contribute to the independent lives of the visually impaired in twofold: First, it helps the visually impaired with independent travel by enabling them to identify a movable obstacle and manipulate the obstacle to make a passage. Second, it assists the visually impaired in effectively grasping an object for non-navigational purpose. The PIs will involve graduate, undergraduate and high school students in the project and use the proposed project activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision and human-robot interaction methods that support accurate and effective object grasping for the visually impaired for their independent daily lives. These methods include: (1) a new real-time object recognition method; (2) an innovative hand-object alignment mechanism; (3) a novel hybrid tactile display system for object shape rendering; and (4) a computationally efficient device localization method. The proposed solutions can be encapsulated in a hand-worn robotic device. The W-ROMA will provide new co-robotic functions for the visually impaired. The PIs have performed proof of concept studies for the computer vision and tactile display methods and the results are promising. The broader impacts include: (1) the research will positively impact the large visually impaired community; (2) the proposed methods can be applied to other small robotic systems that have a wide range of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the PI's university and train graduate and undergraduate students for their future careers in science and engineering. RELEVANCE (See instructions): The project addresses a growing public health care issue--visual impairment. The research fits well into the NEI's Low Vision and Blindness Rehabilitation program that supports development of new technologies for minimizing the impact of visual impairment. The project addresses the NEI's mission by developing new assistive technology that will help the visually impaired to maintain a higher quality of life.",NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired,9050942,R01EY026275,"['Address', 'Blindness', 'Canes', 'Code', 'Communities', 'Companions', 'Computer Vision Systems', 'Data', 'Detection', 'Development', 'Devices', 'Encapsulated', 'Engineering', 'Environment', 'Funding', 'Future', 'Goals', 'Grant', 'Graph', 'Hand', 'Healthcare', 'High School Student', 'Human', 'Hybrids', 'Image', 'Independent Living', 'Individual', 'Instruction', 'Law Enforcement', 'Maps', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'Motion', 'Movement', 'National Institute of Biomedical Imaging and Bioengineering', 'Performance', 'Persons', 'Polymers', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Rotation', 'Scheme', 'Science', 'Self-Help Devices', 'Solid', 'Solutions', 'Speech', 'Speed', 'Students', 'System', 'Tactile', 'Testing', 'Thumb structure', 'Time', 'Training', 'Translations', 'Travel', 'United States National Institutes of Health', 'Universities', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'base', 'blind', 'career', 'design', 'experience', 'graduate student', 'grasp', 'improved', 'innovation', 'new technology', 'novel', 'object recognition', 'object shape', 'programs', 'research study', 'robotic device', 'tactile display', 'undergraduate student']",NEI,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2015,280721,0.04216719998666237
"Providing Access to Appliance Displays for Visually Impaired Users DESCRIPTION (provided by applicant):  Providing Access to Appliance Displays for Visually Impaired Users Summary The goal of this project is to develop a computer vision system that runs on smartphones and tablets to enable blind and visually impaired persons to read appliance displays.  This ability is increasingly necessary to use a variety of household and commercial appliances equipped with displays, such as microwave ovens and thermostats, and is essential for independent living, education and employment.  No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents.  Our proposed system runs as software on a standard, off-the-shelf smartphone or tablet and uses computer vision algorithms to analyze video images taken by the user and to detect and read the text within each image.  For blind users, this text is either read aloud using synthesized speech, or for low vision users, presented in magnified, contrast- enhanced form (which is most suited to the use of a tablet).  We propose to build on our past work on a prototype display reader using powerful new techniques that will enable the resulting system to read a greater variety of LED/LCD display text fonts, and to better accommodate the conditions that often make reading displays difficult, including glare, reflections and poor contrast.  These techniques include novel character recognition techniques adapted specifically to the domain of digital displays; finger detection to allow the user to point to a specific location of interest in the display; the use of display templates as reference images to match to, and thereby help interpret, noisy images of displays; and the integration of multiple views of the display into a single clear view.  Special user interface features such as the use of finger detection to help blind users aim the camera towards the display and contrast-enhancement for low vision users address the particular needs of different users.  Blind and visually impaired subjects will test the system periodically to provide feedback and performance data, driving the development and continual improvement of the system, which will be assessed with objective performance measures.  The resulting display reader system will be released as an app for smartphones and tablets that can be downloaded and used by anybody for free, and also as an open source project that can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self-sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays.  The proposed research would result in a smartphone/tablet-based assistive technology system (available for free) to provide increased access to such display information for the approximately 10 million Americans with significant vision impairments or blindness.",Providing Access to Appliance Displays for Visually Impaired Users,8916115,R01EY018890,"['Access to Information', 'Address', 'Adoption', 'Algorithms', 'American', 'Automobile Driving', 'Blindness', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Detection', 'Development', 'Devices', 'Economics', 'Education', 'Employment', 'Evaluation', 'Feedback', 'Fingers', 'Glare', 'Goals', 'Hand', 'Health', 'Home environment', 'Household', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging problem', 'Impairment', 'Independent Living', 'Location', 'Mainstreaming', 'Measures', 'Movement', 'Performance', 'Prevalence', 'Printing', 'Procedures', 'Reader', 'Reading', 'Research', 'Research Infrastructure', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'System', 'Tablets', 'Techniques', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'consumer product', 'contrast enhanced', 'contrast imaging', 'design', 'digital', 'improved', 'information display', 'interest', 'microwave electromagnetic radiation', 'novel', 'open source', 'optical character recognition', 'prototype']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2015,368560,0.04284455887579613
"Development of a low-cost, portable autorefractor for the measuring of refractive errors in low-resource settings ﻿    DESCRIPTION (provided by applicant): ""Development of a low-cost, extended-range, autorefractor""    This  project  proposal  seeks  to  develop  technologies  that  will  lower  the cost  of  and  increase  the  accessibility  of  refractive  eye  care,  especially  in  low-resoure  settings.  We propose to advance our previously developed low-cost, handheld device capable of automatically measuring the optical properties of the eye based on the technique of wavefront aberrometry.  The  proposed  work  will  specifically  focus  on  the development of optical and software systems will extend the device's measurement  range  to  work  on  a  larger  range  of  refractive  errors.  First,  optical  systems  with  no  moving  parts  will  be  developed  which  enable  the  device  to  work  on  subjects  with  severe  myopia  or  hyperopia  (<-­-6  diopters or  >6  diopters  of  refractive  error).  Second,  algorithms  that  make  the  device  easy  and  intuitive  to  use  will  be  implemented.  Third,  a  handheld  prototype  incorporating  these  improvements  will  be  constructed  and  validated  in  model  eye  systems.  The  output  of  this  project  will  be  a  functional,  extended-range  prototype  that  will  facilitate  the  fild-testing  and  commercialization  of  a  low-cost,  easy-to-use  device  to  dispense  eyeglass  prescriptions.                 PUBLIC HEALTH RELEVANCE: ""Development of a low-cost, extended-range, autorefractor""    This  project  proposal  seeks  to  develop  technologies  that  will  lower  the  cost  and  increase  the  accessibility  of  refractive  eye  care,  especially  in  low-resource  settings.  Specifically,  optical  and  software  systems  will  be  developed  that  will  extend  the  measurement  range  of  a  low-cost  device  that  automatically  prescribes  eyeglasses  for patients with a large range of refractive errors.             ","Development of a low-cost, portable autorefractor for the measuring of refractive errors in low-resource settings",8981552,R43EY025452,"['Adoption', 'Algorithms', 'Brazil', 'Caring', 'China', 'Client satisfaction', 'Clinical Trials', 'Communities', 'Databases', 'Detection', 'Development', 'Devices', 'Education', 'Ensure', 'Eye', 'Eyeglasses', 'Feedback', 'Goals', 'Health', 'Hospitals', 'Human', 'Human Resources', 'Hyperopia', 'Image', 'India', 'Laser In Situ Keratomileusis', 'Letters', 'Licensing', 'Lighting', 'Machine Learning', 'Marketing', 'Measurement', 'Measures', 'Methods', 'Mission', 'Modeling', 'Myopia', 'New England', 'Nurses', 'Operative Surgical Procedures', 'Optics', 'Optometrist', 'Optometry', 'Output', 'Patients', 'Performance', 'Pharmacists', 'Phase', 'Population', 'Positioning Attribute', 'Productivity', 'Property', 'Protocols documentation', 'Provider', 'Pupil', 'Quality of life', 'Refractive Errors', 'Resources', 'Rest', 'Retina', 'Source', 'Spottings', 'System', 'Target Populations', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Vision', 'Visual Acuity', 'Work', 'base', 'college', 'commercialization', 'cost', 'design', 'digital', 'disability', 'improved', 'lens', 'meetings', 'prototype', 'software systems', 'success', 'usability']",NEI,"PLENOPTIKA, INC.",R43,2015,149265,0.014172835362485548
"Development of epidermal, wireless sensor tattoos for non-invasive monitoring of biofluid composition ﻿    DESCRIPTION (provided by applicant): Non-invasive medical devices are fast emerging as powerful tools in providing quantitative, simple to parse data in assessing the health and wellness of users. These devices can provide information feedback to users on their state of health, and can potentially provide valuable real-time insight to doctors on the links between user consumption and activity on their respective health. Recently described epidermal electronics / multifunctional tattoos introduced from our lab and others have demonstrated sensing of biopressure, bioelectricity, analyte concentration, bacteria, and more. These structures potentially represent the evolution of wearable devices as they possess a negligible form factor and conform to any surface (such as skin or teeth), minimizing user impact. These devices also bypass more traditional, ""wearable"" gadgets that often require bulky mechanical fixtures or straps, require complex microfluidic systems or integrated electronics, cannot provide dynamic readout, and cannot be disposed of easily. Dielectric sensors are a class of structures that are able to probe the composition of a biofluid via their impedance spectrum, and can be configured for remote sensing via radio waves. These devices can be composed of isolated, thin film circuits, and are directly amenable to epidermal or tattoo formats. This measurement methodology is inherently tremendously powerful, as it can potentially measure multiple analytes at once by probing multiple resonance peaks, and can potentially be piggybacked onto existing RFID infrastructure for data readout. However, these capabilities have not yet been demonstrated, and under real-world applications dielectric sensors have often proven difficult to use. This is often due to simplistic readout (devices will directly correlate a single metric such s the real value of the impedance at a frequency to a biomarker) and are thus can be sensitive from user to user or to environmental conditions. Herein, the applicant will leverage the expertise and knowledge of the labs of Dr. Omenetto and colleagues to bring practical usability to these dielectric antennas, bridging the gap between laboratory measurement and real-time, and real-world health monitoring applications. The end-goal is to generate disposable, skin and teeth-mounted, dielectric sensors to remotely probe the presence of biomarkers in sweat, saliva, and potentially blood to draw definitive links between user nutrition and their health.         PUBLIC HEALTH RELEVANCE: This proposal seeks to create non-invasive, wireless sensor tattoos to be tagged onto the human body for probing the composition of saliva, sweat, or blood. These wireless sensors would present negligible impediment to the user, and would require next to no upkeep to maintain, potentially facilitating an unprecedented level of subject data collection and dissemination. Such data could yield conclusive links between diet, activity, biomarkers, and public health.            ","Development of epidermal, wireless sensor tattoos for non-invasive monitoring of biofluid composition",8980210,F32EB021159,"['Architecture', 'Bacteria', 'Behavior', 'Biocompatible Materials', 'Biological', 'Biological Markers', 'Blood', 'Bypass', 'Cells', 'Characteristics', 'Classification', 'Complex', 'Consumption', 'Data', 'Data Analyses', 'Data Collection', 'Detection', 'Development', 'Devices', 'Diet', 'Electronics', 'Evolution', 'Feedback', 'Film', 'Fingerprint', 'Frequencies', 'Goals', 'Health', 'Human body', 'Ions', 'Knowledge', 'Laboratories', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Mechanics', 'Medical Device', 'Methodology', 'Microfluidics', 'Monitor', 'Nafion', 'Pattern', 'Performance', 'Principal Component Analysis', 'Public Health', 'Radio', 'Radio Waves', 'Reader', 'Research Infrastructure', 'Running', 'Saliva', 'Silk', 'Skin', 'Sodium', 'Solutions', 'Stimulus', 'Structure', 'Surface', 'Sweat', 'Sweating', 'System', 'Tattooing', 'Testing', 'Time', 'Tooth structure', 'Training', 'Urea', 'Validation', 'Wireless Technology', 'World Health', 'bioelectricity', 'design', 'electric impedance', 'enzyme activity', 'flexibility', 'glucose monitor', 'improved', 'insight', 'non-invasive monitor', 'nutrition', 'public health relevance', 'real world application', 'remote sensing', 'response', 'saliva composition', 'sensor', 'tool', 'usability']",NIBIB,TUFTS UNIVERSITY MEDFORD,F32,2015,56042,0.038855736288279596
"Unassisted Blood Pressure Monitoring Using Arterial Tonometry and Photoplethysmography ﻿    DESCRIPTION (provided by applicant):  We propose a tonometry based solution to the problem of inexpensive, unencumbering and non- invasive measurement of blood pressure. The proposed solution will be useful in clinical as well as ambulatory blood pressure measurements. Specifically, we propose to develop a large (1cm x 5cm) two-dimensional array of pressure sensors with 0.2mm spacing (24x128 pressure-sensing elements), together with robust signal processing algorithms with a feedback controlled band-tightening mechanism in order to measure blood pressure at the wrist. The pressure sensor array together with a photoplethysmogram (PPG) sensor will be embedded in a band, and will provide signals through a multiplexed circuit designed. Signal conditioning techniques will be used to get the large amount of data in the form that can be efficiently processed on a microcontroller. The proposed two-dimensional array of pressure sensors will be built using flexible plastic and micro-fabrication techniques. The increased size of the sensor array will ensure proper contact and pressure application with arteries in the wrist for tonometric measurement of BP. The signals generated from the sensor array will be processed through advanced signal processing and optimization techniques to handle the huge amount of data and to mitigate noise. The PPG signals will be used at the wrist to improve the efficiency and accuracy of the system. The sensor array system will be embodied in the form of a band, which will be integrated in a wearable device such as smartwatch. The capabilities of the smartwatch will be used for data processing and analytics. a) Design, develop, and test a two-dimensional pressure sensor array on a polyimide flexible substrate b) Process large amount of analog data from sensor array using signal conditioning and processing techniques  to generate blood pressure estimates c) Design a user friendly prototype form factor, and perform a clinical trial to validate device performance  The engineering members of our multidisciplinary team have specialized in cardiac instrumentation, signal and image processing for medical applications, pressure and touch sensors, signal processing and robust optimization. The team members with clinical expertise have been engaged in blood pressure trials in US, and have been working with leading institutions in India engaged in cardiovascular research. This proposal brings together their unique expertise and experience to innovatively address this challenging problem through a joint effort.         PUBLIC HEALTH RELEVANCE: Unmanaged hypertension is a major problem, and its management based on occasional measurement is known to be suboptimal. The ability to measure blood pressure using non- invasive techniques in an ambulatory setting has many significant benefits. We propose to research and develop a large (1cm x 5cm) two-dimensional array of pressure sensors based device together with robust signal processing algorithms with a feedback controlled to measure blood pressure at the wrist. The device will be test in a clinical trial based on the European Society of Hypertension International Protocol.                ",Unassisted Blood Pressure Monitoring Using Arterial Tonometry and Photoplethysmography,8936340,U01EB020589,"['Address', 'Algorithms', 'Area', 'Arteries', 'Blood Pressure', 'Blood Pressure Monitors', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Engineering', 'Clinical Trials', 'Data', 'Devices', 'Elements', 'Engineering', 'Ensure', 'Environment', 'European', 'Fatigue', 'Feedback', 'Generations', 'Goals', 'Hypertension', 'India', 'Institution', 'International', 'Joints', 'Lead', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Noise', 'Operative Surgical Procedures', 'Output', 'Patients', 'Performance', 'Photoplethysmography', 'Plastics', 'Process', 'Protocols documentation', 'Research', 'Signal Transduction', 'Societies', 'Solutions', 'Somatotype', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Touch sensation', 'Training', 'Untrained Personnel', 'Work', 'Wrist', 'analog', 'arterial tonometry', 'base', 'computerized data processing', 'conditioning', 'design', 'experience', 'flexibility', 'image processing', 'improved', 'instrumentation', 'member', 'multidisciplinary', 'pressure', 'process optimization', 'prototype', 'public health relevance', 'rural area', 'sensor', 'signal processing', 'tonometry', 'two-dimensional', 'user-friendly']",NIBIB,NORTHWESTERN UNIVERSITY,U01,2015,362070,0.017275847091809935
"Low-Cost, Compact, and Portable Robot for Autonomous Intravenous Access using Nea DESCRIPTION (provided by applicant): Peripheral venous access is pivotal to a wide range of clinical interventions and is consequently the leading cause of medical injury in the U.S. Complications associated with the procedure are exacerbated in difficult settings, where the rate of success depends heavily on the patient's physiology and the practitioner's experience. My dissertation thesis pertains to the development of imaging and robotic technologies to improve the accuracy and speed of blood draws and IV's. The core technology is an image-guided robotic device that accurately and autonomously introduces a cannula for venous access. The device operates by mapping in real-time the 3D structure of peripheral veins in order to robotically direct a needle into a selected vein. A working prototype has been developed and validated in several studies, the results of which are described in two journal publications. The device combines a 3D near-infrared vein imager, a robot, and computer vision software; these three components form the basis of the three Specific Aims described in this proposal. The Aims fit into the overall dissertation by 1) incorporating the current imaging hardware into a standalone, handheld imaging device; 2) introducing software for the imaging device that assists in selecting suitable cannulation sites; and 3) integrating the imaging device and software with a miniaturized version of the current robot. The outcome of this work will be a compact and low-cost system that is suited for beta-stage development. PUBLIC HEALTH RELEVANCE: Blood draws and IV therapies are one of the most commonly performed medical routines in hospitals and clinics. Injuries to doctors and patients happen frequently because of how difficult it can be to find veins and accurately insert the needle. We are developing a portable and lightweight medical robot to perform the procedure in situations where the doctor is unable to successfully access the veins. This device may greatly improve the safety and accuracy of venous access, and has wide applications in many clinical areas.","Low-Cost, Compact, and Portable Robot for Autonomous Intravenous Access using Nea",8832591,F31EB018191,"['Algorithms', 'Anatomy', 'Area', 'Benchmarking', 'Blood', 'Cannulas', 'Cannulations', 'Catheters', 'Childhood', 'Clinical', 'Clinics and Hospitals', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Cues', 'Custom', 'Development', 'Device Safety', 'Devices', 'Emergency Care', 'Evaluation', 'Failure', 'Goals', 'Graph', 'Health', 'Healthcare Systems', 'Human', 'Image', 'Imaging Device', 'In Vitro', 'Injury', 'Institutional Review Boards', 'Intervention', 'Intravenous', 'Journals', 'Knowledge', 'Literature', 'Location', 'Maps', 'Medical', 'Motivation', 'Needles', 'Neonatal', 'Outcome', 'Patients', 'Peripheral', 'Physiology', 'Pilot Projects', 'Population', 'Population Sizes', 'Positioning Attribute', 'Procedures', 'Publications', 'Reporting', 'Robot', 'Robotics', 'Safety', 'Site', 'Speed', 'Sprague-Dawley Rats', 'Staging', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Validation', 'Veins', 'Venous', 'Visual', 'Weight', 'Work', 'arm', 'base', 'cost', 'design', 'experience', 'image guided', 'imaging software', 'imaging system', 'improved', 'in vivo', 'meetings', 'miniaturize', 'pre-clinical', 'prototype', 'robotic device', 'skeletal', 'success', 'three dimensional structure', 'three-dimensional modeling', 'tissue phantom', 'visual motor']",NIBIB,"RUTGERS, THE STATE UNIV OF N.J.",F31,2015,32250,0.020075506684854545
"Enabling access to printed text for blind people via assisted mobile OCR     DESCRIPTION (provided by applicant): This application proposes new technology development and user studies aiming to facilitate the use of mobile Optical Character Recognition (OCR) for blind people. Mobile OCR systems, implemented as smartphones apps, have recently appeared on the market. This technology unleashes the power of modern computer vision algorithms to enable a blind person to hear (via synthetic speech) the content of printed text imaged by the smartphone's camera. Unlike traditional OCR, that requires scanning of a document with a flatbed scanner, mobile OCR apps enable access to text anywhere, anytime. Using their own smartphones, blind people can read store receipts, menus, flyers, business cards, utility bills, and many other printed documents of the type normally encountered in everyday life. Unfortunately, current mobile OCR systems suffer from a chicken-and-egg problem, which limits their usability. They require the user to take a well-framed snapshot of the document to be scanned, with the full text in view, and at a close enough distance that each character can be well resolved and thus readable by the machine. However, taking a good picture of a document is difficult without sight, and thus without the ability to look at the scene being imaged by the camera through the smartphone's screen. Anecdotal evidence, supported by results of preliminary studies conducted by the principal investigator's group, confirms that acquisition of an OCR-readable image of a document can indeed by very challenging for some blind users. We plan to address this problem by developing and testing a new technique of assisted mobile OCR. As the user aims the camera at the document, the system analyzes in real time the stream of images acquired by the camera, and determines how the camera position and orientation should be adjusted so that an OCR-readable image of the document can be acquired. This information is conveyed to the user via a specially designed acoustic signal. This acoustic feedback allows users to quickly adjust and reorient the camera or the document, resulting in reduced access time and in more satisfactory user experience. Multiple user studies with blind participants are planned with the purpose of selecting an appropriate acoustic interface and of evaluating the effectiveness of the proposed assisted mobile OCR modality.         PUBLIC HEALTH RELEVANCE: This application is concerned with the development of new technology designed to facilitate use of mobile Optical Character Recognition (OCR) systems to access printed text without sight. Specifically, this exploratory research will develop and test a novel system that, by means of a specially designed acoustic interface, will help a blind person take a well-framed, well-resolved image of a document for OCR processing using a smartphone or wearable camera. If successful, this novel approach to assisted mobile OCR will reduce access time and improve user experience of blind mobile OCR users.                ",Enabling access to printed text for blind people via assisted mobile OCR,8812658,R21EY025077,"['Acoustics', 'Address', 'Algorithms', 'Businesses', 'Cellular Phone', 'Chest', 'Chickens', 'Clothing', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Disabled Persons', 'Education', 'Effectiveness', 'Employment', 'Environment', 'Eyeglasses', 'Feedback', 'Goals', 'Hand', 'Hearing', 'Image', 'Knowledge', 'Life', 'Light', 'Location', 'Marketing', 'Modality', 'Monitor', 'Participant', 'Pattern', 'Positioning Attribute', 'Principal Investigator', 'Printing', 'Process', 'Quality of life', 'Reading', 'Report (document)', 'Research', 'Resolution', 'Restaurants', 'Scanning', 'Series', 'Signal Transduction', 'Solutions', 'Speech', 'Stream', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Technology Development Study', 'Testing', 'Text', 'Time', 'Translating', 'Travel', 'Vision', 'Visual', 'Visually Impaired Persons', 'blind', 'design', 'egg', 'experience', 'handicapping condition', 'improved', 'new technology', 'novel', 'novel strategies', 'object recognition', 'optical character recognition', 'public health relevance', 'research study', 'technology development', 'usability', 'way finding']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2015,191510,0.022555166948246042
"CRCNS: Model-driven single-neuron studies of cortical remapping DESCRIPTION (provided by applicant): During natural vision humans and non-human primates make several saccadic eye movements each second that result in large changes in the retinal input. Despite these often dramatic changes, our visual percept remains remarkably stable and we can readily attend to and direct motor actions towards objects in our visual environment. This project will use a model-driven approach to investigate the neural circuits linking vision, attention and oculomotor planning that stabilize perceptual and attentional representations during natural vision. Experimental data will be collected and used to design a detailed computational model of the visual and oculomotor areas involved in saccade compensation. The proposed collaboration between a computational (DE) and experimental neurophysiological (US) laboratories leverages the power of both disciplines. Biologically accurate models of visually guided behavior and trans-saccadic integration developed in the Hamker lab will guide the design of and interpretation of data obtained from neurophysiological experiments in awake, behaving primates performed in the Mazer lab in an iterative fashion, with experimental results informing model revisions and new model predictions altering experimental designs. The proposed studies will characterize both dorsal and ventral stream visual area contributions to stabilizing visual and attentional representations in the primate brai. Data obtained from these experiments will identify the neural circuits responsible for integrating oculomotor commands, bottom-up visual inputs and top-down attention signals. This approach will yield novel insights into interactions between the dorsal and ventral streams during natural vision and facilitate our understanding of goal-directed, active visual perception, a defining feature of human and non-human primate natural vision. A critical component of this project is the highly collaborative nature of the planned research. We expect great benefits from this interdisciplinary approach, which depends critically on computational models that strictly adhere to the known physiological and anatomical constraints to guide our neurophysiological experiments.     1. Training. The proposal includes a detailed training plan intended to facilitate international training of future modelers and neurophysiologists. Specifically, we will train students and post-doctoral researchers to be experts in both experimental and theoretical approaches in order to advance the field using the hybrid approach outlined in the proposal.    2. Education and Outreach. We plan to organize two in-depth workshops on attention and eye movements. These events (one in Germany and one in the US) will bring together investigators from other institutions and related scientific disciplines to advance the field. In addition investigators will organize and chair 1-2 workshops/symposia at annual meetings (e.g., SFN and COSYNE) during the funding period. Finally, we will participate in science education for underrepresented groups through Yale's STARS program by providing training, research and mentoring opportunities in the Mazer lab.    3. Data Sharing. The software tools generated and behavioral and neurophysiological data collected during this project will be distributed to the neuroscience community to facilitate data mining and secondary analyses of experimental data.    4. Impact in other scientific fields. Efficient allocation of limited sensor resources is also a important problem faced by computer vision and robotics researchers. Understanding how the primate brain efficiently allocates visual resources using an active-sensing approach will guide development of biologically inspired computer vision algorithms and humanoid cognitive robots.    5. Translational Implications. Although the proposed research is not translational, there is a growing body of evidence suggesting that several clinically important conditions, including Autism Spectrum Disorder, Attention Deficit Hyperactivity Disorder and Schizophrenia, are associated with impaired behavioral links between saccade planning and visual attention. The proposed basic science studies could have significant implications for future translational research potentially leading to improved understanding disease etiology, development of early diagnostic tools and possible interventional strategies. n/a",CRCNS: Model-driven single-neuron studies of cortical remapping,8928626,R01EY025103,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Area', 'Attention', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Dorsal', 'Education and Outreach', 'Educational workshop', 'Environment', 'Etiology', 'Event', 'Experimental Designs', 'Eye Movements', 'Financial compensation', 'Funding', 'Future', 'Germany', 'Goals', 'Human', 'Hybrids', 'Institution', 'International', 'Intervention', 'Laboratories', 'Link', 'Mentors', 'Modeling', 'Monkeys', 'Motor', 'Nature', 'Neurons', 'Neurosciences', 'Performance', 'Physiological', 'Postdoctoral Fellow', 'Primates', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Retinal', 'Robot', 'Robotics', 'Saccades', 'Schizophrenia', 'Signal Transduction', 'Software Tools', 'Stream', 'Students', 'Testing', 'Training', 'Translational Research', 'Underrepresented Minority', 'Update', 'Vision', 'Visual', 'Visual Perception', 'Visual attention', 'Work', 'area V4', 'autism spectrum disorder', 'awake', 'base', 'cell type', 'computer framework', 'data mining', 'data sharing', 'design', 'extrastriate visual cortex', 'improved', 'insight', 'interdisciplinary approach', 'meetings', 'model building', 'neural circuit', 'neurophysiology', 'nonhuman primate', 'novel', 'oculomotor', 'programs', 'relating to nervous system', 'research study', 'science education', 'sensor', 'simulation', 'spatiotemporal', 'symposium', 'tool', 'vector', 'visual motor']",NEI,YALE UNIVERSITY,R01,2015,203962,-0.006237284113856249
"Understanding Real-Life Falls in Amputees using Mobile Phone Technology DESCRIPTION (provided by applicant): Falls are a significant cause of death and serious injury and result in significant health-care costs. Individuals with a lower extremity amputation due to vascular disease are overwhelmingly elderly (at least 65 years of age) and are at especially high risk of falling. Successful fall prevention strategies depend on understanding how, why, when, and where individuals fall, and what types of falls (e.g., trip, slip, or lateral fll) are likely in a given population. Most studies on falls in amputees to date have relied surveys or questionnaires that are often completed a significant time after the fall and thus rely both on the individual's ability to remember the details of their fall and their willingness to be objective abut how and why they fell. Such approaches are susceptible both to inaccurate memories of the fall and to recall bias-for example, due to embarrassment about falling- and are especially unreliable in the elderly amputees. Mobile phones provide a simple, cost-effective method for detection and characterization of falls. Most available smart phones today have a tri-axial accelerometer, which provides highly accurate fall detection in real-time. Other available applications (or apps) can provide data on activity (running, walking etc.) and environment-such as the weather conditions or population density-that may have contributed to the fall and can pin-point the location of the fall-using GPS technology and highly accurate maps. Mobile phones also have inbuilt data storage and transfer capability, allowing for real-time acquisition and transmission of data. Additionally, mobile phones provide a simple means to contact the individual immediately after a suspected fall to confirm details of the fall (and to ascertain the need for medical assistance). Because mobile phone use is so widespread, there is no stigma associated with carrying such a device, which is likely to lead to high compliance. This study aims to use a mobile phone-based fall detection system in dysvascular amputees to detect falls, characterize the type of fall, analyze environmental conditions that may have contributed to the fall, and determine the longer-term consequences of each type of fall. Data acquired may be used to improve rehabilitation protocols or design better prostheses in order to prevent falls. This technology is ultimately transferrable to many populations with a high risk of falling-for example, the elderly, stroke survivors, or those with other musculoskeletal disorders or disabilities-leading to the design of specific fall prevention strategies for those populations. PUBLIC HEALTH RELEVANCE: Successful fall prevention strategies for the elderly dysvascular amputee population-including better prosthesis design and improved rehabilitation/fall prevention strategies-depend on understanding how, why, when, and where such individuals fall, and what types of fall occur (e.g., trip, slip, or lateral fall). Mobile smartphones with built-in triaxial accelerometers can accurately detect and classify falls, in addition to providing other applications (or 'apps') that can identify contributory environmental conditions (e.g., weather or traffic) or activities (e.g. walking or running) that have may contributed to the fall. Combined with server side analysis of wirelessly transmitted phone data-using machine learning techniques-mobile smartphones provide a simple, portable fall-detection system that generates real-time information on the mechanisms, contributory environmental factors, and consequences of falls in elderly amputees-or in any population at risk of falling.",Understanding Real-Life Falls in Amputees using Mobile Phone Technology,8898070,R01EB019406,"['3-Dimensional', 'Age-Years', 'Algorithms', 'Amputation', 'Amputees', 'Car Phone', 'Cause of Death', 'Cellular Phone', 'Classification', 'Communication', 'Communities', 'Crowding', 'Data', 'Data Collection', 'Data Quality', 'Data Set', 'Data Storage and Retrieval', 'Detection', 'Devices', 'Elderly', 'Emergency department visit', 'Environment', 'Environmental Risk Factor', 'Etiology', 'Event', 'Fall prevention', 'Family', 'Fright', 'Goals', 'Health', 'Health Care Costs', 'Hospitals', 'Individual', 'Injury', 'Interview', 'Knowledge', 'Laboratories', 'Lateral', 'Lead', 'Life', 'Location', 'Longitudinal Studies', 'Lower Extremity', 'Machine Learning', 'Maps', 'Medical Assistance', 'Medical Care Costs', 'Memory', 'Methods', 'Morbidity - disease rate', 'Musculoskeletal Diseases', 'Outcome', 'Patients', 'Persons', 'Population', 'Population Density', 'Populations at Risk', 'Prevalence', 'Prevention strategy', 'Prosthesis', 'Prosthesis Design', 'Protocols documentation', 'Publications', 'Quality of life', 'Questionnaires', 'Rain', 'Real-Time Systems', 'Recovery', 'Rehabilitation therapy', 'Reporting', 'Research', 'Running', 'Side', 'Stroke', 'Surveys', 'Survivors', 'System', 'Techniques', 'Technology', 'Telephone', 'Time', 'Vascular Diseases', 'Visit', 'Walking', 'Weather', 'Wireless Technology', 'Work', 'base', 'cohort', 'cost effective', 'data exchange', 'design', 'diaries', 'disability', 'falls', 'fear of falling', 'health care quality', 'high risk', 'improved', 'improved mobility', 'information gathering', 'mortality', 'new technology', 'novel', 'prospective', 'sensor', 'social stigma', 'trafficking', 'willingness']",NIBIB,REHABILITATION INSTITUTE OF CHICAGO D/B/A SHIRLEY RYAN ABILITYLAB,R01,2015,164698,0.0028040063003246223
"A Visual Assessment System for Retinal Function/Drug Discovery ﻿    DESCRIPTION (provided by applicant): Preclinical evaluation of treatment strategies for retinal neurodegenerative diseases is highly dependent on mouse models. Classical methods to assess the visual function of animals, such as electroretinogram (ERG), which measures electrical responses in the retina, do not address connections between the eye and brain or visual perception by the visual system. This often raises concerns regarding the functional relevance of the therapeutic benefit. Difficulty in assessing visual perception and related behavior in mice and rats, largely due to their subtle visual behavior cues and the lack of adequate measuring devices, presents a critical barrier to the application of mouse models for evaluating treatment efficacy of new drugs, and for scaling up for behavior phenotyping to screen genetic vision defects. Pupillary light reflex (PLR) and optokinetic reflex (OKR) tests are useful methods in clinics for assessing human visual responses and perception. However, such tests have been difficult to conduct in rodents because current rodent visual testing methods or devices either do not allow accurate quantitative assessment for PLR or OKR or use subjective measures to score visual responses. To address these challenges, we propose to advance the technology by designing an easy-to-use automated platform that employs an eye/pupil tracking device equipped with a computer vision system (chiefly the interactive tracking system) for unambiguous objective scoring of visual responses. Our proposed new device will allow real-time quantitative and accurate assessment of rodent visual function including light responses, visual acuity and contrast sensitivity. The novelty of our system also lies in that it does not require complicated calibration procedures needed in commonly used human eye tracking. Rather than precisely measuring the extent of eye turning (or orientation), we propose to detect the signature eye movement in accordance with the speed and direction of visual stimuli. The system will be validated using normal wildtype mice and mouse models of retinal neurodegeneration known to develop visual behavior changes in the parameters mentioned above. Although rodent eye tracking has been investigated before, this proposed visual assessment system would be the first commercially viable product that uses an eye/pupil tracking device to automatically assess visual perception in rodents. The combined PLR and OKR tests and vastly simplified and automated quantification methods will also provide the first scalable behavior platform for phenotyping and drug discovery in the vision research area. In the future, this technology has the potential of being expanded to measure responses from various visual stimuli. This may translate into broader applications for evaluating brain diseases that afflict the visual pathways. This platform for mouse visual behavior assessment will therefore greatly facilitate drug discovery and development aimed at preventing and slowing vision loss or restoring sight, helping to combat devastating blinding conditions such as age-related macular degeneration (AMD) and glaucoma.         PUBLIC HEALTH RELEVANCE: The objective of the current proposal is to design and develop an automated system for the measure of rodent (mice and rats) light response, visual acuity, and contrast sensitivity. The system will apply human eye/pupil tracking techniques for objective and unambiguous evaluation of light response and visual perception. This platform will provide a powerful tool for phenotypic studies as well as for discovery of new drugs that can prevent or restore sight caused by blinding conditions such as age-related macular degeneration and glaucoma.                ",A Visual Assessment System for Retinal Function/Drug Discovery,8980787,R41EY025913,"['Address', 'Age related macular degeneration', 'Algorithms', 'Animal Model', 'Animals', 'Area', 'Behavior', 'Behavior assessment', 'Behavior monitoring', 'Behavioral', 'Biological Assay', 'Blindness', 'Brain', 'Brain Diseases', 'Calibration', 'Clinic', 'Collaborations', 'Coma', 'Computer Vision Systems', 'Contrast Sensitivity', 'Cues', 'Data Analyses', 'Defect', 'Development', 'Devices', 'Disease', 'Electroretinography', 'Evaluation', 'Eye', 'Eye Movements', 'Eye diseases', 'Funding', 'Future', 'Genetic Screening', 'Glaucoma', 'Head', 'Head Movements', 'Human', 'Image', 'Impairment', 'Laboratory Animals', 'Lead', 'Light', 'Marketing', 'Measurement', 'Measures', 'Medical', 'Methods', 'Mus', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Patients', 'Pattern', 'Perception', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Photic Stimulation', 'Preclinical Drug Evaluation', 'Procedures', 'Pupil', 'Pupil light reflex', 'Rattus', 'Reflex action', 'Research Institute', 'Retina', 'Retinal', 'Retinal Degeneration', 'Retinal Diseases', 'Rodent', 'Small Business Technology Transfer Research', 'Speed', 'System', 'Techniques', 'Technology', 'Technology Transfer', 'Testing', 'Therapeutic', 'Time', 'Training', 'Transgenic Mice', 'Translating', 'Translations', 'Treatment Efficacy', 'Vision', 'Vision research', 'Visual', 'Visual Acuity', 'Visual Pathways', 'Visual Perception', 'Visual system structure', 'approach behavior', 'base', 'behavior change', 'combat', 'commercialization', 'computer generated', 'data acquisition', 'design', 'drug development', 'drug discovery', 'genetic approach', 'innovation', 'instrument', 'liquid crystal', 'mouse model', 'new technology', 'performance tests', 'photoreceptor degeneration', 'preclinical evaluation', 'prevent', 'prototype', 'public health relevance', 'response', 'scale up', 'success', 'tool', 'touchscreen', 'treatment strategy', 'visual performance', 'visual stimulus']",NEI,"AFASCI, INC.",R41,2015,233235,-0.001051326629599775
"Context Understanding Technology to improve internet accessibility for users with DESCRIPTION (provided by applicant): The purpose of this SBIR project is to develop a new tool to improve the ability of blind and visually impaired people to quickly get to the right web pages, to avoid ending up on the wrong web pages, and to reach the desired part of each web page efficiently. The motivation for this effort is that getting a 'big picture' understanding of a web page is one of the biggest challenges facing this population of computer users. Existing tools for blind and visually impaired people, such as screen readers and screen magnifiers, present the entire web page serially, in full detail, either by textual output of the details, or b providing a magnified view of a small part of the page. However, if the user is not already familiar with the website, the resulting lack of context requires a laborious and time-consuming process to scan through sometimes hundreds of details before knowing whether anything of interest even exists on the page in question. In contrast, the proposed technology analyzes web pages in a similar way that sighted users quickly scan pages before reading in detail, to provide a succinct, informative guidance on the context and layout of the page, so that a user quickly gains a much richer ""big-picture"" understanding. The proposed Phase II project builds on a successful Phase I user evaluation of a proof- of-concept of the software, in which users expressed a strong preference for the contextual cues provided by the approach. Phase II includes creating a fully-functional prototype of the software tool. Throughout the technology development, feedback from users and practitioners will guide the path of the R&D for maximum usability. At the conclusion of Phase II, an end-user, in-home evaluation study will verify the effectiveness of the tool, providing the starting point for full-scale commercialization. PUBLIC HEALTH RELEVANCE: The R&D in this Phase II SBIR project will result in an improved way to use the Internet for individuals with visual disabilities. The technology will help to overcome the lack of accessibility and high level of frustration currently found among blind and visually impaired users. The results of the project will enable such individuals to explore new opportunities and be more productive, thus providing improved employment potential and an increase in the quality of life.",Context Understanding Technology to improve internet accessibility for users with,8795182,R44EY020082,"['Advertisements', 'Area', 'Arizona', 'Artificial Intelligence', 'Boxing', 'Characteristics', 'Color', 'Computer software', 'Computers', 'Cues', 'Effectiveness', 'Employment', 'Evaluation', 'Evaluation Studies', 'Feedback', 'Frustration', 'Generations', 'Government', 'Grouping', 'Health', 'Home environment', 'Individual', 'Interest Group', 'Internet', 'Literature', 'Marketing', 'Motivation', 'Mus', 'Output', 'Persons', 'Phase', 'Population', 'Process', 'Quality of life', 'Reader', 'Reading', 'Research', 'Scanning', 'Small Business Innovation Research Grant', 'Software Tools', 'Speech', 'System', 'Technology', 'Text', 'Time', 'Training', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'base', 'blind', 'commercialization', 'disability', 'experience', 'improved', 'interest', 'natural language', 'phrases', 'preference', 'prototype', 'research and development', 'technology development', 'tool', 'usability', 'web page', 'web site']",NEI,"VORTANT TECHNOLOGIES, LLC",R44,2015,195445,0.041752215235868716
"Designing Visually Accessible Spaces DESCRIPTION (provided by applicant):  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals, including those with physical disabilities and to enhance safety for older people and others who may need to operate under low lighting and other visually challenging conditions.  We plan to develop a computer-based design tool enabling architectural design professionals to assess the visual accessibility of a wide range of environments (such as a hotel lobby, subway station, or eye-clinic reception area).  This tool will simulate such environments with sufficient accuracy to predict the visibility of key landmarks or hazards, such as steps or benches, for different levels and types of low vision, and for spaces varying in lighting, surface properties and geometric arrangement.  Our project addresses one of the National Eye Institute's program objectives:  ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments."" Our research plan has three specific goals:  1) Empirical:  determine factors that influence low-vision accessibility related to hazard detection and navigation in real-world spaces.  2) Computational:  develop working models to predict low vision visibility and navigability in real-world spaces.  3) Deployment:  translate findings from basic vision science and clinical low vision into much needed industrial usage by producing a set of open source software modules to enhance architectural and lighting design for visual accessibility.  The key scientific personnel in our partnership come from three institutions:  University of Minnesota -- Gordon Legge and Daniel Kersten; University of Utah -- William Thompson and Sarah Creem-Regehr; and Indiana University -- Robert Shakespeare.  This interdisciplinary team has expertise in the necessary areas required for programmatic research on visual accessibility -- empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), computer graphics and photometrically accurate rendering (Thompson & Shakespeare) and architectural lighting design (Shakespeare).  We have collaborative arrangements with additional architectural design professionals who will participate in the translation of our research and development into practice. PUBLIC HEALTH RELEVANCE:  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our BRP partnership, with interdisciplinary expertise in vision science, computer science and lighting design, plans to develop computer-based tools enabling architecture professionals to assess the visual accessibility of their designs.",Designing Visually Accessible Spaces,8815314,R01EY017835,"['Accounting', 'Address', 'Age', 'American', 'Architecture', 'Area', 'Biological Models', 'Blindness', 'Central Scotomas', 'Characteristics', 'Clinic', 'Clinical', 'Complement', 'Complex', 'Computer Analysis', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Contrast Sensitivity', 'Cues', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Disorientation', 'Distance Perception', 'Effectiveness', 'Environment', 'Evaluation', 'Eye', 'Geometry', 'Glare', 'Goals', 'Grant', 'Guidelines', 'Health', 'Height', 'Human', 'Human Resources', 'Indiana', 'Individual', 'Injury', 'Institution', 'Investigation', 'Judgment', 'Learning', 'Light', 'Lighting', 'Lobbying', 'Location', 'Minnesota', 'Modeling', 'Modification', 'National Eye Institute', 'Nature', 'Optics', 'Perception', 'Peripheral', 'Phase', 'Photometry', 'Physical environment', 'Physically Handicapped', 'Positioning Attribute', 'Process', 'Production', 'Property', 'Research', 'Risk', 'Safety', 'Shapes', 'Source', 'Specific qualifier value', 'Stimulus', 'Structure', 'Subway', 'Surface', 'Surface Properties', 'System', 'Test Result', 'Testing', 'Translating', 'Translations', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual Acuity', 'Visual Fields', 'Visual impairment', 'Work', 'base', 'computer science', 'design', 'disability', 'falls', 'hazard', 'imaging system', 'improved', 'knowledge base', 'luminance', 'novel strategies', 'open source', 'programs', 'research and development', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2015,550561,0.06011487784224445
"NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair     DESCRIPTION (provided by applicant):         The aim of this proposal is to conduct research on the foundational models and algorithms in computer vision and machine learning for an egocentric vision based active learning co-robot wheelchair system to improve the quality of life of elders and disabled who have limited hand functionality or no hand functionality at all, and rely on wheelchairs for mobility. In this co-robt system, the wheelchair users wear a pair of egocentric camera glasses, i.e., the camera is capturing the users' field-of-the-views. This project help reduce the patients' reliance on care-givers. It fits NINR's mission in addressing key issues raised by the Nation's aging population and shortages of healthcare workforces, and in supporting patient-focused research that encourage and enable individuals to become guardians of their own well-beings.  The egocentric camera serves two purposes. On one hand, from vision based motion sensing, the system can capture unique head motion patterns of the users to control the robot wheelchair in a noninvasive way. Secondly, it serves as a unique environment aware vision sensor for the co-robot system as the user will naturally respond to the surroundings by turning their focus of attention, either consciously or subconsciously. Based on the inputs from the egocentric vision sensor and other on-board robotic sensors, an online learning reservoir computing network is exploited, which not only enables the robotic wheelchair system to actively solicit controls from the users when uncertainty is too high for autonomous operation, but also facilitates the robotic wheelchair system to learn from the solicited user controls. This way, the closed- loop co-robot wheelchair system will evolve and be more capable of handling more complicated environment overtime.  The aims ofthe project include: 1) develop an method to harness egocentric computer vision-based sensing of head movements as an alternative method for wheelchair control; 2) develop a method leveraging visual motion from the egocentric camera for category independent moving obstacle detection; and 3) close the loop of the active learning co-robot wheelchair system through uncertainty based active online learning.          PUBLIC HEALTH RELEVANCE:          The project will improve the quality of life of those elders and disabled who rely on wheelchairs for mobility, and reduce their reliance on care-givers. It builds an intelligent wheelchair robot system that can adapt itself to the personalized behavior of the user. The project addresses the need of approximately 1 % of our world's population, including millions of Americans, who rely on wheelchair for mobility.             ",NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair,8838311,R01NR015371,"['Active Learning', 'Address', 'Algorithms', 'American', 'Attention', 'Behavior', 'Build-it', 'Caregivers', 'Categories', 'Computer Vision Systems', 'Detection', 'Disabled Persons', 'E-learning', 'Elderly', 'Environment', 'Glass', 'Hand', 'Head', 'Head Movements', 'Healthcare', 'Individual', 'Learning', 'Machine Learning', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motivation', 'Patients', 'Pattern', 'Population', 'Principal Investigator', 'Quality of life', 'Reliance', 'Research', 'Robot', 'Robotics', 'System', 'Uncertainty', 'Vision', 'Visual Motion', 'Wheelchairs', 'aging population', 'base', 'improved', 'operation', 'programs', 'public health relevance', 'sensor']",NINR,STEVENS INSTITUTE OF TECHNOLOGY,R01,2014,155663,0.036111410160356607
"Computational Image Analysis for Cellular and Developmental Biology     DESCRIPTION (provided by applicant): This proposal requests support for an intensive ten-day course on Computational Image Analysis for Cellular and Developmental Biology. The course is designed for graduate students and postdoctoral fellows, and takes place at the Marine Biological Laboratory in Woods Hole, MA. The course is the first of its kind, giving students formal training in computer vision for the specific analysis of cell and developmental biology image data. Building strong foundations in this topic is critical for pushing cell and developmental biology forward, as imaging has become more and more an indispensable tool in these fields. The course covers the fundamentals of computer vision, taking the students through the sequence of low-, intermediate-, and high-level computer vision tasks that are required to solve image analysis problems in quantitative cellular and developmental biology. The curriculum starts with filtering, thresholding and edge/line/generic feature detection, followed by more sophisticated detection algorithms that employ model fitting. After this introductory block to low-level computer vision tasks, the course moves on to intermediate and higher-level tasks, including object association in space and time (such as tracking) and machine learning tools for phenotype classification. Each topic is covered first by a lecture, generally taught by one of the four core faculty, followed by a 3-4 hour computer programming session where students immediately implement the concepts they learn. There are usually two lectures + computer labs per day. Most programming exercises are individual, giving each student the opportunity to ""get their hands dirty,"" while two are team projects allowing the students to also learn and practice methods of code sharing. Over the course's ten days there are three guest lectures by leading researchers in the fields of biological imaging and computer vision, each followed by in-depth discussion, as well as research talks given by the students, faculty and teaching assistants. With this, the core lectures and labs teach the students the fundamentals of computer vision in a logical, continuous manner, the guest lecturers introduce the students to exciting new challenges in imaging and image analysis, and the student/faculty research talks encourage communication between all course participants and give especially the students the opportunity to reflect on how the course can help them with their research. !         PUBLIC HEALTH RELEVANCE:  Imaging has become an indispensable tool in cellular and developmental biology research, but without rigorous, quantitative image analysis it cannot achieve its full potential. This proposal requests support for a new course that will fill the voidof education in computer vision as applied to cellular and developmental biology. The curriculum incorporates a carefully balanced mixture of lectures and associated programming exercises. We have given a pilot course once in October 2010, with very positive reviews from the students and their advisers.             ",Computational Image Analysis for Cellular and Developmental Biology,8628140,R25GM103792,"['Address', 'Algorithms', 'Biological', 'Cellular biology', 'Classification', 'Code', 'Collection', 'Communication', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Data', 'Detection', 'Development', 'Developmental Biology', 'Developmental Cell Biology', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Equilibrium', 'Exercise', 'Faculty', 'Foundations', 'Future', 'Generic Drugs', 'Goals', 'Hand', 'Home environment', 'Hour', 'Image', 'Image Analysis', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Marines', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Phenotype', 'Postdoctoral Fellow', 'Request for Proposals', 'Research', 'Research Personnel', 'Seeds', 'Software Design', 'Solid', 'Students', 'Time', 'Training', 'Wood material', 'computer program', 'design', 'graduate student', 'lecturer', 'lectures', 'programs', 'public health relevance', 'tool']",NIGMS,MARINE BIOLOGICAL LABORATORY,R25,2014,59383,0.007972048264027903
"Micromachined microphones with in-plane and out-of-plane directivity  Project Summary We aim to introduce to the hearing-assistive device industry the first commercialized microphone that combines all three axes of acoustic pressure gradient onto a single silicon chip. We expect the technology to empower the signal-processing community with a new tool which, when used in conjunction with a conventional omnidirectional microphone, will facilitate new features like ultra-sharp directionality adaptable in real-time by the user and/or artificial intelligence algorithms which scan for desired inputs while filtering out unwanted noise. Directional sensing and the ability to filter out undesirable background acoustic noise are important for those with hearing impairments. Hearing impairment is associated with a loss of fidelity to quiet sounds, while the threshold of pain remains the same. As such, hearing impairment causes a loss of dynamic range or ""window"" of detectable sound amplitudes. Directional sensing enables preferentially amplifying desired sounds without amplifying background noise. As the first step, we aim to accelerate the commercialization of recently introduced biologically- inspired ""rocking"" style microphones by synthesizing these designs with integrated, robust piezoelectric readout which is ideal for addressing the low-power, small-size, and high levels of integration required of the hearing-aid industry. Previous work in this field using laboratory prototypes and optical readout have demonstrated the merits of the biologically-inspired sensing approach (i.e. a simultaneous 20-dB SNR improvement and 10x reduction size improvement beyond what is achievable with present-day hearing-aid or MEMS microphones). By synthesizing a piezoelectric embodiment as an alternative to optical readout, we aim to accelerate through many of the commercialization challenges so that an impact to the hearing device industry can be made. Further, the proposed readout is better adapted towards integrating multiple microphones in the same silicon chip. In Phase II, we aim to integrate a microphone with both in-plane axis of directivity with an out-of-plane directional design to form a complete 3-axis pressure gradient sensor. PUBLIC HEALTH RELEVANCE: Studies show that today 2% of Americans wear a hearing aid, whereas at least 10% of Americans could benefit from a hearing assistive device. The major reason for this gap is patient dissatisfaction. Hearing-aid wearers suffer from what is known as the ""cocktail party"" effect. When the gain is turned up to hear the person speaking across from you, noises in the background are equally amplified - making every scenario sound like a cocktail party. This research aims to make a positive, long-term improvement to hearing-aid patient satisfaction by making commercially available directional microphones with high fidelity.            ",Micromachined microphones with in-plane and out-of-plane directivity,8648777,R43DC013746,"['Acoustics', 'Address', 'Algorithms', 'American', 'Artificial Intelligence', 'Client satisfaction', 'Communities', 'Devices', 'Environment', 'Goals', 'Hearing', 'Hearing Aids', 'Industry', 'Investigation', 'Laboratories', 'Microfabrication', 'Noise', 'Optics', 'Pain Threshold', 'Patients', 'Persons', 'Phase', 'Research', 'Scanning', 'Self-Help Devices', 'Signal Transduction', 'Silicon', 'Small Business Innovation Research Grant', 'Structure', 'Techniques', 'Technology', 'Time', 'Work', 'commercialization', 'design', 'empowered', 'hearing impairment', 'pressure', 'prototype', 'public health relevance', 'sensor', 'signal processing', 'sound', 'tool']",NIDCD,"SILICON AUDIO, INC.",R43,2014,149828,0.008548901686733529
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired     DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering.         PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,8704450,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Healthcare', 'High School Student', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Solutions', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'public health relevance', 'rehabilitation engineering', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2014,51689,0.06226317079937248
"Sign Finding and Reading SFAR on GPU Accelerated Mobile Devices     DESCRIPTION (provided by applicant): The inability to access information on printed signs directly impacts the mobility independence of the over 1.2 million blind persons in the U.S. Many previously proposed technological solutions to this problem either required physical modifications to the environment (talking signs or the placement of coded markers) or required the user to carry around specialized computational equipment, which can be stigmatizing. A recently pursued strategy is to utilize the computational capabilities of smart phones and techniques from computer vision to allow blind persons to read signs at a distance using commercially available, non-stigmatizing, smart- phones. However, despite the fact that sophisticated algorithms exist to recognize and extract sign text from cluttered video input (as evidenced, for example, by mapping services such as Google Maps automatically locating and blurring out only license plate text in street-view maps) current mobile solutions for reading sign text at a distance perform relatively poorly. This poor performance is largely because until recently, smart-phone processors have simply not been able to execute state-of-the-art computer vision text extraction and recognition algorithms at real-time rates, which forced previous mobile sign readers to utilize older, simplistic, less effective algorithms. Next-generation smart-phones run on fundamentally different, hybrid processor architectures (such as the Tegra 4, Snapdragon 800, both released in 2013) with dedicated embedded graphical processing units (GPUs) and multi-core CPUs, which make them ideal for high-performance, vision-heavy computation. In this study, we propose to develop a smart-phone-based system for finding and reading signs at a distance which significantly outperforms previous such readers by implementing state-of-the-art text extraction algorithms on modern smart-phone hybrid GPU/CPU processor architectures. In Phase I, the proposed system will be developed and tested with blind users. In Phase II, feedback from user testing will be integrated into system design and the performance will be improved to permit operation in extremely challenging (such as low light) environments.         PUBLIC HEALTH RELEVANCE: Over 1.2 million people in the US are blind, and lack of safe and independent mobility substantially impacts the quality of life of this population. Printed textual signs, which are ubiquitously used in sighted navigation, are inaccessible to visually impaired persons, and this lack of access to environmental information contributes significantly to the mobility problem. This research would help develop a system whereby blind persons could use commercially available smart-phones to locate and read sign text at a distance.            ",Sign Finding and Reading SFAR on GPU Accelerated Mobile Devices,8779810,R43EY024800,"['Acceleration', 'Access to Information', 'Algorithms', 'Antirrhinum', 'Architecture', 'Back', 'Code', 'Computer Vision Systems', 'Distant', 'Environment', 'Equipment', 'Eye', 'Feedback', 'Hybrids', 'Licensing', 'Light', 'Literature', 'Maps', 'Modification', 'Performance', 'Phase', 'Population', 'Printing', 'Process', 'Quality of life', 'Reader', 'Reading', 'Research', 'Research Institute', 'Risk', 'Running', 'SKI gene', 'Self-Help Devices', 'Services', 'Solutions', 'System', 'Techniques', 'Telephone', 'Test Result', 'Testing', 'Text', 'Time', 'Vision', 'Visually Impaired Persons', 'assistive device/technology', 'authority', 'base', 'blind', 'design', 'experience', 'handheld mobile device', 'improved', 'next generation', 'operation', 'phase 1 study', 'public health relevance', 'volunteer']",NEI,"LYNNTECH, INC.",R43,2014,229742,0.017595335520182217
"Automating Directly Observed Therapy as a Platform Technology     DESCRIPTION (provided by applicant): Introduction: Ai Cure Technologies LLC was established in 2009 to develop automated medication adherence monitoring solutions using computer vision technology. This SBIR Phase II will allow Ai Cure Technologies to test the accuracy and validity of its flagship product, AiView"". The SBIR Phase I demonstrated that the AiView"" platform was technically feasible and capable of confirming medication administration. Significance: Poor medication adherence is a huge burden on clinical research and clinical practice. The inability to accurately measure or improve adherence significantly compounds the problem. Clinical trials depend on people taking the drug being tested. The problem of medication adherence has been addressed - determinants of adherence are being studied and new monitoring methods developed - but no solution has been able to accurately confirm real-time medication adherence while also being affordable, flexible, and likable. The Product: Ai Cure Technologies will provide an automated DOT (Directly Observed Therapy) software platform, AiView"", for use in clinical trials which uses sophisticated computer vision technology on webcam- enabled smart phones or tablets to visually confirm medication administration. AiView"" will visually track and confirm medication administration without human supervision. Long-Term Goal: The AiView"" system will combine sophisticated computer vision technology with the best attributes of DOT for 1/400th of the cost. Automating and standardizing the way medication adherence is captured will help clinical trials better define their subjects' rates of compliance and allow them to intervene immediately in case of non-compliance. Phase II hypothesis: AiView"" can be used to accurately measure and improve medication adherence across different patient populations, and positively impact self-perception and clinical outcomes. Specific Aim #1: To demonstrate that the AiView"" system can accurately measure and improve medication adherence in a depression and a stroke patient population. Specific Aim #2: To demonstrate that the AiView"" system can improve self-perception and improve clinical outcomes in the AiView"" intervention groups Expected Outcome: The patients in the AiView"" intervention groups (depression and stroke) are expected to have statistically significant higher adherence rates than those in the pill counting groups.         PUBLIC HEALTH RELEVANCE: Poor medication adherence is a huge burden on clinical research and clinical practice with the inability to accurately measure or improve adherence significantly compounding the problem. In accordance with this SBIR Phase II grant, Ai Cure Technologies will continue development and testing of its Automated DOTSM (Directly Observed Therapy) software platform, AiView"", for use in clinical trials which uses sophisticated computer vision technology on webcam-enabled smart phones or tablets to visually confirm medication administration. Automating and standardizing the way medication adherence is captured will help clinical trials better define their subjects' rates of compliance and allow tria administrators to intervene immediately in case of non-compliance, thus improving the accuracy of clinical trials the overall safety of the drug development process.            ",Automating Directly Observed Therapy as a Platform Technology,8670794,R44TR000873,"['Address', 'Adherence', 'Administrator', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communication', 'Computer Vision Systems', 'Computer software', 'Control Groups', 'Controlled Environment', 'Data', 'Data Quality', 'Development', 'Directly Observed Therapy', 'Disease Management', 'Drug Prescriptions', 'Electronics', 'Ensure', 'Event', 'Goals', 'Grant', 'Health', 'Health Insurance Portability and Accountability Act', 'Human', 'Intervention', 'Libraries', 'Marketing', 'Measures', 'Mental Depression', 'Methods', 'Monitor', 'Oral cavity', 'Outcome', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phase III Clinical Trials', 'Phase IV Clinical Trials', 'Population', 'Process', 'Recording of previous events', 'Regimen', 'Research', 'Research Personnel', 'Risk', 'Running', 'Safety', 'Secure', 'Self Perception', 'Small Business Innovation Research Grant', 'Solutions', 'Speed', 'Stroke', 'Supervision', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Time', 'Travel', 'base', 'clinical application', 'clinical practice', 'commercial application', 'compliance behavior', 'cost', 'design', 'drug development', 'flexibility', 'group intervention', 'improved', 'medication compliance', 'meetings', 'non-compliance', 'patient population', 'pill', 'public health relevance', 'tool', 'usability']",NCATS,"AI CURE TECHNOLOGIES, LLC",R44,2014,888295,0.001987128903237681
"Feasibility Trial of a Problem-Solving Weight Loss Mobile Application     DESCRIPTION (provided by applicant): Lifestyle interventions, while effective at reducing weight and diabetes risk, are intensive (i.e., requiring 16 or more face-to-face visits) which has prevented widespread implementation. Mobile technology may reduce intervention intensity while preserving outcomes by assisting in the delivery of behavioral strategies, but very little research has explored this. Weight loss mobile applications are proliferating in the open market; however our work and others2 show that the range of evidence-based strategies addressed by these apps is narrow, primarily including self-monitoring, prompts, goal-setting, and sometimes a social network.3 A key strategy missing across apps in both the market and research is problem solving, an essential component of behavioral weight loss interventions.4 We propose to develop and test the feasibility of Smart Coach, a weight loss mobile application that includes common features such as self-monitoring, goal setting, and a social network, but even more importantly, an avatar-facilitated, idiographic problem solving feature that processes information intelligently to help patients identify solutions to their weight loss problems. We hypothesize tha Smart Coach when combined with a lower intensity (half the sessions) weight loss intervention will be more effective than a lower intensity weight loss intervention alone, with biggest differences observable after face-to- face visits end. Using a ""crowd-sourcing"" model, we will populate a database with problems and solutions via 1) expert-delivered problem solving sessions with a sample of obese participants trying to lose weight and 2) a pre-pilot test of the app. Using principles of ""artificial intelligence"" we will convert the algorithm of problem solving counseling into the mobile application so that it may perform this strategy on its own, but based on expert and crowd-sourced information. We will then use a series of iterative steps involving qualitative research methods (usability testing, focus groups, and pre-piloting) to refine the tool A randomized pilot feasibility trial will test the feasibility and initial effects of the Smart Coah mobile application when paired with a shortened (8- week) behavioral weight loss intervention relative to a shortened behavioral weight intervention alone. Feasibility outcomes include frequency and duration of usage of the mobile app and each feature, recruitment, and retention. We will also do exploratory analyses comparing conditions on problem solving skills and weight loss at 8- and 16-weeks. Data will support a larger efficacy trial of a Smart Coach-assisted brief behavioral weight loss intervention relative to a brief behavioral weight loss intervention alone. Our overarching goal is to develop mobile technology that reduces the intensity of lifestyle interventions as far as possible while preserving weight loss outcomes, to ultimately broaden the reach to people and settings that currently have little access.         PUBLIC HEALTH RELEVANCE: Mobile technology that effectively delivers behavioral strategies to reduce the intensity of weight loss intervention while also preserving outcomes is needed to increase the impact and reach of behavioral weight loss interventions. A key strategy missing across mobile applications developed in both the market and research is problem solving, an active ingredient and central feature of behavioral weight loss interventions. We propose to develop and test the feasibility of Smart Coach, a weight loss mobile application that includes common features such as self-monitoring, goal setting, and a social network, but even more importantly an avatar-facilitated, idiographic problem solving feature that processes information intelligently to help patients identify solutions to their weight loss problems. We hypothesize that Smart Coach when combined with a lower intensity (half the sessions as the standard program) weight loss intervention will be more effective than a lower intensity weight loss intervention alone.            ",Feasibility Trial of a Problem-Solving Weight Loss Mobile Application,8734413,R21DK098556,"['Address', 'Algorithms', 'Artificial Intelligence', 'Behavior Therapy', 'Behavioral', 'Body Weight decreased', 'Clinical', 'Collaborations', 'Computers', 'Counseling', 'Crowding', 'Data', 'Databases', 'Development', 'Environment', 'Event', 'Exercise', 'Feedback', 'Female', 'Focus Groups', 'Frequencies', 'Goals', 'Group Meetings', 'Individual', 'Institutes', 'Intervention', 'Logic', 'Market Research', 'Marketing', 'Massachusetts', 'Modeling', 'Modification', 'Monitor', 'Obesity', 'Outcome', 'Participant', 'Patients', 'Persons', 'Phase', 'Principal Investigator', 'Problem Solving', 'Process', 'Professional counselor', 'Proliferating', 'Psychologist', 'Qualitative Research', 'Randomized', 'Relative (related person)', 'Research', 'Research Methodology', 'Sampling', 'Schedule', 'Scientist', 'Series', 'Social Network', 'Solutions', 'Technology', 'Testing', 'Time', 'Treatment Efficacy', 'Universities', 'Visit', 'Weight', 'Work', 'base', 'diabetes risk', 'diet and exercise', 'efficacy trial', 'evidence base', 'follow-up', 'information processing', 'lifestyle intervention', 'medical schools', 'mobile application', 'post intervention', 'prevent', 'programs', 'public health relevance', 'randomized trial', 'response', 'skills', 'social', 'tool', 'usability', 'web interface', 'weight loss intervention']",NIDDK,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R21,2014,207275,-0.015964044706455315
"Providing Access to Appliance Displays for Visually Impaired Users     DESCRIPTION (provided by applicant):  Providing Access to Appliance Displays for Visually Impaired Users Summary The goal of this project is to develop a computer vision system that runs on smartphones and tablets to enable blind and visually impaired persons to read appliance displays.  This ability is increasingly necessary to use a variety of household and commercial appliances equipped with displays, such as microwave ovens and thermostats, and is essential for independent living, education and employment.  No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents.  Our proposed system runs as software on a standard, off-the-shelf smartphone or tablet and uses computer vision algorithms to analyze video images taken by the user and to detect and read the text within each image.  For blind users, this text is either read aloud using synthesized speech, or for low vision users, presented in magnified, contrast- enhanced form (which is most suited to the use of a tablet).  We propose to build on our past work on a prototype display reader using powerful new techniques that will enable the resulting system to read a greater variety of LED/LCD display text fonts, and to better accommodate the conditions that often make reading displays difficult, including glare, reflections and poor contrast.  These techniques include novel character recognition techniques adapted specifically to the domain of digital displays; finger detection to allow the user to point to a specific location of interest in the display; the use of display templates as reference images to match to, and thereby help interpret, noisy images of displays; and the integration of multiple views of the display into a single clear view.  Special user interface features such as the use of finger detection to help blind users aim the camera towards the display and contrast-enhancement for low vision users address the particular needs of different users.  Blind and visually impaired subjects will test the system periodically to provide feedback and performance data, driving the development and continual improvement of the system, which will be assessed with objective performance measures.  The resulting display reader system will be released as an app for smartphones and tablets that can be downloaded and used by anybody for free, and also as an open source project that can be freely built on or modified for use in 3rd-party software.         PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self-sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays.  The proposed research would result in a smartphone/tablet-based assistive technology system (available for free) to provide increased access to such display information for the approximately 10 million Americans with significant vision impairments or blindness.",Providing Access to Appliance Displays for Visually Impaired Users,8712492,R01EY018890,"['Access to Information', 'Address', 'Adoption', 'Algorithms', 'American', 'Automobile Driving', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Detection', 'Development', 'Devices', 'Economics', 'Education', 'Employment', 'Evaluation', 'Feedback', 'Fingers', 'Glare', 'Goals', 'Hand', 'Home environment', 'Household', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging problem', 'Impairment', 'Independent Living', 'Location', 'Mainstreaming', 'Measures', 'Movement', 'Performance', 'Prevalence', 'Printing', 'Procedures', 'Reader', 'Reading', 'Research', 'Research Infrastructure', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'System', 'Tablets', 'Techniques', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'consumer product', 'design', 'digital', 'improved', 'information display', 'interest', 'microwave electromagnetic radiation', 'novel', 'open source', 'optical character recognition', 'prototype', 'public health relevance']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2014,368560,0.04284455887579613
"Low-Cost, Compact, and Portable Robot for Autonomous Intravenous Access using Nea     DESCRIPTION (provided by applicant): Peripheral venous access is pivotal to a wide range of clinical interventions and is consequently the leading cause of medical injury in the U.S. Complications associated with the procedure are exacerbated in difficult settings, where the rate of success depends heavily on the patient's physiology and the practitioner's experience. My dissertation thesis pertains to the development of imaging and robotic technologies to improve the accuracy and speed of blood draws and IV's. The core technology is an image-guided robotic device that accurately and autonomously introduces a cannula for venous access. The device operates by mapping in real-time the 3D structure of peripheral veins in order to robotically direct a needle into a selected vein. A working prototype has been developed and validated in several studies, the results of which are described in two journal publications. The device combines a 3D near-infrared vein imager, a robot, and computer vision software; these three components form the basis of the three Specific Aims described in this proposal. The Aims fit into the overall dissertation by 1) incorporating the current imaging hardware into a standalone, handheld imaging device; 2) introducing software for the imaging device that assists in selecting suitable cannulation sites; and 3) integrating the imaging device and software with a miniaturized version of the current robot. The outcome of this work will be a compact and low-cost system that is suited for beta-stage development.          PUBLIC HEALTH RELEVANCE: Blood draws and IV therapies are one of the most commonly performed medical routines in hospitals and clinics. Injuries to doctors and patients happen frequently because of how difficult it can be to find veins and accurately insert the needle. We are developing a portable and lightweight medical robot to perform the procedure in situations where the doctor is unable to successfully access the veins. This device may greatly improve the safety and accuracy of venous access, and has wide applications in many clinical areas.            ","Low-Cost, Compact, and Portable Robot for Autonomous Intravenous Access using Nea",8718641,F31EB018191,"['Algorithms', 'Anatomy', 'Area', 'Benchmarking', 'Blood', 'Cannulas', 'Cannulations', 'Catheters', 'Childhood', 'Clinical', 'Clinics and Hospitals', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Cues', 'Custom', 'Development', 'Device Safety', 'Devices', 'Emergency Care', 'Evaluation', 'Failure', 'Goals', 'Graph', 'Healthcare Systems', 'Human', 'Image', 'Imaging Device', 'In Vitro', 'Injury', 'Institutional Review Boards', 'Intervention', 'Intravenous', 'Journals', 'Knowledge', 'Literature', 'Location', 'Maps', 'Medical', 'Motivation', 'Needles', 'Neonatal', 'Outcome', 'Patients', 'Peripheral', 'Physiology', 'Pilot Projects', 'Population', 'Population Sizes', 'Positioning Attribute', 'Procedures', 'Publications', 'Reporting', 'Robot', 'Robotics', 'Safety', 'Site', 'Speed', 'Sprague-Dawley Rats', 'Staging', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Validation', 'Veins', 'Venous', 'Visual', 'Weight', 'Work', 'arm', 'base', 'cost', 'design', 'experience', 'improved', 'in vivo', 'meetings', 'miniaturize', 'pre-clinical', 'prototype', 'public health relevance', 'robotic device', 'skeletal', 'success', 'three dimensional structure', 'three-dimensional modeling', 'tissue phantom', 'visual motor']",NIBIB,"RUTGERS, THE STATE UNIV OF N.J.",F31,2014,34424,0.020075506684854545
"CRCNS: Model-driven single-neuron studies of cortical remapping     DESCRIPTION (provided by applicant): During natural vision humans and non-human primates make several saccadic eye movements each second that result in large changes in the retinal input. Despite these often dramatic changes, our visual percept remains remarkably stable and we can readily attend to and direct motor actions towards objects in our visual environment. This project will use a model-driven approach to investigate the neural circuits linking vision, attention and oculomotor planning that stabilize perceptual and attentional representations during natural vision. Experimental data will be collected and used to design a detailed computational model of the visual and oculomotor areas involved in saccade compensation. The proposed collaboration between a computational (DE) and experimental neurophysiological (US) laboratories leverages the power of both disciplines. Biologically accurate models of visually guided behavior and trans-saccadic integration developed in the Hamker lab will guide the design of and interpretation of data obtained from neurophysiological experiments in awake, behaving primates performed in the Mazer lab in an iterative fashion, with experimental results informing model revisions and new model predictions altering experimental designs. The proposed studies will characterize both dorsal and ventral stream visual area contributions to stabilizing visual and attentional representations in the primate brai. Data obtained from these experiments will identify the neural circuits responsible for integrating oculomotor commands, bottom-up visual inputs and top-down attention signals. This approach will yield novel insights into interactions between the dorsal and ventral streams during natural vision and facilitate our understanding of goal-directed, active visual perception, a defining feature of human and non-human primate natural vision. A critical component of this project is the highly collaborative nature of the planned research. We expect great benefits from this interdisciplinary approach, which depends critically on computational models that strictly adhere to the known physiological and anatomical constraints to guide our neurophysiological experiments.     1. Training. The proposal includes a detailed training plan intended to facilitate international training of future modelers and neurophysiologists. Specifically, we will train students and post-doctoral researchers to be experts in both experimental and theoretical approaches in order to advance the field using the hybrid approach outlined in the proposal.    2. Education and Outreach. We plan to organize two in-depth workshops on attention and eye movements. These events (one in Germany and one in the US) will bring together investigators from other institutions and related scientific disciplines to advance the field. In addition investigators will organize and chair 1-2 workshops/symposia at annual meetings (e.g., SFN and COSYNE) during the funding period. Finally, we will participate in science education for underrepresented groups through Yale?s STARS program by providing training, research and mentoring opportunities in the Mazer lab.    3. Data Sharing. The software tools generated and behavioral and neurophysiological data collected during this project will be distributed to the neuroscience community to facilitate data mining and secondary analyses of experimental data.    4. Impact in other scientific fields. Efficient allocation of limited sensor resources is also a important problem faced by computer vision and robotics researchers. Understanding how the primate brain efficiently allocates visual resources using an active-sensing approach will guide development of biologically inspired computer vision algorithms and humanoid cognitive robots.    5. Translational Implications. Although the proposed research is not translational, there is a growing body of evidence suggesting that several clinically important conditions, including Autism Spectrum Disorder, Attention Deficit Hyperactivity Disorder and Schizophrenia, are associated with impaired behavioral links between saccade planning and visual attention. The proposed basic science studies could have significant implications for future translational research potentially leading to improved understanding disease etiology, development of early diagnostic tools and possible interventional strategies.              n/a",CRCNS: Model-driven single-neuron studies of cortical remapping,8837252,R01EY025103,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Area', 'Attention', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Dorsal', 'Education and Outreach', 'Educational workshop', 'Environment', 'Etiology', 'Event', 'Experimental Designs', 'Eye Movements', 'Financial compensation', 'Funding', 'Future', 'Germany', 'Goals', 'Human', 'Hybrids', 'Institution', 'International', 'Intervention', 'Laboratories', 'Link', 'Mentors', 'Modeling', 'Monkeys', 'Motor', 'Nature', 'Neurons', 'Neurosciences', 'Performance', 'Physiological', 'Postdoctoral Fellow', 'Primates', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Retinal', 'Robot', 'Robotics', 'Saccades', 'Schizophrenia', 'Signal Transduction', 'Simulate', 'Software Tools', 'Stream', 'Students', 'Testing', 'Training', 'Translational Research', 'Underrepresented Minority', 'Update', 'Vision', 'Visual', 'Visual Perception', 'Visual attention', 'Work', 'area V4', 'autism spectrum disorder', 'awake', 'base', 'cell type', 'computer framework', 'data mining', 'data sharing', 'design', 'extrastriate visual cortex', 'improved', 'insight', 'interdisciplinary approach', 'meetings', 'neural circuit', 'neurophysiology', 'nonhuman primate', 'novel', 'oculomotor', 'programs', 'relating to nervous system', 'research study', 'science education', 'sensor', 'simulation', 'spatiotemporal', 'symposium', 'tool', 'vector', 'visual motor']",NEI,YALE UNIVERSITY,R01,2014,249750,-0.006183307229771167
"Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance     DESCRIPTION (provided by applicant): The neuromuscular disorders include a wide group of conditions ranging from relatively mild focal problems, such as carpal tunnel syndrome, to severe, generalized diseases, such as amyotrophic lateral sclerosis (ALS). Current methods for evaluating these disorders remain limited to a variety of tests and procedures that are either invasive or remarkably qualitative in nature. One newer technique, electrical impedance myography (EIM), offers the prospect of obtaining quantitative data on muscle condition painlessly and non-invasively. However, no commercial devices specifically tailored for EIM use exist, with nearly all previous EIM work relying upon off-the-shelf bioimpedance devices designed for ""whole-body"" rather than single-muscle assessment. In Phase 1 of this SBIR, Convergence Medical Devices, Inc (CMD) developed an initial prototype system to test proof-of-principle innovative concepts in circuit design and data acquisition to provide highly sensitive and accurate data at an extended frequency range (out to 10 MHz) and over multiple angles relative to the major muscle fiber direction. This work was followed by supplemental work that produced the first hand-held device with robust electronics capable of similar measurements with a detachable, interchangeable electrode array. In this 3-year Phase 2 SBIR, CMD proposes to further refine that technology so as to assist in disease diagnosis and for following disease progression, with a specific focus on amyotrophic lateral sclerosis. In Aim 1 of the proposed grant, testing of multiple electrode array designs will be performed on a group of normal subjects and those with ALS to determine which electrode array designs offer the greatest reproducibility and ability to discriminate healthy from diseased individuals. In Aim 2, the device and the 3 ""best"" array designs identified in Aim 1 will be tested in 30 ALS patients, 30 patients with syndromes mimicking ALS (e.g., polyradiculopathy and motor-predominant neuropathy), and 30 subjects with suspected/possible ALS to determine the single best array design for diagnosis and to determine its sensitivity and specificity. This study will take place at CMD and four external sites and will be managed by the Northeast ALS trials Consortium (NEALS), the premier organization for overseeing clinical trials in ALS. In Aim 3, the 30 suspected/possible ALS patients recruited in SA2 and 30 additional suspected/possible ALS patients will be assessed monthly using EIM and the 3 arrays identified in SA1. All subjects will also undergo standard measurements of disease progression including handheld dynamometry, ALS Functional Rating Scale, and spirometry. The best array design for assessing disease progression will be identified and compared to conventional markers of disease progression to determine EIM's sensitivity to disease status. Thus, at the conclusion of this research program, we anticipate having developed and vigorously tested an EIM device and associated electrode designs that together will serve as a powerful new diagnostic and monitoring tool for neuromuscular disease.          The neuromuscular disorders are a group of conditions that impact the lives of millions of people in the United States alone. In this study, Convergence Medical Devices, in collaboration with neurologists at several medical centers, will develop and test a new device based on the concept of electrical impedance myography, a technique that offers the promise of rapid and accurate disease assessment. With the completion of this study, a new, painless tool will be developed and studied that will assist in individual patient care and will speed drug testing in clinical trials in diseases ranging from amyotrophic lateral sclerosis and muscular dystrophy to radiculopathy and polyneuropathy.            ",Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance,8650344,R44NS070385,"['Amyotrophic Lateral Sclerosis', 'Area', 'Biological Markers', 'Biopsy', 'Businesses', 'Carpal Tunnel Syndrome', 'Clinical', 'Clinical Trials', 'Collaborations', 'Cross-Sectional Studies', 'Data', 'Data Analyses', 'Device Designs', 'Devices', 'Diagnosis', 'Diagnostic', 'Discrimination', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Electrodes', 'Electromyography', 'Electronics', 'Entrapment Neuropathies', 'Frequencies', 'Future', 'Genetic', 'Goals', 'Grant', 'Individual', 'Limb structure', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Measures', 'Medical Device', 'Medical center', 'Methods', 'Monitor', 'Motor', 'Motor Neurons', 'Muscle', 'Muscle Fibers', 'Muscular Dystrophies', 'Myography', 'Nature', 'Needles', 'Neurologist', 'Neuromuscular Diseases', 'Neuropathy', 'Outcome', 'Painless', 'Patient Care', 'Patients', 'Phase', 'Physicians', 'Polyneuropathy', 'Polyradiculopathy', 'Procedures', 'Radiculopathy', 'Recruitment Activity', 'Relative (related person)', 'Reproducibility', 'Research', 'Sensitivity and Specificity', 'Site', 'Small Business Innovation Research Grant', 'Speed', 'Spirometry', 'Symptoms', 'Syndrome', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Clinical Trial', 'Time', 'United States', 'Widespread Disease', 'Work', 'base', 'clinical care', 'commercial application', 'data acquisition', 'design', 'diagnosis design', 'disease diagnosis', 'drug testing', 'electric impedance', 'improved', 'indexing', 'innovation', 'muscle strength', 'muscular structure', 'neuromuscular', 'novel diagnostics', 'programs', 'prototype', 'pulmonary function', 'tool', 'validation studies']",NINDS,"MYOLEX, INC.",R44,2014,536206,0.01686953393282622
"Understanding Real-Life Falls in Amputees using Mobile Phone Technology     DESCRIPTION (provided by applicant): Falls are a significant cause of death and serious injury and result in significant health-care costs. Individuals with a lower extremity amputation due to vascular disease are overwhelmingly elderly (at least 65 years of age) and are at especially high risk of falling. Successful fall prevention strategies depend on understanding how, why, when, and where individuals fall, and what types of falls (e.g., trip, slip, or lateral fll) are likely in a given population. Most studies on falls in amputees to date have relied surveys or questionnaires that are often completed a significant time after the fall and thus rely both on the individual's ability to remember the details of their fall and their willingness to be objective abut how and why they fell. Such approaches are susceptible both to inaccurate memories of the fall and to recall bias-for example, due to embarrassment about falling- and are especially unreliable in the elderly amputees. Mobile phones provide a simple, cost-effective method for detection and characterization of falls. Most available smart phones today have a tri-axial accelerometer, which provides highly accurate fall detection in real-time. Other available applications (or apps) can provide data on activity (running, walking etc.) and environment-such as the weather conditions or population density-that may have contributed to the fall and can pin-point the location of the fall-using GPS technology and highly accurate maps. Mobile phones also have inbuilt data storage and transfer capability, allowing for real-time acquisition and transmission of data. Additionally, mobile phones provide a simple means to contact the individual immediately after a suspected fall to confirm details of the fall (and to ascertain the need for medical assistance). Because mobile phone use is so widespread, there is no stigma associated with carrying such a device, which is likely to lead to high compliance. This study aims to use a mobile phone-based fall detection system in dysvascular amputees to detect falls, characterize the type of fall, analyze environmental conditions that may have contributed to the fall, and determine the longer-term consequences of each type of fall. Data acquired may be used to improve rehabilitation protocols or design better prostheses in order to prevent falls. This technology is ultimately transferrable to many populations with a high risk of falling-for example, the elderly, stroke survivors, or those with other musculoskeletal disorders or disabilities-leading to the design of specific fall prevention strategies for those populations.         PUBLIC HEALTH RELEVANCE: Successful fall prevention strategies for the elderly dysvascular amputee population-including better prosthesis design and improved rehabilitation/fall prevention strategies-depend on understanding how, why, when, and where such individuals fall, and what types of fall occur (e.g., trip, slip, or lateral fall). Mobile smartphones with built-in triaxial accelerometers can accurately detect and classify falls, in addition to providing other applications (or 'apps') that can identify contributory environmental conditions (e.g., weather or traffic) or activities (e.g. walking or running) that have may contributed to the fall. Combined with server side analysis of wirelessly transmitted phone data-using machine learning techniques-mobile smartphones provide a simple, portable fall-detection system that generates real-time information on the mechanisms, contributory environmental factors, and consequences of falls in elderly amputees-or in any population at risk of falling.            ",Understanding Real-Life Falls in Amputees using Mobile Phone Technology,8738041,R01EB019406,"['3-Dimensional', 'Accident and Emergency department', 'Age-Years', 'Algorithms', 'Amputation', 'Amputees', 'Car Phone', 'Cause of Death', 'Classification', 'Communication', 'Communities', 'Crowding', 'Data', 'Data Collection', 'Data Quality', 'Data Set', 'Data Storage and Retrieval', 'Detection', 'Devices', 'Elderly', 'Environment', 'Environmental Risk Factor', 'Etiology', 'Event', 'Fall prevention', 'Family', 'Fright', 'Goals', 'Health Care Costs', 'Hospitals', 'Individual', 'Injury', 'Interview', 'Knowledge', 'Laboratories', 'Lateral', 'Lead', 'Life', 'Location', 'Longitudinal Studies', 'Lower Extremity', 'Machine Learning', 'Maps', 'Medical Assistance', 'Medical Care Costs', 'Memory', 'Methods', 'Morbidity - disease rate', 'Musculoskeletal Diseases', 'Outcome', 'Patients', 'Persons', 'Population', 'Population Density', 'Populations at Risk', 'Prevalence', 'Prevention strategy', 'Prosthesis', 'Prosthesis Design', 'Protocols documentation', 'Publications', 'Quality of life', 'Questionnaires', 'Rain', 'Real-Time Systems', 'Recovery', 'Rehabilitation therapy', 'Reporting', 'Research', 'Running', 'Side', 'Simulate', 'Stroke', 'Surveys', 'Survivors', 'System', 'Techniques', 'Technology', 'Telephone', 'Time', 'Vascular Diseases', 'Visit', 'Walking', 'Weather', 'Wireless Technology', 'Work', 'base', 'cohort', 'cost effective', 'data exchange', 'design', 'diaries', 'disability', 'falls', 'fear of falling', 'health care quality', 'high risk', 'improved', 'improved mobility', 'information gathering', 'mortality', 'new technology', 'novel', 'prospective', 'public health relevance', 'sensor', 'social stigma', 'trafficking', 'willingness']",NIBIB,REHABILITATION INSTITUTE OF CHICAGO D/B/A SHIRLEY RYAN ABILITYLAB,R01,2014,180608,0.0028040063003246223
"Development of a Centralized Intelligence and Control AP Hub Device ﻿    DESCRIPTION (provided by applicant):  Despite major advances in pharmaceuticals and medical device technology, the ability to safely and effectively control blood glucose in patients with Type 1 diabetes has remained a recalcitrant challenge and a significant source of human suffering and economic cost. With the advent of continuous glucose monitoring (CGM), increasing effort has been focused on the development of artificial pancreas (AP) systems using CGM coupled with insulin pump via closed-loop control (CLC) algorithms. Integration of these disparate technologies and implementation in a commercial product has faced numerous challenges to date. TypeZero Technologies' objective is the commercialization of the robust and extensively tested Diabetes Assistant (DiAs) software platform and supporting technology into a commercial-grade device ""hub"" designed to organize and control a network of medical devices for the improved management of blood glucose in the context of Type 1 diabetes. The DiAs prototype has been extensively tested to date, and has generated over 24,000 hours of on- patient data. In numerous clinical trials, this technology has reduced dramatically the frequency of occurrence of severe hyper- and hypoglycemic events, reduced glycemic variability and improved patient's ""time-in-range"" (rate of euglycemia). DiAs and supporting technologies represent the core elements of a radically different treatment paradigm that integrates and maximizes the capabilities of existing and currently approved medical devices with a proprietary software platform and control technologies. The key characteristics of the TypeZero hub are:  "" Modular hub functionality residing optionally within and across available networked devices including the  patient's pump, meter, CGM, smartphone, or within cloud services;  "" Ability to accommodate any control strategy that is deemed optimal for a specific patient;  "" Inherently layered architecture, designed with an upward pathway of sequential module deployment;  "" Inherent safety, featuring a downward pathway of graceful degradation to a known system state in the  event of component failure; and  "" Local and Global modes of operation enabling the availability of certain processes and patient  interaction through the portable device; other services and remote monitoring of subject and system  state are available via telecommunication (e.g. 3G, WiFi, etc.). We envision that in the near future consumer electronics hardware equipped with appropriate software (e.g. DiAs) would gradually replace specialized CGM receivers or insulin pump controllers, merging into a single ultra-portable platform the functions of continuous monitoring, insulin delivery, and closed-loop control. In the near-term, we envision creating a dedicated hub controller that will host key functionalities of artificial pancreas as a critical step toward the future.     PUBLIC HEALTH RELEVANCE: The persistent challenge of safely and effectively managing blood glucose levels in the context of Type 1 diabetes indicates the inadequacy of currently approved and available medical device technologies. Artificial pancreas approaches to algorithm-based control of medical devices for improved patient safety and outcomes has been an active area of research for many years. TypeZero Technologies seeks to commercialize a proprietary suite of technologies that have been developed and extensively tested in clinical trial settings and shown to deliver significant improvements in safety and efficacy. The successful development of TypeZero's hub device will represent a novel integration of capabilities that will dramatically accelerate the commercial availability of artificial pancreas technologies.        ",Development of a Centralized Intelligence and Control AP Hub Device,8823176,R43DK104293,"['Adverse event', 'Algorithms', 'Architecture', 'Area', 'Artificial Pancreas', 'Blood Glucose', 'Carbohydrates', 'Cardiovascular system', 'Caring', 'Characteristics', 'Clinical', 'Clinical Trials', 'Communication', 'Complex', 'Computer software', 'Continuous Infusion', 'Contracts', 'Coupled', 'Data', 'Databases', 'Development', 'Devices', 'Diabetes Mellitus', 'Diagnostic', 'Drug Kinetics', 'Electronics', 'Elements', 'Environment', 'Event', 'Exercise', 'Exogenous Factors', 'Failure', 'Focus Groups', 'Frequencies', 'Future', 'Goals', 'Health Care Costs', 'Health Insurance Portability and Accountability Act', 'Hour', 'Human', 'Hyperglycemia', 'Hypoglycemia', 'Incidence', 'Insulin', 'Insulin Infusion Systems', 'Insulin-Dependent Diabetes Mellitus', 'Intelligence', 'Judgment', 'Kidney Failure', 'Licensing', 'Life', 'Machine Learning', 'Manufacturer Name', 'Medical Device', 'Metabolism', 'Modeling', 'Monitor', 'Neuropathy', 'Outcome', 'Pathway interactions', 'Patient Monitoring', 'Patients', 'Pharmacologic Substance', 'Phase', 'Phase II Clinical Trials', 'Preparation', 'Procedures', 'Process', 'Protocols documentation', 'Provider', 'Pump', 'Quality of life', 'Replacement Therapy', 'Research', 'Retinal Diseases', 'Running', 'Safety', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Solutions', 'Source', 'Supervision', 'System', 'Technology', 'Telecommunications', 'Telemedicine', 'Testing', 'Time', 'Transition Elements', 'Uncertainty', 'Universities', 'Virginia', 'Wireless Technology', 'base', 'cloud based', 'commercialization', 'design', 'economic cost', 'experience', 'glucose monitor', 'glucose sensor', 'glycemic control', 'graphical user interface', 'improved', 'meter', 'middleware', 'monitoring device', 'novel', 'operation', 'patient safety', 'prototype', 'public health relevance', 'safety testing', 'subcutaneous']",NIDDK,"TYPEZERO TECHNOLOGIES, LLC",R43,2014,224354,0.011430806380787728
"Context Understanding Technology to improve internet accessibility for users with DESCRIPTION (provided by applicant): The purpose of this SBIR project is to develop a new tool to improve the ability of blind and visually impaired people to quickly get to the right web pages, to avoid ending up on the wrong web pages, and to reach the desired part of each web page efficiently. The motivation for this effort is that getting a 'big picture' understanding of a  web page is one of the biggest challenges facing this population of computer users. Existing tools for blind and visually impaired people, such as screen readers and screen magnifiers, present the entire web page serially, in full detail, either by textual output of the details, or b providing a magnified view of a small part of the page. However, if the user is not already familiar with the website, the resulting lack of context requires a laborious and time-consuming process to scan through sometimes hundreds of details before knowing whether anything of interest even exists on the page in question. In contrast, the proposed technology analyzes web pages in a similar way that sighted users quickly scan pages before reading in detail, to provide a succinct, informative guidance on the context and layout of the page, so that a user quickly gains a much richer ""big-picture"" understanding. The proposed Phase II project builds on a successful Phase I user evaluation of a proof- of-concept of the software, in which users expressed a strong preference for the contextual cues provided by the approach. Phase II includes creating a fully-functional prototype of the software tool. Throughout the technology development, feedback from users and practitioners will guide the path of the R&D for maximum usability. At the conclusion of Phase II, an end-user, in-home evaluation study will verify the effectiveness of the tool, providing the starting point for full-scale commercialization. PUBLIC HEALTH RELEVANCE: The R&D in this Phase II SBIR project will result in an improved way to use the Internet for individuals with visual disabilities. The technology will help to overcome the lack of accessibility and high level of frustration currently found among blind and visually impaired users. The results of the project will enable such individuals to explore new opportunities and be more productive, thus providing improved employment potential and an increase in the quality of life.",Context Understanding Technology to improve internet accessibility for users with,8609036,R44EY020082,"['Advertisements', 'Arizona', 'Artificial Intelligence', 'Boxing', 'Characteristics', 'Color', 'Computer software', 'Computers', 'Cues', 'Effectiveness', 'Employment', 'Evaluation', 'Evaluation Studies', 'Feedback', 'Frustration', 'Generations', 'Grouping', 'Health', 'Home environment', 'Individual', 'Interest Group', 'Internet', 'Literature', 'Marketing', 'Motivation', 'Mus', 'Output', 'Phase', 'Population', 'Process', 'Quality of life', 'Reader', 'Reading', 'Scanning', 'Small Business Innovation Research Grant', 'Software Tools', 'Speech', 'System', 'Technology', 'Text', 'Time', 'Training', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'base', 'blind', 'commercialization', 'disability', 'experience', 'improved', 'interest', 'natural language', 'phrases', 'preference', 'prototype', 'research and development', 'technology development', 'tool', 'usability', 'web page', 'web site']",NEI,"VORTANT TECHNOLOGIES, LLC",R44,2014,357073,0.041752215235868716
"Designing Visually Accessible Spaces  Title: Designing Visually Accessible Spaces NIH Program Announcement: 10-234 Bioengineering Research Partnerships (BRP)[R01] Abstract Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals, including those with physical disabilities, and to enhance safety for older people and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool enabling architectural design professionals to assess the visual accessibility of a wide range of environments (such as a hotel lobby, subway station, or eye-clinic reception area). This tool will simulate such environments with sufficient accuracy to predict the visibility of key landmarks or hazards, such as steps or benches, for different levels and types of low vision, and for spaces varying in lighting, surface properties and geometric arrangement. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments."" Our research plan has three specific goals: 1) Empirical: determine factors that influence low-vision accessibility related to hazard detection and navigation in real-world spaces. 2) Computational: develop working models to predict low vision visibility and navigability in real-world spaces. 3) Deployment: translate findings from basic vision science and clinical low vision into much needed industrial usage by producing a set of open source software modules to enhance architectural and lighting design for visual accessibility. The key scientific personnel in our partnership come from three institutions: University of Minnesota -- Gordon Legge and Daniel Kersten; University of Utah -- William Thompson and Sarah Creem-Regehr; and Indiana University -- Robert Shakespeare. This interdisciplinary team has expertise in the necessary areas required for programmatic research on visual accessibility -- empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), computer graphics and photometrically accurate rendering (Thompson & Shakespeare) and architectural lighting design (Shakespeare). We have collaborative arrangements with additional architectural design professionals who will participate in the translation of our research and development into practice. PUBLIC HEALTH RELEVANCE:  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our BRP partnership, with interdisciplinary expertise in vision science, computer science and lighting design, plans to develop computer-based tools enabling architecture professionals to assess the visual accessibility of their designs.                ",Designing Visually Accessible Spaces,8630772,R01EY017835,"['Accounting', 'Address', 'Age', 'American', 'Architecture', 'Area', 'Biological Models', 'Blindness', 'Central Scotomas', 'Characteristics', 'Clinic', 'Clinical', 'Complement', 'Complex', 'Computer Analysis', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Contrast Sensitivity', 'Cues', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Disorientation', 'Distance Perception', 'Effectiveness', 'Environment', 'Evaluation', 'Eye', 'Geometry', 'Glare', 'Goals', 'Grant', 'Guidelines', 'Height', 'Human', 'Human Resources', 'Image', 'Indiana', 'Individual', 'Injury', 'Institution', 'Investigation', 'Judgment', 'Learning', 'Light', 'Lighting', 'Lobbying', 'Location', 'Minnesota', 'Modeling', 'Modification', 'National Eye Institute', 'Nature', 'Optics', 'Perception', 'Peripheral', 'Phase', 'Photometry', 'Physical environment', 'Positioning Attribute', 'Process', 'Production', 'Property', 'Research', 'Risk', 'Safety', 'Shapes', 'Simulate', 'Source', 'Specific qualifier value', 'Stimulus', 'Structure', 'Subway', 'Surface', 'Surface Properties', 'System', 'Test Result', 'Testing', 'Translating', 'Translations', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual Acuity', 'Visual Fields', 'Visual impairment', 'Work', 'base', 'computer science', 'design', 'disability', 'falls', 'hazard', 'improved', 'knowledge base', 'luminance', 'novel strategies', 'open source', 'programs', 'public health relevance', 'research and development', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2014,595880,0.058202387016754766
"Computational Image Analysis for Cellular and Developmental Biology     DESCRIPTION (provided by applicant): This proposal requests support for an intensive ten-day course on Computational Image Analysis for Cellular and Developmental Biology. The course is designed for graduate students and postdoctoral fellows, and takes place at the Marine Biological Laboratory in Woods Hole, MA. The course is the first of its kind, giving students formal training in computer vision for the specific analysis of cell and developmental biology image data. Building strong foundations in this topic is critical for pushing cell and developmental biology forward, as imaging has become more and more an indispensable tool in these fields. The course covers the fundamentals of computer vision, taking the students through the sequence of low-, intermediate-, and high-level computer vision tasks that are required to solve image analysis problems in quantitative cellular and developmental biology. The curriculum starts with filtering, thresholding and edge/line/generic feature detection, followed by more sophisticated detection algorithms that employ model fitting. After this introductory block to low-level computer vision tasks, the course moves on to intermediate and higher-level tasks, including object association in space and time (such as tracking) and machine learning tools for phenotype classification. Each topic is covered first by a lecture, generally taught by one of the four core faculty, followed by a 3-4 hour computer programming session where students immediately implement the concepts they learn. There are usually two lectures + computer labs per day. Most programming exercises are individual, giving each student the opportunity to ""get their hands dirty,"" while two are team projects allowing the students to also learn and practice methods of code sharing. Over the course's ten days there are three guest lectures by leading researchers in the fields of biological imaging and computer vision, each followed by in-depth discussion, as well as research talks given by the students, faculty and teaching assistants. With this, the core lectures and labs teach the students the fundamentals of computer vision in a logical, continuous manner, the guest lecturers introduce the students to exciting new challenges in imaging and image analysis, and the student/faculty research talks encourage communication between all course participants and give especially the students the opportunity to reflect on how the course can help them with their research. !         PUBLIC HEALTH RELEVANCE:  Imaging has become an indispensable tool in cellular and developmental biology research, but without rigorous, quantitative image analysis it cannot achieve its full potential. This proposal requests support for a new course that will fill the voidof education in computer vision as applied to cellular and developmental biology. The curriculum incorporates a carefully balanced mixture of lectures and associated programming exercises. We have given a pilot course once in October 2010, with very positive reviews from the students and their advisers.             ",Computational Image Analysis for Cellular and Developmental Biology,8414506,R25GM103792,"['Address', 'Algorithms', 'Biological', 'Cellular biology', 'Classification', 'Code', 'Collection', 'Communication', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Data', 'Detection', 'Development', 'Developmental Biology', 'Developmental Cell Biology', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Equilibrium', 'Exercise', 'Faculty', 'Foundations', 'Future', 'Generic Drugs', 'Goals', 'Hand', 'Home environment', 'Hour', 'Image', 'Image Analysis', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Marines', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Phenotype', 'Postdoctoral Fellow', 'Request for Proposals', 'Research', 'Research Personnel', 'Seeds', 'Software Design', 'Solid', 'Students', 'Time', 'Training', 'Wood material', 'computer program', 'design', 'graduate student', 'lecturer', 'lectures', 'programs', 'public health relevance', 'tool']",NIGMS,MARINE BIOLOGICAL LABORATORY,R25,2013,59383,0.007972048264027903
"Smart Anatomic Recognition System to Guide Emergency Intubation and Resuscitation     DESCRIPTION (provided by applicant): Over 3 million emergency intubations are performed in the US every year and failure rates can be as high as 50% (3-5). Success is highly dependent on how frequently the responder performs this life-saving procedure on humans (6). Brio Device, LLC, an airway management medical device company, is addressing the need to decouple the success of the procedure from the experience of the user with their ""smart"" intubation device which integrates anatomic structure recognition algorithms and visual guidance feedback with an articulating stylet. Brio's intubation device is specifically designed fo the needs of emergency responders, such as paramedics, emergency department personnel, code teams in hospitals and military medics, who often arrive at the patient first. The smart intubation device will reduce failure rates by providing the user with visual instruction of the correct path to the trachea as he places the endotracheal tube. The guidance software uses machine learning and computer vision algorithms to recognize the anatomy and determine the path to insert the tube. Ultimately, the intubation device will include both a guidance display on an LCD screen and an optical stylet that has single-axis angulation control of the distal tip. For the purpose of this Phase I study, a laptop or desktop computer will be used for the image processing and the guidance display that accompanies the articulating stylet. The long-term goal is to create a device that is compact, light-weight and portable to suit the needs of ambulances and hospital crash carts.  The hypothesis for this study is that by incorporating a video guidance display with an articulating stylet, inexperienced users will be more successful in correctly placing the endotracheal tube using this device compared to direct laryngoscopy. To achieve this goal, image processing and machine learning algorithms will be developed to recognize key anatomic structures in the airway. Software will also be developed determine the path the tube should follow and to display this information for the user. Finally, the efficacy of the device will be validated in airway simulation mannequins with medical students serving as the inexperienced users. Phase II will focus on integrating the guidance software, articulating optical stylet and display into a portable device with embedded hardware and software contained within the stylet handle. At completion of Phase II, the device will be ready for clinica trials and FDA testing.  Brio will enter the $20 billion airway market with its intubation device. Initial sales will begin with anesthesiologists who are early adopters of new technology to assist with difficult airways. Brio will market its product to ~327,000 clinicians who use intubation devices. The U.S. addressable market for emergency intubation is ~$900M for the 41,000 ambulances and 5,800 emergency departments and hospital code teams.         PUBLIC HEALTH RELEVANCE: In this SBIR Phase I, Brio Device, LLC plans to create and evaluate a device that improves the success rate of emergency intubations by coupling a smart guidance display with a user-controlled single-axis articulating stylet. Emergency intubations are often performed in challenging situations by personnel who do the procedure infrequently. Since failure rates are as high as 50% and approximately 180,000 deaths occur each year from failed pre-hospital intubations, a device is needed to provide visual guidance information to assist the users and increase their success rates in emergency situations.            ",Smart Anatomic Recognition System to Guide Emergency Intubation and Resuscitation,8453607,R43HL114160,"['Accident and Emergency department', 'Address', 'Algorithms', 'Ambulances', 'Anatomic structures', 'Anatomy', 'Brain Death', 'Brain Injuries', 'Cessation of life', 'Clinical Trials', 'Code', 'Computer Vision Systems', 'Computer software', 'Computers', 'Coupling', 'Critical Care', 'Destinations', 'Devices', 'Distal', 'Emergency Situation', 'Failure', 'Feasibility Studies', 'Feedback', 'Goals', 'Hospitals', 'Human', 'Human Resources', 'Image', 'Imagery', 'Instruction', 'Intubation', 'Knowledge', 'Laryngoscopes', 'Laryngoscopy', 'Left', 'Life', 'Light', 'Location', 'Lung', 'Machine Learning', 'Manikins', 'Marketing', 'Medical Device', 'Medical Students', 'Military Hospitals', 'Military Personnel', 'Optics', 'Outcome', 'Outcome Measure', 'Oxygen', 'Paramedical Personnel', 'Patients', 'Phase', 'Physicians', 'Procedures', 'Resuscitation', 'Sales', 'Small Business Innovation Research Grant', 'Structure', 'System', 'Testing', 'Time', 'Trachea', 'Tube', 'Visual', 'commercial application', 'design', 'endotracheal', 'experience', 'flexibility', 'image processing', 'improved', 'information display', 'laptop', 'light weight', 'new technology', 'novel', 'phase 1 study', 'public health relevance', 'secondary outcome', 'simulation', 'success']",NHLBI,"BRIO DEVICE, LLC",R43,2013,244710,0.049583276660274844
"First Person Vision for Objective Evaluation of Caregiving Quality    DESCRIPTION (provided by applicant): More than 40 million family members provide informal care to older individuals annually, making them an essential component of the health care system in the US. Almost 11 million Americans provide unpaid care for a person with Alzheimer's disease or other dementia. Health care professionals typically rely heavily on self-report by family members to understand the dynamics of the caregiving dyad, the quality of caregiving, and the impact of dementia on everyday life. Though invaluable, this information may be devoid of context, or biased due to caregiver reticence or imperfect recall. First-person vision (FPV) technology offers a novel approach to understand the challenges of caregiver/care-recipient interactions, and caregivers' responsiveness to clinicians' suggestions for handling difficult caregiving challenges. Equipped with tiny sensors and worn on eyeglasses or clothing, FPV devices capture video and sound within the user's field of view. These devices can document actual interaction, permit identification of antecedent circumstances or consequences, and provide the basis for targeted intervention designed to assure safe interactions and to prevent, eliminate, or ameliorate distressing care-recipient behaviors. This project with caregiving dyads aims: to describe the feasibility, usability, and interpretability of FPV device use at home (Aim 1); to explore the effect of nurse- delivered intervention, informed by FPV data, on reducing problem behaviors and caregiver burden when delivered in person within 2 weeks (Aim 2) or delivered by phone in ""near real-time,"" that is, within 1 hour, of problem behavior events (Aim 3); and to develop algorithms for an automated system capable of generating real-time Machine Learning responses that promote effective handling of problem behaviors (Aim 4). This project will involve 40 caregiving dyads comprising family caregivers age 21 or older who experience these problem behaviors and care-recipients age 50 or older with moderate to severe dementia (MMSE score <18) due to Alzheimer's disease, frontal temporal dementia, Lewy body dementia, or vascular dementia. Baseline data will include demographic, health, and caregiving information including problem behaviors, functional status, and attitudes toward technology. Caregivers among the first 20 dyads will wear two FPV devices during waking hours over 7 days and signal when the care-recipient takes medications and engages in problematic behavior. Within 2 weeks the caregiver will be shown video clips of his or her FPV data as part of a nurse- delivered intervention focused on handling problem behaviors, then wear the FPV devices for another 7 days and receive brief telephone contact to reinforce the intervention, which we anticipate will reduce problem behaviors and caregiver burden. This pilot intervention study will be replicated with the second 20 dyads, though the nurse will access FPV data remotely and intervene by phone within 1 hour of problem events. FPV data gathered in these investigations will be used to develop algorithms for an innovative, automated tool for objective assessment and timely, tailored intervention aimed at improving caregiving quality and safety.        Family caregivers often find it difficult to deal with the agitation, resistance, and repetitive behaviors of their family members with moderate or severe dementia, and they frequently ask health care professionals how to handle these difficult interactions. First-person vision (FPV) technology offers a novel approach to providing health care professionals with objective evidence of caregiving quality and responsiveness to recommended strategies for dealing with everyday problem behaviors. This project explores the use of FPV technology to assess specific interactions between caregivers and care-recipients, and it examines how FPV data can inform nurse-delivered intervention and provides the basis for development of an automated, intelligent assessment and intervention tool to reduce problem behaviors and decrease caregiver burden.         ",First Person Vision for Objective Evaluation of Caregiving Quality,8442258,R21NR013450,"['21 year old', 'Advanced Development', 'Affect', 'Age', 'Agitation', 'Algorithms', 'Alzheimer&apos', 's Disease', 'American', 'Attitude', 'Behavior', 'Caregiver Burden', 'Caregivers', 'Caring', 'Clip', 'Clothing', 'Computer Vision Systems', 'Data', 'Data Collection', 'Dementia', 'Development', 'Devices', 'Distress', 'Elements', 'Engineering', 'Ensure', 'Evaluation', 'Event', 'Eyeglasses', 'Family', 'Family Caregiver', 'Family member', 'Funding', 'Health', 'Health Professional', 'Healthcare Systems', 'Hearing', 'Home environment', 'Hour', 'Independent Living', 'Individual', 'Intention', 'Intervention', 'Intervention Studies', 'Investigation', 'Knowledge', 'Lewy Body Dementia', 'Life', 'Literature', 'Machine Learning', 'Modeling', 'Nature', 'Nurses', 'Patient Self-Report', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Prevalence', 'Problem behavior', 'Process', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Personnel', 'Resistance', 'Safety', 'Sampling', 'Signal Transduction', 'Specific qualifier value', 'Stress', 'Suggestion', 'System', 'Techniques', 'Technology', 'Telephone', 'Time', 'Vascular Dementia', 'Vision', 'Visual', 'Work', 'base', 'care giving burden', 'caregiving', 'design', 'dyadic interaction', 'experience', 'functional status', 'improved', 'innovation', 'iterative design', 'medical appointment', 'new technology', 'novel strategies', 'post intervention', 'prevent', 'response', 'sensor', 'social', 'software development', 'sound', 'stressor', 'therapy design', 'tool', 'usability', 'user centered design', 'vigilance']",NINR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2013,176249,-0.02527530220473907
"Mobile Search for the Visually Impaired    DESCRIPTION (provided by applicant):    The technology developed as part of this NIH SBIR project will transform the cell phone camera of visually impaired individuals into a powerful tool capable of identifying the objects they encounter, track the items they own, or navigate complex new environments. Broad access to low-cost visual intelligence technologies developed in this project will improve the independence and capabilities of the visually impaired. There has been tremendous technological progress in computer vision and in the computational power and network bandwidth of and Smartphone platforms. The synergy of these advances stands to revolutionize the way people find information and interact with the physical world. However, these technologies are not yet fully in the hands of the visually impaired, arguably the population that could benefit the most from these developments. Part of the barrier to progress in this area has been that computer vision can accurately handle only a small fraction of the typical images coming from a cell phone camera. To cope with these limitations and make any-image recognition possible, IQ Engines will develop a hybrid system that uses both computer vision and crowdsourcing: if the computer algorithms are not able to understand an image, then the image is sent to a unique crowdsourcing network of people for image analysis. The proposed research includes specific aims to both develop advanced computer vision algorithms for object recognition and advanced crowdsourced networks optimized to the needs of the visually impaired community. This approach combines the speed and accuracy of computer vision with the robustness and understanding of human vision, ultimately providing the user fast and accurate information about the content of any image.           The image recognition technology developed in this application will enable the visually impaired to access visual information using a mobile phone camera, a device most people already have. Transforming the camera into an intelligent visual sensor will lower the cost of assistance and improve the quality of life of the visually impaired community through increased independence and capabilities.            ",Mobile Search for the Visually Impaired,8389864,R44EY019790,"['Address', 'Algorithms', 'Area', 'Car Phone', 'Cellular Phone', 'Classification', 'Client', 'Clip', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Crowding', 'Data', 'Databases', 'Detection', 'Development', 'Devices', 'Ensure', 'Environment', 'Family', 'Feedback', 'Friends', 'Glosso-Sterandryl', 'Human', 'Hybrid Computers', 'Hybrids', 'Image', 'Image Analysis', 'Individual', 'Intelligence', 'Label', 'Learning', 'Location', 'Modeling', 'Monitor', 'Phase', 'Population', 'Preparation', 'Process', 'Quality of life', 'Research', 'Running', 'Scanning', 'Services', 'Small Business Innovation Research Grant', 'Social Network', 'Source', 'Speed', 'System', 'Technology', 'Telephone', 'Time', 'United States National Institutes of Health', 'Vision', 'Visual', 'Visual impairment', 'base', 'blind', 'cell transformation', 'coping', 'cost', 'improved', 'innovation', 'novel', 'object recognition', 'open source', 'sensor', 'tool', 'visual information', 'visual search', 'volunteer']",NEI,"IQ ENGINES, INC.",R44,2013,499358,0.052051299394304265
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired     DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering.         PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.            ",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,8650411,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Healthcare', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Solutions', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'high school', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'public health relevance', 'rehabilitation engineering', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2013,81362,0.06226317079937248
"Automating Directly Observed Therapy as a Platform Technology     DESCRIPTION (provided by applicant): Introduction: Ai Cure Technologies LLC was established in 2009 to develop automated medication adherence monitoring solutions using computer vision technology. This SBIR Phase II will allow Ai Cure Technologies to test the accuracy and validity of its flagship product, AiView"". The SBIR Phase I demonstrated that the AiView"" platform was technically feasible and capable of confirming medication administration. Significance: Poor medication adherence is a huge burden on clinical research and clinical practice. The inability to accurately measure or improve adherence significantly compounds the problem. Clinical trials depend on people taking the drug being tested. The problem of medication adherence has been addressed - determinants of adherence are being studied and new monitoring methods developed - but no solution has been able to accurately confirm real-time medication adherence while also being affordable, flexible, and likable. The Product: Ai Cure Technologies will provide an automated DOT (Directly Observed Therapy) software platform, AiView"", for use in clinical trials which uses sophisticated computer vision technology on webcam- enabled smart phones or tablets to visually confirm medication administration. AiView"" will visually track and confirm medication administration without human supervision. Long-Term Goal: The AiView"" system will combine sophisticated computer vision technology with the best attributes of DOT for 1/400th of the cost. Automating and standardizing the way medication adherence is captured will help clinical trials better define their subjects' rates of compliance and allow them to intervene immediately in case of non-compliance. Phase II hypothesis: AiView"" can be used to accurately measure and improve medication adherence across different patient populations, and positively impact self-perception and clinical outcomes. Specific Aim #1: To demonstrate that the AiView"" system can accurately measure and improve medication adherence in a depression and a stroke patient population. Specific Aim #2: To demonstrate that the AiView"" system can improve self-perception and improve clinical outcomes in the AiView"" intervention groups Expected Outcome: The patients in the AiView"" intervention groups (depression and stroke) are expected to have statistically significant higher adherence rates than those in the pill counting groups.         PUBLIC HEALTH RELEVANCE: Poor medication adherence is a huge burden on clinical research and clinical practice with the inability to accurately measure or improve adherence significantly compounding the problem. In accordance with this SBIR Phase II grant, Ai Cure Technologies will continue development and testing of its Automated DOTSM (Directly Observed Therapy) software platform, AiView"", for use in clinical trials which uses sophisticated computer vision technology on webcam-enabled smart phones or tablets to visually confirm medication administration. Automating and standardizing the way medication adherence is captured will help clinical trials better define their subjects' rates of compliance and allow tria administrators to intervene immediately in case of non-compliance, thus improving the accuracy of clinical trials the overall safety of the drug development process.            ",Automating Directly Observed Therapy as a Platform Technology,8524716,R44TR000873,"['Address', 'Adherence', 'Administrator', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communication', 'Computer Vision Systems', 'Computer software', 'Control Groups', 'Controlled Environment', 'Data', 'Data Quality', 'Development', 'Directly Observed Therapy', 'Disease Management', 'Drug Prescriptions', 'Electronics', 'Ensure', 'Event', 'Goals', 'Grant', 'Health', 'Health Insurance Portability and Accountability Act', 'Human', 'Intervention', 'Libraries', 'Marketing', 'Measures', 'Mental Depression', 'Methods', 'Monitor', 'Oral cavity', 'Outcome', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phase III Clinical Trials', 'Phase IV Clinical Trials', 'Population', 'Process', 'Recording of previous events', 'Regimen', 'Research', 'Research Personnel', 'Risk', 'Running', 'Safety', 'Secure', 'Self Perception', 'Small Business Innovation Research Grant', 'Solutions', 'Speed', 'Stroke', 'Supervision', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Time', 'Travel', 'base', 'clinical application', 'clinical practice', 'commercial application', 'compliance behavior', 'cost', 'design', 'drug development', 'flexibility', 'group intervention', 'improved', 'medication compliance', 'meetings', 'non-compliance', 'patient population', 'pill', 'public health relevance', 'tool', 'usability']",NCATS,"AI CURE TECHNOLOGIES, LLC",R44,2013,895818,0.001987128903237681
"Feasibility Trial of a Problem-Solving Weight Loss Mobile Application     DESCRIPTION (provided by applicant): Lifestyle interventions, while effective at reducing weight and diabetes risk, are intensive (i.e., requiring 16 or more face-to-face visits) which has prevented widespread implementation. Mobile technology may reduce intervention intensity while preserving outcomes by assisting in the delivery of behavioral strategies, but very little research has explored this. Weight loss mobile applications are proliferating in the open market; however our work and others2 show that the range of evidence-based strategies addressed by these apps is narrow, primarily including self-monitoring, prompts, goal-setting, and sometimes a social network.3 A key strategy missing across apps in both the market and research is problem solving, an essential component of behavioral weight loss interventions.4 We propose to develop and test the feasibility of Smart Coach, a weight loss mobile application that includes common features such as self-monitoring, goal setting, and a social network, but even more importantly, an avatar-facilitated, idiographic problem solving feature that processes information intelligently to help patients identify solutions to their weight loss problems. We hypothesize tha Smart Coach when combined with a lower intensity (half the sessions) weight loss intervention will be more effective than a lower intensity weight loss intervention alone, with biggest differences observable after face-to- face visits end. Using a ""crowd-sourcing"" model, we will populate a database with problems and solutions via 1) expert-delivered problem solving sessions with a sample of obese participants trying to lose weight and 2) a pre-pilot test of the app. Using principles of ""artificial intelligence"" we will convert the algorithm of problem solving counseling into the mobile application so that it may perform this strategy on its own, but based on expert and crowd-sourced information. We will then use a series of iterative steps involving qualitative research methods (usability testing, focus groups, and pre-piloting) to refine the tool A randomized pilot feasibility trial will test the feasibility and initial effects of the Smart Coah mobile application when paired with a shortened (8- week) behavioral weight loss intervention relative to a shortened behavioral weight intervention alone. Feasibility outcomes include frequency and duration of usage of the mobile app and each feature, recruitment, and retention. We will also do exploratory analyses comparing conditions on problem solving skills and weight loss at 8- and 16-weeks. Data will support a larger efficacy trial of a Smart Coach-assisted brief behavioral weight loss intervention relative to a brief behavioral weight loss intervention alone. Our overarching goal is to develop mobile technology that reduces the intensity of lifestyle interventions as far as possible while preserving weight loss outcomes, to ultimately broaden the reach to people and settings that currently have little access.         PUBLIC HEALTH RELEVANCE: Mobile technology that effectively delivers behavioral strategies to reduce the intensity of weight loss intervention while also preserving outcomes is needed to increase the impact and reach of behavioral weight loss interventions. A key strategy missing across mobile applications developed in both the market and research is problem solving, an active ingredient and central feature of behavioral weight loss interventions. We propose to develop and test the feasibility of Smart Coach, a weight loss mobile application that includes common features such as self-monitoring, goal setting, and a social network, but even more importantly an avatar-facilitated, idiographic problem solving feature that processes information intelligently to help patients identify solutions to their weight loss problems. We hypothesize that Smart Coach when combined with a lower intensity (half the sessions as the standard program) weight loss intervention will be more effective than a lower intensity weight loss intervention alone.            ",Feasibility Trial of a Problem-Solving Weight Loss Mobile Application,8492838,R21DK098556,"['Address', 'Algorithms', 'Artificial Intelligence', 'Behavior Therapy', 'Behavioral', 'Body Weight decreased', 'Clinical', 'Collaborations', 'Computers', 'Counseling', 'Crowding', 'Data', 'Databases', 'Development', 'Environment', 'Event', 'Exercise', 'Feedback', 'Female', 'Focus Groups', 'Frequencies', 'Goals', 'Group Meetings', 'Individual', 'Institutes', 'Intervention', 'Logic', 'Market Research', 'Marketing', 'Massachusetts', 'Modeling', 'Modification', 'Monitor', 'Obesity', 'Outcome', 'Participant', 'Patients', 'Persons', 'Phase', 'Principal Investigator', 'Problem Solving', 'Process', 'Professional counselor', 'Proliferating', 'Psychologist', 'Qualitative Research', 'Randomized', 'Relative (related person)', 'Research', 'Research Methodology', 'Sampling', 'Schedule', 'Scientist', 'Series', 'Social Network', 'Solutions', 'Technology', 'Testing', 'Time', 'Treatment Efficacy', 'Universities', 'Visit', 'Weight', 'Work', 'base', 'diabetes risk', 'diet and exercise', 'efficacy trial', 'evidence base', 'follow-up', 'information processing', 'lifestyle intervention', 'medical schools', 'post intervention', 'prevent', 'programs', 'public health relevance', 'randomized trial', 'response', 'skills', 'social', 'tool', 'usability', 'web interface', 'weight loss intervention']",NIDDK,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R21,2013,263542,-0.015964044706455315
"Providing Access to Appliance Displays for Visually Impaired Users  Providing Access to Appliance Displays for Visually Impaired Users Summary The goal of this project is to develop a computer vision system that runs on smartphones and tablets to enable blind and visually impaired persons to read appliance displays. This ability is increasingly necessary to use a variety of household and commercial appliances equipped with displays, such as microwave ovens and thermostats, and is essential for independent living, education and employment. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed system runs as software on a standard, off-the-shelf smartphone or tablet and uses computer vision algorithms to analyze video images taken by the user and to detect and read the text within each image. For blind users, this text is either read aloud using synthesized speech, or for low vision users, presented in magnified, contrast- enhanced form (which is most suited to the use of a tablet).  We propose to build on our past work on a prototype display reader using powerful new techniques that will enable the resulting system to read a greater variety of LED/LCD display text fonts, and to better accommodate the conditions that often make reading displays difficult, including glare, reflections and poor contrast. These techniques include novel character recognition techniques adapted specifically to the domain of digital displays; finger detection to allow the user to point to a specific location of interest in the display; the use of display templates as reference images to match to, and thereby help interpret, noisy images of displays; and the integration of multiple views of the display into a single clear view. Special user interface features such as the use of finger detection to help blind users aim the camera towards the display and contrast-enhancement for low vision users address the particular needs of different users. Blind and visually impaired subjects will test the system periodically to provide feedback and performance data, driving the development and continual improvement of the system, which will be assessed with objective performance measures. The resulting display reader system will be released as an app for smartphones and tablets that can be downloaded and used by anybody for free, and also as an open source project that can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self-sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays.  The proposed research would result in a smartphone/tablet-based assistive technology system (available for free) to provide increased access to such display information for the approximately 10 million Americans with significant vision impairments or blindness.                ",Providing Access to Appliance Displays for Visually Impaired Users,8579051,R01EY018890,"['Access to Information', 'Address', 'Adoption', 'Algorithms', 'American', 'Automobile Driving', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Detection', 'Development', 'Devices', 'Economics', 'Education', 'Employment', 'Evaluation', 'Feedback', 'Fingers', 'Glare', 'Goals', 'Hand', 'Home environment', 'Household', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging problem', 'Impairment', 'Independent Living', 'Location', 'Mainstreaming', 'Measures', 'Movement', 'Performance', 'Prevalence', 'Printing', 'Procedures', 'Reader', 'Reading', 'Research', 'Research Infrastructure', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'System', 'Tablets', 'Techniques', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'consumer product', 'design', 'digital', 'improved', 'information display', 'interest', 'microwave electromagnetic radiation', 'novel', 'open source', 'optical character recognition', 'prototype', 'public health relevance']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2013,376082,0.042979379187502005
"A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians    DESCRIPTION (provided by applicant): A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians Abstract Project Summary Urban intersections are among the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard cell phone, to provide real-time feedback. Building on our past work on a prototype ""Crosswatch"" system that uses computer vision algorithms to find crosswalks and Walk lights, we will greatly enhance the functionality of the system with information about the intersection layout and the identity of its connecting streets, the presence of stop signs, one-way signs and other controls indicating right-of-way, and timing information integrated from Walk/Don't Walk lights, countdown timers and other traffic lights. The system will convey intersection information, and will actively guide the user to align himself/herself with crosswalks, using a combination of synthesized speech and audio tones. We will conduct human factors studies to optimize the system functionality and the configuration of the user interface, as well as develop interactive training applications to equip users with the skills to better use the system. These training applications, implemented as additional cell phone software to complement the intersection system, will train users to hold the camera horizontal and forward and to minimize veer when traversing a crosswalk. The intersection analysis and training software will be made freely available for download onto popular cell phones (such as iPhone, Android or Symbian models). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function.       PUBLIC HEALTH RELEVANCE: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.            ",A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians,8435501,R01EY018345,"['Address', 'Adoption', 'Algorithms', 'American', 'Cellular Phone', 'Complement', 'Complement component C4', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Crowding', 'Custom', 'Data', 'Development', 'Devices', 'Equipment', 'Face', 'Feedback', 'Glosso-Sterandryl', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Length', 'Light', 'Location', 'Mainstreaming', 'Modeling', 'Modification', 'Names', 'Process', 'Reading', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Running', 'Safety', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Speed', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'abstracting', 'base', 'blind', 'consumer product', 'cost', 'design', 'improved', 'interest', 'legally blind', 'meter', 'prototype', 'public health relevance', 'sensor', 'skills', 'trafficking', 'volunteer', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2013,383018,-0.01538677034178153
"Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance     DESCRIPTION (provided by applicant): The neuromuscular disorders include a wide group of conditions ranging from relatively mild focal problems, such as carpal tunnel syndrome, to severe, generalized diseases, such as amyotrophic lateral sclerosis (ALS). Current methods for evaluating these disorders remain limited to a variety of tests and procedures that are either invasive or remarkably qualitative in nature. One newer technique, electrical impedance myography (EIM), offers the prospect of obtaining quantitative data on muscle condition painlessly and non-invasively. However, no commercial devices specifically tailored for EIM use exist, with nearly all previous EIM work relying upon off-the-shelf bioimpedance devices designed for ""whole-body"" rather than single-muscle assessment. In Phase 1 of this SBIR, Convergence Medical Devices, Inc (CMD) developed an initial prototype system to test proof-of-principle innovative concepts in circuit design and data acquisition to provide highly sensitive and accurate data at an extended frequency range (out to 10 MHz) and over multiple angles relative to the major muscle fiber direction. This work was followed by supplemental work that produced the first hand-held device with robust electronics capable of similar measurements with a detachable, interchangeable electrode array. In this 3-year Phase 2 SBIR, CMD proposes to further refine that technology so as to assist in disease diagnosis and for following disease progression, with a specific focus on amyotrophic lateral sclerosis. In Aim 1 of the proposed grant, testing of multiple electrode array designs will be performed on a group of normal subjects and those with ALS to determine which electrode array designs offer the greatest reproducibility and ability to discriminate healthy from diseased individuals. In Aim 2, the device and the 3 ""best"" array designs identified in Aim 1 will be tested in 30 ALS patients, 30 patients with syndromes mimicking ALS (e.g., polyradiculopathy and motor-predominant neuropathy), and 30 subjects with suspected/possible ALS to determine the single best array design for diagnosis and to determine its sensitivity and specificity. This study will take place at CMD and four external sites and will be managed by the Northeast ALS trials Consortium (NEALS), the premier organization for overseeing clinical trials in ALS. In Aim 3, the 30 suspected/possible ALS patients recruited in SA2 and 30 additional suspected/possible ALS patients will be assessed monthly using EIM and the 3 arrays identified in SA1. All subjects will also undergo standard measurements of disease progression including handheld dynamometry, ALS Functional Rating Scale, and spirometry. The best array design for assessing disease progression will be identified and compared to conventional markers of disease progression to determine EIM's sensitivity to disease status. Thus, at the conclusion of this research program, we anticipate having developed and vigorously tested an EIM device and associated electrode designs that together will serve as a powerful new diagnostic and monitoring tool for neuromuscular disease.          The neuromuscular disorders are a group of conditions that impact the lives of millions of people in the United States alone. In this study, Convergence Medical Devices, in collaboration with neurologists at several medical centers, will develop and test a new device based on the concept of electrical impedance myography, a technique that offers the promise of rapid and accurate disease assessment. With the completion of this study, a new, painless tool will be developed and studied that will assist in individual patient care and will speed drug testing in clinical trials in diseases ranging from amyotrophic lateral sclerosis and muscular dystrophy to radiculopathy and polyneuropathy.            ",Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance,8456053,R44NS070385,"['Amyotrophic Lateral Sclerosis', 'Area', 'Biological Markers', 'Biopsy', 'Businesses', 'Carpal Tunnel Syndrome', 'Clinical', 'Clinical Trials', 'Collaborations', 'Cross-Sectional Studies', 'Data', 'Data Analyses', 'Device Designs', 'Devices', 'Diagnosis', 'Diagnostic', 'Discrimination', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Electrodes', 'Electromyography', 'Electronics', 'Entrapment Neuropathies', 'Frequencies', 'Future', 'Genetic', 'Goals', 'Grant', 'Individual', 'Limb structure', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Measures', 'Medical Device', 'Medical center', 'Methods', 'Monitor', 'Motor', 'Motor Neurons', 'Muscle', 'Muscle Fibers', 'Muscular Dystrophies', 'Myography', 'Nature', 'Needles', 'Neurologist', 'Neuromuscular Diseases', 'Neuropathy', 'Outcome', 'Painless', 'Patient Care', 'Patients', 'Phase', 'Physicians', 'Polyneuropathy', 'Polyradiculopathy', 'Procedures', 'Radiculopathy', 'Recruitment Activity', 'Relative (related person)', 'Reproducibility', 'Research', 'Sensitivity and Specificity', 'Site', 'Small Business Innovation Research Grant', 'Speed', 'Spirometry', 'Symptoms', 'Syndrome', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Clinical Trial', 'Time', 'United States', 'Widespread Disease', 'Work', 'base', 'clinical care', 'commercial application', 'data acquisition', 'design', 'diagnosis design', 'disease diagnosis', 'drug testing', 'electric impedance', 'improved', 'indexing', 'innovation', 'muscle strength', 'muscular structure', 'neuromuscular', 'novel diagnostics', 'programs', 'prototype', 'pulmonary function', 'tool', 'validation studies']",NINDS,"MYOLEX, INC.",R44,2013,531082,0.01686953393282622
"Context Understanding Technology to improve internet accessibility for users with     DESCRIPTION (provided by applicant): The purpose of this SBIR project is to develop a new tool to improve the ability of blind and visually impaired people to quickly get to the right web pages, to avoid ending up on the wrong web pages, and to reach the desired part of each web page efficiently. The motivation for this effort is that getting a 'big picture' understanding of a web page is one of the biggest challenges facing this population of computer users. Existing tools for blind and visually impaired people, such as screen readers and screen magnifiers, present the entire web page serially, in full detail, either by textual output of the details, or b providing a magnified view of a small part of the page. However, if the user is not already familiar with the website, the resulting lack of context requires a laborious and time-consuming process to scan through sometimes hundreds of details before knowing whether anything of interest even exists on the page in question. In contrast, the proposed technology analyzes web pages in a similar way that sighted users quickly scan pages before reading in detail, to provide a succinct, informative guidance on the context and layout of the page, so that a user quickly gains a much richer ""big-picture"" understanding. The proposed Phase II project builds on a successful Phase I user evaluation of a proof- of-concept of the software, in which users expressed a strong preference for the contextual cues provided by the approach. Phase II includes creating a fully-functional prototype of the software tool. Throughout the technology development, feedback from users and practitioners will guide the path of the R&D for maximum usability. At the conclusion of Phase II, an end-user, in-home evaluation study will verify the effectiveness of the tool, providing the starting point for full-scale commercialization.         PUBLIC HEALTH RELEVANCE: The R&D in this Phase II SBIR project will result in an improved way to use the Internet for individuals with visual disabilities. The technology will help to overcome the lack of accessibility and high level of frustration currently found among blind and visually impaired users. The results of the project will enable such individuals to explore new opportunities and be more productive, thus providing improved employment potential and an increase in the quality of life.                ",Context Understanding Technology to improve internet accessibility for users with,8459121,R44EY020082,"['Advertisements', 'Area', 'Arizona', 'Artificial Intelligence', 'Boxing', 'Characteristics', 'Color', 'Computer software', 'Computers', 'Cues', 'Effectiveness', 'Employment', 'Evaluation', 'Evaluation Studies', 'Feedback', 'Frustration', 'Generations', 'Government', 'Grouping', 'Home environment', 'Individual', 'Interest Group', 'Internet', 'Literature', 'Marketing', 'Motivation', 'Mus', 'Output', 'Persons', 'Phase', 'Population', 'Process', 'Quality of life', 'Reader', 'Reading', 'Research', 'Scanning', 'Small Business Innovation Research Grant', 'Software Tools', 'Speech', 'System', 'Technology', 'Text', 'Time', 'Training', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'base', 'blind', 'commercialization', 'disability', 'experience', 'improved', 'interest', 'natural language', 'phrases', 'preference', 'prototype', 'public health relevance', 'research and development', 'technology development', 'tool', 'usability', 'web page', 'web site']",NEI,"VORTANT TECHNOLOGIES, LLC",R44,2013,371933,0.041752215235868716
"Advanced training platform and methodologies for emergency responders and skilled     DESCRIPTION (provided by applicant): There is a need to be able to deliver just-in-time training and reference materials for first responders and skilled support personnel. Mobile computing platforms are becoming ubiquitous and provide an ideal means to reach users at any time in any location. The process of translating existing reference materials into mobile-friendly formats is currently manual and very labor intensive. Nicolalde R&D LLC is well under way to commercialize its mTraining mobile technology and service prototyped during a phase I SBIR from NIEHS. The mTraining technology is an objective and checklist-based method for delivering just-in-time training and reference materials, making it an effective Electronic Performance Support System (EPSS) for providing workers easy access to information after training, and on site prior to or during an assignment. It provides short, incident specific awareness and safety training that can be delivered prior to responding to an emergency situation. The proposed development under this phase II SBIR includes: a) a back-end document processing engine that is able to automatically parse, analyze, mark-up, and organize documents so that their content is easily cross-referenced, linked and re-organized for effective delivery on a mobile training platform or other electronic medium. This will be connected to a server and database architecture to facilitate its operation and support storing and accessing content; b) the front-end interface for the mobile training platform (mTraining) was prototyped in Phase I of this project for delivering training content to emergency responders, skilled support personnel, and volunteers before or during an incident. The improved back-end architecture will support intelligent search capabilities for a large repository of training documents with different structures. This capability relies on the document processing engine's ability to semi-automatically extract relevant data and automatically translate this data into a structured format. This data can then be used for display in the mobile application, stored into databases, and automatically populated into ontologies. Throughout this project the participatory-based design paradigm has been used for facilitating the integration of user requirements and the fast prototyping and testing of design alternatives. This approach will continue to be utilized in Phase 2 of the project.         PUBLIC HEALTH RELEVANCE: The proposed research and development will advance the field of environmental health and safety training by bringing to it new and innovative advanced training technologies that are based on the mobile and just-in-time paradigm. Furthermore, the research and development proposed herein will advance the mobile information technology field by developing robust and scalable tools for processing and linking information residing in different source documents that are semantically related.            ",Advanced training platform and methodologies for emergency responders and skilled,8518023,R44ES020135,"['Access to Information', 'Architecture', 'Awareness', 'Back', 'Case Study', 'Data', 'Databases', 'Development', 'Educational Curriculum', 'Educational Materials', 'Electronics', 'Emergency Situation', 'Environmental Health', 'Focus Groups', 'Human Resources', 'Information Technology', 'Link', 'Location', 'Manuals', 'Medical', 'Medical Students', 'Methodology', 'Methods', 'National Institute of Environmental Health Sciences', 'Natural Language Processing', 'Ontology', 'Performance', 'Phase', 'Process', 'Provider', 'Research Infrastructure', 'Retrieval', 'Safety', 'Semantics', 'Services', 'Simulate', 'Site', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Structure', 'Support System', 'Technology', 'Testing', 'Time', 'Time Management', 'Training', 'Training and Education', 'Translating', 'Update', 'base', 'computer based Semantic Analysis', 'design', 'emergency service responder', 'improved', 'innovation', 'interest', 'operation', 'public health relevance', 'repository', 'research and development', 'response', 'tool', 'usability', 'volunteer']",NIEHS,"NICOLALDE R AND D, LLC",R44,2013,142612,-0.012454427034089504
"Mobile Search for the Visually Impaired    DESCRIPTION (provided by applicant):    The technology developed as part of this NIH SBIR project will transform the cell phone camera of visually impaired individuals into a powerful tool capable of identifying the objects they encounter, track the items they own, or navigate complex new environments. Broad access to low-cost visual intelligence technologies developed in this project will improve the independence and capabilities of the visually impaired. There has been tremendous technological progress in computer vision and in the computational power and network bandwidth of and Smartphone platforms. The synergy of these advances stands to revolutionize the way people find information and interact with the physical world. However, these technologies are not yet fully in the hands of the visually impaired, arguably the population that could benefit the most from these developments. Part of the barrier to progress in this area has been that computer vision can accurately handle only a small fraction of the typical images coming from a cell phone camera. To cope with these limitations and make any-image recognition possible, IQ Engines will develop a hybrid system that uses both computer vision and crowdsourcing: if the computer algorithms are not able to understand an image, then the image is sent to a unique crowdsourcing network of people for image analysis. The proposed research includes specific aims to both develop advanced computer vision algorithms for object recognition and advanced crowdsourced networks optimized to the needs of the visually impaired community. This approach combines the speed and accuracy of computer vision with the robustness and understanding of human vision, ultimately providing the user fast and accurate information about the content of any image.      PUBLIC HEALTH RELEVANCE:    The image recognition technology developed in this application will enable the visually impaired to access visual information using a mobile phone camera, a device most people already have. Transforming the camera into an intelligent visual sensor will lower the cost of assistance and improve the quality of life of the visually impaired community through increased independence and capabilities.                 The image recognition technology developed in this application will enable the visually impaired to access visual information using a mobile phone camera, a device most people already have. Transforming the camera into an intelligent visual sensor will lower the cost of assistance and improve the quality of life of the visually impaired community through increased independence and capabilities.            ",Mobile Search for the Visually Impaired,8198847,R44EY019790,"['Address', 'Algorithms', 'Area', 'Car Phone', 'Cellular Phone', 'Classification', 'Client', 'Clip', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Crowding', 'Data', 'Databases', 'Detection', 'Development', 'Devices', 'Ensure', 'Environment', 'Family', 'Feedback', 'Friends', 'Glosso-Sterandryl', 'Human', 'Hybrid Computers', 'Hybrids', 'Image', 'Image Analysis', 'Individual', 'Intelligence', 'Label', 'Learning', 'Location', 'Modeling', 'Monitor', 'Phase', 'Population', 'Preparation', 'Process', 'Quality of life', 'Research', 'Running', 'Scanning', 'Services', 'Small Business Innovation Research Grant', 'Social Network', 'Source', 'Speed', 'System', 'Technology', 'Telephone', 'Time', 'United States National Institutes of Health', 'Vision', 'Visual', 'Visual impairment', 'base', 'blind', 'cell transformation', 'coping', 'cost', 'improved', 'innovation', 'novel', 'object recognition', 'open source', 'sensor', 'tool', 'visual information', 'visual search', 'volunteer']",NEI,"IQ ENGINES, INC.",R44,2012,499358,0.055970352893246554
"Fractal Identification System for Medication     DESCRIPTION (provided by applicant): Ai Cure Technologies LLC, was established in 2009 to develop computer vision solutions to ensure safer medication usage. This SBIR Phase I will allow Ai Cure Technologies to prove feasibility of an innovative identification and anti-counterfei labeling solution that replaces traditional barcodes and utilizes the self-similarity properties of fractals to encode and print unique secure fractal patterns directly onto pills or capsules. This SBIR Phase I will allow technical feasibility of this solution, Fractal-ID"", to be demonstrated and will test if a fractal pattern can be printed onto a pill and then recognized with computer vision software. The ability to accurately identify and authenticate pills enables medication to be correctly prescribed and administered. Yet a drug's packaging and labeling is not a stamp of authenticity. It is almost impossible for patients and healthcare professionals to identify pills based on their physical characteristics and to decipher the pill's imprint, its unique identifying code. As a result, the twin problems of counterfeit medication and medication errors - both of which depend upon pill identification - are very difficult to stem. Medication errors contribute heavily to the public health burden. 1.5 million people are harmed every year in the US because of medication errors and the cost for hospitals alone is approximately $3.5 billion. The physical appearances of pills play a large part - it is estimated that a pill's packaging and labeling are responsible for one third of medication errors. More than 10% of the world's drugs are counterfeit, rising to 50% for certain drugs in developing countries. The counterfeit drug market is estimated to be between $75 and $200 billion per year and is growing in countries with strict regulations. 100,000 deaths are attributed to counterfeit drugs every year and there is an unknown morbidity toll since substandard or counterfeit medicines are also responsible for drug resistance, therapeutic failure and dangerous health outcomes. Since medications are repackaged and change hands many times in the supply chain, direct pill and capsule labeling offers a more robust solution to medication identification. While barcodes may be used to serialize individual pills, their fixed designs are unsuitable for the shape, color, texture variablity found in pills and capsules. Furthermore, barcodes are easy to replicate, require fixed surface areas and specific alignment for printing, and are rendered unusable if occlusion occurs due to handling or damage. In addition, the item's physical attributes are distinct from the barcode itself. Ai Cure Technologies will offer pharmaceutical manufacturers, Fractal-ID(tm), a medication identification and authentication solution. The solution will equip manufacturers with printers and a license to a secure fractal label library; consumers will have access to a free computer vision software application to identify fractal pills; and investigators will be provided with higher resolution authentication tools.         PUBLIC HEALTH RELEVANCE: The ability to accurately identify and authenticate pills enables medication to be correctly prescribed and administered. As a result, the twin problems of counterfeit medication and medication errors - both of which depend upon pill identification - are very difficult to stem. Ai Cure Technologies will offer a medication identification solution for pils and capsules that will allow manufacturers, anti-counterfeit investigators, and patients to identif and authenticate medication.                       The ability to accurately identify and authenticate pills enables medication to be correctly prescribed and administered. As a result, the twin problems of counterfeit medication and medication errors - both of which depend upon pill identification - are very difficult to stem. Ai Cure Technologies will offer a medication identification solution for pils and capsules that will allow manufacturers, anti-counterfeit investigators, and patients to identif and authenticate medication.                     ",Fractal Identification System for Medication,8394091,R43TR000190,"['Address', 'Algorithms', 'Appearance', 'Area', 'Biomedical Research', 'Cessation of life', 'Characteristics', 'Code', 'Color', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Country', 'Data', 'Data Protection', 'Data Security', 'Developing Countries', 'Development', 'Devices', 'Drug Industry', 'Drug Packaging', 'Drug resistance', 'Ensure', 'Environment', 'Failure', 'Fingerprint', 'Fractals', 'Goals', 'Hand', 'Health', 'Health Professional', 'Health system', 'Hospital Costs', 'Image', 'Imaging technology', 'Individual', 'Label', 'Libraries', 'Licensing', 'Lighting', 'Manufacturer Name', 'Marketing', 'Medical', 'Medication Errors', 'Medication Systems', 'Medicine', 'Morbidity - disease rate', 'New York', 'Outcome', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Play', 'Printing', 'Process', 'Property', 'Public Health', 'Regulation', 'Research Personnel', 'Resolution', 'Secure', 'Security', 'Shapes', 'Small Business Innovation Research Grant', 'Solutions', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Twin Multiple Birth', 'Work', 'base', 'capsule', 'commercial application', 'design', 'drug market', 'imprint', 'innovation', 'lens', 'phase 1 study', 'pill', 'stem', 'tool', 'transmission process']",NCATS,"AI CURE TECHNOLOGIES, LLC",R43,2012,227209,-0.01667308479878778
"Vision Without Sight: Exploring the Environment with a Portable Camera  Vision without Sight: Exploring the Environment with a Portable Camera Project Summary As computer vision object recognition algorithms improve in accuracy and speed, and computers become more powerful and compact, it is becoming increasingly practical to implement such algorithms on portable devices such as camera-enabled cell phones. This ""mobile vision"" approach allows normally sighted users to identify objects, signs, places and other features in the environment simply by snapping a photo and waiting a few seconds for the results of the object recognition analysis. The approach holds great promise for blind or visually impaired (VI) users, who may have no other means of identifying important features that are undetectable by non-visual cues. However, in order for the approach to be practical for VI users, the interaction between the user and the environment using the camera must be properly facilitated. For instance, since the user may not know in advance which direction to point the camera towards a desired target, he or she must be able to pan the camera left and right to search for it, and receive rapid feedback whenever it is detected. Drawing on past experience of the PI and his colaborators on object recognition systems for VI users, we propose to study the use of mobile vision technologies for exploring features in the environment, specificaly examining the process of discovering these features and obtaining guidance towards them. Our main objectives are to investigate the strategies adopted by users of these technologies to expedite the exploration process, devise and test maximally effective user interfaces consistent with these strategies, and to assess and benchmark the efficiency of the technologies. The result will be a set of minimum design standards that will specify the system performance parameters, the user interface functionality and the operational strategies necessary for any mobile vision object recognition system for VI users.  Vision without Sight: Exploring the Environment with a Portable Camera Project Narrative The ability to locate and identify objects, places and other features in the environment is taken for granted every day by the sighted, but this fundamental capability is missing or severely degraded in the approximately 10 million Americans with significant vision impairments and a million who are legally blind. The proposed research would investigate how computer vision object recognition technologies, which are now being implemented on mobile vision devices such as cel phones but are typicaly designed for normaly sighted users, could be modified and harnessed to meet the special needs of blind and visually impaired persons. Such research could lead to new technologies to dramatically improve independence for this population",Vision Without Sight: Exploring the Environment with a Portable Camera,8334623,R21EY021643,"['Address', 'Adopted', 'Algorithms', 'American', 'Benchmarking', 'Cellular Phone', 'Computer Hardware', 'Computer Vision Systems', 'Computers', 'Cues', 'Development', 'Devices', 'Environment', 'Feedback', 'Glosso-Sterandryl', 'Goals', 'Goggles', 'Grant', 'Image Analysis', 'Impairment', 'Lead', 'Learning', 'Left', 'Location', 'Performance', 'Population', 'Printing', 'Process', 'Research', 'Self-Help Devices', 'Specific qualifier value', 'Speed', 'System', 'Task Performances', 'Technology', 'Telephone', 'Testing', 'Time', 'Touch sensation', 'Training', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'blind', 'design', 'experience', 'improved', 'insight', 'interest', 'legally blind', 'meetings', 'new technology', 'object recognition', 'operation', 'usability', 'visual feedback']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2012,229834,0.06367523255688948
"Longitudinal Assessment of Fall Risk    DESCRIPTION (provided by applicant): Falling is not a normal part of the aging process and yet 1/3 to 1/2 of adults 65 years and older sustain at least one fall annually. Older adults are hospitalized for fall related injuries five times more often than from injuries from other causes contributing to a cost of $19 billion for nonfatal falls in the United States. Projected for the increasing aging population in the year 2020, it is expected that the costs related to falls will reach a staggering 54.9 billion dollars. Current research and clinical practice guidelines focus on multifactorial fall risk assessments as the critical deterrent to falls in the elderly. A primary factor within these assessments is activity of daily living performance of the individual elder. While current standardized clinical balance assessment tools have been proven effective for predicting fall risk, the tests are most commonly performed in the clinical environment and at isolated times during an individual's day. The goal of this application is to develop and validate a novel wearable device (Automatic Longitudinal Assessment Risk Monitor - ALARM) for longitudinal assessment of risk of falling. Such a device: - will allow early detection of risk of falling, when therapeutic interventions are most efficient - will provide real-time feedback about activity pattern - will provide feedback about compliance with interventions and effectiveness of interventions - will be incorporated into conventional footwear and require no extra effort to operate - can be used in research, clinical and potentially in consumer applications The development of the ALARM system will be addressed in three specific aims:  Specific Aims 1: Develop a pattern recognition method that will improve recognition accuracy for activities of interest (such as walking and stepping up) by reducing the range of variation from current 76%- 100% to 9911%. Specific Aim 2: Collect data using the ALARM device on a group of elderly adults during clinical tests. Specific Aim 3: Develop algorithms for automatic assessment of risk of falling. In this Aim we will develop signal processing algorithms that automatically evaluate metrics indicative of the risk of falling in each activity of interest (e.g. duration of swing and stance phase during walking). Specific Aim 4: Validate the ALARM device in a double-blind unrestricted free living study. This set of Specific Aims will validate lead to creation of a unique wearable device capable of objective characterization of risk of falling.        This application aims at development of a novel wearable device (Automatic Longitudinal Assessment Risk Monitor - ALARM) for longitudinal assessment of risk of falling. In our previous research we have shown that major activities and posture allocations such as standing, sitting, walking, etc. can be recognized with high degree of accuracy (76%-100%) by a wearable device incorporated into conventional footwear. We also have shown that sensor signals captured by the wearable shoe device during activities such as walking are well- correlated with the risk of falling (with numerical estimates of risk obtained through signal processing being directly proportional to the normalized scores from the clinical tests). The goal of this application is to develop and validate a novel wearable device (ALARM) for longitudinal assessment of risk of falling.         ",Longitudinal Assessment of Fall Risk,8339885,R21EB013183,"['Acceleration', 'Activities of Daily Living', 'Address', 'Adult', 'Aging-Related Process', 'Algorithms', 'Classification', 'Clinical', 'Clinical Practice Guideline', 'Communities', 'Comparative Study', 'Computational algorithm', 'Computers', 'Data', 'Data Set', 'Development', 'Devices', 'Double-Blind Method', 'Early Diagnosis', 'Effectiveness of Interventions', 'Elderly', 'Engineering', 'Environment', 'Equilibrium', 'Evaluation', 'Feasibility Studies', 'Feedback', 'Goals', 'Heel', 'Individual', 'Injury', 'Intervention', 'Laboratories', 'Lead', 'Life', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Metric', 'Monitor', 'Neural Network Simulation', 'Outcome', 'Pattern', 'Pattern Recognition', 'Performance', 'Phase', 'Posture', 'Process', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Risk', 'Risk Assessment', 'Risk Estimate', 'Series', 'Shoes', 'Signal Transduction', 'System', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'United States', 'Validation', 'Variant', 'Walking', 'aging population', 'base', 'computerized data processing', 'cost', 'fall risk', 'falls', 'human old age (65+)', 'improved', 'interest', 'novel', 'pressure', 'sensor', 'tool', 'volunteer']",NIBIB,UNIVERSITY OF ALABAMA IN TUSCALOOSA,R21,2012,185300,-0.018757445995440773
"Perception of Tactile Graphics    DESCRIPTION (provided by applicant): The broad objective of the proposed research is to answer the following question: why are tactile graphics difficult to understand? People with normal vision can easily recognize line drawings of objects. However, both blind and sighted people find it very difficult to recognize the same drawings when they are presented as tactile images. For blind people, tactile graphics are the only solution for accessing information in visual diagrams and illustrations found in textbooks. Consequently, the results of the proposed research will be used to improve the production of tactile graphics so that they are better understood by blind people. The specific aims of this project are to: 1) explore how the complexity of tactile images affects perception, 2) determine the effects of spatial and temporal integration on perception of tactile images, and 3) investigate how well people can recognize tactile images of objects embedded in backgrounds. The general methodology of the proposed experiments is to present participants with tactile images and to have them draw what they perceive the images to be. Blind individuals will draw tactile images using special paper and a stylus. The experimenters will evaluate the drawings by using a quantitative measure that computes a distance score reflecting the discrepancy between the original image and the participant's drawing. In the first study, participants will feel tactile stimuli of varying complexity, from simple lines in different orientations to complex depictions of objects. The second study will determine the limitations of tactile perceptual integration by limiting either the spatial or temporal window over which participants feel the image. Participants will either view or feel images through apertures of various sizes (spatial window) or they will have a limited amount of time to view or feel the images (temporal window). In the third study, participants will feel tactile images of objects embedded in simple backgrounds. This research will impact several areas of study, including computer vision, human object and scene recognition, and low vision rehabilitation.      PUBLIC HEALTH RELEVANCE: Relevance The proposed research seeks to improve the translation of visual graphics into tactile graphics for use by visually-impaired individuals. By making the information in visual graphics more accessible, this research will improve educational services for people with low vision. This work will also facilitate the development of better visual-to-tactile substitution technologies.              Relevance The proposed research seeks to improve the translation of visual graphics into tactile graphics for use by visually-impaired individuals. By making the information in visual graphics more accessible, this research will improve educational services for people with low vision. This work will also facilitate the development of better visual-to-tactile substitution technologies.",Perception of Tactile Graphics,8265243,F32EY019622,"['Affect', 'Area', 'Categories', 'Child', 'Complex', 'Computer Vision Systems', 'Development', 'Devices', 'Disadvantaged', 'Education', 'Elements', 'Goals', 'Grouping', 'Human', 'Image', 'India', 'Individual', 'Link', 'Measures', 'Methodology', 'Methods', 'Names', 'Nature', 'Paper', 'Participant', 'Perception', 'Population', 'Production', 'Psychophysics', 'Rehabilitation therapy', 'Research', 'Science', 'Sensory', 'Services', 'Shapes', 'Solutions', 'Stimulus', 'Swelling', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Time', 'Touch sensation', 'Translating', 'Translations', 'Vision', 'Visual', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'blind', 'braille', 'improved', 'object recognition', 'public health relevance', 'research study', 'sight for the blind', 'skills', 'tactile vision substitution system', 'two-dimensional', 'vision development', 'visual process', 'visual processing']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,F32,2012,52190,0.0048055188056710385
"A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians    DESCRIPTION (provided by applicant): A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians Abstract Project Summary Urban intersections are among the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard cell phone, to provide real-time feedback. Building on our past work on a prototype ""Crosswatch"" system that uses computer vision algorithms to find crosswalks and Walk lights, we will greatly enhance the functionality of the system with information about the intersection layout and the identity of its connecting streets, the presence of stop signs, one-way signs and other controls indicating right-of-way, and timing information integrated from Walk/Don't Walk lights, countdown timers and other traffic lights. The system will convey intersection information, and will actively guide the user to align himself/herself with crosswalks, using a combination of synthesized speech and audio tones. We will conduct human factors studies to optimize the system functionality and the configuration of the user interface, as well as develop interactive training applications to equip users with the skills to better use the system. These training applications, implemented as additional cell phone software to complement the intersection system, will train users to hold the camera horizontal and forward and to minimize veer when traversing a crosswalk. The intersection analysis and training software will be made freely available for download onto popular cell phones (such as iPhone, Android or Symbian models). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function.      PUBLIC HEALTH RELEVANCE: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.              Public Health Relevance The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.",A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians,8227997,R01EY018345,"['Address', 'Adoption', 'Algorithms', 'American', 'Cellular Phone', 'Complement', 'Complement component C4', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Crowding', 'Custom', 'Data', 'Development', 'Devices', 'Equipment', 'Face', 'Feedback', 'Glosso-Sterandryl', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Length', 'Light', 'Location', 'Mainstreaming', 'Modeling', 'Modification', 'Names', 'Process', 'Reading', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Running', 'Safety', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Speed', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'abstracting', 'base', 'blind', 'consumer product', 'cost', 'design', 'improved', 'interest', 'legally blind', 'meter', 'prototype', 'public health relevance', 'sensor', 'skills', 'trafficking', 'volunteer', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2012,403177,0.0010568300282785628
"Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance     DESCRIPTION (provided by applicant): The neuromuscular disorders include a wide group of conditions ranging from relatively mild focal problems, such as carpal tunnel syndrome, to severe, generalized diseases, such as amyotrophic lateral sclerosis (ALS). Current methods for evaluating these disorders remain limited to a variety of tests and procedures that are either invasive or remarkably qualitative in nature. One newer technique, electrical impedance myography (EIM), offers the prospect of obtaining quantitative data on muscle condition painlessly and non-invasively. However, no commercial devices specifically tailored for EIM use exist, with nearly all previous EIM work relying upon off-the-shelf bioimpedance devices designed for ""whole-body"" rather than single-muscle assessment. In Phase 1 of this SBIR, Convergence Medical Devices, Inc (CMD) developed an initial prototype system to test proof-of-principle innovative concepts in circuit design and data acquisition to provide highly sensitive and accurate data at an extended frequency range (out to 10 MHz) and over multiple angles relative to the major muscle fiber direction. This work was followed by supplemental work that produced the first hand-held device with robust electronics capable of similar measurements with a detachable, interchangeable electrode array. In this 3-year Phase 2 SBIR, CMD proposes to further refine that technology so as to assist in disease diagnosis and for following disease progression, with a specific focus on amyotrophic lateral sclerosis. In Aim 1 of the proposed grant, testing of multiple electrode array designs will be performed on a group of normal subjects and those with ALS to determine which electrode array designs offer the greatest reproducibility and ability to discriminate healthy from diseased individuals. In Aim 2, the device and the 3 ""best"" array designs identified in Aim 1 will be tested in 30 ALS patients, 30 patients with syndromes mimicking ALS (e.g., polyradiculopathy and motor-predominant neuropathy), and 30 subjects with suspected/possible ALS to determine the single best array design for diagnosis and to determine its sensitivity and specificity. This study will take place at CMD and four external sites and will be managed by the Northeast ALS trials Consortium (NEALS), the premier organization for overseeing clinical trials in ALS. In Aim 3, the 30 suspected/possible ALS patients recruited in SA2 and 30 additional suspected/possible ALS patients will be assessed monthly using EIM and the 3 arrays identified in SA1. All subjects will also undergo standard measurements of disease progression including handheld dynamometry, ALS Functional Rating Scale, and spirometry. The best array design for assessing disease progression will be identified and compared to conventional markers of disease progression to determine EIM's sensitivity to disease status. Thus, at the conclusion of this research program, we anticipate having developed and vigorously tested an EIM device and associated electrode designs that together will serve as a powerful new diagnostic and monitoring tool for neuromuscular disease.        PUBLIC HEALTH RELEVANCE: The neuromuscular disorders are a group of conditions that impact the lives of millions of people in the United States alone. In this study, Convergence Medical Devices, in collaboration with neurologists at several medical centers, will develop and test a new device based on the concept of electrical impedance myography, a technique that offers the promise of rapid and accurate disease assessment. With the completion of this study, a new, painless tool will be developed and studied that will assist in individual patient care and will speed drug testing in clinical trials in diseases ranging from amyotrophic lateral sclerosis and muscular dystrophy to radiculopathy and polyneuropathy.              The neuromuscular disorders are a group of conditions that impact the lives of millions of people in the United States alone. In this study, Convergence Medical Devices, in collaboration with neurologists at several medical centers, will develop and test a new device based on the concept of electrical impedance myography, a technique that offers the promise of rapid and accurate disease assessment. With the completion of this study, a new, painless tool will be developed and studied that will assist in individual patient care and will speed drug testing in clinical trials in diseases ranging from amyotrophic lateral sclerosis and muscular dystrophy to radiculopathy and polyneuropathy.            ",Noninvasive Assessment of Neuromuscular Disease using Electrical Impedance,8315044,R44NS070385,"['Amyotrophic Lateral Sclerosis', 'Area', 'Biological Markers', 'Biopsy', 'Businesses', 'Carpal Tunnel Syndrome', 'Clinical', 'Clinical Trials', 'Collaborations', 'Cross-Sectional Studies', 'Data', 'Data Analyses', 'Device Designs', 'Devices', 'Diagnosis', 'Diagnostic', 'Discrimination', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Electrodes', 'Electromyography', 'Electronics', 'Entrapment Neuropathies', 'Frequencies', 'Future', 'Genetic', 'Goals', 'Grant', 'Individual', 'Limb structure', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Measures', 'Medical Device', 'Medical center', 'Methods', 'Monitor', 'Motor', 'Motor Neurons', 'Muscle', 'Muscle Fibers', 'Muscular Dystrophies', 'Myography', 'Nature', 'Needles', 'Neurologist', 'Neuromuscular Diseases', 'Neuropathy', 'Outcome', 'Painless', 'Patient Care', 'Patients', 'Phase', 'Physicians', 'Polyneuropathy', 'Polyradiculopathy', 'Procedures', 'Radiculopathy', 'Recruitment Activity', 'Relative (related person)', 'Reproducibility', 'Research', 'Sensitivity and Specificity', 'Site', 'Small Business Innovation Research Grant', 'Speed', 'Spirometry', 'Symptoms', 'Syndrome', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Clinical Trial', 'Time', 'United States', 'Widespread Disease', 'Work', 'base', 'clinical care', 'commercial application', 'data acquisition', 'design', 'diagnosis design', 'disease diagnosis', 'drug testing', 'electric impedance', 'improved', 'indexing', 'innovation', 'muscle strength', 'muscular structure', 'neuromuscular', 'novel diagnostics', 'programs', 'prototype', 'pulmonary function', 'tool', 'validation studies']",NINDS,"MYOLEX, INC.",R44,2012,620921,0.014768568639787488
"Handsight: Mobile Services for Low Vision    DESCRIPTION (provided by applicant): Handsight is a mobile phone service that offers an affordable, extensible set of automated sight-assistant functions to millions of blind and low-vision persons at $30/month atop standard voice and data fees. To the user, Handsight is simply a mobile phone application, requiring no special equipment or updating. By aiming a mobile telephone<s camera in roughly the right direction and pressing just one button, a Handsight user can snap a picture, send it to the Handsight computer center along with location data, and have a response within seconds. Moving the key computation and data from the handset to web servers cut cost, eases technology upgrades, and enables numerous data and technology partnerships needed to rapidly solve a broad spectrum of tasks. To allow widespread adoption Handsight uses voice, keypad, and vibration for user interaction; and for extensibility it is built on open-source software both on the handset and on the web application infrastructure. The range of tasks encountered by the blind and low-vision requires an array of components to solve: some are more heavily text-oriented and involve no location/navigational feedback (distinguishing and identifying different medicines; finding the phone bill in a stack of letters); some are more specifically navigational (locating the exact store entrance, finding the bus stop); yet others are informational (finding out when the next bus is due). Since we aim to provide a broadly useful tool, Handsight has to accommodate the whole range of task types. We are therefore proposing to build an architecture that integrates a set of components addressing the various task types, from text-detection and recognition software to navigational databases. Handsight<s application programming interface (API) will enable third parties to add capabilities to solve new tasks. As a web service, Handsight can evolve as computer vision and navigation technologies advance without requiring users to upgrade handsets or buy new versions of software. Phase I of this project was funded by the Dept. of Education / NIDRR and completed successfully between 10/01/2007 and 04/01/2008 under grant # H133S070044. It demonstrated not just the viability of the proposed project but also likely demand for such a service. (The NIDRR<s immutable deadlines precluded us from pursuing a Phase II with that agency.) Phase II consists in building the cell phone application and remote processing infrastructure with APIs to enable plug-in of the basic and future features. Subcontractor Smith-Kettlewell Institute for Eye Research will assure usability by blind and low-vision users; data partner NAVTEQ will provide a range of leading-edge digital map data. Blindsight already owns fast, accurate text detection and recognition software, and has experience in building scalable web services. A minimum set of features that make for a system with widespread appeal will be developed and/or licensed, and integrated into the service.      PUBLIC HEALTH RELEVANCE: The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today<s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, and shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.        Re: Project Title: Handsight: Mobile Services for Low Vision  FOA: PHS 2009-02 Omnibus Solicitation of the NIH, CDC, FDA and ACF for Small Business  Innovation Research Grant Applications (Parent SBIR [R43/R44])  Proposed project period: April 1, 2010 - March 31, 2012  Principal Investigator: Hallinan, Peter W. SF424 Research and Related Other Project Information: Project Narrative The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today�s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.",Handsight: Mobile Services for Low Vision,8473426,R44EY020707,"['Activities of Daily Living', 'Address', 'Adoption', 'Architecture', 'Area', 'Car Phone', 'Cellular Phone', 'Centers for Disease Control and Prevention (U.S.)', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Dependence', 'Destinations', 'Detection', 'Devices', 'Education', 'Eye', 'Feedback', 'Fees', 'Focus Groups', 'Friends', 'Funding', 'Future', 'Grant', 'Health', 'Human', 'Individual', 'Institutes', 'Internet', 'Letters', 'Licensing', 'Location', 'Maps', 'Medicine', 'Mission', 'Parents', 'Persons', 'Phase', 'Plug-in', 'Principal Investigator', 'Process', 'Provider', 'Reading', 'Research', 'Research Design', 'Research Infrastructure', 'Research Institute', 'Research Methodology', 'Research Support', 'Services', 'Simulate', 'Small Business Innovation Research Grant', 'Social Interaction', 'Special Equipment', 'System', 'Target Populations', 'Technology', 'Telephone', 'Text', 'Touch sensation', 'Training', 'Travel', 'United States National Institutes of Health', 'Update', 'Vision', 'Vision Disorders', 'Visual impairment', 'Voice', 'Work', 'blind', 'computer center', 'cost', 'digital', 'experience', 'falls', 'open source', 'programs', 'public health relevance', 'response', 'success', 'tool', 'usability', 'vibration', 'voice recognition', 'web services']",NEI,BLINDSIGHT CORPORATION,R44,2012,407051,0.039068733573841156
"Low-Cost Electronic Nose for Groundwater Contaminants    DESCRIPTION (provided by applicant): Several US agencies and regulators require low-cost chemical sensors for detecting and monitoring environmental clean-up, remediation, and decommissioning processes where groundwater may be contaminated. The sensors must be capable of detecting contaminants in the sub-surface groundwater and must be compatible with use in a range of environments. Most significantly, these customers require a low-cost alternative to its current expensive and labor intensive methods, namely using mobile laboratories. The project will result in the innovative use of low-cost sensor systems that will be capable of detecting and monitoring for dense non-aqueous phase liquids in the subsurface and groundwater, unattended, and in real- time from within a push-probe, using a chemicapacitor array and miniature preconcentrator.  The ultimate goal of this SBIR project is to provide the DOD, DOE, and other agencies with a method to map and track subsurface contamination plumes in real-time without requiring an operator. In Phase I, Seacoast successfully demonstrated the feasibility of using a microsensor array with a proprietary trap-and- purge preconcentrator to detect chlorinated solvents, specifically TCE, and TCA, at levels low enough to meet EPA mandated levels for drinking water. In Phase II Seacoast proposes to improve the selectivity and sensitivity of the system to better meet the needs identified by the Phase I consultant. The systems have MEMS microcapacitor sensor arrays that can monitor for leaks of toxic chemicals, contaminants from wastes, and changes in groundwater streams. A preconcentrator traps the contaminants and releases them to a microsensor array. These sensor arrays are filled with several chemoselective polymers whose dielectric permittivity changes when exposed to different vapors, creating a fingerprint response for each chemical.  In Phase II Seacoast will specifically develop new materials to improve the sensor array selectivity, 1) by using impedance spectroscopy to study the mechanisms by which the polymer-based sensors sorb the target chemicals, 2) by implementing pattern recognition algorithms to identify chemicals for the sensor responses, and 3) by designing new preconcentrator materials that can bind these chemicals more strongly.  The most important application to public health and safety is unattended monitoring of drinking water, water treatment processes, and water sources. Potential markets include building chemical process monitoring and control, toxic vapor leak detection, industrial process control, and industrial health and safety. Transitioning the developed prototype to other markets where worker and public health, environmental health and regulatory compliance will be investigated to reduce the financial risks and broaden the acceptance of the technology.      PUBLIC HEALTH RELEVANCE: This proposal describes a novel technology that specifically addresses the need for detecting groundwater contaminants and long-term monitoring of contaminated sites, by providing an unattended sensor system that tracks contamination in real-time and transmits contaminant concentrations. Such a system would be used in tandem with other methods, to provide comprehensive contamination management at DOE, DOD, and Superfund sites where ground and water clean-up projects are already underway. The proposed work will focus on detection of chlorinated hydrocarbons, which are described as among the most common pollutants in groundwater and soils at DOE sites.           PROJECT NARRATIVE This proposal describes a novel technology that specifically addresses the need for detecting groundwater contaminants and long-term monitoring of contaminated sites, by providing an unattended sensor system that tracks contamination in real-time and transmits contaminant concentrations. Such a system would be used in tandem with other methods, to provide comprehensive contamination management at DOE, DOD, and Superfund sites where ground and water clean-up projects are already underway. The proposed work will focus on detection of chlorinated hydrocarbons, which are described as among the most common pollutants in groundwater and soils at DOE sites.",Low-Cost Electronic Nose for Groundwater Contaminants,8260510,R44ES016941,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Benchmarking', 'Benzene', 'Carcinogens', 'Chemicals', 'Chlorinated Hydrocarbons', 'Classification', 'Collection', 'Computer software', 'Cost Analysis', 'Cost Savings', 'Data', 'Data Collection', 'Detection', 'Development', 'Electronics', 'Engineering', 'Environment', 'Environmental Health', 'Environmental Monitoring', 'Equation', 'Equipment', 'Evaluation', 'Fingerprint', 'Fluorescence', 'Gases', 'Goals', 'Guidelines', 'Hazardous Waste', 'Industrial Health', 'Intervention', 'Laboratories', 'Lasers', 'Left', 'Liquid substance', 'Maps', 'Marketing', 'Measures', 'Metals', 'Methods', 'Monitor', 'National Institute of Environmental Health Sciences', 'Nose', 'Pattern Recognition', 'Pesticides', 'Phase', 'Plants', 'Poison', 'Pollution', 'Polymers', 'Process', 'Public Health', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Route', 'Safety', 'Sampling', 'Science', 'Scientific Advances and Accomplishments', 'Site', 'Small Business Innovation Research Grant', 'Soil', 'Solvents', 'Source', 'Spectrum Analysis', 'Stream', 'Surface', 'System', 'Technology', 'Temperature', 'Time', 'Trichloroethylene', 'Water', 'Wireless Technology', 'Work', 'analytical method', 'base', 'chemical binding', 'cost', 'design', 'detector', 'drinking water', 'electric impedance', 'ground water', 'improved', 'innovation', 'instrument', 'knowledge base', 'meetings', 'method development', 'new technology', 'novel', 'operation', 'pollutant', 'programs', 'prototype', 'public health relevance', 'purge', 'remediation', 'response', 'sensor', 'success', 'superfund site', 'vapor', 'wasting', 'water treatment']",NIEHS,"SEACOAST SCIENCE, INC.",R44,2012,459419,0.025281801194894434
"Accessible Artificial Intelligence Tutoring Software for Mathematics    DESCRIPTION (provided by applicant): This Fast-Track application focuses on developing the first artificial intelligence (AI) educational software to teach developmental mathematics to the blind and visually impaired. This project responds to the National Eye Institute's General Research Topics for ""teaching tools"" and the Visual Impairment and Blindness Program for ""other devices that meet the rehabilitative and everyday living needs of persons who are blind or have low vision."" The intervention being developed will place a comprehensive set of AI mathematics tutoring systems with integrated AI assessment capabilities in the hands of the blind K-12, college and adult student, for use on demand during study at home and at school. The formulation of an advanced AI tutoring methodology with accessibility inherent to its design will have broad implications for development in many subject areas beyond mathematics. Project objectives include: Horizontal Expansion of Accessible Curriculum Content Coverage (Ratio and Proportion, Percentages, Linear Equations, Metric Units, Scientific Notation) 1) Conduct initial accessibility review and analysis of AI tutor's existing user interface. 2) Implement accessibility requirements and recommendations from NFB, instructors and other partners. 3) Conduct final review to gain NFB accessibility certification after implementation of requirements. 4) Develop and issue survey of instructors on mathematics pedagogy and technology. Vertical Expansion of Accessible Features and Technological Capability 5) Implement Braille support in AI technology. 6) Develop additional AI tutor on Fractions that is automatically accessible from first principles using accessible AI framework. Evaluation of Accessible AI Educational Technology 7) Field evaluation of accessible AI technology with blind students and their instructors. 8) Continued demonstration and review of accessible AI technology by partners and other stakeholders. Preparation for success in Phase III has already been undertaken by involving partners that are important commercially as well as technically, such as the National Federation of the Blind and the American Printing House for the Blind (APH). In addition, Quantum already has long-term partnerships established with McGraw- Hill and Holt, Rinehart and Winston, two of the country's leading educational publishers, as well as a major science education catalog company, CyberEd, Inc., a PLATO Learning Company. PUBLIC HEALTH RELEVANCE: There is a considerable need for improved educational software for mathematics in general, but the problem of quality educational software materials for the blind and visually impaired is particularly acute. A weak mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of science, technology, engineering and mathematics. Through previous federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI) tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power and benefit of this cutting-edge educational technology to students who are blind and visually impaired.          PROJECT NARRATIVE:  There is a considerable need for improved educational software for mathematics in general, but the problem of  quality educational software materials for the blind and visually impaired is particularly acute. A weak  mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or  even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of  science, technology, engineering and mathematics. Through previous federally-supported research, Quantum  Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI)  tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power  and benefit of this cutting-edge educational technology to students who are blind and visually impaired.",Accessible Artificial Intelligence Tutoring Software for Mathematics,8055353,R44EY019414,"['Activities of Daily Living', 'Acute', 'Address', 'Adult', 'American', 'Area', 'Artificial Intelligence', 'Blindness', 'Businesses', 'Cataloging', 'Catalogs', 'Categories', 'Certification', 'Chemicals', 'Chemistry', 'Collaborations', 'Computer software', 'Country', 'Development', 'Development Plans', 'Devices', 'Dimensions', 'Drug Formulations', 'Dyslexia', 'Education', 'Educational Curriculum', 'Educational Technology', 'Educational process of instructing', 'Elements', 'Engineering', 'Ensure', 'Equation', 'Equilibrium', 'Evaluation', 'Feedback', 'Future', 'Goals', 'Health', 'Home environment', 'Housing', 'Individual', 'Institution', 'Instruction', 'Internet', 'Intervention', 'Language', 'Learning', 'Letters', 'Life', 'Mathematics', 'Measures', 'Mediation', 'Methodology', 'Metric', 'Mission', 'Modeling', 'National Eye Institute', 'Nature', 'Outcome', 'Performance', 'Persons', 'Phase', 'Philosophy', 'Play', 'Preparation', 'Printing', 'Process', 'Publishing', 'Reader', 'Reading Disabilities', 'Recommendation', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Research Support', 'Role', 'Schools', 'Science', 'Small Business Innovation Research Grant', 'Software Tools', 'Students', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Visual impairment', 'Work', 'blind', 'braille', 'career', 'college', 'commercial application', 'commercialization', 'design', 'disability', 'high school', 'improved', 'innovation', 'innovative technologies', 'instructor', 'meetings', 'middle school', 'programs', 'prospective', 'prototype', 'quantum', 'quantum chemistry', 'remediation', 'research and development', 'science education', 'simulation', 'success', 'teacher', 'technological innovation', 'tool']",NEI,"QUANTUM SIMULATIONS, INC.",R44,2011,394165,0.013723456480975671
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,8142000,R01EY016093,"['Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Peripheral', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,1141143,0.045889024392851814
"Cue Reliability and Depth Calibration During Space Perception    DESCRIPTION (provided by applicant): The long-term objective of the proposed work is to understand how learning by the visual system helps it to represent the immediate environment during perception. Because perception is accurate, we can know spatial layout: the shapes, orientations, sizes, and spatial locations of the objects and surfaces around us. But this accuracy requires that the visual system learn over time how best to interpret visual ""cues"". These cues are the signals from the environment that the visual system extracts from the retinal images that are informative about spatial layout. Known cues include binocular disparity, texture gradients, occlusion relations, motion parallax, and familiar size, to name a few. How do these cues come to be interpreted correctly? A fundamental problem is that visual cues are ambiguous. Even if cues could be measured exactly (which they cannot, the visual system being a physical device) there would still be different possible 3D interpretations for a given set of cues. As a result, the visual system is forced to operate probabilistically: the way things ""look"" to us reflects an implicit guess as to which interpretation of the cues is most likely to be correct. Each additional cue helps improve the guess. For example, the retinal image of a door could be interpreted as a vertical rectangle or as some other quadrilateral at a non-vertical orientation in space, and the shadow cues at the bottom of the door helps the system know that it's a vertical rectangle. What mechanisms do the visual system use to discern which cues are available for interpreting images correctly? The proposed work aims to answer this fundamental question about perceptual learning. It was recently shown that the visual system can detect and start using new cues for perception. This phenomenon can be studied in the laboratory using classical conditioning procedures that were previously developed to study learning in animals. In the proposed experiments, a model system is used to understand details about when this learning occurs and what is learned. The data will be compared to predictions based on older, analogous studies in the animal learning literature, and interpreted in the context of Bayesian statistical inference, especially machine learning theory. The proposed work benefits public health by characterizing the brain mechanisms that keep visual perception accurate. These mechanisms are at work in the many months during which a person with congenital cataracts learns to use vision after the cataracts are removed, and it is presumably these mechanisms that go awry when an individual with a family history of synesthesia or autism develops anomalous experience-dependent perceptual responses. Neurodegenerative diseases may disrupt visual learning, in which case visual learning tests could be used to detect disease; understanding the learning of new cues in human vision could lead to better computerized aids for the visually impaired; and knowing what causes a new cue to be learned could lead to new technologies for training people to perceive accurately in novel work environments.          n/a",Cue Reliability and Depth Calibration During Space Perception,8139754,R01EY013988,"['Address', 'Adult', 'Animal Behavior', 'Animals', 'Appearance', 'Autistic Disorder', 'Binocular Vision', 'Biological Models', 'Brain', 'Calibration', 'Cataract', 'Computer Vision Systems', 'Cues', 'Data', 'Devices', 'Diagnosis', 'Disease', 'Environment', 'Experimental Designs', 'Family history of', 'Food', 'Funding', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Learning Disabilities', 'Literature', 'Location', 'Longevity', 'Machine Learning', 'Measures', 'Memory', 'Motion', 'Motion Perception', 'Names', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathology', 'Perception', 'Perceptual learning', 'Persons', 'Positioning Attribute', 'Primates', 'Procedures', 'Process', 'Public Health', 'Recruitment Activity', 'Research', 'Retinal', 'Reversal Learning', 'Rotation', 'Shadowing (Histology)', 'Shapes', 'Signal Transduction', 'Source', 'Space Perception', 'Stimulus', 'Surface', 'System', 'Testing', 'Texture', 'Time', 'Training', 'Translations', 'Trust', 'Ursidae Family', 'Vision', 'Vision Disparity', 'Visual', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'Workplace', 'area MT', 'base', 'classical conditioning', 'clinical application', 'computerized', 'congenital cataract', 'design', 'devices for the visually impaired', 'experience', 'improved', 'meetings', 'neuromechanism', 'new technology', 'novel', 'programs', 'relating to nervous system', 'research study', 'response', 'stereoscopic', 'theories', 'tool', 'visual information', 'visual learning', 'visual process', 'visual processing']",NEI,STATE COLLEGE OF OPTOMETRY,R01,2011,219694,0.004071114958156312
"The Development of a Noninvasive Monitoring System for Cigarette Smoking    DESCRIPTION (provided by applicant): Cigarette smoking is the leading cause of preventable death in the United States. Smoking produces over 440,000 deaths each year in this country and generates an estimated $167 billion in annual health-related economic losses. Available methods of smoking assessment (e.g., self-report, portable puff-topography instruments) do not permit the collection of accurate, non-reactive measures of smoking behavior that capture real-time smoking frequency and comprehensive within-cigarette puff topography. The objective of this project is to develop a non-invasive wearable system (Personal Automatic Cigarette Tracker - PACT) that is completely transparent to the end user and does not require any conscience effort to achieve reliable monitoring of smoking behavior in free living individuals. Methodologically, PACT will consist of two major components: 1. Wearable sensors. Miniature sensors integrated into the clothing will monitors the breathing and activity patterns of individuals. The signals from the sensors will be processed and recognized to identify and objectively characterize each individual puff. 2. Software for signal processing and pattern recognition. Automatic computer software will analyze sensor signals and detect patterns uniquely identifying smoking events. Objective metrics such as number of puffs and inter-puff interval will be extracted. The software will be based on the state-of-art machine learning methods. The development of the PACT system will be addressed in four specific aims: Specific Aim 1: Develop a wearable sensor system comprised of a breathing sensor integrated into conventional underwear and a hand gesture sensor integrated into a hand bracelet. Specific Aim 2: Collect sensor data from individuals wearing the instrumented system and performing everyday activities (including smoking) in laboratory conditions. Specific Aim 3: Develop pattern recognition methods to recognize individual puffs and smoke inhalation. Specific Aim 4: Evaluate the utility and sensitivity of the wearable sensor PACT system and pattern recognition method in people smoking in the natural environment. This set of Specific Aims will validate lead to creation of a unique wearable device capable of objective characterization of smoking behavior.      PUBLIC HEALTH RELEVANCE: Cigarette smoking is the leading cause of preventable death in the United States. Smoking produces over 440,000 deaths each year in this country and generates an estimated $167 billion in annual health-related economic losses. The goal of this research is to develop a non-invasive wearable system (Personal Automatic Cigarette Tracker - PACT) that is completely transparent to the end user and does not require any conscious effort to achieve reliable monitoring of smoking behavior in free living individuals. The PACT device will provide an accurate and precise measure of real-world smoking. The device can provide the user and health professional feedback on the frequency of smoking and inhalation patterns (such as depth of inhalation and smoke holding) throughout the day in their home and community. This information can be used to inform behavioral strategies in smoking cessation programs. The data collected by PACT can also provide an objective method of assessing the effectiveness of behavioral and pharmacological smoking interventions.           7. Project Narrative Cigarette smoking is the leading cause of preventable death in the United States. Smoking produces over 440,000 deaths each year in this country and generates an estimated $167 billion in annual health-related economic losses. The goal of this research is to develop a non-invasive wearable system (Personal Automatic Cigarette Tracker - PACT) that is completely transparent to the end user and does not require any conscious effort to achieve reliable monitoring of smoking behavior in free living individuals. The PACT device will provide an accurate and precise measure of real-world smoking. The device can provide the user and health professional feedback on the frequency of smoking and inhalation patterns (such as depth of inhalation and smoke holding) throughout the day in their home and community. This information can be used to inform behavioral strategies in smoking cessation programs. The data collected by PACT can also provide an objective method of assessing the effectiveness of behavioral and pharmacological smoking interventions.",The Development of a Noninvasive Monitoring System for Cigarette Smoking,8044829,R21DA029222,"['Address', 'Algorithms', 'Applications Grants', 'Arts', 'Behavioral', 'Breathing', 'Burn injury', 'Cellular Phone', 'Cessation of life', 'Characteristics', 'Cigarette', 'Clothing', 'Collection', 'Communities', 'Computer software', 'Computers', 'Conscience', 'Conscious', 'Consumption', 'Country', 'Data', 'Data Set', 'Development', 'Devices', 'Eating', 'Economics', 'Effectiveness', 'Electronics', 'Environment', 'Event', 'Exhalation', 'Exposure to', 'Feedback', 'Frequencies', 'Future', 'Gestures', 'Goals', 'Hand', 'Health', 'Health Professional', 'Home environment', 'Hour', 'Individual', 'Instruction', 'Laboratories', 'Lead', 'Life', 'Machine Learning', 'Measures', 'Memory', 'Methodology', 'Methods', 'Metric', 'Monitor', 'Oral cavity', 'Patient Self-Report', 'Pattern', 'Pattern Recognition', 'Pattern Recognition Systems', 'Phase', 'Process', 'Reporting', 'Research', 'Signal Transduction', 'Smoke', 'Smoker', 'Smoking', 'Smoking Behavior', 'System', 'Testing', 'Time', 'Training', 'United States', 'United States Dept. of Health and Human Services', 'Validation', 'Variant', 'Walking', 'Work', 'base', 'cigarette smoking', 'cigarette smoking', 'computerized data processing', 'diaries', 'expiration', 'instrument', 'programs', 'public health relevance', 'respiratory', 'sensor', 'smoke inhalation', 'smoking cessation', 'smoking intervention']",NIDA,UNIVERSITY OF ALABAMA IN TUSCALOOSA,R21,2011,215353,-0.018524814667922163
"NLP for Augmentative and Alternative Communication in Adults    DESCRIPTION (provided by applicant):  This proposal relates to the technology of Augmentative and Alternative Communication (AAC).  The research, to be developed over the three-year course of this project, relates to increasing communication speed for adult users of typing-based AAC devices. The proposed method has commonalities both with chatter bots and more sophisticated automated question answering systems. In particular, we propose to develop a program that will mine a very large database of stored interactions for sentences that are similar to the sentence currently being uttered by the interlocutor, and propose a set of plausible responses for the AAC user. The outcome of this research will be a system that improves over the current state of the art in whole utterance approaches in AAC, making use of sophisticated natural language processing techniques.    Through this research and its practical application to helping real people with real communications needs, as well as coursework, seminars, participation in the AAC and disabilities community in Portland, OR, and intensive one-on-one meetings with his mentor Dr. Melanie Fried-Oken, the PI will accrue substantial clinical experience in AAC, and will gain a deep understanding of how technology can be used to help people.      PUBLIC HEALTH RELEVANCE: The proposed project will develop a research program in the field of Augmentative and Alternative Communication. The program proposes to improve the throughput of AAC devices for conversation by modeling dialogue context for literate adult users. Specifically, we will use corpus-based techniques from question answering and chatter bots to select appropriate utterances in response to utterances from interlocutors.              The proposed project will develop a research program in the field of Augmentative and Alternative Communication. The program proposes to improve the throughput of AAC devices for conversation by modeling dialogue context for literate adult users. Specifically, we will use corpus-based techniques from question answering and chatter bots to select appropriate utterances in response to utterances from interlocutors.            ",NLP for Augmentative and Alternative Communication in Adults,8189460,K25DC011308,"['Address', 'Adult', 'Area', 'Clinical', 'Communication', 'Communication Aids for Disabled', 'Communication Disability', 'Communities', 'Computers', 'Data', 'Databases', 'Devices', 'Environment', 'Food', 'Generations', 'Hobbies', 'Interview', 'Length', 'Measures', 'Mentors', 'Methods', 'Mining', 'Modeling', 'Modification', 'Names', 'Natural Language Processing', 'Oregon', 'Outcomes Research', 'Participant', 'Play', 'Questionnaires', 'Recruitment Activity', 'Relative (related person)', 'Research', 'Restaurants', 'Role', 'Savings', 'Self Assessment', 'Simulate', 'Source', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Travel', 'Universities', 'alternative communication', 'base', 'efficacy testing', 'experience', 'improved', 'literate', 'meetings', 'movie', 'novel', 'practical application', 'programs', 'response', 'satisfaction', 'speech recognition', 'usability']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,K25,2011,162459,0.004978623487096352
"Continuous Fall Risk Monitoring System: Walking vs Activities of Daily Living    DESCRIPTION (provided by applicant): Falls are the leading cause of injury-related visits to U.S. emergency departments and the primary etiology of accidental deaths in persons over the age of 65 years1. Interventions such as physical therapy, adjusting medications, or behavior changes can reduce the elderly fall rate2. Fall risk determination is needed to identify who may benefit from interventions. Changes in fall risk may occur suddenly or gradually, and are more likely to become apparent in the home environment as an individual goes about their normal activities of daily living (ADLs) rather than during a limited and periodic care provider assessment. Consequently, a continuous, all-in-one system that would monitor elderly individuals in the home for signs they are becoming more susceptible to falls is needed to reduce falls in the elderly. To be effective, the monitoring device must be non-intrusive and socially acceptable. The long-term goal of this project is to [extend an existing non-intrusive, commercially available monitoring system capable of location tracking, physiologic monitoring, and alerting to also assess fall risk in real time; the system would consider factors such as frequent bathroom use, fitful sleep and changes in gait characteristics]. The monitor for the proposed research has a watch form factor, is worn at the wrist, and has demonstrated high acceptance rates by elderly users. Research has shown that abnormal gait is indicative of fall risk, leading to the use of a variety of measurements of gait in fall risk determinations. To analyze gait from data gathered during ADLs, it is necessary to differentiate periods of walking, which can then be analyzed for abnormal characteristics. The purpose of this Phase I proposal is to test the [feasibility of using tri-axial acceleration data gathered from a commercially available wrist monitor to recognize periods of walking. Walking data can then be used in conjunction with other system data to] make inferences about changes in fall risk. Thirty elderly (aged 65 and over), ambulatory volunteers residing in an independent living facility will be recruited. The volunteers will be asked to engage in normal ADLs while being monitored over a 4-hour time period. During the study, volunteers will be videotaped, monitored using the wrist device, and monitored using a body-area sensor network technology developed at the University of Virginia. The specific aims of this project will be to: 1) determine if it is feasible to distinguish between periods of walking and other ADLs using the presence of frequencies generally associated with walking in wrist- gathered acceleration data as the differentiator, 2) determine whether individuals typically demonstrate a narrower range of walking frequencies than that suggested for the entire population, and 3) determine if it is feasible to use machine learning and time-series techniques to distinguish between periods of walking and other ADLs using characteristics learned from walking and non-walking data.      PUBLIC HEALTH RELEVANCE: Falls are the leading cause of injury-related visits to emergency departments in the United States and the primary cause of accidental deaths in persons over age 651. In order to reduce the number of falls, a continuous, non-intrusive, convenient monitoring system in an acceptable form factor which will monitor elderly individuals in their home environment for signs they are becoming more susceptible to falls needs to be developed. The long-term goal of this project is to develop such a system; the proposed project would take the necessary step of identifying and differentiating between walking and other normal activities [so that data gathered during walking can be used to recognize changes in stability for an individual, and then can be used in conjunction with other system data already being automatically collected in real-time to recognize an increased probability of falling both during walking or other activities.]           Falls are the leading cause of injury-related visits to emergency departments in the United States and the primary cause of accidental deaths in persons over age 651. In order to reduce the number of falls, a continuous, non-intrusive, convenient monitoring system in an acceptable form factor which will monitor elderly individuals in their home environment for signs they are becoming more susceptible to falls needs to be developed. The long-term goal of this project is to develop such a system; the proposed project would take the necessary step of identifying and differentiating between walking and other normal activities [so that data gathered during walking can be used to recognize changes in stability for an individual, and then can be used in conjunction with other system data already being automatically collected in real-time to recognize an increased probability of falling both during walking or other activities.]         ",Continuous Fall Risk Monitoring System: Walking vs Activities of Daily Living,8199136,R43AG039176,"['Acceleration', 'Accident and Emergency department', 'Activities of Daily Living', 'Adherence', 'Admission activity', 'Age', 'Area', 'Assisted Living Facilities', 'Caring', 'Cessation of life', 'Characteristics', 'Climacteric', 'Data', 'Devices', 'Early treatment', 'Elderly', 'Environment', 'Equilibrium', 'Etiology', 'Evaluation', 'Fall prevention', 'Frequencies', 'Gait', 'Gait abnormality', 'Goals', 'Health Care Costs', 'Home environment', 'Hospitals', 'Hour', 'Independent Living', 'Individual', 'Information Systems', 'Injury', 'Intervention', 'Laboratories', 'Learning', 'Leg', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Medical History', 'Methods', 'Middle Insomnia', 'Monitor', 'Periodicity', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Physical therapy', 'Physiologic Monitoring', 'Population', 'Probability', 'Provider', 'Quality of life', 'Receiver Operating Characteristics', 'Recording of previous events', 'Recruitment Activity', 'Research', 'Series', 'Specificity', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trauma', 'United States', 'Universities', 'Video Recording', 'Videotape', 'Virginia', 'Vision', 'Visit', 'Walking', 'Wrist', 'base', 'behavior change', 'fall risk', 'falls', 'improved', 'monitoring device', 'phase 1 study', 'sensor', 'volunteer']",NIA,"AFRAME DIGITAL, INC.",R43,2011,147264,-0.012328448286168464
"Micro-environment Glasses as a Treatment for CVS Computer Vision Syndrome (CVS) refers to a collection of eye problems associated with computer use, and  about three-quarters of computer users have it. Conservative estimates indicate that over $2 billion is  currently spent on examinations and special eyewear for CVS treatment. The most common symptoms of  CVS include: eyestrain or eye fatigue, dry eyes, burning eyes, sensitivity to light, and blurred vision. Non-  ocular symptoms include headaches, pain in the shoulders, neck, or back. As diverse as the symptoms are,  they may be related and can be subdivided into to three potential pathophysiological causes:   1) Ocular Surface Mechanisms  2) Accommodative Mechanisms  3) Extra-Ocular Mechanisms  There is a significant gap in the fund of knowledge regarding the diagnosis of this disease. In the near-term,  we plan to focus on the ocular surface category of disorders as a cause of CVS, identify clinical conditions  associated with this syndrome and develop a treatment that addresses this cause. In phase t, we propose to:  ¿Clinically define CVS by observing the incidence of ocular surface abnormalities in symptomatic subjects  and compare them with an age and sex matched non-symptomatic control population  ¿Develop specialized micro-environment glasses to combat CVS symptoms  ¿Study the efficacy of micro-environment glasses in symptomatic and control populations  ¿Critically evaluate viability of CVS micro-environment glasses as a commercial product using both statistical   methods and subjective questionnaires n/a",Micro-environment Glasses as a Treatment for CVS,8203808,R41EY015023,"['Address', 'Age', 'Asthenopia', 'Back', 'Blurred vision', 'Categories', 'Clinical', 'Collection', 'Computer Vision Systems', 'Computers', 'Devices', 'Disease', 'Environment', 'Eye', 'Eye Burns', 'Funding', 'Glass', 'Headache', 'Incidence', 'Knowledge', 'Light', 'Neck', 'Pain', 'Phase', 'Population Control', 'Process', 'Questionnaires', 'Shoulder', 'Statistical Methods', 'Symptoms', 'Syndrome', 'combat', 'disease diagnosis', 'effective therapy', 'eye dryness', 'improved', 'ocular surface', 'sex']",NEI,"SEEFIT, INC.",R41,2011,47724,-0.01608650134067434
"Vision Without Sight: Exploring the Environment with a Portable Camera    DESCRIPTION (provided by applicant): As computer vision object recognition algorithms improve in accuracy and speed, and computers become more powerful and compact, it is becoming increasingly practical to implement such algorithms on portable devices such as camera-enabled cell phones. This ""mobile vision"" approach allows normally sighted users to identify objects, signs, places and other features in the environment simply by snapping a photo and waiting a few seconds for the results of the object recognition analysis. The approach holds great promise for blind or visually impaired (VI) users, who may have no other means of identifying important features that are undetectable by non-visual cues. However, in order for the approach to be practical for VI users, the interaction between the user and the environment using the camera must be properly facilitated. For instance, since the user may not know in advance which direction to point the camera towards a desired target, he or she must be able to pan the camera left and right to search for it, and receive rapid feedback whenever it is detected. Drawing on past experience of the PI and his collaborators on object recognition systems for VI users, we propose to study the use of mobile vision technologies for exploring features in the environment, specific examining the process of discovering these features and obtaining guidance towards them. Our main objectives are to investigate the strategies adopted by users of these technologies to expedite the exploration process, devise and test maximally effective user interfaces consistent with these strategies, and to assess and benchmark the efficiency of the technologies. The result will be a set of minimum design standards that will specify the system performance parameters, the user interface functionality and the operational strategies necessary for any mobile vision object recognition system for VI users.      PUBLIC HEALTH RELEVANCE: The ability to locate and identify objects, places and other features in the environment is taken for granted every day by the sighted, but this fundamental capability is missing or severely degraded in the approximately 10 million Americans with significant vision impairments and a million who are legally blind. The proposed research would investigate how computer vision object recognition technologies, which are now being implemented on mobile vision devices such as cell phones but are typically designed for normally sighted users, could be modified and harnessed to meet the special needs of blind and visually impaired persons. Such research could lead to new technologies to dramatically improve independence for this population              The ability to locate and identify objects, places and other features in the environment is taken for granted every day by the sighted, but this fundamental capability is missing or severely degraded in the approximately 10 million Americans with significant vision impairments and a million who are legally blind. The proposed research would investigate how computer vision object recognition technologies, which are now being implemented on mobile vision devices such as cell phones but are typically designed for normally sighted users, could be modified and harnessed to meet the special needs of blind and visually impaired persons. Such research could lead to new technologies to dramatically improve independence for this population            ",Vision Without Sight: Exploring the Environment with a Portable Camera,8097202,R21EY021643,"['Address', 'Adopted', 'Algorithms', 'American', 'Benchmarking', 'Cellular Phone', 'Computer Hardware', 'Computer Vision Systems', 'Computers', 'Cues', 'Development', 'Devices', 'Environment', 'Feedback', 'Glosso-Sterandryl', 'Goals', 'Goggles', 'Grant', 'Image Analysis', 'Impairment', 'Lead', 'Learning', 'Left', 'Location', 'Performance', 'Population', 'Printing', 'Process', 'Research', 'Self-Help Devices', 'Specific qualifier value', 'Speed', 'System', 'Task Performances', 'Technology', 'Testing', 'Time', 'Touch sensation', 'Training', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'blind', 'design', 'experience', 'improved', 'insight', 'interest', 'legally blind', 'meetings', 'new technology', 'object recognition', 'operation', 'usability', 'visual feedback']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2011,205070,0.06398953339288471
"Longitudinal Assessment of Fall Risk    DESCRIPTION (provided by applicant): Falling is not a normal part of the aging process and yet 1/3 to 1/2 of adults 65 years and older sustain at least one fall annually. Older adults are hospitalized for fall related injuries five times more often than from injuries from other causes contributing to a cost of $19 billion for nonfatal falls in the United States. Projected for the increasing aging population in the year 2020, it is expected that the costs related to falls will reach a staggering 54.9 billion dollars. Current research and clinical practice guidelines focus on multifactorial fall risk assessments as the critical deterrent to falls in the elderly. A primary factor within these assessments is activity of daily living performance of the individual elder. While current standardized clinical balance assessment tools have been proven effective for predicting fall risk, the tests are most commonly performed in the clinical environment and at isolated times during an individual's day. The goal of this application is to develop and validate a novel wearable device (Automatic Longitudinal Assessment Risk Monitor - ALARM) for longitudinal assessment of risk of falling. Such a device: - will allow early detection of risk of falling, when therapeutic interventions are most efficient - will provide real-time feedback about activity pattern - will provide feedback about compliance with interventions and effectiveness of interventions - will be incorporated into conventional footwear and require no extra effort to operate - can be used in research, clinical and potentially in consumer applications The development of the ALARM system will be addressed in three specific aims:  Specific Aims 1: Develop a pattern recognition method that will improve recognition accuracy for activities of interest (such as walking and stepping up) by reducing the range of variation from current 76%- 100% to 9911%. Specific Aim 2: Collect data using the ALARM device on a group of elderly adults during clinical tests. Specific Aim 3: Develop algorithms for automatic assessment of risk of falling. In this Aim we will develop signal processing algorithms that automatically evaluate metrics indicative of the risk of falling in each activity of interest (e.g. duration of swing and stance phase during walking). Specific Aim 4: Validate the ALARM device in a double-blind unrestricted free living study. This set of Specific Aims will validate lead to creation of a unique wearable device capable of objective characterization of risk of falling.      PUBLIC HEALTH RELEVANCE: This application aims at development of a novel wearable device (Automatic Longitudinal Assessment Risk Monitor - ALARM) for longitudinal assessment of risk of falling. In our previous research we have shown that major activities and posture allocations such as standing, sitting, walking, etc. can be recognized with high degree of accuracy (76%-100%) by a wearable device incorporated into conventional footwear. We also have shown that sensor signals captured by the wearable shoe device during activities such as walking are well- correlated with the risk of falling (with numerical estimates of risk obtained through signal processing being directly proportional to the normalized scores from the clinical tests). The goal of this application is to develop and validate a novel wearable device (ALARM) for longitudinal assessment of risk of falling.           This application aims at development of a novel wearable device (Automatic Longitudinal Assessment Risk Monitor - ALARM) for longitudinal assessment of risk of falling. In our previous research we have shown that major activities and posture allocations such as standing, sitting, walking, etc. can be recognized with high degree of accuracy (76%-100%) by a wearable device incorporated into conventional footwear. We also have shown that sensor signals captured by the wearable shoe device during activities such as walking are well- correlated with the risk of falling (with numerical estimates of risk obtained through signal processing being directly proportional to the normalized scores from the clinical tests). The goal of this application is to develop and validate a novel wearable device (ALARM) for longitudinal assessment of risk of falling.         ",Longitudinal Assessment of Fall Risk,8240357,R21EB013183,"['Acceleration', 'Activities of Daily Living', 'Address', 'Adult', 'Aging-Related Process', 'Algorithms', 'Classification', 'Clinical', 'Clinical Practice Guideline', 'Communities', 'Comparative Study', 'Computational algorithm', 'Computers', 'Data', 'Data Set', 'Development', 'Devices', 'Double-Blind Method', 'Early Diagnosis', 'Effectiveness of Interventions', 'Elderly', 'Engineering', 'Environment', 'Equilibrium', 'Evaluation', 'Feasibility Studies', 'Feedback', 'Goals', 'Heel', 'Individual', 'Injury', 'Intervention', 'Laboratories', 'Lead', 'Life', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Metric', 'Monitor', 'Neural Network Simulation', 'Outcome', 'Pattern', 'Pattern Recognition', 'Performance', 'Phase', 'Posture', 'Process', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Risk', 'Risk Assessment', 'Risk Estimate', 'Series', 'Shoes', 'Signal Transduction', 'System', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'United States', 'Validation', 'Variant', 'Walking', 'aging population', 'base', 'computerized data processing', 'cost', 'fall risk', 'falls', 'human old age (65+)', 'improved', 'interest', 'novel', 'pressure', 'sensor', 'tool', 'volunteer']",NIBIB,UNIVERSITY OF ALABAMA IN TUSCALOOSA,R21,2011,228569,-0.005174740519616488
"Perception of Tactile Graphics    DESCRIPTION (provided by applicant): The broad objective of the proposed research is to answer the following question: why are tactile graphics difficult to understand? People with normal vision can easily recognize line drawings of objects. However, both blind and sighted people find it very difficult to recognize the same drawings when they are presented as tactile images. For blind people, tactile graphics are the only solution for accessing information in visual diagrams and illustrations found in textbooks. Consequently, the results of the proposed research will be used to improve the production of tactile graphics so that they are better understood by blind people. The specific aims of this project are to: 1) explore how the complexity of tactile images affects perception, 2) determine the effects of spatial and temporal integration on perception of tactile images, and 3) investigate how well people can recognize tactile images of objects embedded in backgrounds. The general methodology of the proposed experiments is to present participants with tactile images and to have them draw what they perceive the images to be. Blind individuals will draw tactile images using special paper and a stylus. The experimenters will evaluate the drawings by using a quantitative measure that computes a distance score reflecting the discrepancy between the original image and the participant's drawing. In the first study, participants will feel tactile stimuli of varying complexity, from simple lines in different orientations to complex depictions of objects. The second study will determine the limitations of tactile perceptual integration by limiting either the spatial or temporal window over which participants feel the image. Participants will either view or feel images through apertures of various sizes (spatial window) or they will have a limited amount of time to view or feel the images (temporal window). In the third study, participants will feel tactile images of objects embedded in simple backgrounds. This research will impact several areas of study, including computer vision, human object and scene recognition, and low vision rehabilitation.      PUBLIC HEALTH RELEVANCE: Relevance The proposed research seeks to improve the translation of visual graphics into tactile graphics for use by visually-impaired individuals. By making the information in visual graphics more accessible, this research will improve educational services for people with low vision. This work will also facilitate the development of better visual-to-tactile substitution technologies.              Relevance The proposed research seeks to improve the translation of visual graphics into tactile graphics for use by visually-impaired individuals. By making the information in visual graphics more accessible, this research will improve educational services for people with low vision. This work will also facilitate the development of better visual-to-tactile substitution technologies.            ",Perception of Tactile Graphics,8060259,F32EY019622,"['Affect', 'Area', 'Categories', 'Child', 'Complex', 'Computer Vision Systems', 'Development', 'Devices', 'Disadvantaged', 'Education', 'Elements', 'Goals', 'Grouping', 'Human', 'Image', 'India', 'Individual', 'Link', 'Measures', 'Methodology', 'Methods', 'Names', 'Nature', 'Paper', 'Participant', 'Perception', 'Population', 'Production', 'Psychophysics', 'Rehabilitation therapy', 'Research', 'Science', 'Sensory', 'Services', 'Shapes', 'Solutions', 'Stimulus', 'Swelling', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Time', 'Touch sensation', 'Translating', 'Translations', 'Vision', 'Visual', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'blind', 'braille', 'improved', 'object recognition', 'research study', 'sight for the blind', 'skills', 'tactile vision substitution system', 'two-dimensional', 'vision development', 'visual process', 'visual processing']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,F32,2011,48398,0.0048055188056710385
"A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians    DESCRIPTION (provided by applicant): A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians Abstract Project Summary Urban intersections are among the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard cell phone, to provide real-time feedback. Building on our past work on a prototype ""Crosswatch"" system that uses computer vision algorithms to find crosswalks and Walk lights, we will greatly enhance the functionality of the system with information about the intersection layout and the identity of its connecting streets, the presence of stop signs, one-way signs and other controls indicating right-of-way, and timing information integrated from Walk/Don't Walk lights, countdown timers and other traffic lights. The system will convey intersection information, and will actively guide the user to align himself/herself with crosswalks, using a combination of synthesized speech and audio tones. We will conduct human factors studies to optimize the system functionality and the configuration of the user interface, as well as develop interactive training applications to equip users with the skills to better use the system. These training applications, implemented as additional cell phone software to complement the intersection system, will train users to hold the camera horizontal and forward and to minimize veer when traversing a crosswalk. The intersection analysis and training software will be made freely available for download onto popular cell phones (such as iPhone, Android or Symbian models). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function.      PUBLIC HEALTH RELEVANCE: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.              The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.            ",A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians,8042468,R01EY018345,"['Address', 'Adoption', 'Algorithms', 'American', 'Cellular Phone', 'Complement', 'Complement component C4', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Crowding', 'Custom', 'Data', 'Development', 'Devices', 'Equipment', 'Face', 'Feedback', 'Glosso-Sterandryl', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Length', 'Light', 'Location', 'Mainstreaming', 'Modeling', 'Modification', 'Names', 'Process', 'Reading', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Running', 'Safety', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Speed', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'abstracting', 'base', 'blind', 'consumer product', 'cost', 'design', 'improved', 'interest', 'legally blind', 'meter', 'prototype', 'sensor', 'skills', 'trafficking', 'volunteer', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2011,403177,0.0008857749539237298
"Handsight: Mobile Services for Low Vision    DESCRIPTION (provided by applicant): Handsight is a mobile phone service that offers an affordable, extensible set of automated sight-assistant functions to millions of blind and low-vision persons at $30/month atop standard voice and data fees. To the user, Handsight is simply a mobile phone application, requiring no special equipment or updating. By aiming a mobile telephone<s camera in roughly the right direction and pressing just one button, a Handsight user can snap a picture, send it to the Handsight computer center along with location data, and have a response within seconds. Moving the key computation and data from the handset to web servers cut cost, eases technology upgrades, and enables numerous data and technology partnerships needed to rapidly solve a broad spectrum of tasks. To allow widespread adoption Handsight uses voice, keypad, and vibration for user interaction; and for extensibility it is built on open-source software both on the handset and on the web application infrastructure. The range of tasks encountered by the blind and low-vision requires an array of components to solve: some are more heavily text-oriented and involve no location/navigational feedback (distinguishing and identifying different medicines; finding the phone bill in a stack of letters); some are more specifically navigational (locating the exact store entrance, finding the bus stop); yet others are informational (finding out when the next bus is due). Since we aim to provide a broadly useful tool, Handsight has to accommodate the whole range of task types. We are therefore proposing to build an architecture that integrates a set of components addressing the various task types, from text-detection and recognition software to navigational databases. Handsight<s application programming interface (API) will enable third parties to add capabilities to solve new tasks. As a web service, Handsight can evolve as computer vision and navigation technologies advance without requiring users to upgrade handsets or buy new versions of software. Phase I of this project was funded by the Dept. of Education / NIDRR and completed successfully between 10/01/2007 and 04/01/2008 under grant # H133S070044. It demonstrated not just the viability of the proposed project but also likely demand for such a service. (The NIDRR<s immutable deadlines precluded us from pursuing a Phase II with that agency.) Phase II consists in building the cell phone application and remote processing infrastructure with APIs to enable plug-in of the basic and future features. Subcontractor Smith-Kettlewell Institute for Eye Research will assure usability by blind and low-vision users; data partner NAVTEQ will provide a range of leading-edge digital map data. Blindsight already owns fast, accurate text detection and recognition software, and has experience in building scalable web services. A minimum set of features that make for a system with widespread appeal will be developed and/or licensed, and integrated into the service.      PUBLIC HEALTH RELEVANCE: The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today<s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, and shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.        Re: Project Title: Handsight: Mobile Services for Low Vision  FOA: PHS 2009-02 Omnibus Solicitation of the NIH, CDC, FDA and ACF for Small Business  Innovation Research Grant Applications (Parent SBIR [R43/R44])  Proposed project period: April 1, 2010 - March 31, 2012  Principal Investigator: Hallinan, Peter W. SF424 Research and Related Other Project Information: Project Narrative The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today�s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.",Handsight: Mobile Services for Low Vision,8133823,R44EY020707,"['Activities of Daily Living', 'Address', 'Adoption', 'Architecture', 'Area', 'Car Phone', 'Cellular Phone', 'Centers for Disease Control and Prevention (U.S.)', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Dependence', 'Destinations', 'Detection', 'Devices', 'Education', 'Eye', 'Feedback', 'Fees', 'Focus Groups', 'Friends', 'Funding', 'Future', 'Grant', 'Health', 'Human', 'Individual', 'Institutes', 'Internet', 'Letters', 'Licensing', 'Location', 'Maps', 'Medicine', 'Mission', 'Parents', 'Persons', 'Phase', 'Plug-in', 'Principal Investigator', 'Process', 'Provider', 'Reading', 'Research', 'Research Design', 'Research Infrastructure', 'Research Institute', 'Research Methodology', 'Research Support', 'Services', 'Simulate', 'Small Business Innovation Research Grant', 'Social Interaction', 'Special Equipment', 'System', 'Target Populations', 'Technology', 'Telephone', 'Text', 'Touch sensation', 'Training', 'Travel', 'United States National Institutes of Health', 'Update', 'Vision', 'Vision Disorders', 'Visual impairment', 'Voice', 'Work', 'blind', 'computer center', 'cost', 'digital', 'experience', 'falls', 'open source', 'programs', 'public health relevance', 'response', 'success', 'tool', 'usability', 'vibration', 'voice recognition', 'web services']",NEI,BLINDSIGHT CORPORATION,R44,2011,776548,0.039068733573841156
"Low-Cost Electronic Nose for Groundwater Contaminants    DESCRIPTION (provided by applicant): Several US agencies and regulators require low-cost chemical sensors for detecting and monitoring environmental clean-up, remediation, and decommissioning processes where groundwater may be contaminated. The sensors must be capable of detecting contaminants in the sub-surface groundwater and must be compatible with use in a range of environments. Most significantly, these customers require a low-cost alternative to its current expensive and labor intensive methods, namely using mobile laboratories. The project will result in the innovative use of low-cost sensor systems that will be capable of detecting and monitoring for dense non-aqueous phase liquids in the subsurface and groundwater, unattended, and in real- time from within a push-probe, using a chemicapacitor array and miniature preconcentrator.  The ultimate goal of this SBIR project is to provide the DOD, DOE, and other agencies with a method to map and track subsurface contamination plumes in real-time without requiring an operator. In Phase I, Seacoast successfully demonstrated the feasibility of using a microsensor array with a proprietary trap-and- purge preconcentrator to detect chlorinated solvents, specifically TCE, and TCA, at levels low enough to meet EPA mandated levels for drinking water. In Phase II Seacoast proposes to improve the selectivity and sensitivity of the system to better meet the needs identified by the Phase I consultant. The systems have MEMS microcapacitor sensor arrays that can monitor for leaks of toxic chemicals, contaminants from wastes, and changes in groundwater streams. A preconcentrator traps the contaminants and releases them to a microsensor array. These sensor arrays are filled with several chemoselective polymers whose dielectric permittivity changes when exposed to different vapors, creating a fingerprint response for each chemical.  In Phase II Seacoast will specifically develop new materials to improve the sensor array selectivity, 1) by using impedance spectroscopy to study the mechanisms by which the polymer-based sensors sorb the target chemicals, 2) by implementing pattern recognition algorithms to identify chemicals for the sensor responses, and 3) by designing new preconcentrator materials that can bind these chemicals more strongly.  The most important application to public health and safety is unattended monitoring of drinking water, water treatment processes, and water sources. Potential markets include building chemical process monitoring and control, toxic vapor leak detection, industrial process control, and industrial health and safety. Transitioning the developed prototype to other markets where worker and public health, environmental health and regulatory compliance will be investigated to reduce the financial risks and broaden the acceptance of the technology.      PUBLIC HEALTH RELEVANCE: This proposal describes a novel technology that specifically addresses the need for detecting groundwater contaminants and long-term monitoring of contaminated sites, by providing an unattended sensor system that tracks contamination in real-time and transmits contaminant concentrations. Such a system would be used in tandem with other methods, to provide comprehensive contamination management at DOE, DOD, and Superfund sites where ground and water clean-up projects are already underway. The proposed work will focus on detection of chlorinated hydrocarbons, which are described as among the most common pollutants in groundwater and soils at DOE sites.           This proposal describes a novel technology that specifically addresses the need for detecting groundwater contaminants and long-term monitoring of contaminated sites, by providing an unattended sensor system that tracks contamination in real-time and transmits contaminant concentrations. Such a system would be used in tandem with other methods, to provide comprehensive contamination management at DOE, DOD, and Superfund sites where ground and water clean-up projects are already underway. The proposed work will focus on detection of chlorinated hydrocarbons, which are described as among the most common pollutants in groundwater and soils at DOE sites.         ",Low-Cost Electronic Nose for Groundwater Contaminants,8059710,R44ES016941,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Benchmarking', 'Benzene', 'Carcinogens', 'Chemicals', 'Chlorinated Hydrocarbons', 'Classification', 'Collection', 'Computer software', 'Cost Analysis', 'Cost Savings', 'Data', 'Data Collection', 'Detection', 'Development', 'Electronics', 'Engineering', 'Environment', 'Environmental Health', 'Environmental Monitoring', 'Equation', 'Equipment', 'Evaluation', 'Fingerprint', 'Fluorescence', 'Gases', 'Goals', 'Guidelines', 'Hazardous Waste', 'Industrial Health', 'Intervention', 'Laboratories', 'Lasers', 'Left', 'Liquid substance', 'Maps', 'Marketing', 'Measures', 'Metals', 'Methods', 'Monitor', 'National Institute of Environmental Health Sciences', 'Nose', 'Pattern Recognition', 'Pesticides', 'Phase', 'Plants', 'Poison', 'Pollution', 'Polymers', 'Process', 'Public Health', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Route', 'Safety', 'Sampling', 'Science', 'Scientific Advances and Accomplishments', 'Site', 'Small Business Innovation Research Grant', 'Soil', 'Solvents', 'Source', 'Spectrum Analysis', 'Stream', 'Surface', 'System', 'Technology', 'Temperature', 'Time', 'Trichloroethylene', 'Water', 'Wireless Technology', 'Work', 'analytical method', 'base', 'chemical binding', 'cost', 'design', 'detector', 'drinking water', 'electric impedance', 'ground water', 'improved', 'innovation', 'instrument', 'knowledge base', 'meetings', 'method development', 'new technology', 'novel', 'operation', 'pollutant', 'programs', 'prototype', 'purge', 'remediation', 'response', 'sensor', 'success', 'superfund site', 'vapor', 'wasting', 'water treatment']",NIEHS,"SEACOAST SCIENCE, INC.",R44,2011,532144,0.025433078455525216
"Computational Methods for Analysis of Mouth Shapes in Sign Languages    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hand) and by the nonmanual components (the face). These facial articulations perform significant semantic, prosodic, pragmatic, and syntactic functions. This proposal will systematically study mouth positions in ASL. Our hypothesis is that ASL mouth positions are more extensive than those used in speech. To study this hypothesis, this project is divided into three aims. In our first aim, we hypothesize that mouth positions are fundamental for the understanding of signs produced in context because they are very distinct from signs seen in isolation. To study this we have recently collected a database of ASL sentences and nonmanuals in over 3600 video clips from 20 Deaf native signers. Our experiments will use this database to identify potential mappings from visual to linguistic features. To successfully do this, our second aim is to design a set of shape analysis and discriminant analysis algorithms that can efficiently analyze the large number of frames in these video clips. The goal is to define a linguistically useful model, i.e., the smallest model that contains the main visual features from which further predictions can be made. Then, in our third aim, we will explore the hypothesis that the linguistically distinct mouth positions are also visually distinct. In particular, we will use the algorithms defined in the second aim to determine if distinct visual features are used to define different linguistic categories. This result will show whether linguistically meaningful mouth positions are not only necessary in ASL (as hypothesized in aim 1), but whether they are defined using non-overlapping visual features (as hypothesized in aim 3). These aims address a critical need. At present, the study of nonmanuals must be carried out manually, that is, the shape and position of each facial feature in each frame must be recorded by hand. Furthermore, to be able to draw conclusive results for the design of a linguistic model, it is necessary to study many video sequences of related sentences as produced by different signers. It has thus proven nearly impossible to continue this research manually. The algorithms designed in the course of this grant will facilitate this analysis of ASL nonmanuals and lead to better teaching materials.      PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.           Project Narrative Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for Analysis of Mouth Shapes in Sign Languages,8109271,R21DC011081,"['Academic achievement', 'Access to Information', 'Address', 'Adult', 'Algorithms', 'Applications Grants', 'Categories', 'Child', 'Clip', 'Communication', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Databases', 'Devices', 'Discriminant Analysis', 'Educational process of instructing', 'Emotions', 'Excision', 'Eye', 'Face', 'Funding', 'Goals', 'Grant', 'Hand', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Joints', 'Knowledge', 'Language', 'Lead', 'Learning', 'Life', 'Linguistics', 'Manuals', 'Modeling', 'Oral cavity', 'Parents', 'Pattern Recognition', 'Positioning Attribute', 'Process', 'Regulation', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Scientist', 'Semantics', 'Shapes', 'Sign Language', 'Social Interaction', 'Specific qualifier value', 'Speech', 'Teaching Materials', 'Technology', 'Testing', 'Training', 'United States National Institutes of Health', 'Visual', 'Work', 'computerized tools', 'deafness', 'design', 'experience', 'innovation', 'instructor', 'interest', 'novel', 'prevent', 'public health relevance', 'research study', 'shape analysis', 'success', 'syntax', 'teacher', 'tool', 'visual map']",NIDCD,OHIO STATE UNIVERSITY,R21,2011,205267,-0.019178821735113982
"Designing Visually Accessible Spaces    DESCRIPTION (provided by applicant): Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our long-term goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals and to enhance safety for the elderly and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool in which environments (such as a hotel lobby, large classroom, or hospital reception area), could be simulated with sufficient accuracy to predict the visibility of key landmarks or obstacles, such as steps or benches, under differing lighting conditions. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments"". Our research plan has four specific goals: 1) Develop methods for predicting the physical levels of light reaching the eye in existing or planned architectural spaces. 2) Acquire performance data for normally sighted subjects with visual restrictions and people with low vision to investigate perceptual capabilities critical to visually-based mobility. 3) Develop models that can predict perceptual competence on tasks critical to visually-based mobility. 4) Demonstrate a proof-of-concept software tool that operates on design models from existing architectural design systems and is able to highlight potential obstacles to visual accessibility. The lead investigators in our partnership come from three institutions: University of Minnesota Gordon Legge, Daniel Kersten; University of Utah William Thompson, Peter Shirley, Sarah Creem-Regehr; and Indiana University Robert Shakespeare. This interdisciplinary team has expertise in the four areas required for programmatic research on visual accessibility empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), photometrically correct computer graphics (Shirley), and architectural design and lighting (Shakespeare).           n/a",Designing Visually Accessible Spaces,8037680,R01EY017835,"['Accounting', 'Address', 'American', 'Area', 'Blindness', 'Central Scotomas', 'Characteristics', 'Classification', 'Competence', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Data', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Elderly', 'Engineering', 'Environment', 'Evaluation', 'Eye', 'Goals', 'Hospitals', 'Indiana', 'Individual', 'Institution', 'Lead', 'Light', 'Lighting', 'Lobbying', 'Location', 'Measurement', 'Methods', 'Minnesota', 'Modeling', 'National Eye Institute', 'Pattern', 'Perception', 'Performance', 'Peripheral', 'Photometry', 'Published Comment', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Priority', 'Safety', 'Simulate', 'Software Tools', 'Space Perception', 'Specialist', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Testing', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual impairment', 'base', 'design', 'innovation', 'knowledge base', 'luminance', 'model design', 'model development', 'physical model', 'predictive modeling', 'programs', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2011,513228,0.05312631660490497
"Accessible Artificial Intelligence Tutoring Software for Mathematics    DESCRIPTION (provided by applicant): This Fast-Track application focuses on developing the first artificial intelligence (AI) educational software to teach developmental mathematics to the blind and visually impaired. This project responds to the National Eye Institute's General Research Topics for ""teaching tools"" and the Visual Impairment and Blindness Program for ""other devices that meet the rehabilitative and everyday living needs of persons who are blind or have low vision."" The intervention being developed will place a comprehensive set of AI mathematics tutoring systems with integrated AI assessment capabilities in the hands of the blind K-12, college and adult student, for use on demand during study at home and at school. The formulation of an advanced AI tutoring methodology with accessibility inherent to its design will have broad implications for development in many subject areas beyond mathematics. Project objectives include: Horizontal Expansion of Accessible Curriculum Content Coverage (Ratio and Proportion, Percentages, Linear Equations, Metric Units, Scientific Notation) 1) Conduct initial accessibility review and analysis of AI tutor's existing user interface. 2) Implement accessibility requirements and recommendations from NFB, instructors and other partners. 3) Conduct final review to gain NFB accessibility certification after implementation of requirements. 4) Develop and issue survey of instructors on mathematics pedagogy and technology. Vertical Expansion of Accessible Features and Technological Capability 5) Implement Braille support in AI technology. 6) Develop additional AI tutor on Fractions that is automatically accessible from first principles using accessible AI framework. Evaluation of Accessible AI Educational Technology 7) Field evaluation of accessible AI technology with blind students and their instructors. 8) Continued demonstration and review of accessible AI technology by partners and other stakeholders. Preparation for success in Phase III has already been undertaken by involving partners that are important commercially as well as technically, such as the National Federation of the Blind and the American Printing House for the Blind (APH). In addition, Quantum already has long-term partnerships established with McGraw- Hill and Holt, Rinehart and Winston, two of the country's leading educational publishers, as well as a major science education catalog company, CyberEd, Inc., a PLATO Learning Company. PUBLIC HEALTH RELEVANCE: There is a considerable need for improved educational software for mathematics in general, but the problem of quality educational software materials for the blind and visually impaired is particularly acute. A weak mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of science, technology, engineering and mathematics. Through previous federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI) tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power and benefit of this cutting-edge educational technology to students who are blind and visually impaired.           PROJECT NARRATIVE: There is a considerable need for improved educational software for mathematics in general, but the problem of quality educational software materials for the blind and visually impaired is particularly acute. A weak mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of science, technology, engineering and mathematics. Through previous federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI) tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power and benefit of this cutting-edge educational technology to students who are blind and visually impaired.",Accessible Artificial Intelligence Tutoring Software for Mathematics,8043275,R44EY019414,"['Activities of Daily Living', 'Acute', 'Address', 'Adult', 'American', 'Area', 'Artificial Intelligence', 'Blindness', 'Businesses', 'Cataloging', 'Catalogs', 'Categories', 'Certification', 'Chemicals', 'Chemistry', 'Collaborations', 'Computer software', 'Country', 'Development', 'Development Plans', 'Devices', 'Dimensions', 'Drug Formulations', 'Dyslexia', 'Education', 'Educational Curriculum', 'Educational Technology', 'Educational process of instructing', 'Elements', 'Engineering', 'Ensure', 'Equation', 'Equilibrium', 'Evaluation', 'Feedback', 'Future', 'Goals', 'Hand', 'Home environment', 'Housing', 'Individual', 'Institution', 'Instruction', 'Internet', 'Intervention', 'Language', 'Learning', 'Letters', 'Life', 'Mathematics', 'Measures', 'Mediation', 'Methodology', 'Metric', 'Mission', 'Modeling', 'National Eye Institute', 'Nature', 'Outcome', 'Performance', 'Persons', 'Phase', 'Philosophy', 'Play', 'Preparation', 'Printing', 'Process', 'Publishing', 'Reader', 'Reading Disabilities', 'Recommendation', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Research Support', 'Role', 'Schools', 'Science', 'Small Business Innovation Research Grant', 'Software Tools', 'Students', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Visual impairment', 'Work', 'blind', 'braille', 'career', 'college', 'commercial application', 'commercialization', 'design', 'disability', 'high school', 'improved', 'innovation', 'innovative technologies', 'instructor', 'meetings', 'middle school', 'programs', 'prospective', 'prototype', 'public health relevance', 'quantum', 'quantum chemistry', 'remediation', 'research and development', 'science education', 'simulation', 'stem', 'success', 'teacher', 'technological innovation', 'tool']",NEI,"QUANTUM SIMULATIONS, INC.",R44,2010,394165,0.013723456480975671
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7904837,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,1196495,0.045889024392851814
"Cue Reliability and Depth Calibration During Space Perception    DESCRIPTION (provided by applicant): The long-term objective of the proposed work is to understand how learning by the visual system helps it to represent the immediate environment during perception. Because perception is accurate, we can know spatial layout: the shapes, orientations, sizes, and spatial locations of the objects and surfaces around us. But this accuracy requires that the visual system learn over time how best to interpret visual ""cues"". These cues are the signals from the environment that the visual system extracts from the retinal images that are informative about spatial layout. Known cues include binocular disparity, texture gradients, occlusion relations, motion parallax, and familiar size, to name a few. How do these cues come to be interpreted correctly? A fundamental problem is that visual cues are ambiguous. Even if cues could be measured exactly (which they cannot, the visual system being a physical device) there would still be different possible 3D interpretations for a given set of cues. As a result, the visual system is forced to operate probabilistically: the way things ""look"" to us reflects an implicit guess as to which interpretation of the cues is most likely to be correct. Each additional cue helps improve the guess. For example, the retinal image of a door could be interpreted as a vertical rectangle or as some other quadrilateral at a non-vertical orientation in space, and the shadow cues at the bottom of the door helps the system know that it's a vertical rectangle. What mechanisms do the visual system use to discern which cues are available for interpreting images correctly? The proposed work aims to answer this fundamental question about perceptual learning. It was recently shown that the visual system can detect and start using new cues for perception. This phenomenon can be studied in the laboratory using classical conditioning procedures that were previously developed to study learning in animals. In the proposed experiments, a model system is used to understand details about when this learning occurs and what is learned. The data will be compared to predictions based on older, analogous studies in the animal learning literature, and interpreted in the context of Bayesian statistical inference, especially machine learning theory. The proposed work benefits public health by characterizing the brain mechanisms that keep visual perception accurate. These mechanisms are at work in the many months during which a person with congenital cataracts learns to use vision after the cataracts are removed, and it is presumably these mechanisms that go awry when an individual with a family history of synesthesia or autism develops anomalous experience-dependent perceptual responses. Neurodegenerative diseases may disrupt visual learning, in which case visual learning tests could be used to detect disease; understanding the learning of new cues in human vision could lead to better computerized aids for the visually impaired; and knowing what causes a new cue to be learned could lead to new technologies for training people to perceive accurately in novel work environments.          n/a",Cue Reliability and Depth Calibration During Space Perception,7911700,R01EY013988,"['Address', 'Adult', 'Animal Behavior', 'Animals', 'Appearance', 'Autistic Disorder', 'Binocular Vision', 'Biological Models', 'Brain', 'Calibration', 'Cataract', 'Computer Vision Systems', 'Cues', 'Data', 'Devices', 'Diagnosis', 'Disease', 'Environment', 'Experimental Designs', 'Family history of', 'Food', 'Funding', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Learning Disabilities', 'Literature', 'Location', 'Longevity', 'Machine Learning', 'Measures', 'Memory', 'Motion', 'Motion Perception', 'Names', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathology', 'Perception', 'Perceptual learning', 'Persons', 'Positioning Attribute', 'Primates', 'Procedures', 'Process', 'Public Health', 'Recruitment Activity', 'Research', 'Retinal', 'Reversal Learning', 'Rotation', 'Shadowing (Histology)', 'Shapes', 'Signal Transduction', 'Source', 'Space Perception', 'Stimulus', 'Surface', 'System', 'Testing', 'Texture', 'Time', 'Training', 'Translations', 'Trust', 'Ursidae Family', 'Vision', 'Vision Disparity', 'Visual', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'Workplace', 'area MT', 'base', 'classical conditioning', 'clinical application', 'computerized', 'congenital cataract', 'design', 'devices for the visually impaired', 'experience', 'improved', 'meetings', 'neuromechanism', 'new technology', 'novel', 'programs', 'relating to nervous system', 'research study', 'response', 'stereoscopic', 'theories', 'tool', 'visual information', 'visual learning', 'visual process', 'visual processing']",NEI,STATE COLLEGE OF OPTOMETRY,R01,2010,226559,0.004071114958156312
"Towards cortical visual prosthetics    Description (provided by applicant): Visual object recognition is crucial for most everyday tasks including face identification, reading and navigation. In spite of the massive increase in computational power over the last two decades, a 3-year-old still outperforms the most sophisticated algorithms even in simple recognition tasks. Understanding the computations performed by the human visual system to recognize objects will have profound implications not only to understand the functions (and malfunction) of the cerebral cortex but also for developing visual prosthetic devices for the visually impaired. We combine neurophysiology, electrical stimulation and tools from machine learning to further our understanding of the neuronal circuits, algorithms and computations performed by the human visual system to perform visual pattern recognition. In the vast majority of visually impaired or blind people, the problems originate at the level of the retina while the visual cortex remains unimpaired. Our proposal constitutes a proof- of-principle approach towards developing visual prosthetic devices that rely on electrical stimulation of visual cortex. The specific aims of this proposal are designed to test the possibility of decoding and recoding information in visual cortex: (1) Read-out of visual information from human visual cortex on line (2) Write-in of visual information in human visual cortex. We take advantage of a rare opportunity to study the human brain at high spatial and temporal resolution by studying patients who have electrodes implanted for clinical reasons. Our electrophysiological recordings provide us with a unique view of the human temporal lobe circuitry and allow us to test the feasibility of cortical visual prosthetics in behaving human subjects. PUBLIC HEALTH RELEVANCE: Towards cortical visual prosthetics one of the key challenges for the visually impaired and blind people is the lack of visual object recognition capabilities. Visual recognition is crucial for most everyday tasks including navigation and face identification. Our proposal is a proof-of-principle approach towards the development of visual prosthetics devices based on electrical stimulation in visual cortex.           7. Project Narrative: Towards cortical visual prosthetics  One of the key challenges for the visually impaired and blind people is the lack of visual object recognition capabilities. Visual recognition is crucial for most everyday tasks including navigation and face identification. Our proposal is a proof-of-principle approach towards the development of visual prosthetics devices based on electrical stimulation in visual cortex.",Towards cortical visual prosthetics,7903931,R21EY019710,"['3 year old', 'Action Potentials', 'Algorithms', 'Animals', 'Auditory', 'Brain', 'Categories', 'Cerebral cortex', 'Clinical', 'Code', 'Computer software', 'Data', 'Data Quality', 'Detection', 'Development', 'Devices', 'Electric Stimulation', 'Electrodes', 'Epilepsy', 'Face', 'Goals', 'Human', 'Implanted Electrodes', 'Inferior', 'Limb structure', 'Macaca', 'Machine Learning', 'Methodology', 'Methods', 'Monkeys', 'Neurons', 'Output', 'Patients', 'Perception', 'Physiological', 'Prosthesis', 'Reading', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resolution', 'Retina', 'Sensory', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'System', 'Temporal Lobe', 'Testing', 'Time', 'Visual', 'Visual Cortex', 'Visual Pattern Recognition', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Wireless Technology', 'Writing', 'base', 'brain machine interface', 'design', 'devices for the visually impaired', 'human data', 'human subject', 'improved', 'interest', 'neural prosthesis', 'neurophysiology', 'object recognition', 'public health relevance', 'relating to nervous system', 'research study', 'response', 'retinal prosthesis', 'tool', 'vision development', 'visual information']",NEI,BOSTON CHILDREN'S HOSPITAL,R21,2010,212644,-0.018255546507640857
"The Development of a Noninvasive Monitoring System for Cigarette Smoking    DESCRIPTION (provided by applicant): Cigarette smoking is the leading cause of preventable death in the United States. Smoking produces over 440,000 deaths each year in this country and generates an estimated $167 billion in annual health-related economic losses. Available methods of smoking assessment (e.g., self-report, portable puff-topography instruments) do not permit the collection of accurate, non-reactive measures of smoking behavior that capture real-time smoking frequency and comprehensive within-cigarette puff topography. The objective of this project is to develop a non-invasive wearable system (Personal Automatic Cigarette Tracker - PACT) that is completely transparent to the end user and does not require any conscience effort to achieve reliable monitoring of smoking behavior in free living individuals. Methodologically, PACT will consist of two major components: 1. Wearable sensors. Miniature sensors integrated into the clothing will monitors the breathing and activity patterns of individuals. The signals from the sensors will be processed and recognized to identify and objectively characterize each individual puff. 2. Software for signal processing and pattern recognition. Automatic computer software will analyze sensor signals and detect patterns uniquely identifying smoking events. Objective metrics such as number of puffs and inter-puff interval will be extracted. The software will be based on the state-of-art machine learning methods. The development of the PACT system will be addressed in four specific aims: Specific Aim 1: Develop a wearable sensor system comprised of a breathing sensor integrated into conventional underwear and a hand gesture sensor integrated into a hand bracelet. Specific Aim 2: Collect sensor data from individuals wearing the instrumented system and performing everyday activities (including smoking) in laboratory conditions. Specific Aim 3: Develop pattern recognition methods to recognize individual puffs and smoke inhalation. Specific Aim 4: Evaluate the utility and sensitivity of the wearable sensor PACT system and pattern recognition method in people smoking in the natural environment. This set of Specific Aims will validate lead to creation of a unique wearable device capable of objective characterization of smoking behavior.      PUBLIC HEALTH RELEVANCE: Cigarette smoking is the leading cause of preventable death in the United States. Smoking produces over 440,000 deaths each year in this country and generates an estimated $167 billion in annual health-related economic losses. The goal of this research is to develop a non-invasive wearable system (Personal Automatic Cigarette Tracker - PACT) that is completely transparent to the end user and does not require any conscious effort to achieve reliable monitoring of smoking behavior in free living individuals. The PACT device will provide an accurate and precise measure of real-world smoking. The device can provide the user and health professional feedback on the frequency of smoking and inhalation patterns (such as depth of inhalation and smoke holding) throughout the day in their home and community. This information can be used to inform behavioral strategies in smoking cessation programs. The data collected by PACT can also provide an objective method of assessing the effectiveness of behavioral and pharmacological smoking interventions.          PUBLIC HEALTH RELEVANCE: Cigarette smoking is the leading cause of preventable death in the United States. Smoking produces over 440,000 deaths each year in this country and generates an estimated $167 billion in annual health-related economic losses. The goal of this research is to develop a non-invasive wearable system (Personal Automatic Cigarette Tracker - PACT) that is completely transparent to the end user and does not require any conscious effort to achieve reliable monitoring of smoking behavior in free living individuals. The PACT device will provide an accurate and precise measure of real-world smoking. The device can provide the user and health professional feedback on the frequency of smoking and inhalation patterns (such as depth of inhalation and smoke holding) throughout the day in their home and community. This information can be used to inform behavioral strategies in smoking cessation programs. The data collected by PACT can also provide an objective method of assessing the effectiveness of behavioral and pharmacological smoking interventions.",The Development of a Noninvasive Monitoring System for Cigarette Smoking,8089048,R21DA029222,"['Address', 'Algorithms', 'Applications Grants', 'Arts', 'Behavioral', 'Breathing', 'Burn injury', 'Calculi', 'Cellular Phone', 'Cessation of life', 'Characteristics', 'Cigarette', 'Clothing', 'Collection', 'Communities', 'Computer software', 'Computers', 'Conscience', 'Conscious', 'Consumption', 'Country', 'Data', 'Data Set', 'Development', 'Devices', 'Eating', 'Economics', 'Effectiveness', 'Electronics', 'Environment', 'Event', 'Exhalation', 'Exposure to', 'Feedback', 'Frequencies', 'Future', 'Gestures', 'Goals', 'Hand', 'Health', 'Health Professional', 'Home environment', 'Hour', 'Individual', 'Instruction', 'Laboratories', 'Lead', 'Life', 'Machine Learning', 'Measures', 'Memory', 'Methodology', 'Methods', 'Metric', 'Monitor', 'Oral cavity', 'Patient Self-Report', 'Pattern', 'Pattern Recognition', 'Pattern Recognition Systems', 'Phase', 'Process', 'Reporting', 'Research', 'Signal Transduction', 'Smoke', 'Smoker', 'Smoking', 'Smoking Behavior', 'System', 'Testing', 'Time', 'Training', 'United States', 'United States Dept. of Health and Human Services', 'Validation', 'Variant', 'Walking', 'Work', 'Workplace', 'base', 'cigarette smoking', 'cigarette smoking', 'computerized data processing', 'diaries', 'expiration', 'instrument', 'programs', 'public health relevance', 'respiratory', 'sensor', 'smoke inhalation', 'smoking cessation', 'smoking intervention']",NIDA,UNIVERSITY OF ALABAMA IN TUSCALOOSA,R21,2010,187368,-0.01839117213604282
"A mobile Enabling Technology to promote adherence to behavioral therapy    DESCRIPTION (provided by applicant): This application addresses broad Challenge Area (06) Enabling Technologies, 06-DA-105: Improving health through ICT/mobile technologies. The ultimate goal of this research is to fundamentally change the ways in which behavioral interventions are delivered. We propose an innovative mobile Enabling Technology-iHeal-that recognizes stressors that threaten a patient's recovery and then delivers evidence-based interventions exactly at the moment of greatest need. Our objective is to determine, within subjects, the extent to which physiologic and affective changes detected by iHeal are predictive, within subjects, of posttraumatic stress or drug cues. The study team has considerable expertise in technology development and in assessment of behavioral interventions in co-occurring disorders. We will study 25 subjects drawn from an existing SAMHSA-funded investigation that utilizes intense case management to monitor progression of PTSD and substance abuse in returning combat veterans. Our proposed investigation will share interventions with the SMAHSA study that are based upon a blending of Motivational Interviewing and Cognitive Behavioral Therapy approaches for PTSD and substance abuse. Specific aims: 1) To evaluate the accuracy with which iHeal characterizes physiological and affective phenomena as acute stress reactions related to PTSD and environmental drug cues; and 2) To evaluate the effect of Motivational Interviewing-based interventions on acute stress reactions related to PTSD and environmental drug cues. This initial proposal is extremely innovative. The proposed iHeal device will employ cutting-edge wireless technology to link wearable sensors to personal mobile computing platforms (e.g., iPhone). This linkage will allow iHeal to detect co-occurring biological and behavioral processes, while embedded computing in the mobile platform permits iHeal to deliver evidence-based empathetic interventions at the opportune moment. iHeal can learn to intervene in ways that are most effective for the user, including scripted text-based dialogues modeled after brief interventions; use of motivating images or messages from loved ones; playing a meaningful song; or contacting a counselor at the moment of greatest need. Ultimately, a wearable wireless device that anticipates stressors and intervenes at a likely transition to risky activities has enormous potential in a variety of social, behavioral, and biomedical research enterprises. Importantly, iHeal has immediate commercial applications that will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises. iHeal is an innovative device that uses wearable sensors to detect pulse, skin conductance, and acceleration; the sensor array links wirelessly to an iPhone which has an app that identifies changes in the user's physiology. Changes consistent with acute stress from PTSD exacerbations or drug use cues generate an empathetic conversation between the iPhone and the user, who enters real-time data on social/behavioral/environmental contexts. The iPhone (which tracks time and GPS data) uses predictive software to anticipate upcoming stressors and helps the user avoid them. The public health significance of this proposal is 1) iHeal will detect co-occurring biological and behavioral processes in real time; 2) it will discern undiscovered behavioral states; 3) it will predict a behavior of interest; and 4) it will deliver empathetic interventions to the user at the opportune moment for intervention. Because it is based on the union of existing technology and has immediate commercial applications, iHeal will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises.               Project Narrative iHeal is an innovative device that uses wearable sensors to detect pulse, skin conductance, and acceleration; the sensor array links wirelessly to an iPhone which has an app that identifies changes in the user's physiology. Changes consistent with acute stress from PTSD exacerbations or drug use cues generate an empathetic conversation between the iPhone and the user, who enters real-time data on social/behavioral/environmental contexts. The iPhone (which tracks time and GPS data) uses predictive software to anticipate upcoming stressors and helps the user avoid them. The public health significance of this proposal is 1) iHeal will detect co-occurring biological and behavioral processes in real time; 2) it will discern undiscovered behavioral states; 3) it will predict a behavior of interest; and 4) it will deliver empathetic interventions to the user at the opportune moment for intervention. Because it is based on the union of existing technology and has immediate commercial applications, iHeal will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises.",A mobile Enabling Technology to promote adherence to behavioral therapy,7941740,RC1DA028428,"['Acceleration', 'Acute', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Affect', 'Affective', 'Afghanistan', 'Area', 'Artificial Intelligence', 'Arts', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Sciences', 'Biological', 'Biomedical Research', 'Case Management', 'Chronic', 'Clinical', 'Cognitive Therapy', 'Communication', 'Computer software', 'Cues', 'Data', 'Devices', 'Disease', 'Drug usage', 'Effectiveness of Interventions', 'Electrical Engineering', 'Enrollment', 'Environment', 'Evidence based intervention', 'Feasibility Studies', 'Feedback', 'Funding', 'Galvanic Skin Response', 'Goals', 'Growth', 'Health', 'Hour', 'Image', 'Intervention', 'Investigation', 'Iraq', 'Lead', 'Learning', 'Link', 'Machine Learning', 'Mental Health', 'Methods', 'Modeling', 'Monitor', 'Occupations', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Physiologic Monitoring', 'Physiologic pulse', 'Physiological', 'Physiology', 'Play', 'Population', 'Population Study', 'Post-Traumatic Stress Disorders', 'Process', 'Professional counselor', 'Public Health', 'Recovery', 'Recruitment Activity', 'Research', 'Risk Behaviors', 'Services', 'Stress', 'Substance abuse problem', 'Technology', 'Telecommunications', 'Text', 'Time', 'United States Substance Abuse and Mental Health Services Administration', 'Veterans', 'Wireless Technology', 'acute stress', 'acute traumatic stress disorder', 'base', 'biomedical Computer science', 'brief intervention', 'combat', 'commercial application', 'cost effectiveness', 'disorder later incidence prevention', 'evidence base', 'experience', 'follow-up', 'improved', 'in vivo', 'innovation', 'instrument', 'interest', 'loved ones', 'motivational enhancement therapy', 'new technology', 'novel', 'response', 'sensor', 'social', 'stressor', 'study characteristics', 'substance abuse treatment', 'technology development']",NIDA,UNIV OF MASSACHUSETTS MED SCH WORCESTER,RC1,2010,496273,0.0117762169296488
"Camera-based Text Recognition from Complex Backgrounds for the Blind or Visually There are more than 10 million blind and visually impaired people living in America today. Recent technology developments in computer vision, digital cameras, and portable computers make it possible to assist these individuals by developing camera-based products that combine computer vision technology with other existing products.  Although a number of reading assistants have been designed specifically for people who are blind or visually impaired, reading text from complex backgrounds or non-flat surfaces is very challenging and has not yet been successfully addressed. Many everyday tasks involve these challenging conditions, such as reading instructions on vending machines, titles of books aligned on a shelf, instructions on medicine bottles or labels on soup cans.  This proposal focuses on the development of new computer vision algorithms to recognize text from complex backgrounds: 1) from backgrounds with multiple different colors (e.g .. the titles of books lined up on a shelf) and 2) from non-flat surfaces (e.g .. labels on medicine bottles or soup cans). The newly developed computer vision techniques will be integrated with off-the-shelf optical character recognition (OCR) and speech-synthesis software products. Visual information will be captured via a head-mounted camera (on sunglasses or hat) and analyzed by a portable computer (PDA or cell phone), while the speech display will be outputted via mini speakers, earphones, or Bluetooth device. A practical reading system prototype will be produced to read text from complex backgrounds and non-flat surfaces. The system will be cost-effective since it requires only a head mounted camera (<US$100 for 1M resolution), a wearable computer (<US$300), and two mini-speakers or earphones. The price of ""ReadIRlS"" [74] OCR software is under $150 and the ""TextAloud"" speech synthesis software is about $30 [75].  This project will be executed over two years at the City College of New York (CCNY) and Lighthouse International, New York. CCNY, located in the Harlem neighborhood of New York City, is designated as both a Minority Institution and a Hispanic-serving Institution (37% Hispanic and 27% African American). Lighthouse International is a leading non-profit organization dedicated to preserving vision and to providing critically needed vision and rehabilitation services to help people of all ages overcome the challenges of vision loss. During the two years, we will 1) develop new algorithms to recognize text from backgrounds with multiple different colors; 2) develop new algorithms to recognize text from non-flat surfaces; and 3) develop a cost-effective prototype reading system for blind users by integrating with off-the-shelf optical character recognition (OCR) and speech-synthesis software products. The effectiveness of the prototype and algorithms will be evaluated by people with normal vision and people with vision impairment. A database of text on complex backgrounds (multiple colors and non-flat surfaces) will be created for algorithm and system evaluation. The database will be made available to research communities in the areas of computer vision and vision rehabilitation science. In summary, this effort will provide a research-based foundation to inform the design of next generation reading assistants for blind persons, as well as produce a practical prototype to help the blind user read text from complex backgrounds in real-world environments. PROJECT NARRATIVE  The goal of the proposed research is to develop new computer vision algorithms for camera-based text recognition from complex backgrounds and non-flat surfaces, as well as produce a practical reading system prototype in combination with off-the-shelf  optical character recognition (OCR) and speech-synthesis software products, to help blind or visually impaired people read instructions on vending machines, titles of books aligned on a shelf, labels on medicine bottles or soup cans, etc. Visual information will be captured via a head-mounted camera (on sunglasses or hat) and analyzed in realtime through a portable computer, such as a mini laptop or a personal digital assistant (PDA). The speech display will be outputted via mini speakers, earphones, or Bluetooth device.",Camera-based Text Recognition from Complex Backgrounds for the Blind or Visually,7977496,R21EY020990,"['Address', 'African American', 'Age', 'Algorithms', 'Americas', 'Area', 'Blindness', 'Books', 'Cellular Phone', 'Cities', 'Color', 'Communities', 'Complex', 'Computer Systems Development', 'Computer Vision Systems', 'Computer software', 'Computers', 'Databases', 'Development', 'Devices', 'Effectiveness', 'Environment', 'Evaluation', 'Event', 'Facial Expression Recognition', 'Foundations', 'Goals', 'Grant', 'Head', 'Hispanics', 'Image', 'Impairment', 'Individual', 'Institution', 'Instruction', 'International', 'Label', 'Letters', 'Life', 'Mails', 'Marketing', 'Medicine', 'Methods', 'Minority', 'Neighborhoods', 'New York', 'New York City', 'Nonprofit Organizations', 'Output', 'Personal Digital Assistant', 'Price', 'Printing', 'Reading', 'Rehabilitation therapy', 'Research', 'Research Project Grants', 'Resolution', 'Running', 'Scientist', 'Shapes', 'Solutions', 'Speech', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Thick', 'Time', 'United States National Institutes of Health', 'Vertebral column', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Writing', 'base', 'blind', 'college', 'computer generated', 'computer human interaction', 'cost', 'design', 'digital', 'experience', 'laptop', 'next generation', 'optical character recognition', 'prototype', 'rehabilitation science', 'rehabilitation service', 'research and development', 'sunglasses', 'technology development', 'visual information']",NEI,CITY COLLEGE OF NEW YORK,R21,2010,190000,0.03623312016986344
"A Non-Document Text and Display Reader for Visually Impaired Persons    DESCRIPTION (provided by applicant): The goal of this project is to develop a computer vision system based on standard camera cell phones to give blind and visually impaired persons the ability to read appliance displays and similar forms of non-document visual information. This ability is increasingly necessary to use everyday appliances such as microwave ovens and DVD players, and to perform many daily activities such as counting paper money. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed software runs on a standard, off-the- shelf camera phone and uses computer vision algorithms to analyze images taken by the user, to detect and read the text within each image, and to then read it aloud using synthesized speech. Preliminary feasibility studies indicate that current cellular phones easily exceed the minimum processing power required for these tasks. Initially, the software will read out three categories of symbols: LED/LCD appliance displays, product or user-defined barcodes, and denominations of paper money. Ultimately these functions will be integrated with other capabilities being developed under separate funding, such as reading a broad range of printed text (including signs), recognizing objects, and analyzing photographs and graphics, etc., all available as free or low-cost software downloads for any cell phone user. Our specific goals are to (1) gather a database of real images taken by blind and visually impaired persons of a variety of LED/LCD appliance displays, barcodes and US paper currency; (2) develop algorithms to process the images and extract the desired information; (3) implement the algorithms on a camera phone; and (4) conduct user testing to establish design parameters and optimize the human interface. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays, and to other types of text and symbolic information hitherto unaddressed by rehabilitation technology. The proposed research would result in an assistive technology system (with zero or minimal cost to users) to provide increased access to such display and non- document text information for the approximately 10 million Americans with significant vision impairments or blindness.          n/a",A Non-Document Text and Display Reader for Visually Impaired Persons,7799708,R01EY018890,"['Access to Information', 'Acoustics', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Blindness', 'Categories', 'Cellular Phone', 'Code', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Databases', 'Development', 'Devices', 'Economics', 'Electronics', 'Employment', 'Evaluation', 'Feasibility Studies', 'Figs - dietary', 'Funding', 'Goals', 'Hand', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Lavandula', 'Mainstreaming', 'Marketing', 'Memory', 'Modification', 'Paper', 'Printing', 'Process', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'Surveys', 'System', 'Telephone', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visit', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Voice', 'Workplace', 'base', 'blind', 'consumer product', 'cost', 'design', 'image processing', 'meetings', 'microwave electromagnetic radiation', 'optical character recognition', 'prevent', 'programs', 'public health relevance', 'rehabilitation technology', 'response', 'time interval', 'tool', 'trafficking', 'visual information', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2010,427932,0.035000264982155325
"Mobile Search for the Visually Impaired    DESCRIPTION (provided by applicant):    IQ Engines' mobile visual search technology will enable the visually impaired to access real-time information about physical objects using their mobile phone camera. The mobile phone provides a visually-driven hyperlink between the physical and digital world: point the camera at an object and get information (for example product information or navigation information). The mobile phone camera is a powerful yet underutilized tool for the visually impaired. Our proposal has two specific aims. Working directly with the visually impaired community, we will build a prototype mobile visual search application that meets their accessibility and use requirements. Our second aim is to improve upon the state of the art for 3D object recognition. We will investigate a novel combination of sparse image representation, feature matching algorithm, and geometric verification in order to advance the performance of 3D object matching. While state-of-the-art image intelligence is robust enough to enable rapid and accurate image search of flat feature-rich objects, current computer vision pales in comparison to the abilities of biological vision systems to recognize 3-dimensional objects. Our underlying goal is to bring inspiration from recent advances in theoretical neuroscience and apply them to image and video search solutions.      PUBLIC HEALTH RELEVANCE:    Mobile visual search, using a cell phone camera to retrieve object information, enables a mobile phone camera to become an artificial 'eye' with object recognition intelligence. Implemented on a cell phone, a mobile visual search tool can be a low cost visual aid for the blind.           Mobile visual search, using a cell phone camera to retrieve object information, enables a mobile phone camera to become an artificial 'eye' with object recognition intelligence. Implemented on a cell phone, a mobile visual search tool can be a low cost visual aid for the blind.",Mobile Search for the Visually Impaired,7909025,R43EY019790,"['3-Dimensional', 'Algorithms', 'Arts', 'Biological', 'Breathing', 'Car Phone', 'Cellular Phone', 'Color', 'Communities', 'Computer Vision Systems', 'Databases', 'Feedback', 'Funding', 'Future', 'Goals', 'Image', 'Intelligence', 'Internet', 'Letters', 'Modeling', 'Neurosciences', 'Ocular Prosthesis', 'Performance', 'Research', 'Solutions', 'Speech Synthesizers', 'System', 'Technology', 'Text', 'Time', 'Vision', 'Visual', 'Visual Aid', 'Visual impairment', 'Work', 'base', 'blind', 'cost', 'digital', 'improved', 'meetings', 'novel', 'object recognition', 'prototype', 'public health relevance', 'technology development', 'tool', 'visual search']",NEI,"IQ ENGINES, INC.",R43,2010,138770,0.015568284000640222
"A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons    DESCRIPTION (provided by applicant): We propose to develop and evaluate a cell-phone-based system to enable blind and visually impaired individuals to find and read street signs and other signs relevant to wayfinding. Using the built-in camera and computing power of a standard cell phone, the system will process images captured by the user to find and analyze signs, and speak their contents. This will provide valuable assistance for blind or visually impaired pedestrians in finding and reading street signs, as well as locating and identifying addresses and store names, without requiring them to carry any special-purpose hardware. The sign finding and reading software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). We will build on our prior and ongoing work in applying computer vision techniques to practical problem-solving for blind persons, including cell-phone implementation of algorithms for indoor wayfinding and for reading digital appliance displays. We will develop, refine and transfer to the cell phone platform a new belief propagation-based algorithm that has shown preliminary success in finding and analyzing signs under difficult real-world conditions including partial shadow coverage. Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. Access to environmental labels, signs or landmarks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.          n/a",A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons,7911722,R01EY018210,"['Accidents', 'Address', 'Algorithms', 'American', 'Belief', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Databases', 'Detection', 'Development', 'Devices', 'Effectiveness', 'Evaluation', 'Face', 'Figs - dietary', 'Generations', 'Grant', 'Human', 'Image', 'Impairment', 'Individual', 'Label', 'Left', 'Mainstreaming', 'Marketing', 'Modification', 'Names', 'Operating System', 'Performance', 'Problem Solving', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Sampling', 'Self-Help Devices', 'Series', 'Shadowing (Histology)', 'Signal Transduction', 'Speech', 'System', 'Target Populations', 'Techniques', 'Testing', 'Text', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'base', 'blind', 'consumer product', 'cost', 'design', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'open source', 'operation', 'prevent', 'programs', 'prototype', 'skills', 'success', 'tool', 'way finding', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2010,423145,0.013299587225119272
"Non-obtrusive Gait & Fall Monitoring    DESCRIPTION (provided by applicant):  Falls among the elderly, one of the most common reasons requiring medical intervention and a contributing factor in 40% of nursing home admissions, are a major health problem. Several studies have identified quantifiable gait markers that appear to distinguish between elderly ""fallers"" and non-fallers. These studies have relied on data acquired in gait-laboratories. Extending gait assessment capability, and falls detection, into the home could provide valuable before-the-fact information on gait weakness evolution, which in turn could be used to assess the efficiency of counter measures. Current mobile gait analysis techniques are insufficient because they rely on compliance or are too intrusive. The development of a new gait assessment and falls monitor is proposed. The device is passive and obtains gait data from sensing floor vibrations as well as a minimally invasive wireless device, precluding the need to walk on special surfaces or be observed by cameras. This study's principal aim is to validate the device's performance through a comparison with accepted gait assessment techniques at the Physical Medicine and Rehabilitation Gait lab at the University of Virginia Health System   PUBLIC HEALTH RELEVANCE:  An estimated 20% - 40% of community-dwelling elderly fall at least once a year 2 and this rate increases for nursing home residents. Fall-related injuries are among the most common reasons requiring medical intervention and are a contributing factor in 40% of nursing home admissions. The cost of falls to the national economy is significant. In 1994 the total cost due to falls was estimated to be $20.2 billion. This number is expected to climb to $32.2 billion by 2020. One suggestion for reducing the number of falls has been the creation of a fall risk assessment for institutional residents, an important component of which is gait assessment. In view of the results obtained during the Phase I effort, it appears that the floor sensor system may be able to answer a well defined need for which there is presently no other solution that promises to be as readily implementable and for which the market potential is significant.           Non-obtrusive Gait & Fall Monitoring Notice Number: NOT-OD-10-034) Notice Title: NIH Announces the Availability of Recovery Act Funds for Competitive Revision Applications for Small Business Innovation Research and Small Business Transfer Technology Research Grants (R43/R44 and R41/R42) through the NIH Basic Behavioral and Social Science Opportunity Network (OppNet) An estimated 20% - 40% of community-dwelling elderly fall at least once a year 2 and this rate increases for nursing home residents. Fall-related injuries are among the most common reasons requiring medical intervention and are a contributing factor in 40% of nursing home admissions. The cost of falls to the national economy is significant. In 1994 the total cost due to falls was estimated to be $20.2 billion. This number is expected to climb to $32.2 billion by 2020. One suggestion for reducing the number of falls has been the creation of a fall risk assessment for institutional residents, an important component of which is gait assessment. In view of the results obtained during the Phase I effort it appears that the floor sensor system may be able to answer a well defined need for which there is presently no other solution that promises to be as readily implementable and for which the market potential is significant.",Non-obtrusive Gait & Fall Monitoring,8053612,R43AG034698,"['Admission activity', 'Algorithms', 'Businesses', 'Classification', 'Communities', 'Computer Interface', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Elderly', 'Employee Strikes', 'Event', 'Evolution', 'Fingerprint', 'Floor', 'Funding', 'Gait', 'Goals', 'Health', 'Health system', 'Heel', 'Home environment', 'Impaired cognition', 'Individual', 'Injury', 'Intervention', 'Laboratories', 'Left', 'Length', 'Machine Learning', 'Marketing', 'Measures', 'Medical', 'Mental Depression', 'Methodology', 'Monitor', 'Muscle Weakness', 'Noise', 'Nursing Homes', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physical Medicine', 'Principal Investigator', 'Process', 'Recovery', 'Rehabilitation therapy', 'Research Project Grants', 'Resolution', 'Retirement', 'Retrospective Studies', 'Risk', 'Risk Assessment', 'Risk Factors', 'Sensitivity and Specificity', 'Small Business Innovation Research Grant', 'Solutions', 'Suggestion', 'Surface', 'System', 'Techniques', 'Technology Transfer', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Ursidae Family', 'Virginia', 'Walkers', 'Walking', 'Wireless Technology', 'Wood material', 'base', 'behavioral/social science', 'comparative', 'cost', 'digital', 'fall risk', 'falls', 'gait examination', 'human subject', 'minimally invasive', 'operation', 'programs', 'prototype', 'response', 'sensor', 'tool', 'transmission process', 'vibration', 'volunteer']",NIA,EMPIRICAL TECHNOLOGIES CORPORATION,R43,2010,113956,-0.006218617882373934
"A Texture Analysis/Synthesis Model of Visual Crowding    DESCRIPTION (provided by applicant): Identifying a visual stimulus can be substantially impaired by the mere presence of additional stimuli in the immediate vicinity. This phenomenon is called ""crowding,"" and it powerfully limits visual perception in many circumstances, especially in the peripheral visual field. There is a rich body of literature detailing the phenomenology of crowding, but we do not know why crowding occurs. We lack a computational model that can predict what information will be available to an observer in an arbitrary crowded display. A popular hypothesis is that crowding results from obligatory ""texture processing,"" but there have been few efforts to formalize and test what this might mean, despite broad agreement that crowding reflects some form of ""excessive integration."" Dr. Rosenholtz has extensive experience with computational models of texture processing, which are a powerful means of defining the exact nature of ""texture processing"" and testing the ability of such models to explain and predict visual behavior. The proposed research has 3 aims: (1) To clarify and formalize the hypothesis that crowding is due to a ""texture"" - i.e. statistical -- representation of the crowded stimuli. (2) To collect behavioral data from a wider variety of displays and tasks than is typically studied in crowding. (3) To develop and validate the first general-purpose model of visual crowding. To achieve these aims, Dr. Rosenholtz will apply state-of-the-art computational tools for texture synthesis to ""crowded"" stimuli. ""Texturizing"" crowded arrays of stimuli affords a tool for visualizing the information available in a crowded display and a vocabulary for describing its representational content. Thus, Dr. Rosenholtz will attack the problem of crowding through a useful synthesis of computer graphics, computer vision, and psychophysics. PUBLIC HEALTH RELEVANCE: Understanding crowding, besides elucidating representations and performance of normal human vision, is crucial for disorders like age-related macular degeneration, for which, without foveal vision, virtually all perception is essentially crowded. In addition, percepts under crowding may be related to percepts under other visual dysfunctions where there is ""excessive integration"", such as amblyopia and simultagnosia. Successfully predicting crowding severity would also advance the design of low-vision aids for older adults and improve our ability to design for the visually-impaired.            Understanding crowding, besides elucidating representations and performance of normal human vision, is crucial for disorders like age-related macular degeneration, for which, without foveal vision, virtually all perception is essentially crowded. In addition, percepts under crowding may be related to percepts under other visual dysfunctions where there is ""excessive integration"", such as amblyopia and simultagnosia. Successfully predicting crowding severity would also advance the design of low-vision aids for older adults and improve our ability to design for the visually-impaired.",A Texture Analysis/Synthesis Model of Visual Crowding,7903934,R21EY019366,"['Age related macular degeneration', 'Agreement', 'Amblyopia', 'Area', 'Arts', 'Attention', 'Behavior', 'Behavioral', 'Binding', 'Cells', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Crowding', 'Data', 'Databases', 'Discrimination', 'Disease', 'Elderly', 'Eye Movements', 'Face', 'Failure', 'Functional disorder', 'Gender', 'Goals', 'Gray unit of radiation dose', 'Human', 'Imagery', 'Individual', 'Joints', 'Lesion', 'Letters', 'Literature', 'Location', 'Masks', 'Methods', 'Modeling', 'Nature', 'Patients', 'Perception', 'Performance', 'Peripheral', 'Process', 'Psychophysics', 'Research', 'Research Personnel', 'Resolution', 'Saccades', 'Severities', 'Stimulus', 'Techniques', 'Testing', 'Texture', 'Time', 'Training', 'Vision', 'Vision Disorders', 'Visual', 'Visual Fields', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Vocabulary', 'Work', 'base', 'clinically significant', 'computerized tools', 'design', 'experience', 'improved', 'neglect', 'novel', 'object recognition', 'public health relevance', 'research study', 'response', 'statistics', 'theories', 'tool', 'vision aid', 'visual information', 'visual search', 'visual stimulus']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R21,2010,166320,-0.003146189335077706
"Handsight: Mobile Services for Low Vision    DESCRIPTION (provided by applicant): Handsight is a mobile phone service that offers an affordable, extensible set of automated sight-assistant functions to millions of blind and low-vision persons at $30/month atop standard voice and data fees. To the user, Handsight is simply a mobile phone application, requiring no special equipment or updating. By aiming a mobile telephone<s camera in roughly the right direction and pressing just one button, a Handsight user can snap a picture, send it to the Handsight computer center along with location data, and have a response within seconds. Moving the key computation and data from the handset to web servers cut cost, eases technology upgrades, and enables numerous data and technology partnerships needed to rapidly solve a broad spectrum of tasks. To allow widespread adoption Handsight uses voice, keypad, and vibration for user interaction; and for extensibility it is built on open-source software both on the handset and on the web application infrastructure. The range of tasks encountered by the blind and low-vision requires an array of components to solve: some are more heavily text-oriented and involve no location/navigational feedback (distinguishing and identifying different medicines; finding the phone bill in a stack of letters); some are more specifically navigational (locating the exact store entrance, finding the bus stop); yet others are informational (finding out when the next bus is due). Since we aim to provide a broadly useful tool, Handsight has to accommodate the whole range of task types. We are therefore proposing to build an architecture that integrates a set of components addressing the various task types, from text-detection and recognition software to navigational databases. Handsight<s application programming interface (API) will enable third parties to add capabilities to solve new tasks. As a web service, Handsight can evolve as computer vision and navigation technologies advance without requiring users to upgrade handsets or buy new versions of software. Phase I of this project was funded by the Dept. of Education / NIDRR and completed successfully between 10/01/2007 and 04/01/2008 under grant # H133S070044. It demonstrated not just the viability of the proposed project but also likely demand for such a service. (The NIDRR<s immutable deadlines precluded us from pursuing a Phase II with that agency.) Phase II consists in building the cell phone application and remote processing infrastructure with APIs to enable plug-in of the basic and future features. Subcontractor Smith-Kettlewell Institute for Eye Research will assure usability by blind and low-vision users; data partner NAVTEQ will provide a range of leading-edge digital map data. Blindsight already owns fast, accurate text detection and recognition software, and has experience in building scalable web services. A minimum set of features that make for a system with widespread appeal will be developed and/or licensed, and integrated into the service.      PUBLIC HEALTH RELEVANCE: The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today<s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, and shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.        Re: Project Title: Handsight: Mobile Services for Low Vision  FOA: PHS 2009-02 Omnibus Solicitation of the NIH, CDC, FDA and ACF for Small Business  Innovation Research Grant Applications (Parent SBIR [R43/R44])  Proposed project period: April 1, 2010 - March 31, 2012  Principal Investigator: Hallinan, Peter W. SF424 Research and Related Other Project Information: Project Narrative The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today�s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.",Handsight: Mobile Services for Low Vision,7913126,R44EY020707,"['Activities of Daily Living', 'Address', 'Adoption', 'Architecture', 'Area', 'Businesses', 'Car Phone', 'Cellular Phone', 'Centers for Disease Control and Prevention (U.S.)', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Dependence', 'Destinations', 'Detection', 'Devices', 'Education', 'Eye', 'Feedback', 'Fees', 'Focus Groups', 'Friends', 'Funding', 'Future', 'Grant', 'Health', 'Human', 'Individual', 'Institutes', 'Internet', 'Letters', 'Licensing', 'Location', 'Maps', 'Medicine', 'Methods', 'Mission', 'Parents', 'Persons', 'Phase', 'Plug-in', 'Principal Investigator', 'Process', 'Provider', 'Reading', 'Research', 'Research Design', 'Research Infrastructure', 'Research Institute', 'Research Project Grants', 'Research Support', 'Services', 'Simulate', 'Small Business Innovation Research Grant', 'Social Interaction', 'Special Equipment', 'System', 'Target Populations', 'Technology', 'Telephone', 'Text', 'Touch sensation', 'Training', 'Travel', 'United States National Institutes of Health', 'Update', 'Vision', 'Vision Disorders', 'Visual impairment', 'Voice', 'Work', 'blind', 'computer center', 'cost', 'design', 'digital', 'experience', 'falls', 'innovation', 'open source', 'programs', 'public health relevance', 'response', 'success', 'tool', 'usability', 'vibration', 'voice recognition']",NEI,BLINDSIGHT CORPORATION,R44,2010,656703,0.039068733573841156
"Computational Methods for Analysis of Mouth Shapes in Sign Languages    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hand) and by the nonmanual components (the face). These facial articulations perform significant semantic, prosodic, pragmatic, and syntactic functions. This proposal will systematically study mouth positions in ASL. Our hypothesis is that ASL mouth positions are more extensive than those used in speech. To study this hypothesis, this project is divided into three aims. In our first aim, we hypothesize that mouth positions are fundamental for the understanding of signs produced in context because they are very distinct from signs seen in isolation. To study this we have recently collected a database of ASL sentences and nonmanuals in over 3600 video clips from 20 Deaf native signers. Our experiments will use this database to identify potential mappings from visual to linguistic features. To successfully do this, our second aim is to design a set of shape analysis and discriminant analysis algorithms that can efficiently analyze the large number of frames in these video clips. The goal is to define a linguistically useful model, i.e., the smallest model that contains the main visual features from which further predictions can be made. Then, in our third aim, we will explore the hypothesis that the linguistically distinct mouth positions are also visually distinct. In particular, we will use the algorithms defined in the second aim to determine if distinct visual features are used to define different linguistic categories. This result will show whether linguistically meaningful mouth positions are not only necessary in ASL (as hypothesized in aim 1), but whether they are defined using non-overlapping visual features (as hypothesized in aim 3). These aims address a critical need. At present, the study of nonmanuals must be carried out manually, that is, the shape and position of each facial feature in each frame must be recorded by hand. Furthermore, to be able to draw conclusive results for the design of a linguistic model, it is necessary to study many video sequences of related sentences as produced by different signers. It has thus proven nearly impossible to continue this research manually. The algorithms designed in the course of this grant will facilitate this analysis of ASL nonmanuals and lead to better teaching materials.      PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.           Project Narrative Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for Analysis of Mouth Shapes in Sign Languages,8101448,R21DC011081,"['Academic achievement', 'Access to Information', 'Address', 'Adult', 'Algorithms', 'Applications Grants', 'Arts', 'Categories', 'Child', 'Clip', 'Communication', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Databases', 'Devices', 'Discriminant Analysis', 'Educational process of instructing', 'Emotions', 'Excision', 'Eye', 'Face', 'Funding', 'Goals', 'Grant', 'Hand', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Joints', 'Knowledge', 'Language', 'Lead', 'Learning', 'Life', 'Linguistics', 'Manuals', 'Modeling', 'Oral cavity', 'Parents', 'Pattern Recognition', 'Positioning Attribute', 'Process', 'Regulation', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Scientist', 'Semantics', 'Shapes', 'Sign Language', 'Social Interaction', 'Specific qualifier value', 'Speech', 'Teaching Materials', 'Technology', 'Testing', 'Training', 'United States National Institutes of Health', 'Visual', 'Work', 'computerized tools', 'deafness', 'design', 'experience', 'innovation', 'instructor', 'interest', 'novel', 'prevent', 'public health relevance', 'research study', 'shape analysis', 'success', 'syntax', 'teacher', 'tool', 'visual map']",NIDCD,OHIO STATE UNIVERSITY,R21,2010,187999,-0.019178821735113982
"Designing Visually Accessible Spaces    DESCRIPTION (provided by applicant): Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our long-term goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals and to enhance safety for the elderly and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool in which environments (such as a hotel lobby, large classroom, or hospital reception area), could be simulated with sufficient accuracy to predict the visibility of key landmarks or obstacles, such as steps or benches, under differing lighting conditions. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments"". Our research plan has four specific goals: 1) Develop methods for predicting the physical levels of light reaching the eye in existing or planned architectural spaces. 2) Acquire performance data for normally sighted subjects with visual restrictions and people with low vision to investigate perceptual capabilities critical to visually-based mobility. 3) Develop models that can predict perceptual competence on tasks critical to visually-based mobility. 4) Demonstrate a proof-of-concept software tool that operates on design models from existing architectural design systems and is able to highlight potential obstacles to visual accessibility. The lead investigators in our partnership come from three institutions: University of Minnesota Gordon Legge, Daniel Kersten; University of Utah William Thompson, Peter Shirley, Sarah Creem-Regehr; and Indiana University Robert Shakespeare. This interdisciplinary team has expertise in the four areas required for programmatic research on visual accessibility empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), photometrically correct computer graphics (Shirley), and architectural design and lighting (Shakespeare).           n/a",Designing Visually Accessible Spaces,7777764,R01EY017835,"['Accounting', 'Address', 'American', 'Area', 'Arts', 'Blindness', 'Central Scotomas', 'Characteristics', 'Classification', 'Competence', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Data', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Elderly', 'Engineering', 'Environment', 'Evaluation', 'Eye', 'Goals', 'Hospitals', 'Indiana', 'Individual', 'Institution', 'Lead', 'Light', 'Lighting', 'Lobbying', 'Location', 'Measurement', 'Methods', 'Minnesota', 'Modeling', 'National Eye Institute', 'Pattern', 'Perception', 'Performance', 'Peripheral', 'Photometry', 'Published Comment', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Priority', 'Safety', 'Simulate', 'Software Tools', 'Space Perception', 'Specialist', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Testing', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual impairment', 'base', 'design', 'innovation', 'knowledge base', 'luminance', 'model design', 'model development', 'physical model', 'predictive modeling', 'programs', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2010,527186,0.05312631660490497
"Accessible Artificial Intelligence Tutoring Software for Mathematics    DESCRIPTION (provided by applicant): This Fast-Track application focuses on developing the first artificial intelligence (AI) educational software to teach developmental mathematics to the blind and visually impaired. This project responds to the National Eye Institute's General Research Topics for ""teaching tools"" and the Visual Impairment and Blindness Program for ""other devices that meet the rehabilitative and everyday living needs of persons who are blind or have low vision."" The intervention being developed will place a comprehensive set of AI mathematics tutoring systems with integrated AI assessment capabilities in the hands of the blind K-12, college and adult student, for use on demand during study at home and at school. The formulation of an advanced AI tutoring methodology with accessibility inherent to its design will have broad implications for development in many subject areas beyond mathematics. Project objectives include: Horizontal Expansion of Accessible Curriculum Content Coverage (Ratio and Proportion, Percentages, Linear Equations, Metric Units, Scientific Notation) 1) Conduct initial accessibility review and analysis of AI tutor's existing user interface. 2) Implement accessibility requirements and recommendations from NFB, instructors and other partners. 3) Conduct final review to gain NFB accessibility certification after implementation of requirements. 4) Develop and issue survey of instructors on mathematics pedagogy and technology. Vertical Expansion of Accessible Features and Technological Capability 5) Implement Braille support in AI technology. 6) Develop additional AI tutor on Fractions that is automatically accessible from first principles using accessible AI framework. Evaluation of Accessible AI Educational Technology 7) Field evaluation of accessible AI technology with blind students and their instructors. 8) Continued demonstration and review of accessible AI technology by partners and other stakeholders. Preparation for success in Phase III has already been undertaken by involving partners that are important commercially as well as technically, such as the National Federation of the Blind and the American Printing House for the Blind (APH). In addition, Quantum already has long-term partnerships established with McGraw- Hill and Holt, Rinehart and Winston, two of the country's leading educational publishers, as well as a major science education catalog company, CyberEd, Inc., a PLATO Learning Company. PUBLIC HEALTH RELEVANCE: There is a considerable need for improved educational software for mathematics in general, but the problem of quality educational software materials for the blind and visually impaired is particularly acute. A weak mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of science, technology, engineering and mathematics. Through previous federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI) tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power and benefit of this cutting-edge educational technology to students who are blind and visually impaired.           PROJECT NARRATIVE: There is a considerable need for improved educational software for mathematics in general, but the problem of quality educational software materials for the blind and visually impaired is particularly acute. A weak mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of science, technology, engineering and mathematics. Through previous federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI) tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power and benefit of this cutting-edge educational technology to students who are blind and visually impaired.",Accessible Artificial Intelligence Tutoring Software for Mathematics,7608855,R44EY019414,"['Activities of Daily Living', 'Acute', 'Address', 'Adult', 'American', 'Area', 'Artificial Intelligence', 'Blindness', 'Businesses', 'Cataloging', 'Catalogs', 'Categories', 'Certification', 'Chemicals', 'Chemistry', 'Collaborations', 'Computer software', 'Country', 'Development', 'Development Plans', 'Devices', 'Dimensions', 'Drug Formulations', 'Dyslexia', 'Education', 'Educational Curriculum', 'Educational Technology', 'Educational process of instructing', 'Elements', 'Engineering', 'Ensure', 'Equation', 'Equilibrium', 'Evaluation', 'Feedback', 'Future', 'Goals', 'Hand', 'Home environment', 'Housing', 'Individual', 'Institution', 'Instruction', 'Internet', 'Intervention', 'Language', 'Learning', 'Letters', 'Life', 'Mathematics', 'Measures', 'Mediation', 'Methodology', 'Metric', 'Mission', 'Modeling', 'National Eye Institute', 'Nature', 'Outcome', 'Performance', 'Persons', 'Phase', 'Philosophy', 'Play', 'Preparation', 'Printing', 'Process', 'Publishing', 'Reader', 'Reading Disabilities', 'Recommendation', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Research Support', 'Role', 'Schools', 'Science', 'Small Business Innovation Research Grant', 'Software Tools', 'Students', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Visual impairment', 'Work', 'blind', 'braille', 'career', 'college', 'commercial application', 'commercialization', 'design', 'disability', 'high school', 'improved', 'innovation', 'innovative technologies', 'instructor', 'meetings', 'middle school', 'programs', 'prospective', 'prototype', 'public health relevance', 'quantum', 'quantum chemistry', 'remediation', 'research and development', 'science education', 'simulation', 'stem', 'success', 'teacher', 'technological innovation', 'tool']",NEI,"QUANTUM SIMULATIONS, INC.",R44,2009,164486,0.013723456480975671
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7668573,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,1187062,0.045889024392851814
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7922310,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,152260,0.045889024392851814
"Cue Reliability and Depth Calibration During Space Perception    DESCRIPTION (provided by applicant): The long-term objective of the proposed work is to understand how learning by the visual system helps it to represent the immediate environment during perception. Because perception is accurate, we can know spatial layout: the shapes, orientations, sizes, and spatial locations of the objects and surfaces around us. But this accuracy requires that the visual system learn over time how best to interpret visual ""cues"". These cues are the signals from the environment that the visual system extracts from the retinal images that are informative about spatial layout. Known cues include binocular disparity, texture gradients, occlusion relations, motion parallax, and familiar size, to name a few. How do these cues come to be interpreted correctly? A fundamental problem is that visual cues are ambiguous. Even if cues could be measured exactly (which they cannot, the visual system being a physical device) there would still be different possible 3D interpretations for a given set of cues. As a result, the visual system is forced to operate probabilistically: the way things ""look"" to us reflects an implicit guess as to which interpretation of the cues is most likely to be correct. Each additional cue helps improve the guess. For example, the retinal image of a door could be interpreted as a vertical rectangle or as some other quadrilateral at a non-vertical orientation in space, and the shadow cues at the bottom of the door helps the system know that it's a vertical rectangle. What mechanisms do the visual system use to discern which cues are available for interpreting images correctly? The proposed work aims to answer this fundamental question about perceptual learning. It was recently shown that the visual system can detect and start using new cues for perception. This phenomenon can be studied in the laboratory using classical conditioning procedures that were previously developed to study learning in animals. In the proposed experiments, a model system is used to understand details about when this learning occurs and what is learned. The data will be compared to predictions based on older, analogous studies in the animal learning literature, and interpreted in the context of Bayesian statistical inference, especially machine learning theory. The proposed work benefits public health by characterizing the brain mechanisms that keep visual perception accurate. These mechanisms are at work in the many months during which a person with congenital cataracts learns to use vision after the cataracts are removed, and it is presumably these mechanisms that go awry when an individual with a family history of synesthesia or autism develops anomalous experience-dependent perceptual responses. Neurodegenerative diseases may disrupt visual learning, in which case visual learning tests could be used to detect disease; understanding the learning of new cues in human vision could lead to better computerized aids for the visually impaired; and knowing what causes a new cue to be learned could lead to new technologies for training people to perceive accurately in novel work environments.          n/a",Cue Reliability and Depth Calibration During Space Perception,7692268,R01EY013988,"['Address', 'Adult', 'Animal Behavior', 'Animals', 'Appearance', 'Autistic Disorder', 'Binocular Vision', 'Biological Models', 'Brain', 'Calibration', 'Cataract', 'Computer Vision Systems', 'Cues', 'Data', 'Devices', 'Diagnosis', 'Disease', 'Environment', 'Experimental Designs', 'Family history of', 'Food', 'Funding', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Learning Disabilities', 'Literature', 'Location', 'Longevity', 'Machine Learning', 'Measures', 'Memory', 'Motion', 'Motion Perception', 'Names', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathology', 'Perception', 'Perceptual learning', 'Persons', 'Positioning Attribute', 'Primates', 'Procedures', 'Process', 'Public Health', 'Recruitment Activity', 'Research', 'Retinal', 'Reversal Learning', 'Rotation', 'Shadowing (Histology)', 'Shapes', 'Signal Transduction', 'Source', 'Space Perception', 'Stimulus', 'Surface', 'System', 'Testing', 'Texture', 'Time', 'Training', 'Translations', 'Trust', 'Ursidae Family', 'Vision', 'Vision Disparity', 'Visual', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'Workplace', 'area MT', 'base', 'classical conditioning', 'clinical application', 'computerized', 'congenital cataract', 'design', 'devices for the visually impaired', 'experience', 'improved', 'meetings', 'neuromechanism', 'new technology', 'novel', 'programs', 'relating to nervous system', 'research study', 'response', 'stereoscopic', 'theories', 'tool', 'visual information', 'visual learning', 'visual process', 'visual processing']",NEI,STATE COLLEGE OF OPTOMETRY,R01,2009,228847,0.004071114958156312
"Towards cortical visual prosthetics    Description (provided by applicant): Visual object recognition is crucial for most everyday tasks including face identification, reading and navigation. In spite of the massive increase in computational power over the last two decades, a 3-year-old still outperforms the most sophisticated algorithms even in simple recognition tasks. Understanding the computations performed by the human visual system to recognize objects will have profound implications not only to understand the functions (and malfunction) of the cerebral cortex but also for developing visual prosthetic devices for the visually impaired. We combine neurophysiology, electrical stimulation and tools from machine learning to further our understanding of the neuronal circuits, algorithms and computations performed by the human visual system to perform visual pattern recognition. In the vast majority of visually impaired or blind people, the problems originate at the level of the retina while the visual cortex remains unimpaired. Our proposal constitutes a proof- of-principle approach towards developing visual prosthetic devices that rely on electrical stimulation of visual cortex. The specific aims of this proposal are designed to test the possibility of decoding and recoding information in visual cortex: (1) Read-out of visual information from human visual cortex on line (2) Write-in of visual information in human visual cortex. We take advantage of a rare opportunity to study the human brain at high spatial and temporal resolution by studying patients who have electrodes implanted for clinical reasons. Our electrophysiological recordings provide us with a unique view of the human temporal lobe circuitry and allow us to test the feasibility of cortical visual prosthetics in behaving human subjects. PUBLIC HEALTH RELEVANCE: Towards cortical visual prosthetics one of the key challenges for the visually impaired and blind people is the lack of visual object recognition capabilities. Visual recognition is crucial for most everyday tasks including navigation and face identification. Our proposal is a proof-of-principle approach towards the development of visual prosthetics devices based on electrical stimulation in visual cortex.           7. Project Narrative: Towards cortical visual prosthetics  One of the key challenges for the visually impaired and blind people is the lack of visual object recognition capabilities. Visual recognition is crucial for most everyday tasks including navigation and face identification. Our proposal is a proof-of-principle approach towards the development of visual prosthetics devices based on electrical stimulation in visual cortex.",Towards cortical visual prosthetics,7701276,R21EY019710,"['3 year old', 'Action Potentials', 'Algorithms', 'Animals', 'Auditory', 'Brain', 'Categories', 'Cerebral cortex', 'Clinical', 'Code', 'Computer software', 'Data', 'Data Quality', 'Detection', 'Development', 'Devices', 'Electric Stimulation', 'Electrodes', 'Epilepsy', 'Face', 'Goals', 'Human', 'Implanted Electrodes', 'Inferior', 'Limb structure', 'Macaca', 'Machine Learning', 'Methodology', 'Methods', 'Monkeys', 'Neurons', 'Output', 'Patients', 'Perception', 'Physiological', 'Prosthesis', 'Reading', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resolution', 'Retina', 'Sensory', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'System', 'Temporal Lobe', 'Testing', 'Time', 'Visual', 'Visual Cortex', 'Visual Pattern Recognition', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Wireless Technology', 'Writing', 'base', 'brain machine interface', 'design', 'devices for the visually impaired', 'human data', 'human subject', 'improved', 'interest', 'neural prosthesis', 'neurophysiology', 'object recognition', 'public health relevance', 'relating to nervous system', 'research study', 'response', 'retinal prosthesis', 'tool', 'vision development', 'visual information']",NEI,BOSTON CHILDREN'S HOSPITAL,R21,2009,247290,-0.018255546507640857
"A mobile Enabling Technology to promote adherence to behavioral therapy    DESCRIPTION (provided by applicant): This application addresses broad Challenge Area (06) Enabling Technologies, 06-DA-105: Improving health through ICT/mobile technologies. The ultimate goal of this research is to fundamentally change the ways in which behavioral interventions are delivered. We propose an innovative mobile Enabling Technology-iHeal-that recognizes stressors that threaten a patient's recovery and then delivers evidence-based interventions exactly at the moment of greatest need. Our objective is to determine, within subjects, the extent to which physiologic and affective changes detected by iHeal are predictive, within subjects, of posttraumatic stress or drug cues. The study team has considerable expertise in technology development and in assessment of behavioral interventions in co-occurring disorders. We will study 25 subjects drawn from an existing SAMHSA-funded investigation that utilizes intense case management to monitor progression of PTSD and substance abuse in returning combat veterans. Our proposed investigation will share interventions with the SMAHSA study that are based upon a blending of Motivational Interviewing and Cognitive Behavioral Therapy approaches for PTSD and substance abuse. Specific aims: 1) To evaluate the accuracy with which iHeal characterizes physiological and affective phenomena as acute stress reactions related to PTSD and environmental drug cues; and 2) To evaluate the effect of Motivational Interviewing-based interventions on acute stress reactions related to PTSD and environmental drug cues. This initial proposal is extremely innovative. The proposed iHeal device will employ cutting-edge wireless technology to link wearable sensors to personal mobile computing platforms (e.g., iPhone). This linkage will allow iHeal to detect co-occurring biological and behavioral processes, while embedded computing in the mobile platform permits iHeal to deliver evidence-based empathetic interventions at the opportune moment. iHeal can learn to intervene in ways that are most effective for the user, including scripted text-based dialogues modeled after brief interventions; use of motivating images or messages from loved ones; playing a meaningful song; or contacting a counselor at the moment of greatest need. Ultimately, a wearable wireless device that anticipates stressors and intervenes at a likely transition to risky activities has enormous potential in a variety of social, behavioral, and biomedical research enterprises. Importantly, iHeal has immediate commercial applications that will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises. iHeal is an innovative device that uses wearable sensors to detect pulse, skin conductance, and acceleration; the sensor array links wirelessly to an iPhone which has an app that identifies changes in the user's physiology. Changes consistent with acute stress from PTSD exacerbations or drug use cues generate an empathetic conversation between the iPhone and the user, who enters real-time data on social/behavioral/environmental contexts. The iPhone (which tracks time and GPS data) uses predictive software to anticipate upcoming stressors and helps the user avoid them. The public health significance of this proposal is 1) iHeal will detect co-occurring biological and behavioral processes in real time; 2) it will discern undiscovered behavioral states; 3) it will predict a behavior of interest; and 4) it will deliver empathetic interventions to the user at the opportune moment for intervention. Because it is based on the union of existing technology and has immediate commercial applications, iHeal will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises.               Project Narrative iHeal is an innovative device that uses wearable sensors to detect pulse, skin conductance, and acceleration; the sensor array links wirelessly to an iPhone which has an app that identifies changes in the user's physiology. Changes consistent with acute stress from PTSD exacerbations or drug use cues generate an empathetic conversation between the iPhone and the user, who enters real-time data on social/behavioral/environmental contexts. The iPhone (which tracks time and GPS data) uses predictive software to anticipate upcoming stressors and helps the user avoid them. The public health significance of this proposal is 1) iHeal will detect co-occurring biological and behavioral processes in real time; 2) it will discern undiscovered behavioral states; 3) it will predict a behavior of interest; and 4) it will deliver empathetic interventions to the user at the opportune moment for intervention. Because it is based on the union of existing technology and has immediate commercial applications, iHeal will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises.",A mobile Enabling Technology to promote adherence to behavioral therapy,7820117,RC1DA028428,"['Acceleration', 'Acute', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Affect', 'Affective', 'Afghanistan', 'Area', 'Artificial Intelligence', 'Arts', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Sciences', 'Biological', 'Biomedical Research', 'Case Management', 'Chronic', 'Clinical', 'Communication', 'Computer software', 'Cues', 'Data', 'Devices', 'Disease', 'Drug usage', 'Effectiveness of Interventions', 'Electrical Engineering', 'Enrollment', 'Environment', 'Evidence based intervention', 'Feasibility Studies', 'Feedback', 'Funding', 'Galvanic Skin Response', 'Goals', 'Growth', 'Health', 'Hour', 'Image', 'Intervention', 'Investigation', 'Iraq', 'Lead', 'Learning', 'Link', 'Machine Learning', 'Mental Health', 'Methods', 'Modeling', 'Monitor', 'Occupations', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Physiologic Monitoring', 'Physiologic pulse', 'Physiological', 'Physiology', 'Play', 'Population', 'Population Study', 'Post-Traumatic Stress Disorders', 'Process', 'Professional counselor', 'Public Health', 'Recovery', 'Recruitment Activity', 'Research', 'Risk Behaviors', 'Services', 'Stress', 'Substance abuse problem', 'Technology', 'Telecommunications', 'Text', 'Time', 'United States Substance Abuse and Mental Health Services Administration', 'Veterans', 'Wireless Technology', 'acute stress', 'acute traumatic stress disorder', 'base', 'biomedical Computer science', 'brief intervention', 'cognitive behavior therapy', 'combat', 'commercial application', 'cost effectiveness', 'disorder later incidence prevention', 'evidence base', 'experience', 'follow-up', 'improved', 'in vivo', 'innovation', 'instrument', 'interest', 'loved ones', 'motivational enhancement therapy', 'new technology', 'novel', 'response', 'sensor', 'social', 'stressor', 'study characteristics', 'technology development']",NIDA,UNIV OF MASSACHUSETTS MED SCH WORCESTER,RC1,2009,499381,0.0117762169296488
"Low-Cost Electronic Nose for Groundwater Contaminants    DESCRIPTION (provided by applicant): Several US agencies and regulators require low-cost chemical sensors for detecting and monitoring environmental clean-up, remediation, and decommissioning processes where groundwater may be contaminated. The sensors must be capable of detecting contaminants in the sub-surface groundwater and must be compatible with use in a range of environments. Most significantly, these customers require a low-cost alternative to its current expensive and labor intensive methods, namely using mobile laboratories. The project will result in the innovative use of low-cost sensor systems that will be capable of detecting and monitoring for dense non-aqueous phase liquids in the subsurface and groundwater, unattended, and in real- time from within a push-probe, using a chemicapacitor array and miniature preconcentrator. Seacoast's Phase I research will focus on developing the sensor array, demonstrating sensitivity to chlorinated hydrocarbons at relevant concentrations, and field tests in actual contaminated sites. The ultimate goal is to provide the DOD, DOE, NIEHS and other agencies with a method to map and track subsurface contamination plumes in real-time without requiring an operator. The systems will have MEMS microcapacitor sensor arrays that can monitor for leaks of toxic chemicals, contaminants from wastes, and changes in groundwater streams. A preconcentrator collects the contaminants and releases them to a microsensor array. The sensor arrays are filled with several chemoselective polymers whose dielectric permittivity changes when exposed to different vapors, creating a fingerprint response for each chemical. An array of differently responding sensors and pattern recognition can thereby compensate for changes in humidity, temperature, and composition. These low-power systems can be left unattended and transmit data wirelessly or through USB to a central location. The most important application to public health and safety is unattended monitoring of drinking water, water treatment processes, and water sources. The potential commercial markets include building chemical process monitoring and control, toxic vapor leak detection, industrial process control, and industrial health and safety. Transitioning the developed prototype to other markets where worker and public health, environmental health and regulatory compliance will be investigated to reduce the financial risks and broaden the acceptance of the technology. PUBLIC HEALTH RELEVANCE: This proposal will describe a potential method to specifically address the need for detecting groundwater contaminants and long-term monitoring of contaminated sites, by providing an unattended sensor system that tracks contamination in real-time and transmits contaminant concentrations. Such a system would be used in tandem with other methods, to provide comprehensive contamination management at DOE, DOD, and Superfund sites where ground and water clean-up projects are already underway. The proposed work will focus on detection of chlorinated hydrocarbons, which are described as among the most common pollutants in groundwater and soils at DOE sites.          n/a",Low-Cost Electronic Nose for Groundwater Contaminants,7847964,R43ES016941,"['Address', 'Algorithms', 'Characteristics', 'Chemicals', 'Chlorinated Hydrocarbons', 'Classification', 'Collection', 'Data', 'Data Collection', 'Detection', 'Electronics', 'Engineering', 'Environment', 'Environmental Health', 'Environmental Monitoring', 'Feedback', 'Fingerprint', 'Fluorescence', 'Goals', 'Humidity', 'Industrial Health', 'Laboratories', 'Lasers', 'Left', 'Liquid substance', 'Location', 'Machine Learning', 'Maps', 'Marketing', 'Measures', 'Methods', 'Modification', 'Monitor', 'National Institute of Environmental Health Sciences', 'Nose', 'Pattern Recognition', 'Phase', 'Poison', 'Polymers', 'Process', 'Public Health', 'Pump', 'ROC Curve', 'Recommendation', 'Research', 'Risk', 'Safety', 'Sampling', 'Science', 'Simulate', 'Site', 'Soil', 'Solutions', 'Source', 'Stream', 'Surface', 'System', 'Technology', 'Temperature', 'Testing', 'Time', 'Trichloroethylene', 'Water', 'Work', 'aqueous', 'base', 'cold temperature', 'computerized data processing', 'cost', 'cost effectiveness', 'design', 'detector', 'drinking water', 'ground water', 'innovation', 'membrane assembly', 'pollutant', 'prototype', 'public health relevance', 'remediation', 'response', 'sensor', 'superfund site', 'vapor', 'wasting', 'water treatment']",NIEHS,"SEACOAST SCIENCE, INC.",R43,2009,11376,0.024193340664906495
"A Non-Document Text and Display Reader for Visually Impaired Persons    DESCRIPTION (provided by applicant): The goal of this project is to develop a computer vision system based on standard camera cell phones to give blind and visually impaired persons the ability to read appliance displays and similar forms of non-document visual information. This ability is increasingly necessary to use everyday appliances such as microwave ovens and DVD players, and to perform many daily activities such as counting paper money. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed software runs on a standard, off-the- shelf camera phone and uses computer vision algorithms to analyze images taken by the user, to detect and read the text within each image, and to then read it aloud using synthesized speech. Preliminary feasibility studies indicate that current cellular phones easily exceed the minimum processing power required for these tasks. Initially, the software will read out three categories of symbols: LED/LCD appliance displays, product or user-defined barcodes, and denominations of paper money. Ultimately these functions will be integrated with other capabilities being developed under separate funding, such as reading a broad range of printed text (including signs), recognizing objects, and analyzing photographs and graphics, etc., all available as free or low-cost software downloads for any cell phone user. Our specific goals are to (1) gather a database of real images taken by blind and visually impaired persons of a variety of LED/LCD appliance displays, barcodes and US paper currency; (2) develop algorithms to process the images and extract the desired information; (3) implement the algorithms on a camera phone; and (4) conduct user testing to establish design parameters and optimize the human interface. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays, and to other types of text and symbolic information hitherto unaddressed by rehabilitation technology. The proposed research would result in an assistive technology system (with zero or minimal cost to users) to provide increased access to such display and non- document text information for the approximately 10 million Americans with significant vision impairments or blindness.          n/a",A Non-Document Text and Display Reader for Visually Impaired Persons,7589644,R01EY018890,"['Access to Information', 'Acoustics', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Blindness', 'Categories', 'Cellular Phone', 'Code', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Databases', 'Development', 'Devices', 'Economics', 'Electronics', 'Employment', 'Evaluation', 'Feasibility Studies', 'Figs - dietary', 'Funding', 'Goals', 'Hand', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Lavandula', 'Mainstreaming', 'Marketing', 'Memory', 'Modification', 'Paper', 'Printing', 'Process', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'Surveys', 'System', 'Telephone', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visit', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Voice', 'Workplace', 'base', 'blind', 'consumer product', 'cost', 'design', 'image processing', 'meetings', 'microwave electromagnetic radiation', 'optical character recognition', 'prevent', 'programs', 'public health relevance', 'rehabilitation technology', 'response', 'time interval', 'tool', 'trafficking', 'visual information', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2009,426946,0.035000264982155325
"A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons    DESCRIPTION (provided by applicant): We propose to develop and evaluate a cell-phone-based system to enable blind and visually impaired individuals to find and read street signs and other signs relevant to wayfinding. Using the built-in camera and computing power of a standard cell phone, the system will process images captured by the user to find and analyze signs, and speak their contents. This will provide valuable assistance for blind or visually impaired pedestrians in finding and reading street signs, as well as locating and identifying addresses and store names, without requiring them to carry any special-purpose hardware. The sign finding and reading software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). We will build on our prior and ongoing work in applying computer vision techniques to practical problem-solving for blind persons, including cell-phone implementation of algorithms for indoor wayfinding and for reading digital appliance displays. We will develop, refine and transfer to the cell phone platform a new belief propagation-based algorithm that has shown preliminary success in finding and analyzing signs under difficult real-world conditions including partial shadow coverage. Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. Access to environmental labels, signs or landmarks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.          n/a",A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons,7373002,R01EY018210,"['Accidents', 'Address', 'Algorithms', 'American', 'Belief', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Databases', 'Detection', 'Development', 'Devices', 'Effectiveness', 'Evaluation', 'Face', 'Figs - dietary', 'Generations', 'Grant', 'Human', 'Image', 'Impairment', 'Individual', 'Label', 'Left', 'Mainstreaming', 'Marketing', 'Modification', 'Names', 'Operating System', 'Operative Surgical Procedures', 'Performance', 'Problem Solving', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Sampling', 'Self-Help Devices', 'Series', 'Shadowing (Histology)', 'Signal Transduction', 'Speech', 'System', 'Target Populations', 'Techniques', 'Testing', 'Text', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'base', 'blind', 'consumer product', 'cost', 'design', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'open source', 'prevent', 'programs', 'prototype', 'skills', 'success', 'tool', 'way finding', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2009,417177,0.013299587225119272
"A Texture Analysis/Synthesis Model of Visual Crowding    DESCRIPTION (provided by applicant): Identifying a visual stimulus can be substantially impaired by the mere presence of additional stimuli in the immediate vicinity. This phenomenon is called ""crowding,"" and it powerfully limits visual perception in many circumstances, especially in the peripheral visual field. There is a rich body of literature detailing the phenomenology of crowding, but we do not know why crowding occurs. We lack a computational model that can predict what information will be available to an observer in an arbitrary crowded display. A popular hypothesis is that crowding results from obligatory ""texture processing,"" but there have been few efforts to formalize and test what this might mean, despite broad agreement that crowding reflects some form of ""excessive integration."" Dr. Rosenholtz has extensive experience with computational models of texture processing, which are a powerful means of defining the exact nature of ""texture processing"" and testing the ability of such models to explain and predict visual behavior. The proposed research has 3 aims: (1) To clarify and formalize the hypothesis that crowding is due to a ""texture"" - i.e. statistical -- representation of the crowded stimuli. (2) To collect behavioral data from a wider variety of displays and tasks than is typically studied in crowding. (3) To develop and validate the first general-purpose model of visual crowding. To achieve these aims, Dr. Rosenholtz will apply state-of-the-art computational tools for texture synthesis to ""crowded"" stimuli. ""Texturizing"" crowded arrays of stimuli affords a tool for visualizing the information available in a crowded display and a vocabulary for describing its representational content. Thus, Dr. Rosenholtz will attack the problem of crowding through a useful synthesis of computer graphics, computer vision, and psychophysics. PUBLIC HEALTH RELEVANCE: Understanding crowding, besides elucidating representations and performance of normal human vision, is crucial for disorders like age-related macular degeneration, for which, without foveal vision, virtually all perception is essentially crowded. In addition, percepts under crowding may be related to percepts under other visual dysfunctions where there is ""excessive integration"", such as amblyopia and simultagnosia. Successfully predicting crowding severity would also advance the design of low-vision aids for older adults and improve our ability to design for the visually-impaired.            Understanding crowding, besides elucidating representations and performance of normal human vision, is crucial for disorders like age-related macular degeneration, for which, without foveal vision, virtually all perception is essentially crowded. In addition, percepts under crowding may be related to percepts under other visual dysfunctions where there is ""excessive integration"", such as amblyopia and simultagnosia. Successfully predicting crowding severity would also advance the design of low-vision aids for older adults and improve our ability to design for the visually-impaired.",A Texture Analysis/Synthesis Model of Visual Crowding,7740891,R21EY019366,"['Age related macular degeneration', 'Agreement', 'Amblyopia', 'Area', 'Arts', 'Attention', 'Behavior', 'Behavioral', 'Binding', 'Cells', 'Classification', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Crowding', 'Data', 'Databases', 'Discrimination', 'Disease', 'Elderly', 'Eye Movements', 'Face', 'Failure', 'Functional disorder', 'Gender', 'Goals', 'Gray unit of radiation dose', 'Human', 'Imagery', 'Individual', 'Joints', 'Lesion', 'Letters', 'Literature', 'Location', 'Masks', 'Methods', 'Modeling', 'Nature', 'Patients', 'Perception', 'Performance', 'Peripheral', 'Process', 'Psychophysics', 'Research', 'Research Personnel', 'Resolution', 'Saccades', 'Severities', 'Stimulus', 'Techniques', 'Testing', 'Texture', 'Time', 'Training', 'Vision', 'Vision Disorders', 'Visual', 'Visual Fields', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Vocabulary', 'Work', 'base', 'clinically significant', 'computerized tools', 'design', 'experience', 'improved', 'neglect', 'novel', 'object recognition', 'public health relevance', 'research study', 'response', 'statistics', 'theories', 'tool', 'vision aid', 'visual information', 'visual search', 'visual stimulus']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R21,2009,168000,-0.003146189335077706
"Designing Visually Accessible Spaces    DESCRIPTION (provided by applicant): Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our long-term goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals and to enhance safety for the elderly and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool in which environments (such as a hotel lobby, large classroom, or hospital reception area), could be simulated with sufficient accuracy to predict the visibility of key landmarks or obstacles, such as steps or benches, under differing lighting conditions. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments"". Our research plan has four specific goals: 1) Develop methods for predicting the physical levels of light reaching the eye in existing or planned architectural spaces. 2) Acquire performance data for normally sighted subjects with visual restrictions and people with low vision to investigate perceptual capabilities critical to visually-based mobility. 3) Develop models that can predict perceptual competence on tasks critical to visually-based mobility. 4) Demonstrate a proof-of-concept software tool that operates on design models from existing architectural design systems and is able to highlight potential obstacles to visual accessibility. The lead investigators in our partnership come from three institutions: University of Minnesota Gordon Legge, Daniel Kersten; University of Utah William Thompson, Peter Shirley, Sarah Creem-Regehr; and Indiana University Robert Shakespeare. This interdisciplinary team has expertise in the four areas required for programmatic research on visual accessibility empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), photometrically correct computer graphics (Shirley), and architectural design and lighting (Shakespeare).           n/a",Designing Visually Accessible Spaces,7586102,R01EY017835,"['Accounting', 'Address', 'American', 'Area', 'Arts', 'Blindness', 'Central Scotomas', 'Characteristics', 'Classification', 'Competence', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Data', 'Detection', 'Development', 'Devices', 'Elderly', 'Engineering', 'Environment', 'Evaluation', 'Eye', 'Goals', 'Hospitals', 'Indiana', 'Individual', 'Institution', 'Lead', 'Light', 'Lighting', 'Lobbying', 'Location', 'Measurement', 'Methods', 'Minnesota', 'Modeling', 'National Eye Institute', 'Pattern', 'Perception', 'Performance', 'Peripheral', 'Photometry', 'Published Comment', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Priority', 'Safety', 'Simulate', 'Software Tools', 'Space Perception', 'Specialist', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Testing', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual impairment', 'base', 'depressed', 'design', 'innovation', 'knowledge base', 'luminance', 'model design', 'model development', 'physical model', 'predictive modeling', 'programs', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2009,522802,0.05312631660490497
"Intelligent Mobil Antiretroviral Adherence Coach    DESCRIPTION (provided by applicant): Technology Developed: We propose to develop the Intelligent Mobile Antiretroviral Adherence Coach (IMAAC), which is an easy- to-use, mobile software system for enhancing adherence to antiretroviral medication. The IMAAC, which will run on industry standard personal digital assistants (PDAs) and cell phones, combines an adherence knowledgebase founded on cognitive-behavioral principals, intelligent interaction and reasoning capabilities, and a patient model adapted for each individual user. It will learn an individual's behavioral response patterns and provide alternative solutions known to improve or enhance medication adherence. The innovation of this product is its ability to adapt to individual user characteristics and generate personalized intelligent interactive guidance based on cognitive-behavioral principles. This will enhance its utility for long-term maintenance of behavior change. The delivery of the application on standard PDAs and cell phones minimizes costs. It also helps users maintain privacy by not drawing the attention a person may get carrying a specialized device. Phase I efforts will focus on the feasibility and acceptability of the IMAAC. In Phase II, we will fully develop the system and compare its efficacy to a control group of patients receiving standard-of-care. Uses of Technology/Products/Service: At the end of 2005, an estimated 1,205,969 persons in the United States were living with HIV/AIDS. By 1998, the number of patients ever receiving ART increased to 71%, however only 53% of these individuals reported receiving antiretroviral therapy (ART) during a second follow-up interview suggesting that while most individuals receiving care for HIV in the United States have taken ART, continuing its use is problematic for a significant proportion of individuals. This is troubling because there is overwhelming evidence that ART reduces morbidity and mortality associated with HIV/AIDS. Research has found that behavioral interventions are successful at enhancing adherence to complicated antiretroviral medication regimens. One of the reasons behavioral interventions are successful is the feedback and support individuals get while they are engaging in a behavioral intervention along with information and strategies to support lifestyle changes. The problem with previous behaviorally-based programs is that they are time limited, whereas ART adherence is a long-term and life-long endeavor for individuals infected with HIV. Developing an innovative system that is based on cognitive-behavioral principles that can be tailored to an individual so that the program actually adapts to individual changes over time is likely to facilitate and maintain significant adherence among diverse HIV infected populations. Performigence Corporation's technology provides the capability to represent an adherence knowledgebase based on cognitive-behavioral principles and deliver intelligent interaction so that the user's physiological model can be understood and the guidance can be adapted through behavioral and life style intelligence. The IMAAC offers a convenient and sophisticated method for individuals to access empirically validated methods of ART adherence that are continually tailored to their needs and circumstances. Benefit to Company: Once the prototype is developed, Phase II funds will be sought after to complete development and test the efficacy of the system. With Phase I and Phase II funding from the STTR program, Performigence will be able to address ART adherence among HIV-positive individuals with a system that combines portability, intelligent interaction, ease of use, and low cost of consumer electronics with cognitive-behavioral principles. While this system is being designed to improve ART adherence among HIV-positive populations, the public health significance of this project is quite high because this system can be easily modified to address other chronic conditions that rely heavily on lifestyle changes such as diabetes and obesity. Both conditions increase morbidity and mortality across all age and ethnic groups in the United States. How Product Will Be Commercialized: Performigence's proprietary software technology will be used as the foundation for IMAAC development. The system will be sold to medical, industrial, research and consumer markets. It is anticipated that the IMAAC will have widespread uptake and use by consumers as well as physicians. PUBLIC HEALTH RELEVANCE: Developing a system that can be used with hand-held devices like telephones or personal digital assistants (PDAs) that can be tailored to an individual to provide motivational messages, behavioral cues, factual information, interactive guidance and reinforcement would likely increase antiretroviral adherence, thereby reducing morbidity and mortality associated with HIV/AIDS. Additionally, the system would be a cost-effective method to provide information to medical providers to enhance treatment and care. Modifications to such a system could be made to meet the needs of other chronic diseases that require behavioral and lifestyle changes, making the utility of the system wide-ranging.           Developing a system that can be used with hand-held devices like telephones or personal digital assistants (PDAs) that can be tailored to an individual to provide motivational messages, behavioral cues, factual information, interactive guidance and reinforcement would likely increase antiretroviral adherence, thereby reducing morbidity and mortality associated with HIV/AIDS. Additionally, the system would be a cost-effective method to provide information to medical providers to enhance treatment and care. Modifications to such a system could be made to meet the needs of other chronic diseases that require behavioral and lifestyle changes, making the utility of the system wide-ranging.",Intelligent Mobil Antiretroviral Adherence Coach,7686409,R41MH084775,"['AIDS/HIV problem', 'Address', 'Adherence', 'Age', 'Anti-Retroviral Agents', 'Applications Grants', 'Artificial Intelligence', 'Attention', 'Behavior Therapy', 'Behavioral', 'Behavioral Model', 'Boxing', 'Businesses', 'Calendar', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Chronic Disease', 'Cognitive', 'Communication', 'Computer software', 'Computerized Medical Record', 'Control Groups', 'Cues', 'Data', 'Development', 'Devices', 'Diabetes Mellitus', 'Electronics', 'Ethnic group', 'Feedback', 'Focus Groups', 'Foundations', 'Funding', 'HIV', 'HIV Seropositivity', 'Hand', 'Individual', 'Industry', 'Instruction', 'Intelligence', 'Intervention', 'Interview', 'Knowledge', 'Learning', 'Life', 'Life Style', 'Maintenance', 'Marketing', 'Medical', 'Methods', 'Modeling', 'Modification', 'Morbidity - disease rate', 'Motivation', 'Obesity', 'Operative Surgical Procedures', 'Parents', 'Participant', 'Patients', 'Pattern', 'Personal Digital Assistant', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Physiological', 'Policies', 'Population', 'Positive Reinforcements', 'Price', 'Privacy', 'Provider', 'Psychological reinforcement', 'Public Health', 'Reporting', 'Research', 'Running', 'Scheme', 'Services', 'Small Business Technology Transfer Research', 'Solutions', 'System', 'Tail', 'Technology', 'Technology Transfer', 'Telephone', 'Testing', 'Time', 'Treatment Protocols', 'United States', 'United States National Institutes of Health', 'antiretroviral therapy', 'base', 'behavior change', 'commercialization', 'cost', 'design', 'dosage', 'efficacy testing', 'experience', 'follow-up', 'improved', 'information organization', 'innovation', 'medication compliance', 'meetings', 'member', 'mortality', 'pill', 'portability', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'software systems', 'theories', 'therapy adherence', 'tool', 'uptake']",NIMH,PERFORMIGENCE CORPORATION,R41,2009,120105,-0.020745656175964564
"Accessible Artificial Intelligence Tutoring Software (Phase II SBIR)    DESCRIPTION (provided by applicant): This Phase II proposal focuses on the development of accessible artificial intelligence (AI) software for individualized tutoring and formative assessment in chemistry education. If successful, an immediate outcome will be the very first AI tutoring systems for chemistry that are accessible to blind students, delivered through the Internet. An AI tutoring methodology formulated with accessibility inherent to the design will have broad implications for the prospect of developing sophisticated accessible educational software in all content areas, beyond chemistry. Furthermore, classroom teachers can obtain individualized assessment reporting and diagnostic information for visually impaired students on demand, as if from a ""virtual teaching assistant"". Feasibility of Phase I was demonstrated by developing a prototype accessible AI tutoring program that received certification in the National Federation of the Blind's (NFB) Nonvisual Accessibility Web Application Certification Program. In previous SBIR projects, Quantum has successfully innovated new concepts in the field of AI and has developed, tested and brought to the classroom tutoring and assessment systems for science and mathematics education. Certain unique attributes of the Quantum AI Tutors make them potentially very well suited for full accessibility to the blind, as well as individuals with other print-related disabilities, using Internet- capable screen reader technology. The potential technological innovation is the development of the first advanced AI chemistry tutoring technology that has accessibility built into its framework design. Important Phase II objectives include:  Continued progress on chemistry-specific accessibility issues. Completion of full accessibility support in AI framework itself. Implementation of Braille support for chemical formulas and equations. Investigation of chemistry-specific pedagogical issues for blind students. Extension to accessible science assessment for blind/VI students, building on AI assessment technology currently under development by Quantum in other projects. Special education is a particular challenge for assessment within the No Child Left Behind legislation. Preparation for success in Phase III has already been undertaken by involving partners that are important commercially as well as technically, such as the National Federation of the Blind and the American Printing House for the Blind (APH). In addition, Quantum has long-term partnerships with McGraw-Hill and Holt, Rinehart and Winston, two of the country's leading educational publishers, as well as two additional commercial agreements with major science education supply companies, Science Kit & Boreal Laboratories and Sargent-Welch. Chemistry comprises the majority of the content standard for physical science in the National Science Education Standards, and yet is one of the most neglected areas in terms of quality educational software, in general, and is a particularly acute problem for the blind and visually impaired. Through recent federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom the very first artificial intelligence (AI) tutoring systems for chemistry. The goal of the present research is to bring the full power and benefit of this cutting-edge new educational technology to students who are blind and visually impaired using Internet-capable screen access technology.          n/a",Accessible Artificial Intelligence Tutoring Software (Phase II SBIR),7404392,R44EY016251,"['Achievement', 'Acute', 'Address', 'Agreement', 'American', 'Area', 'Artificial Intelligence', 'Categories', 'Certification', 'Chemicals', 'Chemistry', 'Child', 'Computer software', 'Country', 'Development', 'Diagnostic', 'Drug Formulations', 'Education', 'Educational Technology', 'Educational process of instructing', 'Equation', 'Evaluation', 'Feedback', 'Future', 'Goals', 'Hand', 'Home environment', 'Housing', 'Individual', 'Institution', 'Instruction', 'Internet', 'Intervention', 'Investigation', 'Laboratories', 'Left', 'Mathematics', 'Methodology', 'Mission', 'Numbers', 'Outcome', 'Performance', 'Phase', 'Philosophy', 'Preparation', 'Printing', 'Purpose', 'Reader', 'Reporting', 'Research', 'Schools', 'Science', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Software Tools', 'Special Education', 'Standards of Weights and Measures', 'Statutes and Laws', 'Students', 'Support of Research', 'System', 'Technology', 'Technology Assessment', 'Testing', 'Visual impairment', 'Work', 'blind', 'braille', 'commercialization', 'concept', 'design', 'disability', 'falls', 'high school', 'improved', 'innovation', 'neglect', 'next generation', 'physical science', 'programs', 'prototype', 'quantum', 'science education', 'simulation', 'success', 'teacher', 'technological innovation', 'virtual']",NEI,"QUANTUM SIMULATIONS, INC.",R44,2008,383858,0.0013353283620771046
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7500697,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Clutterings', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Purpose', 'Range', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2008,1146026,0.045889024392851814
"Cue Reliability and Depth Calibration During Space Perception    DESCRIPTION (provided by applicant): The long-term objective of the proposed work is to understand how learning by the visual system helps it to represent the immediate environment during perception. Because perception is accurate, we can know spatial layout: the shapes, orientations, sizes, and spatial locations of the objects and surfaces around us. But this accuracy requires that the visual system learn over time how best to interpret visual ""cues"". These cues are the signals from the environment that the visual system extracts from the retinal images that are informative about spatial layout. Known cues include binocular disparity, texture gradients, occlusion relations, motion parallax, and familiar size, to name a few. How do these cues come to be interpreted correctly? A fundamental problem is that visual cues are ambiguous. Even if cues could be measured exactly (which they cannot, the visual system being a physical device) there would still be different possible 3D interpretations for a given set of cues. As a result, the visual system is forced to operate probabilistically: the way things ""look"" to us reflects an implicit guess as to which interpretation of the cues is most likely to be correct. Each additional cue helps improve the guess. For example, the retinal image of a door could be interpreted as a vertical rectangle or as some other quadrilateral at a non-vertical orientation in space, and the shadow cues at the bottom of the door helps the system know that it's a vertical rectangle. What mechanisms do the visual system use to discern which cues are available for interpreting images correctly? The proposed work aims to answer this fundamental question about perceptual learning. It was recently shown that the visual system can detect and start using new cues for perception. This phenomenon can be studied in the laboratory using classical conditioning procedures that were previously developed to study learning in animals. In the proposed experiments, a model system is used to understand details about when this learning occurs and what is learned. The data will be compared to predictions based on older, analogous studies in the animal learning literature, and interpreted in the context of Bayesian statistical inference, especially machine learning theory. The proposed work benefits public health by characterizing the brain mechanisms that keep visual perception accurate. These mechanisms are at work in the many months during which a person with congenital cataracts learns to use vision after the cataracts are removed, and it is presumably these mechanisms that go awry when an individual with a family history of synesthesia or autism develops anomalous experience-dependent perceptual responses. Neurodegenerative diseases may disrupt visual learning, in which case visual learning tests could be used to detect disease; understanding the learning of new cues in human vision could lead to better computerized aids for the visually impaired; and knowing what causes a new cue to be learned could lead to new technologies for training people to perceive accurately in novel work environments.          n/a",Cue Reliability and Depth Calibration During Space Perception,7388324,R01EY013988,"['Address', 'Adult', 'Animal Behavior', 'Animals', 'Appearance', 'Autistic Disorder', 'Binocular Vision', 'Biological Models', 'Brain', 'Calibration', 'Cataract', 'Computer Vision Systems', 'Condition', 'Cues', 'Data', 'Depth', 'Devices', 'Diagnosis', 'Disease', 'Environment', 'Experimental Designs', 'Family history of', 'Food', 'Funding', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Learning Disabilities', 'Literature', 'Location', 'Longevity', 'Machine Learning', 'Measures', 'Memory', 'Motion', 'Motion Perception', 'Names', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathology', 'Perception', 'Perceptual learning', 'Persons', 'Positioning Attribute', 'Primates', 'Procedures', 'Process', 'Public Health', 'Rate', 'Recruitment Activity', 'Research', 'Retinal', 'Reversal Learning', 'Rotation', 'Shadowing (Histology)', 'Shapes', 'Signal Transduction', 'Source', 'Space Perception', 'Stimulus', 'Surface', 'System', 'Testing', 'Texture', 'Time', 'Training', 'Translations', 'Trust', 'Ursidae Family', 'Vision', 'Vision Disparity', 'Visual', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'Workplace', 'area MT', 'base', 'classical conditioning', 'clinical application', 'computerized', 'concept', 'congenital cataract', 'design', 'devices for the visually impaired', 'experience', 'improved', 'neuromechanism', 'new technology', 'novel', 'programs', 'relating to nervous system', 'research study', 'response', 'size', 'stereoscopic', 'theories', 'tool', 'visual information', 'visual learning', 'visual process', 'visual processing']",NEI,STATE COLLEGE OF OPTOMETRY,R01,2008,228847,0.004071114958156312
"Low-Cost Electronic Nose for Groundwater Contaminants    DESCRIPTION (provided by applicant): Several US agencies and regulators require low-cost chemical sensors for detecting and monitoring environmental clean-up, remediation, and decommissioning processes where groundwater may be contaminated. The sensors must be capable of detecting contaminants in the sub-surface groundwater and must be compatible with use in a range of environments. Most significantly, these customers require a low-cost alternative to its current expensive and labor intensive methods, namely using mobile laboratories. The project will result in the innovative use of low-cost sensor systems that will be capable of detecting and monitoring for dense non-aqueous phase liquids in the subsurface and groundwater, unattended, and in real- time from within a push-probe, using a chemicapacitor array and miniature preconcentrator. Seacoast's Phase I research will focus on developing the sensor array, demonstrating sensitivity to chlorinated hydrocarbons at relevant concentrations, and field tests in actual contaminated sites. The ultimate goal is to provide the DOD, DOE, NIEHS and other agencies with a method to map and track subsurface contamination plumes in real-time without requiring an operator. The systems will have MEMS microcapacitor sensor arrays that can monitor for leaks of toxic chemicals, contaminants from wastes, and changes in groundwater streams. A preconcentrator collects the contaminants and releases them to a microsensor array. The sensor arrays are filled with several chemoselective polymers whose dielectric permittivity changes when exposed to different vapors, creating a fingerprint response for each chemical. An array of differently responding sensors and pattern recognition can thereby compensate for changes in humidity, temperature, and composition. These low-power systems can be left unattended and transmit data wirelessly or through USB to a central location. The most important application to public health and safety is unattended monitoring of drinking water, water treatment processes, and water sources. The potential commercial markets include building chemical process monitoring and control, toxic vapor leak detection, industrial process control, and industrial health and safety. Transitioning the developed prototype to other markets where worker and public health, environmental health and regulatory compliance will be investigated to reduce the financial risks and broaden the acceptance of the technology. PUBLIC HEALTH RELEVANCE: This proposal will describe a potential method to specifically address the need for detecting groundwater contaminants and long-term monitoring of contaminated sites, by providing an unattended sensor system that tracks contamination in real-time and transmits contaminant concentrations. Such a system would be used in tandem with other methods, to provide comprehensive contamination management at DOE, DOD, and Superfund sites where ground and water clean-up projects are already underway. The proposed work will focus on detection of chlorinated hydrocarbons, which are described as among the most common pollutants in groundwater and soils at DOE sites.          n/a",Low-Cost Electronic Nose for Groundwater Contaminants,7537117,R43ES016941,"['Address', 'Algorithms', 'Characteristics', 'Chemicals', 'Chlorinated Hydrocarbons', 'Classification', 'Collection', 'Compatible', 'Condition', 'Data', 'Data Collection', 'Detection', 'Electronics', 'Engineering', 'Environment', 'Environmental Health', 'Environmental Monitoring', 'Feedback', 'Fingerprint', 'Fluorescence', 'Goals', 'Humidity', 'Industrial Health', 'Laboratories', 'Lasers', 'Left', 'Liquid substance', 'Location', 'Machine Learning', 'Maps', 'Marketing', 'Measures', 'Methods', 'Modification', 'Monitor', 'Nose', 'Numbers', 'Pattern Recognition', 'Phase', 'Poison', 'Polymers', 'Process', 'Public Health', 'Pump', 'ROC Curve', 'Range', 'Rate', 'Recommendation', 'Research', 'Risk', 'Safety', 'Sampling', 'Science', 'Simulate', 'Site', 'Soil', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Stream', 'Surface', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Trichloroethylene', 'Water', 'Work', 'aqueous', 'base', 'cold temperature', 'computerized data processing', 'cost', 'cost effectiveness', 'design', 'detector', 'drinking water', 'ground water', 'innovation', 'membrane assembly', 'pollutant', 'prototype', 'remediation', 'response', 'sensor', 'superfund site', 'vapor', 'wasting', 'water treatment']",NIEHS,"SEACOAST SCIENCE, INC.",R43,2008,97690,0.024193340664906495
"A Non-Document Text and Display Reader for Visually Impaired Persons    DESCRIPTION (provided by applicant): The goal of this project is to develop a computer vision system based on standard camera cell phones to give blind and visually impaired persons the ability to read appliance displays and similar forms of non-document visual information. This ability is increasingly necessary to use everyday appliances such as microwave ovens and DVD players, and to perform many daily activities such as counting paper money. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed software runs on a standard, off-the- shelf camera phone and uses computer vision algorithms to analyze images taken by the user, to detect and read the text within each image, and to then read it aloud using synthesized speech. Preliminary feasibility studies indicate that current cellular phones easily exceed the minimum processing power required for these tasks. Initially, the software will read out three categories of symbols: LED/LCD appliance displays, product or user-defined barcodes, and denominations of paper money. Ultimately these functions will be integrated with other capabilities being developed under separate funding, such as reading a broad range of printed text (including signs), recognizing objects, and analyzing photographs and graphics, etc., all available as free or low-cost software downloads for any cell phone user. Our specific goals are to (1) gather a database of real images taken by blind and visually impaired persons of a variety of LED/LCD appliance displays, barcodes and US paper currency; (2) develop algorithms to process the images and extract the desired information; (3) implement the algorithms on a camera phone; and (4) conduct user testing to establish design parameters and optimize the human interface. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays, and to other types of text and symbolic information hitherto unaddressed by rehabilitation technology. The proposed research would result in an assistive technology system (with zero or minimal cost to users) to provide increased access to such display and non- document text information for the approximately 10 million Americans with significant vision impairments or blindness.          n/a",A Non-Document Text and Display Reader for Visually Impaired Persons,7446299,R01EY018890,"['Access to Information', 'Acoustics', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Blindness', 'Categories', 'Cellular Phone', 'Code', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Count', 'Custom', 'Daily', 'Data', 'Databases', 'Development', 'Devices', 'Economics', 'Electronics', 'Employment', 'Evaluation', 'Feasibility Studies', 'Figs - dietary', 'Funding', 'Goals', 'Hand', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Lavandula', 'Mainstreaming', 'Marketing', 'Memory', 'Modification', 'Paper', 'Printing', 'Process', 'Public Health', 'Range', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'Standards of Weights and Measures', 'Surveys', 'System', 'Telephone', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visit', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Voice', 'Workplace', 'base', 'blind', 'consumer product', 'cost', 'design', 'desire', 'image processing', 'microwave electromagnetic radiation', 'optical character recognition', 'prevent', 'programs', 'rehabilitation technology', 'response', 'time interval', 'tool', 'trafficking', 'visual information']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2008,421791,0.035000264982155325
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to build and test a ""Smart Telescope,"" a device for persons with low vision that uses computer vision algorithms to search for, detect and enhance targets such as text and faces to aid in everyday tasks such as travel, navigation and social interactions. The practical, cosmetically acceptable packaging will consist of a miniature camera and visual display discreetly mounted on spectacles or a hat, and a compact computing device and set of controls that fit into a pocket. The Smart Telescope advances today's state of the art in assistive devices for low vision by automatically searching for, detecting and enhancing target objects even when they fill only a small portion of the device's field of view, without the user having to point the device directly or accurately at the target as with optical telescopes. The Smart Telescope is small and lightweight, but large enough for the elderly to handle and control; simple to operate and easy to carry, store, recharge, don and remove. Advanced options are hidden during day-to-day use, but easy to access when necessary. In Phase I, we developed and evaluated a working prototype and received enthusiastic feedback from subjects in our target population. In Phase II we propose to prototype a commercially viable consumer version of the Smart Telescope. The Phase II work plan has four tracks: 1) User interaction and interface design, 2) physical design and configuration, 3) software design and development, and 4) hardware design and development. Smith-Kettlewell's Rehabilitation Engineering Research Center (RERC) will provide expertise for the human factors portions of the project. Blindsight will design and build the device hardware from off-the-shelf components with the help of Bolton Engineering. Low vision experts Drs. Don Fletcher, Melissa Chun and Ian Bailey will work with the RERC to guarantee a practical product for the target audience. The overall aim is to create a commercial version of the proposed device for persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems, increasing mobility and independence for those with acuity between approximately 20/200 and 20/600. At under $1,000, the total market for such a device is estimated at up to 300,000, i.e., 10% of low vision persons in the United States. The commercial version of the Smart Telescope will significantly increase mobility and independence for persons with visual acuity between approximately 20/200 and 20/600, aiding them in everyday tasks such as travel, navigation, and social interactions. It will advance today's state of the art in assistive devices for low vision by improving on and surpassing the capabilities of the traditional optical telescope, greatly benefiting persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems.          n/a",A Smart Telescope for Low Vision,7486800,R44EY014487,"['Algorithms', 'Arts', 'Cataract', 'Computer Vision Systems', 'Contrast Sensitivity', 'Development', 'Devices', 'Diabetic Retinopathy', 'Elderly', 'Engineering', 'Eye', 'Eyeglasses', 'Face', 'Feedback', 'Glaucoma', 'Human', 'Macular degeneration', 'Marketing', 'Melissa', 'Optics', 'Persons', 'Phase', 'Research', 'Self-Help Devices', 'Social Interaction', 'Software Design', 'Target Populations', 'Testing', 'Text', 'Today', 'Travel', 'United States', 'Vision', 'Visual', 'Visual Acuity', 'Visual impairment', 'Work', 'day', 'design', 'improved', 'low vision telescope', 'prototype', 'rehabilitation engineering']",NEI,BLINDSIGHT CORPORATION,R44,2008,434041,0.029216514960113363
"Webcam Interface for Audio/touch Graphics Access by Blind People    DESCRIPTION (provided by applicant):  The goal of this project is to develop a compact inexpensive alternative to the bulky expensive touchpads now required by blind people for audio/touch access to graphical information. Audio/touch is known to provide excellent access to computer-literate blind people as well as people with dyslexia or other severe print disabilities. Preparing Audio/touch materials was very expensive until ViewPlus introduced the IVEO Scalable Vector Graphic (SVG) Authoring/conversion software in 2005. IVEO permits virtually any graphical information to be created or converted/imported easily to a well- structured highly accessible SVG format. Tactile copy was also very expensive before 2000 when ViewPlus introduced the Tiger embossing Windows printers that ""print"" by embossing. The new ViewPlus Emprint printer/embossers emboss and also print color images, creating color tactile images particularly useful for people with dyslexia and a number of other print disabilities. An audio/touch user reads an IVEO SVG graphic using the free IVEO Viewer, a tactile copy of the image, and a touchpad. The user places the tactile graphic on the touchpad and presses a point of interest. The touchpad communicates the position of that point back to the computer, and the IVEO Viewer speaks the appropriate information. Tactile text made from mainstream graphics has a distinctive pattern. When a user presses, that text is spoken by the IVEO Viewer. When the user presses a graphic object having a SVG title within the file, that title will be spoken. Objects may also have arbitrarily long description fields that can be spoken and browsed. All spoken information can be displayed on an attached braille display if desired. Graphical information is ubiquitous today, but almost none is accessible to blind people. Government agencies, libraries, companies, and agencies serving people with disabilities could easily send highly accessible IVEO graphics files and tactile graphic copies to clients with disabilities, but there is a ""chicken and egg"" dilemma that must be overcome before they are likely to do so. Few blind people have a touchpad (which cost $500 or more), so few could use that information. The specific aim of this Phase I proposal is to develop an affordable webcam-based prototype as an alternative to touchpads. It is based on an inexpensive webcam that is focused on the graphic and follows a finger. A touchpad press is emulated in this prototype by pressing some computer key with the other hand. This project could be the key to bringing accessible graphics to all blind computer users and is clearly of interest to NEI whose mission statement includes mental health and quality of life of blind people. PUBLIC HEALTH RELEVANCE:  This proposal is relevant to the mission of the National Eye Institute, because it could be the key to making nearly all graphical information easily accessible to people who are blind or have other severe print disabilities. Graphical information is ubiquitous in the world today but is not presently accessible to blind people except through expensive and time-consuming conversion by trained transcribers. Making all graphical information accessible would have an obviously highly beneficial direct effect on education and professional opportunities, mental health, and quality of life of blind people. Mental health and quality of life issues for blind people are parts of the mission of the National Eye Institute.          n/a",Webcam Interface for Audio/touch Graphics Access by Blind People,7480812,R43EY018973,"['Back', 'Braille Display', 'Businesses', 'Chickens', 'Client', 'Color', 'Communities', 'Computer Vision Systems', 'Computer software', 'Computers', 'Consultations', 'Development', 'Devices', 'Disabled Persons', 'Dyslexia', 'Event', 'Fingers', 'Goals', 'Government Agencies', 'Hand', 'Home environment', 'Image', 'Information Systems', 'Institution', 'Internet', 'Libraries', 'Link', 'Mainstreaming', 'Marketing', 'Mental Health', 'Methods', 'Mission', 'Modeling', 'Mus', 'National Eye Institute', 'Numbers', 'Oregon', 'Pattern', 'Phase', 'Positioning Attribute', 'Printing', 'Professional Education', 'Public Health', 'Publications', 'Quality of life', 'Range', 'Reading', 'Site', 'Structure', 'Structure of nail of finger', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Text', 'Tigers', 'Time', 'Title', 'Today', 'Touch sensation', 'Training', 'Universities', 'Visual', 'Visually Impaired Persons', 'base', 'blind', 'braille', 'cost', 'desire', 'digital', 'disability', 'egg', 'interest', 'literate', 'print disabilities', 'programs', 'prototype', 'research and development', 'tool', 'touchpad', 'vector']",NEI,"VIEWPLUS TECHNOLOGIES, INC.",R43,2008,100001,0.012062162817820379
"Indoor Magnetic Wayfinding For The Visually Impaired    DESCRIPTION (provided by applicant): Advanced Medical Electronics (AME) proposes the development of an indoor way-finding device utilizing the unique magnetic anomaly patterns that exist in modern, man-made structures. The proposed system will record the magnitude of magnetic field strength from sensors in three orthogonal axes. The time history of these magnetic data points can be continuously compared with an electronic map of magnetic anomalies (or, ""signature"") to determine current position within a building. The phase I developed prototype system tracked in feasibility experiments with an accuracy of 1 foot (radius). Magnetic anomalies render a magnetic compass useless for finding a directional bearing. However, these same invisible anomalies represent valuable, unique indoor terrain features measurable by magnetic sensors located inside a small, portable device. Such a device would be able to provide low-vision users with a valuable indoor low-cost way-finding tool analogous to a Global Positioning System (GPS) device used outdoors. About 3.7 million Americans are visually disabled. Of these, 200,000 are blind, and the rest have low vision. The key advantage of the way-finding concept presented in this proposal, over other methods, is that the benefits are made available to the visually impaired community without requiring expensive building infrastructure investments. This is of particular advantage to large government buildings and educational campuses. The proposed approach allows a cost effective solution to way-finding within these buildings.          n/a",Indoor Magnetic Wayfinding For The Visually Impaired,7477498,R44EY015616,"['American', 'Appointment', 'Building Codes', 'Cognition', 'Communities', 'Computer Vision Systems', 'Computers', 'Data', 'Development', 'Devices', 'Disabled Persons', 'Doctor of Philosophy', 'Education', 'Electronics', 'Engineering', 'Fee-for-Service Plans', 'Funding', 'Government', 'Hand', 'Housing', 'Human', 'Indoor Magnetic Wayfinding', 'Investments', 'Joints', 'Label', 'Location', 'Magnetism', 'Maps', 'Measurable', 'Measurement', 'Measures', 'Medical Electronics', 'Minnesota', 'Modeling', 'Modification', 'Neurosciences', 'Numbers', 'Oceans', 'Pattern', 'Pattern Recognition', 'Pennsylvania', 'Persons', 'Phase', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychology', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Rest', 'Services', 'Silicon Dioxide', 'Solutions', 'Somatotype', 'Speech', 'Steel', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Today', 'Universities', 'Vision', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Wireless Technology', 'World Health Organization', 'base', 'blind', 'college', 'computer science', 'concept', 'cost', 'cost effective', 'court', 'design', 'digital', 'foot', 'human subject', 'innovation', 'interest', 'magnetic field', 'miniaturize', 'motor control', 'performance tests', 'professor', 'prototype', 'radius bone structure', 'research study', 'sensor', 'sensory integration', 'tool', 'way finding']",NEI,ADVANCED MEDICAL ELECTRONICS CORPORATION,R44,2008,365248,0.03517211314249582
"Designing Visually Accessible Spaces    DESCRIPTION (provided by applicant): Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our long-term goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals and to enhance safety for the elderly and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool in which environments (such as a hotel lobby, large classroom, or hospital reception area), could be simulated with sufficient accuracy to predict the visibility of key landmarks or obstacles, such as steps or benches, under differing lighting conditions. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments"". Our research plan has four specific goals: 1) Develop methods for predicting the physical levels of light reaching the eye in existing or planned architectural spaces. 2) Acquire performance data for normally sighted subjects with visual restrictions and people with low vision to investigate perceptual capabilities critical to visually-based mobility. 3) Develop models that can predict perceptual competence on tasks critical to visually-based mobility. 4) Demonstrate a proof-of-concept software tool that operates on design models from existing architectural design systems and is able to highlight potential obstacles to visual accessibility. The lead investigators in our partnership come from three institutions: University of Minnesota Gordon Legge, Daniel Kersten; University of Utah William Thompson, Peter Shirley, Sarah Creem-Regehr; and Indiana University Robert Shakespeare. This interdisciplinary team has expertise in the four areas required for programmatic research on visual accessibility empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), photometrically correct computer graphics (Shirley), and architectural design and lighting (Shakespeare).           n/a",Designing Visually Accessible Spaces,7351808,R01EY017835,"['Accounting', 'Address', 'American', 'Area', 'Arts', 'Blindness', 'Characteristics', 'Classification', 'Competence', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Condition', 'Contrast Sensitivity', 'Data', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Elderly', 'Engineering', 'Environment', 'Evaluation', 'Eye', 'Goals', 'Hospitals', 'Indiana', 'Individual', 'Institution', 'Lead', 'Light', 'Lighting', 'Lobbying', 'Location', 'Measurement', 'Methods', 'Minnesota', 'Modeling', 'National Eye Institute', 'Pattern', 'Perception', 'Performance', 'Peripheral', 'Photometry', 'Published Comment', 'Range', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Priority', 'Safety', 'Simulate', 'Software Tools', 'Space Models', 'Space Perception', 'Specialist', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Testing', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual impairment', 'base', 'concept', 'design', 'innovation', 'knowledge base', 'luminance', 'model design', 'model development', 'physical model', 'predictive modeling', 'programs', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2008,482736,0.05312631660490497
"Accessible Artificial Intelligence Tutoring Software (Phase II SBIR)    DESCRIPTION (provided by applicant): This Phase II proposal focuses on the development of accessible artificial intelligence (AI) software for individualized tutoring and formative assessment in chemistry education. If successful, an immediate outcome will be the very first AI tutoring systems for chemistry that are accessible to blind students, delivered through the Internet. An AI tutoring methodology formulated with accessibility inherent to the design will have broad implications for the prospect of developing sophisticated accessible educational software in all content areas, beyond chemistry. Furthermore, classroom teachers can obtain individualized assessment reporting and diagnostic information for visually impaired students on demand, as if from a ""virtual teaching assistant"". Feasibility of Phase I was demonstrated by developing a prototype accessible AI tutoring program that received certification in the National Federation of the Blind's (NFB) Nonvisual Accessibility Web Application Certification Program. In previous SBIR projects, Quantum has successfully innovated new concepts in the field of AI and has developed, tested and brought to the classroom tutoring and assessment systems for science and mathematics education. Certain unique attributes of the Quantum AI Tutors make them potentially very well suited for full accessibility to the blind, as well as individuals with other print-related disabilities, using Internet- capable screen reader technology. The potential technological innovation is the development of the first advanced AI chemistry tutoring technology that has accessibility built into its framework design. Important Phase II objectives include:  Continued progress on chemistry-specific accessibility issues. Completion of full accessibility support in AI framework itself. Implementation of Braille support for chemical formulas and equations. Investigation of chemistry-specific pedagogical issues for blind students. Extension to accessible science assessment for blind/VI students, building on AI assessment technology currently under development by Quantum in other projects. Special education is a particular challenge for assessment within the No Child Left Behind legislation. Preparation for success in Phase III has already been undertaken by involving partners that are important commercially as well as technically, such as the National Federation of the Blind and the American Printing House for the Blind (APH). In addition, Quantum has long-term partnerships with McGraw-Hill and Holt, Rinehart and Winston, two of the country's leading educational publishers, as well as two additional commercial agreements with major science education supply companies, Science Kit & Boreal Laboratories and Sargent-Welch. Chemistry comprises the majority of the content standard for physical science in the National Science Education Standards, and yet is one of the most neglected areas in terms of quality educational software, in general, and is a particularly acute problem for the blind and visually impaired. Through recent federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom the very first artificial intelligence (AI) tutoring systems for chemistry. The goal of the present research is to bring the full power and benefit of this cutting-edge new educational technology to students who are blind and visually impaired using Internet-capable screen access technology.          n/a",Accessible Artificial Intelligence Tutoring Software (Phase II SBIR),7220194,R44EY016251,"['Achievement', 'Acute', 'Address', 'Agreement', 'American', 'Area', 'Artificial Intelligence', 'Categories', 'Certification', 'Chemicals', 'Chemistry', 'Child', 'Computer software', 'Country', 'Development', 'Diagnostic', 'Drug Formulations', 'Education', 'Educational Technology', 'Educational process of instructing', 'Equation', 'Evaluation', 'Feedback', 'Future', 'Goals', 'Hand', 'Home environment', 'Housing', 'Individual', 'Institution', 'Instruction', 'Internet', 'Intervention', 'Investigation', 'Laboratories', 'Left', 'Mathematics', 'Methodology', 'Mission', 'Numbers', 'Outcome', 'Performance', 'Phase', 'Philosophy', 'Preparation', 'Printing', 'Purpose', 'Reader', 'Reporting', 'Research', 'Schools', 'Science', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Software Tools', 'Special Education', 'Standards of Weights and Measures', 'Statutes and Laws', 'Students', 'Support of Research', 'System', 'Technology', 'Technology Assessment', 'Testing', 'Visual impairment', 'Work', 'blind', 'braille', 'commercialization', 'concept', 'design', 'disability', 'falls', 'high school', 'improved', 'innovation', 'neglect', 'next generation', 'physical science', 'programs', 'prototype', 'quantum', 'science education', 'simulation', 'success', 'teacher', 'technological innovation', 'virtual']",NEI,"QUANTUM SIMULATIONS, INC.",R44,2007,366168,0.0013353283620771046
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7172503,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Clutterings', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Purpose', 'Range', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2007,1159531,0.045889024392851814
"Home Sensor Date Fusion to Support Aging in Place    DESCRIPTION (provided by applicant): The aging of the U.S. population presents many challenges. The existing paradigm of care will not allocate resources efficiently as the size of the population requiring home care assistance grows. Our objective is to enhance elder independence by providing both better and more timely predictive health-status assessments and direct, real-time, recommendations and warnings. These together will lower the risk of elders remaining at home or in low intensity care settings. The objective of the Phase II research and development effort is to develop technology that can detect and track activities in the home environment and to demonstrate its usefulness in allowing elders to remain in their homes longer than is now possible. CleverSet will develop and deploy a prototype CleverSet Activity Tracker, CAT, that processes data from a robust set of simple sensors to (1) track the activities of daily living over time (2) modify these tracked activities to include uncertainty about the environment and risk to produce notifications of Events Requiring Intervention (ERIs); and (3) demonstrate the results of the models. The technological innovation of the proposed work is the application of dynamic relational Bayesian networks (DRBNs) to activities in the home environment. CleverSet's DRBN algorithms collectively referred to as CleverSet Modeler, exploit the data model and meta-data from the schema to guide and frame relational queries about behavior and events. In the proposed work, DRBNs will be used to represent complex, dynamic, multi-scale processes involving multiple actors, as probability distributions over the elements, queries, and relationships in the DRBN model. Activities of daily living (ADLs) will be identified using DRBN machine learning algorithms from sensor data and tracked through time. Short-term rhythms of daily life as well as longer-term transitions will be tracked. Risk modifiers relevant to elders will be integrated into the model and used to adapt the sensor data input. Sensor studies will also be performed to determine the relative contribution of sensors to the DRBN ADL models. A software prototype integrating the elements of the Phase II effort will be developed.         n/a",Home Sensor Date Fusion to Support Aging in Place,7287365,R44AG024687,"['Activities of Daily Living', 'Address', 'Aging', 'Algorithms', 'Behavior', 'Caregivers', 'Caring', 'Case Manager', 'Communities', 'Complex', 'Computer software', 'Computers', 'Contracts', 'Copyright', 'Daily', 'Data', 'Detection', 'Drops', 'Elderly', 'Elements', 'Environment', 'Equilibrium', 'Event', 'Family', 'Family member', 'Goals', 'Health Status', 'Home Care Services', 'Home environment', 'Household', 'Intervention', 'Licensing', 'Life', 'Machine Learning', 'Maintenance', 'Marketing', 'Methodology', 'Modeling', 'Monitor', 'Notification', 'Outsourcing', 'Phase', 'Placement', 'Population', 'Population Sizes', 'Privacy', 'Probability', 'Process', 'Recommendation', 'Relative (related person)', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Assessment', 'Security', 'Services', 'Site', 'Staging', 'Stream', 'System', 'Technology', 'Time', 'Uncertainty', 'Work', 'base', 'commercial application', 'computer based statistical methods', 'computerized data processing', 'cost', 'data modeling', 'network models', 'patient home care', 'programs', 'prototype', 'research and development', 'sensor', 'technological innovation', 'tool']",NIA,"CLEVERSET, INC.",R44,2007,378793,-0.021351204187241436
"The formation of visual objects    DESCRIPTION (provided by applicant): Perceptual grouping is the process by which the initially raw and inchoate visual image is organized into perceptual ""objects"". What spatial factors induce perceptual grouping? What is the sequence of computations whereby the image is progressively organized? One source of difficulty in modeling this process is that, unlike many aspects of early vision, perceptual grouping inherently involves non-local: computations - integration of cues from potentially distant locations in the image. Another difficulty in understanding perceptual grouping has been the lack of objective and temporally precise methods for actually measuring the observer's subjective organization of an image. This proposal seeks to combine (a) recent advances in understanding the non-local computations involved in perceptual grouping with (b) novel experimental methods for determining subjective organization. The experimental methods are based on the finding that perceptual objects enjoy certain objectively measurable benefits, including more efficient visual comparisons within them than between distinct objects. This proposal seeks to use this effect to discover what the visual system in fact treats as a perceptual object, and how this percept develops over the course of processing. Most of the proposed experiments involve carefully constructed artificial stimuli with various grouping cues in force, designed to allow detailed comparisons of the strength, interaction, and time-course of each potential grouping cue. In addition, several experiments involve natural images, in order to uncover how perceptual organization proceeds under more naturalistic conditions. This research may lead to technological advancement in the area of computer vision, as well as to better understanding of disorders of perceptual organization such as visual agnosia and dyslexia.         n/a",The formation of visual objects,7194202,R01EY015888,"['Agnosia', 'Area', 'Awareness', 'Color', 'Communication', 'Computer Vision Systems', 'Condition', 'Conscious', 'Cues', 'Development', 'Disease', 'Distant', 'Dyslexia', 'Elements', 'Goals', 'Grouping', 'Image', 'Lateral', 'Lead', 'Literature', 'Location', 'Measurable', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Motion', 'Nature', 'Neighborhoods', 'Paint', 'Perception', 'Process', 'Rate', 'Research', 'Research Personnel', 'Source', 'Staging', 'Stimulus', 'Structure', 'Textbooks', 'Texture', 'Time', 'Vision', 'Visual', 'Visual Fields', 'Visual system structure', 'base', 'design', 'interest', 'millisecond', 'neurophysiology', 'novel', 'perceptual organization', 'receptive field', 'relating to nervous system', 'research study', 'visual process', 'visual processing']",NEI,RUTGERS THE ST UNIV OF NJ NEW BRUNSWICK,R01,2007,216928,-0.005791752915079041
"Wayfinding for the blind & visually impaired using passive environmental labels    DESCRIPTION (provided by applicant): The objective of this proposal is to tackle the problem of way finding (finding one's way in an environment), faced by blind and severely visually impaired persons who are unable to find or read signs, landmarks and locations. We propose a novel and very inexpensive environmental labeling system to provide this population with access to information needed for indoor way finding (where GPS is not available). The system uses simple passive landmark symbols printed on paper or other material, placed next to text, Braille signs or barcode at locations of interest (offices, bathrooms, etc.) in an environment such as an office building. These printed patterns contain spatial and semantic information that is detected using computer vision algorithms running on a standard camera cell phone. By scanning the environment with the device, which detects all landmark symbols in its line of sight up to distances of 10 meters, the user can determine his or her approximate location in the environment as well as the information encoded near each landmark symbol. The system extracts this information in real-time and communicates it to the user by sound, synthesized speech and/or tactile feedback. This information includes spatial (e.g. audio tones to indicate the presence and direction of a label in the camera's field of view) and semantic information (""Mr. Johnson's office, room 429, at 11 o'clock""). The research proposed here will produce a prototype system that will be tested by blind and low vision subjects. Our team includes a blind expert on psychoacoustics (and other in-house blind staff) and an expert consultant on low-vision way finding and navigation to help optimize the user interface and guide development into a practical, easy-to-use system.              n/a",Wayfinding for the blind & visually impaired using passive environmental labels,7295688,R21EY017003,"['Access to Information', 'Address', 'Algorithms', 'Auditory', 'Bar Codes', 'Canes', 'Canis familiaris', 'Cellular Phone', 'Clutterings', 'Cognitive', 'Color', 'Complement component C1s', 'Computer Vision Systems', 'Computer software', 'Condition', 'Consultations', 'Databases', 'Detection', 'Development', 'Devices', 'Education', 'Elderly', 'Employment', 'Environment', 'Exhibits', 'Feedback', 'Future', 'Goals', 'Home environment', 'Housing', 'Image', 'Individual', 'Instruction', 'Label', 'Localized', 'Location', 'Modality', 'Museums', 'Paper', 'Pattern', 'Persons', 'Population', 'Printing', 'Psychoacoustics', 'Quality of life', 'Range', 'Rate', 'Reading', 'Research', 'Running', 'Scanning', 'Semantics', 'Shapes', 'Source', 'Speech', 'Standards of Weights and Measures', 'Stress', 'System', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'age group', 'base', 'blind', 'braille', 'concept', 'cost', 'design', 'interest', 'legally blind', 'meter', 'novel', 'optical character recognition', 'programs', 'prototype', 'research study', 'size', 'skills', 'sound', 'success', 'symposium', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2007,193398,0.02283525058172149
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to build and test a ""Smart Telescope,"" a device for persons with low vision that uses computer vision algorithms to search for, detect and enhance targets such as text and faces to aid in everyday tasks such as travel, navigation and social interactions. The practical, cosmetically acceptable packaging will consist of a miniature camera and visual display discreetly mounted on spectacles or a hat, and a compact computing device and set of controls that fit into a pocket. The Smart Telescope advances today's state of the art in assistive devices for low vision by automatically searching for, detecting and enhancing target objects even when they fill only a small portion of the device's field of view, without the user having to point the device directly or accurately at the target as with optical telescopes. The Smart Telescope is small and lightweight, but large enough for the elderly to handle and control; simple to operate and easy to carry, store, recharge, don and remove. Advanced options are hidden during day-to-day use, but easy to access when necessary. In Phase I, we developed and evaluated a working prototype and received enthusiastic feedback from subjects in our target population. In Phase II we propose to prototype a commercially viable consumer version of the Smart Telescope. The Phase II work plan has four tracks: 1) User interaction and interface design, 2) physical design and configuration, 3) software design and development, and 4) hardware design and development. Smith-Kettlewell's Rehabilitation Engineering Research Center (RERC) will provide expertise for the human factors portions of the project. Blindsight will design and build the device hardware from off-the-shelf components with the help of Bolton Engineering. Low vision experts Drs. Don Fletcher, Melissa Chun and Ian Bailey will work with the RERC to guarantee a practical product for the target audience. The overall aim is to create a commercial version of the proposed device for persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems, increasing mobility and independence for those with acuity between approximately 20/200 and 20/600. At under $1,000, the total market for such a device is estimated at up to 300,000, i.e., 10% of low vision persons in the United States. The commercial version of the Smart Telescope will significantly increase mobility and independence for persons with visual acuity between approximately 20/200 and 20/600, aiding them in everyday tasks such as travel, navigation, and social interactions. It will advance today's state of the art in assistive devices for low vision by improving on and surpassing the capabilities of the traditional optical telescope, greatly benefiting persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems.          n/a",A Smart Telescope for Low Vision,7327116,R44EY014487,"['Algorithms', 'Arts', 'Back', 'Cataract', 'Clutterings', 'Computer Vision Systems', 'Contrast Sensitivity', 'Development', 'Devices', 'Diabetic Retinopathy', 'Elderly', 'Engineering', 'Eye', 'Eyeglasses', 'Face', 'Feedback', 'Glaucoma', 'Human', 'Lighting', 'Location', 'Macular degeneration', 'Marketing', 'Melissa', 'Motion', 'Optics', 'Peripheral', 'Persons', 'Phase', 'Reading', 'Research', 'Self-Help Devices', 'Social Interaction', 'Software Design', 'Target Populations', 'Testing', 'Text', 'Today', 'Travel', 'United States', 'Vision', 'Visual', 'Visual Acuity', 'Visual impairment', 'Work', 'day', 'design', 'improved', 'low vision telescope', 'monocular', 'prototype', 'rehabilitation engineering']",NEI,BLINDSIGHT CORPORATION,R44,2007,448477,0.029216514960113363
"A General Model of Saccadic Selectivity in Visual Search    DESCRIPTION (provided by applicant): Many of our everyday tasks, such as spotting a friend in a crowd or picking a bottle of soda from the refrigerator, require us to perform visual search. Due to this ubiquity of visual search, its study promises to shed light on the fundamental processes that control our visual attention so efficiently in natural tasks. To quantitatively assess search behavior, previous research using simple, artificial displays has employed eye-movement recording to analyze saccadic selectivity, that is, the bias of saccadic endpoints (""landing points"" of eye movements) towards display items that share certain features with the search target. Recently, saccadic selectivity in natural, complex displays has been examined as well (Pomplun, 2006), giving a first insight into eye-movement control as it is performed during everyday tasks. The aim of this project is to devise, implement, and evaluate a general, computational model of saccadic selectivity in visual search tasks. Due to its quantitative nature, absence of freely adjustable parameters, and support from empirical research results, the Area Activation Model (Pomplun, Shen, & Reingold, 2003) is a promising starting point for developing such a model. Its basic assumption is that eye movements in visual search tasks tend to target display areas that provide a maximum amount of task-relevant information for processing. To advance this model towards a general model of saccadic selectivity in visual search, additional eye-movement studies are performed to provide detailed information on the influence of color and target size on saccadic selectivity. Based on the data obtained, various aspects of the influence of display and target features on eye- movement patterns are quantified. These data are used to devise the advanced version of the Area Activation Model. The crucial improvements include the elimination of required empirical a-priori information, the consideration of bottom-up activation, and the applicability of the model to search displays beyond artificial images with discrete items and features. Ideally, the resulting model will be straightforward, consistent with natural principles, and carefully avoiding any freely adjustable model parameters to qualify it as a streamlined and general approach to eye-movement control in visual search. Such a model will be important for understanding the functionality of the visual system and will also have significant impact on the fields of computer vision, human-computer interaction, and cognitive modeling. The project will deepen our understanding of visual attention in general and the visual factors underlying saccade programming in particular. Such understanding will advance the possibilities for surgical and therapeutic treatment of visual illnesses. Moreover, the results of the study can directly be applied to improve current human-computer interfaces for computer-assisted surgery and x-ray image analysis.          n/a",A General Model of Saccadic Selectivity in Visual Search,7305151,R15EY017988,"['Appetitive Behavior', 'Area', 'Attention', 'Cognitive', 'Collection', 'Color', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer information processing', 'Computer-Assisted Surgery', 'Crowding', 'Data', 'Data Analyses', 'Dimensions', 'Empirical Research', 'End Point', 'Eye Movements', 'Frequencies', 'Friends', 'Image', 'Image Analysis', 'Light', 'Measurement', 'Modeling', 'Modification', 'Nature', 'Numbers', 'Operative Surgical Procedures', 'Pattern', 'Performance', 'Personal Satisfaction', 'Pliability', 'Positioning Attribute', 'Process', 'Qualifying', 'Research', 'Resources', 'Saccades', 'Scanning', 'Spottings', 'Stimulus', 'Testing', 'Therapeutic', 'User-Computer Interface', 'Visual', 'Visual attention', 'Visual system structure', 'Work', 'base', 'computer human interaction', 'design', 'improved', 'insight', 'programs', 'sample fixation', 'size', 'visual process', 'visual processing', 'visual search']",NEI,UNIVERSITY OF MASSACHUSETTS BOSTON,R15,2007,209463,-0.0065111535101417236
"A Laser-Based Device for Work Site Stability Assessment    DESCRIPTION (provided by applicant): Summary: A laser-based acoustic emission (AE) detection device is proposed for work site structural stability assessment. This new device will take advantage of innovations in laser ultrasonics, artificial intelligence (Al) and advanced acoustic emission technology to provide mine workers with a unique instant, real time stability assessment of immediate rock structures in the working environment, which was not attainable in the past. Nonlinear optical interferometry based on two-wave mixing / photo-induced electromotive force techniques will be used for AE signal detection from rock structures in mine sites. Al criteria will be established by wave pattern recognition to identify unstable areas in mine sites. This research will also result in a unique non-contact monitoring device for acoustic emission/microseismic studies, which will be very useful in many areas of application. The primary objective of the Phase II research is to develop the prototype of the AE detector and test it in real-world mining facilities. The primary objective consists of six specific aims: 1. instrumentation development, 2. pre-field experiment preparation, 3. in-situ data collection, 4. Al criteria development, 5. system integration and in-situ trial, and 6. documentation and reporting. Relevance to Public Health: The innovation will contribute to a reduction the occupational injuries and fatalities caused by roof falls, sidewall crumples, stope collapses, and slope slides, etc., in the mining industry. The research and development addresses the miner's safety and contributes to ensuring the mineworker's right to ""safe and healthful working conditions"" (Occupational Safety and Health Act of 1970).          n/a",A Laser-Based Device for Work Site Stability Assessment,7278614,R44OH007662,[' '],NIOSH,AAC INTERNATIONAL,R44,2007,363328,0.02258296741050994
"Smart Wheelchair Component System    DESCRIPTION (provided by applicant):  Independent mobility is critical to individuals of any age. While the needs of many individuals with disabilities can be satisfied with power wheelchairs, some members of the disabled community find it difficult or impossible to operate a standard power wheelchair. This population includes, but is not limited to, individuals with low vision, visual field neglect, spasticity, tremors, or cognitive deficits. The goal of this project is to develop a set of components that can be added to standard power wheelchairs to convert them into ""smart"" wheelchairs which can assist the user in navigation and obstacle avoidance. During Phase I, a prototype of the Smart Wheelchair Component System (SWCS) was developed from a laptop computer and a collection of sonar, infrared and bump sensors. The evaluation activities performed during Phase I demonstrated that the system is compatible with multiple brands of wheelchairs, can accept both continuous and switch-based input, and can support front-, mid-, and rear-wheel drive wheelchairs. During Phase II, we propose to refine the system hardware and software; replace the laptop computer with an embedded microprocessor; fabricate enclosures for the system components; and develop tools to support clinicians in installing and configuring the system. The system will be evaluated in tests involving potential users, clinicians, and wheelchair design standards. The final product will be a market-ready modular system which can be attached to a variety of standard power wheelchairs. This product has the potential to increase the independence and quality of life of many wheelchair users and potential wheelchair users whose disabilities limit their capacity for independent wheelchair navigation.       n/a",Smart Wheelchair Component System,7237214,R44HD040023,"['Adult', 'Age', 'Child', 'Client', 'Cognitive deficits', 'Collection', 'Communities', 'Compatible', 'Computer Vision Systems', 'Computer software', 'Computers', 'Condition', 'Destinations', 'Development', 'Disabled Persons', 'Disadvantaged', 'Documentation', 'Equipment', 'Evaluation', 'Future', 'Goals', 'Individual', 'Joystick', 'Laboratories', 'Learning', 'Location', 'Locomotion', 'Manufacturer Name', 'Marketing', 'Methods', 'Microprocessor', 'Numbers', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Powered wheelchair', 'Production', 'Quality of life', 'Range', 'Relative (related person)', 'Research Personnel', 'Robot', 'Self Perception', 'Standards of Weights and Measures', 'System', 'Technology', 'Testing', 'Touch sensation', 'Travel', 'Tremor', 'Visual Fields', 'Visual impairment', 'Wheelchairs', 'Work', 'base', 'data acquisition', 'design', 'disability', 'laptop', 'member', 'neglect', 'peer', 'prototype', 'sensor', 'sonar', 'tool']",NICHD,AT SCIENCES,R44,2007,387828,0.028293269703479295
"Indoor Magnetic Wayfinding For The Visually Impaired    DESCRIPTION (provided by applicant): Advanced Medical Electronics (AME) proposes the development of an indoor way-finding device utilizing the unique magnetic anomaly patterns that exist in modern, man-made structures. The proposed system will record the magnitude of magnetic field strength from sensors in three orthogonal axes. The time history of these magnetic data points can be continuously compared with an electronic map of magnetic anomalies (or, ""signature"") to determine current position within a building. The phase I developed prototype system tracked in feasibility experiments with an accuracy of 1 foot (radius). Magnetic anomalies render a magnetic compass useless for finding a directional bearing. However, these same invisible anomalies represent valuable, unique indoor terrain features measurable by magnetic sensors located inside a small, portable device. Such a device would be able to provide low-vision users with a valuable indoor low-cost way-finding tool analogous to a Global Positioning System (GPS) device used outdoors. About 3.7 million Americans are visually disabled. Of these, 200,000 are blind, and the rest have low vision. The key advantage of the way-finding concept presented in this proposal, over other methods, is that the benefits are made available to the visually impaired community without requiring expensive building infrastructure investments. This is of particular advantage to large government buildings and educational campuses. The proposed approach allows a cost effective solution to way-finding within these buildings.          n/a",Indoor Magnetic Wayfinding For The Visually Impaired,7326673,R44EY015616,"['American', 'Appointment', 'Building Codes', 'Cognition', 'Communities', 'Computer Vision Systems', 'Computers', 'Data', 'Development', 'Devices', 'Disabled Persons', 'Doctor of Philosophy', 'Education', 'Electronics', 'Engineering', 'Fee-for-Service Plans', 'Funding', 'Government', 'Hand', 'Housing', 'Human', 'Indoor Magnetic Wayfinding', 'Investments', 'Joints', 'Label', 'Location', 'Magnetism', 'Maps', 'Measurable', 'Measurement', 'Measures', 'Medical Electronics', 'Minnesota', 'Modeling', 'Modification', 'Neurosciences', 'Numbers', 'Oceans', 'Pattern', 'Pattern Recognition', 'Pennsylvania', 'Persons', 'Phase', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychology', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Rest', 'Services', 'Silicon Dioxide', 'Solutions', 'Somatotype', 'Speech', 'Steel', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Today', 'Universities', 'Vision', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Wireless Technology', 'World Health Organization', 'base', 'blind', 'college', 'computer science', 'concept', 'cost', 'cost effective', 'court', 'design', 'digital', 'foot', 'human subject', 'innovation', 'interest', 'magnetic field', 'miniaturize', 'motor control', 'performance tests', 'professor', 'prototype', 'radius bone structure', 'research study', 'sensor', 'sensory integration', 'tool', 'way finding']",NEI,ADVANCED MEDICAL ELECTRONICS CORPORATION,R44,2007,386674,0.03517211314249582
"Designing Visually Accessible Spaces    DESCRIPTION (provided by applicant): Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our long-term goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals and to enhance safety for the elderly and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool in which environments (such as a hotel lobby, large classroom, or hospital reception area), could be simulated with sufficient accuracy to predict the visibility of key landmarks or obstacles, such as steps or benches, under differing lighting conditions. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments"". Our research plan has four specific goals: 1) Develop methods for predicting the physical levels of light reaching the eye in existing or planned architectural spaces. 2) Acquire performance data for normally sighted subjects with visual restrictions and people with low vision to investigate perceptual capabilities critical to visually-based mobility. 3) Develop models that can predict perceptual competence on tasks critical to visually-based mobility. 4) Demonstrate a proof-of-concept software tool that operates on design models from existing architectural design systems and is able to highlight potential obstacles to visual accessibility. The lead investigators in our partnership come from three institutions: University of Minnesota Gordon Legge, Daniel Kersten; University of Utah William Thompson, Peter Shirley, Sarah Creem-Regehr; and Indiana University Robert Shakespeare. This interdisciplinary team has expertise in the four areas required for programmatic research on visual accessibility empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), photometrically correct computer graphics (Shirley), and architectural design and lighting (Shakespeare).           n/a",Designing Visually Accessible Spaces,7172766,R01EY017835,"['Accounting', 'Address', 'American', 'Area', 'Arts', 'Blindness', 'Characteristics', 'Classification', 'Competence', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Condition', 'Contrast Sensitivity', 'Data', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Elderly', 'Engineering', 'Environment', 'Evaluation', 'Eye', 'Goals', 'Hospitals', 'Indiana', 'Individual', 'Institution', 'Lead', 'Light', 'Lighting', 'Lobbying', 'Location', 'Measurement', 'Methods', 'Minnesota', 'Modeling', 'National Eye Institute', 'Pattern', 'Perception', 'Performance', 'Peripheral', 'Photometry', 'Published Comment', 'Range', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Priority', 'Safety', 'Simulate', 'Software Tools', 'Space Models', 'Space Perception', 'Specialist', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Testing', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual impairment', 'base', 'concept', 'design', 'innovation', 'knowledge base', 'luminance', 'model design', 'model development', 'physical model', 'predictive modeling', 'programs', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2007,487230,0.05312631660490497
"A Laser-Based Device for Work Site Stability Assessment    DESCRIPTION (provided by applicant): Summary: A laser-based acoustic emission (AE) detection device is proposed for work site structural stability assessment. This new device will take advantage of innovations in laser ultrasonics, artificial intelligence (Al) and advanced acoustic emission technology to provide mine workers with a unique instant, real time stability assessment of immediate rock structures in the working environment, which was not attainable in the past. Nonlinear optical interferometry based on two-wave mixing / photo-induced electromotive force techniques will be used for AE signal detection from rock structures in mine sites. Al criteria will be established by wave pattern recognition to identify unstable areas in mine sites. This research will also result in a unique non-contact monitoring device for acoustic emission/microseismic studies, which will be very useful in many areas of application. The primary objective of the Phase II research is to develop the prototype of the AE detector and test it in real-world mining facilities. The primary objective consists of six specific aims: 1. instrumentation development, 2. pre-field experiment preparation, 3. in-situ data collection, 4. Al criteria development, 5. system integration and in-situ trial, and 6. documentation and reporting. Relevance to Public Health: The innovation will contribute to a reduction the occupational injuries and fatalities caused by roof falls, sidewall crumples, stope collapses, and slope slides, etc., in the mining industry. The research and development addresses the miner's safety and contributes to ensuring the mineworker's right to ""safe and healthful working conditions"" (Occupational Safety and Health Act of 1970).          n/a",A Laser-Based Device for Work Site Stability Assessment,7109905,R44OH007662,"['artificial intelligence', 'bioengineering /biomedical engineering', 'data collection methodology /evaluation', 'human mortality', 'injury prevention', 'interferometry', 'lasers', 'mechanical stress', 'minings', 'monitoring device', 'occupational hazard', 'occupational health /safety', 'sound perception', 'technology /technique development', 'ultrasonography', 'work site']",NIOSH,AAC INTERNATIONAL,R44,2006,386471,0.02258296741050994
"Sign Finder: Computer Vision to Find and Read Signs DESCRIPTION (provided by applicant): We propose to build a device that enhances the mobility of visually impaired persons by finding and reading signs aloud without the need for infrastructure beyond ordinary signs. Using new computer vision techniques, it will detect and read text in images captured by a camera worn like a pendant around the user's neck.      We will build two commercially viable, self-contained consumer versions, a $1,500 device using consumer computers and cameras, and a $750 proprietary device.      In typical use, a wearer will select a mode (city street, supermarket) by pressing buttons and optionally speaking commands, and then either point the device at a scene, or scan the scene using auto-repeat image capture. The device will find and read signs, but only output audio for signs relevant to the mode.      The Phase II work plan has three tracks: 1) Computer vision software development and testing, 2) Human interface design and development, and 3) development and testing of the two forms of the device.       Continuing our collaboration from Phase I, we use Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for human factors. Bolton Engineering will design and build the proprietary device hardware. n/a",Sign Finder: Computer Vision to Find and Read Signs,7004518,R44EY011821,"['assistive device /technology', 'blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer system design /evaluation', 'functional ability', 'human subject', 'medical rehabilitation related tag', 'questionnaires']",NEI,BLINDSIGHT CORPORATION,R44,2006,457462,0.06935835221804788
"Home Sensor Date Fusion to Support Aging in Place    DESCRIPTION (provided by applicant): The aging of the U.S. population presents many challenges. The existing paradigm of care will not allocate resources efficiently as the size of the population requiring home care assistance grows. Our objective is to enhance elder independence by providing both better and more timely predictive health-status assessments and direct, real-time, recommendations and warnings. These together will lower the risk of elders remaining at home or in low intensity care settings. The objective of the Phase II research and development effort is to develop technology that can detect and track activities in the home environment and to demonstrate its usefulness in allowing elders to remain in their homes longer than is now possible. CleverSet will develop and deploy a prototype CleverSet Activity Tracker, CAT, that processes data from a robust set of simple sensors to (1) track the activities of daily living over time (2) modify these tracked activities to include uncertainty about the environment and risk to produce notifications of Events Requiring Intervention (ERIs); and (3) demonstrate the results of the models. The technological innovation of the proposed work is the application of dynamic relational Bayesian networks (DRBNs) to activities in the home environment. CleverSet's DRBN algorithms collectively referred to as CleverSet Modeler, exploit the data model and meta-data from the schema to guide and frame relational queries about behavior and events. In the proposed work, DRBNs will be used to represent complex, dynamic, multi-scale processes involving multiple actors, as probability distributions over the elements, queries, and relationships in the DRBN model. Activities of daily living (ADLs) will be identified using DRBN machine learning algorithms from sensor data and tracked through time. Short-term rhythms of daily life as well as longer-term transitions will be tracked. Risk modifiers relevant to elders will be integrated into the model and used to adapt the sensor data input. Sensor studies will also be performed to determine the relative contribution of sensors to the DRBN ADL models. A software prototype integrating the elements of the Phase II effort will be developed.         n/a",Home Sensor Date Fusion to Support Aging in Place,7051911,R44AG024687,"['aging', 'artificial intelligence', 'assistive device /technology', 'behavioral /social science research tag', 'caregivers', 'clinical research', 'computer program /software', 'confidentiality', 'frail elderly', 'functional ability', 'home health care', 'human subject', 'injury prevention', 'mathematical model', 'medical rehabilitation related tag', 'microprocessor /microchip', 'monitoring device', 'outpatient care', 'quality of life', 'safety equipment', 'self care', 'technology /technique development']",NIA,"CLEVERSET, INC.",R44,2006,361154,-0.021351204187241436
"Wayfinding for the blind & visually impaired using passive environmental labels    DESCRIPTION (provided by applicant): The objective of this proposal is to tackle the problem of way finding (finding one's way in an environment), faced by blind and severely visually impaired persons who are unable to find or read signs, landmarks and locations. We propose a novel and very inexpensive environmental labeling system to provide this population with access to information needed for indoor way finding (where GPS is not available). The system uses simple passive landmark symbols printed on paper or other material, placed next to text, Braille signs or barcode at locations of interest (offices, bathrooms, etc.) in an environment such as an office building. These printed patterns contain spatial and semantic information that is detected using computer vision algorithms running on a standard camera cell phone. By scanning the environment with the device, which detects all landmark symbols in its line of sight up to distances of 10 meters, the user can determine his or her approximate location in the environment as well as the information encoded near each landmark symbol. The system extracts this information in real-time and communicates it to the user by sound, synthesized speech and/or tactile feedback. This information includes spatial (e.g. audio tones to indicate the presence and direction of a label in the camera's field of view) and semantic information (""Mr. Johnson's office, room 429, at 11 o'clock""). The research proposed here will produce a prototype system that will be tested by blind and low vision subjects. Our team includes a blind expert on psychoacoustics (and other in-house blind staff) and an expert consultant on low-vision way finding and navigation to help optimize the user interface and guide development into a practical, easy-to-use system.              n/a",Wayfinding for the blind & visually impaired using passive environmental labels,7143942,R21EY017003,"['clinical research', 'computers', 'reading', 'semantics', 'touch', 'vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2006,224601,0.02283525058172149
"Intelligent Tutor for WMD EMS Incident Management    DESCRIPTION (provided by applicant): We propose to develop EMS/IM ITS, a suite of simulation-based intelligent tutoring systems and scenarios that will enable practice-based learning of WMD emergency medical services incident management principles and skills, including situation assessment, decision-making, and real-time execution of EMS tasks within an incident command structure. To support practical and economical development of many EMS/IM ITS training scenarios, we will also develop software tools and development methods that enable efficient authoring of new scenarios and adaptation/enhancement of existing scenarios by instructors or subject matter experts, without programming. We will leverage our tutoring system development tools and our experience developing tutoring systems for medical training, command and control, and tactical decision-making. The National Incident Management System (NIMS) was mandated by HSPD-5 to provide a comprehensive, national approach to domestic incident management, so that all levels of government across the nation could work efficiently and effectively together to prepare for, respond to, and recover from domestic incidents. We believe that EMS/IM ITS can contribute to NIMS by providing scenario-based learning of incident management principles for medical first responders, consistent with NIMS, and tailorable via scenario authoring to the specific circumstances and incident management plans of each government organization. This proposed Phase I effort will lay the groundwork for the Phase II effort, by producing 1) requirements and design of the system to be developed during Phase II, 2) a software prototype that illustrates our concept, and 3) a formative evaluation of the prototype and design that provides a basis for estimating the feasibility and effectiveness of the operational system that would be developed during Phase II.             n/a",Intelligent Tutor for WMD EMS Incident Management,7115108,R43ES014801,"['artificial intelligence', 'computer assisted instruction', 'computer assisted medical decision making', 'computer assisted patient care', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'educational resource design /development', 'emergency service /first responder', 'health care personnel education', 'health care professional practice', 'health services research tag', 'medical education', 'method development', 'patient care management', 'training']",NIEHS,"STOTTLER HENKE ASSOCIATES, INC.",R43,2006,99999,0.002804050098633537
"Traffic Intersection Analysis Algorithms for the Blind DESCRIPTION (provided by applicant): This project aims to explore, develop and test computer vision algorithms to analyze images of street intersections from a camera worn by a blind person.  Urban intersections are the most dangerous parts of a blind person's travel.  They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult.  We will explore computer vision algorithms to help a blind person find the crosswalk, find the pedestrian signal button, determine when the ""walk"" light is on, and alert him/her to any veering out of the crosswalk.  We will emphasize the development of completely novel methods of analyzing non-ideal images including shadows, occlusions and other irregularities using spatial grouping techniques based on Bayesian inference.  The resulting algorithms are intended for eventual integration as modules for a computer vision system we are already developing to help blind persons with travel tasks such as finding and reading aloud printed signs and negotiating street crossings.  The combined system would have potential for a radical advance in independent travel for blind persons.  In this exploratory project, we aim to: (1) Explore and test alternative approaches to algorithm design to process intersection images and extract the information about the crosswalk, crossing signal, etc., using a database of real-world images taken by blind persons at a variety of different kinds of intersections.  (2) Test the algorithms using a portable camera connected to a notebook computer with speech output. n/a",Traffic Intersection Analysis Algorithms for the Blind,7096566,R21EY015187,"['blind aid', 'blindness', 'clinical research', 'computer simulation', 'computer system design /evaluation', 'gait', 'human subject', 'injury prevention', 'mathematical model', 'statistics /biometry', 'transportation /recreation safety', 'urban area']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2006,214738,0.013089636074358084
"Instrument development & fabrication for vision research    DESCRIPTION (provided by applicant):  The objective of this proposal is to enhance the research capabilities and collaborative efforts of the vision researchers at the Columbia University Medical Center.  State-of-the-art vision research often requires the custom fabrication of mechanical instruments to support the research. Support is requested for a single module to renovate and support the machine shop in the Harkness Eye Institute at Columbia University, to be shared primarily between the Department of Ophthalmology and the Mahoney Center for Brain and Behavior.  The module will have 10 users, 7 of whom have NEI-funded RO1 grants, and 3 of whom perform research in the area of visual systems neuroscience on grants funded by the NIMH. All of the investigators are also mentors on an NEI-funded training grant. The current systems projects include studies of the neurophysiology   and psychophysics of spatial vision, visual attention, early cortical processing, visual emotional association, and visual motion; the cellular and molecular projects include studies of fluid transport across corneal epithelium, retinal axon guidance, ocular wound healing, and the impact of the lipofuscin fluorophores on retinal pigmented epithelial cell function and viability. All of these projects require the development and fabrication of devices primarily designed for a given project. A great number of these can, when perfected, be shared among a number of projects. Examples of such devices include custom-made nanoliter injection devices, recording chambers, multiple-microdrive platforms, dual recording-iontophoretic devices, illumination devices, and recording gdds. The PI has extensive experience collaborating with machinists, and several of the devices in whose development he participated have been marketed commercially.  Currently the Department of Ophthalmology has a fully-equipped machine shop the machines of which are all fine old Bridgeport and Hardinge manual machines. This proposal is to upgrade the machine shop, with a computer-controlled lathe and a computer-controlled milling machine, and to support the salary of the machinist who was hired in June, 2003, using university startup funds. The availability of an in-house professionally certified machinist will significantly speed the process of design and fabrication of custom instruments.  The use of computer controlled machine tools will facilitate duplication of instruments usable in multiple laboratories.            n/a",Instrument development & fabrication for vision research,7057358,R24EY015634,"['biomedical equipment', 'biomedical resource', 'clinical research', 'computers', 'neurosciences', 'vision']",NEI,COLUMBIA UNIVERSITY HEALTH SCIENCES,R24,2006,180672,0.04533715930987901
"The formation of visual objects    DESCRIPTION (provided by applicant): Perceptual grouping is the process by which the initially raw and inchoate visual image is organized into perceptual ""objects"". What spatial factors induce perceptual grouping? What is the sequence of computations whereby the image is progressively organized? One source of difficulty in modeling this process is that, unlike many aspects of early vision, perceptual grouping inherently involves non-local: computations - integration of cues from potentially distant locations in the image. Another difficulty in understanding perceptual grouping has been the lack of objective and temporally precise methods for actually measuring the observer's subjective organization of an image. This proposal seeks to combine (a) recent advances in understanding the non-local computations involved in perceptual grouping with (b) novel experimental methods for determining subjective organization. The experimental methods are based on the finding that perceptual objects enjoy certain objectively measurable benefits, including more efficient visual comparisons within them than between distinct objects. This proposal seeks to use this effect to discover what the visual system in fact treats as a perceptual object, and how this percept develops over the course of processing. Most of the proposed experiments involve carefully constructed artificial stimuli with various grouping cues in force, designed to allow detailed comparisons of the strength, interaction, and time-course of each potential grouping cue. In addition, several experiments involve natural images, in order to uncover how perceptual organization proceeds under more naturalistic conditions. This research may lead to technological advancement in the area of computer vision, as well as to better understanding of disorders of perceptual organization such as visual agnosia and dyslexia.         n/a",The formation of visual objects,7037390,R01EY015888,"['clinical research', 'computational neuroscience', 'cues', 'form /pattern perception', 'human subject', 'mathematical model', 'mental process', 'motion perception', 'neural information processing', 'neuropsychological tests', 'neuropsychology', 'psychophysics', 'space perception', 'statistics /biometry', 'time perception', 'vision tests', 'visual depth perception', 'visual stimulus', 'visual tracking']",NEI,RUTGERS THE ST UNIV OF NJ NEW BRUNSWICK,R01,2006,215583,-0.005791752915079041
"MobileEye OCR for the Visually Impaired    DESCRIPTION (provided by applicant): In this SBIR we propose to demonstrate the technical feasibility of Mobile OCR, a portable software system which makes use of existing personal devices to provide access to textual materials for the elderly or the visually impaired. The system will help these low vision individuals with basic daily activities, such as shopping, preparing meals, taking medication, and reading traffic signs. It will step beyond our proposed MobileEyes vision enhancement system to apply cutting edge recognition technology for mobile devices. The system will use common camera phone hardware to capture and enhance textual information, perform Optical Character Recognition (OCR) and provide audio or visual feedback. Our research will focus on implementing and integrating new vision enhancement and analysis techniques on limited resource mobile devices. Specifically, we will develop algorithms for detection and rectification of text on planes and generalized cylinders subject to perspective distortions, implement more robust and efficient algorithms and systems for stabilization and enhancement of text blocks, provide mobile OCR on complex textured backgrounds, and implement these techniques on small devices across a variety of platforms. The recognized text will be presented through Text-to-Speech (TTS), or displayed on the device with enhanced quality which can be easily read by low vision users. Phase I will focus on demonstrating the technical feasibility of our approach, and will incorporate a performance measurement methodology to quantitatively evaluate progress and evaluate our system against other approaches. In comparison to existing vision enhancement devices, such as magnifying glasses, telescopes, and text reading devices such as scanner-based OCR, our solution has several advantages: 1) it makes use of a single, portable device (camera cell phone) that is commonly available and typically already carried for its telecommunications capabilities; 2) it can be used selectively by users so they will not be overwhelmed by irrelevant information; and 3) it can be integrated directly with other applications for specialized tasks. Our research results will impact the millions of low-vision individuals and the blind, as well as vision and computer vision researchers. Our team is uniquely qualified to explore the feasibility of extending visual applications to these devices, and provide a platform for integrating future vision algorithms.         n/a",MobileEye OCR for the Visually Impaired,7053650,R43EY017216,"['reading', 'solutions', 'vision']",NEI,"APPLIED MEDIA ANALYSIS, LLC",R43,2006,104935,0.07419541775008838
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6995047,R43EY014487,"['artificial intelligence', 'biomedical equipment development', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection', 'digital imaging', 'functional ability', 'human subject', 'image processing', 'medical rehabilitation related tag', 'patient oriented research', 'portable biomedical equipment', 'questionnaires', 'vision aid', 'vision disorders', 'visual fields', 'visual perception', 'visual threshold', 'visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2005,144106,0.046983800037901924
"Sign Finder: Computer Vision to Find and Read Signs DESCRIPTION (provided by applicant): We propose to build a device that enhances the mobility of visually impaired persons by finding and reading signs aloud without the need for infrastructure beyond ordinary signs. Using new computer vision techniques, it will detect and read text in images captured by a camera worn like a pendant around the user's neck.      We will build two commercially viable, self-contained consumer versions, a $1,500 device using consumer computers and cameras, and a $750 proprietary device.      In typical use, a wearer will select a mode (city street, supermarket) by pressing buttons and optionally speaking commands, and then either point the device at a scene, or scan the scene using auto-repeat image capture. The device will find and read signs, but only output audio for signs relevant to the mode.      The Phase II work plan has three tracks: 1) Computer vision software development and testing, 2) Human interface design and development, and 3) development and testing of the two forms of the device.       Continuing our collaboration from Phase I, we use Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for human factors. Bolton Engineering will design and build the proprietary device hardware. n/a",Sign Finder: Computer Vision to Find and Read Signs,6832762,R44EY011821,"['assistive device /technology', 'blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer system design /evaluation', 'functional ability', 'human subject', 'medical rehabilitation related tag', 'questionnaires']",NEI,BLINDSIGHT CORPORATION,R44,2005,461157,0.06935835221804788
"Visual & Interactive Issues in the Design of Web Surveys    DESCRIPTION (provided by applicant): The rapid acceptance of the Worldwide Web as a vehicle for survey data collection raises important questions about how the new method works. Key features of Web surveys include the use of rich visual presentation of questions and the capability of interaction with the respondent. The rapid growth of the Web makes a close examination of these issues even more urgent. Neither set of features has been explored thoroughly even with earlier modes and the Web offers widely increased resources for both visual display (Web questionnaires can readily incorporate still pictures or video clips) and interaction (such as, floating screens and scrolling for help with definitions). Our application outlines a set of studies designed to address key questions about these issues. The studies focus on Web surveys, but we believe that the results would generalize to other modes of data collection that rely on visual presentation or incorporate interactive design features.   Experiments 1-5 examine how respondents interpret the visual cues in Web questionnaires. These studies test the general proposition that incidental features of the presentation of the questions (for example, the spacing of the response options, the color assigned to different response options) can give rise to unintended inferences about their meaning. These studies test predictions derived from a theoretical framework that assumes respondents use simple interpretive heuristics to assign meaning to visual features of the questions. The next two experiments examine the effects of including images as a supplement to the text of the question. Images are necessarily concrete, and Experiment 6 tests the hypothesis that this concreteness may lead respondents to interpret the questions more narrowly when they are accompanied by images. Experiment 7 tests the idea that the item depicted in an image may serve as a standard of comparison for respondents' judgments. Again, the results of these studies will lead to practical guidelines about the dangers involved in using images as an adjunct to verbal questions. The final series of studies examines when respondents are likely to take advantage of interactive features of a questionnaire. These experiments test three general hypotheses; respondents are more likely to utilize the information available to them interactively when 1) the information is easy to obtain, 2) it is clearly helpful, and 3) respondents are highly motivated to seek help. These six experiments would yield a better understanding of methods for getting respondents to use features that could yield better survey data.         n/a",Visual & Interactive Issues in the Design of Web Surveys,6879624,R01HD041386,"['Internet', 'artificial intelligence', 'attitude', 'behavior prediction', 'behavior test', 'behavioral /social science research tag', 'clinical research', 'computer human interaction', 'cues', 'data collection methodology /evaluation', 'human subject', 'imagery', 'interactive multimedia', 'mathematics', 'population survey', 'questionnaires', 'space perception', 'visual perception']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2005,206550,0.006017116378966218
"Traffic Intersection Analysis Algorithms for the Blind DESCRIPTION (provided by applicant): This project aims to explore, develop and test computer vision algorithms to analyze images of street intersections from a camera worn by a blind person.  Urban intersections are the most dangerous parts of a blind person's travel.  They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult.  We will explore computer vision algorithms to help a blind person find the crosswalk, find the pedestrian signal button, determine when the ""walk"" light is on, and alert him/her to any veering out of the crosswalk.  We will emphasize the development of completely novel methods of analyzing non-ideal images including shadows, occlusions and other irregularities using spatial grouping techniques based on Bayesian inference.  The resulting algorithms are intended for eventual integration as modules for a computer vision system we are already developing to help blind persons with travel tasks such as finding and reading aloud printed signs and negotiating street crossings.  The combined system would have potential for a radical advance in independent travel for blind persons.  In this exploratory project, we aim to: (1) Explore and test alternative approaches to algorithm design to process intersection images and extract the information about the crosswalk, crossing signal, etc., using a database of real-world images taken by blind persons at a variety of different kinds of intersections.  (2) Test the algorithms using a portable camera connected to a notebook computer with speech output. n/a",Traffic Intersection Analysis Algorithms for the Blind,6920594,R21EY015187,"['blind aid', 'blindness', 'clinical research', 'computer simulation', 'computer system design /evaluation', 'gait', 'human subject', 'injury prevention', 'mathematical model', 'statistics /biometry', 'transportation /recreation safety', 'urban area']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2005,255198,0.013089636074358084
"Just in Time Information for Exercise Adoption DESCRIPTION (provided by applicant):     A novel personal digital assistant (PDA)-based system that can automatically detect bouts of moderate or greater walking and deliver health behavior change information to users to increase their levels of physical activity will be developed and evaluated. The system will be an extension of work already performed by the investigators, and will incorporate a validated wireless motion sensor, pattern classification software to identify bouts of walking, and a personified, relational user interface designed to maintain engagement and trust in the tailored behavior change information delivered to users over multiple interactions. The system will be designed to be worn and used continuously by free-living populations and provide users with health behavior change information at the moment it is needed.      Users will interact with the PDA via a simulated face-to-face conversation with the animated relational agent, and will conduct a daily progress review and goal-setting session at which time they will schedule specific times they intend to walk on the following day (bouts of 10 minutes or more of moderate or greater intensity). If they complete a scheduled walk, the agent provides immediate social reinforcement. If they fail to initiate a walk at a scheduled time the agent engages them in a problem-solving session in which it attempts to help them overcome the specific obstacle to exercise they are experiencing.       In the proposed effort the components of the PDA-based system will be developed, integrated and tested, and a randomized pilot study conducted to: 1) evaluate the efficacy of the PDA-based behavior change intervention for increasing walking; 2) evaluate the effect of timeliness of health behavior change information on walking (time of need vs. retrospective); and 3) compare the efficacy of the personified user interface with that of a text-based interface for delivering health behavior change information on a PDA.      The proposed work will make significant contributions to several areas within the science of medical informatics, extending and integrating work in knowledge representation, bio-signal analysis, natural language processing, and consumer health informatics. This research will advance our understanding of the role of time in health behavior change, and result in a model of the temporal relationships among sensor data, user behavior, user goals, and the delivery of health information intended to change behavior. n/a",Just in Time Information for Exercise Adoption,7127056,R21LM008553,[' '],NLM,NORTHEASTERN UNIVERSITY,R21,2005,105924,-0.025833959701035646
"Just in Time Information for Exercise Adoption DESCRIPTION (provided by applicant):     A novel personal digital assistant (PDA)-based system that can automatically detect bouts of moderate or greater walking and deliver health behavior change information to users to increase their levels of physical activity will be developed and evaluated. The system will be an extension of work already performed by the investigators, and will incorporate a validated wireless motion sensor, pattern classification software to identify bouts of walking, and a personified, relational user interface designed to maintain engagement and trust in the tailored behavior change information delivered to users over multiple interactions. The system will be designed to be worn and used continuously by free-living populations and provide users with health behavior change information at the moment it is needed.      Users will interact with the PDA via a simulated face-to-face conversation with the animated relational agent, and will conduct a daily progress review and goal-setting session at which time they will schedule specific times they intend to walk on the following day (bouts of 10 minutes or more of moderate or greater intensity). If they complete a scheduled walk, the agent provides immediate social reinforcement. If they fail to initiate a walk at a scheduled time the agent engages them in a problem-solving session in which it attempts to help them overcome the specific obstacle to exercise they are experiencing.       In the proposed effort the components of the PDA-based system will be developed, integrated and tested, and a randomized pilot study conducted to: 1) evaluate the efficacy of the PDA-based behavior change intervention for increasing walking; 2) evaluate the effect of timeliness of health behavior change information on walking (time of need vs. retrospective); and 3) compare the efficacy of the personified user interface with that of a text-based interface for delivering health behavior change information on a PDA.      The proposed work will make significant contributions to several areas within the science of medical informatics, extending and integrating work in knowledge representation, bio-signal analysis, natural language processing, and consumer health informatics. This research will advance our understanding of the role of time in health behavior change, and result in a model of the temporal relationships among sensor data, user behavior, user goals, and the delivery of health information intended to change behavior. n/a",Just in Time Information for Exercise Adoption,6852192,R21LM008553,"['actigraphy', 'behavioral /social science research tag', 'biomedical equipment development', 'body physical activity', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'exercise', 'health behavior', 'health education', 'human subject', 'human therapy evaluation', 'information display', 'miniature biomedical equipment', 'patient monitoring device', 'personal computers', 'personal log /diary']",NLM,BOSTON MEDICAL CENTER,R21,2005,79484,-0.025833959701035646
"Instrument development & fabrication for vision research    DESCRIPTION (provided by applicant):  The objective of this proposal is to enhance the research capabilities and collaborative efforts of the vision researchers at the Columbia University Medical Center.  State-of-the-art vision research often requires the custom fabrication of mechanical instruments to support the research. Support is requested for a single module to renovate and support the machine shop in the Harkness Eye Institute at Columbia University, to be shared primarily between the Department of Ophthalmology and the Mahoney Center for Brain and Behavior.  The module will have 10 users, 7 of whom have NEI-funded RO1 grants, and 3 of whom perform research in the area of visual systems neuroscience on grants funded by the NIMH. All of the investigators are also mentors on an NEI-funded training grant. The current systems projects include studies of the neurophysiology   and psychophysics of spatial vision, visual attention, early cortical processing, visual emotional association, and visual motion; the cellular and molecular projects include studies of fluid transport across corneal epithelium, retinal axon guidance, ocular wound healing, and the impact of the lipofuscin fluorophores on retinal pigmented epithelial cell function and viability. All of these projects require the development and fabrication of devices primarily designed for a given project. A great number of these can, when perfected, be shared among a number of projects. Examples of such devices include custom-made nanoliter injection devices, recording chambers, multiple-microdrive platforms, dual recording-iontophoretic devices, illumination devices, and recording gdds. The PI has extensive experience collaborating with machinists, and several of the devices in whose development he participated have been marketed commercially.  Currently the Department of Ophthalmology has a fully-equipped machine shop the machines of which are all fine old Bridgeport and Hardinge manual machines. This proposal is to upgrade the machine shop, with a computer-controlled lathe and a computer-controlled milling machine, and to support the salary of the machinist who was hired in June, 2003, using university startup funds. The availability of an in-house professionally certified machinist will significantly speed the process of design and fabrication of custom instruments.  The use of computer controlled machine tools will facilitate duplication of instruments usable in multiple laboratories.            n/a",Instrument development & fabrication for vision research,6891373,R24EY015634,"['biomedical equipment', 'biomedical resource', 'clinical research', 'computers', 'neurosciences', 'vision']",NEI,COLUMBIA UNIVERSITY HEALTH SCIENCES,R24,2005,175413,0.04533715930987901
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6910762,P30EY006883,"['biomedical facility', 'health science research', 'vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2005,611743,0.030081459944841037
"The formation of visual objects    DESCRIPTION (provided by applicant): Perceptual grouping is the process by which the initially raw and inchoate visual image is organized into perceptual ""objects"". What spatial factors induce perceptual grouping? What is the sequence of computations whereby the image is progressively organized? One source of difficulty in modeling this process is that, unlike many aspects of early vision, perceptual grouping inherently involves non-local: computations - integration of cues from potentially distant locations in the image. Another difficulty in understanding perceptual grouping has been the lack of objective and temporally precise methods for actually measuring the observer's subjective organization of an image. This proposal seeks to combine (a) recent advances in understanding the non-local computations involved in perceptual grouping with (b) novel experimental methods for determining subjective organization. The experimental methods are based on the finding that perceptual objects enjoy certain objectively measurable benefits, including more efficient visual comparisons within them than between distinct objects. This proposal seeks to use this effect to discover what the visual system in fact treats as a perceptual object, and how this percept develops over the course of processing. Most of the proposed experiments involve carefully constructed artificial stimuli with various grouping cues in force, designed to allow detailed comparisons of the strength, interaction, and time-course of each potential grouping cue. In addition, several experiments involve natural images, in order to uncover how perceptual organization proceeds under more naturalistic conditions. This research may lead to technological advancement in the area of computer vision, as well as to better understanding of disorders of perceptual organization such as visual agnosia and dyslexia.         n/a",The formation of visual objects,6924971,R01EY015888,"['clinical research', 'computational neuroscience', 'cues', 'form /pattern perception', 'human subject', 'mathematical model', 'mental process', 'motion perception', 'neural information processing', 'neuropsychological tests', 'neuropsychology', 'psychophysics', 'space perception', 'statistics /biometry', 'time perception', 'vision tests', 'visual depth perception', 'visual stimulus', 'visual tracking']",NEI,RUTGERS THE ST UNIV OF NJ NEW BRUNSWICK,R01,2005,222488,-0.005791752915079041
"A Laser-Based Device for Work Site Stability Assessment DESCRIPTION (provided by applicant): A laser-based acoustic emission (AE) detection device is proposed (Phase I & II) for work site structural stability assessment in order to reduce the occupational injuries and fatalities caused by roof falls, sidewall crumples, stop collapses, slope slides, etc., in the mining industry. This applied research and development addresses the miner's safety and contributes to ensuring the mineworker's right to ""safe and healthful working conditions"" (Occupational Safety and Health Act of 1970). This new device will take advantage of innovations in laser ultrasonic, artificial intelligence (AI) and conventional acoustic emission technology to provide mine workers with a unique instant, real time stability assessment of immediate rock structures in the working environment, which was not attainable in the past. This research will also result in a unique non-contact monitoring device for acoustic emission/microseismic studies, which will be very useful in many areas of application. The primary objective of the Phase I research is to demonstrate under laboratory conditions the concept of the laser device for stability assessment, and to construct a prototype setup for further development and optimization in the subsequent Phase II research. This primary objective consists of five specific aims: 1. Specimen preparation, 2. Development of laser-based AE monitor, 3. AE data collection and failure criteria development, 4. laboratory demonstration, and 5. final report and Phase II proposal. n/a",A Laser-Based Device for Work Site Stability Assessment,6730974,R43OH007662,"['artificial intelligence', 'bioengineering /biomedical engineering', 'data collection methodology /evaluation', 'human mortality', 'injury prevention', 'interferometry', 'lasers', 'mechanical stress', 'minings', 'monitoring device', 'occupational hazard', 'occupational health /safety', 'sound perception', 'technology /technique development', 'ultrasonography', 'work site']",NIOSH,AAC INTERNATIONAL,R43,2004,99998,0.03726084706221945
"Sign Finder: Computer Vision to Find and Read Signs DESCRIPTION (provided by applicant): We propose to build a device that enhances the mobility of visually impaired persons by finding and reading signs aloud without the need for infrastructure beyond ordinary signs. Using new computer vision techniques, it will detect and read text in images captured by a camera worn like a pendant around the user's neck.      We will build two commercially viable, self-contained consumer versions, a $1,500 device using consumer computers and cameras, and a $750 proprietary device.      In typical use, a wearer will select a mode (city street, supermarket) by pressing buttons and optionally speaking commands, and then either point the device at a scene, or scan the scene using auto-repeat image capture. The device will find and read signs, but only output audio for signs relevant to the mode.      The Phase II work plan has three tracks: 1) Computer vision software development and testing, 2) Human interface design and development, and 3) development and testing of the two forms of the device.       Continuing our collaboration from Phase I, we use Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for human factors. Bolton Engineering will design and build the proprietary device hardware. n/a",Sign Finder: Computer Vision to Find and Read Signs,6739928,R44EY011821,"['assistive device /technology', 'blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer system design /evaluation', 'functional ability', 'human subject', 'medical rehabilitation related tag', 'questionnaires']",NEI,BLINDSIGHT CORPORATION,R44,2004,462571,0.06935835221804788
"Physiological Controller for Rotary Blood Pumps DESCRIPTION (provided by applicant):    As the prospects of chronic mechanical assistance for the failing human heart are fast becoming a reality, and patients are indeed returning home to regain a normal lifestyle, the limitations of this technology upon quality of life are becoming more apparent. To address many of these limitations, investigators are developing next-generation ventricular assist devices. Based on turbopump technology, these new devices offer smaller size, greater efficiency (hence smaller batteries), high reliability, and are more cost effective as compared to their pulsatile predecessors. For all the virtues of these new turbopumps, they bring additional challenges. Arguably the most urgent is the need for added ""intelligence."" These relatively ignorant devices are highly dependent on feedback-control to provide normal physiological response. The goal of the Phase- II effort proposed herein is to design a robust controller that may be incorporated into these turbodynamic pump systems for clinical use. The primary end product of this program would be a validated algorithm, in the form of firmware that will be embedded into existing rotary pump controller -- capable of maintaining optimal perfusion of the patient under a variety of hemodynamic demands and disturbances, while avoiding deleterious conditions such as ventricular suction. n/a",Physiological Controller for Rotary Blood Pumps,6803953,R44HL066656,"['artificial intelligence', 'auxiliary heart prosthesis', 'biomedical device power system', 'biomedical equipment development', 'cardiac output', 'circulatory assist', 'computer system design /evaluation', 'cow', 'electrophysiology', 'hemolysis', 'mathematical model', 'microprocessor /microchip', 'sheep']",NHLBI,"LAUNCHPOINT TECHNOLOGIES, INC.",R44,2004,374766,0.012282641139187131
"Visual & Interactive Issues in the Design of Web Surveys    DESCRIPTION (provided by applicant): The rapid acceptance of the Worldwide Web as a vehicle for survey data collection raises important questions about how the new method works. Key features of Web surveys include the use of rich visual presentation of questions and the capability of interaction with the respondent. The rapid growth of the Web makes a close examination of these issues even more urgent. Neither set of features has been explored thoroughly even with earlier modes and the Web offers widely increased resources for both visual display (Web questionnaires can readily incorporate still pictures or video clips) and interaction (such as, floating screens and scrolling for help with definitions). Our application outlines a set of studies designed to address key questions about these issues. The studies focus on Web surveys, but we believe that the results would generalize to other modes of data collection that rely on visual presentation or incorporate interactive design features.   Experiments 1-5 examine how respondents interpret the visual cues in Web questionnaires. These studies test the general proposition that incidental features of the presentation of the questions (for example, the spacing of the response options, the color assigned to different response options) can give rise to unintended inferences about their meaning. These studies test predictions derived from a theoretical framework that assumes respondents use simple interpretive heuristics to assign meaning to visual features of the questions. The next two experiments examine the effects of including images as a supplement to the text of the question. Images are necessarily concrete, and Experiment 6 tests the hypothesis that this concreteness may lead respondents to interpret the questions more narrowly when they are accompanied by images. Experiment 7 tests the idea that the item depicted in an image may serve as a standard of comparison for respondents' judgments. Again, the results of these studies will lead to practical guidelines about the dangers involved in using images as an adjunct to verbal questions. The final series of studies examines when respondents are likely to take advantage of interactive features of a questionnaire. These experiments test three general hypotheses; respondents are more likely to utilize the information available to them interactively when 1) the information is easy to obtain, 2) it is clearly helpful, and 3) respondents are highly motivated to seek help. These six experiments would yield a better understanding of methods for getting respondents to use features that could yield better survey data.         n/a",Visual & Interactive Issues in the Design of Web Surveys,6743701,R01HD041386,"['Internet', 'artificial intelligence', 'attitude', 'behavior prediction', 'behavior test', 'behavioral /social science research tag', 'clinical research', 'computer human interaction', 'cues', 'data collection methodology /evaluation', 'human subject', 'imagery', 'interactive multimedia', 'mathematics', 'population survey', 'questionnaires', 'space perception', 'visual perception']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2004,201780,0.006017116378966218
"Locating and Reading Informational Signs  DESCRIPTION (provided by applicant): Our goal is to construct computer vision systems to enable the blind and severely visually impaired to detect and read informational text in city scenes. The informational text can be street signs, bus numbers hospital signs, supermarket signs, and names of products (eg. Kellogg's cornflakes). We will construct portable prototype computer vision systems implemented by digital cameras attached to personal, or hand held, computers. The camera need only be pointed in the general direction of the text (so the text is only one percent of the image). A speech synthesizer will read the text to the user. Blind and visually impaired users will test the device in the field (under supervision) and give feedback to improve the algorithms. We argue that this work will make a significant contribution to improving human health (rehabilitation). Computer vision is a rapidly maturing technology with immense potential to help the blind and visually impaired. Reports suggest that detecting and reading informational text is one of the main unsatisfied desires of these groups. Written signs and information in the environment are used for navigation, shopping, operating equipment, identifying buses, and many other purposes (to which a blind person does not otherwise have independent access). The blind and severely visually impaired make up a large fraction of the US population (3 million). Moreover, this proportion is expected to increase by a factor of two in the next ten years due to increased life expectancy. Our proposal is design-driven. It uses a new class of computer vision algorithms known as Data Driven Monte Carlo (DDMCMC). The algorithms are used to: (i) search for text, and (ii) to read it. Recent developments in digital cameras and portable/handheld computers make it practical to implement these algorithms in portable prototype systems. The three scientists in this proposal have the necessary expertise to accomplish it. Dr.'s Yuille and Zhu have backgrounds in computer vision and Dr. Brabyn has experience in developing and testing engineering systems to help the blind and visually impaired. Our proposal falls within the scope of the Bioengineering initiative because we are applying techniques from the mathematical/engineering sciences to develop informatic approaches for patient rehabilitation. More specifically, our work will facilitate the development of portable devices to help the blind and visually impaired.   n/a",Locating and Reading Informational Signs,6801171,R01EY013875,"['blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer program /software', 'cues', 'human subject', 'reading', 'vision aid', 'vision disorders']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2004,329706,0.040659008014867125
"Instrument development & fabrication for vision research    DESCRIPTION (provided by applicant):  The objective of this proposal is to enhance the research capabilities and collaborative efforts of the vision researchers at the Columbia University Medical Center.  State-of-the-art vision research often requires the custom fabrication of mechanical instruments to support the research. Support is requested for a single module to renovate and support the machine shop in the Harkness Eye Institute at Columbia University, to be shared primarily between the Department of Ophthalmology and the Mahoney Center for Brain and Behavior.  The module will have 10 users, 7 of whom have NEI-funded RO1 grants, and 3 of whom perform research in the area of visual systems neuroscience on grants funded by the NIMH. All of the investigators are also mentors on an NEI-funded training grant. The current systems projects include studies of the neurophysiology   and psychophysics of spatial vision, visual attention, early cortical processing, visual emotional association, and visual motion; the cellular and molecular projects include studies of fluid transport across corneal epithelium, retinal axon guidance, ocular wound healing, and the impact of the lipofuscin fluorophores on retinal pigmented epithelial cell function and viability. All of these projects require the development and fabrication of devices primarily designed for a given project. A great number of these can, when perfected, be shared among a number of projects. Examples of such devices include custom-made nanoliter injection devices, recording chambers, multiple-microdrive platforms, dual recording-iontophoretic devices, illumination devices, and recording gdds. The PI has extensive experience collaborating with machinists, and several of the devices in whose development he participated have been marketed commercially.  Currently the Department of Ophthalmology has a fully-equipped machine shop the machines of which are all fine old Bridgeport and Hardinge manual machines. This proposal is to upgrade the machine shop, with a computer-controlled lathe and a computer-controlled milling machine, and to support the salary of the machinist who was hired in June, 2003, using university startup funds. The availability of an in-house professionally certified machinist will significantly speed the process of design and fabrication of custom instruments.  The use of computer controlled machine tools will facilitate duplication of instruments usable in multiple laboratories.            n/a",Instrument development & fabrication for vision research,6795629,R24EY015634,"['biomedical equipment', 'biomedical resource', 'clinical research', 'computers', 'neurosciences', 'vision']",NEI,COLUMBIA UNIVERSITY HEALTH SCIENCES,R24,2004,367302,0.04533715930987901
"Micro-environment Glasses as a Treatment for CVS DESCRIPTION (provided by applicant) Computer Vision Syndrome (CVS) refers to a collection of eye problems associated with computer use, and about three-quarters of computer users have it. Conservative estimates indicate that over $2 billion is currently spent on examinations and special eyewear for CVS treatment. The most common symptoms of CVS include: eyestrain or eye fatigue, dry eyes, burning eyes, sensitivity to light, and blurred vision. Non-ocular symptoms include headaches, pain in the shoulders, neck, or back. As diverse as the symptoms are, they may be related and can be subdivided into to three potential pathophysiological causes:   1) Ocular Surface Mechanisms   2) Accommodative Mechanisms   3) Extra-Ocular Mechanisms   There is a significant gap in the fund of knowledge regarding the diagnosis of this disease. In the near-term, we plan to focus on the ocular surface category of disorders as a cause of CVS, identify clinical conditions associated with this syndrome and develop a treatment that addresses this cause. In phase 1, we propose to:   Clinically define CVS by observing the incidence of ocular surface abnormalities in symptomatic subjects and compare them with an age and sex matched non-symptomatic control population   Develop specialized micro-environment glasses to combat CVS symptoms   Study the efficacy of micro-environment glasses in symptomatic and control populations   Critically evaluate viability of CVS micro-environment glasses as a commercial product      using both statistical methods and subjective questionnaires.            n/a",Micro-environment Glasses as a Treatment for CVS,6792878,R41EY015023,"['age difference', 'bioengineering /biomedical engineering', 'biomedical equipment development', 'clinical biomedical equipment', 'clinical research', 'computers', 'data collection methodology /evaluation', 'eye disorder diagnosis', 'gender difference', 'human subject', 'keratoconjunctivitis sicca', 'occupational health /safety', 'portable biomedical equipment', 'questionnaires', 'syndrome', 'vision aid', 'vision disorders', 'visual photosensitivity', 'work site']",NEI,"SEEFIT, INC.",R41,2004,100000,-0.015969458569303784
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6766751,P30EY006883,"['biomedical facility', 'health science research', 'vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2004,593925,0.030081459944841037
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6710523,R43EY014487,"['artificial intelligence', ' biomedical equipment development', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection', ' digital imaging', ' functional ability', ' human subject', ' image processing', ' medical rehabilitation related tag', ' patient oriented research', ' portable biomedical equipment', ' questionnaires', ' vision aid', ' vision disorders', ' visual fields', ' visual perception', ' visual threshold', ' visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2003,139234,0.046983800037901924
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6665322,R43EY014487,"['artificial intelligence', ' biomedical equipment development', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection', ' digital imaging', ' functional ability', ' human subject', ' image processing', ' medical rehabilitation related tag', ' patient oriented research', ' portable biomedical equipment', ' questionnaires', ' vision aid', ' vision disorders', ' visual fields', ' visual perception', ' visual threshold', ' visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2003,245656,0.046983800037901924
"Physiological Controller for Rotary Blood Pumps DESCRIPTION (provided by applicant):    As the prospects of chronic mechanical assistance for the failing human heart are fast becoming a reality, and patients are indeed returning home to regain a normal lifestyle, the limitations of this technology upon quality of life are becoming more apparent. To address many of these limitations, investigators are developing next-generation ventricular assist devices. Based on turbopump technology, these new devices offer smaller size, greater efficiency (hence smaller batteries), high reliability, and are more cost effective as compared to their pulsatile predecessors. For all the virtues of these new turbopumps, they bring additional challenges. Arguably the most urgent is the need for added ""intelligence."" These relatively ignorant devices are highly dependent on feedback-control to provide normal physiological response. The goal of the Phase- II effort proposed herein is to design a robust controller that may be incorporated into these turbodynamic pump systems for clinical use. The primary end product of this program would be a validated algorithm, in the form of firmware that will be embedded into existing rotary pump controller -- capable of maintaining optimal perfusion of the patient under a variety of hemodynamic demands and disturbances, while avoiding deleterious conditions such as ventricular suction. n/a",Physiological Controller for Rotary Blood Pumps,6690298,R44HL066656,"['artificial intelligence', ' auxiliary heart prosthesis', ' biomedical device power system', ' biomedical equipment development', ' cardiac output', ' circulatory assist', ' computer system design /evaluation', ' cow', ' electrophysiology', ' hemolysis', ' mathematical model', ' microprocessor /microchip', ' sheep']",NHLBI,"LAUNCHPOINT TECHNOLOGIES, INC.",R44,2003,370076,0.012282641139187131
"Smart Power Assistance Module for Manual Wheelchairs    DESCRIPTION (provided by applicant): We propose to use power assistance as the basis for a Smart Power Assistance Module (SPAM) that provides independent power assistance to the right and left rear wheels of a manual wheelchair. The SPAM will detect obstacles near the wheelchair, and modify the forces applied to each wheel to avoid obstacles.  For individuals with visual impairments that are unable to walk with a long cane or walker, the SPAM will provide safe travel by assisting the user to avoid obstacles. This research will build on the investigative team's previous experience with power assistance for manual wheelchairs and obstacle avoidance for power wheelchairs and rollators. Extensive outside evaluation of the SPAM will be provided throughout the course of the project by clinicians active in wheelchair seating and mobility.         n/a",Smart Power Assistance Module for Manual Wheelchairs,6667132,R43EY014490,"['artificial intelligence', ' assistive device /technology', ' biomedical device power system', ' biomedical equipment development', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' field study', ' human subject', ' medical rehabilitation related tag', ' vision aid', ' vision disorders']",NEI,AT SCIENCES,R43,2003,209800,0.011345564190739694
"Visual & Interactive Issues in the Design of Web Surveys    DESCRIPTION (provided by applicant): The rapid acceptance of the Worldwide Web as a vehicle for survey data collection raises important questions about how the new method works. Key features of Web surveys include the use of rich visual presentation of questions and the capability of interaction with the respondent. The rapid growth of the Web makes a close examination of these issues even more urgent. Neither set of features has been explored thoroughly even with earlier modes and the Web offers widely increased resources for both visual display (Web questionnaires can readily incorporate still pictures or video clips) and interaction (such as, floating screens and scrolling for help with definitions). Our application outlines a set of studies designed to address key questions about these issues. The studies focus on Web surveys, but we believe that the results would generalize to other modes of data collection that rely on visual presentation or incorporate interactive design features.   Experiments 1-5 examine how respondents interpret the visual cues in Web questionnaires. These studies test the general proposition that incidental features of the presentation of the questions (for example, the spacing of the response options, the color assigned to different response options) can give rise to unintended inferences about their meaning. These studies test predictions derived from a theoretical framework that assumes respondents use simple interpretive heuristics to assign meaning to visual features of the questions. The next two experiments examine the effects of including images as a supplement to the text of the question. Images are necessarily concrete, and Experiment 6 tests the hypothesis that this concreteness may lead respondents to interpret the questions more narrowly when they are accompanied by images. Experiment 7 tests the idea that the item depicted in an image may serve as a standard of comparison for respondents' judgments. Again, the results of these studies will lead to practical guidelines about the dangers involved in using images as an adjunct to verbal questions. The final series of studies examines when respondents are likely to take advantage of interactive features of a questionnaire. These experiments test three general hypotheses; respondents are more likely to utilize the information available to them interactively when 1) the information is easy to obtain, 2) it is clearly helpful, and 3) respondents are highly motivated to seek help. These six experiments would yield a better understanding of methods for getting respondents to use features that could yield better survey data.         n/a",Visual & Interactive Issues in the Design of Web Surveys,6629976,R01HD041386,"['Internet', ' artificial intelligence', ' attitude', ' behavior prediction', ' behavior test', ' behavioral /social science research tag', ' clinical research', ' computer human interaction', ' cues', ' data collection methodology /evaluation', ' human subject', ' imagery', ' interactive multimedia', ' mathematics', ' population survey', ' questionnaires', ' space perception', ' visual perception']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2003,205876,0.006017116378966218
"Locating and Reading Informational Signs  DESCRIPTION (provided by applicant): Our goal is to construct computer vision systems to enable the blind and severely visually impaired to detect and read informational text in city scenes. The informational text can be street signs, bus numbers hospital signs, supermarket signs, and names of products (eg. Kellogg's cornflakes). We will construct portable prototype computer vision systems implemented by digital cameras attached to personal, or hand held, computers. The camera need only be pointed in the general direction of the text (so the text is only one percent of the image). A speech synthesizer will read the text to the user. Blind and visually impaired users will test the device in the field (under supervision) and give feedback to improve the algorithms. We argue that this work will make a significant contribution to improving human health (rehabilitation). Computer vision is a rapidly maturing technology with immense potential to help the blind and visually impaired. Reports suggest that detecting and reading informational text is one of the main unsatisfied desires of these groups. Written signs and information in the environment are used for navigation, shopping, operating equipment, identifying buses, and many other purposes (to which a blind person does not otherwise have independent access). The blind and severely visually impaired make up a large fraction of the US population (3 million). Moreover, this proportion is expected to increase by a factor of two in the next ten years due to increased life expectancy. Our proposal is design-driven. It uses a new class of computer vision algorithms known as Data Driven Monte Carlo (DDMCMC). The algorithms are used to: (i) search for text, and (ii) to read it. Recent developments in digital cameras and portable/handheld computers make it practical to implement these algorithms in portable prototype systems. The three scientists in this proposal have the necessary expertise to accomplish it. Dr.'s Yuille and Zhu have backgrounds in computer vision and Dr. Brabyn has experience in developing and testing engineering systems to help the blind and visually impaired. Our proposal falls within the scope of the Bioengineering initiative because we are applying techniques from the mathematical/engineering sciences to develop informatic approaches for patient rehabilitation. More specifically, our work will facilitate the development of portable devices to help the blind and visually impaired.   n/a",Locating and Reading Informational Signs,6666671,R01EY013875,"['blind aid', ' blindness', ' clinical research', ' computer human interaction', ' computer program /software', ' cues', ' human subject', ' reading', ' vision aid', ' vision disorders']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2003,327524,0.040659008014867125
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6635595,P30EY006883,"['biomedical facility', ' health science research', ' vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2003,576626,0.030081459944841037
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6580977,R43EY014487,"['artificial intelligence', ' biomedical equipment development', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection', ' digital imaging', ' functional ability', ' human subject', ' image processing', ' medical rehabilitation related tag', ' patient oriented research', ' portable biomedical equipment', ' questionnaires', ' vision aid', ' vision disorders', ' visual fields', ' visual perception', ' visual threshold', ' visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2002,246164,0.046983800037901924
"Applying Usability to A Knowledge Based System A cancer genetics-tracking database will be redesigned using usability engineering techniques to improve the functionality and usability of the current system. This is important because it will lead to a system that is easier to use and learn, will decrease the chance of errors, and will increase productivity, and user satisfaction. The current state of informatics offers the potential for the creation of tools to assist in the reduction of medical errors. The redesign of this tracking database will be completed through a three-phase process. The first phase will use the results of a usability analysis to redesign and prototype the cancer genetics-tracking database. In the second phase, usability studies will then be conducted to ensure that the system is functional, easy to use, easy to learn, and meets the goals of the users. The usability studies will include heuristic evaluations, keystroke level models, talk-aloud methods, and cognitive walkthrough techniques. The system will be modified based upon the results of these studies. Research will be compiled on the advantages and disadvantages of ICD coding Vs. SNOMED followed by the selection of the most useful system for coding medical information. In the third phase the final redesign will be compared to the old system using a within-subject design to determine if the redesign decreases the error rate, increases productivity, and user satisfaction. This will be followed-up with a survey to determine the perceived usability of the redesigned application. Throughout the redesign process, specific usability guidelines will be developed for designing healthcare software that is computational and knowledge- based in nature.  n/a",Applying Usability to A Knowledge Based System,6538226,F38LM007188,"['artificial intelligence', ' cancer information system', ' cancer registry /resource', ' computer assisted medical decision making', ' computer human interaction', ' computer system design /evaluation', ' family genetics', ' human data', ' neoplasm /cancer genetics']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,F38,2002,66954,-0.0049769588885673795
"COMMUNICATION AID UTILIZING WORD LEVEL DISAMBIGUATION There are approximately 2.5 million people in the US who are speech impaired to the extent that it is considered a functional limitation.  Today, many people with severe communication disabilities lack access to electronic and even printed material, have a lack of opportunity for interaction and opportunity for self-advocacy, and experience isolation.  Providing accessibility to wireless voice, data and Internet communications directly from the device is of tremendous importance to people with severe communication disabilities. The primary objective under the Phase I grant was to investigate the potential of word-level disambiguation technology for text generation on a communication aid to meet the needs of many individuals requiring augmentative and alternative communication (AAC).  Phase I findings validated T9 technology as a viable method for text generation and also AAC device users want to access wireless voice, data and Internet communications directly from the device.  The specific aims of Phase II are to: investigate hardware platforms for AAC device host candidates, develop Windows software modules for T9 and AAC, develop portable AAC resources, integrate wireless voice and data communications, and conduct usability testing of the product as it develops. PROPOSED COMMERCIAL APPLICATION The outcome of Phase II will be a communication aid device for production in Phase III that is based on commercially available hardware with minimal custom AAC hardware support.  The goal is that the software platform developed in Phase II will be ported to existing low-cost hardware as much as possible to: make use of newly developed platforms, provide more choice to AAC device users, provide more flexibility in user interface, and reduce overall device costs to the end user when commercially manufactured.  n/a",COMMUNICATION AID UTILIZING WORD LEVEL DISAMBIGUATION,6569890,R44RR013191,"['Internet', ' artificial intelligence', ' biomedical equipment development', ' clinical biomedical equipment', ' clinical research', ' communication disorder aid', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' human subject', ' online computer', ' vocabulary', ' voice']",NCRR,"MADENTEC (USA), INC.",R44,2002,139082,0.024176091564276782
"A Personal Status Monitor for the Home The purpose of this Phase II SBIR is to develop a portable and completely wireless system that detects, processes, and analyzes muscle activity for remotely monitoring the functional status of an individual. The Personal Status Monitor (PSM) will remotely monitor the use of muscles during the exertion of motor tasks in a continuous and unobstructed fashion. Through pattern recognition of surface electromyographic (EMG) signals, the PSM will provide the caregiver with an objective parametric measure of how physically active their patient has been, such as walling, sitting, personal care, and feeding. The PSM will consist of three components: 1) four wireless EMG sensors, 2) a body worn transceiver (Repeater), and 3) the Base Station, which processes the signals for pattern recognition and feature extraction. The information can be sent to a remote location via telephone lines or the Internet. The specific aims of this program for Phase II are: Aim 1: To continue with Phase I development of the pattern recognition algorithms in patients with stroke; Aim 2: Design and build a working prototype of the hardware and software for the PSM; and Aim 3: Field test the prototype wireless system among stroke patients in the home environment. PROPOSED COMMERCIAL APPLICATIONS: The MA will primarily augment clinical service by making the line an effective place for rehabilitation. In addition, the PSM will have direct applicability to the field of ergonomics for work-site assessment, or in sports or recreational activities as a feedback device to facilitate training of skilled movements. Numerous other applications in the field of rehabilitation could include monitoring of drug therapies for tremor or other neuromuscular conditions, or home-exercise compliance. The knowledge base and prototypes developed in this project are directly transferable to other acquisition systems for biological signals recorded from the skin, such as EEGs and EKGs.  n/a",A Personal Status Monitor for the Home,6534517,R44AR047272,"['artificial intelligence', ' biomedical equipment development', ' caregivers', ' computer program /software', ' computer system design /evaluation', ' electromyography', ' functional ability', ' home health care', ' human subject', ' muscle function', ' patient monitoring device', ' portable biomedical equipment', ' stroke', ' telemedicine', ' telemetry']",NIAMS,"ALTEC, INC.",R44,2002,404087,-0.03527789755389557
"Smart Power Assistance Module for Manual Wheelchairs    DESCRIPTION (provided by applicant): We propose to use power assistance as the basis for a Smart Power Assistance Module (SPAM) that provides independent power assistance to the right and left rear wheels of a manual wheelchair. The SPAM will detect obstacles near the wheelchair, and modify the forces applied to each wheel to avoid obstacles.  For individuals with visual impairments that are unable to walk with a long cane or walker, the SPAM will provide safe travel by assisting the user to avoid obstacles. This research will build on the investigative team's previous experience with power assistance for manual wheelchairs and obstacle avoidance for power wheelchairs and rollators. Extensive outside evaluation of the SPAM will be provided throughout the course of the project by clinicians active in wheelchair seating and mobility.         n/a",Smart Power Assistance Module for Manual Wheelchairs,6581049,R43EY014490,"['artificial intelligence', ' assistive device /technology', ' biomedical device power system', ' biomedical equipment development', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' field study', ' human subject', ' medical rehabilitation related tag', ' vision aid', ' vision disorders']",NEI,AT SCIENCES,R43,2002,249727,0.011345564190739694
"Locating and Reading Informational Signs  DESCRIPTION (provided by applicant): Our goal is to construct computer vision systems to enable the blind and severely visually impaired to detect and read informational text in city scenes. The informational text can be street signs, bus numbers hospital signs, supermarket signs, and names of products (eg. Kellogg's cornflakes). We will construct portable prototype computer vision systems implemented by digital cameras attached to personal, or hand held, computers. The camera need only be pointed in the general direction of the text (so the text is only one percent of the image). A speech synthesizer will read the text to the user. Blind and visually impaired users will test the device in the field (under supervision) and give feedback to improve the algorithms. We argue that this work will make a significant contribution to improving human health (rehabilitation). Computer vision is a rapidly maturing technology with immense potential to help the blind and visually impaired. Reports suggest that detecting and reading informational text is one of the main unsatisfied desires of these groups. Written signs and information in the environment are used for navigation, shopping, operating equipment, identifying buses, and many other purposes (to which a blind person does not otherwise have independent access). The blind and severely visually impaired make up a large fraction of the US population (3 million). Moreover, this proportion is expected to increase by a factor of two in the next ten years due to increased life expectancy. Our proposal is design-driven. It uses a new class of computer vision algorithms known as Data Driven Monte Carlo (DDMCMC). The algorithms are used to: (i) search for text, and (ii) to read it. Recent developments in digital cameras and portable/handheld computers make it practical to implement these algorithms in portable prototype systems. The three scientists in this proposal have the necessary expertise to accomplish it. Dr.'s Yuille and Zhu have backgrounds in computer vision and Dr. Brabyn has experience in developing and testing engineering systems to help the blind and visually impaired. Our proposal falls within the scope of the Bioengineering initiative because we are applying techniques from the mathematical/engineering sciences to develop informatic approaches for patient rehabilitation. More specifically, our work will facilitate the development of portable devices to help the blind and visually impaired.   n/a",Locating and Reading Informational Signs,6547549,R01EY013875,"['blind aid', ' blindness', ' clinical research', ' computer human interaction', ' computer program /software', ' cues', ' human subject', ' reading', ' vision aid', ' vision disorders']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2002,338540,0.040659008014867125
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6518379,P30EY006883,"['biomedical facility', ' health science research', ' vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2002,559831,0.030081459944841037
"Advanced Vision Intervention Algorithm(AVIA)   Description (from the investigator's abstract): The objective of this                application is to implement an iterative, nine-step advanced vision                  intervention algorithm (AVIA) in software to optimize the predictability of          virtually any current or anticipated customized human vision intervention            method. The software program will use the investigator's Visual Optics class         library, as well as new software for the ray transfer element, database              analysis routines, and the ray tracing surface optimization algorithm. The           program will allow, but not require, exam data from commercially available           ophthalmic instruments such as corneal topography and wavefront aberration for       input in the optical modeling of an individual's eye. This algorithm is, to the      investigator's knowledge, the only formal framework designed specifically to         optimize the predictability of surgical and non-surgical correction methods. It      is not only a technological innovation in its own right, it also makes the most      of the current and future vision correction methods to which it is applied.          PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                                                                     n/a",Advanced Vision Intervention Algorithm(AVIA),6403968,R43EY013666,"['artificial intelligence', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' eye surgery', ' laser therapy', ' ophthalmoscopy', ' statistics /biometry', ' vision disorders']",NEI,"SARVER AND ASSOCIATES, INC.",R43,2001,99785,0.007987587512439987
"PHYSIOLOGICAL CONTROLLER FOR ROTARY BLOOD PUMPS   DESCRIPTION (Verbatim From Applicant's Abstract): As the prospects of a              mechanical replacement for a failing human heart are fast becoming a reality,        and patients are indeed returning home to regain a ""normal"" lifestyle, the           limitations of this technology upon quality of life are becoming more apparent.      To address many of these limitations, investigators are developing                   next-generation ventricular assist devices. Based on turbopump technology,           these new devices offer smaller size, greater efficiency (hence smaller              batteries), high reliability, and are more cost effective as compared to their       pulsatile predecessessors. For all the virtues of these new turbopumps, they         bring with additional challenges. Arguably the most urgent is the need for           added intelligence."" These, relatively stupid, devices are highly dependent on       feedback-control to provide physiological response. Unfortunately, developers        have yet to wage a systematic assault on this problem. Preoccupied with              apparently more urgent issues, such as biocompatibility etc., there has been         relatively little attention or resources directed at developing a physiological      controller.                                                                                                                                                               For the past eight years, the P.I. has had an interest in this problem, and has      conducted basic and applied research towards developing control algorithms. He       now proposes to devise a general-purpose controller product, which can be            incorporated into a variety of rotary pump systems for clinical use.                                                                                                      The goal of the Phase-I effort proposed herein are to design a robust control        algorithm which may then be implemented, in Phase-Il, into an applications           specific integrated circuit. The P.I. envisions that this chip would be              made available to device developers much like control circuits produced by           Intel, Motorola, Texas Instruments, etc., are adopted by a wide variety of           users for their specific products.                                                   PROPOSED COMMERCIAL APPLICATION:  Direct application to virtually all rotary-type blood pumps for critical care and chronic use.  The P.I. envisions that the Antakamatics control chip would be made available to device  developers much like integrated circuits producted by Intel, Motorola, Texas Instruments,   etc. are adopted by a wide variety of users for their specific products.  The market for  this product is estimated to exceed 200,000 units per annum, and there currently exists  no competing product.                                                                                     n/a",PHYSIOLOGICAL CONTROLLER FOR ROTARY BLOOD PUMPS,6292387,R43HL066656,"['artificial intelligence', ' auxiliary heart prosthesis', ' biomedical device power system', ' biomedical equipment development', ' cardiac output', ' circulatory assist', ' computer system design /evaluation', ' electrophysiology', ' microprocessor /microchip']",NHLBI,"ANTAKAMATICS, INC.",R43,2001,98465,0.020845752600563917
"Applying Usability to A Knowledge Based System A cancer genetics-tracking database will be redesigned using usability engineering techniques to improve the functionality and usability of the current system. This is important because it will lead to a system that is easier to use and learn, will decrease the chance of errors, and will increase productivity, and user satisfaction. The current state of informatics offers the potential for the creation of tools to assist in the reduction of medical errors. The redesign of this tracking database will be completed through a three-phase process. The first phase will use the results of a usability analysis to redesign and prototype the cancer genetics-tracking database. In the second phase, usability studies will then be conducted to ensure that the system is functional, easy to use, easy to learn, and meets the goals of the users. The usability studies will include heuristic evaluations, keystroke level models, talk-aloud methods, and cognitive walkthrough techniques. The system will be modified based upon the results of these studies. Research will be compiled on the advantages and disadvantages of ICD coding Vs. SNOMED followed by the selection of the most useful system for coding medical information. In the third phase the final redesign will be compared to the old system using a within-subject design to determine if the redesign decreases the error rate, increases productivity, and user satisfaction. This will be followed-up with a survey to determine the perceived usability of the redesigned application. Throughout the redesign process, specific usability guidelines will be developed for designing healthcare software that is computational and knowledge- based in nature.  n/a",Applying Usability to A Knowledge Based System,6340157,F38LM007188,"['artificial intelligence', ' cancer information system', ' cancer registry /resource', ' computer assisted medical decision making', ' computer human interaction', ' computer system design /evaluation', ' family genetics', ' human data', ' neoplasm /cancer genetics']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,F38,2001,68753,-0.0049769588885673795
"COMMUNICATION AID UTILIZING WORD LEVEL DISAMBIGUATION There are approximately 2.5 million people in the US who are speech impaired to the extent that it is considered a functional limitation.  Today, many people with severe communication disabilities lack access to electronic and even printed material, have a lack of opportunity for interaction and opportunity for self-advocacy, and experience isolation.  Providing accessibility to wireless voice, data and Internet communications directly from the device is of tremendous importance to people with severe communication disabilities. The primary objective under the Phase I grant was to investigate the potential of word-level disambiguation technology for text generation on a communication aid to meet the needs of many individuals requiring augmentative and alternative communication (AAC).  Phase I findings validated T9 technology as a viable method for text generation and also AAC device users want to access wireless voice, data and Internet communications directly from the device.  The specific aims of Phase II are to: investigate hardware platforms for AAC device host candidates, develop Windows software modules for T9 and AAC, develop portable AAC resources, integrate wireless voice and data communications, and conduct usability testing of the product as it develops. PROPOSED COMMERCIAL APPLICATION The outcome of Phase II will be a communication aid device for production in Phase III that is based on commercially available hardware with minimal custom AAC hardware support.  The goal is that the software platform developed in Phase II will be ported to existing low-cost hardware as much as possible to: make use of newly developed platforms, provide more choice to AAC device users, provide more flexibility in user interface, and reduce overall device costs to the end user when commercially manufactured.  n/a",COMMUNICATION AID UTILIZING WORD LEVEL DISAMBIGUATION,6188611,R44RR013191,"['Internet', ' artificial intelligence', ' biomedical equipment development', ' clinical biomedical equipment', ' clinical research', ' communication disorder aid', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' human subject', ' online computer', ' vocabulary', ' voice']",NCRR,"MADENTEC (USA), INC.",R44,2001,316991,0.024176091564276782
"A Personal Status Monitor for the Home The purpose of this Phase II SBIR is to develop a portable and completely wireless system that detects, processes, and analyzes muscle activity for remotely monitoring the functional status of an individual. The Personal Status Monitor (PSM) will remotely monitor the use of muscles during the exertion of motor tasks in a continuous and unobstructed fashion. Through pattern recognition of surface electromyographic (EMG) signals, the PSM will provide the caregiver with an objective parametric measure of how physically active their patient has been, such as walling, sitting, personal care, and feeding. The PSM will consist of three components: 1) four wireless EMG sensors, 2) a body worn transceiver (Repeater), and 3) the Base Station, which processes the signals for pattern recognition and feature extraction. The information can be sent to a remote location via telephone lines or the Internet. The specific aims of this program for Phase II are: Aim 1: To continue with Phase I development of the pattern recognition algorithms in patients with stroke; Aim 2: Design and build a working prototype of the hardware and software for the PSM; and Aim 3: Field test the prototype wireless system among stroke patients in the home environment. PROPOSED COMMERCIAL APPLICATIONS: The MA will primarily augment clinical service by making the line an effective place for rehabilitation. In addition, the PSM will have direct applicability to the field of ergonomics for work-site assessment, or in sports or recreational activities as a feedback device to facilitate training of skilled movements. Numerous other applications in the field of rehabilitation could include monitoring of drug therapies for tremor or other neuromuscular conditions, or home-exercise compliance. The knowledge base and prototypes developed in this project are directly transferable to other acquisition systems for biological signals recorded from the skin, such as EEGs and EKGs.  n/a",A Personal Status Monitor for the Home,6403447,R44AR047272,"['artificial intelligence', ' biomedical equipment development', ' caregivers', ' computer program /software', ' computer system design /evaluation', ' electromyography', ' functional ability', ' home health care', ' human subject', ' muscle function', ' patient monitoring device', ' portable biomedical equipment', ' stroke', ' telemedicine', ' telemetry']",NIAMS,"ALTEC, INC.",R44,2001,403938,-0.03527789755389557
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6346620,P30EY006883,"['biomedical facility', ' health science research', ' vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2001,527694,0.030081459944841037
"SIGN FINDER: COMPUTER VISION TO FIND AND READ SIGNS In this Phase l proposal we plan to develop and test a new vision technology to locat and read general informational signs (street names, building directories, office door plates) and location and directional signs (EXIT, Information, aisle signs in supermarkets). To strengthen feasibility, we will target a restricted class of signs: those consisting primarily of one- color text on a different one-color background, and whose shape falls within a prescribed set. The intended market is for people who are blind or whose sight is impaired and hence cannot read these signs unaided. Our approach makes extensive use of recently developed computer vision recognition algorithms. We also make use of the Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for determining what the potential users will require from such a system. The ultimate goal, for Phase II, is to build and test a highly portable PC- based device implementing this vision technology using a CCD camera as input and a voice-generator as output. The user would scan/point the device at a scene and it would locate and read one or more signs. Given the pace of increase in power and decrease in size of computing devices, a hand-held Sign-Finder system may be plausible to build entirely with commercial, off-the-shelf hardware in two to three years. PROPOSED COMMERCIAL APPLICATION: The potential utility to blind and visually impaired individuals is great; a commercial product could have a market potential of 500,000.  n/a",SIGN FINDER: COMPUTER VISION TO FIND AND READ SIGNS,2720318,R43EY011821,"['artificial intelligence', ' blind aid', ' charge coupled device camera', ' computer graphics /printing', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' information display', ' portable biomedical equipment', ' symbolism', ' technology /technique development', ' vision aid']",NEI,BLINDSIGHT CORPORATION,R43,2000,100000,0.061000818204171144
"ADVANCED DIAGNOSTIC LOGIC FOR PSYCHIATRY   DESCRIPTION: (Verbatim from the Applicant's Abstract): This Phase I project          proposes to assess the feasibility of an advanced diagnostic logic system            (Diagnostica) to support the clinical assessment process for diagnosis in            psychiatry. A working prototype of the diagnostic rules in the American              Psychiatric Association Diagnostic and Statistical Manual (DSM-IV) uses and          artificial intelligence engine (""XSB"") to implement the logic of DSM-IV along        with and an interactive graphical user interface to allow a user to add              information and understand conclusions reached by the system. The Phase I            programming objectives are to make Diagnostica ready for commercial use by           improving its graphical user interface, and finalizing implementation of its         logical rules. The resulting system will be a practical tool in clinical             settings, and relies on computer science innovations that have preciously            neither been explored nor applied in the domain of medical reasoning. With the       emergence of decision support systems, the need for better quality diagnostic        information is becoming increasingly apparent. This has been due, in part, to        the complexity of diagnostic processes and the emphasis on support of financial      processes. Within mental health, the DSM-IV provides both a model and a              standard for making diagnoses. A software component that provides flexible,          complete, and efficient application of this standard is of great value. The          innovation of Diagnostica relies on the sophistication of its modeling of            DSM-IV rules, and it's flexibility in applying those rules. Diagnostica will         automatically track the status of the information entered and allow users to         tie up 'loose ends' in documenting the proof of diagnoses formally. AS example,      the user may indicate that a set of diagnoses in 'believed true' without             specifying the symptoms needed to make the diagnoses formally ( a procedure          used routinely in clinical practice). the application will track whatever            'residual' data this is necessary in order to complete formal diagnoses, while       leaving the option of when, or if, to complete the process up to the user.                                                                                                Phase II objectives include: (1) extending Diagnostica to provide other              software applications needing diagnostic decision support services, and              specifically to link Diagnostica to the World Health Organization Schedules for      Clinical Assessment in Neuropsychiatry (SCAN); (2) addressing logical modeling       of time and creating an effective user interface for repeated assessment; (3)        incorporating probabilistic information about sets of symptoms based on              empirical information initially obtained in Phase I; and (4) developing and          testing ""belief revision"" functions to changes in knowledge stemming from            repeated clinical assessment.                                                        PROPOSED COMMERCIAL APPLICATION:                                                                                     Computerization of diagnostic logic for clinical use can improve the quality of      mental health services by efficient standardization of assessment and through        motivating and making more practical the creation of data bases which can be         used for clinical quality improvement and knowledge discovery.                                                                                                            n/a",ADVANCED DIAGNOSTIC LOGIC FOR PSYCHIATRY,6210194,R43MH059420,"['artificial intelligence', ' computer assisted diagnosis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' diagnosis design /evaluation', ' interactive multimedia', ' mental disorder diagnosis', ' psychiatry']",NIMH,"MEDICINE RULES, INC.",R43,2000,98441,0.0009803987878387248
"CLINICAL RESEARCH TOWARD CLOSED LOOP INSULIN DELIVERY DESCRIPTION (Adapted from applicant's abstract): The ""closed-loop                artificial pancreas,"" a device that would measure glucose level and              deliver insulin automatically as needed, has been an elusive goal in the         treatment of diabetes. There are three essential components: the blood           glucose sensor, linking algorithms and the delivery system. For the first        time, a viable sensor and a proven delivery system are now available for         research. The broad goal of this clinical research proposal is to complete       the studies needed to link the sensor to the delivery system, paving the         way for a functional closed-loop artificial pancreas. First, we will make        a detailed analysis of sensor signal as it reflects glucose level in             normal and diabetic humans. Second, we will study the precise                    pharmacokinetics of insulin delivery by external and implantable insulin         pumps. Third, analysis of these two data sets will provide the basis for         algorithms that link the sensor signal to insulin delivery. A formal             safety analysis will evaluate the safety features needed in a closed loop        device. In the last year of the project, the entire system will be tested        and fine-tuned. This project takes advantage of our relatively extensive         investigational experience with mechanical insulin delivery pumps in             people with diabetes, and the recent availability, for research, of a            subcutaneously placed, glucose oxidase-based continuous glucose sensor.          The investigators have established experienced with clinical research in         diabetes, and the resources of an excellent General Clinical Research            Center. The co-investigators have extensive experience with mathematical         modeling of biologic systems. There is a close working relationship              between the research team and the manufacturer of the sensor and pumps, as       reflected by the Interactive Research Project Grant collaboration, and by        a long-standing history of collaboration. It is essential to emphasize           that we do not anticipate completion of a manufacturable, clinically             usable, commercially viable artificial pancreas within the time-frame of         this work. Rather, we aim to complete the basic studies and modeling             analyses that would form the basis of such a system, and demonstrate the         feasibility of linking the sensor to the delivery device. If these studies       and these trials were successful, they would be a major step towards             development of a clinically useful close-loop artificial pancreas.                n/a",CLINICAL RESEARCH TOWARD CLOSED LOOP INSULIN DELIVERY,6177804,R01DK055132,"['artificial endocrine pancreas', ' artificial intelligence', ' biosensor device', ' clinical research', ' drug delivery systems', ' glucose metabolism', ' human subject', ' insulin', ' insulin dependent diabetes mellitus', ' medical implant science', ' pharmacokinetics']",NIDDK,JOHNS HOPKINS UNIVERSITY,R01,2000,252566,0.014828098282595613
"MICRO-OPTICS-BASED DIGITAL ALLERGEN COUNTER The current pollen identification and counting method based on microscopic visual examination is very time consuming and labor intensive. More importantly, it is very ""subjective"" and not truly scientific. Intelligent Optical Systems, Inc. proposes to develop a portable digital allergen counter (DAC) to accurately and reliably count and identify airborne pollen grains and fungal spores. The proposed DAC combines a micro-image scanner, a high-speed video chip, an allergen morphology data bank, and a built-in image processor into an integrated and automated pollen counter. The DAC will rapidly identify and quantify pollen, grains and spores. By making it much easier to collect allergen information in multiple locations, the proposed device will reduce morbidity by providing improved warnings on days with high pollen counts. The specific aims of the Phase I project are to design and construct optical image scanner suitable for allergen detection, identify the morphology of several types of pollen, grains, and spores, integrate the DAC system and test and evaluate the system feasibility. In Phase II, an engineering prototype of a portable instrument will be built and field-validated with real-world samples. We will also expand its capability to increase the pollen types of interest. PROPOSED COMMERCIAL APPLICATIONS: A compact, simple, and easy-to-use digital allergen counting system that can monitor indoor or outdoor air quality that will minimize people's overexposure to allergens.  This device is for aerobiological research that could be beneficial for public health, medical pharmaceutical and engineering applications. Universities, physicians, public health organizations, National Allergy Bureau (NAB) stations, and private air sampling consultants will  purchase the device.  n/a",MICRO-OPTICS-BASED DIGITAL ALLERGEN COUNTER,6211164,R43HL064459,"['air sampling /monitoring', ' allergens', ' artificial intelligence', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' computer program /software', ' computer system design /evaluation', ' image processing', ' monitoring device', ' optics', ' particle counter', ' pollen']",NHLBI,"INTELLIGENT OPTICAL SYSTEMS, INC.",R43,2000,99995,0.013253131986114847
"PORTABLE DROWSINESS MONITORING DEVICE DESCRIPTION: (Applicant's abstract) The feasibility of a Drowsiness              Monitoring Device (DMD) to detect EEG indices of drowsiness in real-             time, was demonstrated during Phase I with an analytical model correctly         classifying 97.1 percent of sleep episodes and 94.3 percent of awake             epochs in 20 sleep-deprived subjects. The model employs discriminant             function analysis (DFA) to characterize and classify one-sec epochs,             validated against a combination of visual scoring by polysomnographers           and/or a behavioral measure. Algorithms to detect artifacts in real-time         (EMG, 60-Hz and gross body/eye movements) were developed. This                   classification accuracy represents a significant advancement over                previously reported models and confirms the feasibility of                       distinguishing EEG characteristics of sleep and waking on a second-by-           second basis. Phase II will implement a three-level DFA classification           system to further refine the model, adding sub-class states for                  vigilance and drowsiness/sleep to improve system accuracy. The multi-            dimensional time-series DFA analyses will correlate EEG parameters with          behavioral measures of driving performance to provide quantitative               predictions of performance decrements associate with sleep onset.                The model will be validated using a population with demographics                 consistent with the target market for the DMD (e.g., truck drivers)              during Phase II. In addition, the system will be evaluated with the              introduction of commonly used legal drugs (caffeine, nicotine, and cold          medications) to determine the robustness of the model.                                                                                                            PROPOSED COMMERCIAL APPLICATION:                                                 The DMD provides three levels of user safety. When sleep onset is                approaching, the DMD will initiate a verbal warning alarm that must be           turned off by the user. Alternatively, the DMD can provide verbal                feedback to ensure the user maintains high levels of alertness during            activities that require sustained vigilance. The user can also select            the option for the DMD to recommend the optimal time to take a short             nap, monitor the length of the nap and awaken the user at the                    appropriate time.                                                                                                                                                 Currently, more that 10% of the U.S. workforce or an estimated 20                million people are engaged in night sift work. The transportation                industry, including airline, railroad, marine and highway transportation         companies, is the nation's third largest employer of shift workers. Long         haul truck drivers, in particular, are vulnerable to sleepiness because          they drive through the night, in most cases unaccompanied, and generally         sleep less than 6 hours per day at irregular intervals. In addition, an          estimated 6 million Americans suffer from chronic sleep disorders which          make them vulnerable to fatigue in the workplace.                                 n/a",PORTABLE DROWSINESS MONITORING DEVICE,6139531,R44NS035387,"['artificial intelligence', ' attention', ' biomedical equipment development', ' clinical biomedical equipment', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' electroencephalography', ' human subject', ' patient monitoring device', ' polysomnography', ' portable biomedical equipment', ' sleep', ' wakefulness']",NINDS,"ADVANCED BRAIN MONITORING, INC.",R44,2000,347762,0.0196231461920225
"PORTABLE DROWSINESS MONITORING DEVICE DESCRIPTION: (Applicant's abstract) The feasibility of a Drowsiness              Monitoring Device (DMD) to detect EEG indices of drowsiness in real-             time, was demonstrated during Phase I with an analytical model correctly         classifying 97.1 percent of sleep episodes and 94.3 percent of awake             epochs in 20 sleep-deprived subjects. The model employs discriminant             function analysis (DFA) to characterize and classify one-sec epochs,             validated against a combination of visual scoring by polysomnographers           and/or a behavioral measure. Algorithms to detect artifacts in real-time         (EMG, 60-Hz and gross body/eye movements) were developed. This                   classification accuracy represents a significant advancement over                previously reported models and confirms the feasibility of                       distinguishing EEG characteristics of sleep and waking on a second-by-           second basis. Phase II will implement a three-level DFA classification           system to further refine the model, adding sub-class states for                  vigilance and drowsiness/sleep to improve system accuracy. The multi-            dimensional time-series DFA analyses will correlate EEG parameters with          behavioral measures of driving performance to provide quantitative               predictions of performance decrements associate with sleep onset.                The model will be validated using a population with demographics                 consistent with the target market for the DMD (e.g., truck drivers)              during Phase II. In addition, the system will be evaluated with the              introduction of commonly used legal drugs (caffeine, nicotine, and cold          medications) to determine the robustness of the model.                                                                                                            PROPOSED COMMERCIAL APPLICATION:                                                 The DMD provides three levels of user safety. When sleep onset is                approaching, the DMD will initiate a verbal warning alarm that must be           turned off by the user. Alternatively, the DMD can provide verbal                feedback to ensure the user maintains high levels of alertness during            activities that require sustained vigilance. The user can also select            the option for the DMD to recommend the optimal time to take a short             nap, monitor the length of the nap and awaken the user at the                    appropriate time.                                                                                                                                                 Currently, more that 10% of the U.S. workforce or an estimated 20                million people are engaged in night sift work. The transportation                industry, including airline, railroad, marine and highway transportation         companies, is the nation's third largest employer of shift workers. Long         haul truck drivers, in particular, are vulnerable to sleepiness because          they drive through the night, in most cases unaccompanied, and generally         sleep less than 6 hours per day at irregular intervals. In addition, an          estimated 6 million Americans suffer from chronic sleep disorders which          make them vulnerable to fatigue in the workplace.                                 n/a",PORTABLE DROWSINESS MONITORING DEVICE,6223739,R44NS035387,"['artificial intelligence', ' attention', ' biomedical equipment development', ' clinical biomedical equipment', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' electroencephalography', ' human subject', ' patient monitoring device', ' polysomnography', ' portable biomedical equipment', ' sleep', ' wakefulness']",NINDS,"ADVANCED BRAIN MONITORING, INC.",R44,2000,41556,0.0196231461920225
"Ghost in the Machine: Melding Brain, Computer and Behavior Implantable devices are playing a greater role in neurologic care, but their effectiveness is limited, because they are blind to human thoughts, feelings, and behavior – factors that most dramatically affect our health. Coupling peripheral sensors to implants might help, but wouldn’t it be easier if the devices just asked us? Armed with this knowledge, next generation machines will more effectively drive neural activity in the brain to healthy states. They will also quickly learn behaviors that worsen health and guide us to better choices. Though DARPA, the NIH, and Neuralink are spending millions of dollars on new hardware for brain-computer interfaces, none focus on reciprocal, natural communication between host and machine. There is a desperate need for novel, practical methods that enable devices to learn from and guide human behavior.  In this application I propose to develop a new generation of autonomous brain-machine interfaces – devices that can question, record, act - and combine learning algorithms applied to neurosignals with teaching by their human hosts. Life with these implants will entail a subtle human- machine dialogue in which devices and humans teach and learn from each other. Humans will inform intelligent algorithms about what we are doing and feeling, while machines will incorporate this information into therapy and guide us to optimize quality of life in personalized ways. This is a paradigm shift from today’s simple devices, which are programmed by physicians during occasional office visits. I propose to demonstrate this paradigm in a practical, scalable way using current epilepsy implants that is rapidly translatable to many neurological disorders.  To achieve this goal, I will meld several cutting-edge technologies in novel ways, including: (1) State-of-the-art, high bandwidth implantables that sample neural activity, link to vast cloud- based computational power to process it, and intervene to modulate brain, spinal cord or peripheral neural activity. This work utilizes my experience from the past 20 years; (2) I will deploy powerful new computer science tools in novel ways. I will use convolutional neural nets (a.k.a. Deep Learning) to learn patterns from vast streams of continuous high-bandwidth neural data, build a two way human-machine interface using Natural Language Processing (NLP)., and probe networks with changes in human behavior and electrical stimulation and guide interventions toward therapeutic goals using Reinforcement Learning. Combining these computer science, machine learning techniques and measurements of human behavior is a new area of investigation for me that will leverage my unique background in clinical neurology and engineering to build a new class of interactive, human therapeutic devices. The goal of this project is to develop a revolutionary, new generation of implantable neurodevices that will communicate with, learn from, and teach their human hosts to better treat disease. Current implantable devices are blind to human actions, thoughts and behavior, which limits their effectiveness. State of the art computer techniques that can measure behavior and link them to brain activity will empower patients to teach, control and learn from their devices to improve health and quality of life.","Ghost in the Machine: Melding Brain, Computer and Behavior",10012013,DP1NS122038,"['Affect', 'Area', 'Behavior', 'Brain', 'Caring', 'Clinical', 'Communication', 'Computers', 'Coupling', 'Data', 'Devices', 'Disease', 'Educational process of instructing', 'Effectiveness', 'Electric Stimulation', 'Engineering', 'Epilepsy', 'Feeling', 'Generations', 'Goals', 'Health', 'Human', 'Implant', 'Intervention', 'Investigation', 'Knowledge', 'Learning', 'Life', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Natural Language Processing', 'Neurologic', 'Neurology', 'Office Visits', 'Patients', 'Pattern', 'Peripheral', 'Physicians', 'Play', 'Process', 'Psychological reinforcement', 'Quality of life', 'Role', 'Sampling', 'Spinal Cord', 'Stream', 'Techniques', 'Technology', 'Therapeutic', 'Thinking', 'United States National Institutes of Health', 'User-Computer Interface', 'Work', 'blind', 'brain computer interface', 'brain machine interface', 'cloud based', 'computer science', 'convolutional neural network', 'deep learning', 'experience', 'implantable device', 'improved', 'intelligent algorithm', 'learned behavior', 'learning algorithm', 'nervous system disorder', 'next generation', 'novel', 'relating to nervous system', 'sensor', 'tool']",NINDS,UNIVERSITY OF PENNSYLVANIA,DP1,2020,1134000,-0.010159444214290237
"Fall Detection and Prevention for Memory Care through Real-Time Artificial Intelligence Applied to Video Abstract In the US, Alzheimer’s disease (AD) is the single most expensive disease, the only one in the top six for which the number of deaths is increasing. The greatest costs are hospitalizations, where falls are the largest culprit, and frequent need for assistance with daily life activities. A fall safety system shows the potential to reduce costs and increase quality of care by reducing the likelihood of emergency events (e.g., detecting falls before a fracture occurs, reducing the number of repeat falls). Unfortunately, no fall detection and prevention technology has been developed specifically for the needs of dementia care where individuals (1) fall more frequently and (2) often cannot tell care staff how they fell, leading to increased use of Emergency Medical Services (EMS) when falls are unwitnessed to ensure affected individuals are safe. Our goal is to perform a randomized wait-list control clinical trial (n=460) of SafelyYou Guardian, an online fall detection system with wall-mounted cameras to automatically detect falls for residents with AD and related dementias (ADRD). The automation is based on algorithms that push the frontier of deep learning, a subfield of Artificial Intelligence (AI), with a human-in-the- loop (HIL). SafelyYou Guardian is designed to primarily operate in memory care facilities (defined herein as assisted living and skilled nursing facilities providing ADRD care). Deep learning has already revolutionized several fields: robotics, self-driving cars, social networks in particular. Our approach is anchored in novel algorithms developed at the Berkeley AI Research Lab (BAIR) and extended by SafelyYou for real-time detection of rare events in video. The HIL is operating from a call center, confirms the fall detection alerts provided by our artificial intelligence algorithms, and places a call to the communities, so an intervention can happen within minutes of the fall detection. Subsequently, an Occupational Therapist (OT) working from our office in San Francisco reviews the fall videos with the front-line staff over video conference and using our web portal to make recommendations on how to re-organize the resident space (intervention) to prevent future falls. We leverage our HIL paradigm, in which our deep learning approach identifies and pre-filters falls with high sensitivity followed by a human who confirms the fall with high specificity and calls the communities in case of detected fall. This project leverages past small scale clinical and technical pilots including 87 residents from 11 partner communities, and our experience with paid commitments for 480 residents from three partner networks. Past pilots leading to this NIH Phase II proposal include:  · Pilot 1: Technical proof of concept with healthy subjects (200 acted falls).  · Pilot 2: We demonstrated acceptance of privacy/safety tradeoffs by residents, family and  staff, through the collection of 3 months of video data at WindChime of Marin, our first  partner facility; we identified 4 total hours of fall data. This led to clinical benefits  including an 80% fall reduction through the intervention of OT. · Pilot 3: We demonstrated scalability and acceptance by deploying the system in 11  communities, for 87 residents monitored by our system (offline, no HIL intervention). · Pilot 4: Small scale NIH Phase I clinical trial. We demonstrated the ability to perform real-time fall detection, with real-time intervention of the HIL through our partner company Magellan-Solutions which provides the 24/7 monitoring service for the facilities. We demonstrated that 93% of 89 falls were detected, that time on the ground was reduced by 42%, that the likelihood of EMS use was 50% lower with video available, and the that total facility falls including participants and non-participants decreased by 38%. The trial proposed for this NIH SBIR Phase II will provide clinical evidence that the preliminary trends observed experimentally (pilot 2) and at small scale (pilot 4) are true phenomena. It will use a wait-list control population (230 residents) to be compared to the population monitored with SafelyYou Guardian (230 residents). After crossover, the wait-list population will also benefit from the technology and be compared to itself before crossover. Narrative The goal of this project is to perform a randomized, wait-list controlled trial (n=460) of SafelyYou Guardian, an online fall detection and prevention system for memory care facilities (defined here as skilled nursing and assisted living facilities providing dementia care). The technology applies breakthroughs in artificial intelligence to video data collected from off- the-shelf, wall-mounted cameras to automatically detect falls from video for residents with Alzheimer’s disease and related dementias (ADRD); it enables care staff (1) to know about falls right away without requiring residents wear a device, (2) to use video review to quickly assess need for emergency medical services (EMS) after unwitnessed falls, and (3) to perform accurate incident review with support from a remote occupational therapist (OT) to assess how to reduce the risk of repeat falls. The Phase I project goal of launching this service at small scale was achieved and demonstrated with 11 memory care facilities (1) 93% of 89 falls detected, generating 1 alarm per camera per 15 days, (2) 37% reduction in EMS use through better understanding of risk when residents with dementia were found on the floor, and (3) 38% reduction in falls through reduced risk of repeat falls following OT video review.",Fall Detection and Prevention for Memory Care through Real-Time Artificial Intelligence Applied to Video,10020322,R44AG058354,"['Accidents', 'Address', 'Adult', 'Affect', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease related dementia', 'Artificial Intelligence', 'Assisted Living Facilities', 'Automation', 'Automobile Driving', 'Awareness', 'Beds', 'Caregivers', 'Caring', 'Cessation of life', 'Clinical', 'Clinical Trials', 'Cognitive', 'Collection', 'Communities', 'Control Groups', 'Data', 'Dementia', 'Detection', 'Devices', 'Discipline of Nursing', 'Disease', 'Emergency Situation', 'Emergency department visit', 'Emergency medical service', 'Ensure', 'Event', 'Family', 'Floor', 'Fracture', 'Future', 'Goals', 'Health Care Costs', 'Health care facility', 'Hospital Costs', 'Hospitalization', 'Hour', 'Human', 'Individual', 'Intervention', 'Lead', 'Letters', 'Life', 'Measures', 'Medical', 'Memory', 'Monitor', 'Morbidity - disease rate', 'Notification', 'Occupational Therapist', 'Outcome', 'Participant', 'Persons', 'Phase', 'Phase I Clinical Trials', 'Population', 'Population Control', 'Prevention', 'Privacy', 'Quality of Care', 'Quality of life', 'Randomized', 'Recommendation', 'Research', 'Risk', 'Risk Factors', 'Robotics', 'Safety', 'Sample Size', 'San Francisco', 'Series', 'Services', 'Skilled Nursing Facilities', 'Small Business Innovation Research Grant', 'Social Network', 'Specificity', 'Statistical Data Interpretation', 'Stream', 'System', 'Technology', 'Time', 'United States National Institutes of Health', 'Visit', 'Waiting Lists', 'base', 'care costs', 'cohort', 'cost', 'deep learning', 'dementia care', 'design', 'experience', 'falls', 'frontier', 'human-in-the-loop', 'improved', 'intelligent algorithm', 'memory care', 'mortality', 'novel', 'phase 1 study', 'prevent', 'sensor', 'standard of care', 'symposium', 'trend', 'web portal']",NIA,"SAFELYYOU, INC.",R44,2020,493633,-0.008239318836446448
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9899994,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2020,416374,0.06415479102337215
"Virtual prototyping for retinal prosthesis patients Project Summary/Abstract Retinal dystrophies such as retinitis pigmentosa and macular degeneration induce progressive loss of photoreceptors, resulting in profound visual impairment in more than ten million people worldwide. Visual neuroprostheses (‘bionic eyes’) aim to restore functional vision by electrically stimulating remaining cells in the retina, analogous to cochlear implants. A wide variety of neuroprostheses are either in development (e.g. optogenetics, cortical) or are being implanted in patients (e.g. subretinal or epiretinal electrical). A limiting factor that affects all device types are perceptual distortions and subsequent loss of information, caused by interactions between the implant technology and the underlying neurophysiology. Understanding the causes of these distortions and finding ways to alleviate them is critically important to the success of current and future sight restoration technologies. In this proposal, human visual psychophysics, computational modeling, data-driven approaches, and virtual reality (VR) will be combined to develop and experimentally validate optimized stimulation protocols for epiretinal prostheses. This approach is analogous to virtual prototyping for airplanes and other complex systems: to use a high-quality model of both the implant electronics and the visual system in order to generate a ‘virtual patient’. Retinal electrophysiological and visual behavioral data will be used to develop and validate a computational model of the expected visual experience of patients when electrically stimulated. One way of using this model will be to generate simulations of the expected perceptual outcome of electrical stimulation across a wide variety of electrical stimulation patterns. These will be used as a training set for machine learning algorithms that will invert the input-output function of the model to find the electrical stimulation protocol that best replicates any desired perceptual experience. The model can also be used to simulate the expected perceptual experience of real patients by using sighted subjects in a VR environment – ‘VR virtual patients’. These virtual patients will be used to discover preprocessing methods (e.g., edge enhancement, retargeting, decluttering) that improve behavioral performance in VR. Although current retinal prostheses have been implanted in over 250 patients worldwide, experimentation with improved stimulation protocols remains challenging and expensive. Implementing ‘virtual patients’ in VR offers an affordable and practical alternative for high-throughput experiments to test new stimulation protocols. Stimulation protocols that result in good VR performance will be experimentally validated in real prosthesis patients in collaboration with Second Sight Medical Products Inc. and Pixium Vision, two leading device manufacturers in the field. This work has the potential to significantly improve the effectiveness of visual neuroprostheses as a treatment option for individuals suffering from blinding retinal diseases. Project Narrative Inadequate stimulation paradigms are currently one of the main factors limiting the effectiveness of visual prostheses as a treatment option for individuals suffering from blinding retinal diseases. My goal is to develop and validate novel stimulation protocols for visual prosthesis patients that minimize perceptual distortions and thereby improve behavioral performance. Developing methods for generating better stimulation protocols through a combination of behavioral testing, virtual reality, computational modeling, and machine learning, has the potential to provide a transformative improvement of this device technology.",Virtual prototyping for retinal prosthesis patients,10200240,R00EY029329,"['Affect', 'Behavioral', 'Bionics', 'Cells', 'Clinical Trials', 'Cochlear Implants', 'Collaborations', 'Complex', 'Computer Models', 'Computer Vision Systems', 'Data', 'Development', 'Devices', 'Effectiveness', 'Electric Stimulation', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Eye', 'Eye Movements', 'Family', 'Financial compensation', 'Future', 'Goals', 'Head', 'Human', 'Implant', 'In Vitro', 'Individual', 'Knowledge', 'Learning', 'Letters', 'Machine Learning', 'Macular degeneration', 'Manufacturer Name', 'Medical', 'Medicare', 'Methods', 'Modeling', 'Motion', 'Neurons', 'Ocular Prosthesis', 'Online Systems', 'Outcome', 'Output', 'Patients', 'Pattern', 'Perceptual distortions', 'Performance', 'Photoreceptors', 'Prosthesis', 'Prosthesis Design', 'Protocols documentation', 'Psychophysics', 'Rehabilitation therapy', 'Reporting', 'Retina', 'Retinal Diseases', 'Retinal Dystrophy', 'Retinitis Pigmentosa', 'Schedule', 'Severities', 'Shapes', 'Specialist', 'Stimulus', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Psychophysics', 'Visual impairment', 'Visual system structure', 'Visualization', 'Work', 'base', 'behavior measurement', 'behavior test', 'deep neural network', 'design', 'experience', 'experimental study', 'gaze', 'implantation', 'improved', 'machine learning algorithm', 'neurophysiology', 'neuroprosthesis', 'novel', 'object recognition', 'optogenetics', 'predictive modeling', 'prototype', 'regression algorithm', 'restoration', 'retinal prosthesis', 'simulation', 'spatiotemporal', 'success', 'virtual', 'virtual reality', 'virtual reality environment']",NEI,UNIVERSITY OF CALIFORNIA SANTA BARBARA,R00,2020,247272,0.0026468985072768338
"CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision   To understand and navigate the environment, sensory systems must solve simultaneously two competing and challenging tasks: the segmentation of a sensory scene into individual objects and the grouping of elementary sensory features to build these objects. Understanding perceptual grouping and segmentation is therefore a major goal of sensory neuroscience, and it is central to advancing artificial perceptual systems that can help restore impaired vision. To make progress in understanding image segmentation and improving algorithms, this project combines two key components. First, a new experimental paradigm that allows for well-controlled measurements of perceptual segmentation of natural images. This addresses a major limitation of existing data that are either restricted to artificial stimuli, or, for natural images, rely on manual labeling and conflate perceptual, motor, and cognitive factors. Second, this project involves developing and testing a computational framework that accommodates bottom-up information about image statistics and top-down information about objects and behavioral goals. This is in contrast with the paradigmatic view of visual processing as a feedforward cascade of feature detectors, that has long dominated computer vision algorithms and our understanding of visual processing. The proposed approach builds instead on the influential theory that perception requires probabilistic inference to extract meaning from ambiguous sensory inputs. Segmentation is a prime example of inference on ambiguous inputs: the pixels of an image often cannot be labeled with certainty as grouped or segmented. This project will test the hypothesis that human visual segmentation is a process of hierarchical probabilistic inference. Specific Aim 1 will determine whether the measured variability of human segmentations reflects the uncertainty predicted by the model, as required for well-calibrated probabilistic inference. Specific Aim 2 addresses how feedforward and feedback processing in human segmentation contribute to efficient integration of visual features across different levels of complexity, from small contours to object parts. Specific Aim 3 will determine reciprocal interactions between perceptual segmentation and top-down influences including: semantic scene content; visual texture discrimination; and expectations reflecting environmental statistics. The proposed approach models these influences as Bayesian priors, and thus, if supported by the proposed experiments, will offer a unified framework to understand the integration of bottom-up and top- down influences in human segmentation of natural inputs. RELEVANCE (See instructions): This project aims to provide a unified understanding of perceptual segmentation and grouping of visual inputs encountered in the natural environment, through correct integration of the information contained in the visual inputs with top-down information about objects and behavioral goals. This understanding is central to advancing artificial perceptual systems that can help restore impaired vision in patient populations. n/a",CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision  ,10018924,R01EY031166,"['Address', 'Algorithms', 'Behavioral', 'Cognitive', 'Computer Vision Systems', 'Cues', 'Data', 'Data Set', 'Discrimination', 'Environment', 'Experimental Designs', 'Feedback', 'Goals', 'Grouping', 'Human', 'Image', 'Impairment', 'Individual', 'Influentials', 'Instruction', 'Label', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Measures', 'Mental disorders', 'Modeling', 'Motor', 'Neurodevelopmental Disorder', 'Neurons', 'Participant', 'Perception', 'Process', 'Protocols documentation', 'Recurrence', 'Semantics', 'Sensory', 'Statistical Models', 'Stimulus', 'System', 'Testing', 'Texture', 'Uncertainty', 'Vision', 'Visual', 'Visual Cortex', 'Visual impairment', 'Work', 'base', 'behavior influence', 'computer framework', 'deep learning', 'detector', 'expectation', 'experimental study', 'flexibility', 'imaging Segmentation', 'improved', 'object recognition', 'patient population', 'predictive modeling', 'segmentation algorithm', 'sensory input', 'sensory integration', 'sensory neuroscience', 'sensory system', 'statistics', 'theories', 'vision science', 'visual processing']",NEI,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2020,190044,-0.012353076463514391
"CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision   To understand and navigate the environment, sensory systems must solve simultaneously two competing and challenging tasks: the segmentation of a sensory scene into individual objects and the grouping of elementary sensory features to build these objects. Understanding perceptual grouping and segmentation is therefore a major goal of sensory neuroscience, and it is central to advancing artificial perceptual systems that can help restore impaired vision. To make progress in understanding image segmentation and improving algorithms, this project combines two key components. First, a new experimental paradigm that allows for well-controlled measurements of perceptual segmentation of natural images. This addresses a major limitation of existing data that are either restricted to artificial stimuli, or, for natural images, rely on manual labeling and conflate perceptual, motor, and cognitive factors. Second, this project involves developing and testing a computational framework that accommodates bottom-up information about image statistics and top-down information about objects and behavioral goals. This is in contrast with the paradigmatic view of visual processing as a feedforward cascade of feature detectors, that has long dominated computer vision algorithms and our understanding of visual processing. The proposed approach builds instead on the influential theory that perception requires probabilistic inference to extract meaning from ambiguous sensory inputs. Segmentation is a prime example of inference on ambiguous inputs: the pixels of an image often cannot be labeled with certainty as grouped or segmented. This project will test the hypothesis that human visual segmentation is a process of hierarchical probabilistic inference. Specific Aim 1 will determine whether the measured variability of human segmentations reflects the uncertainty predicted by the model, as required for well-calibrated probabilistic inference. Specific Aim 2 addresses how feedforward and feedback processing in human segmentation contribute to efficient integration of visual features across different levels of complexity, from small contours to object parts. Specific Aim 3 will determine reciprocal interactions between perceptual segmentation and top-down influences including: semantic scene content; visual texture discrimination; and expectations reflecting environmental statistics. The proposed approach models these influences as Bayesian priors, and thus, if supported by the proposed experiments, will offer a unified framework to understand the integration of bottom-up and top- down influences in human segmentation of natural inputs. RELEVANCE (See instructions): This project aims to provide a unified understanding of perceptual segmentation and grouping of visual inputs encountered in the natural environment, through correct integration of the information contained in the visual inputs with top-down information about objects and behavioral goals. This understanding is central to advancing artificial perceptual systems that can help restore impaired vision in patient populations. n/a",CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision  ,10135248,R01EY031166,"['Address', 'Algorithms', 'Behavioral', 'Cognitive', 'Computer Vision Systems', 'Cues', 'Data', 'Data Set', 'Discrimination', 'Environment', 'Experimental Designs', 'Feedback', 'Goals', 'Grouping', 'Human', 'Image', 'Impairment', 'Individual', 'Influentials', 'Instruction', 'Label', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Measures', 'Mental disorders', 'Modeling', 'Motor', 'Neurodevelopmental Disorder', 'Neurons', 'Participant', 'Perception', 'Process', 'Protocols documentation', 'Recurrence', 'Semantics', 'Sensory', 'Statistical Models', 'Stimulus', 'System', 'Testing', 'Texture', 'Uncertainty', 'Vision', 'Visual', 'Visual Cortex', 'Visual impairment', 'Work', 'base', 'behavior influence', 'computer framework', 'deep learning', 'detector', 'expectation', 'experimental study', 'flexibility', 'imaging Segmentation', 'improved', 'object recognition', 'patient population', 'predictive modeling', 'segmentation algorithm', 'sensory input', 'sensory integration', 'sensory neuroscience', 'sensory system', 'statistics', 'theories', 'vision science', 'visual processing']",NEI,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2020,7300,-0.012353076463514391
"Optimizing BCI-FIT: Brain Computer Interface - Functional Implementation Toolkit SUMMARY  Many of the estimated four million adults in the U.S. with severe speech and physical impairments (SSPI) resulting from neurodevelopmental or neurodegenerative diseases cannot rely on current assistive technologies (AT) for communication. During a single day, or as their disease progresses, they may transition from one access technology to another due to fatigue, medications, changing physical status, or progressive motor dysfunction. There are currently no clinical or AT solutions that adapt to the multiple, dynamic access needs of these individuals, leaving many people poorly served. This competitive renewal, called BCI-FIT (Brain Computer Interface-Functional Implementation Toolkit) adds to our innovative multidisciplinary translational research conducted over the past 11 years for the advancement of science related to non-invasive BCIs for communication for these clinical populations. BCI-FIT relies on active inference and transfer learning to customize a completely adaptive intent estimation classifier to each user's multiple modality signals in real-time. The BCI-FIT acronym has many implications: our BCI fits to each user's brain signals; to the environment, offering relevant personal language; to the user's internal states, adjusting signals based on drowsiness, medications, physical and cognitive abilities; and to users' learning patterns from BCI introduction to expert use.  Three specific aims are proposed: (1) Develop and evaluate methods for optimizing system and user performance with on-line, robust adaptation of multi-modal signal models. (2) Develop and evaluate methods for efficient user intent inference through active querying. (3) Integrate language interaction and letter/word supplementation as input modalities in real-time BCI use. Four single case experimental research designs will evaluate both user performance and technology performance for functional communication with 35 participants with SSPI in the community, and 30 healthy controls for preliminary testing. The same dependent variables will be tested in all experiments: typing accuracy (correct character selections divided by total character selections), information transfer rate (ITR), typing speed (correct characters/minute), and user experience (UX) questionnaire responses about comfort, workload, and satisfaction. Our goal is to establish individualized recommendations for each user based on a combination of clinical and machine expertise. The clinical expertise plus user feedback added to active sensor fusion and reinforcement learning for intent inference will produce optimized multi-modal BCIs for each end-user that can adjust to short- and long-term fluctuating function. Our research is conducted by four sub-teams who have collaborated successfully to implement translational science: Electrical/computer engineering; Neurophysiology and systems science; Natural language processing; and Clinical rehabilitation. The project is grounded in solid machine learning approaches with models of participatory action research and AAC participation. This project will improve technologies and BCI technical capabilities, demonstrate BCI implementation paradigms and clinical guidelines for people with severe disabilities. PROJECT NARRATIVE The populations of US citizens with severe speech and physical impairments secondary to neurodevelopmental and neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means that adapt to their best access methods in communication technologies, as proposed in BCI-FIT. This project implements the translation of basic computer science and engineering into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Optimizing BCI-FIT: Brain Computer Interface - Functional Implementation Toolkit,10044301,R01DC009834,"['Adult', 'Attention', 'Behavioral', 'Brain', 'Calibration', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Clinical assessments', 'Cognition', 'Cognitive', 'Communication', 'Communities', 'Computers', 'Custom', 'Data', 'Decision Making', 'Disease', 'Drowsiness', 'Electroencephalography', 'Engineering', 'Environment', 'Eye Movements', 'Fatigue', 'Feedback', 'Goals', 'Guidelines', 'Head Movements', 'Impairment', 'Individual', 'Informed Consent', 'Knowledge', 'Language', 'Learning', 'Letters', 'Life', 'Locked-In Syndrome', 'Machine Learning', 'Measures', 'Medical', 'Medical Technology', 'Methods', 'Modality', 'Modeling', 'Motor Skills', 'Movement', 'Muscle', 'Natural Language Processing', 'Neurodegenerative Disorders', 'Participant', 'Partner Communications', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Policies', 'Population', 'Protocols documentation', 'Psychological Transfer', 'Psychological reinforcement', 'Public Health', 'Questionnaires', 'Recommendation', 'Rehabilitation therapy', 'Research', 'Research Design', 'Role', 'Science', 'Secondary to', 'Self-Help Devices', 'Sensory', 'Signal Transduction', 'Solid', 'Source', 'Speech', 'Speed', 'Supplementation', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Vocabulary', 'Workload', 'acronyms', 'alternative communication', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'clinical implementation', 'cognitive ability', 'community based participatory research', 'computer science', 'disability', 'experience', 'experimental study', 'improved', 'innovation', 'learning strategy', 'motor disorder', 'multidisciplinary', 'multimodality', 'neurophysiology', 'phrases', 'residence', 'response', 'satisfaction', 'sensor', 'signal processing', 'simulation', 'spelling', 'theories', 'visual tracking']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2020,929399,-0.013232914101689253
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Abstract COVID-19 has made traveling as a blind or visually impaired person much riskier and more difficult than before the pandemic. As a result, people with visual impairments may limit their essential travel such as trips to the doctor’s office, the pharmacy and grocery shopping and walks for exercise or leisure. Accordingly, the goal of this COVID Supplement, which builds on and expands the work being conducted by the parent grant, is to develop a COVID map tool that provides fully accessible, non-visual access to maps. This tool will allow visually impaired persons to explore maps and preview routes from the comfort of their home, allowing them to plan their travel along safer, less congested routes using crowdedness data. In addition, the tool will present county-by-county COVID incidence data in a fully accessible form, which will inform their travel plans over greater distances. Thus, this project will give visually impaired persons the tools and confidence to undertake safer, more independent travel. Health Relevance The COVID-19 pandemic has an especially severe impact on people with significant vision impairments or blindness. The need for social distancing and reduced touching of one’s surroundings has made traveling as a blind or visually impaired person much riskier and more difficult than before the pandemic. As a result, people with visual impairments may limit their essential travel such as trips to the doctor’s office, the pharmacy and grocery shopping and walks for exercise or leisure. These travel limitations may have adverse impacts on their physical and mental health. The proposed research would result in a new software tool that could greatly increase the confidence of the approximately 10 million Americans with significant vision impairments or blindness to undertake safe, independent travel.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,10220178,R01EY029033,"['American', 'Blindness', 'COVID-19', 'COVID-19 pandemic', 'Cellular Phone', 'Color', 'Communities', 'Computer Vision Systems', 'Computers', 'County', 'Crowding', 'Data', 'Destinations', 'Development', 'Ensure', 'Evaluation', 'Exercise', 'Goals', 'Health', 'Home environment', 'Incidence', 'Internet', 'Knowledge', 'Leisures', 'Maps', 'Mental Health', 'Pharmacy facility', 'Process', 'Publications', 'Research', 'Route', 'Running', 'Social Distance', 'Software Tools', 'System', 'Tablets', 'Tactile', 'Target Populations', 'Touch sensation', 'Travel', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'blind', 'braille', 'coronavirus disease', 'design', 'outreach', 'pandemic disease', 'parent grant', 'physical conditioning', 'software development', 'symposium', 'tool', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2020,406525,0.03884149135698663
"Validation of a lab-free low-cost screening test for prevention of cervical cancer: automated visual evaluation PROJECT SUMMARY/ABSTRACT Artificial intelligence (AI) has the potential to revolutionize medicine by improving productivity, reducing human error, and assisting with diagnosis and treatment. Image classification algorithms can be used to develop automated visual evaluation (AVE): a potential game-changer for cervical cancer prevention in low- and middle-income countries (LMICs). AVE technology reads digital photographs of a cervix to provide diagnosis and treatment recommendations in seconds. AVE is a true point of care test, low cost and does not require a laboratory. AVE could be used either for stand-alone primary screening, or to triage HPV-positive women. We will compare AVE to common screening methods in LMICs: visual inspection with acetic acid (VIA) and conventional cytology. Enhanced Visual Assessment (EVA) System by MobileODT is a cloud-connected mobile colposcope on a smartphone platform. It is FDA cleared and used in 42 countries. MobileODT is uniquely poised to integrate AVE into the EVA System. Our aim is to validate and commercialize AVE on the EVA platform. Phase I aims will adapt AVE to run on the EVA system using an optimal neural network architecture, running either directly on the phone or as a cloud- based service. Phase II is a prospective clinical trial of 10,000 patients recruited at ministry of health sites in El Salvador. All screen-positive patients, and 10% of negative patients, will undergo colposcopy with biopsy. Sensitivity of AVE as a primary screening test will be compared to cytology and to VIA. In HPV-positive women, AVE will be compared to VIA as a triage test. PROJECT NARRATIVE The proposal involves developing and testing a cervical cancer screening test: automated visual evaluation (AVE) based on an image classification algorithm that runs on smartphone-based colposcope. Included are both technical development to integrate AVE to a mobile phone application (Phase I), and a prospective validation on a screening population of 10,000 women in El Salvador (Phase II). AVE will be compared to standard tests (conventional cytology and visual inspection with acetic acid: VIA) for primary screening, and against VIA triage in an HPV+ population.",Validation of a lab-free low-cost screening test for prevention of cervical cancer: automated visual evaluation,10008280,R44CA247137,"['Acetic Acids', 'Address', 'Algorithm Design', 'Algorithms', 'Architecture', 'Area', 'Artificial Intelligence', 'Bedside Testings', 'Biopsy', 'Car Phone', 'Cellular Phone', 'Cervical', 'Cervical Cancer Screening', 'Cervical Intraepithelial Neoplasia', 'Cervix Uteri', 'Clinical', 'Clinical Trials', 'Colposcopes', 'Colposcopy', 'Country', 'Cytology', 'Data', 'Decision Making', 'Detection', 'Development', 'Diagnosis', 'Documentation', 'El Salvador', 'Guidelines', 'HPV-High Risk', 'Health', 'Histology', 'Histopathology', 'Human Papillomavirus', 'Image', 'Internet', 'Laboratories', 'Medicine', 'Methods', 'Oncogenic', 'Patient Recruitments', 'Patients', 'Pattern', 'Performance', 'Phase', 'Population', 'Predictive Value', 'Prevention', 'Productivity', 'Provider', 'ROC Curve', 'Receiver Operating Characteristics', 'Recommendation', 'Resources', 'Running', 'Sampling', 'Screening procedure', 'Services', 'Site', 'Speed', 'System', 'Technology', 'Telephone', 'Testing', 'Triage', 'Validation', 'Visual', 'Woman', 'World Health Organization', 'automated visual evaluation', 'base', 'cervical cancer prevention', 'classification algorithm', 'cloud based', 'cost', 'data quality', 'deep learning algorithm', 'digital', 'human error', 'improved', 'innovation', 'low and middle-income countries', 'mobile application', 'neural network', 'neural network architecture', 'overtreatment', 'phase II trial', 'primary endpoint', 'product development', 'programs', 'prospective', 'quality assurance', 'screening', 'screening program', 'secondary analysis', 'secondary endpoint', 'tool']",NCI,"MOBILEODT, INC.",R44,2020,297844,-0.012460510089150851
"Assessment of murine retinal acuity ex vivo by machine learning of multielectrode array recordings Project Summary: Darwin Babino, PhD, a trained pharmacologist/electrophysiologist, has spent the last ten years working on several disciplines in the vision sciences. His proposal entitled “Assessment of murine retinal acuity ex vivo by machine learning of multielectrode array recordings” presents his overarching goal to improve vision restoration approaches by developing methods to test the potential of these techniques thereby accelerating the development of effective interventions. Dr. Babino and his primary mentor, Dr. Russell Van Gelder, have assembled a strong team of co-mentors at the University of Washington SOM and collaborators to guide him through the proposed training and research. His previous training will be supplemented with goals to help his development as an independent investigator: 1) Study design and practical learning in performing panretinal (MEA) biological experiments; 2) Fundamental and advanced techniques of the proposed optogenetic and stem-cell restoration techniques; 3) Application of advanced machine learning techniques; 4) Develop leadership and professional skills to establish an independent group. The ability to assess the function of panretinal circuitry will foster our understanding of the advantages and weaknesses of different restoration techniques (Aim 1). The work proposed here will improve an existing retinal acuity assessment tool which combines machine learning techniques on novel, high-density multielectrode array recordings of ganglion cell responses in several mouse models. The utility of this system will be demonstrated in assessing visual potential of the mouse retina in three different approaches to vision restoration that are challenging for in vivo assessment (Aim 2). In collaboration with Dr. Deepak A. Lamba at UCSF, we will apply our system to animals which have undergone stem-cell replacement of retinal cells including photoreceptor cells. An optogenetics approach will also be evaluated in collaboration with Dr. John Flannery at UC Berkeley whose group has developed vectors for expressing rhodopsin and cone opsins in ganglion and bipolar cells. Finally, differences between native and restored vison with small molecule photoswitches, light-activated inhibitors of voltage-gated potassium channels, which confer light-dependent firing on treated cells, will be assessed. The resulting advanced electrophysiology application will help elucidate fundamental questions about the functional retina, mechanisms that lead to retinal degeneration and the potential of several therapeutics for the treatment of retinal diseases. Furthermore, this career development award will facilitate Dr. Babino’s development into an independent investigator by priming an R01 grant application. Project Narrative: Project Narrative: The prevalence of vision loss from retinal degeneration numbers in the millions world-wide and is expected to double by the year 2050, and despite the development of several promising approaches to restore vision in the blind, progress in developing these therapies has been hampered by challenges in analysis of these methods in animal models. We describe a novel system that analyzes, by machine learning, retinal ganglion cell output in native, degenerated and therapeutically treated blind retinas which can characterize the visual information content of the ‘reanimated’ blind retina and thereby facilitate the development of these technologies. The system developed through this grant, as well as the career development pursued by the investigator, will be readily applicable to the assessment of potential retinal acuity restoration by current and novel therapeutic approaches.",Assessment of murine retinal acuity ex vivo by machine learning of multielectrode array recordings,9943144,K99EY031333,"['Aftercare', 'Amacrine Cells', 'Animal Model', 'Animals', 'Applications Grants', 'Assessment tool', 'Behavioral Assay', 'Biological', 'Blindness', 'Cells', 'Collaborations', 'Cone', 'Contrast Sensitivity', 'Data', 'Development', 'Discipline', 'Dissection', 'Doctor of Philosophy', 'Ectopic Expression', 'Electrophysiology (science)', 'Electroretinography', 'Evolution', 'Feedback', 'Fostering', 'Ganglia', 'Genetic', 'Goals', 'Grant', 'Human', 'Image', 'In Vitro', 'Individual', 'Intervention', 'K-Series Research Career Programs', 'Knockout Mice', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Light', 'MW opsin', 'Machine Learning', 'Measurable', 'Measurement', 'Measures', 'Mediating', 'Mentors', 'Methods', 'Movement', 'Mus', 'Opsin', 'Output', 'Photoreceptors', 'Prevalence', 'Protocols documentation', 'Psychophysics', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Retina', 'Retinal Cone', 'Retinal Degeneration', 'Retinal Diseases', 'Retinal Ganglion Cells', 'Retinal gene therapy', 'Rhodopsin', 'Rod', 'Rodent', 'Saccades', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Specificity', 'Stimulus', 'Synapsins', 'System', 'Systems Analysis', 'Systems Development', 'Techniques', 'Testing', 'Therapeutic', 'Training', 'Transgenic Organisms', 'Universities', 'Vertebrate Photoreceptors', 'Viral', 'Vision', 'Visual', 'Visual Acuity', 'Visual system structure', 'Voltage-Gated Potassium Channel', 'Washington', 'Wild Type Mouse', 'Work', 'base', 'behavior test', 'blind', 'career development', 'cell type', 'cost effective', 'density', 'effective intervention', 'experimental study', 'ganglion cell', 'improved', 'in vivo', 'induced pluripotent stem cell', 'inhibitor/antagonist', 'interest', 'light intensity', 'mimicry', 'mouse model', 'multi-electrode arrays', 'mutant', 'nonhuman primate', 'novel', 'novel therapeutic intervention', 'optogenetics', 'promoter', 'rapid eye movement', 'response', 'restoration', 'scale up', 'skills', 'small molecule', 'stem cells', 'technology development', 'therapy development', 'tool', 'vector', 'vision science', 'visual information']",NEI,UNIVERSITY OF WASHINGTON,K99,2020,113080,-0.025828738166589732
"User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control ABSTRACT Approximately 41,000 individuals live with upper-limb loss (loss of at least one hand) in the US. Fortunately, prosthetic devices have advanced considerably in the past decades with the development of dexterous, anthropomorphic hands. However, potentially the most promising used control strategy, myoelectric control, lacks a correspondingly high-level of performance and hence the use of dexterous hands remains highly limited. The need for a complete overhaul in upper limb prosthesis control is well highlighted by the abandonment rates of myoelectric devices, which can reach up to 40% in the case of trans-humeral amputees. The area of research that has received the most focus over the past decade has been “pattern recognition,” which is a signal processing based control method that uses multi-channel surface electromyography as the control input. While pattern recognition provides intuitive operation of multiple prosthetic degrees of freedom, it lacks robustness and requires frequent, often daily calibration. Thus, it has not yet achieved the desired clinical acceptance. Our team proposes clinical translation of a novel highly adaptive upper limb prosthesis control system that incorporates two major advances: 1) machine learning (robust classification by implementing a non-boundary based algorithm), and 2) training by retrospectively incorporating user data from activities of daily living (ADL). The proposed system will enable machine intelligence with user input for prosthesis control. Our work is organized as follows: Phase I: (a) First, we will implement a fundamentally new machine intelligence technique, Extreme Learning Machine with Adaptive Sparse Representation Classification (EASRC), that is more resilient to untrained noisy conditions that users may encounter in the real-world and requires less data than traditional myoelectric signal processing. (b) In parallel, we will implement an adaptive learning algorithm, Nessa, which allows users to relabel misclassified data recorded during use and then update the EASRC classifier to adapt to any major extrinsic or intrinsic changes in the signals. Taken together, EASRC and Nessa comprise the Retrospectively Supervised Classification Updating (RESCU) system. Once, the RESCU implementation is complete, we will optimize the system through a joint effort with Johns Hopkins University, and complete an iterative benchtop RESCU evaluation with a focus group of 3 amputee subjects and their prosthetists. Phase II: Verification and validation of RESCU will be completed, culminating in third-party validation testing and certification. Finally, we will complete a clinical assessment including self-reporting subjective measures, and real-world usage metrics in a long-term clinical study. PROJECT NARRATIVE In this project, we aim to empower the user by bringing them into the control loop of their prosthesis and improve the stability of their control strategy over time. Specifically, we implement to a robust classifier, an adaptive learning algorithm, and a smartwatch interface, which allows the user to teach their device when it misunderstands the commands that the user is sending to control the prosthesis. This will result in improved control without cumbersome or time-consuming effort on the part of the user and, more importantly, we hope that it will give the user a greater sense of empowerment and ownership over their prosthesis.",User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control,10078697,U44NS108894,"['Activities of Daily Living', 'Adoption', 'Algorithms', 'Amputees', 'Area', 'Artificial Intelligence', 'Award', 'Calibration', 'Certification', 'Classification', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Communication', 'Consumption', 'Data', 'Development', 'Devices', 'Electromyography', 'Evaluation', 'Focus Groups', 'Freedom', 'Goals', 'Hand', 'Individual', 'Intuition', 'Joints', 'Label', 'Limb Prosthesis', 'Machine Learning', 'Measures', 'Methods', 'Outcome', 'Ownership', 'Parents', 'Patient Self-Report', 'Pattern Recognition', 'Performance', 'Phase', 'Prosthesis', 'Research', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Supervision', 'Surface', 'Surveys', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Update', 'Upper Extremity', 'Validation', 'Work', 'adaptive learning', 'base', 'clinical translation', 'empowerment', 'functional improvement', 'improved', 'innovation', 'intelligent algorithm', 'learning algorithm', 'myoelectric control', 'novel', 'operation', 'programs', 'prospective', 'prosthesis control', 'satisfaction', 'signal processing', 'smart watch', 'verification and validation']",NINDS,"INFINITE BIOMEDICAL TECHNOLOGIES, LLC",U44,2020,64079,0.0008360341991336726
"Dense life-log health analytics from wearable senors using functional analysis and Riemannian geometry The growth and acceptance of wearable devices (e.g., accelerometers) and personal technologies (e.g., smartphones), coupled with larger storage capacities, waterproofing, and more unobtrusive wear locations, has made long-term monitoring of behaviors throughout the 24-hour spectrum more feasible. Wearable devices relevant for human activity (e.g., GENEActiv accelerometer) contain several complementary sensors (accelerometers, gyro, heart- rate monitor etc.) and sample at high rates (e.g., 100Hz for accelerometer). These high-sampling rates and the long duration of capture result in life-log data that truly qualifies as multimodal and big time-series data. The challenges and opportunities involved in fully harvesting these types of data, for widely applicable interventions, suggest that an interdisciplinary approach spanning mathematical sciences, signal processing, and health is needed. Our innovation includes the use of functional-data analysis tools to represent and process the dense time-series data. Functional data analysis is then integrated into machine learning and pattern discovery algorithms for activity classification, prediction of attributes, and discovery of new activity classes. We anticipate that the proposed framework will lead to new insights about human activity and its impact on health outcomes. This interdisciplinary project builds on several research activities of the team. Our past work includes: a) new mathematical developments for computing statistics on time-series data viewed as elements of a function-spaces, b) algorithms for activity recognition that integrate the function-space techniques, and c) data from long-term observational studies of human activity from multimodal sensors. The new work we propose addresses the unique mathematical and computational challenges posed by densely multimodal, long-term, densely-sampled Iifelog big-data in a comprehensive framework. The fusion of ideas from human activity modeling, functional-analysis, geometric metrics, and algorithmic machine learning, present unique opportunities for fundamental advancement of the state-of-the-art in objective measurement and quantification of behavioral markers from wearable devices. The proposed approach also brings to fore: a) new mathematical developments of elastic metrics over multi-modal time-series data, b) comparing sequences evolving on different feature manifolds, c) estimation of quasi- periodicities, d) and a new generation of machine-learning and pattern discovery algorithms. The mathematical and algorithmic tools proposed have the potential to significantly advance how wearable data from contemporary devices with high-sampling rates and large storage capabilities are represented, processed, and transformed into accurate inferences about human activity. Wearable devices are becoming more widely adopted in recent years for general health and recreational uses by the broad populace. This research will result in improved algorithms to process the data available from such wearable devices. The long-term goal of the research is to enable personalized home-based physical activity regimens for conditions such as stroke and diabetes. n/a",Dense life-log health analytics from wearable senors using functional analysis and Riemannian geometry,10023190,R01GM135927,"['Accelerometer', 'Address', 'Adopted', 'Algorithmic Software', 'Algorithms', 'Behavior monitoring', 'Behavioral', 'Big Data', 'Cellular Phone', 'Classification', 'Coupled', 'Data', 'Data Analyses', 'Development', 'Devices', 'Diabetes Mellitus', 'Elements', 'Generations', 'Geometry', 'Goals', 'Growth', 'Harvest', 'Health', 'Home environment', 'Hour', 'Human Activities', 'Intervention', 'Life', 'Location', 'Machine Learning', 'Mathematics', 'Measurement', 'Modeling', 'Observational Study', 'Outcome', 'Pattern', 'Periodicity', 'Physical activity', 'Process', 'Regimen', 'Research', 'Research Activity', 'Sampling', 'Series', 'Stroke', 'Techniques', 'Technology', 'Time', 'Work', 'base', 'heart rate monitor', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'machine learning algorithm', 'mathematical algorithm', 'mathematical sciences', 'multimodality', 'sensor', 'signal processing', 'statistics', 'tool', 'wearable device']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2020,311680,-0.04022099869133258
"Improved AD/ADRD Assessment Sensitivities Using a Novel In-Situ Sensor System Project Summary/Abstract  Accurate assessment of daily functions for individuals at risk for and with AD/ADRD, is fundamental to detection, diagnosis, and characterization of its progression and prescribed treatments. Current assessment techniques typically rely on non- continuous, discreet observations provided from a third party and covering single or limited performance domains. With significantly larger portions of American’s choosing to age in place, any assessment technology must be able to be in-situ (low-cost, ubiquitous) and operate without user interface (autonomous) to provide objective, cross-domain, and continuous daily function measurements and reporting.  The primary objective of this fast track SBIR project is to demonstrate the feasibility and effectiveness of using the Birkeland Current Sovrin IoT system to continuously and accurately assess daily functions, ADLs, and IADLs, for persons experiencing cognitive decline in a home or assisted care settings. This includes direct comparison with an accepted assessment technique, ADCS-ADL/23. Machine learning and artificial intelligent techniques will be employed to identify novel subfactors for improved sensitivities from available sensor data combinations. Secondary objectives include establishing a significant data set of detailed daily actions (<10 sec resolution) for 100+ individuals with AD/ADRD. Long-term goals support future intervention studies through improved assessment tools with enhanced sensitivity to early and mid-stage decline.  The Birkeland Current Sovrin IoT system makes use of patented proximity-based energy monitoring and control sensors, data analytics and change detection algorithms to continuously monitor activities of individuals in a home or assisted care environment. Intelligent power-strips and battery-based sensors located throughout the home or facility, monitor real time absolute location of individuals, caregivers, and devices they interact with. Correlation of high-fidelity data allows accurate determination of activities, attribution to a specific individual, mobility measurement, and behavior assessment across traditional and novel ADL/IADL categories. Birkeland Current is teamed with Texas A&M Center for Population Health and Aging, Georgia, Tech Institute for People and Technology, Baylor Scott and White Division of Gerontology, and multiple home-care and assisted-care facilities, in the development of the study approach, implementation plan, analytics tools, and applications to aging populations and future intervention studies. Project Narrative  The proposed research would utilize novel, ubiquitous Internet-of-Things sensors and automated analytics to demonstrate enhanced sensitivity and future utility of continuous in-situ IADL/ADL data for dementia research and its effectiveness in characterizing interventions for Alzheimer’s and related dementias of aging populations in support of NIA stated priorities.",Improved AD/ADRD Assessment Sensitivities Using a Novel In-Situ Sensor System,10131376,R44AG065118,"['Address', 'Adoption', 'Aging', 'Algorithms', 'Alzheimer&apos', 's disease related dementia', 'American', 'Artificial Intelligence', 'Assessment tool', 'Behavior assessment', 'Behavioral Symptoms', 'Caregivers', 'Caring', 'Categories', 'Centers for Population Health', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Dementia', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Documentation', 'Early Diagnosis', 'Early identification', 'Effectiveness', 'Environment', 'Future', 'Gerontology', 'Goals', 'Grouping', 'Health care facility', 'Home environment', 'Impaired cognition', 'In Situ', 'Individual', 'Industry', 'Institutes', 'Intelligence', 'Internet of Things', 'Intervention', 'Intervention Studies', 'Legal patent', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Monitor', 'Outcome', 'Participant', 'Patients', 'Performance', 'Persons', 'Phase', 'Population', 'Problem Solving', 'Protocols documentation', 'Publishing', 'Recommendation', 'Reporting', 'Research', 'Resolution', 'Resources', 'Risk', 'Series', 'Small Business Innovation Research Grant', 'System', 'Techniques', 'Technology', 'Technology Assessment', 'Testing', 'Texas', 'Time', 'Training', 'United States National Institutes of Health', 'Use Effectiveness', 'aging in place', 'aging population', 'analytical tool', 'base', 'cost', 'daily functioning', 'data acquisition', 'data integration', 'database structure', 'design', 'experience', 'improved', 'insight', 'instrumental activity of daily living', 'learning algorithm', 'novel', 'patient home care', 'personalized care', 'real time monitoring', 'sensor', 'symposium', 'tool']",NIA,BIRKELAND CURRENT LLC,R44,2020,1992588,-0.015483852213078375
"Dense Life-log Health Analytics from Wearable Sensors using Functional Analysis and Riemannian Geometry Project Summary The growth and acceptance of wearable devices (e.g., accelerometers) and personal technologies (e.g., smartphones), coupled with larger storage capacities, waterproofing, and more unobtrusive wear locations, has made long-term monitoring of behaviors throughout the 24-hour spectrum more feasible. Wearable devices relevant for human activity (e.g., GENEActiv accelerometer) contain several complementary sensors (accelerometers, gyro, heart- rate monitor etc.) and sample at high rates (e.g., 100Hz for accelerometer). These high-sampling rates and the long duration of capture result in life-log data that truly qualifies as multimodal and big time-series data. The challenges and opportunities involved in fully harvesting these types of data, for widely applicable interventions, suggest that an interdisciplinary approach spanning mathematical sciences, signal processing, and health is needed. Our innovation includes the use of functional-data analysis tools to represent and process the dense time-series data. Functional data analysis is then integrated into machine learning and pattern discovery algorithms for activity classification, prediction of attributes, and discovery of new activity classes. We anticipate that the proposed framework will lead to new insights about human activity and its impact on health outcomes. This interdisciplinary project builds on several research activities of the team. Our past work includes: a) new mathematical developments for computing statistics on time-series data viewed as elements of a function-spaces, b) algorithms for activity recognition that integrate the function-space techniques, and c) data from long-term observational studies of human activity from multimodal sensors. The new work we propose addresses the unique mathematical and computational challenges posed by densely multimodal, long-term, densely-sampled lifelog big-data in a comprehensive framework. The fusion of ideas from human activity modeling, functional-analysis, geometric metrics, and algorithmic machine learning, present unique opportunities for fundamental advancement of the state-of-the-art in objective measurement and quantification of behavioral markers from wearable devices. The proposed approach also brings to fore: a) new mathematical developments of elastic metrics over multi-modal time-series data, b) comparing sequences evolving on different feature manifolds, c) estimation of quasi- periodicities, d) and a new generation of machine-learning and pattern discovery algorithms. The mathematical and algorithmic tools proposed have the potential to significantly advance how wearable data from contemporary devices with high-sampling rates and large storage capabilities are represented, processed, and transformed into accurate inferences about human activity. Wearable devices are becoming more widely adopted in recent years for general health and recreational uses by the broad populace. This research will result in improved algorithms to process the data available from such wearable devices. The long-term goal of the research is to enable personalized home-based physical activity regimens for conditions such as stroke and diabetes. Project Number: 1R01GM135927-01 Title: Dense Life-log Health Analytics from Wearable Sensors using Functional Analysis and Riemannian Geometry Project Narrative In this revision application, we seek to submit an equipment supplement to our existing R01 referenced above. As our project progressed, we found that it is important to consider the role of new emerging feature-learning approaches to extract downstream time-series features. To fully develop our approach and conduct additional experiments, we need significant GPU computational resources that will be dedicated to this project.",Dense Life-log Health Analytics from Wearable Sensors using Functional Analysis and Riemannian Geometry,10135658,R01GM135927,"['Accelerometer', 'Address', 'Adopted', 'Algorithmic Analysis', 'Algorithmic Software', 'Algorithms', 'Awareness', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Big Data', 'Cellular Phone', 'Classification', 'Coupled', 'Data', 'Data Analyses', 'Development', 'Devices', 'Diabetes Mellitus', 'Dimensions', 'Elements', 'Equipment', 'Generations', 'Geometry', 'Goals', 'Growth', 'Harvest', 'Health', 'Home environment', 'Hour', 'Human Activities', 'Intervention', 'Learning', 'Life', 'Location', 'Machine Learning', 'Mathematics', 'Measurement', 'Methodology', 'Methods', 'Modeling', 'Observational Study', 'Outcome', 'Pattern', 'Periodicity', 'Physical activity', 'Process', 'Regimen', 'Research', 'Research Activity', 'Role', 'Running', 'Sampling', 'Series', 'Statistical Methods', 'Stroke', 'Supervision', 'Techniques', 'Technology', 'Time', 'Time Series Analysis', 'Validation', 'Walking', 'Work', 'analysis pipeline', 'base', 'computing resources', 'density', 'experimental study', 'heart rate monitor', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'machine learning algorithm', 'mathematical algorithm', 'mathematical sciences', 'multimodality', 'preservation', 'sedentary lifestyle', 'sensor', 'signal processing', 'statistics', 'tool', 'wearable device', 'wearable sensor technology']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2020,69169,-0.03473760384994483
"SCH: INT: Conversations for Vision: Human-Computer Synergies in Prosthetic Interactions  The project will investigate prosthetic support for people with visual impairment (PVI) that integrates computer vision-based prosthetics with video-mediated human-in-the-loop prosthetics. Computer vision- based (CV) prosthetics construe the fundamental technical challenge for visual prosthetics as one of parsing and identifying objects across scales, distances, and orientations. Visual prosthetic applications have been central drivers in the development of computer vision technology through the past 50 years. Video-mediated remote sighted assistance (RSA) prosthetics are more recent, enabled by different technologies, and construe the orienting technical challenge for visual prosthetics as one of effective helping interactions. RSA services are commercially available now, and have evoked much excitement in the PVI community. The two approaches, CV and RSA, will be successively integrated through a series of increasingly refined Wizard of Oz simulations, and investigate possible synergies between the two approaches. We will employ a human-centered design approach, identifying a set of key assistive interaction scenarios that represent authentic needs and concerns of PVIs, by leveraging our 6-year relationship working directly with our local chapter of the National Federation of the Blind. RELEVANCE (See Instructions): 23.7 million American adults have vision loss; 1.3 million people in US are legally blind. This project addresses a transformational opportunity to enhance human performance and experience, to diversify workplace participation, and to enhance economic and social well-being. n/a",SCH: INT: Conversations for Vision: Human-Computer Synergies in Prosthetic Interactions ,10020434,R01LM013330,"['Address', 'Adult', 'American', 'Articulation', 'Back', 'Blindness', 'Communities', 'Computer Vision Systems', 'Computers', 'Data Set', 'Development', 'Economics', 'Emotional', 'Female', 'Goals', 'Human', 'Information Sciences', 'Instruction', 'Mediating', 'Modeling', 'Ocular Prosthesis', 'Performance', 'Prosthesis', 'Route', 'Self-Help Devices', 'Series', 'Services', 'Social Well-Being', 'Technology', 'Time', 'Underrepresented Students', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'design', 'experience', 'graduate student', 'human-in-the-loop', 'learning materials', 'legally blind', 'outreach', 'prototype', 'simulation', 'synergism', 'undergraduate student']",NLM,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R01,2020,229482,0.03222135392768901
"Healthcare Impact of Consumer-Driven Atrial Fibrillation Detection PROJECT SUMMARY/ABSTRACT  Companies are increasingly marketing mobile technologies as FDA-cleared medical devices, yet we do not know the consequences of these devices on healthcare utilization, cost, and outcomes. Recently, Apple released the Apple Watch Series 4 as an FDA-cleared medical device. The device includes an alert for the presence of atrial fibrillation (AF) and allows anyone to monitor their heart rhythm for the presence of AF. Apple has an enormous global audience, and the number of people who will use this (and other similar devices) to self-diagnose or monitor AF will be substantial. On one hand, the device may allow new diagnoses that result in treatment, improved quality of life, fewer AF related complications. On the other hand, the device may result in false positives in otherwise healthy people, resulting in more testing and treatments with associated harms. In fact, the U.S. Preventative Task Force recommends against routine surveillance for AF in the general population, citing lack of evidence and possible harm. We have an urgent need for a population-based infrastructure to ensure that technologies entering the market as medical devices are beneficial and safe.  The overall goal of this project is to measure the uptake and effect of the Apple Watch 4 release on healthcare utilization among first-time and known AF patients. Dr. Shah is an early stage investigator with a K08 Career Development Award from the NHLBI. As part of the K08, she has developed a detailed cohort of contemporary AF patients, including clinical notes. Along with a team, she will use real world data, as proposed by the FDA, to generate evidence about risks and benefits of consumer-driven AF detection. She will use natural language processing to leverage the notes and identify AF patients who seek care due to the medical device, and evaluate downstream healthcare utilization, such as additional clinic visits, cardioversions, additional remote monitoring, and cost. The goals of this project will be accomplished through the following Specific Aims: 1) Estimate the proportion of first-time AF patient visits attributable to a mobile device before and after FDA clearance of the Apple Watch 4, and characterize device accuracy and downstream healthcare utilization in this population; and 2) Evaluate healthcare utilization patterns among prevalent AF patients who use mobile devices with AF alerts.  In 2017, Apple sold 17.7 million smart watches, in a device market that continues to grow. Extrapolating from prior annual sales and conservatively assuming a 5% increase in users each year, almost 60 million people will have an Apple Watch by the end of 2020 (not accounting for non-Apple devices with similar functionality). Thus, even in this short period of time, uptake will be substantial and warrant immediate feedback. The results of this project will provide preliminary data for a long-term, multicenter study that evaluates the benefits (improved quality of life, fewer strokes) and harms (increased treatment complications, increased cost) of consumer-driven AF detection. PROJECT NARRATIVE The consequence of mobile technologies marketed as medical devices are unknown, including devices that provide alerts for the presence of atrial fibrillation. The goal of this project is to evaluate the benefits and harm associated with consumer-driven atrial fibrillation detection.",Healthcare Impact of Consumer-Driven Atrial Fibrillation Detection,9980996,R03HL148372,"['Adult', 'Advisory Committees', 'Affect', 'Apple', 'Apple watch', 'Arrhythmia', 'Atrial Fibrillation', 'Benefits and Risks', 'Cardiovascular system', 'Caring', 'Case-Control Studies', 'Clinic Visits', 'Clinical', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Electric Countershock', 'Ensure', 'Feedback', 'General Population', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Hemorrhage', 'Holter Electrocardiography', 'Infrastructure', 'Interruption', 'Intervention', 'K-Series Research Career Programs', 'Lead', 'Marketing', 'Measures', 'Medical Device', 'Monitor', 'Multicenter Studies', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Outcome', 'Patients', 'Pattern', 'Population', 'Prevalence', 'Preventive', 'Quality of life', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Sales', 'Series', 'Sinus', 'Stroke', 'Technology', 'Testing', 'Text', 'Time', 'United States Food and Drug Administration', 'Universities', 'Utah', 'Visit', 'base', 'care seeking', 'cohort', 'cost', 'cost outcomes', 'cryptogenic stroke', 'design', 'follow-up', 'handheld mobile device', 'health care service utilization', 'heart rhythm', 'improved', 'mobile computing', 'population based', 'routine screening', 'self diagnosis', 'smart watch', 'uptake']",NHLBI,UNIVERSITY OF UTAH,R03,2020,76250,0.02309047624067382
"DIGITAL HEALTH SOLUTIONS FOR COVID-19: COVID-19 ONGOING MONITORING (COMMUNITY) The goal of this proposal is to develop a COVID-19 detection algorithm based on self-report survey data and wearable sensor data. Data from 25K COVID-19 Experiences participants and 25K Large-scale Flu Surveillance (COVID-19 Questions added March 2020) will be used with an existing machine learning model to develop this new detection algorithm, which will be validated in a large-scale pilot population to identify individuals with undiagnosed COVID-19. Evidation will incorporate the model into an established web and multi-platform (Android, iOS) smartphone platform called Achieve which allows users to share person-generated health data (PGD) from their everyday lives. Data collected under this project will be deidentified and securely transmitted to an NIH data hub. n/a",DIGITAL HEALTH SOLUTIONS FOR COVID-19: COVID-19 ONGOING MONITORING (COMMUNITY),10274140,5N91020C00034,"['Algorithms', 'Android', 'COVID-19', 'Cellular Phone', 'Communities', 'Data', 'Detection', 'Goals', 'Health', 'Individual', 'Internet', 'Machine Learning', 'Modeling', 'Monitor', 'Participant', 'Patient Self-Report', 'Persons', 'Population', 'Secure', 'Surveys', 'United States National Institutes of Health', 'base', 'data hub', 'digital', 'experience', 'health data', 'influenza surveillance', 'wearable sensor technology']",NCI,"EVIDATION HEALTH, INC.",N01,2020,240000,0.006415311638882265
"Development of a Wheelchair Maintenance Alert Application for Elderly Wheelchair Users Project Summary Elderly wheelchair users experience wheelchair breakdowns every 2-3 months in low- and middle-income countries (LMICs) and rural areas of high-income countries. One in three breakdowns leads to adverse physical, social, psychosocial and economic consequences to wheelchair users which increases the public health and personal burden. Preventative wheelchair maintenance has been found to reduce the frequency of wheelchair breakdowns by ten-fold, but compliance with maintenance recommendations is extremely low because they are generic and not reflective of how and where the wheelchair is being used. To address this issue, we are developing a low-cost, scalable maintenance application that leverages artificial intelligence tools to provide maintenance recommendations tailored to how a wheelchair is used. The availability of low-cost technology and widespread use of smartphones by the elderly and people with disabilities in LMICs has led us to develop a smartphone application called WheelTrak that measures wheelchair wear as a function of usage in community. Based on the wear factors, the application produces a Wheelchair Wear Index (WWI) that is representative of wear of critical wheelchair parts that are prone to breakdown. Once a WWI threshold is reached, maintenance is required, and the application notifies the user and/or caregiver who can conduct maintenance to avoid breakdowns and related health consequences. We will conduct a data collection study in collaboration with our wheelchair industry partner – UCP Wheels in El Salvador – and characterize the WWI for the elderly by tracking wear factors which include user’s travel distance, ground shocks and surface vibrations using WheelTrak and a wheel sensor. Based on the trained WWI algorithm, a preventative maintenance schedule will be developed for older adults that can be employed through WheelTrak for maintenance reminders. Semi-structured interviews will be conducted to evaluate the usability of the application and gather barriers to maintenance. User feedback will assist us in improving WheelTrak for greater user satisfaction and compliance with maintenance, and addressing any personal or logistical challenges that elderly users and their caregivers or family members may face with conducting maintenance activities in LMICs. Findings from the proposed studies in this application will assist us in planning future studies to investigate the WWI-enabled WheelTrak tool as an intervention to prevent or reduce breakdowns and health consequences with the elderly in LMICs. Preventative maintenance of wheelchairs is necessary to reduce frequent wheelchair breakdowns and corresponding health consequences experienced by the elderly in adverse environments which are commonly present in low- and middle-income countries (LMICs). WheelTrak is a smartphone application that measures real-time wheelchair wear in the community during use and triggers preventative maintenance reminders. In this study, we are modelling the application algorithm and collecting user and caregiver feedback to transform WheelTrak into a maintenance intervention tool for elderly wheelchair users in LMICs.",Development of a Wheelchair Maintenance Alert Application for Elderly Wheelchair Users,10095020,R03AG069836,"['Activities of Daily Living', 'Address', 'Adult', 'Algorithms', 'Artificial Intelligence', 'Beds', 'Caregivers', 'Cause of Death', 'Cellular Phone', 'Clinic', 'Collaborations', 'Communities', 'Country', 'Data', 'Data Collection', 'Development', 'Devices', 'Disabled Persons', 'Economics', 'El Salvador', 'Elderly', 'Environment', 'Face', 'Failure', 'Family member', 'Feedback', 'Frequencies', 'Future', 'Goals', 'Health', 'Hospitalization', 'Income', 'Injury', 'Intervention', 'Intervention Studies', 'Interview', 'Life Style', 'Logistics', 'Long-Term Care', 'Maintenance', 'Manual wheelchair', 'Measures', 'Mental Depression', 'Modeling', 'Monitor', 'Names', 'Notification', 'Pattern', 'Personal Satisfaction', 'Population', 'Preventive', 'Provider', 'Public Health', 'Recommendation', 'Risk', 'Schedule', 'Services', 'Shock', 'Societies', 'Structure', 'Study models', 'Surface', 'Technology', 'Time', 'Training', 'Travel', 'User Compliance', 'Wheelchairs', 'aged', 'base', 'cohort', 'cost', 'decubitus ulcer', 'disability', 'evidence base', 'experience', 'improved', 'indexing', 'industry partner', 'low and middle-income countries', 'prevent', 'prospective', 'psychosocial', 'rural area', 'satisfaction', 'sensor', 'sensor technology', 'smartphone Application', 'social', 'tool', 'usability', 'vibration']",NIA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R03,2020,75877,-0.005794296454740726
"Mobilize Center: Models for Mobile Sensing and Precision Rehabilitation Limited mobility due to conditions like osteoarthritis (OA), cerebral palsy, and Parkinson’s disease affects millions of individuals, at enormous personal and societal cost. Rehabilitation can dramatically improve mobility and function, but current rehabilitation practice requires in-person guidance by a skilled clinician, increasing expense and limiting access. Mobile sensing technologies are now ubiquitous and have the potential to measure patient function and guide treatment outside the clinic, but they currently fail to capture the characteristics of motion required to accurately monitor function and customize treatment. Millions of low-cost mobile sensors are generating terabytes of data that could be analyzed in combination with other data, such as images, clinical records, and video, to enable studies of unprecedented scale, but machine learning models for analyzing these large-scale, heterogeneous, time-varying data are lacking.  To address these challenges, we will establish a Biomedical Technology Resource Center —The Mobilize Center. Through the leadership of an experienced scientific team, we will create and disseminate innovative tools to quantify movement biomechanics with mobile sensors.  Specifically, we will:  1. Push the bounds of what we can measure via wearable sensors using models that compute muscle  and joint forces and metabolic cost of locomotion. These models, based on biomechanical and machine  learning models, will be disseminated via our newly created OpenSense software, which will be used  by thousands of researchers to gain new insights into patient biomechanics using mobile sensors.  2. Meet the need for tools that analyze data about movement dynamics and develop machine learning  models to analyze and generate insights from unstructured, high-dimensional data, including time-  series (e.g., from mobile sensors), images (e.g., MRI), and video (e.g., smartphone video of a patient’s gait).  3. Provide tools needed to intervene in the real-world. We will develop algorithms to accurately quantify  kinematics outside the lab for long durations using data from inertial measurement units (IMUs). We will  also build behavioral models to adapt and personalize goal setting, drawing on movement records from  6 million individuals, as well as health goals and exercise for 1.7 million people.  Through intensive interactions with our Collaborative Projects, we will focus on improving rehabilitation outcomes for individuals with limited mobility due to osteoarthritis, obesity, Parkinson’s disease, and cerebral palsy. The Center’s tools and services will enable researchers to revolutionize how we diagnose, monitor, and treat mobility disorders, providing tools needed to deliver precision rehabilitation at low cost and on a massive scale in the future. Limited mobility due to conditions like osteoarthritis, cerebral palsy, and Parkinson’s affects millions of individuals, at a great cost to public health and personal well-being. Rehabilitation can dramatically improve mobility and function, but current rehabilitation practice requires in- person guidance by a skilled clinician, increasing expense and limiting access. This project will revolutionize how we diagnose, monitor, and treat mobility limitations and enable personalized rehabilitation at low cost and on a massive scale using wearable sensing technology in the future.",Mobilize Center: Models for Mobile Sensing and Precision Rehabilitation,9855893,P41EB027060,"['Address', 'Affect', 'Algorithms', 'Behavioral Model', 'Biomechanics', 'Biomedical Engineering', 'Biomedical Technology', 'Cellular Phone', 'Cerebral Palsy', 'Characteristics', 'Clinic', 'Clinical', 'Communities', 'Computer software', 'Custom', 'Data', 'Data Science', 'Degenerative polyarthritis', 'Diagnosis', 'Disease', 'Documentation', 'Educational workshop', 'Engineering', 'Exercise', 'Exposure to', 'Feedback', 'Foundations', 'Freezing', 'Future', 'Gait', 'Goals', 'Guidelines', 'Home environment', 'Human', 'Image', 'Individual', 'Joints', 'Leadership', 'Literature', 'Locomotion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measurement', 'Measures', 'Metabolic', 'Modeling', 'Monitor', 'Motion', 'Movement', 'Muscle', 'Obesity', 'Parkinson Disease', 'Pathologic', 'Patients', 'Personal Satisfaction', 'Persons', 'Public Health', 'Records', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Resources', 'Series', 'Services', 'Software Tools', 'Time', 'Training', 'Vision', 'base', 'biomechanical model', 'biomedical informatics', 'cohesion', 'coral', 'cost', 'evidence base', 'experience', 'handheld mobile device', 'health goals', 'improved', 'improved functioning', 'improved mobility', 'individualized medicine', 'industry partner', 'innovation', 'insight', 'joint loading', 'kinematics', 'large scale data', 'mHealth', 'mobile computing', 'multidimensional data', 'open source', 'programs', 'sensor', 'sensor technology', 'smart watch', 'societal costs', 'symposium', 'terabyte', 'tool', 'tool development', 'wearable sensor technology']",NIBIB,STANFORD UNIVERSITY,P41,2020,752316,-0.008677632732266098
"BlueBox: A Complete Code Blue Data Recorder, Phase II “Code blue” is the signal used in hospitals to call for an immediate cardiopulmonary resuscitation (CPR) following a cardiac or respiratory arrest. Reviewing the performance of the “code blue team” is a cornerstone for improving outcomes. The current standard of using handwritten records on a paper “code sheet” does not allow measurement of key quality indicators and is subject to human error. In the Phase I STTR project, we developed an electronic device for complete recording of code blue events, called BlueBox. The BlueBox is a small electronic recorder on an adhesive patch to be placed on the left chest next to the mid-sternum. The prototype we developed in Phase I was successfully tested on high fidelity mannequins and on pigs. In Phase II, our goal is to complete the product development and testing and prepare the BlueBox for regulatory clearance and market launch. To achieve this goal, we propose 3 Specific Aims. Aim 1 is to complete the product development of the BlueBox device and the software user interface (UI) for the “electronic code sheet.” We will turn the engineering prototype we developed in Phase I into a product ready for commercialization through rigorous product development processes. We will develop a mobile app for iPads with a software UI for the “electronic code sheet.” Aim 2 is to conduct human factors and usability engineering (HF/UE) testing and prepare for regulatory submission. The alpha prototype will undergo HF/UE testing in the Simulation Center. We will establish and maintain quality management records and conduct a pilot production run of 200 units of BlueBox. Aim 3 is to validate the BlueBox system in clinical studies. The objectives of the clinical study are: 1) to establish equivalence of the electronic code sheet to the current standard of paper code sheet; 2) to demonstrate the effectiveness of the electronic code sheet in identifying key CPR quality indicators. We will conduct a code blue simulation study of 50 sessions on high fidelity mannequins with hospital code blue teams to compare BlueBox recording with paper code sheets. We will conduct a study of 30 healthy volunteers for BlueBox sensor validation. The criteria for successful development of the product will be that it passes all required regulatory testing and is validated in the clinical study for its equivalence and effectiveness in code blue recording. There will be two major milestones in this project: (1) finalizing product development with successful test production of 200 units; and (2) completing the clinical study and preparing for a 510(k) submission. Achieving the aims will result in a validated BlueBox system ready for submission to the FDA and commercialization. We intend to first introduce the BlueBox system to hospitals as a tool for staff training and quality improvement. We will continue the technology development with machine learning to provide instant feedback in the second generation BlueBox. Our ultimate goal is to minimize human error and improve patient outcomes through the BlueBox system’s better documentation and continuous feedback mechanism. Modified Specific Aims  “Code blue” is the alert used in hospitals to initiate immediate cardiopulmonary resuscitation (CPR) following a cardiac or respiratory arrest.1 These situations are dire emergencies. Medical errors are likely to occur, and lives can be lost. Reviewing the performance quality of the “code blue team” is a cornerstone for improving outcomes of in-hospital arrests.2-4 Thorough and accurate recording of code blue events facilitates the detailed analyses needed for quality improvement.5-7 However, the current standard of using handwritten records on a paper “code sheet” does not allow measurement of key quality indicators and is subject to human errors. In our Phase I STTR project, we developed an electronic device for complete recording of code blue events, called BlueBox. The BlueBox is a small electronic recorder on an adhesive patch to be placed on the left chest next to the mid-sternum. It captures and records all code blue events -- vital signs, cardiac rhythm, verbal orders and their execution, chest compressions, cardioversion/defibrillation, procedures, medications, and labs. The prototype we developed in Phase I was successfully tested on high fidelity mannequins in the Simulation Center, and on pigs in the Animal Lab. The purpose of the BlueBox is to support medical training and quality improvement in code blue situations, and to enhance safety for patients undergoing CPR. In Phase II, our goal is to complete the product development and testing to prepare the BlueBox for regulatory clearance and market launch. To achieve this goal, we propose 3 Specific Aims: Specific Aim 1. Completing the product development of the BlueBox recorder and the software user interface (UI) for the “electronic code sheet” In Phase I, after developing the BlueBox technology and completing its proof-of-concept, the engineering prototype was tested successfully. The firmware drives all sensors and enables simultaneous recordings of all parameters with time stamps. The circuit can withstand 5kV, which is what is used in cardioversion and defibrillation. In Phase II, we will turn the prototype into a product ready for commercialization through rigorous product development processes. The product development processes include: miniaturization, mechanical design, industrial design, and usability engineering, as well as development of a mobile app for an iPad with an “electronic code sheet” user interface (UI) displaying the code blue events. To provide instant feedback during CPR, we will develop model-based and machine learning data analytics during and beyond the Phase II project. Specific Aim 2. Conducting human factors and usability testing, quality management and regulatory support and preparation We will conduct human factors and usability engineering (HF/UE) testing on the alpha prototype in the Simulation Center. The first HF/UE study aims to test the use of the BlueBox recorder by members and captains of the hospital code team in a code blue scenario. The second HF/UE study aims to test the software and UI of the electronic code sheet on iPads, as used by members of the hospital code blue team, hospital administrators, and EMR and IT specialists. We will establish and maintain quality management records and conduct a pilot production run of 200 units of BlueBox. The pilot run units will be tested for reliability and validity in the Simulation Center. We will request a pre-submission meeting (Qsub) with the FDA. In the Qsub meeting, we will discuss specific regulatory submission requirements and obtain feedback on the clinical validation study. Specific Aim 3. Validating the BlueBox system in clinical studies We will first conduct a prospective study of 50 sessions of simulated code blue resuscitations. Each session will be attended by a team of 4 clinicians-- a captain (physician), a nurse, an ancillary staff, and a code sheet recording staff (typically a nurse). We will also conduct a study of BlueBox sensor validation in 30 healthy volunteers. We will conduct a code blue simulation study on high fidelity mannequins with a hospital code blue team to compare BlueBox recording with paper code sheets. The objectives of the clinical studies are: 1) to establish equivalence of the electronic code sheet to the current standard of paper code sheet; 2) to demonstrate the effectiveness of the electronic code sheet in identifying key CPR quality indicators specified in the American Heart Association (AHA) guidelines. Feasibility Criteria: The criteria for successful development of the BlueBox are:1) it passes all required regulatory testing; 2) it is validated in the clinical study for its equivalence and effectiveness in code blue recording and quality review and improvement. Expected Outcomes and Impact: Two major milestones are (1) finalizing product development in Year 1, and (2) completing the clinical study in Year 2. Achieving the aims will result in a validated BlueBox system ready for regulatory submission to the FDA and commercialization. We intend to first market the BlueBox system to hospitals as a tool for staff training and quality improvement. We will continue the development of BlueBox technology with machine learning algorithms to provide instant feedback. Our ultimate goal is to minimize human error and improve patient outcomes through the BlueBox system’s continuous feedback mechanism. Debriefings and detailed reviews of the performance of the “code blue team” in cardiopulmonary resuscitation (CPR) can improve quality of care and patient outcomes. In Phase I, we developed and successfully tested an electronic device, the BlueBox, for recording all CPR events and enabling full displays of code blue resuscitations in an “electronic code sheet.” We will turn the engineering prototype into a product ready for regulatory submission and commercialization in the proposed Phase II project.","BlueBox: A Complete Code Blue Data Recorder, Phase II",9857035,R42GM113463,"['Accident and Emergency department', 'Adhesives', 'American Heart Association', 'Animals', 'Cardiac', 'Cardiopulmonary Resuscitation', 'Chest', 'Clinical', 'Clinical Research', 'Code', 'Code Blue', 'Data', 'Data Analytics', 'Development', 'Devices', 'Documentation', 'Effectiveness', 'Electric Countershock', 'Electronics', 'Emergency Situation', 'Engineering', 'Event', 'Family suidae', 'Feedback', 'Generations', 'Goals', 'Guidelines', 'Hospital Administrators', 'Hospitals', 'Human', 'Industrialization', 'Left', 'Machine Learning', 'Manikins', 'Measurement', 'Mechanics', 'Medical', 'Medical Errors', 'Miniaturization', 'Modeling', 'Outcome', 'Paper', 'Patient Recruitments', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Pilot Projects', 'Preparation', 'Procedures', 'Process', 'Production', 'Quality Indicator', 'Quality of Care', 'Records', 'Resuscitation', 'Running', 'Signal Transduction', 'Small Business Technology Transfer Research', 'Specialist', 'Specific qualifier value', 'Sternum', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Validity and Reliability', 'base', 'care outcomes', 'commercialization', 'design', 'graphical user interface', 'heart rhythm', 'human error', 'improved', 'improved outcome', 'machine learning algorithm', 'meetings', 'member', 'mobile application', 'patient safety', 'product development', 'prototype', 'respiratory', 'sensor', 'simulation', 'technology development', 'tool', 'usability', 'validation studies']",NIGMS,"NEOVATIVE, INC.",R42,2020,821493,-0.0010270848138077466
"Wearable Sensors for Biofeedback & Remote Monitoring Rotator cuff tears are a common condition affecting approximately 25% of the population older than 60 years, and rotator cuff repair is a standard surgical procedure with nearly 500,000 procedures performed annually. In the United States alone the direct costs are over $7 billion per year. There is a relatively high rate of retears: ranging from 10% to 78%. Healing of the repair cuff is a protracted process during which the repaired tendon has to be unloaded by carefully restricting active muscle contraction. There is an inherent conflict in immobilizing the shoulder to protect the repair and mobilizing the shoulder to prevent stiffness. Teaching the patient passive exercises that do not activate contraction of the supraspinatus muscle is challenging and very difficult to monitor. To address this unmet need, we have developed a wearable sensor that provides direct real-time continuous biofeedback of muscle activity, joint angle, skin temperature, and swelling to enable passive shoulder exercises while minimizing muscle contraction. To test and validate this device we propose to the following Specific Aims. Aim 1: Design, manufacture, and test a wearable device with surface EMG (sEMG), inertial measurement unit (IMU), temperature, and bioimpedance sensors. Aim 2: Measure accuracy of Active4D sEMG and IMUs relative to standard clinical measurements. Active4D is a surgeon-driven company devoted to enhancing patient recovery from surgery by leveraging new technology. Active4D’s unique wearable sensor provides continuous long-term sensor augmented biofeedback to patients and enables remote patient monitoring for surgeons. The device measures seven physiologic metrics in real time – muscle activation, joint range of motion, skin temperature, swelling, activity, gait, and fall risk. Real time physiologic data will increase safety, improve outcomes, and enhance patient experience while lowering the cost of rehabilitation and complications. Unique artificial intelligence driven algorithms can decrease complications through predictive analytics which enable early detection and intervention for high risk patients.  The specific innovation to the application of rotator cuff repair, is the instantaneous feedback of muscle activity and motion which can accelerate recovery while reducing the risk of retears. At the successful completion of Phase I, we plan to launch a clinical trial comparing the outcomes of rotator cuff repair in patients guided by the A4D device in comparison to the current standard of postoperative rehabilitation.  Active4D is a small business with the objective of developing smart wearables, which integrate multiple biological and environmental sensors, wireless communication, computer models, and data analysis for providing feedback and remote monitoring, to enhance surgical rehabilitation and improved patient outcomes in orthopaedics. Co-founders Drs. Hoenecke and D’Lima have extensive experience in shoulder surgery and rehabilitation, multi-center clinical trials, and biomechanics research; and have successfully executed challenging projects such as implantation of innovative electronic knee designs and development of new shoulder implants. Dr. Hoenecke has over 30 years of experience in the practice of sports medicine with an emphasis on shoulder repair and reconstruction. He provides care for elite athletes and is design surgeon for two shoulder implants. Active4D has partnered with expert physical therapists and orthopedists to develop and test our innovative technology. Rotator cuff tears are a common condition affecting approximately 25% of the population older than 60 years. Despite the popularity of surgical repair, the rate of retears is high: ranging from 10% to 78%. Healing of the repair cuff is a lengthy process during which the repaired tendon has to be unloading by carefully restricting active muscle contraction. This Phase I SBIR will develop an innovative wearable device with the potential to facilitate safe exercises, alert the patient during activities at risk for retears, and accelerate recovery while reducing the risk for retears. The device can also remotely monitor progress of the patient over the postoperative period and can modulate the feedback appropriate to the phase of recovery.",Wearable Sensors for Biofeedback & Remote Monitoring,10156598,R43AR078082,"['Address', 'Affect', 'Algorithms', 'Articular Range of Motion', 'Artificial Intelligence', 'Biofeedback', 'Biological', 'Biomechanics', 'Biophysics', 'Businesses', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Complication', 'Computer Models', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Analyses', 'Data Analytics', 'Development', 'Devices', 'Direct Costs', 'Early Diagnosis', 'Early Intervention', 'Educational process of instructing', 'Exercise', 'Feedback', 'Funding', 'Gait', 'Health', 'Immobilization', 'Implant', 'Industry', 'Infection', 'Joints', 'Knee', 'Laboratories', 'Legal patent', 'Letters', 'Measurement', 'Measures', 'Mission', 'Monitor', 'Motion', 'Multi-Institutional Clinical Trial', 'Muscle', 'Muscle Contraction', 'Older Population', 'Operative Surgical Procedures', 'Orthopedics', 'Outcome', 'Patient Monitoring', 'Patient-Focused Outcomes', 'Patients', 'Phase', 'Physiological', 'Postoperative Period', 'Predictive Analytics', 'Procedures', 'Process', 'Recovery', 'Rehabilitation therapy', 'Reproducibility', 'Research', 'Resources', 'Risk', 'Rotator Cuff', 'Safety', 'Science', 'Seeds', 'Series', 'Shoulder', 'Skin Temperature', 'Small Business Innovation Research Grant', 'Sports Medicine', 'Surface', 'Surgeon', 'Swelling', 'System', 'Temperature', 'Tendon structure', 'Testing', 'Thick', 'Time', 'United States', 'cost', 'design', 'experience', 'fall risk', 'healing', 'high risk', 'implantation', 'improved', 'improved outcome', 'innovation', 'innovative technologies', 'interest', 'miniaturize', 'new technology', 'physical therapist', 'post-operative rehabilitation', 'prevent', 'prototype', 'reconstruction', 'repaired', 'rotator cuff tear', 'sensor', 'success', 'supraspinatus muscle', 'trial comparing', 'wearable device', 'wearable sensor technology', 'wireless communication']",NIAMS,"ACTIVE4D, INC.",R43,2020,250998,-0.012595531309811576
"BehaviorSight: Privacy enhancing wearable system to detect health risk behaviors in real-time. Project Summary/Abstract Health-risk behaviors, such as overeating, smoking, consuming alcohol, and not adhering to medication, are responsible for increases in morbidity and mortality. To track and intervene during these health-risk behaviors, clinicians traditionally rely on self-reports. However, self-reports are inaccurate and biased. Therefore, we cannot use self-reports to validate health-risk behaviors in free-living conditions. Thus, an automated technique for validating health-risk behaviors is extremely necessary. With the growth and popularity of wearable devices (e.g., smartwatches), automatic monitoring of physical activity is possible. However, the devices often do not provide any visual confirmation, making it challenging to verify activities performed in free-living conditions. Cameras can capture point-of-view videos and can thus be used as a wearable device to capture videos for visual confirmation of activities, including health-risk behaviors. Such recordings can help us better understand health-risk behaviors. Additionally, video information can be automatically processed to confirm and validate health-risk behaviors. Recording videos of sensitive content and bystanders is associated with privacy and ethical concerns. Currently there is no privacy-preserving camera that can automatically detect health-risk behaviors, and most people are unwilling to wear cameras without raising privacy concerns. In addition to privacy concerns, people prefer wearables that are unobtrusive and small and that do not require frequent charging. Thus, a privacy-preserving, unobtrusive wearable camera would increase wearability. Infrared (IR) sensor arrays have the potential to provide independent temperature readings, which allows determining whether an object is near or far. The IR sensor array can help record only the wearer and objects near the wearer, while filtering out distant objects. IR sensor arrays have a small power footprint, thus providing longer battery life. Our project aims to develop a privacy-conscious, unobtrusive, wearable, behavior-detection platform that will make it possible to detect and intervene upon health-risk behaviors in real time. In this project, we will (1) develop the wearable behavior-detection device that allows visual confirmation without burdening the wearer. The device will augment RGB camera data with IR sensor array data for privacy-conscious recording and automatic behavior detection. (2) We will test various designs to determine a user's acceptability to wear the device. Then, we will test various image processing techniques and machine learning algorithms to determine the best algorithm for detecting health-risk behaviors. (3) We will incorporate the best-performing behavior-detection algorithm so that it can run on the developed wearable device. With a behavior-detection algorithm running on an acceptable wearable device, the ability to detect health-risk behaviors in real time will become a reality. Ultimately, our wearable device will allow researchers to test and apply appropriate behavioral interventions in real time, rather than relying on self-reports, whenever health-risk behaviors occur. Project Narrative Several activities involving hand-to-mouth gestures (e.g., overeating, smoking, consuming alcohol, or non- adherence to medication) are associated with health-risk behaviors that are a leading cause of preventable deaths. Being able to automatically monitor health-risk behaviors using wearable video cameras will improve our understanding of these behaviors and will ultimately enable us to design effective methods to intervene when they occur. We will develop BehaviorSight, a privacy-conscious, unobtrusive, wearable, behavior-detection device that will allow future researchers and behavioral scientists to use the device for monitoring numerous everyday health-risk behaviors in free-living settings.",BehaviorSight: Privacy enhancing wearable system to detect health risk behaviors in real-time.,10043674,R21EB030305,"['Address', 'Alcohol consumption', 'Algorithms', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Bluetooth', 'Cellular Phone', 'Charge', 'Chest', 'Clinical', 'Communication', 'Conscious', 'Data', 'Detection', 'Devices', 'Dietitian', 'Disease', 'Distant', 'Eating', 'Ensure', 'Ethics', 'Future', 'Gestures', 'Goals', 'Grant', 'Growth', 'Hand', 'Health', 'Hyperphagia', 'Intervention', 'Laboratory Study', 'Learning', 'Life', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Notification', 'Obesity', 'Oral cavity', 'Participant', 'Patient Self-Report', 'Pharmaceutical Preparations', 'Physical activity', 'Privacy', 'Process', 'Reading', 'Records', 'Research', 'Research Personnel', 'Resources', 'Risk Behaviors', 'Running', 'Science', 'Scientist', 'Smoke', 'Smoking', 'Substance abuse problem', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Video Recording', 'Visual', 'cost', 'data privacy', 'design', 'drinking', 'image processing', 'improved', 'light weight', 'machine learning algorithm', 'medication nonadherence', 'miniaturize', 'monitoring device', 'mortality', 'multimodality', 'novel', 'prevent', 'preventable death', 'privacy preservation', 'response', 'sensor', 'smart watch', 'wearable device', 'wearable sensor technology', 'willingness']",NIBIB,NORTHWESTERN UNIVERSITY AT CHICAGO,R21,2020,606713,-0.00013986153639279733
"Continuous Monitoring of COVID-19 Symptomatology for Elderly Patients in Long Term Care Facilities Using Advanced, Soft, and Flexible Sensors Mounted on the Suprasternal Notch PROJECT SUMMARY: COVID-19 is significantly more lethal in the elderly1 with the greatest risk in those cared for in long-term care facilities (LTCs) where mortality rates range from 19% to 72% worldwide. Monitoring COVID-19 infections in LTCs remains a particular challenging. The existing and a continued expected shortage of sufficient molecular COVID-19 testing coupled to false negative rates as high as 15% necessitates a critical need for new and complementary technologies that can surveil, alert, and track COVID-19 infections in this population. Our group are pioneers in the development of novel soft electronics. Our recent publication, supported by our active Phase I STTR, was published in Nature Biomedical Engineering detailing a next generation ultra-low profile, soft, and flexible sensor (ADAM) that continuously measures subtle acousto- mechanic signals generated by the body via an embedded high-frequency, 3-axis accelerometer in direct mechanical communication with the skin. The ADAM sensor communicates via Bluetooth with our custom mobile application for real time streaming as well as on sensor data storage enabling stand-alone operation. All data streams are cloud synchronized (HIPAA compliant). The highly novel soft, flexible nature allows for the ADAM sensor to be mountable on unusual locations of high information density. Specifically, we exploit the SN—the only location on the body where there is no dampening effect at the skin level with the intrathoracic cavity. This enables a SN- mounted ADAM sensor to capture heart rate (HR), respiratory rate (RR), temperature, physical activity (PA), swallow count, and talk time, along with additional novel respiratory biomarkers relevant to COVID-19. In this proposal, we propose to develop a new COVID-19 software package, machine learning enhancements to our cough algorithm, and validation in LTCs with both elderly patients and staff to evaluate usability, feasibility, and adherence. The high level of technology readiness with partner LTCs allows us to deploy efficiently to generate essential data for a future FDA Emergency Use Authorization. Our team of experts in engineering, dermatology, gerontology, and machine learning are highly qualified to develop this COVID-19 surveillance system that offers both commercial and clinical value with broad applicability to a wide range of other respiratory and chronic medical conditions after the pandemic subsides. PROJECT NARRATIVE COVID-19 is significantly more lethal in the elderly resident in long-term care facilities. The shortage of molecular COVID-19 testing necessitates a critical need for new and complementary technologies that can surveil, alert, and track COVID-19 infections in this population. Our recent publication, supported by this active Phase I STTR, was published in Nature Biomedical Engineering detailing a next generation ultra-low profile, soft, and flexible sensor (ADAM) that continuously measures heart rate (HR), respiratory rate (RR), temperature, physical activity (PA), swallow count, and talk time, along with additional novel respiratory biomarkers relevant to COVID-19 directly adapted to the elderly.","Continuous Monitoring of COVID-19 Symptomatology for Elderly Patients in Long Term Care Facilities Using Advanced, Soft, and Flexible Sensors Mounted on the Suprasternal Notch",10167884,R41AG062023,"['Accelerometer', 'Adherence', 'Age', 'Algorithms', 'Alzheimer&apos', 's Disease', 'American', 'Apple watch', 'Authorization documentation', 'Biological Markers', 'Biomedical Engineering', 'Bluetooth', 'COVID-19', 'COVID-19 pandemic', 'Caring', 'Cessation of life', 'Chronic', 'Classification', 'Clinical', 'Clinical Research', 'Communication', 'Computer software', 'Consent', 'Coughing', 'Coupled', 'Custom', 'Data', 'Data Storage and Retrieval', 'Deglutition', 'Deglutition Disorders', 'Dementia', 'Dermatology', 'Detection', 'Development', 'Devices', 'Disease Progression', 'Elderly', 'Electronics', 'Emergency Situation', 'Engineering', 'Ensure', 'Exanthema', 'Fever', 'Focus Groups', 'Frequencies', 'Future', 'Gerontology', 'Goals', 'Gold', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Health care facility', 'Heart Rate', 'Hour', 'Infection', 'Location', 'Long-Term Care', 'Machine Learning', 'Measures', 'Mechanics', 'Medical', 'Molecular', 'Monitor', 'Motion', 'Nature', 'Nursing Homes', 'Operating System', 'Patients', 'Phase', 'Physical activity', 'Physicians', 'Physiological', 'Population', 'Provider', 'Psychometrics', 'Public Health', 'Publications', 'Publishing', 'Quarantine', 'Readiness', 'Recovery', 'Reporting', 'Respiratory Signs and Symptoms', 'Respiratory Sounds', 'Risk', 'Sensitivity and Specificity', 'Shortness of Breath', 'Signal Transduction', 'Skin', 'Small Business Technology Transfer Research', 'Smell Perception', 'Stream', 'Stretching', 'Surveys', 'Symptoms', 'System', 'Taste Perception', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Training', 'Validation', 'Wireless Technology', 'Wisconsin', 'arm', 'body position', 'dashboard', 'data quality', 'data streams', 'density', 'design', 'experience', 'fitbit', 'flexibility', 'improved', 'microphone', 'mobile application', 'mortality', 'next generation', 'notch protein', 'novel', 'older patient', 'operation', 'pandemic disease', 'programs', 'respiratory', 'sensor', 'success', 'symptomatology', 'terabyte', 'tool', 'usability', 'virtual', 'volunteer', 'wearable sensor technology']",NIA,"SONICA, LLC",R41,2020,249304,-0.05368190097577299
"Just-In-Time Fall Prevention: Development of an mHealth Intervention for Persons with Multiple Sclerosis One out of every two of the 2.3 million persons with Multiple Sclerosis (PwMS) report a fall in any given three- month period. The onset of MS is often during early or middle adulthood, making MS-induced falls a significant, long-term problem in need of new preventative interventions. Here we propose to advance several components of Just-In-Time Fall Prevention – a novel mHealth (mobile health) approach for preventing falls and to demonstrate each component in a sample of PwMS. The proposed mHealth system will be composed of wireless, wearable sensors and a mobile phone application. The wearable sensors will capture patient biomechanics and a network of statistical models, created using machine learning and deployed on the mobile phone, will predict fall risk based upon these measurements. Fall risk predictions will inform personalized interventions, delivered through the mobile application, that leverage strategies from social psychology designed to induce biomechanical and behavioral changes to immediately reduce fall risk. This approach enables real- time assessment and intervention to prevent future falls. Data to develop and pilot each component of this mHealth system will be collected in a study of N=50 PwMS that will a) capture concurrent measurements from wearable sensors, optical motion capture, force platforms, and an instrumented treadmill during functional assessments and simulated daily activities, b) track falls and objective biomechanical and behavioral measures during a 3-month in-home study, and c) assess the efficacy of point-of-choice prompts for altering fall-related biomechanics during balance-challenging daily activities. These data will be used to accomplish the following specific aims: 1) Validate wearable sensor algorithms for capturing biomechanics and behavior of PwMS, 2) Identify digital biomarkers for quantifying fall risk in real time during daily life in PwMS, and 3) Pilot point-of- choice prompts for inducing biomechanical changes in PwMS under realistic cognitive load. These aims are the first step toward our goal of developing a paradigm-shifting mHealth system for fall prevention and lead naturally to future R01 support for a randomized, controlled clinical trial testing the efficacy of this approach. This project will advance several components of a novel mobile health intervention for preventing falls. If successful, this approach has the potential to shift fall preventions efforts from reactive and physician centered to preventative and patient centered improving long-term health outcomes and quality of life for those with balance and mobility impairment.",Just-In-Time Fall Prevention: Development of an mHealth Intervention for Persons with Multiple Sclerosis,10009334,R21EB027852,"['Acute', 'Adult', 'Affect', 'Algorithms', 'Behavior', 'Behavioral', 'Biological Markers', 'Biomechanics', 'Car Phone', 'Cognitive', 'Data', 'Development', 'Equilibrium', 'Exercise', 'Fall prevention', 'Future', 'Goals', 'Gold', 'Health', 'Health Personnel', 'Healthcare Systems', 'Heart', 'Home environment', 'Impairment', 'Injury', 'Intervention', 'Judgment', 'Lead', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Medical Care Costs', 'Modality', 'Motion', 'Multiple Sclerosis', 'Optics', 'Outcome', 'Patient Self-Report', 'Patients', 'Persons', 'Physicians', 'Preventive Intervention', 'Quality of life', 'Randomized Controlled Clinical Trials', 'Reporting', 'Research Design', 'Sampling', 'Science', 'Self-Help Devices', 'Social Psychology', 'Statistical Models', 'Study Subject', 'Symptoms', 'System', 'Techniques', 'Time', 'Walking', 'Wireless Technology', 'Work', 'base', 'behavior measurement', 'cognitive load', 'design', 'digital', 'efficacy testing', 'fall risk', 'falls', 'improved', 'instrument', 'mHealth', 'medical attention', 'mobile application', 'nervous system disorder', 'novel', 'patient oriented', 'personalized intervention', 'prevent', 'programs', 'prospective', 'standard measure', 'treadmill', 'wearable sensor technology']",NIBIB,UNIVERSITY OF VERMONT & ST AGRIC COLLEGE,R21,2020,229404,-0.0039126834176078
"Inferential methods for functional data from wearable devices Project Summary/Abstract This is a project to develop new statistical methods for comparing groups of subjects in terms of health outcomes that are assessed using data from wearable devices. Inexpensive wearable sensors for health monitoring are now capable of generating massive amounts of data collected longitudinally, up to months at a time. The project will develop inferential methods that can deal with the complexity of such data. A serious challenge is the presence of unmeasured time-dependent confounders (e.g., circadian and dietary patterns), making direct comparisons or borrowing strength across subjects untenable unless the studies are carried out in controlled experimental con- ditions. Generic data mining and machine learning tools have been widely used to provide predictions of health status from such data. However, such tools cannot be used for signiﬁcance testing of covariate effects, which is necessary for designing precision medicine interventions, for example, without taking the inherent model selection or the presence of the unmeasured confounders into account. To overcome these difﬁculties, a systematic de- velopment of inferential methods for functional outcome data obtained from wearable devices will be carried out. There are three speciﬁc aims: 1) Develop metrics for functional outcome data from wearable devices, 2) Develop nonparametric estimation and testing methods for activity proﬁles and a screening method for predictors of activity proﬁles, 3) Implement the methods in an R package and carry out two case studies using accelerometer data. For Aim 1, the approach is to reduce the sensor data to occupation time proﬁles (e.g., as a function of activity level), and formulate the statistical modeling in terms of these proﬁles using survival and functional data analytic meth- ods. This will have a number of advantages, the principal one being that time-dependent confounders become less problematic because the effect of differences in temporal alignment across subjects is mitigated. In addition, survival analysis methods can be applied by viewing the occupation time as a time-to-event outcome indexed by activity level. For Aim 2, nonparametric methods will be used to compare and order occupation time distributions between groups of subjects that are speciﬁed in terms of baseline covariate levels or treatment groups. Further, a new method of post-selection inference based on marginal screening for function-on-scalar regression will be developed to identify and formally test whether covariates are signiﬁcantly associated with activity proﬁles. Aim 3 will develop an R-package implementation, and as a test-bed for the proposed methods they will be applied to two Columbia-based clinical studies: to the study of physical activity in children enrolled in New York City Head Start, and to the study of experimental drugs for the treatment of mitochondrial depletion syndrome. Project Narrative The relevance of the project to public health is that it will develop statistical methods for the physiological eval- uation of patients on the basis of data collected by inexpensive wearable sensors (e.g., accelerometers). By introducing methods for the rigorous comparison of healthcare status among groups of patients observed longi- tudinally over time using such devices, treatment decisions that can beneﬁt targeted populations of patients in terms of continuously-assessed health outcomes will become possible.",Inferential methods for functional data from wearable devices,9924432,R01AG062401,"['Acceleration', 'Accelerometer', 'Beds', 'Bypass', 'Case Study', 'Characteristics', 'Child', 'Clinical Research', 'Computer software', 'Data', 'Data Analytics', 'Development', 'Devices', 'Dietary Practices', 'Drug Combinations', 'Enrollment', 'Evaluation', 'Event', 'Grant', 'Head Start Program', 'Health', 'Health Status', 'Healthcare', 'Intervention', 'Lead', 'Machine Learning', 'Measures', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Monitor', 'Motivation', 'Nature', 'New York City', 'Obesity', 'Occupations', 'Outcome', 'Outcome Measure', 'Patients', 'Pharmacotherapy', 'Physical activity', 'Physiological', 'Preschool Child', 'Process', 'Proxy', 'Public Health', 'Recording of previous events', 'Regimen', 'Signal Transduction', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Stochastic Processes', 'Survival Analysis', 'Syndrome', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Work', 'analytical method', 'base', 'circadian', 'data mining', 'design', 'experimental study', 'functional outcomes', 'indexing', 'interest', 'lower income families', 'novel', 'patient population', 'precision medicine', 'screening', 'sensor', 'theories', 'time use', 'tool', 'treatment group', 'wearable device', 'wearable sensor technology']",NIA,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,298890,-0.034260751299422416
"Development of assistive self-care robot technologies for people with disabilities Towards Autonomy in Daily Living: A Formalism for Intelligent Assistive Feeding Systems  Applicant PI, Tapomayukh Bhattacharjee Overview We propose to develop a design space framework and co-design methodology for the development of assistive self-care robot technologies that are informed by the social model of disability. Our model of assistive robots in the domain of self-care considers an individual's social and environmental context, coping processes and other factors that can affect independent functioning. Our design methods utilize embedded sensing to intelligently respond to these con- siderations. We speciﬁcally focus on assistive feeding tasks, proposing a formalism that enables a robotic system to feed a person with upper-extremity disability. Our guiding principle is that human-level interaction is feasible only if the robot itself relies on human-level semantics. We im- plement this principle by relying on data to learn and develop object-dependent control policies and timing models for acquiring and transferring a bite to a user at a proper time. The system's ob- server detects world states and arbitrator invokes different control policies based on these states. The tangible result will be an intelligent assistive feeding robot whose performance can generalize to different activities, adapt to user preferences, and recover from failures. Objectives and Relevance to NIH A design framework for assistive robots would provide for- malisms that let us address the fundamental challenge of designing robots that are responsive to context of use and support assisted self-care in a variety of social settings. We combine method- ologies from human-robot interaction, cognitive science, machine learning, robotics and haptics with user studies and our formalism to address the following research questions: (Q1) Mechanics of Feeding-Control Policies: How can control policies be designed for dexterous non-prehensile manipu- lation of deformable objects such as food? (Q2) Social Aspects of Feeding-Bite Timing: How should an assistive feeding robot decide the right timing for feeding a user? (Q3) Human-in-the-Loop: How can human-directed feedback be added into the loop for an autonomous assistive feeding system?  The proposed work will allow users with upper-arm disabilities to use this system for intelli- gent assistance with daily feeding tasks. This can in turn help them increase their independence and autonomy making eating easier and more enjoyable. While we presently focus on this spe- ciﬁc application, the tools and insights we gain can generalize to the ﬁelds of robotic assistance and human-robot interaction across other activities of daily living and instrumental activities of daily living. Thus, our work is clearly motivated by the intent to improve the quality of health and life of the aging population and is very relevant to the theme of NIH. 1 Towards Autonomy in Daily Living: A Formalism for Intelligent Assistive Feeding Systems  Applicant PI, Tapomayukh Bhattacharjee  The proposed work will allow users with upper-arm disabilities to use this system for intelli- gent assistance with daily feeding tasks, potentially increasing their independence and autonomy making eating easier and more enjoyable. The long-term promise of this research is to have robots in society that are able to seamlessly and ﬂuently perform complex manipulation tasks in dynamic human environments in real homes which could impact individuals with other disabilities as well as able-bodied individuals. Through improved access to independent living and customizing to the unique needs and preferences of users, the results of this project can positively impact mil- lions of people worldwide, especially given the vast variability in our target population by being transformational in the scalability of assistive robotics for self-care. 1",Development of assistive self-care robot technologies for people with disabilities,9907705,F32HD101192,"['Activities of Daily Living', 'Address', 'Affect', 'Aging', 'Bathing', 'Bite', 'Caregiver Burden', 'Caring', 'Child', 'Cognitive Science', 'Communities', 'Complex', 'Cues', 'Custom', 'Data', 'Development', 'Disabled Persons', 'Eating', 'Emotional', 'Environment', 'Expert Systems', 'Failure', 'Family', 'Feedback', 'Food', 'Generations', 'Health', 'Home environment', 'Human', 'Improve Access', 'Independent Living', 'Individual', 'Intelligence', 'Learning', 'Life', 'Machine Learning', 'Mechanics', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Panthera leo', 'Parents', 'Performance', 'Persons', 'Play', 'Policies', 'Population', 'Process', 'Quality of life', 'Research', 'Robot', 'Robotics', 'Role', 'Self Care', 'Semantics', 'Societies', 'Sterile coverings', 'System', 'Target Populations', 'Taxonomy', 'Technology', 'Time', 'Tweens', 'United States National Institutes of Health', 'Upper Extremity', 'Upper arm', 'Work', 'aging population', 'assistive robot', 'base', 'care recipients', 'coping', 'design', 'disability', 'experience', 'experimental study', 'feeding', 'haptics', 'human subject', 'human-in-the-loop', 'human-robot interaction', 'improved', 'insight', 'instrumental activity of daily living', 'intergenerational', 'kinematics', 'patient oriented', 'peer', 'preference', 'robot assistance', 'robotic system', 'social', 'social model', 'tool']",NICHD,UNIVERSITY OF WASHINGTON,F32,2020,65310,-0.026043969044302853
"Device to control circadian-effective light in Alzheimer's disease environments Project Summary This proposed project will develop and field-test a device that accurately monitors and controls the circadian stimulus (CS) for Alzheimer disease (AD) and Alzheimer-disease-related dementia (ADRD) patients in nursing homes. Human biology has evolved to have two distinct optical systems: the visual system, by which we see and process images, and the circadian system, which regulates our biological clock and associated biological systems. These two systems have significantly different spectral and temporal responses to optical input. Specifically, circadian stimulation peaks at 460 nm and responds after several minutes of optical activation, while the visual system peaks at 555 nm and responds nearly instantaneously to inputs. All lighting systems are designed and installed in buildings with consideration only given to the photopic (visual) system and all light meters used to characterize lighting buildings are calibrated to measure photopic light, not CS. While a broad and growing body of research has documented the impacts of the circadian system on human health, including regulating sleep and improving cognition in AD/ADRD patients, research on the CS experienced by AD/ADRD patients is extremely limited. Researchers at the Lighting Research Center at Rensselaer Polytechnic Institute developed the Daysimeter, a calibrated light meter that measures circadian light and circadian stimulus. In Phase I of this project, researchers modified an existing workstation-based lighting control system they previously developed for the visual system to include Daysimeter technology, allowing this control system to record CS measurements. The accuracy of these CS measurements was confirmed in the laboratory and field-testing of 20 of devices is currently ongoing in AD/ADRD nursing homes. In this Phase II application, researchers propose adding control features to this device so that lighting can be controlled to optimize CS dosages in AD/ADRD patient environments. Machine learning-based lighting control algorithms will be driven by continuous light level and spectrum measurements as well as periodic (e.g., daily) patient health data. Data from these devices would be wirelessly transmitted to researchers via an Internet gateway and associated cloud-based data management systems. These data would be of immediate value for gaining a better understanding of AD/ADRD patients' CS exposure and could ultimately result in new lighting systems and/or building codes that consider both our visual and circadian systems. Following the development phase, 30 CS-enabled lighting control systems will be field tested over a 22-week test period. Researchers aim to commercialize this CS-enabled lighting control system shortly after the completion of this field test and the Phase II project specifically targeting AD/ADRD nursing home applications. Project Narrative A growing body of research has demonstrated how light impacts human circadian systems and how these impacts can affect sleep, alertness, cognition and agitation in people with Alzheimer's disease (AD) and Alzheimer's-disease-related dementia (ADRD). Still, significant knowledge gaps exist in determining how much circadian stimulation is typically provided to AD/ADRD patients and there are no commercial products designed to control lighting in AD/ADRD environments in ways that promote circadian-related health. This project aims to fill in these gaps by developing and testing a device specifically designed to measure and control the circadian stimulation experienced by AD/ADRD patients in nursing homes.",Device to control circadian-effective light in Alzheimer's disease environments,10018621,R44AG060857,"['Affect', 'Agitation', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease related dementia', 'Back', 'Behavior', 'Biological Clocks', 'Building Codes', 'Characteristics', 'Clinical Trials', 'Cognition', 'Data', 'Database Management Systems', 'Development', 'Device or Instrument Development', 'Devices', 'Dose', 'Effectiveness', 'Elderly', 'Environment', 'Feeds', 'Health', 'Hour', 'Human', 'Human Biology', 'Image', 'Institutes', 'Internet', 'Intervention', 'Knowledge', 'Laboratories', 'Light', 'Lighting', 'Machine Learning', 'Measurement', 'Measures', 'Monitor', 'Moods', 'Nursing Homes', 'Optics', 'Patients', 'Pattern', 'Performance', 'Periodicity', 'Phase', 'Phototherapy', 'Planet Earth', 'Population', 'Process', 'Reporting', 'Research', 'Research Personnel', 'Retina', 'Rotation', 'Running', 'Sleep', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Vision', 'Visual', 'Visual system structure', 'Wakefulness', 'Wireless Technology', 'Work', 'active control', 'alertness', 'appropriate dose', 'awake', 'base', 'biological systems', 'circadian', 'circadian pacemaker', 'cloud based', 'commercialization', 'design', 'dosage', 'effectiveness testing', 'experience', 'falls', 'field study', 'health data', 'improved', 'interest', 'meter', 'next generation', 'novel', 'prototype', 'residence', 'response', 'success', 'therapy design']",NIA,"ERIK PAGE AND ASSOCIATES, INC.",R44,2020,1232387,-0.010740698434711392
"Graphene-based Nanosensor Device for Rapid, Onsite Detection of Total Lead in Tap Water PROJECT SUMMARY Detrimental health impacts of lead are largely attributed to long-term exposures to undetected lead, which are particularly troublesome and problematic because of the neurological damage to children, a situation that should not be tolerated by an advanced society like the U.S. The Flint Water Crisis and many other water catastrophes could have been avoided if early warning can be made possible through timely detection of lead in drinking water at the point of use. Our extensive customer interviews unambiguously suggest that current options for lead detection are unsatisfactory for on-site testing, as they represent two extremes: one being accurate but expensive, slow, and hard to use; and the other being low-cost, fast, and easy to use but inaccurate. NanoAffix Science LLC (NAFX) proposes to address the above unmet need and niche market product gap by empowering water users (particularly those in economically disadvantaged communities) and water service providers with a low-cost, easy-to-use, and accurate handheld tester for rapid detection of total lead in the tap water, right from the kitchen sink. The handheld lead tester combines a novel proprietary micro-sized sensor chip embedded in a proprietary test cell with a portable digital meter for direct readout of testing results. The Phase I project has successfully established the feasibility for detection of soluble lead in the tap water using an earlier version of the prototype handheld tester. The Phase II project will continue to develop the handheld tester toward total lead detection, better device uniformity, pilot scale-up manufacturing, and accurate calibration. At the end of the Phase II project, NAFX plans to produce 20 beta units of the handheld lead tester meeting all performance specifications for field validation by 10 initial customers (e.g., schools/daycares, end water users, and well water drillers). Major innovations of the proposed approach include accurate prediction of the particulate lead through partial digestion based on lead digestion kinetics, and strategic and synergistic improvement of the ultimate sensor prediction accuracy by (1) improving the physical sensor device uniformity (both intra-wafer and inter-wafer) through innovative device configuration and rigorous quality control; and (2) improving the calibration accuracy through innovative theoretical equilibrium chemistry modeling and machine learning data analytics. The NAFX handheld lead tester is the first of its kind to (1) offer all three features sought by customers: accurate, cheap, and fast; and (2) to simultaneously report all three types of lead: total lead (indicative of overall toxicity), soluble lead (indicative of slow leaching of lead), and particulate lead (indicative of sporadic flaking of lead), which thus can not only alert customers to the lead hazard in their drinking water but also enable customers to identify possible causes and most effective solutions to mitigate the lead contamination. Therefore, the project will result in not only considerable economic impact but also immense societal impact. The regular use of NAFX handheld tester - even if intermittently - will virtually eliminate the chance of chronic exposure to undetected lead, thereby accruing significant and predictable public health impact, especially in locations with the highest risk. PROJECT NARRATIVE The NanoAffix Phase II project aims to continue the development of a handheld lead tester for accurate and low- cost onsite detection of total lead in tap water by untrained users, based on the success of the Phase I project. The project will contribute to enhancing the public health by offering an accessible tool for quantitative monitoring of all three types of lead: total lead (indicative of overall toxicity), soluble lead (indicative of slow leaching of lead), and particulate lead (indicative of sporadic flaking of lead) in tap water. The regular use of NanoAffix handheld tester - even if intermittently - will virtually eliminate the chance of chronic exposure to undetected lead, thereby accruing significant and predictable public health impact, especially in locations with the highest risk.","Graphene-based Nanosensor Device for Rapid, Onsite Detection of Total Lead in Tap Water",10024064,R44ES028656,"['Address', 'Algorithms', 'Calibration', 'Cations', 'Cells', 'Chemistry', 'Child', 'Chronic', 'Communication', 'Communities', 'Complex', 'Contracts', 'Data', 'Data Analytics', 'Detection', 'Development', 'Devices', 'Digestion', 'Disinfection', 'Economically Deprived Population', 'Equilibrium', 'Equipment', 'Exposure to', 'Goals', 'Gold', 'Health', 'International', 'Interview', 'Kinetics', 'Laboratories', 'Lead', 'Lead Poisoning', 'Location', 'Machine Learning', 'Measurement', 'Michigan', 'Modeling', 'Monitor', 'Nervous System Trauma', 'Paper', 'Particulate', 'Performance', 'Phase', 'Procedures', 'Process', 'Public Health', 'Quality Control', 'Reporting', 'Research', 'Schools', 'Science', 'Site', 'Societies', 'Specialist', 'System', 'Test Result', 'Testing', 'Time', 'Toxic effect', 'Training', 'Uncertainty', 'Validation', 'Variant', 'Water', 'Water Supply', 'Wireless Technology', 'aqueous', 'base', 'cost', 'digital', 'drinking water', 'economic impact', 'empowered', 'graphene', 'hazard', 'high risk', 'improved', 'innovation', 'lead concentration', 'lead contamination', 'manufacturing scale-up', 'meetings', 'meter', 'nanosensors', 'novel', 'operation', 'portability', 'prototype', 'rapid detection', 'real time monitoring', 'response', 'sample collection', 'sensor', 'service providers', 'success', 'tool', 'virtual', 'water quality', 'well water']",NIEHS,"NANOAFFIX SCIENCE, LLC",R44,2020,719088,0.011070473748861831
"Clinical Evaluation of Burns using Spatial Frequency Domain Imaging Program Director/Principal Investigator (Last, First, Middle): Durkin, Anthony J. Abstract The central aim of this 3 year competing R01 renewal is to characterize and apply a new, compact, clinic- friendly Spatial Frequency Domain Imaging (SFDI) device to objectively and non-invasively classify burn severity (burn grade) over a large areas of skin. Delays in determining burn severity directly impacts patient treatment plans (including decisions whether to graft), rates of infection and scarring, duration of hospitalization and ultimately cost of care. Currently, the primary method of determining burn severity continues to be clinical assessment, which is highly subjective. While both superficial thickness and full-thickness burns are typically readily diagnosed based on visual clinical impression, partial thickness burns are difficult to classify and carry with them considerable potential for complications. Burn severity classification accuracy, even by experts, is only 60–80%. Our research in animal models demonstrates that SFDI data can successfully be used to classify different regions of burn severities. Typically, these differences are not apparent to the unaided eye and a great deal of training and experience is required in order for clinicians to accurately differentiate them Our work using a research grade, hybrid-SFDI device suggests that objective parameters provided by SFDI can be used within 24 hours after injury, to accurately classify burn severity. Specifically, we have demonstrated in a porcine burn model that the research grade SFDI outperforms laser speckle imaging and thermal imaging at 24 hours post-burn, in terms of predicting whether a burn will require a graft or not. However, translating these results to the clinic has been difficult due to several device limitations. The research grade SFDI device has slow acquisition times that can result in motion artifacts. It is also sensitive to ambient light which is often an issue in a clinical setting. Additionally, the SFDI device generates so much diverse data (oxygenated and deoxygenated hemoglobin, water fraction, reduced scattering coefficients at multiple wavelengths), there is no obvious way to present it to a clinical user to make a quick decision. To this end, we propose to methodically investigate an improved next generation SFDI device that addresses these issues by using brighter LEDs and fewer wavelengths to rapidly collect data in a way that reduces motion artifacts and is independent of clinical lighting conditions. In addition, we will develop a machine learning based classification framework that will provide the clinical with actionalble diagnostic information. The central aim of this 3 year competing R01 renewal is to characterize and then modify a new clinic-friendly SFDI device (Clarifi) to objectively classify in- vivo regions of different burn severity over large areas. The proposed research seeks to investigate this via the following Specific Aims: 1) Test & Validate Clinical SFDI Instrument, 2) Compare Clinical SFDI Instrument to other Modalities on a Long Term Swine Model of Graded Burns, 3) Develop Spatially Resolved Classification Maps of Burn Severity based on SFDI Data, 4) Conduct Clinical Measurements of Burn Severity using the new SFDI device and Spatially Resolved Burn Severity Classification Maps based on SFDI data. Program Director (Last, first, middle): Durkin, Anthony J. PROJECT NARRATIVE Burn injuries rank in the top 15 causes of global burden of disease. Burn severity assessment, which is a critical step in treatment planning, is subjective, depending on the experience of the treating physician. This leads to misdiagnosis and increased days of hospitalization and cost. In order to address this, we propose to test, validate and apply a novel optical imaging device in order to provide noninvasive objective assessment of burn wound severity. This has the potential to improve management of burn patients and reduce rates of complications.",Clinical Evaluation of Burns using Spatial Frequency Domain Imaging,10052657,R01GM108634,"['Address', 'Animal Model', 'Area', 'Biometry', 'Blood Vessels', 'Burn Centers', 'Burn injury', 'Cicatrix', 'Classification', 'Clinic', 'Clinical', 'Clinical assessments', 'Collaborations', 'Custom', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Enrollment', 'Eye', 'Family suidae', 'Female', 'Hemoglobin', 'Hospital Costs', 'Hospitalization', 'Hour', 'Hybrids', 'Image', 'Imaging Device', 'Injury', 'Laser Speckle Imaging', 'Lasers', 'Light', 'Lighting', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Medical center', 'Methods', 'Modality', 'Modeling', 'Morphologic artifacts', 'Motion', 'Noise', 'Optics', 'Output', 'Patients', 'Physicians', 'Principal Investigator', 'Property', 'Reporting', 'Research', 'Severities', 'Side', 'Signal Transduction', 'Skin', 'Spatial Frequency Domain Imaging', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Tissues', 'Training', 'Translating', 'Ulcer', 'Variant', 'Visual', 'Water', 'Work', 'base', 'burden of illness', 'burn model', 'burn wound', 'care costs', 'clinical imaging', 'cost', 'data acquisition', 'data integrity', 'data modeling', 'diverse data', 'experience', 'healing', 'human data', 'imaging system', 'impression', 'improved', 'in vivo', 'infection rate', 'male', 'next generation', 'novel', 'optical imaging', 'pre-clinical', 'programs', 'research clinical testing', 'stability testing', 'treatment planning']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2020,430325,0.006776060913818049
"An Unobtrusive Continuous Cuff-less Blood Pressure Monitor for Nocturnal Hypertension PROJECT SUMMARY/ABSTRACT The objective of this project is to create an unobtrusive, wrist-worn, cuff-less blood pressure monitor for measurement and identification of nocturnal nondipping hypertension. The investigation includes extensive validation with state-of-the-art ambulatory blood pressure monitors at nighttime in presence of heterogeneous treatment paradigms. Cardiovascular disease (CVD) is one of the major causes of ailments worldwide. Hypertension alone affects one in three adults according to the World Health Organization. Therefore, monitoring blood pressure has become a critical part of healthcare as it is known to be linked to many CVDs. Traditionally, clinical practitioners have relied on the mercury-based (or digital equivalent) inflatable cuff-based sphygmomanometer. However, the nature of the device allows for only infrequent measurements and its somewhat invasive nature and associated discomfort prohibits additional nocturnal measurements. There is certainly a value to measuring blood pressure continuously in the natural context of the user’s environment, in particular during sleep, without being disturbed by the instrument. Our proposed technology can provide a wealth of information to physicians, help identify certain short-term dynamics/variations of blood pressure, and allow effective monitoring of response to medication, among other things. Nocturnal measurements provide additional prognostic value in identifying risk. Despite these benefits, no wearable, non-invasive device for continuous blood pressure monitoring exists on the market simply because none have been reliable enough to be considered clinical grade. This project aims to develop a robust and reliable blood pressure monitor in the form of a wrist-worn device that uses bio-impedance sensors, and for the first time, demonstrate clinical grade reliability. These sensors measure pulse wave velocity (PWV) along with several other derivatives for cardiovascular parameters including heart rate and blood volume changes in arteries, which correlate with the blood pressure. The system will incorporate clever hardware design to localize underlying vasculature and focus on arterial sites for enhanced accuracy. The device will include a motion sensor to take into account the user’s movements and motion artifacts, the contact quality, and reliability of the measurements. Advanced machine learning techniques, leveraging both general and personalized models, will be developed to convert bio-impedance measurements to blood pressure. This project aims to then validate the system and analytics in both a healthy patient cohort and a hypertensive cohort, learning the impact that nocturnal ‘nondipping’ hypertension and anti-hypertensive treatments have on PWV/other cardiovascular correlates and blood pressure estimates. After decades of relying on the inflatable cuff- based technique, this system could represent a significant change in how we measure blood pressure. PROJECT NARRATIVE Continuous monitoring of nocturnal blood pressure can help early diagnosis of developing cardiac conditions, reveal short term blood pressure variations, and also help the physician monitor differences in variations in response to medication for hypertensive patients. Moreover, the comfort and convenience of a wearable monitor would allow measurement in the natural context of daily life, including important nocturnal measurements, and reduce the burden of adherence on the user. The system will also provide feedback on quality of measurements to allow the users or care-givers to gauge reliability.",An Unobtrusive Continuous Cuff-less Blood Pressure Monitor for Nocturnal Hypertension,9998433,R01HL151240,"['Adherence', 'Adult', 'Affect', 'Age', 'Ambulatory Blood Pressure Monitoring', 'Antihypertensive Agents', 'Arteries', 'Awareness', 'Biometry', 'Blood Pressure', 'Blood Pressure Monitors', 'Blood Volume', 'Blood flow', 'Calibration', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular system', 'Caregivers', 'Characteristics', 'Clinical', 'Data', 'Data Collection', 'Development', 'Devices', 'Early Diagnosis', 'Environment', 'FDA approved', 'Feedback', 'Future', 'Gold', 'Healthcare', 'Heart Rate', 'Home environment', 'Hour', 'Human', 'Hypertension', 'Investigation', 'Learning', 'Legal patent', 'Life', 'Link', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Mercury', 'Methods', 'Microfabrication', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Motion', 'Movement', 'Nature', 'Organ', 'Outcome', 'Outcomes Research', 'Participant', 'Patient Monitoring', 'Patient risk', 'Patients', 'Pattern', 'Penetration', 'Pharmaceutical Preparations', 'Physicians', 'Physiologic pulse', 'Physiology', 'Positioning Attribute', 'Proxy', 'Reading', 'Recording of previous events', 'Regimen', 'Research', 'Risk', 'Risk Factors', 'Science', 'Signal Transduction', 'Site', 'Skin', 'Sleep', 'Sleep Deprivation', 'Sphygmomanometers', 'Structural Models', 'Supine Position', 'System', 'Techniques', 'Technology', 'Time', 'Uncertainty', 'Validation', 'Variant', 'Work', 'World Health Organization', 'Wrist', 'advanced analytics', 'analytical method', 'arterial stiffness', 'base', 'cohort', 'comorbidity', 'design', 'digital', 'effectiveness validation', 'electric impedance', 'insight', 'instrument', 'model development', 'monitoring device', 'motion sensor', 'multidisciplinary', 'novel', 'novel strategies', 'patient stratification', 'patient subsets', 'performance tests', 'prognostic value', 'response', 'sensor', 'sex', 'sleep position', 'supine sleep', 'wearable device', 'wearable sensor technology', 'willingness']",NHLBI,TEXAS ENGINEERING EXPERIMENT STATION,R01,2020,766675,0.003854116083652537
"An unobtrusive monitoring device used for tracking asthma symptoms and lungfunction variability Executive Summary of Predicate (One Page) Summary of Specific Aims of Phase I Specific Aim 1: Train and evaluate an algorithm to detect pediatric asthma symptoms (cough and wheeze) on a low power, small form factor wearable device. Specifications: 90% sensitivity; false alarm rate: 1 cough episode/day or 1 wheeze episode/day. Evaluate algorithm against medical expert (physician) scoring using the two best available asthma scoring tools (AS: asthma score; PRAM: Pediatric Respiratory Assessment Measure). Specific Aim 2: Design and evaluate algorithm to detect lung function variability on a low power, small form factor wearable device. Specifications: Using respiratory signals from sensor patch, detect variations ≥ 10% in forced expiratory volume in 1 second to forced vital capacity (FEV1/FVC). Evaluate algorithm against spirometry gold standard. Progress towards Specific Aims As of today, January 21, 2020, the work on the predicate NIH STTR award has not yet begun. The work related to this NIH STTR is anticipated to begin April 2020. Prior to submitting our NIH STTR application, significant work was completed to test the viability of collecting lung function using our wearable technology. We tested on over 20 patients in the hospital, and in this study we found a positive correlation between our measurements and those of spirometry. Our NIH STTR work will build upon this. Technical, administrative, or commercial challenges and how they’ve been addressed During our customer discovery so far, we have learned about the complexities of achieving reimbursement, even if we have identified applicable CPT codes. Realizing this challenge, we selected an initial customer that will not require reimbursement. Our initial customer will be respiratory clinical trials. From our interviews, we have learned that they have a strong unmet need, they are willing to pay a large amount, and we will be able to serve them earlier than other customer types. We hope to explore this further during this NIH I-Corps program. Brief intro to team members Principal Investigator: Justice Amoh, PhD, CTO of Clairways - Justice is a pioneer in embedded systems for stochastic modelling of physiological signals. His focus is on deep neural network models for detecting the onset of symptoms in respiratory diseases. C-Level Corporate Officer: Jeff Bemowski, MBA, CEO of Clairways - Jeff is experienced in product management, market research, and customer discovery for novel biomedical devices. He previously worked in product management for Endotronix, a Series C funded medical device company. Industry Expert: Bob Gatewood, VP Digital Health at Portal Instruments - Bob Gatewood is an experienced healthcare and digital health entrepreneur. Bob was one of the founding members of Athenahealth, which is now valued at over $5.5 billion. He has already worked through many of the challenges Clairways will have to overcome, so his perspective will be very valuable to our team. Additionally, Bob is well connected within the industry and will aid in connecting with 100 potential customers within the short timeline. n/a",An unobtrusive monitoring device used for tracking asthma symptoms and lungfunction variability,10087375,R41HL146027,"['Address', 'Algorithms', 'Asthma', 'Award', 'Childhood', 'Childhood Asthma', 'Clinical Trials', 'Coughing', 'Current Procedural Terminology Codes', 'Devices', 'Doctor of Philosophy', 'Funding', 'Gold', 'Health', 'Healthcare', 'Hospitals', 'Industry', 'Innovation Corps', 'Interview', 'Justice', 'Lung diseases', 'Market Research', 'Measurement', 'Measures', 'Medical', 'Medical Device', 'Neural Network Simulation', 'Patients', 'Phase', 'Physicians', 'Principal Investigator', 'Pulmonary Function Test/Forced Expiratory Volume 1', 'Respiratory physiology', 'Series', 'Signal Transduction', 'Small Business Technology Transfer Research', 'Spirometry', 'Symptoms', 'System', 'Testing', 'TimeLine', 'Training', 'United States National Institutes of Health', 'Variant', 'Vital capacity', 'Wheezing', 'Work', 'deep neural network', 'design', 'digital', 'experience', 'instrument', 'member', 'monitoring device', 'novel', 'physiologic model', 'programs', 'respiratory', 'sensor', 'tool', 'wearable device']",NHLBI,"CLAIRWAYS, LLC",R41,2020,55000,-0.1079356098653023
SCH: INT: A Context-aware Cuff-less Wearable Ambulatory Blood Pressure Monitor using a Bio-Impedance Sensor Array No abstract provided n/a,SCH: INT: A Context-aware Cuff-less Wearable Ambulatory Blood Pressure Monitor using a Bio-Impedance Sensor Array,9982327,R01EB028106,"['Address', 'Aging', 'Algorithms', 'Ambulatory Blood Pressure Monitoring', 'American', 'American Heart Association', 'Arteries', 'Awareness', 'Blood Pressure', 'Blood Pressure Monitors', 'Blood Vessels', 'Calibration', 'Cardiology', 'Cardiovascular Diseases', 'Cardiovascular system', 'Caring', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Trials', 'Data', 'Devices', 'Diagnosis', 'Disease', 'Disease Management', 'Environment', 'Frequencies', 'Funding', 'Guidelines', 'Hour', 'Human Resources', 'Hypertension', 'Institutes', 'Institution', 'International', 'Intervention Trial', 'Investigation', 'Laboratories', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Patients', 'Pattern', 'Phenotype', 'Physiologic pulse', 'Play', 'Positioning Attribute', 'Posture', 'Reading', 'Research', 'Risk', 'Risk Factors', 'Role', 'Skin', 'Source', 'System', 'Technology', 'Testing', 'Texas', 'Time', 'Tissues', 'United States National Institutes of Health', 'Universities', 'Validation', 'Wearable Computer', 'Wrist', 'advanced analytics', 'base', 'clinical practice', 'cohesion', 'college', 'cost', 'design', 'electric impedance', 'health disparity', 'machine learning method', 'minority health', 'novel strategies', 'patient population', 'sensor', 'signal processing', 'therapy development', 'validation studies', 'wearable sensor technology']",NIBIB,TEXAS ENGINEERING EXPERIMENT STATION,R01,2020,289073,0.015609540521454544
"Automating and Obtaining FDA Approval for a Digital Intervention for Depression Phase IIB Abstract -FINAL A large body of evidence supports the idea that disruptions in the regularity of behavioral routines (or ‘social rhythms’) can lead to the onset of mood symptoms and full-blown episodes via their impact on endogenous circadian rhythms and, conversely, that stable social routines protect against new mood episodes. Based on our conviction that monitoring, evaluating, and increasing the regularity of behavioral rhythms has broad applicability for improving mental health, we established HealthRhythms, Inc. (www.healthrhythms.com) with the goal of using mobile technology to enhance our capacity to monitor and treat such conditions, with a particular emphasis on mood disorders. Our product vision capitalizes on the ubiquity and intimacy of smartphones and their capacity to automatically and continuously sense parameters of behavior that represent key indicators of depressive symptomatology captured on a 24/7 basis in the user’s natural environment. It also capitalizes on the capacity of smartphones to deliver interventions that are easily accessible, can be delivered with optimal timing and, perhaps most important, are enormously scalable. Finally, smartphone technology gives us the capacity to bring empirically validated psychosocial interventions to the hundreds of thousands of depressed patients who otherwise would not be able to access such interventions and, thus, speed their recovery and prevent relapse. Under the auspices of our Phase II SBIR funding (R44MH113520-01-02), we have developed both such a monitoring product (Measure) and have developed and tested such a digital intervention platform (Cue). Analyses conducted to date indicate that Cue leads to significantly greater decreases in depressive symptomatology than monitoring alone via Measure. We now propose to: 1) upgrade to Cue and its accompanying clinician dashboard to be commercially marketable and 2) carry out the clinical trial necessary for FDA licensing of Cue 2.0 as a mobile device. Cue 2.0 will include enhanced designs, more in-depth psychoeducational material, and a fully automated engine for delivering behavior change suggestions to improve the regularity of users’ social and behavioral routines. Building on the accomplishments of our Phase II funding, this SBIR Phase IIB proposal is focused on bringing Cue to market as a scientifically validated and profitable digital intervention for individuals with depression with or without co-occurring anxiety disorder. The Specific Aims are: Aim 1: Develop and validate Cue 2.0 a fully automated digital intervention platform. Aim 2: Carry out all necessary steps to apply for FDA licensing of Cue 2.0.  PROJECT NARRATIVE HealthRhythms, Inc. is on a mission to become the world leader in tracking clinically meaningful behavioral rhythms in order to provide automated behavior change suggestions to patients around the globe. The work we propose in this SBIR Phase IIB application will enable HealthRhythms to further develop Cue, our smartphone-based intervention platform for depression, and carry out the work necessary to obtain FDA clearance for Cue as a medical device. This will enable us to bring a tested psychosocial treatment to the hundreds of thousands of depressed patients who otherwise would not be able to access such treatment and, thus, speed their recovery and prevent relapse.",Automating and Obtaining FDA Approval for a Digital Intervention for Depression,10081882,R44MH113520,"['Anxiety', 'Anxiety Disorders', 'Applications Grants', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Biological Markers', 'Cellular Phone', 'Centers for Disease Control and Prevention (U.S.)', 'Circadian Rhythms', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Consultations', 'Cues', 'Data', 'Environment', 'FDA approved', 'Feedback', 'Funding', 'Funding Opportunities', 'Generations', 'Goals', 'Grant', 'Human', 'Individual', 'Intervention', 'Lead', 'Licensing', 'Link', 'Marketing', 'Measures', 'Medical Device', 'Mental Depression', 'Mental Health', 'Methods', 'Mission', 'Modeling', 'Modification', 'Monitor', 'Mood Disorders', 'Moods', 'National Institute of Mental Health', 'Parents', 'Patients', 'Pharmacologic Substance', 'Pharmacotherapy', 'Phase', 'Process', 'Reading', 'Recovery', 'Relapse', 'Risk', 'Small Business Innovation Research Grant', 'Small Business Technology Transfer Research', 'Speed', 'Suggestion', 'Technology', 'Testing', 'Theoretical model', 'United States National Institutes of Health', 'Vision', 'Work', 'base', 'behavior change', 'convict', 'dashboard', 'depressed patient', 'depressive symptoms', 'design', 'digital', 'handheld mobile device', 'improved', 'intimate behavior', 'meetings', 'mobile application', 'mobile computing', 'mood symptom', 'prevent', 'psychoeducational', 'psychosocial', 'social', 'symptomatology', 'usability']",NIMH,"HEALTH RHYTHMS, INC.",R44,2020,974672,-0.009079803591575395
"Gaze-contingent computer screen magnification control for people with low vision ! Project Summary This application describes proposed research with the goal of facilitating use of a computer screen magnifier by people with low vision. Screen magnification is a well-established, popular technology for access of onscreen content. Its main shortcoming is that it requires the user to continuously control, with the mouse or trackpad, the location of the focus of magnification, in order to ensure that the magnified content of interest is within the screen viewport. This tedious process may be time-consuming and ineffective. For example, the simple task of reading the news on a web site requires continuous horizontal scrolling, which affects the experience of using this otherwise very beneficial technology, and may discourage its use, especially by those with poor manual coordination.  We propose to develop a software system that enables hands-free control of a screen magnifier. This system will rely on the user’s eye gaze (measured by a regular IR-based tracker, or from analysis of the images in a camera embedded in the screen) to update the location of the focus of magnification as desired. This research is inspired by preliminary work, which showed promising results with two simple gaze-based control algorithms, tested on three individuals with low vision.  This project will be a collaboration between the Department of Computer Science and Engineering at UC Santa Cruz (PI: Manduchi, Co-I: Prado) and the School of Optometry at UC Berkeley (PI: Chung). Dr. Legge from the Department of Psychology at U. Minnesota will participate as a consultant. Two human subjects studies are planned. In Study 1 with 80 low vision subjects from four different categories of visual impairment, we will investigate the failure rate of a commercial gaze tracker (Aim 1), and will record mouse tracks, gaze tracks, and images from the subjects while performing a number of tasks using two modalities of screen magnification (Aim 2). In Study 2, with the same number of subjects, we will repeat the Study 1 experiment, but using a gaze-based controller trained from the data collected in Study 1, and individually tunable for best performance (Aim 3). In addition, we will experiment with an appearance-based gaze tracker that uses images from the screen camera, thereby removing the need for specialized gaze tracking hardware, as well as with a computer tablet form factor (Aim 4). We expect that reading speed and error rate using our gaze-based controller will be no worse than using mouse-based control. If successful, this study will show that the convenience of hands-free control offered by the proposed system comes at no additional cost in terms of individual performance at the considered tasks. ! ! Project Narrative People with low vision often use screen magnification software to read on a computer screen. Since a magnifier expands the screen content beyond the physical size of the screen (the “viewport”), it is necessary to move the content using the mouse so that the portion of interest falls within the viewport. This project will facilitate use of a screen magnifier by means of a new software system that relies on the user’s own gaze to control scrolling when reading with magnification. !",Gaze-contingent computer screen magnification control for people with low vision,10053172,R01EY030952,"['Affect', 'Age', 'Algorithms', 'Appearance', 'Apple', 'Behavior Control', 'Benchmarking', 'Blindness', 'Categories', 'Collaborations', 'Communication', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Consumption', 'Correlation Studies', 'Data', 'Data Set', 'Desktop Video', 'Engineering', 'Ensure', 'Eye', 'Face', 'Failure', 'Funding', 'Glass', 'Goals', 'Hand', 'Image', 'Individual', 'Learning', 'Location', 'Magic', 'Manuals', 'Measures', 'Minnesota', 'Modality', 'Mus', 'Operating System', 'Optometry', 'Performance', 'Peripheral', 'Process', 'Psychological reinforcement', 'Psychology', 'Reader', 'Reading', 'Research', 'Resort', 'Role', 'Schools', 'Science', 'Speech', 'Speed', 'Structure', 'Study Subject', 'System', 'Tablet Computer', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Update', 'Vision', 'Visual', 'Visual impairment', 'Work', 'algorithm development', 'algorithm training', 'base', 'computer science', 'control trial', 'cost', 'data acquisition', 'design', 'experience', 'experimental study', 'falls', 'gaze', 'human subject', 'interest', 'motor control', 'news', 'recurrent neural network', 'sample fixation', 'software systems', 'tool', 'web page', 'web site']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R01,2020,350753,0.025697900918183584
"Environmental Localization Mapping and Guidance for Visual Prosthesis Users Project Summary About 1.3 million Americans aged 40 and older are legally blind, a majority because of diseases with onset later in life, such as glaucoma and age-related macular degeneration. Second Sight has developed the world's first FDA approved retinal implant, Argus II, intended to restore some functional vision for people suffering from retinitis pigmentosa (RP). In this era of smart devices, generic navigation technology, such as GPS mapping apps for smartphones, can provide directions to help guide a blind user from point A to point B. However, these navigational aids do little to enable blind users to form an egocentric understanding of the surroundings, are not suited to navigation indoors, and do nothing to assist in avoiding obstacles to mobility. The Argus II, on the other hand, provides blind users with a limited visual representation of their surroundings that improves users' ability to orient themselves and traverse obstacles, yet lacks features for high-level navigation and semantic interpretation of the surroundings. The proposed research aims to address these limitations of the Argus II through a synergy of state-of-the-art stimultaneous localization and mapping (SLAM) and object recognition technologies. For the past three years, JHU/APL has collaborated with Second Sight to develop similar advanced vision-based capabilities for the Argus II, including capabilities for object recognition and obstacle detection by stereo vision. This proposal is driven by the hypothesis that navigation for users of retinal prosthetics can be greatly improved by incorporating SLAM and object recognition technology conveying environmental information via a retinal prosthesis and auditory feedback. SLAM enables the visual prosthesis system to construct a map of the user's environment and locate the user within that map. The system then provides object location and navigational cues via appropriate sensory modalities enabling the user to mentally form an egocentric map of the environment. We propose to develop and test a visual prosthesis system which 1) constructs a map of unfamiliar environments and localizes the user using SLAM technology 2) automatically identifies navigationally-relevant objects and landmarks using object recognition and 3) provides sensory feedback for navigation, obstacle avoidance, and object/landmark identification. Project Narrative The proposed system, when realized, will use advanced simultaneous localization and mapping, and object recognition techniques, to enable visual prosthesis users with unprecedented abilities to autonomously navigate and identify objects/landmarks in unfamiliar environments.",Environmental Localization Mapping and Guidance for Visual Prosthesis Users,10019559,R01EY029741,"['3-Dimensional', 'Address', 'Age related macular degeneration', 'Algorithms', 'American', 'Competence', 'Complex', 'Computer Vision Systems', 'Cues', 'Data', 'Dependence', 'Detection', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Environment', 'Evaluation', 'FDA approved', 'Feedback', 'Glaucoma', 'Goals', 'Image', 'Implant', 'Late-Onset Disorder', 'Lead', 'Learning', 'Life', 'Location', 'Maps', 'Medical Device', 'Modality', 'Motion', 'Ocular Prosthesis', 'Patients', 'Performance', 'Psyche structure', 'Research', 'Retinitis Pigmentosa', 'Running', 'Semantics', 'Sensory', 'Societies', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Update', 'Vision', 'Visual', 'Volition', 'aged', 'auditory feedback', 'base', 'behavior test', 'blind', 'cognitive load', 'falls', 'human subject', 'improved', 'innovation', 'legally blind', 'navigation aid', 'object recognition', 'portability', 'prosthesis wearer', 'prototype', 'research and development', 'retina implantation', 'retinal prosthesis', 'sensory feedback', 'smartphone Application', 'synergism', 'visual feedback', 'visual information']",NEI,JOHNS HOPKINS UNIVERSITY,R01,2020,662134,0.02669711505429278
"Novel Approaches to Advance Coordinated Registry Networks (CRNs). Project Summary The technological transformation of US health care with the explosion of new devices and iterative changes mandates the acquisition of real-world evidence (RWE) to study devices and technologies pragmatically. The US Food and Drug Administration (FDA) has spearheaded the RWE framework development in the pursuit of sufficient evidence that is required for regulatory decision-making such as device approvals and surveillance. With regulatory support since the launch of the National Medical Device Registry Task Force in 2015, the Medical Device Epidemiology Network (MDEpiNet) created 15 national and international coordinated registry networks (CRNs), which develop or link well-curated national RWE sources such as registries, administrative, and electronic health records (EHRs) data. The MDEpiNet is a key partner of the National Evaluation System of Technologies (NEST) coordinating center and is an international public-private partnership focusing on building global infrastructure and methodologies to advance the use of RWE for medical device evaluation. CRNs not only focus on prevention of harms but also the promotion of safer device innovation through the development of study designs that expedites patient recruitment at lower costs than traditional clinical research. MDEpiNet developed a maturity model for CRNs with various levels of achievements in seven key domains: 1) device identification, 2) quality improvement, 3) total product life-cycle, 4) data quality, 5) efficiency, 6) governance and sustainability, 7) patient engagement. This proposal focuses on the creation of innovative tools and methods necessary to achieve maturation of the networks through efficient curation of robust RWE. We will capitalize on established partnerships with registries, professional societies, integrated health systems, and many academic institutions to advance this critical national infrastructure as a foundational component of NEST. We will facilitate advancements of RWE through stakeholder roundtables, patient-facing mobile app development, and continued innovative methods development to link registries with Medicare, commercial, statewide, and EHR data to enable better research and surveillance for devices. Our specific aims facilitate stakeholder engagement for device-specific core minimum data development in women's health, prostate cancer, orthopedics, vascular disease, robot-assisted surgery, and temporomandibular joints. We will also advance and enrich linked data capacities in vascular disease, hernia repair, breast implant, prostate cancer, Women's Health, and gastrointestinal cancer CRNs. Finally, we conduct advanced analytics to determine gender disparities in device outcomes and use machine learning and active surveillance methods in hernia repair, orthopedics, stroke treatment, vascular disease, and Women's health CRNs. The CRN community of practice will enable centralized knowledge sharing to support cross-specialty and technology learning and applications. Through this, we advance the CRNs using innovative, scalable, and dynamic approaches and help them become foundational components of NEST. Narrative Medical Device Epidemiology Network will advance the research and surveillance capabilities of coordinated registry networks through stakeholder roundtables, patient-facing mobile app development, and linkages of real-world data sources. We will also conduct advanced analytics to determine gender disparities, use machine learning for risk predictions, and implement active surveillance for devices and technologies.",Novel Approaches to Advance Coordinated Registry Networks (CRNs).,10128755,U01FD006936,[' '],FDA,WEILL MEDICAL COLL OF CORNELL UNIV,U01,2020,1770000,-0.024509933761910806
"Vision in Natural Tasks Summary/Abstract  In the context of natural behavior, humans make continuous sequences of sensory-motor decisions to satisfy current behavioral goals, and vision must provide the information needed to achieve those goals. The proposed work examines gaze and walking decisions in locomotion in outdoor environments, taking advantage of our novel system for measuring combined eye and body movements in these contexts. Currently we have only limited understanding of the constituent tasks in natural locomotion, or the requisite information, and the proposal attempts to specify these.  in the context of natural gait, the patterns of optic flow are unexpectedly complex, raising questions about its role. The patterns of motion on the retina during locomotion depend critically on both eye and body motion, and these in turn depend on behavioral goals. Our first Aim is therefore to comprehensively describe the statistics of retinal motion patterns in a variety of terrains and task contexts. We will measure binocular eye and body movements while walking in outdoor terrains of varying roughness, crossing a busy intersection, and making coffee. These contexts will induce different gaze patterns. We will provide a comprehensive description of the motion stimulus in natural locomotion and help separate out self-motion signals from externally generated motion. These data will allow a more precise specification of the response patterns in cortical motion sensitive areas. Because of the complexity of natural motion patterns, we will re-examine the influence of optic flow on walking direction in a virtual reality environment and test alternative explanations for the role of flow.  A central task in walking is foot placement, and we will focus on identifying the image properties that make a good foothold. Stereo, structure from motion, and spatial image structure are all likely contenders. We directly investigate the role of stereo in foothold selection by examining gait patterns in stereo-deficient subjects in terrains with varying degrees of roughness. Using a different strategy, we will attempt to predict gaze locations and footholds in rough terrain using convolution neural nets (CNN’s) to identify potential search templates for footholds in rough terrain. We will describe fixation patterns from crosswalk and sidewalk navigation and attempt to make inferences about their purpose, and use Modular Inverse Reinforcement Learning (MIRL) to predict direction decisions and decompose the behavior into sub-tasks.  The collection of integrated gaze, body kinematics, and scene images in a range of natural environments is innovative, as little comparable data exists The work will be strengthened by the investigation of stereo- deficient subjects for whom there is almost no integrated eye and body data. Since much of the work in robotics has no visual input at all this should help in development of visual guidance for robots and also help better define the necessary information for individuals with impaired vision. The data set will be made publicly available. Project Narrative  The central goal of this work is to understand vision in its natural context. This is very important information in order to devise suitable vision aids and rehabilitation strategies for individuals with visual impairments, and it is becoming increasingly accessible because of developments in technology for monitoring eye and body movements. The proposed work examines gaze and walking decisions in locomotion in outdoor environments, taking advantage of our novel system for measuring combined eye and body movements in these contexts. Currently we have only limited understanding of the constituent tasks and requisite information in natural locomotion, and the proposal attempts to specify these. The collection of integrated gaze, body kinematics, and scene images in a range of natural environments is innovative, as little comparable data exists. The work will be strengthened by the investigation of stereo-deficient subjects for whom there is almost no integrated eye and body data. Since much of the work in robotics has no visual input at all this should help in development of visual guidance for robots and also help better define the necessary information for individuals with impaired vision. The data set will be made publicly available.",Vision in Natural Tasks,10004035,R01EY005729,"['Affect', 'Area', 'Behavior', 'Behavioral', 'Binocular Vision', 'Cells', 'Characteristics', 'Coffee', 'Collection', 'Complex', 'Cues', 'Data', 'Data Set', 'Development', 'Distant', 'Environment', 'Eye', 'Eye Movements', 'Gait', 'Goals', 'Grant', 'Head', 'Human', 'Image', 'Individual', 'Investigation', 'Knowledge', 'Learning', 'Link', 'Location', 'Locomotion', 'Machine Learning', 'Measures', 'Monitor', 'Motion', 'Motor', 'Movement', 'Pattern', 'Psychological reinforcement', 'Retina', 'Rewards', 'Robot', 'Robotics', 'Role', 'Sampling', 'Seminal', 'Sensory', 'Signal Transduction', 'Speed', 'Stimulus', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'To specify', 'Uncertainty', 'Vision', 'Visit', 'Visual', 'Visual Fields', 'Visual impairment', 'Walkers', 'Walking', 'Work', 'base', 'convolutional neural network', 'cost', 'experimental study', 'foot', 'gaze', 'imaging properties', 'innovation', 'kinematics', 'novel', 'optic flow', 'rehabilitation strategy', 'response', 'sample fixation', 'statistics', 'virtual reality environment', 'vision aid', 'vision development', 'vision rehabilitation', 'visual information']",NEI,"UNIVERSITY OF TEXAS, AUSTIN",R01,2020,381743,-0.0037439143325081795
"Development/Commercialization of a Sensing Device to Detect Vaping Development/Commercialization of a Sensing Device to Detect Vaping Summary The use of e-cigarettes or vaping has been steadily increasing since its introduction. While potentially a tool to wean cigarette smokers from combustible tobacco, one consequence of the introduction of these devices has been the adoption of vaping by adolescents. While companies that offer vaping instruments for sale note that their material is directed to adults and intended as an aid for smoking cessation, recent reports have demonstrated that middle school and high school students in many countries, some as young as thirteen, have taken to vaping. Data analysis from a 2015 study in the U.S. indicated that 16% of high school students and 5% of middle school students reported vaping in the past thirty days. Most researchers speculated that the number of users would increase from these baselines and evidence indicates that this prediction is correct. Anecdotal evidence indicates that vaping in middle school and high school bathrooms is a major problem. FreshAir Sensor currently sells tobacco and marijuana smoking sensors along with 24/7 monitoring of the devices. The company has leveraged the knowledge of sensor development to produce preliminary components of an early stage sensing system capable of detecting vaping. Preliminary data to demonstrate this accomplishment is provided. The fast track research described in this proposal will enable the optimization of the sensor as well as commercialization of the resulting instrument in minimal time. The need to reduce and eventually eliminate adolescent vaping is urgent. The deployment of the proposed device in schools and other educational institutions will eliminate vaping during school hours and will, therefore, contribute to improvements in the overall health of adolescents by curtailing nicotine intake. Narrative Vaping has become a problem in schools with students, in steadily increasing numbers, using bathrooms and other less monitored spaces to indulge in the use of the newest vaping hardware. FreshAir Sensor is developing a sensor to detect vaping in otherwise unmonitored spaces. The use of this sensing system has the potential to reduce and, eventually, eliminate vaping behavior in schools, thereby reducing the harmful effects of nicotine in adolescents.",Development/Commercialization of a Sensing Device to Detect Vaping,10092402,R44DA049595,"['Adolescent', 'Adoption', 'Adult', 'Air', 'Algorithms', 'Behavior', 'Chemicals', 'Cigarette Smoker', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Dose', 'Effectiveness', 'Electronic cigarette', 'Electronics', 'Engineering', 'Environmental Risk Factor', 'Event', 'Exposure to', 'Fatigue', 'Film', 'Goals', 'High School Student', 'Hour', 'Humidity', 'Institution', 'Intake', 'JUUL', 'Knowledge', 'Laboratories', 'Longevity', 'Marijuana', 'Marijuana Smoking', 'Methods', 'Middle School Student', 'Minor', 'Modality', 'Monitor', 'Morphology', 'Neurotoxins', 'Nicotine', 'Phase', 'Polymers', 'Production', 'Property', 'Public Housing', 'Reporting', 'Research', 'Research Personnel', 'Sales', 'Schools', 'Science', 'Smoking', 'Specificity', 'Students', 'System', 'Temperature', 'Testing', 'Time', 'Tobacco', 'Tobacco smoking behavior', 'Weaning', 'adolescent health', 'base', 'commercialization', 'design', 'detector', 'electronic cigarette use', 'high school', 'instrument', 'junior high school', 'machine learning algorithm', 'monitoring device', 'prototype', 'research and development', 'response', 'sensor', 'sensor technology', 'smoking cessation', 'tool', 'vaping', 'vapor']",NIDA,FRESHAIR SENSOR CORPORATION,R44,2020,830874,0.01018520045033583
"Eliminating the human factor from stereotaxic surgeries Project Summary: The main goal of this research project is to develop a new line of new stereotaxic devices for small animal research that outperforms existing devices in terms of accuracy, reproducibility, and ease of use. Advancing a tool such as an electrode, injection pipette or optical fiber through a small hole in the cranium, sometimes over long distances, and placing it precisely in a particular brain area, often much less than one millimeter in diameter, is a significant experimental challenge. Any time an investigator misses the target brain area and the experiment fails as a result, a significant amount of work is lost, additional animals get sacrificed, materials are wasted, and the pace of scientific discovery has been slowed. Even in cases when experiments succeed, they can be difficult to reproduce because many research groups rely on their most experienced lab members and their “special touch” to perform these procedures – thereby adding an element of non- quantitativeness to the procedures, effectively making the experiment less reproducible. We propose to develop a novel stereotaxic apparatus which will overcome many of these shortcomings. Our device features a radically different mechanical design which is natively compatible with both traditional and novel in-vivo techniques. We propose to combine computer 3D vision and robotics for automatic and software guided adjustments of the animal's skull. Landmarks are measured with 3D vision, based on structured illumination at a level of accuracy that has not been accomplished by any of the existing devices. This information will guide a robotic platform to position the animal for the experiment. Finally, we propose to develop an open software platform for neuronavigation that will allow investigators to use the platform with any small animal species they desire to use. Brain atlas systems for neuronavigation can either be downloaded from a cloud based site, or produced de-novo by the investigator by preparing a single set of MRI and CT scans from one sample animal. Our device will help make stereotaxic procedures more accurate and less dependent on human input and thereby increase the repeatability of experiments within a laboratory as well as the reproducibility of experiments across laboratories. Narrative: The main goal of this research project is to develop a new line of new stereotaxic devices for small animal research that outperforms existing devices in terms of accuracy, reproducibility, and ease of use. These devices will help make stereotaxic procedures less dependent on human input and thereby increase the repeatability of experiments within a laboratory as well as the reproducibility of experiments across laboratories. Most importantly, they will help reduce or eliminate failed experiments due to mistargeted interventions, thereby accelerating the pace of scientific discovery.",Eliminating the human factor from stereotaxic surgeries,10080673,R41NS119079,"['3-Dimensional', 'Animal Experimentation', 'Animal Experiments', 'Animals', 'Area', 'Atlases', 'Base of the Brain', 'Brain', 'Caliber', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Databases', 'Devices', 'Dorsal', 'Electrodes', 'Elements', 'Ensure', 'Frustration', 'Goals', 'Human', 'Image', 'Injections', 'Intervention', 'Laboratories', 'Lighting', 'Location', 'MRI Scans', 'Magnetic Resonance Imaging', 'Manuals', 'Measurement', 'Measures', 'Mechanics', 'Monitor', 'Neuronavigation', 'Operative Surgical Procedures', 'Persons', 'Positioning Attribute', 'Procedures', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Robotics', 'Sampling', 'Savings', 'Scanning', 'Side', 'Site', 'Speed', 'Structure', 'Surgical sutures', 'System', 'Techniques', 'Technology', 'Time', 'Touch sensation', 'Translations', 'Vision', 'Work', 'X-Ray Computed Tomography', 'age group', 'base', 'bone', 'bone imaging', 'brain tissue', 'cloud based', 'cost effective', 'cranium', 'design', 'experimental study', 'genetic strain', 'hexapod', 'in vivo', 'laboratory experience', 'member', 'millimeter', 'novel', 'operation', 'optical fiber', 'programs', 'prototype', 'soft tissue', 'software development', 'tool', 'virtual', 'wasting']",NINDS,POPNEURON LTD.,R41,2020,251960,0.014434208212742236
"Developing and Evaluating In-Home Supportive Technology for Dementia Caregivers Abstract  Caring for a loved one with Alzheimer’s disease, frontotemporal dementia, or another neurodegenerative disease is a highly meaningful part of family life. However, the associated burden and strain can have adverse effects on caregivers including mental and physical health problems, reduced well-being, and increased mortality. These effects, in turn, can compromise care quality and shorten survival times for people with dementia (PWD). Research has consistently found that behavioral symptoms in PWD are most strongly associated with adverse caregiver effects, even more so than cognitive and functional symptoms. Empirically- supported interventions are needed that: (a) target mechanisms/pathways shown to connect behavioral symptoms in PWD with adverse effects in caregivers, and (b) can be disseminated successfully into larger community settings. In this SBIR Fast Track application, we will develop, refine, and evaluate People Power Caregiver (PPCg), a flexible and expandable hardware/software system designed to integrate in-home sensors and devices, emergency responding, social networking, and Internet-of-Things (i.e., devices that can be controlled and communicated with via the internet) technologies to create a more supportive and safe home environment for caregivers and PWD. PPCg monitors troublesome behaviors in PWD (e.g., wandering), and targets mechanisms (e.g., worry, social isolation) thought to link behavioral symptoms in PWD with adverse caregiver outcomes. PPCg is also designed to minimize demands on caregivers’ limited time and energy and to provide a platform for data collection that can be used by researchers and care professionals.  This application is an innovative partnership between People Power (CEO: Gene Wang, www.peoplepowerco.com) in Redwood City, California and the Berkeley Psychophysiology Laboratory (Director: Robert W. Levenson) at the University of California, Berkeley. People Power is an award-winning, established leader in home monitoring and Internet-of-Things technology and has recently started developing assistive technologies for the elderly. The Berkeley Psychophysiology Laboratory has been engaged in basic and applied research with PWD and other neurodegenerative diseases and their caregivers for the past 15 years. The proposal addresses three specific aims: Aim 1: Focus groups. In Phase I of the project, a preliminary version of PPCg will be developed and refined with input from focus groups of caregivers and in- home testing (Study 1). Aim 2: Efficacy. In the first year of Phase II of the project, the first production version of PPCg will be installed by the research team in 80 homes and evaluated in a randomized controlled efficacy trial that includes careful diagnosis and assessment of emotional functioning in PWD and caregivers (Study 2). Aim 3: Effectiveness. In the second year of Phase II of the project, working with energy industry partners, a refined and expanded second production version of PPCg will be provided to 400 homes with familial caregivers for self-installation and evaluation in a community-based effectiveness trial (Study 3). Relevance  Dementias cause profound cognitive, emotional, and functional deficits. As the disease progresses, people with dementia become increasingly dependent on caregivers, who are at heightened risk for mental and physical health problems. Applying assistive technology to monitor worrisome behaviors, improve safety, and reduce social isolation in the home environment can reduce caregiver burden and improve care in ways that have major public health benefits.",Developing and Evaluating In-Home Supportive Technology for Dementia Caregivers,9858205,R44AG059458,"['Address', 'Adverse effects', 'Age', 'Aggressive behavior', 'Alzheimer&apos', 's Disease', 'Applied Research', 'Artificial Intelligence', 'Award', 'Basic Science', 'Behavior', 'Behavior monitoring', 'Behavioral Symptoms', 'California', 'Caregiver Burden', 'Caregivers', 'Caring', 'Cities', 'Clinical Trials', 'Cognitive', 'Communities', 'Computer software', 'Data Collection', 'Dementia', 'Dementia caregivers', 'Devices', 'Diagnosis', 'Disease', 'Effectiveness', 'Elderly', 'Emergency Situation', 'Emotional', 'Evaluation', 'Family', 'Family member', 'Fire - disasters', 'Floods', 'Focus Groups', 'Friends', 'Fright', 'Frontotemporal Dementia', 'Future', 'GTP-Binding Protein alpha Subunits, Gs', 'Genes', 'Health', 'Health Benefit', 'Home environment', 'Impairment', 'Internet', 'Internet of Things', 'Intervention', 'Laboratories', 'Lead', 'Life', 'Link', 'Loneliness', 'Mental Health', 'Monitor', 'Neurodegenerative Disorders', 'Outcome', 'Pathway interactions', 'Pattern', 'Personal Satisfaction', 'Phase', 'Population', 'Production', 'Progressive Disease', 'Psychophysiology', 'Public Health', 'Quality of Care', 'Randomized', 'Redwood', 'Research', 'Research Personnel', 'Risk', 'Safety', 'Self-Help Devices', 'Small Business Innovation Research Grant', 'Social Behavior', 'Social Network', 'Social isolation', 'Symptoms', 'Technology', 'Testing', 'Time', 'Universities', 'Voice', 'adverse outcome', 'base', 'caregiver interventions', 'community setting', 'dementia caregiving', 'design', 'effectiveness trial', 'efficacy trial', 'emotional behavior', 'emotional functioning', 'flexibility', 'hazard', 'improved', 'industry partner', 'innovation', 'learning algorithm', 'loved ones', 'mortality', 'physical conditioning', 'prevent', 'psychosocial', 'response', 'sensor', 'service providers', 'social media', 'software systems', 'theories', 'user-friendly']",NIA,PEOPLE POWER COMPANY,R44,2020,2217090,-0.01779854857399466
"Incorporating Learning Effects into Medical Device Active Safety Surveillance Methods Implantable medical devices have revolutionized contemporary cardiovascular care, and are used in a wide spectrum of acute and chronic cardiovascular conditions. However, medical device design fault or incorrect use may lead to significant risk of patient injury and represents an important preventable public health risk in the United States. To help identify device-related safety issues, a strategy of active, prospective, post-market safety surveillance has been recommended by the FDA, and evaluated methodologically. This type of surveillance offers significant advantages over traditional adverse event reporting strategies. However, all such approaches are challenged by the need to incorporate learning effects into expectations regarding safety. These learning impacts been repeatedly shown to have dramatic impacts on outcomes during early device experience. Quantifying learning effects on the outcomes associated with high-risk cardiovascular devices will improve our understanding of intrinsic device performance, thereby identifying patient populations best treated with such devices while simultaneously providing necessary feedback to device manufacturers to support iterative improvement in device design. Separately, understanding the impacts of learning may identify opportunities for targeted training as well as help to tease apart institutional and operator characteristics that may accelerate the achievement of optimal outcomes in the use of the specific cardiovascular device.  This proposal seeks to extend the previously validated, open-source, active, prospective device safety surveillance tool, by developing and validating robust learning curve (LC) detection and quantification algorithms, designed to simultaneously account for the effects at the operator and institutional levels. We propose a “blinded” development strategy, in which one team will generate robust synthetic clinical data simulator with LC impacts, and the other team develops and applies LC detection and quantification algorithms, without knowledge of the underlying relationships, determine performance and accuracy through sequential refinement and validation steps. We propose to formally validate the optimized LC tools in real-world data through re-analysis of previously published LC effects on transcatheter valves and vascular closure devices using national cardiovascular registries. In addition, the LC tools will be incorporated into two active, prospective device safety surveillance studies of novel implantable cardiovascular devices using large clinical registries. This proposal seeks to understand the impact of institutional and physician learning on the safety of newly approved cardiovascular devices, and to use this knowledge to support and improve effective medical device safety surveillance. We propose a “blinded” strategy of separating simulated dataset generation from the learning effects detection and quantification algorithm development. Incorporating learning effects adjustment into a validated, prospective, near-real-time safety surveillance system, this research will improve public health by identifying poorer performing cardiovascular devices, and provide physicians, device manufacturers and public health officials with better information to optimize the use of medical devices, iteratively improve their design, and identify opportunities for enhanced training that will result in improved patient outcomes.",Incorporating Learning Effects into Medical Device Active Safety Surveillance Methods,9863048,R01HL149948,"['Achievement', 'Acute', 'Address', 'Adverse event', 'Algorithm Design', 'Algorithms', 'Blinded', 'Blood Vessels', 'Cardiovascular system', 'Caring', 'Characteristics', 'Chronic', 'Clinical', 'Clinical Data', 'Complex', 'Data', 'Data Aggregation', 'Data Analytics', 'Data Set', 'Detection', 'Development', 'Device Designs', 'Device Safety', 'Devices', 'Early Diagnosis', 'Elements', 'Environment', 'Etiology', 'Evaluation', 'Event', 'Feedback', 'Generations', 'Implant', 'Injections', 'Injury', 'Institution', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Literature', 'Machine Learning', 'Manufacturer Name', 'Medical Device', 'Medical Device Designs', 'Medical Device Safety', 'Methodology', 'Methods', 'Modeling', 'Outcome', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Physicians', 'Process', 'Provider', 'Public Health', 'Publishing', 'Registries', 'Reporting', 'Risk', 'Safety', 'Signal Transduction', 'Specific qualifier value', 'Statistical Models', 'Structure', 'Surveillance Methods', 'Time', 'Training', 'United States', 'Validation', 'Variant', 'adverse outcome', 'algorithm development', 'cardiovascular risk factor', 'clinical heterogeneity', 'design', 'expectation', 'experience', 'high risk', 'implantable device', 'improved', 'novel', 'open source', 'patient population', 'post-market', 'prospective', 'safety outcomes', 'simulation', 'surveillance strategy', 'surveillance study', 'systems research', 'tool']",NHLBI,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2020,828534,0.014978053845331083
"Development of a visual-to-tactile conversion system for automating tactile graphic generation process PROJECT SUMMARY/ABSTRACT There are an estimated 23.7 million people who are blind or visually-impaired (BVI) in the U.S. and 285 million globally. Of this population, 30% do not travel independently outside of their home, only ~11% have a bachelor’s degree, and more than 70% are unemployed. The goal of this SBIR effort is to develop a novel system, which performs principled down-sampling and translation of visual information from digital documents into tactile equivalents. Timely access to information is one of the biggest challenges for BVI people. While access to textual information has largely been solved via screen reading software (e.g., JAWS or VoiceOver), very little progress has been made in making graphical information accessible. Although few assistive technology (AT) devices aim provide non-visual graphical access, they suffer from several shortcomings including high cost, limited portability, lack of multi-purpose, and inability to present information in a real-time context. Importantly, a common underlying problem across all extant approaches is that they require intensive human effort for producing or authoring tactile (and/or multimodal) graphics, which leads to high production costs and significant delays in the time between when the accessible materials are needed, and when they are actually delivered, adversely impacting BVI individuals in K-12 schools, colleges, and workplace settings. To address this long-standing problem, UNAR Labs aims to develop a novel system, which will automatically down-sample and translate visual graphical information into an intuitive tactile equivalent that can be used in tactile embossers. Building upon eight years of empirical research, this Phase I SBIR effort will prove the technical feasibility and functional viability of a prototype system for automating visual-to-tactile graphic conversion process and using the output in embossers. Two specific aims will guide this Phase I project: (1) to develop a prototype of an automated system for performing visual-to-tactile conversion without human intervention, and (2) to assess the technical feasibility and functional utility of the system through a rigorous human study. Success in this effort will provide a robust automated system for tactile graphic generation and promote empowerment of millions of BVI individuals by supporting increased educational attainment, proliferation of vocational opportunities, and enhancing overall quality of life for BVI people. PROJECT NARRATIVE Lack of equitable and timely access to information among persons who are blind or visually impaired (BVI) is key to realizing an inclusive world for all as it alleviates a known impediment that is hugely detrimental to their success in activities affecting quality of life and socio-economic status. The proposed innovation presents a first- of-its kind on-demand visual-to-tactile translation system, which will fully automate the tactile graphic generation process using bio-inspired sensory substitution rules and will instantly deliver the translated information for use in tactile embossers. Successful completion of this project will significantly reduce tactile graphic production costs and preparation time, and will promote empowerment of millions of BVI individuals by supporting increased educational attainment, vocational opportunities, and overall better quality of life.",Development of a visual-to-tactile conversion system for automating tactile graphic generation process,10008494,R43EY031628,"['Access to Information', 'Address', 'Adoption', 'Affect', 'Bachelor&apos', 's Degree', 'Benchmarking', 'Braille Display', 'Characteristics', 'Cognitive', 'Computer software', 'Computers', 'Data', 'Development', 'Devices', 'Elements', 'Empirical Research', 'Evaluation', 'Floor', 'Generations', 'Goals', 'Graph', 'Home environment', 'Human', 'Individual', 'Information Retrieval', 'Intervention', 'Intuition', 'Maine', 'Maps', 'Nature', 'Output', 'Performance', 'Persons', 'Phase', 'Plant Roots', 'Population', 'Preparation', 'Process', 'Production', 'Productivity', 'Psychophysics', 'Quality of life', 'Readability', 'Reading', 'Research', 'Route', 'Sampling', 'Schools', 'Self-Help Devices', 'Sensory', 'Small Business Innovation Research Grant', 'Socioeconomic Status', 'Software Framework', 'Support System', 'System', 'Tactile', 'Text', 'Time', 'Touch sensation', 'Translating', 'Translations', 'Unemployment', 'Universities', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'braille', 'college', 'cost', 'data modeling', 'deep learning', 'digital', 'empowerment', 'human study', 'innovation', 'multimodality', 'multisensory', 'novel', 'operation', 'portability', 'prototype', 'success', 'touchscreen', 'usability', 'visual information', 'visual learning']",NEI,"UNAR LABS, LLC",R43,2020,300000,0.007529869927835469
"Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs PROJECT ABSTRACT Above-knee amputees often struggle to perform the varying activities of daily life with conventional prostheses. Emerging powered knee-ankle prostheses have motors that can restore normative biomechanics, but these devices are limited to a small set of pre-defined activities that must be tuned to the user by technical experts over several hours. The overall goal of this project is to model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning. The universal use of different task-specific controllers in current powered legs is a direct consequence of the prevailing paradigm for viewing human locomotion as a discrete set of activities. There is a fundamental gap in knowledge about how to analyze, model, and control continuously varying locomotion, which greatly limits the adaptability and agility of powered prostheses. The central hypothesis of this project is that continuously varying activities can be represented by a single mathematical model based on measureable physical quantities called task variables. The proposed project will be scientifically significant to understanding how humans continuously adapt to varying activities and environments, technologically significant to the design of agile, user-synchronized powered prosthetic legs, and clinically significant to the adoption of powered knee-ankle prostheses for improved community ambulation. The proposed model of human locomotion will enable new prosthetic strategies for controlling and adapting to the environment, which aligns with the missions of the NICHD/NCMRR Devices and Technology Development program area and the NIBIB Mathematical Modeling, Simulation, and Analysis program. The innovation of this work is encompassed in 1) a continuous paradigm for variable locomotor activities that challenges the existing discrete paradigm, 2) a unified task control methodology that drastically improves the agility of powered prosthetic legs, and 3) a partially automated tuning process that significantly reduces the time and technical expertise required to configure powered knee- ankle prostheses. This continuous task paradigm will provide new methods and models for studying human locomotion across tasks and task transitions. This innovation will address a key roadblock in control technology that currently restricts powered legs to a small set of activities that do not generalize well across users. The adaptability of the proposed control paradigm across users and activities will transform the prosthetics field with a new generation of “plug-and-play” powered legs for community ambulation. PROJECT NARRATIVE The proposed research is relevant to public health because the clinical application of variable-activity powered prosthetic legs can significantly improve community mobility and therefore quality of life for nearly a million American amputees. Recently developed powered knee-ankle prostheses are limited to a small set of pre- defined activities that require several hours of expert tuning for each user. This project will model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning, which aligns with the missions of the Devices and Technology Development program area of the NICHD National Center for Medical Rehabilitation Research and the Mathematical Modeling, Simulation, and Analysis program of the NIBIB.",Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs,9925236,R01HD094772,"['Address', 'Adoption', 'American', 'Amputees', 'Ankle', 'Area', 'Artificial Leg', 'Biomechanics', 'Clinical', 'Communities', 'Data', 'Degree program', 'Device or Instrument Development', 'Devices', 'Doctor of Philosophy', 'Electrical Engineering', 'Environment', 'Gait', 'Gait speed', 'Generations', 'Goals', 'Hand', 'Home environment', 'Hour', 'Human', 'Human body', 'Joints', 'Knee', 'Knowledge', 'Lead', 'Leg', 'Life', 'Locomotion', 'Lower Extremity', 'Machine Learning', 'Mathematical Model Simulation', 'Measurable', 'Measures', 'Mechanics', 'Medical center', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motor', 'Motor Activity', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'Orthotic Devices', 'Outcome', 'Phase', 'Play', 'Process', 'Program Development', 'Prosthesis', 'Public Health', 'Quality of life', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Speed', 'Spinal cord injury', 'Stroke', 'Study models', 'System', 'Technical Expertise', 'Technology', 'Time', 'United States National Institutes of Health', 'Walking', 'Work', 'base', 'clinical application', 'clinically significant', 'design', 'exoskeleton', 'experience', 'human data', 'human model', 'improved', 'innovation', 'kinematics', 'mathematical model', 'multidisciplinary', 'powered prosthesis', 'programs', 'prosthesis control', 'rehabilitation research', 'robot control', 'sensor', 'success', 'technology development', 'temporal measurement', 'trend']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,446707,-0.02235590852367701
"Continuous Non-Invasive Blood Pressure Monitor for Neonates Summary/Abstract Each year, about 5-18% of babies are born preterm, accounting for over 0.5M births in the US and 15M globally. Many of these babies are admitted to Neonatal Intensive Care Units (NICUs) where the medical staff generally have the option of using either invasive arterial lines (IALs) or inflatable-cuff non-invasive blood pressure (NIBP) monitoring. The former introduces the risk of infection, tissue and nerve damage, and the latter is less accurate, especially for hypotensive infants, and may add the risk of ischemic and nerve damage upon repeated measurement. There is a clear need for a safer, continuous, and cost effective form of NIBP measurement to meet the challenge of managing unhealthy blood pressures for neonates. PyrAmes Inc. has developed a novel capacitive sensor technology that is paper thin and flexible and can accurately detect blood pressure (BP). This sensor technology is part of a unique continuous BP monitoring platform that provides accurate, lightweight and comfortable BP monitoring in a wireless, wrist-worn package that is easy to use. The system uses lightweight neural networks to analyze pulse waveform data to provide continuous determination of systolic, diastolic, and mean BP, heart rate, and their variabilities. The sensor is easy to apply non-invasively and records pulsatile data similar to an arterial line, while avoiding the difficulties of placing and maintaining an arterial line. This device can provide gold standard BP monitoring without perturbing the patients for more accurate and relevant measurements. The objective of this project is to extend the platform for use with term and pre-term neonates. This goal will be accomplished through redesign of the sensor hardware and optimization of the data analytics software. We will validate these modifications with clinical data from the NICU at Stanford University Medical Center. In Phase I, we will miniaturize the electronics and modify the sensor array of a wrist-worn pulse wave monitor to be sized more appropriately for neonates. We will validate the new device design by collecting NICU clinical data from patients who have IALs in place in an IRB-approved study. From IAL and sensor data taken simultaneously, we will determine ground truth values on a pulse-by-pulse basis and use these data in conjunction with additional IAL data from historical databases to improve our sensor quality and predictive BP models. Our success metric will be to equal or exceed the quality and accuracy of our data for adults. Phase II will be a follow-up IRB-approved pivotal study using the device from Phase 1 to position our device for FDA submission and clearance and scale up to pilot production of this device. Project Narrative The goal of this Phase 1 project is to confirm that machine learning can be used to extract blood pressure values for critically ill neonates from pulse waveform data collected with a wearable, non-invasive device that is comfortable, low-cost and easy to use. The proposed device will significantly reduce the need for frequent cuff-based measurements and/or invasive arterial lines, thereby decreasing morbidity, risk of complications, patient discomfort, and overall cost of care. This project is based on the pioneering efforts of Prof. Zhenan Bao’s lab at Stanford on thin film sensors for electronic skin and includes the design and use of a miniaturized device to collect clinical data from neonates that will be used to validate a model which derives blood pressure values from the pulse waveform without external calibration.",Continuous Non-Invasive Blood Pressure Monitor for Neonates,9910153,R43HD101175,"['Academic Medical Centers', 'Accounting', 'Adult', 'Antihypertensive Agents', 'Area', 'Arterial Lines', 'Birth', 'Blood Pressure', 'Blood Pressure Monitors', 'Blood pressure determination', 'Bluetooth', 'Calibration', 'Cells', 'Child', 'Childhood', 'Clinical', 'Clinical Data', 'Clinical Research', 'Coin', 'Computer Analysis', 'Computer software', 'Consumption', 'Critical Illness', 'Data', 'Data Analytics', 'Databases', 'Development', 'Device Designs', 'Devices', 'Disadvantaged', 'Electromagnetics', 'Electronics', 'Female', 'Film', 'Goals', 'Gold', 'Heart Rate', 'Hemorrhage', 'Infant', 'Institutional Review Boards', 'Lead', 'Limb structure', 'Machine Learning', 'Measurement', 'Measures', 'Medical Staff', 'Methods', 'Modeling', 'Modification', 'Monitor', 'Morbidity - disease rate', 'Morphologic artifacts', 'Neonatal', 'Neonatal Intensive Care Units', 'Nerve', 'Neural Network Simulation', 'Noise', 'Pain', 'Paper', 'Patients', 'Phase', 'Photoplethysmography', 'Physiologic pulse', 'Polychlorinated Biphenyls', 'Positioning Attribute', 'Process', 'Production', 'Pulse Pressure', 'Reading', 'Records', 'Resolution', 'Risk', 'Signal Transduction', 'Skin', 'Surface', 'System', 'Technology', 'Testing', 'Thick', 'Thinness', 'Time', 'Tissues', 'Training', 'Wireless Technology', 'Work', 'Wrist', 'artificial neural network', 'base', 'care costs', 'cost', 'cost effective', 'demographics', 'design', 'encryption', 'flexibility', 'follow-up', 'improved', 'infection risk', 'light weight', 'male', 'miniaturize', 'neonate', 'neural network', 'novel', 'pressure', 'preterm newborn', 'scale up', 'sensor', 'sensor technology', 'skills', 'success', 'tonometry']",NICHD,"PYRAMES, INC.",R43,2020,224556,0.01915081115873016
"Development of a web-based platform implementing novel Predictor of Skin Sensitization for Medical Devices (PreSS/MD) PROJECT SUMMARY Medical devices have been documented to contain toxic chemicals that can leach and cause acute contact dermatitis (ACD) after repeated exposure or prolonged contact of the skin to these toxins. ACD is credited for 10-15% of all occupational illnesses and is also the second highest reported occupational hazard. Given its prevalence, ACD is also a great public health burden with combined yearly costs of up to $1 billion, which spans including medical costs, worker’s compensation and lost working time due to workplace absence. To this end, the U.S. Food and Drug Administration has mandated that all medical devices must be evaluated for possible skin sensitization using in vivo animal assays, which includes the Guinea pig maximization test (GPMT). Although GPMT tests provide valuable data on the skin sensitization effects of potential toxins, these assays are time-consuming and expensive. Moreover, the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM) recently published a Strategic Roadmap, calling for the development of alternative approaches to reduce animal testing of chemical and medical agents. Thus, there is a stated need to modernize safety evaluation of medical devices to reduce animal testing and shorten the regulatory review time, which would ultimately bring safer devices to the market faster. To address this unmet need, the key objectives of our FDA Phase I SBIR project are to (i) produce rigorously validated computational models for the GPMT assay integrating data obtained in human, mouse, and in vitro assays; and (ii) integrate these models into a software product termed PreSS/MD (Predictor of Skin Sensitization for Medical Devices). Our specific aims for this study include: 1) collecting, curating, and integrating the largest publicly available dataset for GMPT; 2) creating and validating novel computational models for GMPT data; 3) developing the PreSS/MD web server to allow users to make predictions of skin sensitization potential in medical devices. We will also develop a model for mixtures, including compounds tested jointly in different concentrations, using an approach that we developed previously. Finally, we will implement novel approaches to help users of our PreSS/MD platform interpret the developed models in terms of key chemical features responsible for skin sensitization. In addition, we will employ biomedical knowledge graphs to elucidate Adverse Outcome Pathways (AOPs) for skin sensitizers. Successful execution of this Phase I project will yield in the development of PreSS/MD as a centralized resource to evaluate the skin sensitization potential for medical devices. We expect this software-as-a-service web server platform will be of great value for companies and sponsors seeking regulatory approval of medical devices. PROJECT NARRATIVE Given that medical devices have been documented to contain toxic chemicals that may lead to allergic contact dermatitis, the US Food and Drug Administration requires that all devices be evaluated for possible skin sensitization effects using in vivo assays such as the Guinea pig maximization test. In the effort to modernize skin sensitization safety evaluation methods to reduce in vivo animal testing, herein we propose to develop a software product, PreSS/-MD (Predictor of Skin Sensitization caused by Medical Devices), as an innovative and unique in silico alternative with the potential to better predict human response compared to the existing approaches for skin sensitization assessment. Successful execution of the objectives described in this project will result in a centralized web server platform to evaluate the skin sensitization potential for medical devices, which will be of significant value for companies and sponsors seeking regulatory approval of medical devices.",Development of a web-based platform implementing novel Predictor of Skin Sensitization for Medical Devices (PreSS/MD),10079701,R43ES032371,"['Acute', 'Address', 'Advanced Development', 'Allergic Contact Dermatitis', 'Animal Testing', 'Animals', 'Bayesian Method', 'Bayesian Modeling', 'Biological Assay', 'Cavia', 'Chemical Structure', 'Chemicals', 'Computer Models', 'Computer software', 'Consumption', 'Contact Dermatitis', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Devices', 'Economics', 'Evaluation', 'Feedback', 'Generations', 'Human', 'Immune response', 'Instruction', 'Interagency Coordinating Committee on the Validation of Alternative Methods', 'International', 'Knowledge', 'Lead', 'Medical', 'Medical Care Costs', 'Medical Device', 'Methods', 'Modeling', 'Modernization', 'Mus', 'Occupational', 'Online Systems', 'Pathway interactions', 'Phase', 'Poison', 'Prevalence', 'Prostheses and Implants', 'Public Health', 'Publishing', 'Pythons', 'Quantitative Structure-Activity Relationship', 'Reaction', 'Reporting', 'Resources', 'Safety', 'Skin', 'Small Business Innovation Research Grant', 'Structure', 'Test Result', 'Testing', 'Time', 'Toxic effect', 'Toxin', 'United States Food and Drug Administration', 'Validation', 'Workers&apos', ' Compensation', 'Workplace', 'adverse outcome', 'chemical release', 'cost', 'experience', 'in silico', 'in vitro Assay', 'in vivo', 'innovation', 'knowledge graph', 'lymph nodes', 'machine learning algorithm', 'model development', 'novel', 'novel strategies', 'occupational hazard', 'operation', 'phase 1 study', 'response', 'skin patch', 'software as a service', 'success', 'systemic toxicity', 'tool', 'web portal', 'web server']",NIEHS,"PREDICTIVE, LLC",R43,2020,167910,0.0036655561141439317

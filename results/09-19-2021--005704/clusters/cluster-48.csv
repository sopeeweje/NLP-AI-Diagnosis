text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Automated domain adaptation for clinical natural language processing Project Summary Automatic extraction of useful information from clinical texts enables new clinical research tasks and new technologies at the point of care. The natural language processing (NLP) systems that perform this extraction rely on supervised machine learning. The learning process uses manually labeled datasets that are limited in size and scope, and as a result, applying NLP systems to unseen datasets often results in severely degraded performance. Obtaining larger and broader datasets is unlikely due to the expense of the manual labeling process and the difficulty of sharing text data between multiple different institutions. Therefore, this project develops unsupervised domain adaptation algorithms to adapt NLP systems to new data. Domain adaptation describes the process of adapting a machine learning system to new data sources. The proposed methods are unsupervised in that they do not require manual labels for the new data. This project has three aims. The first aim makes use of multiple existing datasets for the same task to study the differences in domains, and uses this information to develop new domain adaptation algorithms. Evaluation uses standard machine learning metrics, and analysis of performance is tightly bounded by strong baselines from below and realistic upper bounds, both based on theoretical research on machine learning generalization. The second aim develops open source software tools to simplify the process of incorporating domain adaptation into clinical text processing workflows. This software will have input interfaces to connect to methods developed in Aim 1 and output interfaces to connect with Apache cTAKES, a widely used open- source NLP tool. Aim 3 tests these methods in an end-to-end use case, adverse drug event (ADE) extraction on a dataset of pediatric pulmonary hypertension notes. ADE extraction relies on multiple NLP systems, so this use case is able to show how broad improvements to NLP methods can improve downstream methods. This aim also creates new manual labels for the dataset for an end-to-end evaluation that directly measures how improvements to the NLP systems lead to improvement in ADE extraction. Project Narrative Software systems that use machine learning to understand clinical text often suffer severe performance loss when they are applied to new data that looks different than the data that they originally learned from. In this project, we develop and implement methods that allow these systems to automatically adapt to the characteristics of a new data source. We evaluate these methods on the clinical research task of adverse drug event detection, which relies on many different variables found in the text of electronic health records.",Automated domain adaptation for clinical natural language processing,9768545,R01LM012918,"['Adult', 'Adverse drug event', 'Algorithms', 'Apache', 'Area', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Informatics', 'Clinical Research', 'Colon Carcinoma', 'Communities', 'Computer software', 'Computers', 'Conceptions', 'Data', 'Data Set', 'Data Sources', 'Detection', 'Dimensions', 'Ecosystem', 'Educational process of instructing', 'Electronic Health Record', 'Evaluation', 'Human', 'Institution', 'Knowledge', 'Label', 'Language', 'Lead', 'Learning', 'Linguistics', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Manuals', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Network-based', 'Output', 'Pathology', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Pulmonary Hypertension', 'Radiology Specialty', 'Research', 'Software Tools', 'Source', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'TimeLine', 'Training', 'Update', 'Vision', 'Work', 'base', 'case finding', 'improved', 'learning strategy', 'malignant breast neoplasm', 'method development', 'natural language', 'neural network', 'new technology', 'news', 'novel', 'open source', 'point of care', 'side effect', 'social media', 'software systems', 'statistics', 'supervised learning', 'tool', 'tumor', 'unsupervised learning']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2019,383874,0.057781738596264835
"Leveraging Unlabeled and Pseudo Data for Clinical Information Extraction Project Summary/Abstract Electronic Health Records (EHRs) contain significant information that can benefit many downstream uses. However, most of this information is in unstructured narrative form and is inaccessible to computerized methods that rely on structured representations for exploring, retrieving, and presenting the information. Natural language processing (NLP) and information extraction (IE) open this trove of information to studies that would otherwise be without. Over the past decades, many IE systems have been developed. These systems have typically focused on one task at a time. In addition, most have studied only specific types of records, e.g., discharge summaries, and addressed their task on data from a single institution. Performances achieved by the state-of-the-art IE systems developed under these conditions ranged from 44% F-measure to 99% F-measure. This observed variation can be attributed to the nature of the tasks: some target entities like dates tend to be better represented in the data and also more rigidly stick to known patterns of expression as opposed to reasons for medication administration which are relatively sparse in the data and can show wider linguistic diversity. However, this may not be the only reason: the data used can also explain the performance variation. Narratives of EHRs vary in their style, format, and content going from one department to another, from one hospital to another. Even the same record type in two different hospitals can be very different in narrative style and pose different challenges for IE. Understanding IE performance therefore requires studies of multiple tasks on multiple record types that come from multiple institutions. One major bottleneck for evaluation of IE systems on such a large scale is annotation. The same bottleneck also limits system development. This proposal aims to address this bottleneck for both evaluation and development. It first generates a multi-institution corpus consisting of multiple record types from five institutions. It studies four different IE tasks that broadly represent IE in clinical records and can inform the field of IE as a whole: de-identification, clinical concept extraction, medication extraction, and adverse drug event extraction. Within the context of these IE tasks, the proposal then puts forward methods that learn from unlabeled or pseudo data that can help alleviate reliance on annotated data for development. It evaluates these methods both for performance and generalizability on multiple types of records from multiple institutions. As a result of these activities, this proposal generates de-identified data, annotations, methods, software, and machine learning models which it then makes available to the research community. Project Narrative Information extraction (IE) systems, i.e., natural language processing (NLP) systems that enable creation of accurate semantic representations of narratives, rely heavily on the availability of gold standard annotated corpora and vary significantly in their performance from task to task, and from data set to data set. We propose methods that augment gold standard data with unlabeled data that are more easily available, and pseudo data which can be derived from gold standard data. We study IE within the context of four tasks and evaluate IE systems enhanced with unlabeled and pseudo data for generalizability on a heterogeneous data set consisting of multiple record types from five institutions.",Leveraging Unlabeled and Pseudo Data for Clinical Information Extraction,9813134,R15LM013209,"['Accident and Emergency department', 'Address', 'Adverse drug event', 'Affect', 'Clinic', 'Clinical', 'Clinical Data', 'Communities', 'Computer software', 'Data', 'Data Set', 'Development', 'Discipline of Nursing', 'Electronic Health Record', 'Engineering', 'Evaluation', 'Frequencies', 'Gold', 'Growth', 'Healthcare', 'Hospitals', 'Institution', 'Israel', 'Knowledge', 'Label', 'Learning', 'Linguistics', 'Location', 'Machine Learning', 'Measures', 'Medical', 'Medical center', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nature', 'Outcome', 'Pattern', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Plant Roots', 'Procedures', 'Psychiatry', 'Publications', 'Records', 'Reporting', 'Research', 'Resources', 'Route', 'Sampling', 'Semantics', 'Signs and Symptoms', 'Social Work', 'Structure', 'Supervision', 'System', 'Systems Development', 'Task Performances', 'Telephone', 'Test Result', 'Testing', 'Text', 'Thinness', 'Time', 'Training', 'Universities', 'Variant', 'Virginia', 'Washington', 'computerized', 'deep learning', 'dosage', 'field study', 'improved', 'learning strategy', 'medication administration', 'novel', 'open source', 'response', 'supervised learning', 'tool']",NLM,GEORGE MASON UNIVERSITY,R15,2019,414798,0.03427730509661387
"Resource Curation and Evaluation for EHR Note Comprehension Title: Resource Curation and Evaluation for EHR Note Comprehension The American Diabetes Association estimates that over 25.8 million Americans have diabetes. Proper patient self-management is perhaps the most critical and under-exercised element for patients to achieve appropriate glycemic control and thus mitigate complications and comorbid conditions, and implement appropriate preventive strategies (e.g., vaccines, exercise, healthy diet). In previous work we have shown that educating patients by using content in their medical records improved diabetes outcomes and that NoteAid, a multi- module natural language processing system that links medical jargon in electronic health records to definitions, has improved self-reported comprehension. Here we propose to develop NoteAid with a high-quality resource of expert-curated lay definitions and evaluation methods for diabetes patients’ note comprehension. ! ! Title: Resource Curation and Evaluation for EHR Note Comprehension The American Diabetes Association estimates that over 25.8 million Americans have diabetes. In previous work we have shown that educating patients by using content in their medical records improved diabetes outcomes and that NoteAid, a natural language processing system that links medical jargon in electronic health records to definitions, has improved self-reported comprehension. Here we propose to develop NoteAid with a high-quality resource of expert-curated lay definitions and evaluation methods for diabetes patients’ note comprehension. ! !",Resource Curation and Evaluation for EHR Note Comprehension,9794757,R01LM012817,"['Adult', 'Age', 'American', 'Artificial Intelligence', 'Attitude', 'Back', 'Behavior', 'Caregivers', 'Caring', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Comorbidity', 'Complex', 'Comprehension', 'Country', 'Data Science', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Elements', 'Ethnic Origin', 'Evaluation', 'Exercise', 'Eye diseases', 'General Population', 'Health', 'Healthcare', 'Heart Diseases', 'Hospitals', 'Individual', 'Informatics', 'Internet', 'Intervention', 'Kidney Diseases', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Link', 'Machine Learning', 'Massachusetts', 'Measurement', 'Mechanics', 'Medical', 'Medical Records', 'Medical Students', 'Methods', 'Modeling', 'Natural Language Processing', 'Nursing Students', 'Ontology', 'Outcome', 'Patient Education', 'Patient Self-Report', 'Patients', 'Physicians', 'Prevention strategy', 'Questionnaires', 'Race', 'Randomized', 'Reading', 'Resources', 'Risk', 'Self Management', 'Speed', 'Supervision', 'System', 'Techniques', 'Testing', 'Time', 'Trust', 'United States National Institutes of Health', 'Universities', 'Vaccines', 'Visual Aid', 'Vocabulary', 'Work', 'base', 'care outcomes', 'clinically relevant', 'cost', 'deep learning', 'design', 'diabetic patient', 'glycemic control', 'good diet', 'improved', 'innovation', 'instrument', 'response', 'sex', 'skills', 'usability']",NLM,UNIVERSITY OF MASSACHUSETTS LOWELL,R01,2019,337238,-0.020906415621700408
"Improving Specialty Care Delivery in the Safety Net with Natural Language Processing Project Summary  Safety net providers treat a substantial share of socioeconomically vulnerable patients in their communities, but struggle to provide timely access to high quality specialty care for their patients. Delayed access to specialty care is associated with worse health outcomes and potentially contributes to health disparities across socioeconomic groups. Given their limited resources, safety net providers must seek creative approaches to improve specialty access. However, to choose what programs to implement, safety net providers need to understand the specialty care needs of their populations. Fortunately, the adoption of eConsult systems by safety net providers across the US provides a valuable opportunity to systematically measure patterns of specialty care referrals for minority, underserved populations.  In this project, we propose using state-of-the-art methods in machine learning and natural language processing (NLP) to help safety net providers extract actionable, population wide data from their electronic consultation systems. We will do this in partnership with three of the most prominent safety net health systems in the US in Los Angeles, San Francisco and New York City. Using specialty request databases from our collaborators, we will build NLP systems to automatically classify specialty requests along two dimensions: the “clinical issue” motivating the request (e.g., chest pain), and the “question type” (e.g., request for a procedure, help with medication management). This automated classification of electronic specialty requests can enable identification of promising targets for interventions to improve specialty access and quality of care.  After developing these NLP systems, we will analyze >1 million specialty requests to describe trends in how safety net patients are referred to specialists and examine variation in referral patterns by clinic and individual provider. The goal is to identify the most impactful opportunities to improve specialty access and quality. For example, a high rate of referrals for esophageal reflux, which most PCPs can treat on their own with specialist guidance, could lead to new treatment algorithms, potentially reducing the need for these requests and improving access for other patients.  This proposal is a “high-risk high-reward” project that creates new research tools to identify and evaluate data-driven interventions to improve specialty care delivery for underserved populations. Project Narrative Access to timely, high-quality specialty care is a fundamental component of a well-functioning health system, yet safety net health care providers face persistent challenges delivering such care. Quality improvement efforts to improve specialty access have been thwarted in part because safety net providers lack the data to understand a basic question – why patients are referred for specialty care. Taking advantage of the growing use of electronic specialty referral systems by safety net providers, we propose using natural language processing to conduct automated analysis and classification of specialty requests in safety net populations, which will enable the design of targeted interventions to improve specialty care access and delivery.",Improving Specialty Care Delivery in the Safety Net with Natural Language Processing,9789060,R21MD012693,"['Acute', 'Adopted', 'Adoption', 'Algorithms', 'Caring', 'Chest Pain', 'Chronic Kidney Failure', 'Classification', 'Clinic', 'Clinical', 'Communities', 'Consultations', 'County', 'Data', 'Data Set', 'Databases', 'Education', 'Epidemiology', 'Face', 'Federally Qualified Health Center', 'Gastroesophageal reflux disease', 'Goals', 'Health', 'Health Personnel', 'Health Services', 'Health Services Accessibility', 'Health system', 'Heart failure', 'Hospitals', 'Improve Access', 'Individual', 'Intervention', 'Lead', 'Los Angeles', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Education', 'Medical center', 'Medication Management', 'Methods', 'Minority', 'Morbidity - disease rate', 'Natural Language Processing', 'New York City', 'Online Systems', 'Ophthalmology', 'Outcome', 'Patients', 'Pattern', 'Play', 'Population', 'Primary Care Physician', 'Procedures', 'Provider', 'Public Hospitals', 'Quality of Care', 'Research', 'Resources', 'Retinal Diseases', 'Role', 'San Francisco', 'Specialist', 'System', 'Taxonomy', 'Telemedicine', 'Text', 'Time', 'Transplantation', 'Triage', 'Underserved Population', 'Variant', 'Visit', 'automated analysis', 'care delivery', 'design', 'diabetic', 'disease classification', 'ethnic minority population', 'follow-up', 'health disparity', 'high reward', 'high risk', 'improved', 'medical specialties', 'medically underserved', 'minority communities', 'mortality', 'performance tests', 'programs', 'racial and ethnic', 'safety net', 'screening', 'socioeconomic disadvantage', 'socioeconomics', 'tool', 'trend', 'two-dimensional']",NIMHD,BOSTON CHILDREN'S HOSPITAL,R21,2019,94263,0.02947424879488561
"Integrative data science approaches for rare disease discovery in health records ABSTRACT: There are nearly 7,000 diseases that have a prevalence of only one in 2,000 individuals or less. Yet, such rare diseases are estimated to collectively affect over 300 million people worldwide, representing a significant healthcare concern. Although rare diseases have predominantly genetic origins, nearly half of them do not manifest symptoms until adulthood and frequently confound discovery and diagnosis. Even in the case of early onset disorders, the sheer number of possible diagnoses can often overwhelm clinicians. As a result, rare diseases are often diagnosed with delay, misdiagnosed or even remain undiagnosed, not only disrupting patient lives but also hindering progress on our understanding of such diseases. Data science methods that mine large-scale retrospective health record data for phenotypic information will aid in timely and accurate diagnoses of rare diseases, especially when combined with additional data types, thus, having significant real- world impact. This proposal will integrate electronic health record (EHR) data sets with publicly available vocabularies and ontologies, and genomic data for the improved identification and characterization of patients with rare diseases, using approaches from machine learning, natural language processing (NLP) and basic bioinformatics. The work has three specific aims and will be carried out in two phases. During the mentored phase, the principal investigator (PI) will develop data-driven methods to extract standardized concepts related to rare diseases from clinical notes and infer the occurrence of each disease (Aim 1). He will also develop data science approaches to compare and contrast longitudinal patterns associated with patients' journeys through the healthcare system when seeking a diagnosis for a rare disease, and aid in clinical decision-making by leveraging these patterns (Aim 2). During the independent phase (Aim 3), computational methods will be developed for the integrated modeling and analysis of genotypic (from Aim 3) and phenotypic information (from Aims 1 and 2). Cohorts to be sequenced will cover diseases for which causal genes or disease definitions are unclear (discovery), as well as those for which these are well known (validation). This work will be carried out under the mentorship of four faculty members with complementary expertise in biomedical informatics, data science, NLP, and rare disease genomics at the University of Washington, the largest medical system in the Pacific Northwest (four million EHRs), world-renowned researchers in medical genetics, and a robust data science environment. In addition, under the direction of the mentoring team, the PI will complete advanced coursework, receive training in translational bioinformatics and clinical research informatics, submit manuscripts, and seek an independent research position. This proposal will yield preliminary results for subsequent studies on data-driven phenotyping and enable the realization of the PI's career goals by providing him with the necessary training to build on his machine learning and basic bioinformatics expertise to transition into an independent investigator in biomedical data science. PROJECT NARRATIVE Rare genetic diseases are estimated to affect the lives of 25 to 30 million Americans and their families, and present a significant economic burden on the healthcare system. Currently, our knowledge of the broad spectrum of the 7,000 observed rare diseases is limited to a few well-studied ones, hindering our ability to make correct and timely diagnoses. The objective of this study is to improve the identification of patients with rare diseases in healthcare systems by developing data science approaches that automatically recognize rare disease-related patterns in patient health records and correlate them with genomic data, thus, aiding in diagnosis and discovery.",Integrative data science approaches for rare disease discovery in health records,9645433,K99LM012992,"['Adult', 'Affect', 'American', 'Award', 'Basic Science', 'Behavioral', 'Bioinformatics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Research', 'Computing Methodologies', 'Consensus', 'Data', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostics Research', 'Disease', 'Economic Burden', 'Electronic Health Record', 'Environment', 'Faculty', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Manuscripts', 'Markov Chains', 'Medical', 'Medical Genetics', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Pacific Northwest', 'Patient Recruitments', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Principal Investigator', 'Rare Diseases', 'Recording of previous events', 'Research', 'Research Personnel', 'Standardization', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Vocabulary', 'Washington', 'Work', 'accurate diagnosis', 'base', 'biomedical informatics', 'career', 'causal variant', 'clinical data warehouse', 'clinical decision-making', 'cohort', 'diagnostic accuracy', 'disease phenotype', 'early onset disorder', 'exome sequencing', 'gene discovery', 'genomic data', 'health care delivery', 'health data', 'health record', 'improved', 'member', 'multimodal data', 'novel', 'open source', 'phenotypic data', 'prototype', 'psychologic', 'rare condition', 'rare genetic disorder', 'recruit', 'skills', 'software development', 'support tools', 'tool', 'trait']",NLM,UNIVERSITY OF WASHINGTON,K99,2019,92070,-0.01063990183129613
"Open Health Natural Language Processing Collaboratory Project Summary One of the major barriers in leveraging Electronic Health Record (EHR) data for clinical and translational science is the prevalent use of unstructured or semi-structured clinical narratives for documenting clinical information. Natural Language Processing (NLP), which extracts structured information from narratives, has received great attention and has played a critical role in enabling secondary use of EHRs for clinical and translational research. As demonstrated by large scale efforts such as ACT (Accrual of patients for Clinical Trials), eMERGE, and PCORnet, using EHR data for research rests on the capabilities of a robust data and informatics infrastructure that allows the structuring of clinical narratives and supports the extraction of clinical information for downstream applications. Current successful NLP use cases often require a strong informatics team (with NLP experts) to work with clinicians to supply their domain knowledge and build customized NLP engines iteratively. This requires close collaboration between NLP experts and clinicians, not feasible at institutions with limited informatics support. Additionally, the usability, portability, and generalizability of the NLP systems are still limited, partially due to the lack of access to EHRs across institutions to train the systems. The limited availability of EHR data limits the training available to improve the workforce competence in clinical NLP. We aim to address the above challenges by extending our existing collaboration among multiple CTSA hubs on open health natural language processing (OHNLP) to share distributional information of NLP artifacts (i.e., words, n-grams, phrases, sentences, concept mentions, concepts, and text segments) acquired from real EHRs across multiple institutions. We will leverage the advanced privacy-preserving computing infrastructure of iDASH (integrating Data for Analysis, Anonymization, and SHaring) for privacy- preserving data analysis models and will partner with diverse communities including Observational Health Data Sciences and Informatics (OHDSI), Precision Medicine Initiative (PMI), PCORnet, and Rare Diseases Clinical Research Network (RDCRN) to demonstrate the utility of NLP for translational research. This CTSA innovation award RFA provides us with a unique opportunity to address the challenges faced with clinical NLP and through strong partnership with multiple research communities and leadership roles of the research team in clinical NLP, we envision that the successful delivery of this project will broaden the utilization of clinical NLP across the research community. There are four aims planned: i) obtain PHI-suppressed NLP artifacts with retained distribution information across multiple institutions and assess the privacy risk of accessing PHI- suppressed artifacts, ii) generate a synthetic text corpus for exploratory analysis of clinical narratives and assess its utility in NLP tasks leveraging various NLP challenges, iii) develop privacy-preserving computational phenotyping models empowered with NLP, and iv) partner with diverse communities to demonstrate the utility of our project for translational research. Project Narratives The proposed project aims to broaden the secondary use of electronic health records (EHRs) across the research community by combining innovative privacy-preserving computing techniques and clinical natural language processing.",Open Health Natural Language Processing Collaboratory,9774338,U01TR002062,"['Address', 'Algorithms', 'Attention', 'Award', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Detection', 'Disease', 'Electronic Health Record', 'Ensure', 'Familial Hypercholesterolemia', 'Frequencies', 'Health', 'Hepatolenticular Degeneration', 'Individual', 'Informatics', 'Information Distribution', 'Infrastructure', 'Institution', 'Kidney Calculi', 'Knowledge', 'Leadership', 'Learning', 'Measures', 'Medical', 'Meta-Analysis', 'Minnesota', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Natural Language Processing', 'Observational Study', 'Patients', 'Phenotype', 'Play', 'Precision Medicine Initiative', 'Privacy', 'Process', 'Rare Diseases', 'Research', 'Research Personnel', 'Rest', 'Risk', 'Role', 'Sampling', 'Security', 'Semantics', 'Site', 'Source', 'Structure', 'System', 'Talents', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Translational Research', 'Universities', 'Work', 'base', 'citizen science', 'cohort', 'collaboratory', 'data registry', 'empowered', 'health data', 'improved', 'indexing', 'individual patient', 'informatics infrastructure', 'innovation', 'interest', 'novel', 'phenotypic data', 'phrases', 'portability', 'preservation', 'recruit', 'statistics', 'tool', 'usability', 'virtual']",NCATS,MAYO CLINIC ROCHESTER,U01,2019,1521748,0.09652376214957706
"National NLP Clinical Challenges (n2c2): Challenges in Natural Language Processing for Clinical Narratives Project Summary and Abstract Narratives of electronic health records (EHRs) contain useful information that is difficult to automatically extract, index, search, or interpret. Natural language processing (NLP) technologies can extract this information and convert it in to a structured format that is more readily accessible by computerized systems. However, the development of NLP systems is contingent on access to relevant data and EHRs are notoriously difficult to obtain because of privacy reasons. Despite the recent efforts to de-identify and release narrative EHRs for research, these data are still very rare. As a result, clinical NLP, as a field has lagged behind. To address this problem, since 2006, we organized thirteen shared tasks, accompanied with workshops and journal publications. Twelve of these shared tasks have focused on the development of clinical NLP systems and the remaining one on the usability of these systems. We have covered both depth and breadth in terms of shared tasks, preparing tasks that study cutting-edge NLP problems on a variety of EHR data from multiple institutions. Our shared tasks are the longest running series of clinical NLP shared tasks, with ever growing EHR data sets, tasks, and participation. Our most popular three data sets have been cited 495 (2010 data), 284 (2006 de-id data), and 274 (2009 data) times, respectively, representing hundreds of articles that have come out of these three data sets alone. Our goal in this proposal is to continue the efforts we started in 2006 under i2b2 shared task challenges (i2b2, NIH NLM U54LM008748, PI: Kohane and R13 LM011411, PI: Uzuner) to de-identify EHRs, annotate them with gold- standard annotations for clinical NLP tasks, and release them to the research community for the development and head-to-head comparison of clinical NLP systems, for the advancement of the state of the art. Continuing our efforts under National NLP Clinical Challenges (n2c2) based at the Health Data Science program of the newly established Department of Biomedical Informatics at Harvard Medical School, we aim to form partnerships with the community to grow the shared task efforts in several ways: (1) grow the available de-identified EHR data sets through partnerships that can contribute to the volume and variety of the data, and (2) grow the available gold-standard annotations in terms of depth and breadth of NLP tasks. Given these aims and partnerships, we plan to hold a series of shared tasks. We will complement these shared tasks with workshops that meet in conjunction with the Fall Symposium of the American Medical Informatics Association and with journal special issues so that advancement of the state of the art can be sped up and future generations can build on the past. Project Narrative We propose to organize a series of shared tasks, workshops, and journal publications for fostering the continuous development of clinical Natural Language Processing (NLP) technologies that can extract information from narratives of Electronic Health Records (EHRs). Our aim is to grow the annotated gold standard EHR data sets that are available to the research community through partnerships and to bring together clinical NLP researchers with informatics researchers for building collaborations. We will engage the community in shared tasks and disseminate the knowledge generated by these shared tasks through workshops and journal special issues for the advancement of the state of the art.",National NLP Clinical Challenges (n2c2): Challenges in Natural Language Processing for Clinical Narratives,9759499,R13LM013127,"['Access to Information', 'Address', 'American', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Community Developments', 'Complement', 'Data', 'Data Science', 'Data Set', 'Development', 'Educational workshop', 'Electronic Health Record', 'Evaluation', 'Fostering', 'Future', 'Future Generations', 'Goals', 'Gold', 'Grant', 'Growth', 'Hand', 'Head', 'Healthcare', 'Improve Access', 'Individual', 'Informatics', 'Institution', 'Israel', 'Journals', 'Knowledge', 'Measures', 'Medical Informatics', 'Medical center', 'Methodology', 'Natural Language Processing', 'Outcome', 'Paper', 'Peer Review', 'Performance', 'Privacy', 'Publications', 'Publishing', 'Records', 'Research', 'Research Personnel', 'Rest', 'Running', 'Series', 'Source', 'Structure', 'System', 'Systems Development', 'Targeted Research', 'Technology', 'Time', 'United States National Institutes of Health', 'Universities', 'base', 'biomedical informatics', 'clinical development', 'computerized', 'falls', 'head-to-head comparison', 'health data', 'indexing', 'medical schools', 'meetings', 'practical application', 'programs', 'symposium', 'usability', 'working group']",NLM,GEORGE MASON UNIVERSITY,R13,2019,20000,0.050121830226298576
"Leveraging Twitter to monitor nicotine and tobacco-related cancer communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to monitor nicotine and tobacco-related cancer communication,9656981,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Stream', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Tobacco-Related Carcinoma', 'Twitter', 'Work', 'automated analysis', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'machine learning algorithm', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tobacco products', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2019,431185,0.0036177928919126725
"EHR Anticoagulants Pharmacovigilance DESCRIPTION (provided by applicant): The timely identification of previously unknown toxicities of cardiovascular drugs is an important, unsolved problem. In the United States, 20% of the 548 drugs introduced into the market between 1975 and 1999 were either withdrawn or acquired a new ""black box"" warning during the 25-year period following initial approval by the Food and Drug Administration. Adverse drug events (ADEs) are an important cause of morbidity and mortality in patients, yet 95% of ADEs are unreported, leading to delays in the detection of previously unknown ADEs and underestimation of the risk to known ADEs. It is known that Electronic Health Record (EHR) notes and lab results contain ADE information and biomedical natural language processing (NLP) provides automated tools that facilitate chart review and thus improve patient surveillance and post-marketing pharmacovigilance. Optimal use of anticoagulants requires accurate and timely detection of ADEs from EHRs. The objectives for this proposal are to develop ""intelligent"" NLP approaches to extract disease, medication, and structured ADE information from EHRs, and then evaluate extracted ADEs for detecting known ADE types as well as clinically unrecognized or novel ADEs whose pattern or effect have not been previously identified. PUBLIC HEALTH RELEVANCE: This project proposes innovative intelligent biomedical natural language process approaches to detect ADEs in patients' electronic health records (EHRs), an important step towards ADE surveillance and pharmacovigilance. The focus of this proposal will bon the national priority area of anticoagulants ADE detection, although the algorithms and systems we develop can be extended to other drug classes and diseases. It is anticipated that the data resources, algorithms, and the Anticoagulant Pharmacovigilance Toolkit (APVTK) developed will significantly enhance ADE detection, patient surveillance and post marketing pharmacovigilance.",EHR Anticoagulants Pharmacovigilance,9628032,R01HL125089,"['Adult', 'Adverse drug event', 'Adverse event', 'Algorithms', 'Anticoagulant therapy', 'Anticoagulants', 'Anticoagulation', 'Area', 'Atrial Fibrillation', 'Biometry', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Controlled Vocabulary', 'Cost efficiency', 'Data', 'Data Sources', 'Detection', 'Development', 'Disease', 'Electronic Health Record', 'Elements', 'Etiology', 'Frequencies', 'Guidelines', 'Health Care Costs', 'Health Promotion', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Illinois', 'Injury', 'Institutes', 'Intelligence', 'Intervention', 'Lead', 'Length of Stay', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Marketing', 'Massachusetts', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Ontology', 'Outcomes Research', 'Package Insert', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Prevention', 'Process', 'Reaction', 'Regimen', 'Risk', 'Risk Factors', 'Route', 'Safety', 'Schedule', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Structure', 'System', 'Systems Analysis', 'Text', 'Thromboembolism', 'Time', 'Toxic effect', 'United States', 'United States Food and Drug Administration', 'Universities', 'Venous', 'Work', 'clinical practice', 'data resource', 'disorder prevention', 'dosage', 'epidemiology study', 'high risk', 'improved', 'innovation', 'mortality', 'natural language', 'novel', 'open source', 'patient safety', 'pharmacovigilance', 'predictive modeling', 'primary endpoint', 'prospective', 'public health relevance', 'supervised learning', 'tool']",NHLBI,UNIVERSITY OF MASSACHUSETTS LOWELL,R01,2019,809552,0.03298865761211533
"Extended Methods and Software Development for Health NLP PROJECT SUMMARY There is a deluge of health-related texts in many genres, from the clinical narrative to newswire and social media. These texts are diverse in content, format, and style, and yet they represent complementary facets of biomedical and health knowledge. Natural Language Processing (NLP) holds much promise to extract, understand, and distill valuable information from these overwhelming large and complex streams of data, with the ultimate goal to advance biomedicine and impact the health and wellbeing of patients. There have been a number of success stories in various biomedical NLP applications, but the NLP methods investigated are usually tailored to one specific phenotype and one institution, thus reducing portability and scalability. Moreover, while there has been much work in the processing of clinical texts, other genres of health texts, like narratives and posts authored by health consumers and patients, are lacking solutions to marshal and make sense of the health information they contain. Robust NLP solutions that answer the needs of biomedicine and health in general have not been fully investigated yet. A unified, data-science approach to health NLP enables the exploration of methods and solutions unprecedented up to now.  Our vision is to unravel the information buried in the health narratives by advancing text-processing methods in a unified way across all the genres of texts. The crosscutting theme is the investigation of methods for health NLP (hNLP) made possible by big data, fused with health knowledge. Our proposal moves the field into exploring semi-supervised and fully unsupervised methods, which only succeed when very large amounts of data are leveraged and knowledge is injected into the methods with care. Our hNLP proposal also targets a key challenge of current hNLP research: the lack of shared software. We seek to provide a clearinghouse for software created under this proposal, and as such all developed tools will be disseminated. Starting from the data characteristics of health texts and information needs of stakeholders, we will develop and evaluate methods for information extraction, information understanding. We will translate our research into the publicly available NLP software platform cTAKES, through robust modules for extraction and understanding across all genres of health texts. We will also demonstrate impact of our methods and tools through several use cases, ranging from clinical point of care to public health, to translational and precision medicine, to participatory medicine. Finally, we will disseminate our work through community activities, such as challenges to advance the state of the art in health natural language processing. PROJECT NARRATIVE  There is a deluge of health texts. Natural Language Processing (NLP) holds much promise to unravel valuable information from these large data streams with the goal to advance medicine and the wellbeing of patients. We will advance state-of-the-art NLP by designing robust, scalable methods that leverage health big data, demonstrating relevance on high-impact use cases, and disseminating NLP tools for the research community and public at large.",Extended Methods and Software Development for Health NLP,9607596,R01GM114355,"['Apache', 'Benchmarking', 'Big Data', 'Big Data Methods', 'Caring', 'Characteristics', 'Clinical', 'Communication', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Event', 'Foundations', 'Goals', 'Gold', 'Health', 'Information Resources', 'Institution', 'International', 'Investigation', 'Knowledge', 'Knowledge Extraction', 'Link', 'Literature', 'Marshal', 'Medicine', 'Methods', 'Names', 'Natural Language Processing', 'Ontology', 'Patients', 'Personal Satisfaction', 'Phenotype', 'Philosophy', 'Public Health', 'Research', 'Semantics', 'Solid', 'Standardization', 'Stream', 'Supervision', 'System', 'Terminology', 'Text', 'Translating', 'Translational Research', 'Vision', 'Work', 'commercialization', 'design', 'health knowledge', 'improved', 'information organization', 'method development', 'novel', 'point of care', 'portability', 'precision medicine', 'programs', 'social media', 'software development', 'success', 'syntax', 'tool', 'translational medicine']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,762619,0.089182480191455
"Exploring the evolving relationship between tobacco, marijuana and e-cigarettes Abstract The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana products (respectively). In order to understand this changing landscape we need new, ﬂexible, and responsive research methods capable of rapidly providing insights into product initiation patterns, use patterns, and cessation strategies. Social media — here deﬁned as including internet discussion forums — provides a ready-made source of abundant, naturalistic, longitudinal, publicly accessible, ﬁrst-person narratives with which to understand health behaviours and attitudes. We propose to use a combination of qualitative methods and automated natural language processing techniques to investigate online discussion forums devoted to tobacco, marijuana, and e-cigarettes in order to understand user trajectories through the three product categories. PROJECT NARRATIVE The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana (respectively). In order to make sense of this rapidly changing landscape, we need new, ﬂexible, and responsive research methods capable of providing insights into tobacco, marijuana, and e- cigarette product use patterns. We propose to use a combination of qualitative and automated natural language processing techniques to investigate online discussion forums related to tobacco, marijuana, and e-cigarettes in order to better understand user trajectories through these different product classes.","Exploring the evolving relationship between tobacco, marijuana and e-cigarettes",9788381,R21DA043775,"['Adolescent and Young Adult', 'Adult', 'Age', 'Algorithms', 'Attitude to Health', 'Categories', 'Chronic Bronchitis', 'Code', 'Consumption', 'Data', 'Data Science', 'Devices', 'Educational Status', 'Electronic cigarette', 'Health', 'Health behavior', 'High School Student', 'Individual', 'Internet', 'Manuals', 'Marijuana', 'Modeling', 'Multiple Marriages', 'Natural Language Processing', 'Pattern', 'Persons', 'Population', 'Qualitative Methods', 'Reporting', 'Research', 'Research Methodology', 'Resources', 'Role', 'Sampling', 'Smoking', 'Source', 'Surgeon', 'Techniques', 'Therapeutic', 'Tobacco', 'Tobacco use', 'Training', 'Work', 'base', 'cigarette smoking', 'combustible cigarette', 'electronic cigarette use', 'flexibility', 'high school', 'innovation', 'insight', 'man', 'marijuana use', 'nicotine replacement', 'smoking cessation', 'social media']",NIDA,UNIVERSITY OF UTAH,R21,2019,225147,0.000471133658427575
"Investigating the documentation of E-cigarette use in the VA EHR PROJECT SUMMARY Electronic cigarettes were developed in China in the early 2000s and first introduced to the US market in 2007. Once established in the US, the product experienced explosive growth, with the number of electronic cigarette users doubling every year between 2008 and 2012. In 2012, it was estimated that 75% of US adults had heard of electronic cigarettes, and 8% had tried them. While electronic cigarettes have been studied over the last sev- eral years, no scientific consensus has emerged regarding either the safety of electronic cigarettes, or their po- tential as a smoking cessation aid. With this proposal, we will investigate how electronic cigarette use is documented in the Veterans Association Electronic Health Record, focusing specifically on the relationship between electronic cigarette use and com- bustible tobacco use, with the goal of understanding both how electronic cigarette use is documented in the context of the United States’ only nationwide health system, and how electronic cigarette related information can be reliably extracted from narrative clinical text using fully automated Natural Language Processing meth- ods. PROJECT NARRATIVE The proposed research focuses on the use of Natural Language Processing methods to automatically extract mentions of electronic cigarette use from the Veterans Association Electronic Health Record. The research will provide insight into important, currently unresolved questions regarding how clinicians record electronic cigarette use in the context of a nationwide health system, and whether patients report the use of electronic cigarettes as a smoking cessation aid or use the devices in conjunction with combustible tobacco.",Investigating the documentation of E-cigarette use in the VA EHR,9652537,R03DA047577,"['Address', 'Adult', 'Algorithms', 'American', 'Area', 'China', 'Cities', 'Clinical', 'Consensus', 'Dangerousness', 'Data', 'Data Set', 'Development', 'Devices', 'Documentation', 'Electronic Health Record', 'Electronic Nicotine Delivery Systems', 'Electronic cigarette', 'Environment', 'Epidemiology', 'Evaluation', 'Geography', 'Goals', 'Government', 'Growth', 'Health', 'Health system', 'Healthcare Systems', 'Hearing', 'Individual', 'Methods', 'Natural Language Processing', 'Patients', 'Pattern', 'Professional Organizations', 'Public Health', 'Public Health Applications Research', 'Reporting', 'Research', 'Risk', 'Safety', 'Scheme', 'Smoking', 'Sodium Chloride', 'Source', 'Structure', 'Technology', 'Text', 'Tobacco', 'Tobacco use', 'Tweens', 'United States', 'Universities', 'Utah', 'Variant', 'Veterans', 'Work', 'authority', 'electronic cigarette use', 'electronic cigarette user', 'electronic hookah', 'evidence base', 'experience', 'information model', 'innovation', 'insight', 'men', 'smoking cessation', 'success', 'systems research', 'tobacco control', 'tool', 'vaping']",NIDA,UNIVERSITY OF UTAH,R03,2019,76250,-0.013386179314306818
"Improving Cardiovascular Drug Safety With Automated Bleeding Classification PROJECT SUMMARY Atrial fibrillation (AF) treatment often includes drug therapy with oral anticoagulants (OAC) to prevent stroke. Bleeding, however, is a common complication of these drugs, affecting up to one in four patients. The Center for Medicare and Medicaid Services recently prioritized OAC-related drug safety as a key quality measure. Currently, however, no method exists to accurately identify bleeding events and severity in large populations. Prior methods use diagnoses codes, which lack sensitivity and clinical detail, or manual chart review, which cannot be implemented in large populations. The proposed research aims address this knowledge gap by applying a natural language processing (NLP)-based approach to identify bleeding events and classify severity in a real-world AF population. The tools will be validated in patients treated at a different institution, to ensure reproducibility across provider settings. In addition, we will apply the bleeding classification tool to evaluate the association between bleeding severity and mortality. Dr. Shah is an emerging young investigator whose career development plan is focused on acquiring the biomedical informatics skills to needed to accurately identify and reduce patient harm. Her training plan focuses on learning core competencies in natural language processing, with the goal of turning the wealth of data in the electronic medical record into useable knowledge. She will combine mentorship from established experts and targeted coursework to acquire skills in biomedical informatics, data science, advanced analytic methods, and research leadership. Completion of these research and training aims will create a platform for future R01 proposals by: (i) enabling safety focused comparative effectiveness research in AF (ii) setting the stage to identify bleeding complications in other cardiovascular diseases and (iii) developing a skill set that allows leadership of a multidisciplinary research team. Through this career development plan, Dr. Shah will build upon her prior training in clinical cardiology and research methodology and lay a strong foundation for a high impact research career. PUBLIC HEALTH RELEVANCE Atrial fibrillation affects six million US adults, and the prevalence is expected to double by 2030. As new drug and device treatments for stroke prevention emerge, patients and providers increasingly need accurate methods to monitor and improve patient safety.",Improving Cardiovascular Drug Safety With Automated Bleeding Classification,9670145,K08HL136850,"['Accounting', 'Address', 'Adult', 'Adverse event', 'Affect', 'Age', 'Algorithms', 'Anticoagulants', 'Atrial Fibrillation', 'Biomedical Research', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Code', 'Competence', 'Complication', 'Computer Assisted', 'Computerized Medical Record', 'Data', 'Data Science', 'Development Plans', 'Devices', 'Diagnosis', 'Ensure', 'Event', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Hemorrhage', 'Incidence', 'Institution', 'Interdisciplinary Study', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Letters', 'Link', 'Manuals', 'Measures', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Oral', 'Patients', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Pragmatic clinical trial', 'Prevalence', 'Provider', 'Quality of life', 'Race', 'Records', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Research Training', 'Risk', 'Safety', 'Severities', 'Site', 'Standardization', 'Stroke', 'Stroke prevention', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'United States Centers for Medicare and Medicaid Services', 'Utah', 'Validation', 'Vital Status', 'acute coronary syndrome', 'aging population', 'analytical method', 'base', 'biomedical informatics', 'career', 'career development', 'clinically relevant', 'cohort', 'comparative effectiveness', 'effectiveness research', 'improved', 'interdisciplinary approach', 'medication compliance', 'medication safety', 'mortality', 'neglect', 'novel', 'novel therapeutics', 'patient safety', 'personalized medicine', 'portability', 'public health relevance', 'screening', 'sex', 'skills', 'stroke risk', 'stroke therapy', 'tool']",NHLBI,UNIVERSITY OF UTAH,K08,2019,163080,0.015669787272953595
"Epidemiology and clinical outcomes of diabetic macular edema Approximately 25% of the millions of veterans (est. 8.92 million FY 2013) enrolled for care in Veterans Health Administration (VHA) have diabetes mellitus, and diabetic macular edema (DME) is the leading cause of vision loss in the adult diabetic population world-wide. Although diabetic retinopathy has been well-studied, comparatively little is known about the burden of DME. In fact, only two national prevalence studies and no national study on the incidence of DME in persons with type 2 diabetes have been conducted. Similarly many risk factors have been characterized for DR, but no large studies have established predictors for DME.  Beyond the Medicare claims database, the VHA National Patient Care Database (NPCD) contains standardized administrative data for several aspects of patient care including diagnoses, procedures, medications, lab test results, vital signs, clinical text notes, and mortality. Because the VA uses teleretinal screening as routine clinical care for all patients with diabetes with these results included in the NPCD, the NPCD is an ideal source for studying the epidemiology of and risk factors for DME.  This study proposes to determine the burden of diabetic macular edema, establish risk factors, and examine treatment outcomes in a previously extracted dataset on 1.98 million veterans who have undergone diabetic retinopathy screening at least once since 2004. Currently invaluable ophthalmic data are encoded in unstructured clinical encounter notes in the Computerized Patient Record System (CPRS), and no validated automated extraction method exists to capture these data elements. An automated extraction method using natural language processing will be created and validated to unlock key ophthalmic variables. These text extraction methods will be applicable to extracting ophthalmology data from not only notes of patients with DME but also any ophthalmology clinical note. This will enable future large scale studies in ophthalmology using NPCD and be immediately valuable to the research community at large.  The candidate, Dr. Aaron Lee, MD MSCI, is an ophthalmologist with subspecialty training in retina surgery with a strong background in computer science and epidemiology. His career goal is to become an independent clinician scientist studying diabetic eye disease with large-scale electronic medical record extracted data. While he possesses the foundational skills, he seeks to gain training in advanced statistics and natural language processing to unlock the data captured in unstructured clinical encounter notes. He has assembled an outstanding mentorship team under the primary mentor, Dr. Edward Boyko, MD MPH. This mentorship team includes renowned experts in clinical epidemiology, health informatics, ophthalmology, and natural language processing. This K23 will provide Dr. Lee the structured coursework, mentorship, and applied learning needed to acquire new research skills. He will leverage key local resources to carry out the proposed research at the University of Washington and the VA Seattle Epidemiologic Research and Information Center. Despite the significant visual loss associated with diabetic macular edema, little is known about the frequency of its occurrence, its risk factors, and the real-world effectiveness of existing treatments. The purpose of this proposed research is to utilize the VA National Patient Care Database to extract relevant data elements to examine these three clinical questions: 1) what is the incidence and prevalence of diabetic macular edema, 2) what are the risk factors associated with its development, and 3) what is the comparative real-world effectiveness of its treatments, including intravitreal anti-VEGF therapy, intravitreal corticosteroid therapy and macular laser. The methods developed in this research proposal will not only further our understanding of DME but also generalize and enable future large-scale ophthalmic studies.",Epidemiology and clinical outcomes of diabetic macular edema,9756410,K23EY029246,"['Adopted', 'Adrenal Cortex Hormones', 'Adult', 'Affect', 'Age', 'Algorithms', 'Anemia', 'Blindness', 'Cardiovascular Diseases', 'Caring', 'Cataract Extraction', 'Clinic', 'Clinical', 'Clinical Data', 'Code', 'Communities', 'Computerized Medical Record', 'Computerized Patient Records', 'Data', 'Data Element', 'Data Set', 'Databases', 'Development', 'Diabetes Mellitus', 'Diabetic Retinopathy', 'Diagnosis', 'Dyslipidemias', 'Effectiveness', 'Enrollment', 'Epidemiology', 'Ethnic Origin', 'Exclusion Criteria', 'Expressed Sequence Tags', 'Eye', 'Eye diseases', 'Foundational Skills', 'Frequencies', 'Future', 'Goals', 'Handedness', 'Hypertension', 'Incidence', 'Information Centers', 'Injections', 'Intervention', 'Lasers', 'Lead', 'Light Coagulation', 'Manuals', 'Masks', 'Measures', 'Medicare claim', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Natural Language Processing', 'Non-Insulin-Dependent Diabetes Mellitus', 'Operative Surgical Procedures', 'Ophthalmologist', 'Ophthalmology', 'Outcome', 'Pathology', 'Patient Care', 'Patients', 'Pattern', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Phenotype', 'Physicians', 'Population', 'Prediction of Response to Therapy', 'Prevalence', 'Prevalence Study', 'Procedures', 'Protocols documentation', 'Public Health Informatics', 'Randomized Controlled Trials', 'Regimen', 'Research', 'Research Proposals', 'Resources', 'Retina', 'Retinal', 'Risk Factors', 'Scientist', 'Severities', 'Sleep Apnea Syndromes', 'Smoking Status', 'Source', 'Standardization', 'Structure', 'System', 'Test Result', 'Text', 'Time', 'Training', 'Treatment Protocols', 'Treatment outcome', 'Universities', 'Use Effectiveness', 'Validation', 'Vascular Endothelial Growth Factors', 'Veterans', 'Visual', 'Visual Acuity', 'Washington', 'bevacizumab', 'career', 'clinical care', 'clinical epidemiology', 'cohort', 'comparative', 'computer science', 'diabetes management', 'diabetic', 'epidemiology study', 'hands-on learning', 'health administration', 'intravitreal injection', 'kidney dysfunction', 'laser photocoagulation', 'macula', 'macular edema', 'mortality', 'proliferative diabetic retinopathy', 'screening', 'skills', 'statistics', 'therapy outcome', 'traditional therapy', 'treatment optimization']",NEI,UNIVERSITY OF WASHINGTON,K23,2019,233078,0.0020010652529038066
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk. PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9665255,R01AI117011,"['Animals', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease Surveillance', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Infrastructure', 'Knowledge', 'Link', 'Literature', 'Location', 'Manuals', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'Zoonoses', 'improved', 'information model', 'interest', 'journal article', 'molecular sequence database', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2019,461012,0.046724346907224205
"Collaborative Research: Statistical algorithms for anomaly  detection and patterns recognition in patient care and safety event reports     Medical errors have been shown to be the third leading cause of death in the United States. The Institute of Medicine and several state legislatures have recommended the use of patient safety event reporting systems (PSRS) to better understand and improve safety hazards. Numerous healthcare providers have adopted these systems, which provide a framework for healthcare provlder staff to report patient safety events. Public databases like MAUDE and VAERS have also been created to collect and trend safety events across healthcare systems. A patient safety event (PSE) report generally consists of both structured and unstructured data elements. Structured data are pre-defined, fixed fields that solicit specific information about the event. The unstructured data fields generally include a free text field where the reporter can enter a text description of the event. The text descriptions are often a rich data source in that the reporter ls not constrained to limited categories or selection options and is able to freely descrlbe the details of the event. The goal of this project is to develop novel statistical methods to analyze unstructured text like patient safety event reports arising in healthcare, which can lead to significant improvements to patient safety and enable timely intervention strategies. We address three problems: (a) Building realistic and meaningful baseline models for near misses, and detecting systematic deterioration of adverse outcomes relative to such baselines; (b) Understanding critical factors that lead to near misses & quantifying severity of outcomes; and (c) ldentifylng document groups of interest. We will use novel statistical approaches that combine Natural Language Processing with Statistical Process Monitoring, Statistical Networks Analysis, and Spatio-temporal Modeling to build a generalizable toolbox that can address these issues in healthcare. An important advantage of our research team is the involvement of healthcare domain experts and access to frontline staff, and we will leverage this strength to develop our algorithms. A key feature of our work is the generalizability of our methods, which will be applicable to biomedical documents arising across a remarkable variety of areas, such as patient safety and equipment malfunction reports, electronic health records, adverse drug or vaccine reports, etc. We will also release open source software via R packages & GitHub, which will enable healthcare staff and researchers to execute our methods on their datasets. Estimates of preventable adverse events in healthcare are staggering, despite the frequently cited Institute of Medicine (IOM) report that first brought attention to the problem over ten years ago. Identifying temporal trends and patterns in the data is particularly important to improving patient safety and patient care. Using our algorithms to effectively analyze documents from reporting systems has the potential to dramatically improve the safety and quality of care by exposing possible weaknesses in the care process.",Collaborative Research: Statistical algorithms for anomaly  detection and patterns recognition in patient care and safety event reports    ,9914443,R01LM013309,"['Address', 'Adopted', 'Adverse event', 'Algorithms', 'Area', 'Attention', 'Caring', 'Categories', 'Cause of Death', 'Computer software', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Deterioration', 'Electronic Health Record', 'Equipment Malfunction', 'Event', 'Goals', 'Health Personnel', 'Healthcare', 'Healthcare Systems', 'Institute of Medicine (U.S.)', 'Instruction', 'Interest Group', 'Intervention', 'Lead', 'Medical Errors', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Outcome', 'Pathway Analysis', 'Patient Care', 'Pattern', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Process', 'Quality of Care', 'Report (document)', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Safety', 'Severities', 'Statistical Algorithm', 'Statistical Computing', 'Statistical Methods', 'Structure', 'System', 'Text', 'Time', 'Time trend', 'United States', 'Vaccines', 'Work', 'adverse outcome', 'hazard', 'improved', 'novel', 'open source', 'patient safety', 'spatiotemporal', 'trend']",NLM,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2019,279001,0.01607990587620123
"Dynamic learning for post-vaccine event prediction using temporal information in VAERS Project Summary Vaccines have been one of the most successful public health interventions to date. They are, however, pharmaceutical products that carry risks. Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine- preventable illnesses. The CDC/FDA Vaccine Adverse Event Reporting System (VAERS) contains up to 30,000 reports per year over the past 25 years. VAERS reports include both structured data (e.g., vaccination date, first onset date, age, and gender) and unstructured narratives that often provide detailed clinical information about the clinical events and the temporal relationship of the series of event occurrences post vaccination. The structured data only provide one onsite date whereas temporal information of the sequence of events post vaccination is contained in the unstructured narratives. Current status –While structured data in the VAERS are widely used, the narratives are generally ignored because of the challenges inherent in working with unstructured data. Without these narratives, potentially valuable information is lost. Goals - In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Specifically, built upon the state-of-art ontology and natural language processing technologies, we will develop and validate a Temporal Information Modeling, Extraction and Reasoning system for Vaccine data (TIMER-V), which will automatically extract post-vaccination events and their temporal relationships from VAERS reports, semantically infer temporal relations, and integrate the exacted unstructured data with the structured data. Furthermore, we will provide and maintain a publicly available data access interface to query the new integrated data repository, which will facilitate vaccine safety research, casual inference, and other temporal related discovery. We will also develop and validate models to predict severe AEs using the co-occurrence or temporal patterns of the series of AEs post vaccination. To the best of our knowledge, this is the first attempt to make use of the unstructured narratives in the VAERS reports to facilitate the temporal related discovery to a broad community of investigators in pharmacology, pharmacoepidemiology, vaccine safety research, among others. Project Narrative Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine-preventable illnesses. In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. Currently the FDA/CDC Vaccine Adverse Event Reporting System (VAERS) only includes one onsite date in its database. The textual narratives in the reports are generally ignored primarily due to their unstructured nature. These narratives, however, contain more detailed information about the series of events that happened after vaccination, which could be valuable for more informed clinical studies. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Our new methods, their applications to VAERS database, and their dissemination will facilitate the entire research network for pursuing temporal related discovery with high methodological rigor.",Dynamic learning for post-vaccine event prediction using temporal information in VAERS,9937918,R01AI130460,"['Abbreviations', 'Address', 'Adverse event', 'Age', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Analyses', 'Data Sources', 'Database Management Systems', 'Databases', 'Development', 'Evaluation', 'Event', 'Frequencies', 'Funding', 'Gender', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Informatics', 'Learning', 'Manuals', 'Measles-Mumps-Rubella Vaccine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Pattern', 'Performance', 'Pharmacoepidemiology', 'Pharmacologic Substance', 'Pharmacology', 'Process', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Series', 'Serious Adverse Event', 'Severities', 'Signal Transduction', 'Source', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Validation', 'base', 'data access', 'data warehouse', 'flexibility', 'improved', 'influenza virus vaccine', 'information model', 'novel', 'predictive modeling', 'public health intervention', 'response', 'risk prediction model', 'vaccine safety']",NIAID,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2019,97055,0.02260942934884757
"Dynamic learning for post-vaccine event prediction using temporal information in VAERS Project Summary Vaccines have been one of the most successful public health interventions to date. They are, however, pharmaceutical products that carry risks. Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine- preventable illnesses. The CDC/FDA Vaccine Adverse Event Reporting System (VAERS) contains up to 30,000 reports per year over the past 25 years. VAERS reports include both structured data (e.g., vaccination date, first onset date, age, and gender) and unstructured narratives that often provide detailed clinical information about the clinical events and the temporal relationship of the series of event occurrences post vaccination. The structured data only provide one onsite date whereas temporal information of the sequence of events post vaccination is contained in the unstructured narratives. Current status –While structured data in the VAERS are widely used, the narratives are generally ignored because of the challenges inherent in working with unstructured data. Without these narratives, potentially valuable information is lost. Goals - In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Specifically, built upon the state-of-art ontology and natural language processing technologies, we will develop and validate a Temporal Information Modeling, Extraction and Reasoning system for Vaccine data (TIMER-V), which will automatically extract post-vaccination events and their temporal relationships from VAERS reports, semantically infer temporal relations, and integrate the exacted unstructured data with the structured data. Furthermore, we will provide and maintain a publicly available data access interface to query the new integrated data repository, which will facilitate vaccine safety research, casual inference, and other temporal related discovery. We will also develop and validate models to predict severe AEs using the co-occurrence or temporal patterns of the series of AEs post vaccination. To the best of our knowledge, this is the first attempt to make use of the unstructured narratives in the VAERS reports to facilitate the temporal related discovery to a broad community of investigators in pharmacology, pharmacoepidemiology, vaccine safety research, among others. Project Narrative Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine-preventable illnesses. In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. Currently the FDA/CDC Vaccine Adverse Event Reporting System (VAERS) only includes one onsite date in its database. The textual narratives in the reports are generally ignored primarily due to their unstructured nature. These narratives, however, contain more detailed information about the series of events that happened after vaccination, which could be valuable for more informed clinical studies. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Our new methods, their applications to VAERS database, and their dissemination will facilitate the entire research network for pursuing temporal related discovery with high methodological rigor.",Dynamic learning for post-vaccine event prediction using temporal information in VAERS,9637319,R01AI130460,"['Abbreviations', 'Address', 'Adverse event', 'Age', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Analyses', 'Data Sources', 'Database Management Systems', 'Databases', 'Development', 'Evaluation', 'Event', 'Frequencies', 'Funding', 'Gender', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Informatics', 'Learning', 'Manuals', 'Measles-Mumps-Rubella Vaccine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Pattern', 'Performance', 'Pharmacoepidemiology', 'Pharmacologic Substance', 'Pharmacology', 'Process', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Series', 'Serious Adverse Event', 'Severities', 'Signal Transduction', 'Source', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Validation', 'base', 'data access', 'data warehouse', 'flexibility', 'improved', 'influenza virus vaccine', 'information model', 'novel', 'predictive modeling', 'public health intervention', 'response', 'risk prediction model', 'vaccine safety']",NIAID,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2019,619389,0.02260942934884757
"Community Surveillance of Coronary Heart Disease Project Summary/Abstract  Preventing the onset of acute myocardial infarction (AMI) and its recurrence, and reducing the morbidity and mortality associated with AMI, remain of significant public health and clinical concern. Monitoring contemporary trends in AMI incidence, treatment, and in-hospital and long-term outcomes is of considerable importance given periodic national updates of treatment guidelines, emphasis on reducing hospital readmissions, and revised definitions and classifications of AMI. Continuously supported by the NHLBI, we have conducted more than 35 years of population-based surveillance of AMI incidence and attack rates, hospital management practices, and the in-hospital and long-term prognosis associated with AMI among residents of central MA hospitalized at all central MA medical centers. We have a highly experienced team of cardiologists, epidemiologists, clinical informatics, and health services researchers who will build on multi- decade long trends (1975-2011) in our principal study endpoints examined previously in this study to the two new study years of patients hospitalized with AMI at all central MA medical centers in 2014 and 2017.  To sustain our efforts into the era of electronic medical records (EMRs), and after implementation of the ICD-10 system in 2015, we will develop a new automated AMI surveillance system that efficiently utilizes EMRs by taking advantage of state-of-art natural language processing (NLP) methods that will be compatible with ICD-10 (Aim 1). We will use the new NLP method to streamline traditional chart review-based collection of socio-demographic, clinical, treatment, and hospital and post-discharge outcomes data in patients hospitalized with AMI at all 11 central MA medical centers in 2014 and 2017. The data extracted from NLP-streamlined chart reviews will be used to validate and refine the NLP system. Issues related to changes from ICD-9 to ICD- 10 will be carefully addressed. The new NLP-enriched EMR-based surveillance system will eventually be implemented in all participating central MA hospitals. Using the NLP-enriched and EMR-based surveillance data, we will monitor the contemporary clinical epidemiology of AMI, and out-of-hospital deaths due to coronary disease, and changing landscape, over a more than 40 year period (1975-2017) (Aim 2).  The new EMR-based and NLP-enriched system will enhance the population-based surveillance of acute coronary disease. This new system will be cost-effective, more efficient and near-real time, have greater accuracy and precision, and can be readily updated to accommodate changes in information technologies and broadly applicable to other hospital systems. It will support our continued efforts to provide unique community- based observational data on several populations that are often excluded from clinical trials, and that are increasing in numbers, namely the elderly and patients with multiple morbidities. Furthermore, it will generate critical data to inform more national clinical guidelines on the enhanced prevention and management of AMI. If successful, the system can serve as a model and be implemented statewide in MA and elsewhere in the US. Project Narrative  The results of the proposed community-based study will provide data about 40 year trends with regards to the changing magnitude of, and outcomes associated with, heart attacks in residents of a large central New England community. The results of this investigation will also provide contemporary insights on how patients who experience heart attacks in the community are treated by physicians.",Community Surveillance of Coronary Heart Disease,9615037,R01HL135219,"['Acute', 'Acute myocardial infarction', 'Address', 'Adoption', 'American', 'Area', 'Big Data', 'Caring', 'Case Fatality Rates', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Informatics', 'Clinical Treatment', 'Clinical Trials', 'Code', 'Collection', 'Communities', 'Computerized Medical Record', 'Coronary', 'Coronary heart disease', 'Data', 'Development', 'Diagnostic', 'Disease Surveillance', 'Epidemiologist', 'Fostering', 'Future', 'Guidelines', 'Harvest', 'Health Care Costs', 'Health Policy', 'Health Services', 'Hospital Administration', 'Hospitals', 'ICD-9', 'Impairment', 'Incidence', 'Information Technology', 'Inpatients', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Investigation', 'Life Expectancy', 'Manuals', 'Medical', 'Medical center', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Myocardial Infarction', 'Myocardial Reperfusion', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'New England', 'Outcome', 'Participant', 'Patients', 'Periodicity', 'Physicians', 'Policy Maker', 'Population', 'Population Study', 'Practice Management', 'Prevention', 'Public Health', 'Quality of life', 'Recurrence', 'Research', 'Research Personnel', 'Research Support', 'Rural', 'Secondary Prevention', 'Source', 'Symptoms', 'System', 'Technology', 'Time', 'Update', 'Work', 'base', 'clinical epidemiology', 'clinically relevant', 'cost effective', 'electronic data', 'experience', 'functional disability', 'health disparity', 'high risk population', 'hospital readmission', 'innovation', 'insight', 'metropolitan', 'mortality', 'older patient', 'optimal treatments', 'outcome forecast', 'population based', 'prevent', 'sociodemographics', 'socioeconomics', 'surveillance data', 'treatment guidelines', 'trend']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2019,784820,0.02135866565566336
"Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign Project Summary/Abstract Written The objective of the proposed research is to reduce the patient safety hazards associated with electronic medication administration records (eMARs) by, (1) understanding current usability and safety gaps, and (2) creating design and development documents, wireframes, and prototypes to serve as the foundation for future eMARs that will eliminate these gaps. In particular, we focus on communication and information flow challenges between nurses, pharmacists, and physicians during medication administration and use of the eMAR. The proposed research is in direct response to special emphasis notice (NOT-HS-16-009). We will develop a broad understanding of usability and safety hazards associated with eMARs by analyzing a large dataset of 1.7 million patient safety event reports and detailed medication error related narratives. We will then conduct heuristic analyses of current eMARs, and interviews and observations of physicians, nurses, and pharmacists. These data will serve to inform the development of eMAR design documents, wireframes, and prototypes as the foundation for future development. This project utilizes the extensive expertise of the research team in human factors and safety science, health information technology (health IT), informatics, and data science. Our research team includes physicians, nurses, pharmacists, and human factors engineers, and experts in natural language processing. In addition, our partnerships include a patient safety organization and a health IT vendor. The proposal addresses fundamental aspects of the call for proposals by providing new insights on the safety of health IT and improves current practices by developing use cases and new prototypes for immediate use by healthIT vendors. Contributions from this research will include a fundamental understanding of the role of health IT during medication administration with a focus on communication and information flow, design, development and testing documents for vendors and providers, and eMAR wireframes and prototypes to improve development. Our research will also provide organizations like the Office of the National Coordinator with medication related test scenarios to assess current health IT systems. Project Narrative This project is relevant to public health because it applies the sciences of human factors and informatics to improve the usability and safety of the electronic medication administration records (eMARs), which will ultimately improve patient care. Patient safety event report data will be analyzed to identify usability and safety hazards in eMARs, and interviews and observations will be conducted to identify clinician needs. Based on this knowledge eMAR design documents, wireframes, and prototypes will be developed, tested, and disseminated with the goal of improving communication and information flow to reduce medication related errors.",Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign,9731456,R01HS025136,[' '],AHRQ,MEDSTAR HEALTH RESEARCH INSTITUTE,R01,2019,399999,0.029230258671862613
"Natural language processing for characterizing psychopathology ﻿    DESCRIPTION (provided by applicant):  Convergent genetic and epidemiologic evidence suggests the importance of understanding psychiatric illness from a dimensional rather than solely a categorical perspective. The limitations of traditional diagnostic categories motivated a major NIMH-supported effort to identify measures of psychopathology that more closely align with underlying disease biology.  At present, however, the available large clinical data sets, whether health claims, registries, or electronic health records, do not include such dimensional measures. Even with the integration of structure clinician and patient-reported outcomes, generating such cohorts could require a decade or more. Moreover, coded data does not systematically capture clinically-important concepts such as health behaviors or stressors.  While such cohorts are developed, natural language processing can facilitate the application of existing electronic health records to enable precision medicine in psychiatry. Specifically, while traditional natural language tools focus on extracting individual terms, emerging methods including those in development by the investigators allow extraction of concepts and dimensions.  The present investigation proposes to develop a toolkit for natural language processing of narrative patient notes to extract measures of psychopathology, including estimated RDoC domains. In preliminary investigations in a large health system, these tools have demonstrated both face validity and predictive validity. This toolkit also allows extraction o complex concepts from narrative notes, such as stressors and health behaviors.  In the proposed study, these natural language processing tools will be applied to a large psychiatric inpatient data set as well as a large general medical inpatient data set, to derive measures of psychopathology and other topics. The resulting measures will then be used in combination with coded data to build regression and machine-learning-based models to predict clinical outcomes including length of hospital stay and risk of readmission. The models will then be validated in independent clinical cohorts.  By combining expertise in longitudinal clinical investigation, natural language processing, and machine learning, the proposed study brings together a team with the needed skills to develop a critical toolkit for understanding health records dimensionally The resulting models can be applied to facilitate investigation of dimensions of psychopathology and related topics, allowing stratification of clinical risk to enable development of targeted interventions. PUBLIC HEALTH RELEVANCE:  Public health significance many aspects of psychiatric illness are not adequately captured by diagnostic codes. This study will apply natural language processing and machine learning to electronic health records from large health systems. The resulting symptom dimensions will allow better stratification of risk for clinically-important outcomes, including prolonged hospital stays and early readmissions.",Natural language processing for characterizing psychopathology,9445485,R01MH106577,"['Admission activity', 'Antidepressive Agents', 'Applaud', 'Area', 'Back', 'Biology', 'Categories', 'Clinical', 'Clinical Data', 'Clinical stratification', 'Code', 'Complex', 'DSM-IV', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electronic Health Record', 'Epidemiology', 'Face', 'Genetic', 'Health', 'Health behavior', 'Health system', 'Healthcare Systems', 'Hospitals', 'Individual', 'Inpatients', 'Intervention', 'Interview', 'Investigation', 'Length of Stay', 'Machine Learning', 'Measures', 'Medical', 'Mental Depression', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Moods', 'National Institute of Mental Health', 'Natural Language Processing', 'New England', 'Outcome', 'Patient Outcomes Assessments', 'Patients', 'Penetration', 'Pharmaceutical Preparations', 'Psychiatric Diagnosis', 'Psychiatry', 'Psychopathology', 'Public Health', 'Registries', 'Reporting', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Risk', 'Risk stratification', 'Severities', 'Structure', 'Symptoms', 'System', 'Text', 'United States National Academy of Sciences', 'Work', 'base', 'clinical investigation', 'clinical risk', 'clinically relevant', 'cohort', 'cost', 'health data', 'health record', 'hospital readmission', 'improved', 'natural language', 'neuropsychiatric symptom', 'novel', 'outcome prediction', 'patient subsets', 'precision medicine', 'predict clinical outcome', 'public health relevance', 'readmission risk', 'skills', 'stressor', 'success', 'terabyte', 'tool', 'translational scientist', 'trend']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,R01,2018,376490,0.019263596652595357
"Automated domain adaptation for clinical natural language processing Project Summary Automatic extraction of useful information from clinical texts enables new clinical research tasks and new technologies at the point of care. The natural language processing (NLP) systems that perform this extraction rely on supervised machine learning. The learning process uses manually labeled datasets that are limited in size and scope, and as a result, applying NLP systems to unseen datasets often results in severely degraded performance. Obtaining larger and broader datasets is unlikely due to the expense of the manual labeling process and the difficulty of sharing text data between multiple different institutions. Therefore, this project develops unsupervised domain adaptation algorithms to adapt NLP systems to new data. Domain adaptation describes the process of adapting a machine learning system to new data sources. The proposed methods are unsupervised in that they do not require manual labels for the new data. This project has three aims. The first aim makes use of multiple existing datasets for the same task to study the differences in domains, and uses this information to develop new domain adaptation algorithms. Evaluation uses standard machine learning metrics, and analysis of performance is tightly bounded by strong baselines from below and realistic upper bounds, both based on theoretical research on machine learning generalization. The second aim develops open source software tools to simplify the process of incorporating domain adaptation into clinical text processing workflows. This software will have input interfaces to connect to methods developed in Aim 1 and output interfaces to connect with Apache cTAKES, a widely used open- source NLP tool. Aim 3 tests these methods in an end-to-end use case, adverse drug event (ADE) extraction on a dataset of pediatric pulmonary hypertension notes. ADE extraction relies on multiple NLP systems, so this use case is able to show how broad improvements to NLP methods can improve downstream methods. This aim also creates new manual labels for the dataset for an end-to-end evaluation that directly measures how improvements to the NLP systems lead to improvement in ADE extraction. Project Narrative Software systems that use machine learning to understand clinical text often suffer severe performance loss when they are applied to new data that looks different than the data that they originally learned from. In this project, we develop and implement methods that allow these systems to automatically adapt to the characteristics of a new data source. We evaluate these methods on the clinical research task of adverse drug event detection, which relies on many different variables found in the text of electronic health records.",Automated domain adaptation for clinical natural language processing,9579181,R01LM012918,"['Adult', 'Adverse drug event', 'Adverse effects', 'Algorithms', 'Apache', 'Area', 'Biological Neural Networks', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Informatics', 'Clinical Research', 'Colon Carcinoma', 'Communities', 'Computer software', 'Computers', 'Conceptions', 'Data', 'Data Set', 'Data Sources', 'Detection', 'Dimensions', 'Ecosystem', 'Educational process of instructing', 'Electronic Health Record', 'Evaluation', 'Human', 'Institution', 'Knowledge', 'Label', 'Language', 'Lead', 'Learning', 'Linguistics', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Manuals', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Network-based', 'Output', 'Pathology', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Pulmonary Hypertension', 'Radiology Specialty', 'Research', 'Software Tools', 'Source', 'Statistical Models', 'Structure', 'Supervision', 'System', 'Testing', 'Text', 'TimeLine', 'Training', 'Update', 'Vision', 'Work', 'base', 'case finding', 'improved', 'learning strategy', 'malignant breast neoplasm', 'method development', 'natural language', 'new technology', 'news', 'novel', 'open source', 'point of care', 'social media', 'software systems', 'statistics', 'tool', 'tumor', 'unsupervised learning']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2018,416066,0.057781738596264835
"Resource Curation and Evaluation for EHR Note Comprehension Title: Resource Curation and Evaluation for EHR Note Comprehension The American Diabetes Association estimates that over 25.8 million Americans have diabetes. Proper patient self-management is perhaps the most critical and under-exercised element for patients to achieve appropriate glycemic control and thus mitigate complications and comorbid conditions, and implement appropriate preventive strategies (e.g., vaccines, exercise, healthy diet). In previous work we have shown that educating patients by using content in their medical records improved diabetes outcomes and that NoteAid, a multi- module natural language processing system that links medical jargon in electronic health records to definitions, has improved self-reported comprehension. Here we propose to develop NoteAid with a high-quality resource of expert-curated lay definitions and evaluation methods for diabetes patients’ note comprehension. ! ! Title: Resource Curation and Evaluation for EHR Note Comprehension The American Diabetes Association estimates that over 25.8 million Americans have diabetes. In previous work we have shown that educating patients by using content in their medical records improved diabetes outcomes and that NoteAid, a natural language processing system that links medical jargon in electronic health records to definitions, has improved self-reported comprehension. Here we propose to develop NoteAid with a high-quality resource of expert-curated lay definitions and evaluation methods for diabetes patients’ note comprehension. ! !",Resource Curation and Evaluation for EHR Note Comprehension,9670540,R01LM012817,"['AIDS education', 'Adult', 'Age', 'American', 'Artificial Intelligence', 'Attitude', 'Back', 'Behavior', 'Caregivers', 'Caring', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Comorbidity', 'Complex', 'Comprehension', 'Country', 'Data Science', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Elements', 'Ethnic Origin', 'Evaluation', 'Exercise', 'Eye diseases', 'General Population', 'Health', 'Healthcare', 'Heart Diseases', 'Hospitals', 'Individual', 'Informatics', 'Internet', 'Intervention', 'Kidney Diseases', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Link', 'Machine Learning', 'Massachusetts', 'Measurement', 'Mechanics', 'Medical', 'Medical Records', 'Medical Students', 'Methods', 'Modeling', 'Natural Language Processing', 'Nursing Students', 'Ontology', 'Outcome', 'Patient Education', 'Patient Self-Report', 'Patients', 'Physicians', 'Prevention strategy', 'Questionnaires', 'Race', 'Randomized', 'Reading', 'Resources', 'Risk', 'Self Management', 'Speed', 'Supervision', 'System', 'Techniques', 'Testing', 'Time', 'Trust', 'United States National Institutes of Health', 'Universities', 'Vaccines', 'Visual Aid', 'Vocabulary', 'Work', 'base', 'care outcomes', 'clinically relevant', 'cost', 'deep learning', 'design', 'diabetic patient', 'glycemic control', 'good diet', 'improved', 'innovation', 'instrument', 'response', 'sex', 'skills', 'usability']",NLM,UNIVERSITY OF MASSACHUSETTS LOWELL,R01,2018,370291,-0.020906415621700408
"Improving Specialty Care Delivery in the Safety Net with Natural Language Processing Project Summary  Safety net providers treat a substantial share of socioeconomically vulnerable patients in their communities, but struggle to provide timely access to high quality specialty care for their patients. Delayed access to specialty care is associated with worse health outcomes and potentially contributes to health disparities across socioeconomic groups. Given their limited resources, safety net providers must seek creative approaches to improve specialty access. However, to choose what programs to implement, safety net providers need to understand the specialty care needs of their populations. Fortunately, the adoption of eConsult systems by safety net providers across the US provides a valuable opportunity to systematically measure patterns of specialty care referrals for minority, underserved populations.  In this project, we propose using state-of-the-art methods in machine learning and natural language processing (NLP) to help safety net providers extract actionable, population wide data from their electronic consultation systems. We will do this in partnership with three of the most prominent safety net health systems in the US in Los Angeles, San Francisco and New York City. Using specialty request databases from our collaborators, we will build NLP systems to automatically classify specialty requests along two dimensions: the “clinical issue” motivating the request (e.g., chest pain), and the “question type” (e.g., request for a procedure, help with medication management). This automated classification of electronic specialty requests can enable identification of promising targets for interventions to improve specialty access and quality of care.  After developing these NLP systems, we will analyze >1 million specialty requests to describe trends in how safety net patients are referred to specialists and examine variation in referral patterns by clinic and individual provider. The goal is to identify the most impactful opportunities to improve specialty access and quality. For example, a high rate of referrals for esophageal reflux, which most PCPs can treat on their own with specialist guidance, could lead to new treatment algorithms, potentially reducing the need for these requests and improving access for other patients.  This proposal is a “high-risk high-reward” project that creates new research tools to identify and evaluate data-driven interventions to improve specialty care delivery for underserved populations. Project Narrative Access to timely, high-quality specialty care is a fundamental component of a well-functioning health system, yet safety net health care providers face persistent challenges delivering such care. Quality improvement efforts to improve specialty access have been thwarted in part because safety net providers lack the data to understand a basic question – why patients are referred for specialty care. Taking advantage of the growing use of electronic specialty referral systems by safety net providers, we propose using natural language processing to conduct automated analysis and classification of specialty requests in safety net populations, which will enable the design of targeted interventions to improve specialty care access and delivery.",Improving Specialty Care Delivery in the Safety Net with Natural Language Processing,9600732,R21MD012693,"['Acute', 'Adopted', 'Adoption', 'Algorithms', 'Caring', 'Chest Pain', 'Chronic Kidney Failure', 'Classification', 'Clinic', 'Clinical', 'Communities', 'Consultations', 'County', 'Data', 'Data Set', 'Databases', 'Education', 'Epidemiology', 'Face', 'Federally Qualified Health Center', 'Gastroesophageal reflux disease', 'Goals', 'Health', 'Health Personnel', 'Health Services', 'Health Services Accessibility', 'Health system', 'Heart failure', 'Hospitals', 'Improve Access', 'Individual', 'Intervention', 'Lead', 'Los Angeles', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Education', 'Medical center', 'Medication Management', 'Methods', 'Minority', 'Morbidity - disease rate', 'Natural Language Processing', 'New York City', 'Online Systems', 'Ophthalmology', 'Outcome', 'Patients', 'Pattern', 'Play', 'Population', 'Primary Care Physician', 'Procedures', 'Provider', 'Public Hospitals', 'Quality of Care', 'Research', 'Resources', 'Retinal Diseases', 'Role', 'San Francisco', 'Specialist', 'System', 'Taxonomy', 'Telemedicine', 'Text', 'Time', 'Transplantation', 'Triage', 'Underserved Population', 'Variant', 'Visit', 'care delivery', 'design', 'diabetic', 'disease classification', 'ethnic minority population', 'follow-up', 'health disparity', 'high reward', 'high risk', 'improved', 'medical specialties', 'medically underserved', 'minority communities', 'mortality', 'performance tests', 'programs', 'racial and ethnic', 'safety net', 'screening', 'socioeconomic disadvantage', 'socioeconomics', 'tool', 'trend', 'two-dimensional']",NIMHD,BOSTON CHILDREN'S HOSPITAL,R21,2018,278902,0.02947424879488561
"Leveraging Twitter to monitor nicotine and tobacco-related cancer communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to monitor nicotine and tobacco-related cancer communication,9503469,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Stream', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Tobacco-Related Carcinoma', 'Work', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial and ethnic', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2018,505649,0.0036177928919126725
"Open Health Natural Language Processing Collaboratory Project Summary One of the major barriers in leveraging Electronic Health Record (EHR) data for clinical and translational science is the prevalent use of unstructured or semi-structured clinical narratives for documenting clinical information. Natural Language Processing (NLP), which extracts structured information from narratives, has received great attention and has played a critical role in enabling secondary use of EHRs for clinical and translational research. As demonstrated by large scale efforts such as ACT (Accrual of patients for Clinical Trials), eMERGE, and PCORnet, using EHR data for research rests on the capabilities of a robust data and informatics infrastructure that allows the structuring of clinical narratives and supports the extraction of clinical information for downstream applications. Current successful NLP use cases often require a strong informatics team (with NLP experts) to work with clinicians to supply their domain knowledge and build customized NLP engines iteratively. This requires close collaboration between NLP experts and clinicians, not feasible at institutions with limited informatics support. Additionally, the usability, portability, and generalizability of the NLP systems are still limited, partially due to the lack of access to EHRs across institutions to train the systems. The limited availability of EHR data limits the training available to improve the workforce competence in clinical NLP. We aim to address the above challenges by extending our existing collaboration among multiple CTSA hubs on open health natural language processing (OHNLP) to share distributional information of NLP artifacts (i.e., words, n-grams, phrases, sentences, concept mentions, concepts, and text segments) acquired from real EHRs across multiple institutions. We will leverage the advanced privacy-preserving computing infrastructure of iDASH (integrating Data for Analysis, Anonymization, and SHaring) for privacy- preserving data analysis models and will partner with diverse communities including Observational Health Data Sciences and Informatics (OHDSI), Precision Medicine Initiative (PMI), PCORnet, and Rare Diseases Clinical Research Network (RDCRN) to demonstrate the utility of NLP for translational research. This CTSA innovation award RFA provides us with a unique opportunity to address the challenges faced with clinical NLP and through strong partnership with multiple research communities and leadership roles of the research team in clinical NLP, we envision that the successful delivery of this project will broaden the utilization of clinical NLP across the research community. There are four aims planned: i) obtain PHI-suppressed NLP artifacts with retained distribution information across multiple institutions and assess the privacy risk of accessing PHI- suppressed artifacts, ii) generate a synthetic text corpus for exploratory analysis of clinical narratives and assess its utility in NLP tasks leveraging various NLP challenges, iii) develop privacy-preserving computational phenotyping models empowered with NLP, and iv) partner with diverse communities to demonstrate the utility of our project for translational research. Project Narratives The proposed project aims to broaden the secondary use of electronic health records (EHRs) across the research community by combining innovative privacy-preserving computing techniques and clinical natural language processing.",Open Health Natural Language Processing Collaboratory,9547946,U01TR002062,"['Address', 'Algorithms', 'Attention', 'Award', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Detection', 'Disease', 'Electronic Health Record', 'Ensure', 'Familial Hypercholesterolemia', 'Frequencies', 'Health', 'Hepatolenticular Degeneration', 'Individual', 'Informatics', 'Information Distribution', 'Institution', 'Kidney Calculi', 'Knowledge', 'Leadership', 'Learning', 'Measures', 'Medical', 'Meta-Analysis', 'Minnesota', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Natural Language Processing', 'Observational Study', 'Patients', 'Phenotype', 'Play', 'Precision Medicine Initiative', 'Privacy', 'Process', 'Rare Diseases', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rest', 'Risk', 'Role', 'Sampling', 'Security', 'Semantics', 'Site', 'Source', 'Structure', 'System', 'Talents', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Translational Research', 'Universities', 'Work', 'base', 'citizen science', 'cohort', 'collaboratory', 'data registry', 'empowered', 'health data', 'improved', 'indexing', 'individual patient', 'informatics infrastructure', 'innovation', 'interest', 'novel', 'phenotypic data', 'phrases', 'portability', 'preservation', 'recruit', 'statistics', 'tool', 'usability', 'virtual']",NCATS,MAYO CLINIC ROCHESTER,U01,2018,1542081,0.09652376214957706
"Semi-structured Information Retrieval in Clinical Text for Cohort Identification DESCRIPTION (provided by applicant):  Natural Language Processing (NLP) techniques have shown promise for extracting data from the free text of electronic health records (EHRs), but studies have consistently found that techniques do not readily generalize across application settings. Unfortunately, most of the focus in applying NLP to real use cases has remained on a paradigm of single, well-defined application settings, so that generalizability to unseen use cases remains implicitly unaddressed. We propose to explicitly account for unseen application settings by adopting an information retrieval (IR) perspective with the objective of patient-level cohort identification. To do so, we introduce layered language models, an IR framework that enables the reuse of NLP-produced artifacts. Our long term goal is to accelerate investigations of patient health and disease by providing robust, user- centric tools that are necessary to process, retrieve, and utilize the free text of EHRs. The main goal of this proposal is to accurately retrieve ad hoc, realistic cohorts from clinical text at Mayo Clinic and OHSU, establishing methods, resources, and evaluation for patient-level IR. We hypothesize that cohort identification can be addressed in a generalizable fashion by a new IR framework: layered language models. We will test this hypothesis through four specific aims. In Aim 1, we will make medical NLP artifacts searchable in our layered language IR framework. This involves storing and indexing the NLP artifacts, as well as using statistical language models to retrieve documents based on text and its associated NLP artifacts. In Aim 2, we deal with the practical setting of ad hoc cohort identification, moving to patient-level (rather than document-level) IR. To accurately handle patient cohorts in which qualifying evidence may be spread over multiple documents, we will develop and implement patient-level retrieval models that account for cross- document relational and temporal combinations of events. In Aim 3, we will construct parallel IR test collections using EHR data from two sites; a diverse set of cohort queries written by multiple people toward various clinical or epidemiological ends; and assessments of which patients are relevant to which queries at both sites. Finally, in Aim 4, we refine and evaluate patient-level layered language IR on the ad hoc cohort identification task, making comparisons across the users, queries, optimization metrics, and institutions. We will draw additional extrinsic comparisons with pre-existing techniques, e.g., for cohorts from the Electronic Medical Records and Genonmics network. The expected outcomes of the proposed work are: (i) An open-source cohort identification tool, usable by clinicians and epidemiologists, that makes principled use of NLP artifacts for unseen queries; ii) A parallel test collection for cohort identification, includig two intra-institutional document collections, diverse test topics and user-produced text queries, and patient-level judgments of relevance to each query; and (iii) Validation of the reusability of medical NLP via the task of retrieving patient cohorts. PUBLIC HEALTH RELEVANCE:  With the widespread adoption of electronic medical records, one might expect that it would be simple for a medical expert to find things like ""patients in the community who suffer from asthma."" Unfortunately, on top of lab tests, medications, and demographic information, there are observations that a physician writes down as text - which are difficult for a computer to understand. Therefore, we aim to process text so that a computer can understand enough of it, and then search that text along with the rest of a patient's medical record; this will allow clinicians or researchers to find and study patients groups of interest.",Semi-structured Information Retrieval in Clinical Text for Cohort Identification,9534183,R01LM011934,"['Address', 'Adopted', 'Adoption', 'Asthma', 'Clinic', 'Clinical', 'Collection', 'Communities', 'Computerized Medical Record', 'Computers', 'Data', 'Dictionary', 'Disease', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Goals', 'Health', 'Information Retrieval', 'Information Retrieval Systems', 'Institution', 'Interest Group', 'Investigation', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Morphologic artifacts', 'Names', 'Natural Language Processing', 'Ontology', 'Outcome', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Process', 'Publishing', 'Qualifying', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Retrieval', 'Sampling', 'Semantics', 'Site', 'Smoke', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'Weight', 'Work', 'Writing', 'asthmatic patient', 'base', 'cohort', 'improved', 'indexing', 'information model', 'novel', 'open source', 'profiles in patients', 'public health relevance', 'query optimization', 'syntax', 'text searching', 'tool']",NLM,MAYO CLINIC ROCHESTER,R01,2018,387966,0.0519060278373671
"Using advanced natural language processing to facilitate documentation of meaningful use and quality payment compliance PROJECT SUMMARY The Center for Medicare and Medicaid Services Quality Payment Program is designed to motivate healthcare providers to adhere to best practices in clinical healthcare and patient safety. Unfortunately, extracting quality measures data from the clinical record is burdensome and as such, participation among clinical healthcare providers is suboptimal. Our aim is to develop a system to facilitate automatic extraction of quality data. This will reduce the burden of data collection and help remove the barrier to participation that keeps more providers from participating in the program. The proposed project, titled “Using advanced natural language processing to facilitate documentation of meaningful use and quality payment compliance”, aims to develop novel natural language processing methods to recognize key elements from the clinical notes to enable proper documentation of meaningful use and compliance with quality payment. We envision this to be an effective research partnership that leverages the complementary assets of SaferMD, a small business unit, and the University of Michigan, a non-profit research institution, to develop and evaluate a prototype tool to extract clinical quality measures data, and increase participation in the Quality Payment Program. PROJECT NARRATIVE The proposed project, titled “Using advanced natural language processing to facilitate documentation of meaningful use and quality payment compliance”, aims to develop novel natural language processing methods to recognize key elements from the clinical notes to enable proper documentation of meaningful use and compliance with quality payment. The project will develop algorithms to identify fields relevant for quality measures and develop tools to extract and analyze these data elements from large sets of radiology reports. Finally, the proposed work will initiate the extracted measures into existing quality service offerings by SaferMD. Successful completion of this project will advance the tools available for CMS clients to achieve higher adherence and compliance to the quality payment initiatives and help public health officials and policy developers advance the meaningful use of electronic health records.",Using advanced natural language processing to facilitate documentation of meaningful use and quality payment compliance,9677579,R41LM013050,"['Address', 'Adherence', 'Algorithms', 'Benchmarking', 'Businesses', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Data', 'Data Collection', 'Data Element', 'Data Quality', 'Development', 'Disease', 'Documentation', 'Electronic Health Record', 'Elements', 'Experimental Models', 'Funding', 'Goals', 'Guidelines', 'Health Personnel', 'Healthcare', 'Human', 'Incentives', 'Institution', 'Label', 'Leadership', 'Manuals', 'Measures', 'Methods', 'Michigan', 'Modeling', 'Monitor', 'Names', 'Natural Language Processing', 'Neural Network Simulation', 'Patient Care', 'Patients', 'Pattern', 'Performance', 'Phase', 'Physicians', 'Policies', 'Procedures', 'Process', 'Production', 'Provider', 'Public Health', 'Radiology Specialty', 'Reporting', 'Research', 'Role', 'Running', 'Semantics', 'Services', 'System', 'Techniques', 'Technology', 'Telephone', 'Text', 'Time', 'Training', 'United States Centers for Medicare and Medicaid Services', 'Universities', 'Work', 'analytical tool', 'base', 'clinical practice', 'clinically relevant', 'computerized data processing', 'dashboard', 'deep neural network', 'design', 'improved', 'interest', 'novel', 'novel strategies', 'patient safety', 'payment', 'programs', 'prototype', 'success', 'technological innovation', 'tool']",NLM,"SAFERMED, LLC",R41,2018,149953,0.026288283006648924
"EHR Anticoagulants Pharmacovigilance DESCRIPTION (provided by applicant): The timely identification of previously unknown toxicities of cardiovascular drugs is an important, unsolved problem. In the United States, 20% of the 548 drugs introduced into the market between 1975 and 1999 were either withdrawn or acquired a new ""black box"" warning during the 25-year period following initial approval by the Food and Drug Administration. Adverse drug events (ADEs) are an important cause of morbidity and mortality in patients, yet 95% of ADEs are unreported, leading to delays in the detection of previously unknown ADEs and underestimation of the risk to known ADEs. It is known that Electronic Health Record (EHR) notes and lab results contain ADE information and biomedical natural language processing (NLP) provides automated tools that facilitate chart review and thus improve patient surveillance and post-marketing pharmacovigilance. Optimal use of anticoagulants requires accurate and timely detection of ADEs from EHRs. The objectives for this proposal are to develop ""intelligent"" NLP approaches to extract disease, medication, and structured ADE information from EHRs, and then evaluate extracted ADEs for detecting known ADE types as well as clinically unrecognized or novel ADEs whose pattern or effect have not been previously identified. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page PUBLIC HEALTH RELEVANCE: This project proposes innovative intelligent biomedical natural language process approaches to detect ADEs in patients' electronic health records (EHRs), an important step towards ADE surveillance and pharmacovigilance. The focus of this proposal will bon the national priority area of anticoagulants ADE detection, although the algorithms and systems we develop can be extended to other drug classes and diseases. It is anticipated that the data resources, algorithms, and the Anticoagulant Pharmacovigilance Toolkit (APVTK) developed will significantly enhance ADE detection, patient surveillance and post marketing pharmacovigilance.",EHR Anticoagulants Pharmacovigilance,9418526,R01HL125089,"['Adult', 'Adverse drug event', 'Adverse event', 'Algorithms', 'Anticoagulant therapy', 'Anticoagulants', 'Anticoagulation', 'Area', 'Atrial Fibrillation', 'Biometry', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Controlled Vocabulary', 'Cost efficiency', 'Data', 'Data Sources', 'Detection', 'Development', 'Disease', 'Electronic Health Record', 'Elements', 'Etiology', 'Frequencies', 'Health Care Costs', 'Health Promotion', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Illinois', 'Injury', 'Institutes', 'Intervention', 'Lead', 'Length of Stay', 'Linguistics', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Marketing', 'Massachusetts', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Ontology', 'Outcomes Research', 'Package Insert', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Prevention', 'Process', 'Regimen', 'Risk', 'Risk Factors', 'Route', 'Safety', 'Schedule', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Structure', 'Supervision', 'System', 'Systems Analysis', 'Text', 'Thromboembolism', 'Time', 'Toxic effect', 'United States', 'United States Food and Drug Administration', 'Universities', 'Venous', 'Work', 'clinical practice', 'data resource', 'disorder prevention', 'dosage', 'epidemiology study', 'high risk', 'improved', 'innovation', 'mortality', 'natural language', 'novel', 'open source', 'patient safety', 'predictive modeling', 'primary endpoint', 'prospective', 'public health relevance', 'tool']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2018,16608,0.03298865761211533
"EHR Anticoagulants Pharmacovigilance DESCRIPTION (provided by applicant): The timely identification of previously unknown toxicities of cardiovascular drugs is an important, unsolved problem. In the United States, 20% of the 548 drugs introduced into the market between 1975 and 1999 were either withdrawn or acquired a new ""black box"" warning during the 25-year period following initial approval by the Food and Drug Administration. Adverse drug events (ADEs) are an important cause of morbidity and mortality in patients, yet 95% of ADEs are unreported, leading to delays in the detection of previously unknown ADEs and underestimation of the risk to known ADEs. It is known that Electronic Health Record (EHR) notes and lab results contain ADE information and biomedical natural language processing (NLP) provides automated tools that facilitate chart review and thus improve patient surveillance and post-marketing pharmacovigilance. Optimal use of anticoagulants requires accurate and timely detection of ADEs from EHRs. The objectives for this proposal are to develop ""intelligent"" NLP approaches to extract disease, medication, and structured ADE information from EHRs, and then evaluate extracted ADEs for detecting known ADE types as well as clinically unrecognized or novel ADEs whose pattern or effect have not been previously identified. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page PUBLIC HEALTH RELEVANCE: This project proposes innovative intelligent biomedical natural language process approaches to detect ADEs in patients' electronic health records (EHRs), an important step towards ADE surveillance and pharmacovigilance. The focus of this proposal will bon the national priority area of anticoagulants ADE detection, although the algorithms and systems we develop can be extended to other drug classes and diseases. It is anticipated that the data resources, algorithms, and the Anticoagulant Pharmacovigilance Toolkit (APVTK) developed will significantly enhance ADE detection, patient surveillance and post marketing pharmacovigilance.",EHR Anticoagulants Pharmacovigilance,9698030,R01HL125089,"['Adult', 'Adverse drug event', 'Adverse event', 'Algorithms', 'Anticoagulant therapy', 'Anticoagulants', 'Anticoagulation', 'Area', 'Atrial Fibrillation', 'Biometry', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Controlled Vocabulary', 'Cost efficiency', 'Data', 'Data Sources', 'Detection', 'Development', 'Disease', 'Electronic Health Record', 'Elements', 'Etiology', 'Frequencies', 'Health Care Costs', 'Health Promotion', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Illinois', 'Injury', 'Institutes', 'Intervention', 'Lead', 'Length of Stay', 'Linguistics', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Marketing', 'Massachusetts', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Ontology', 'Outcomes Research', 'Package Insert', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Prevention', 'Process', 'Regimen', 'Risk', 'Risk Factors', 'Route', 'Safety', 'Schedule', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Structure', 'Supervision', 'System', 'Systems Analysis', 'Text', 'Thromboembolism', 'Time', 'Toxic effect', 'United States', 'United States Food and Drug Administration', 'Universities', 'Venous', 'Work', 'clinical practice', 'data resource', 'disorder prevention', 'dosage', 'epidemiology study', 'high risk', 'improved', 'innovation', 'mortality', 'natural language', 'novel', 'open source', 'patient safety', 'predictive modeling', 'primary endpoint', 'prospective', 'public health relevance', 'tool']",NHLBI,UNIVERSITY OF MASSACHUSETTS LOWELL,R01,2018,874586,0.03298865761211533
"Identification of Patients with Low Life Expectancy ﻿    DESCRIPTION (provided by applicant): It is increasingly recognized that optimal treatment is not the same for every patient - it depends on the individual patient's circumstances. One important factor that determines the optimal clinical management is the patient's life expectancy, which determines the temporal horizon within which medical decisions have to operate. For example, while adding an extra diabetes medication may save a 40-year-old individual from going blind or developing kidney failure 20 years later, it will not bring any benefits to an 80-yer old with a metastatic malignancy who is expected to live only a few months.  Consequently, it is critical that when we measure quality of care, suggest treatment options to clinicians through clinical decision support or compare different treatment strategies, we take into account the patient's life expectancy. However, currently there are no methods available that can do this with sufficient accuracy.  Most commonly used techniques to assess a patient's mortality risk draw primarily on administrative data and other structured data fields in electronic medical records. This approach leaves out a large amount of information that is only available in narrative documents such as provider notes, radiology reports, etc. In this project we propose to test the hypothesis that application of two novel technologies - artificial intelligence technique Dynamic Logic and natural language processing (NLP) - could leverage the information in narrative electronic documents to significantly improve the accuracy of identification of patients with low life expectancy.  Dynamic Logic allows to circumvent the challenge of combinatorial complexity that limits the number of variables and their combinations that can be considered as predictors of an outcome by most currently used analytical methods. Dynamic Logic makes use of a limited number of iterative approximations to reduce the complexity of a problem with multiple predictor variables from exponential to approximately linear. Utilization of Dynamic Logic will allow us to greatly increase the richness of the models for identification of patients with low life expectancy and ultimately improve their accuracy.  Information such as the patient's functional status that is usually only found in narrative documents may be critical to improving accuracy of identifying frail patients at high mortality risk. Modern NLP techniques can effectively identify key concepts in medical text but until now analytical methods allowed consideration of only a few of pre-selected concepts in prediction models. Combining NLP with Dynamic Logic will allow us to greatly expand the number of concepts from narrative text that could be included in the life expectancy prediction model, likely leading to a considerable improvement in accuracy.  In the proposed translational multidisciplinary project our team that will include experts on artificial intelligence, natural language processing, analysis of data in electronic medical records, and geriatrics will test whether a combination of Dynamic Logic and NLP improves identification of patients at high risk of death. PUBLIC HEALTH RELEVANCE: Identification of patients with limited life expectancy is important for accurate measurement of quality of care delivered by clinicians, clinical decision support in electronic medical records and research that compares different treatment options for patients. However, currently available methods suffer from low accuracy or use information that is not widely available. In this project we propose to combine two advanced computational technologies - artificial intelligence technique Dynamic Logic and natural language processing - to improve accuracy of identification of patients with low life expectancy.",Identification of Patients with Low Life Expectancy,9481256,R01HS024090,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2018,227899,0.04019309786649032
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,9770622,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2018,380000,0.0025174994903373145
"Extended Methods and Software Development for Health NLP PROJECT SUMMARY There is a deluge of health-related texts in many genres, from the clinical narrative to newswire and social media. These texts are diverse in content, format, and style, and yet they represent complementary facets of biomedical and health knowledge. Natural Language Processing (NLP) holds much promise to extract, understand, and distill valuable information from these overwhelming large and complex streams of data, with the ultimate goal to advance biomedicine and impact the health and wellbeing of patients. There have been a number of success stories in various biomedical NLP applications, but the NLP methods investigated are usually tailored to one specific phenotype and one institution, thus reducing portability and scalability. Moreover, while there has been much work in the processing of clinical texts, other genres of health texts, like narratives and posts authored by health consumers and patients, are lacking solutions to marshal and make sense of the health information they contain. Robust NLP solutions that answer the needs of biomedicine and health in general have not been fully investigated yet. A unified, data-science approach to health NLP enables the exploration of methods and solutions unprecedented up to now.  Our vision is to unravel the information buried in the health narratives by advancing text-processing methods in a unified way across all the genres of texts. The crosscutting theme is the investigation of methods for health NLP (hNLP) made possible by big data, fused with health knowledge. Our proposal moves the field into exploring semi-supervised and fully unsupervised methods, which only succeed when very large amounts of data are leveraged and knowledge is injected into the methods with care. Our hNLP proposal also targets a key challenge of current hNLP research: the lack of shared software. We seek to provide a clearinghouse for software created under this proposal, and as such all developed tools will be disseminated. Starting from the data characteristics of health texts and information needs of stakeholders, we will develop and evaluate methods for information extraction, information understanding. We will translate our research into the publicly available NLP software platform cTAKES, through robust modules for extraction and understanding across all genres of health texts. We will also demonstrate impact of our methods and tools through several use cases, ranging from clinical point of care to public health, to translational and precision medicine, to participatory medicine. Finally, we will disseminate our work through community activities, such as challenges to advance the state of the art in health natural language processing. PROJECT NARRATIVE  There is a deluge of health texts. Natural Language Processing (NLP) holds much promise to unravel valuable information from these large data streams with the goal to advance medicine and the wellbeing of patients. We will advance state-of-the-art NLP by designing robust, scalable methods that leverage health big data, demonstrating relevance on high-impact use cases, and disseminating NLP tools for the research community and public at large.",Extended Methods and Software Development for Health NLP,9421556,R01GM114355,"['Apache', 'Benchmarking', 'Big Data', 'Caring', 'Characteristics', 'Clinical', 'Communication', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Event', 'Foundations', 'Goals', 'Gold', 'Health', 'Information Resources', 'Institution', 'International', 'Investigation', 'Knowledge', 'Knowledge Extraction', 'Link', 'Literature', 'Marshal', 'Medicine', 'Methods', 'Names', 'Natural Language Processing', 'Ontology', 'Patients', 'Personal Satisfaction', 'Phenotype', 'Philosophy', 'Public Health', 'Research', 'Semantics', 'Solid', 'Standardization', 'Stream', 'Supervision', 'System', 'Terminology', 'Text', 'Translating', 'Translational Research', 'Vision', 'Work', 'commercialization', 'design', 'health knowledge', 'improved', 'information organization', 'method development', 'novel', 'point of care', 'portability', 'precision medicine', 'programs', 'social media', 'software development', 'success', 'syntax', 'tool', 'translational medicine']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2018,793522,0.089182480191455
"Exploring the evolving relationship between tobacco, marijuana and e-cigarettes Abstract The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana products (respectively). In order to understand this changing landscape we need new, ﬂexible, and responsive research methods capable of rapidly providing insights into product initiation patterns, use patterns, and cessation strategies. Social media — here deﬁned as including internet discussion forums — provides a ready-made source of abundant, naturalistic, longitudinal, publicly accessible, ﬁrst-person narratives with which to understand health behaviours and attitudes. We propose to use a combination of qualitative methods and automated natural language processing techniques to investigate online discussion forums devoted to tobacco, marijuana, and e-cigarettes in order to understand user trajectories through the three product categories. PROJECT NARRATIVE The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana (respectively). In order to make sense of this rapidly changing landscape, we need new, ﬂexible, and responsive research methods capable of providing insights into tobacco, marijuana, and e- cigarette product use patterns. We propose to use a combination of qualitative and automated natural language processing techniques to investigate online discussion forums related to tobacco, marijuana, and e-cigarettes in order to better understand user trajectories through these different product classes.","Exploring the evolving relationship between tobacco, marijuana and e-cigarettes",9530020,R21DA043775,"['Adolescent and Young Adult', 'Adult', 'Age', 'Algorithms', 'Attitude to Health', 'Categories', 'Chronic Bronchitis', 'Code', 'Data', 'Data Science', 'Devices', 'Educational Status', 'Electronic cigarette', 'Health', 'Health behavior', 'High School Student', 'Individual', 'Internet', 'Manuals', 'Marijuana', 'Modeling', 'Multiple Marriages', 'Natural Language Processing', 'Pattern', 'Persons', 'Population', 'Qualitative Methods', 'Reporting', 'Research', 'Research Methodology', 'Resources', 'Role', 'Sampling', 'Smoking', 'Source', 'Surgeon', 'Techniques', 'Therapeutic', 'Tobacco', 'Tobacco use', 'Training', 'Work', 'base', 'cigarette smoking', 'combustible cigarette', 'electronic cigarette use', 'flexibility', 'high school', 'innovation', 'insight', 'man', 'marijuana use', 'nicotine replacement', 'smoking cessation', 'social media']",NIDA,UNIVERSITY OF UTAH,R21,2018,201771,0.000471133658427575
"Improving Cardiovascular Drug Safety With Automated Bleeding Classification PROJECT SUMMARY Atrial fibrillation (AF) treatment often includes drug therapy with oral anticoagulants (OAC) to prevent stroke. Bleeding, however, is a common complication of these drugs, affecting up to one in four patients. The Center for Medicare and Medicaid Services recently prioritized OAC-related drug safety as a key quality measure. Currently, however, no method exists to accurately identify bleeding events and severity in large populations. Prior methods use diagnoses codes, which lack sensitivity and clinical detail, or manual chart review, which cannot be implemented in large populations. The proposed research aims address this knowledge gap by applying a natural language processing (NLP)-based approach to identify bleeding events and classify severity in a real-world AF population. The tools will be validated in patients treated at a different institution, to ensure reproducibility across provider settings. In addition, we will apply the bleeding classification tool to evaluate the association between bleeding severity and mortality. Dr. Shah is an emerging young investigator whose career development plan is focused on acquiring the biomedical informatics skills to needed to accurately identify and reduce patient harm. Her training plan focuses on learning core competencies in natural language processing, with the goal of turning the wealth of data in the electronic medical record into useable knowledge. She will combine mentorship from established experts and targeted coursework to acquire skills in biomedical informatics, data science, advanced analytic methods, and research leadership. Completion of these research and training aims will create a platform for future R01 proposals by: (i) enabling safety focused comparative effectiveness research in AF (ii) setting the stage to identify bleeding complications in other cardiovascular diseases and (iii) developing a skill set that allows leadership of a multidisciplinary research team. Through this career development plan, Dr. Shah will build upon her prior training in clinical cardiology and research methodology and lay a strong foundation for a high impact research career. PUBLIC HEALTH RELEVANCE Atrial fibrillation affects six million US adults, and the prevalence is expected to double by 2030. As new drug and device treatments for stroke prevention emerge, patients and providers increasingly need accurate methods to monitor and improve patient safety.",Improving Cardiovascular Drug Safety With Automated Bleeding Classification,9460286,K08HL136850,"['Accounting', 'Address', 'Adult', 'Adverse event', 'Affect', 'Age', 'Algorithms', 'Anticoagulants', 'Atrial Fibrillation', 'Biomedical Research', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Code', 'Competence', 'Complication', 'Computer Assisted', 'Computerized Medical Record', 'Data', 'Data Science', 'Development Plans', 'Devices', 'Diagnosis', 'Ensure', 'Event', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Hemorrhage', 'Incidence', 'Institution', 'Interdisciplinary Study', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Letters', 'Link', 'Manuals', 'Measures', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Oral', 'Patients', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Pragmatic clinical trial', 'Prevalence', 'Provider', 'Quality of life', 'Race', 'Records', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Research Training', 'Risk', 'Safety', 'Severities', 'Site', 'Standardization', 'Stroke', 'Stroke prevention', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'United States Centers for Medicare and Medicaid Services', 'Utah', 'Validation', 'Vital Status', 'acute coronary syndrome', 'aging population', 'analytical method', 'base', 'biomedical informatics', 'career', 'career development', 'clinically relevant', 'cohort', 'comparative effectiveness', 'effectiveness research', 'improved', 'interdisciplinary approach', 'medication compliance', 'mortality', 'neglect', 'novel', 'novel therapeutics', 'patient safety', 'personalized medicine', 'portability', 'public health relevance', 'screening', 'sex', 'skills', 'stroke risk', 'stroke treatment', 'tool']",NHLBI,UNIVERSITY OF UTAH,K08,2018,163080,0.015669787272953595
"Epidemiology and clinical outcomes of diabetic macular edema Approximately 25% of the millions of veterans (est. 8.92 million FY 2013) enrolled for care in Veterans Health Administration (VHA) have diabetes mellitus, and diabetic macular edema (DME) is the leading cause of vision loss in the adult diabetic population world-wide. Although diabetic retinopathy has been well-studied, comparatively little is known about the burden of DME. In fact, only two national prevalence studies and no national study on the incidence of DME in persons with type 2 diabetes have been conducted. Similarly many risk factors have been characterized for DR, but no large studies have established predictors for DME.  Beyond the Medicare claims database, the VHA National Patient Care Database (NPCD) contains standardized administrative data for several aspects of patient care including diagnoses, procedures, medications, lab test results, vital signs, clinical text notes, and mortality. Because the VA uses teleretinal screening as routine clinical care for all patients with diabetes with these results included in the NPCD, the NPCD is an ideal source for studying the epidemiology of and risk factors for DME.  This study proposes to determine the burden of diabetic macular edema, establish risk factors, and examine treatment outcomes in a previously extracted dataset on 1.98 million veterans who have undergone diabetic retinopathy screening at least once since 2004. Currently invaluable ophthalmic data are encoded in unstructured clinical encounter notes in the Computerized Patient Record System (CPRS), and no validated automated extraction method exists to capture these data elements. An automated extraction method using natural language processing will be created and validated to unlock key ophthalmic variables. These text extraction methods will be applicable to extracting ophthalmology data from not only notes of patients with DME but also any ophthalmology clinical note. This will enable future large scale studies in ophthalmology using NPCD and be immediately valuable to the research community at large.  The candidate, Dr. Aaron Lee, MD MSCI, is an ophthalmologist with subspecialty training in retina surgery with a strong background in computer science and epidemiology. His career goal is to become an independent clinician scientist studying diabetic eye disease with large-scale electronic medical record extracted data. While he possesses the foundational skills, he seeks to gain training in advanced statistics and natural language processing to unlock the data captured in unstructured clinical encounter notes. He has assembled an outstanding mentorship team under the primary mentor, Dr. Edward Boyko, MD MPH. This mentorship team includes renowned experts in clinical epidemiology, health informatics, ophthalmology, and natural language processing. This K23 will provide Dr. Lee the structured coursework, mentorship, and applied learning needed to acquire new research skills. He will leverage key local resources to carry out the proposed research at the University of Washington and the VA Seattle Epidemiologic Research and Information Center. Despite the significant visual loss associated with diabetic macular edema, little is known about the frequency of its occurrence, its risk factors, and the real-world effectiveness of existing treatments. The purpose of this proposed research is to utilize the VA National Patient Care Database to extract relevant data elements to examine these three clinical questions: 1) what is the incidence and prevalence of diabetic macular edema, 2) what are the risk factors associated with its development, and 3) what is the comparative real-world effectiveness of its treatments, including intravitreal anti-VEGF therapy, intravitreal corticosteroid therapy and macular laser. The methods developed in this research proposal will not only further our understanding of DME but also generalize and enable future large-scale ophthalmic studies.",Epidemiology and clinical outcomes of diabetic macular edema,9574973,K23EY029246,"['Adopted', 'Adrenal Cortex Hormones', 'Adult', 'Affect', 'Age', 'Algorithms', 'Anemia', 'Blindness', 'Cardiovascular Diseases', 'Caring', 'Cataract Extraction', 'Clinic', 'Clinical', 'Clinical Data', 'Code', 'Communities', 'Computerized Medical Record', 'Computerized Patient Records', 'Data', 'Data Element', 'Data Set', 'Databases', 'Development', 'Diabetes Mellitus', 'Diabetic Retinopathy', 'Diagnosis', 'Dyslipidemias', 'Effectiveness', 'Enrollment', 'Epidemiology', 'Ethnic Origin', 'Exclusion Criteria', 'Expressed Sequence Tags', 'Eye', 'Eye diseases', 'Foundational Skills', 'Frequencies', 'Functional disorder', 'Future', 'Goals', 'Handedness', 'Hypertension', 'Incidence', 'Information Centers', 'Injections', 'Intervention', 'Kidney', 'Lasers', 'Lead', 'Light Coagulation', 'Manuals', 'Masks', 'Measures', 'Medicare claim', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Natural Language Processing', 'Non-Insulin-Dependent Diabetes Mellitus', 'Operative Surgical Procedures', 'Ophthalmologist', 'Ophthalmology', 'Outcome', 'Pathology', 'Patient Care', 'Patients', 'Pattern', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Phenotype', 'Physicians', 'Population', 'Prediction of Response to Therapy', 'Prevalence', 'Prevalence Study', 'Procedures', 'Protocols documentation', 'Public Health Informatics', 'Randomized Controlled Trials', 'Regimen', 'Research', 'Research Proposals', 'Resources', 'Retina', 'Retinal', 'Risk Factors', 'Scientist', 'Severities', 'Sleep Apnea Syndromes', 'Smoking Status', 'Source', 'Standardization', 'Structure', 'System', 'Test Result', 'Text', 'Time', 'Training', 'Treatment Protocols', 'Treatment outcome', 'Universities', 'Use Effectiveness', 'Validation', 'Vascular Endothelial Growth Factors', 'Veterans', 'Visual', 'Visual Acuity', 'Washington', 'bevacizumab', 'career', 'clinical care', 'clinical epidemiology', 'cohort', 'comparative', 'computer science', 'diabetes management', 'diabetic', 'epidemiology study', 'hands-on learning', 'health administration', 'intravitreal injection', 'laser photocoagulation', 'macula', 'macular edema', 'mortality', 'proliferative diabetic retinopathy', 'screening', 'skills', 'statistics', 'therapy outcome', 'traditional therapy', 'treatment optimization']",NEI,UNIVERSITY OF WASHINGTON,K23,2018,237398,0.0020010652529038066
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk. PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9454246,R01AI117011,"['Animals', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease Surveillance', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Knowledge', 'Link', 'Literature', 'Location', 'Manuals', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'Zoonoses', 'improved', 'information model', 'interest', 'journal article', 'molecular sequence database', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2018,461012,0.046724346907224205
"Dynamic learning for post-vaccine event prediction using temporal information in VAERS Project Summary Vaccines have been one of the most successful public health interventions to date. They are, however, pharmaceutical products that carry risks. Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine- preventable illnesses. The CDC/FDA Vaccine Adverse Event Reporting System (VAERS) contains up to 30,000 reports per year over the past 25 years. VAERS reports include both structured data (e.g., vaccination date, first onset date, age, and gender) and unstructured narratives that often provide detailed clinical information about the clinical events and the temporal relationship of the series of event occurrences post vaccination. The structured data only provide one onsite date whereas temporal information of the sequence of events post vaccination is contained in the unstructured narratives. Current status –While structured data in the VAERS are widely used, the narratives are generally ignored because of the challenges inherent in working with unstructured data. Without these narratives, potentially valuable information is lost. Goals - In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Specifically, built upon the state-of-art ontology and natural language processing technologies, we will develop and validate a Temporal Information Modeling, Extraction and Reasoning system for Vaccine data (TIMER-V), which will automatically extract post-vaccination events and their temporal relationships from VAERS reports, semantically infer temporal relations, and integrate the exacted unstructured data with the structured data. Furthermore, we will provide and maintain a publicly available data access interface to query the new integrated data repository, which will facilitate vaccine safety research, casual inference, and other temporal related discovery. We will also develop and validate models to predict severe AEs using the co-occurrence or temporal patterns of the series of AEs post vaccination. To the best of our knowledge, this is the first attempt to make use of the unstructured narratives in the VAERS reports to facilitate the temporal related discovery to a broad community of investigators in pharmacology, pharmacoepidemiology, vaccine safety research, among others. Project Narrative Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine-preventable illnesses. In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. Currently the FDA/CDC Vaccine Adverse Event Reporting System (VAERS) only includes one onsite date in its database. The textual narratives in the reports are generally ignored primarily due to their unstructured nature. These narratives, however, contain more detailed information about the series of events that happened after vaccination, which could be valuable for more informed clinical studies. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Our new methods, their applications to VAERS database, and their dissemination will facilitate the entire research network for pursuing temporal related discovery with high methodological rigor.",Dynamic learning for post-vaccine event prediction using temporal information in VAERS,9419767,R01AI130460,"['Abbreviations', 'Address', 'Adverse event', 'Age', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Development', 'Evaluation', 'Event', 'Frequencies', 'Funding', 'Gender', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Informatics', 'Learning', 'Manuals', 'Measles-Mumps-Rubella Vaccine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Pattern', 'Performance', 'Pharmacoepidemiology', 'Pharmacologic Substance', 'Pharmacology', 'Process', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Series', 'Severe Adverse Event', 'Severities', 'Signal Transduction', 'Source', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Validation', 'base', 'data access', 'data warehouse', 'flexibility', 'improved', 'influenza virus vaccine', 'information model', 'novel', 'predictive modeling', 'public health intervention', 'response', 'vaccine safety']",NIAID,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2018,619389,0.02260942934884757
"Secondary use of EMRs for surgical complication surveillance DESCRIPTION (provided by applicant):  Recent statistics indicate that worldwide almost 234 million major surgical procedures are performed each year with the rates of major postsurgical complications (PSCs) range from 3% to 16% and rates of permanent disability or death range from 0.4% to 0.8%. Early detection of PSCs is crucial since early intervention could be lifesaving. Meanwhile, with the rapid adoption of electronic medical records (EMRs) and the accelerated advance of health information technology (HIT), detection of PSCs by applying advanced analytics on EMRs makes it possible for near real-time PSC surveillance. We have developed a rule-based PSC surveillance system to detect most frequent colorectal PSCs near real-time from EMRs where a pattern-based natural language processing (NLP) engine is used to extract PSC related information from text and a set of expert rules is used to detect PSCs. Two challenges are identified. First, it is very challenging to integrate a diverse set of relevant data using expert rules. In the past, probabilistic approaches such as Bayesian Network which can integrate a diverse set of relevant data have become popular in clinical decision support and disease outbreak surveillance. Can we implement probabilistic approaches for PSC surveillance? Secondly, a large portion of the clinical information is embedded in text and it has been quite expensive to manually obtain the patterns used in the NLP system since it requires team effort of subject matter experts and NLP specialists. In the research field, statistical NLP has been quite popular. However, decision making in clinical practice demands tractable evidences while models for statistical NLP are not human interpretable. Can we incorporate statistical NLP to accelerate the NLP knowledge engineering process? We hypothesize that a probabilistic approach for PSC surveillance can be developed for improved case detection which can integrate multiple evidences from structured as well as unstructured EMR data. We also hypothesize that empirical NLP can accelerate the knowledge engineering process needed for building pattern- based NLP systems used in practice. Specific aims include: i) developing and evaluating an innovative Bayesian PSC surveillance system that incorporates evidences from both structured and unstructured EMR data; and ii) incorporating and evaluating statistical NLP in accelerating the NLP knowledge engineering process of pattern-based NLP for PSC surveillance. Given the significance of HIT, our study results will advance the science in developing practical NLP systems that can be translated to meet NLP needs in health care practice. Additionally, given the significance of PSCs, our study results will address significant patient safety and quality issues in surgical practice. Utilizing automated methods to detect postsurgical complications will enable early detection of complications compared to other methods and therefore have great potential of improving patient safety and health care quality while reducing cost. The results could lead to large scale PSC surveillance and quality improvement towards safer and better health care. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to unprecedented opportunities to use EMRs for clinical practice and research. We explore the use of EMRs for near real-time postsurgical complication surveillance with the aim of improving health care quality and reducing health care cost through enhanced analytics towards surgical excellence.",Secondary use of EMRs for surgical complication surveillance,9476980,R01EB019403,"['Abscess', 'Address', 'Adoption', 'Age', 'Anesthetics', 'Area', 'Bayesian Method', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Research', 'Colorectal', 'Complex', 'Complication', 'Computerized Medical Record', 'Data', 'Decision Making', 'Detection', 'Development', 'Disease Outbreaks', 'Early Diagnosis', 'Early Intervention', 'Educational workshop', 'Engineering', 'Goals', 'Health Care Costs', 'Healthcare', 'Hemorrhage', 'Human', 'Ileus', 'Knowledge', 'Lead', 'Manuals', 'Methods', 'Minor', 'Motivation', 'Natural Language Processing', 'Nature', 'Nutritional', 'Operative Surgical Procedures', 'Output', 'Patients', 'Pattern', 'Perioperative', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Reporting', 'Research', 'Risk Factors', 'Science', 'Severities', 'Specialist', 'Statistical Models', 'Structure', 'Surgeon', 'Surgical complication', 'System', 'Testing', 'Text', 'Time', 'Translating', 'Uncertainty', 'Work', 'Wound Infection', 'base', 'clinical decision support', 'clinical implementation', 'clinical practice', 'computer based statistical methods', 'cost', 'disability', 'health care quality', 'health information technology', 'improved', 'innovation', 'patient safety', 'public health relevance', 'rapid growth', 'statistics']",NIBIB,MAYO CLINIC ROCHESTER,R01,2018,300000,0.06872473409625994
"Improved Disease Stratification Using Electronic Health Records ABSTRACT This Career Development Application describes targeted coursework and mentored research for progression to independent research in the use of electronic health record data for disease subtyping. Electronic health records have demonstrated great promise as a scalable source of data for biomedical research to enable “precision medicine.” Use of natural language processing techniques has enabled computational analysis of specific terms found in free text clinical notes. An improved ability to extract symptom information from clinical notes would improve researchers’ ability to use de-identified data from patient records for discovery of disease subtypes. Symptom-related terms are particularly important in the context of mental health, but also harder to detect in notes than other terms like diseases or drug names.  The research aims of this proposal present a novel approach to scalable extension of biomedical terminologies and improved detection of those terms and their modifiers (e.g. severe, familial, absent). The richer dataset that can be extracted using these enhanced approaches is then used to define patient cohorts and to detect disease subtypes and predictors of response to specific pharmaceutical intervention.  Resulting patient stratification will be compared to groupings made without the enriched data and validated on an independent data set. The overarching hypothesis of this work is that enhanced mining of clinical notes will enable statistically significant and clinically relevant symptom-based stratification of psychiatric disorders. In order to test this hypothesis, I will: Aim 1: Develop a semi-automated pipeline for domain-specific terminology extension Aim 2: Define and stratify patient cohorts through use of enhanced term extraction Aim 3: Evaluate the validity and utility of the richer set of data obtained through Aims 1 and 2  One area of greatest need for more evidence-based disease stratification, and also of greatest challenge for a number of reasons, is that of mental health. Mental health disorders account for 30% of non-fatal disease burden world-wide, and pose an economic burden of trillions of dollars and climbing. Moreover, mental health symptoms are generally subjective and self-reported, with few objectively measurable signs. The impact of this proposal is that it will dramatically improve our ability to use EHR data to stratify patients in this drastically underserved area of health and healthcare.  The major innovations of this project are the adaptation and application of a semi-supervised pattern learning pipeline to augment mental health terminologies, and a novel approach to disease stratification using a significantly underutilized source of biomedical data, namely clinical notes.  This work addresses a major challenge for mining clinical notes in rapidly evolving biomedical domains and leverages a valuable source of medical evidence that is largely untapped and underutilized. Together, these methods for enhanced use of clinical notes will enable identification of distinct patient subgroups using data that is sitting idle in EHRs. Project Narrative Electronic health records have demonstrated great promise as a scalable source of data for biomedical research to enable “precision medicine.” Natural language processing has enabled the use of information from clinical notes in addition to structured data like diagnoses and lab values. This work aims to improve our ability to extract useful information from electronic health records to enable disease subtyping, particularly in the area of mental health.",Improved Disease Stratification Using Electronic Health Records,9566983,K01LM012529,"['Address', 'Area', 'Biological', 'Biomedical Research', 'Categories', 'Classification', 'Clinical', 'Clinical Data', 'Complement', 'Computer Analysis', 'Data', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease stratification', 'Economic Burden', 'Electrocardiogram', 'Electronic Health Record', 'Genotype', 'Grouping', 'Health', 'Healthcare', 'Intervention', 'Learning', 'Measurable', 'Medical', 'Mental Health', 'Mental disorders', 'Mentors', 'Methods', 'Mining', 'Names', 'Natural Language Processing', 'Outcome', 'Patient Self-Report', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phenotype', 'Records', 'Research', 'Research Personnel', 'Rheumatology', 'Severities', 'Source', 'Statistical Models', 'Stratification', 'Structure', 'Supervision', 'Symptoms', 'Techniques', 'Terminology', 'Testing', 'Text', 'Work', 'base', 'burden of illness', 'career development', 'clinical decision-making', 'clinically actionable', 'clinically relevant', 'cohort', 'disease classification', 'disorder subtype', 'electronic data', 'evidence base', 'improved', 'innovation', 'novel strategies', 'patient stratification', 'patient subsets', 'phenotypic data', 'precision medicine', 'predicting response', 'response', 'treatment response']",NLM,DUKE UNIVERSITY,K01,2018,187483,-0.0033848740547602475
"Community Surveillance of Coronary Heart Disease Project Summary/Abstract  Preventing the onset of acute myocardial infarction (AMI) and its recurrence, and reducing the morbidity and mortality associated with AMI, remain of significant public health and clinical concern. Monitoring contemporary trends in AMI incidence, treatment, and in-hospital and long-term outcomes is of considerable importance given periodic national updates of treatment guidelines, emphasis on reducing hospital readmissions, and revised definitions and classifications of AMI. Continuously supported by the NHLBI, we have conducted more than 35 years of population-based surveillance of AMI incidence and attack rates, hospital management practices, and the in-hospital and long-term prognosis associated with AMI among residents of central MA hospitalized at all central MA medical centers. We have a highly experienced team of cardiologists, epidemiologists, clinical informatics, and health services researchers who will build on multi- decade long trends (1975-2011) in our principal study endpoints examined previously in this study to the two new study years of patients hospitalized with AMI at all central MA medical centers in 2014 and 2017.  To sustain our efforts into the era of electronic medical records (EMRs), and after implementation of the ICD-10 system in 2015, we will develop a new automated AMI surveillance system that efficiently utilizes EMRs by taking advantage of state-of-art natural language processing (NLP) methods that will be compatible with ICD-10 (Aim 1). We will use the new NLP method to streamline traditional chart review-based collection of socio-demographic, clinical, treatment, and hospital and post-discharge outcomes data in patients hospitalized with AMI at all 11 central MA medical centers in 2014 and 2017. The data extracted from NLP-streamlined chart reviews will be used to validate and refine the NLP system. Issues related to changes from ICD-9 to ICD- 10 will be carefully addressed. The new NLP-enriched EMR-based surveillance system will eventually be implemented in all participating central MA hospitals. Using the NLP-enriched and EMR-based surveillance data, we will monitor the contemporary clinical epidemiology of AMI, and out-of-hospital deaths due to coronary disease, and changing landscape, over a more than 40 year period (1975-2017) (Aim 2).  The new EMR-based and NLP-enriched system will enhance the population-based surveillance of acute coronary disease. This new system will be cost-effective, more efficient and near-real time, have greater accuracy and precision, and can be readily updated to accommodate changes in information technologies and broadly applicable to other hospital systems. It will support our continued efforts to provide unique community- based observational data on several populations that are often excluded from clinical trials, and that are increasing in numbers, namely the elderly and patients with multiple morbidities. Furthermore, it will generate critical data to inform more national clinical guidelines on the enhanced prevention and management of AMI. If successful, the system can serve as a model and be implemented statewide in MA and elsewhere in the US. Project Narrative  The results of the proposed community-based study will provide data about 40 year trends with regards to the changing magnitude of, and outcomes associated with, heart attacks in residents of a large central New England community. The results of this investigation will also provide contemporary insights on how patients who experience heart attacks in the community are treated by physicians.",Community Surveillance of Coronary Heart Disease,9395941,R01HL135219,"['Acute', 'Acute myocardial infarction', 'Address', 'Adoption', 'American', 'Area', 'Big Data', 'Caring', 'Case Fatality Rates', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Informatics', 'Clinical Treatment', 'Clinical Trials', 'Code', 'Collection', 'Communities', 'Computerized Medical Record', 'Coronary', 'Coronary heart disease', 'Data', 'Data Sources', 'Development', 'Diagnostic', 'Disease Surveillance', 'Epidemiologist', 'Fostering', 'Future', 'Guidelines', 'Harvest', 'Health Care Costs', 'Health Policy', 'Health Services', 'Hospital Administration', 'Hospitals', 'ICD-9', 'Impairment', 'Incidence', 'Information Technology', 'Inpatients', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Investigation', 'Life Expectancy', 'Manuals', 'Medical', 'Medical center', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Myocardial Infarction', 'Myocardial Reperfusion', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'New England', 'Outcome', 'Participant', 'Patients', 'Periodicity', 'Physicians', 'Policy Maker', 'Population', 'Population Study', 'Practice Management', 'Prevention', 'Public Health', 'Quality of life', 'Recurrence', 'Research', 'Research Personnel', 'Research Support', 'Rural', 'Secondary Prevention', 'Symptoms', 'System', 'Technology', 'Time', 'Update', 'Work', 'base', 'clinical epidemiology', 'clinically relevant', 'cost effective', 'electronic data', 'experience', 'functional disability', 'health disparity', 'high risk population', 'hospital readmission', 'innovation', 'insight', 'metropolitan', 'mortality', 'older patient', 'optimal treatments', 'outcome forecast', 'population based', 'prevent', 'socioeconomics', 'surveillance data', 'treatment guidelines', 'trend']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2018,803425,0.02135866565566336
"Enabling large-scale research on autism spectrum disorders through automated processing of EHR using natural language understanding Project Summary/Abstract  With increasing use of electronic medical records for a variety of patients, a large investment is being made in a resource still vastly underused. Especially in mental health, where problems are highly individualized, requiring personalized intervention, and often accompanied by rich data not easily captured in structured templates, the need for extracting information from free text in existing records for use as large-scale stand- alone datasets or in combination with other data is real. Without scalable and effective computational approaches to capture this data, much time, effort and money is used to create limited-use records that instead could be leveraged into precious data sources to inform existing research and lead to new insights, progress and treatments. Our broad, long-term goal is processing free text in EHR in mental health. We focus on Autism Spectrum Disorders (ASD), a particularly interesting example of both shortcomings and opportunities.  ASD’s prevalence has increased over the years, and estimates range from 1 in 150 in 2000 to 1 in 68 in 2010(1-5). These numbers are based on surveillance using electronic health records. The increasing prevalence is not well understood, and hypotheses range from changing diagnostic criteria to environmental factors. The lines of inquiry used to find cures are similarly broad and range from brain scans and genetics, resulting in large structured datasets, to highly individualized therapies, resulting in rich but unstructured data. Currently the text information in the electronic records is not being leveraged on a large scale.  The proposed project continues our preliminary work and uses a data-driven approach to create human- interpretable models that allow automated extraction of relevant structured data from free text. The Diagnostic and Statistical Manual of Mental Disorders (DSM) is the starting point for identifying features. A database of thousands of records is leveraged to design and test the algorithms. The two specific aims are: 1) design and test natural language processing (NLP) algorithms to detect DSM criteria for ASD in free text in EHR, and 2) demonstrate feasibility and usefulness of the models for large-scale analysis of ASD cases, which is inconceivable today with current approaches. Our methods include analysis of free text in electronic records and end-user annotations to create a large gold standard of instances of DSM criteria for ASD, application of machine learning and rule-based approaches to create human-interpretable models for automated annotation of diagnostic patterns in textual records, and demonstrate usefulness with new research (e.g., Automatically detect ASD vs. no-ASD status for challenging cases; evaluate prevalence of symptoms over time). Through NLP algorithms, this project has the potential to significantly shift away from the current paradigm of attempting to understand ASD by relying on small-scale data from individual interventions and lack of integration between different data sources, to leveraging information from existing large-scale data sources to propose novel analyses and hypotheses. Project Narrative  Lack of sophisticated tools to extract relevant diagnostic patterns from free text from the increasingly large number of electronic medical/health records is a critical barrier in the field of mental health to leverage and utilize the already available data. Natural language processing (NLP) algorithms designed specifically for mental health can make new data analysis and integration with other sources possible at a scale previously unseen. Using a data-driven process, this project will design NLP algorithms to annotate free text with criteria from the Diagnostic and Statistical Manual of Mental Disorders (DSM) and demonstrate scope, feasibility and usefulness by focusing on Autism Spectrum Disorders (ASD) where prevalence is increasing and much rich clinical text is stored in electronic health records (EHR).",Enabling large-scale research on autism spectrum disorders through automated processing of EHR using natural language understanding,9547263,R21HS024988,[' '],AHRQ,UNIVERSITY OF ARIZONA,R21,2018,146202,-0.04988165897278136
"Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign Project Summary/Abstract Written The objective of the proposed research is to reduce the patient safety hazards associated with electronic medication administration records (eMARs) by, (1) understanding current usability and safety gaps, and (2) creating design and development documents, wireframes, and prototypes to serve as the foundation for future eMARs that will eliminate these gaps. In particular, we focus on communication and information flow challenges between nurses, pharmacists, and physicians during medication administration and use of the eMAR. The proposed research is in direct response to special emphasis notice (NOT-HS-16-009). We will develop a broad understanding of usability and safety hazards associated with eMARs by analyzing a large dataset of 1.7 million patient safety event reports and detailed medication error related narratives. We will then conduct heuristic analyses of current eMARs, and interviews and observations of physicians, nurses, and pharmacists. These data will serve to inform the development of eMAR design documents, wireframes, and prototypes as the foundation for future development. This project utilizes the extensive expertise of the research team in human factors and safety science, health information technology (health IT), informatics, and data science. Our research team includes physicians, nurses, pharmacists, and human factors engineers, and experts in natural language processing. In addition, our partnerships include a patient safety organization and a health IT vendor. The proposal addresses fundamental aspects of the call for proposals by providing new insights on the safety of health IT and improves current practices by developing use cases and new prototypes for immediate use by healthIT vendors. Contributions from this research will include a fundamental understanding of the role of health IT during medication administration with a focus on communication and information flow, design, development and testing documents for vendors and providers, and eMAR wireframes and prototypes to improve development. Our research will also provide organizations like the Office of the National Coordinator with medication related test scenarios to assess current health IT systems. Project Narrative This project is relevant to public health because it applies the sciences of human factors and informatics to improve the usability and safety of the electronic medication administration records (eMARs), which will ultimately improve patient care. Patient safety event report data will be analyzed to identify usability and safety hazards in eMARs, and interviews and observations will be conducted to identify clinician needs. Based on this knowledge eMAR design documents, wireframes, and prototypes will be developed, tested, and disseminated with the goal of improving communication and information flow to reduce medication related errors.",Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign,9447854,R01HS025136,[' '],AHRQ,MEDSTAR HEALTH RESEARCH INSTITUTE,R01,2018,400000,0.029230258671862613
"Developing Evidence for Safety Surveillance from Device Adverse Event Reports Title: Developing Evidence for Safety Surveillance from Device Adverse Event Reports Project Summary/Abstract Population level studies have shown that the device-based hysteroscopic sterilization was associated with increased risks of reoperation during follow-up, when compared to traditional laparoscopic sterilization. However, secondary data sources often lack the granularity to understand the nature of patient and device complications related to the device removal and additional surgery. The Manufacturer and User Facility Device Experience (MAUDE) database houses medical device reports submitted to the FDA by mandatory and voluntary reporters. These reports contain detailed information of patient and device adverse events. But due to its narrative structure, research with the reports has been limited, partially due to the restrictions of keyword search and manual review. The proposed study will innovatively apply natural language processing (NLP) to analyze MAUDE reports of device removals. NLP is a powerful tool capable of extracting information efficiently from documents such as medical notes, allowing the summarization of thousands of adverse event reports in a cost-effective way. The primary aim is to develop an NLP program to extract and summarize patient- and device-specific complications associated with device removal and additional surgeries following hysteroscopic sterilization. Secondary objective is to evaluate the impact of regulatory activities on adverse event reporting behavior and structure. The hypotheses are that the majority of reported removals were associated with device-related complications as opposed to persistent symptoms only, and that after the FDA convened a panel discussion in September 2015, adverse event reports were more likely to be submitted by mandatory reporters, with improvement in structured presentation. Adverse event reports related to device removal will be selected from the MAUDE database using keyword search first, and 1,000 reports will be annotated and used to develop and validate the NLP tool. Applying the developed NLP to all reports, extracted information will be used for the analysis, and comparisons will be made before and after September 2015. The significance of the proposed research is that it will develop a method to better utilize adverse event reports to obtain crucial device safety information supplemental to regular population-level studies. By achieving this, the long term goal is to create a useful tool for future medical device safety surveillance to understand the nature of adverse events. The immediate next step will be to use the tool to investigate device safety in other areas. The comprehension of the nature of device adverse events and the elucidation of the crucial role of regulatory activity in facilitating reliable adverse event reporting will help promote patient safety evaluation and monitoring. Project Narrative The proposed research will focus on device adverse event reports and develop a powerful tool to analyze reports to understand the nature of patient and device complications related to device removals and additional surgeries. The study will also elucidate the impact of regulatory activities on adverse event reporting. Thus, the proposed research is relevant to part of AHRQ's mission to make health care safer.",Developing Evidence for Safety Surveillance from Device Adverse Event Reports,9586941,R03HS026291,[' '],AHRQ,WEILL MEDICAL COLL OF CORNELL UNIV,R03,2018,96907,0.01806655311242886
"Interactive machine learning methods for clinical natural language processing DESCRIPTION (provided by applicant): Growing deployments of electronic health records (EHRs) systems have made massive clinical data available electronically. However, much of detailed clinical information of patients is embedded in narrative text and is not directly accessible for computerized clinical applications. Therefore, natural language processing (NLP) technologies, which can unlock information in narrative document, have received great attention in the medical domain. Current state-of-the-art NLP approaches often involve building probabilistic models. However, the wide adoption of statistical methods in clinical NLP faces two grand challenges: 1) the lack of large annotated clinical corpora; and 2) the lack of methodologies that can efficiently integrate linguistic and domain knowledge with statistical learning. High-performance statistical NLP methods rely on large scale and high quality annotations of clinical text, but it is time-consuming and costly to create large annotated clinica corpora as it often requires manual review by physicians. Moreover, the medical domain is knowledge intensive. To achieve optimal performance, probabilistic models need to leverage medical domain knowledge. Therefore, methods that can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost would be highly desirable for clinical text processing.    In this study, we propose to investigate interactive machine learning (IML) methods to address the above challenges in clinical NLP. An IML system builds a classification model in an iterative process, which can actively select informative samples for annotation based on models built on previously annotated samples, thus reducing the annotation cost for model development. More importantly, an IML system also involves human inputs to the learning process (e.g., an expert can specify important features for a classification task based on domain knowledge). Thus, IML is an ideal framework for efficiently integrating rule-based (via domain experts specifying features) and statistics-based (via different learning algorithms) approaches to clinical NLP. To achieve our goal, we propose three specific aims. In Aim 1, we plan to investigate different aspects of IML for word sense disambiguation, including developing new active learning algorithms and conducting cognitive usability analysis for efficient feature annotation by users. To demonstrate the broad uses of IML, we further extend IML approaches to two other important clinical NLP classification tasks: named entity recognition and clinical phenoytping in Aim 2. Finally we propose to disseminate the IML methods and tools to the biomedical research community in Aim 3. Project Narrative In this project, we propose to develop interactive machine learning methods to process clinical text stored in electronic health records (EHRs) systems. Such methods can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost, thus improving performance of text processors. This technology will allow more accurate data extraction from clinical documents, thus to facilitate clinical research that rely on EHRs data.",Interactive machine learning methods for clinical natural language processing,9337267,R01LM010681,"['Abbreviations', 'Active Learning', 'Address', 'Adoption', 'Algorithms', 'Attention', 'Biomedical Research', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electronic Health Record', 'Face Processing', 'Goals', 'Grant', 'Human', 'Hybrids', 'Informatics', 'Knowledge', 'Label', 'Learning', 'Linguistics', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Research', 'Research Personnel', 'Research Priority', 'Resources', 'Sampling', 'Source', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Supervision', 'System', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'base', 'clinical application', 'clinical phenotype', 'cohort', 'computer human interaction', 'computerized', 'cost', 'experience', 'improved', 'learning strategy', 'model development', 'novel', 'open source', 'real world application', 'statistics', 'success', 'tool', 'usability', 'word learning']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2017,463061,0.04384407401116556
"Natural language processing for characterizing psychopathology ﻿    DESCRIPTION (provided by applicant):  Convergent genetic and epidemiologic evidence suggests the importance of understanding psychiatric illness from a dimensional rather than solely a categorical perspective. The limitations of traditional diagnostic categories motivated a major NIMH-supported effort to identify measures of psychopathology that more closely align with underlying disease biology.  At present, however, the available large clinical data sets, whether health claims, registries, or electronic health records, do not include such dimensional measures. Even with the integration of structure clinician and patient-reported outcomes, generating such cohorts could require a decade or more. Moreover, coded data does not systematically capture clinically-important concepts such as health behaviors or stressors.  While such cohorts are developed, natural language processing can facilitate the application of existing electronic health records to enable precision medicine in psychiatry. Specifically, while traditional natural language tools focus on extracting individual terms, emerging methods including those in development by the investigators allow extraction of concepts and dimensions.  The present investigation proposes to develop a toolkit for natural language processing of narrative patient notes to extract measures of psychopathology, including estimated RDoC domains. In preliminary investigations in a large health system, these tools have demonstrated both face validity and predictive validity. This toolkit also allows extraction o complex concepts from narrative notes, such as stressors and health behaviors.  In the proposed study, these natural language processing tools will be applied to a large psychiatric inpatient data set as well as a large general medical inpatient data set, to derive measures of psychopathology and other topics. The resulting measures will then be used in combination with coded data to build regression and machine-learning-based models to predict clinical outcomes including length of hospital stay and risk of readmission. The models will then be validated in independent clinical cohorts.  By combining expertise in longitudinal clinical investigation, natural language processing, and machine learning, the proposed study brings together a team with the needed skills to develop a critical toolkit for understanding health records dimensionally The resulting models can be applied to facilitate investigation of dimensions of psychopathology and related topics, allowing stratification of clinical risk to enable development of targeted interventions. PUBLIC HEALTH RELEVANCE:  Public health significance many aspects of psychiatric illness are not adequately captured by diagnostic codes. This study will apply natural language processing and machine learning to electronic health records from large health systems. The resulting symptom dimensions will allow better stratification of risk for clinically-important outcomes, including prolonged hospital stays and early readmissions.",Natural language processing for characterizing psychopathology,9254614,R01MH106577,"['Admission activity', 'Antidepressive Agents', 'Applaud', 'Area', 'Back', 'Biology', 'Categories', 'Clinical', 'Clinical Data', 'Clinical stratification', 'Code', 'Complex', 'DSM-IV', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electronic Health Record', 'Epidemiology', 'Face', 'Genetic', 'Health', 'Health behavior', 'Health system', 'Healthcare Systems', 'Hospitals', 'Individual', 'Inpatients', 'Intervention', 'Interview', 'Investigation', 'Length of Stay', 'Machine Learning', 'Measures', 'Medical', 'Mental Depression', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Moods', 'National Institute of Mental Health', 'Natural Language Processing', 'New England', 'Outcome', 'Patient Outcomes Assessments', 'Patients', 'Penetration', 'Pharmaceutical Preparations', 'Psychiatric Diagnosis', 'Psychiatry', 'Psychopathology', 'Public Health', 'Registries', 'Reporting', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Risk', 'Risk stratification', 'Severities', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Text', 'United States National Academy of Sciences', 'Work', 'base', 'clinical investigation', 'clinically relevant', 'cohort', 'cost', 'health data', 'health record', 'hospital readmission', 'improved', 'natural language', 'neuropsychiatric symptom', 'novel', 'outcome prediction', 'precision medicine', 'predict clinical outcome', 'public health relevance', 'skills', 'stressor', 'success', 'terabyte', 'tool', 'translational scientist', 'trend']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,R01,2017,377260,0.019263596652595357
"IGF::OT::IGF  Natural Language Processing Meeting, December 8-9, 2016; POP November 4, 2016 - February 3, 2017. Labor for NLP Natural Language Processing Meeting, December 8-9, 2016; POP November 4, 2016 - February 3, 2017. Labor for NLP n/a","IGF::OT::IGF  Natural Language Processing Meeting, December 8-9, 2016; POP November 4, 2016 - February 3, 2017. Labor for NLP",9581371,61201400011I,"['Natural Language Processing', 'meetings']",NCI,"SCIENTIFIC CONSULTING GROUP, INC.",N01,2017,9923,0.06208724333348831
"Open Health Natural Language Processing Collaboratory Project Summary One of the major barriers in leveraging Electronic Health Record (EHR) data for clinical and translational science is the prevalent use of unstructured or semi-structured clinical narratives for documenting clinical information. Natural Language Processing (NLP), which extracts structured information from narratives, has received great attention and has played a critical role in enabling secondary use of EHRs for clinical and translational research. As demonstrated by large scale efforts such as ACT (Accrual of patients for Clinical Trials), eMERGE, and PCORnet, using EHR data for research rests on the capabilities of a robust data and informatics infrastructure that allows the structuring of clinical narratives and supports the extraction of clinical information for downstream applications. Current successful NLP use cases often require a strong informatics team (with NLP experts) to work with clinicians to supply their domain knowledge and build customized NLP engines iteratively. This requires close collaboration between NLP experts and clinicians, not feasible at institutions with limited informatics support. Additionally, the usability, portability, and generalizability of the NLP systems are still limited, partially due to the lack of access to EHRs across institutions to train the systems. The limited availability of EHR data limits the training available to improve the workforce competence in clinical NLP. We aim to address the above challenges by extending our existing collaboration among multiple CTSA hubs on open health natural language processing (OHNLP) to share distributional information of NLP artifacts (i.e., words, n-grams, phrases, sentences, concept mentions, concepts, and text segments) acquired from real EHRs across multiple institutions. We will leverage the advanced privacy-preserving computing infrastructure of iDASH (integrating Data for Analysis, Anonymization, and SHaring) for privacy- preserving data analysis models and will partner with diverse communities including Observational Health Data Sciences and Informatics (OHDSI), Precision Medicine Initiative (PMI), PCORnet, and Rare Diseases Clinical Research Network (RDCRN) to demonstrate the utility of NLP for translational research. This CTSA innovation award RFA provides us with a unique opportunity to address the challenges faced with clinical NLP and through strong partnership with multiple research communities and leadership roles of the research team in clinical NLP, we envision that the successful delivery of this project will broaden the utilization of clinical NLP across the research community. There are four aims planned: i) obtain PHI-suppressed NLP artifacts with retained distribution information across multiple institutions and assess the privacy risk of accessing PHI- suppressed artifacts, ii) generate a synthetic text corpus for exploratory analysis of clinical narratives and assess its utility in NLP tasks leveraging various NLP challenges, iii) develop privacy-preserving computational phenotyping models empowered with NLP, and iv) partner with diverse communities to demonstrate the utility of our project for translational research. Project Narratives The proposed project aims to broaden the secondary use of electronic health records (EHRs) across the research community by combining innovative privacy-preserving computing techniques and clinical natural language processing.",Open Health Natural Language Processing Collaboratory,9385056,U01TR002062,"['Address', 'Algorithms', 'Attention', 'Award', 'Biological Preservation', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Detection', 'Disease', 'Electronic Health Record', 'Ensure', 'Familial Hypercholesterolemia', 'Frequencies', 'Health', 'Hepatolenticular Degeneration', 'Individual', 'Informatics', 'Information Distribution', 'Institution', 'Kidney Calculi', 'Knowledge', 'Leadership', 'Learning', 'Measures', 'Medical', 'Meta-Analysis', 'Minnesota', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Natural Language Processing', 'Observational Study', 'Patients', 'Phenotype', 'Play', 'Precision Medicine Initiative', 'Privacy', 'Process', 'Rare Diseases', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rest', 'Risk', 'Role', 'Sampling', 'Security', 'Semantics', 'Site', 'Source', 'Structure', 'System', 'Talents', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Translational Research', 'Universities', 'Work', 'base', 'citizen science', 'cohort', 'collaboratory', 'data registry', 'empowered', 'health data', 'improved', 'indexing', 'individual patient', 'informatics infrastructure', 'innovation', 'interest', 'novel', 'phenotypic data', 'phrases', 'portability', 'statistics', 'tool', 'usability', 'virtual']",NCATS,MAYO CLINIC ROCHESTER,U01,2017,1589604,0.09652376214957706
"Semi-structured Information Retrieval in Clinical Text for Cohort Identification DESCRIPTION (provided by applicant):  Natural Language Processing (NLP) techniques have shown promise for extracting data from the free text of electronic health records (EHRs), but studies have consistently found that techniques do not readily generalize across application settings. Unfortunately, most of the focus in applying NLP to real use cases has remained on a paradigm of single, well-defined application settings, so that generalizability to unseen use cases remains implicitly unaddressed. We propose to explicitly account for unseen application settings by adopting an information retrieval (IR) perspective with the objective of patient-level cohort identification. To do so, we introduce layered language models, an IR framework that enables the reuse of NLP-produced artifacts. Our long term goal is to accelerate investigations of patient health and disease by providing robust, user- centric tools that are necessary to process, retrieve, and utilize the free text of EHRs. The main goal of this proposal is to accurately retrieve ad hoc, realistic cohorts from clinical text at Mayo Clinic and OHSU, establishing methods, resources, and evaluation for patient-level IR. We hypothesize that cohort identification can be addressed in a generalizable fashion by a new IR framework: layered language models. We will test this hypothesis through four specific aims. In Aim 1, we will make medical NLP artifacts searchable in our layered language IR framework. This involves storing and indexing the NLP artifacts, as well as using statistical language models to retrieve documents based on text and its associated NLP artifacts. In Aim 2, we deal with the practical setting of ad hoc cohort identification, moving to patient-level (rather than document-level) IR. To accurately handle patient cohorts in which qualifying evidence may be spread over multiple documents, we will develop and implement patient-level retrieval models that account for cross- document relational and temporal combinations of events. In Aim 3, we will construct parallel IR test collections using EHR data from two sites; a diverse set of cohort queries written by multiple people toward various clinical or epidemiological ends; and assessments of which patients are relevant to which queries at both sites. Finally, in Aim 4, we refine and evaluate patient-level layered language IR on the ad hoc cohort identification task, making comparisons across the users, queries, optimization metrics, and institutions. We will draw additional extrinsic comparisons with pre-existing techniques, e.g., for cohorts from the Electronic Medical Records and Genonmics network. The expected outcomes of the proposed work are: (i) An open-source cohort identification tool, usable by clinicians and epidemiologists, that makes principled use of NLP artifacts for unseen queries; ii) A parallel test collection for cohort identification, includig two intra-institutional document collections, diverse test topics and user-produced text queries, and patient-level judgments of relevance to each query; and (iii) Validation of the reusability of medical NLP via the task of retrieving patient cohorts. PUBLIC HEALTH RELEVANCE:  With the widespread adoption of electronic medical records, one might expect that it would be simple for a medical expert to find things like ""patients in the community who suffer from asthma."" Unfortunately, on top of lab tests, medications, and demographic information, there are observations that a physician writes down as text - which are difficult for a computer to understand. Therefore, we aim to process text so that a computer can understand enough of it, and then search that text along with the rest of a patient's medical record; this will allow clinicians or researchers to find and study patients groups of interest.",Semi-structured Information Retrieval in Clinical Text for Cohort Identification,9325065,R01LM011934,"['Address', 'Adopted', 'Adoption', 'Asthma', 'Clinic', 'Clinical', 'Collection', 'Communities', 'Computerized Medical Record', 'Computers', 'Data', 'Dictionary', 'Disease', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Goals', 'Health', 'Information Retrieval', 'Information Retrieval Systems', 'Institution', 'Interest Group', 'Investigation', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Morphologic artifacts', 'Names', 'Natural Language Processing', 'Ontology', 'Outcome', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Process', 'Publishing', 'Qualifying', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Retrieval', 'Sampling', 'Semantics', 'Site', 'Smoke', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'Weight', 'Work', 'Writing', 'asthmatic patient', 'base', 'cohort', 'improved', 'indexing', 'information model', 'novel', 'open source', 'profiles in patients', 'public health relevance', 'query optimization', 'syntax', 'text searching', 'tool']",NLM,MAYO CLINIC ROCHESTER,R01,2017,387966,0.0519060278373671
"From genomics to natural language processing: A protected environment for research computing in the health science NIH S10 equipment proposal: From genomics to natural language processing:  A protected environment for research computing in the health sciences. Health sciences researchers are often required to manage, mine, and analyze restricted patient data (Protected Health Information, PHI) to facilitate and advance their research aims. They are often required to do this without access to central information technology expertise or resources to facilitate their research aims. These researchers are often left to their own devices to “solve” their research compute and data needs and are challenged due to lack of available resources, barriers from central IT, and/or lack of knowledge of available resources. A further challenge is that “small” data sets— data that researchers could formerly handle on office resources—have morphed and grown into the big data domain through the explosion of technical advances and significant expansion in various research directions. Examples include: genomics research, image analysis, simulation, natural language processing, and mining of EMRs. Therefore, the need exists to develop a framework for managing and processing this data securely and reliably. This S10 equipment proposal is to replace the “protected environment” (PE) prototype the University of Utah’s Center for High Performance Computing (CHPC) and Department of Biomedical Informatics built six years ago and has operated since. The PE consists of both high performance computing and virtual machine (VM) components and associated storage sufficient to manage, protect and analyze HIPAA protected health information. This environment has been very successful and has grown significantly in scope. CHPC isolated this protected environment in the secured University of Utah Downtown Data Center and setup a network protected logical partition that provided research groups specific access to individual data sets. As the environment and technology developed, CHPC added additional security features such as two-factor authentication for entry and audit/monitoring. Unfortunately, the prototype has reached the point where demand is surpassing capability and all the hardware is aged and off-warranty. To give an idea of users of the virtual machine farm component, the Biomedical Informatics Core (BMIC) REDCap (Research Electronic Data Capture) environment for data collection has over 2,500 users in 1,500 projects supporting over $25M in NIH funding at the University of Utah, including support for more than 25 active NIH R-01 grants. Moreover, the HIPAA compliant protected environment was a key factor that aided passing the recent University of Utah HIPAA audit. The “protected environment” also helped the University of Utah Health Sciences Center and the BMIC justify the NCATS Center Clinical and Translational Science award (1ULTR001067). NIH S10 equipment proposal: From genomics to natural language processing:  A protected environment for research computing in the health sciences. Project Narrative: The proposed “Protected Environment” instrument will provide research computing and data management capabilities for health sciences researchers to properly manage, secure, and analyze HIPAA regulated protected health information. The technology will not only support a large number of clinical trials, but also enable research in Human Genetics and Natural Language Processing of electronic health records.",From genomics to natural language processing: A protected environment for research computing in the health science,9274445,S10OD021644,"['Award', 'Big Data', 'Clinical Sciences', 'Data', 'Data Collection', 'Data Set', 'Devices', 'Environment', 'Equipment', 'Explosion', 'Farming environment', 'Funding', 'Genomics', 'Grant', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'High Performance Computing', 'Image Analysis', 'Individual', 'Information Technology', 'Knowledge', 'Left', 'Mining', 'Monitor', 'Natural Language Processing', 'Patients', 'Research', 'Research Personnel', 'Resources', 'Secure', 'Security', 'Technology', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Utah', 'aged', 'biomedical informatics', 'computerized data processing', 'electronic data', 'prototype', 'simulation', 'virtual']",OD,UNIVERSITY OF UTAH,S10,2017,493595,0.010306334933553615
"EHR Anticoagulants Pharmacovigilance DESCRIPTION (provided by applicant): The timely identification of previously unknown toxicities of cardiovascular drugs is an important, unsolved problem. In the United States, 20% of the 548 drugs introduced into the market between 1975 and 1999 were either withdrawn or acquired a new ""black box"" warning during the 25-year period following initial approval by the Food and Drug Administration. Adverse drug events (ADEs) are an important cause of morbidity and mortality in patients, yet 95% of ADEs are unreported, leading to delays in the detection of previously unknown ADEs and underestimation of the risk to known ADEs. It is known that Electronic Health Record (EHR) notes and lab results contain ADE information and biomedical natural language processing (NLP) provides automated tools that facilitate chart review and thus improve patient surveillance and post-marketing pharmacovigilance. Optimal use of anticoagulants requires accurate and timely detection of ADEs from EHRs. The objectives for this proposal are to develop ""intelligent"" NLP approaches to extract disease, medication, and structured ADE information from EHRs, and then evaluate extracted ADEs for detecting known ADE types as well as clinically unrecognized or novel ADEs whose pattern or effect have not been previously identified. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page PUBLIC HEALTH RELEVANCE: This project proposes innovative intelligent biomedical natural language process approaches to detect ADEs in patients' electronic health records (EHRs), an important step towards ADE surveillance and pharmacovigilance. The focus of this proposal will bon the national priority area of anticoagulants ADE detection, although the algorithms and systems we develop can be extended to other drug classes and diseases. It is anticipated that the data resources, algorithms, and the Anticoagulant Pharmacovigilance Toolkit (APVTK) developed will significantly enhance ADE detection, patient surveillance and post marketing pharmacovigilance.",EHR Anticoagulants Pharmacovigilance,9190384,R01HL125089,"['Adult', 'Adverse drug event', 'Adverse event', 'Algorithms', 'Anticoagulant therapy', 'Anticoagulants', 'Anticoagulation', 'Area', 'Atrial Fibrillation', 'Biometry', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Controlled Vocabulary', 'Data', 'Data Sources', 'Detection', 'Development', 'Disease', 'Electronic Health Record', 'Elements', 'Etiology', 'Frequencies', 'Health Care Costs', 'Health Promotion', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Illinois', 'Injury', 'Institutes', 'Intervention', 'Lead', 'Length of Stay', 'Linguistics', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Marketing', 'Massachusetts', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Ontology', 'Outcomes Research', 'Package Insert', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Prevention', 'Process', 'Regimen', 'Risk', 'Risk Factors', 'Route', 'Safety', 'Schedule', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Structure', 'Supervision', 'System', 'Systems Analysis', 'Text', 'Thromboembolism', 'Time', 'Toxic effect', 'United States', 'United States Food and Drug Administration', 'Universities', 'Venous', 'Work', 'clinical practice', 'cost', 'data resource', 'disorder prevention', 'dosage', 'epidemiology study', 'high risk', 'improved', 'innovation', 'mortality', 'natural language', 'novel', 'open source', 'patient safety', 'predictive modeling', 'prospective', 'public health relevance', 'tool']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2017,1177432,0.03298865761211533
"Identification of Patients with Low Life Expectancy ﻿    DESCRIPTION (provided by applicant): It is increasingly recognized that optimal treatment is not the same for every patient - it depends on the individual patient's circumstances. One important factor that determines the optimal clinical management is the patient's life expectancy, which determines the temporal horizon within which medical decisions have to operate. For example, while adding an extra diabetes medication may save a 40-year-old individual from going blind or developing kidney failure 20 years later, it will not bring any benefits to an 80-yer old with a metastatic malignancy who is expected to live only a few months.  Consequently, it is critical that when we measure quality of care, suggest treatment options to clinicians through clinical decision support or compare different treatment strategies, we take into account the patient's life expectancy. However, currently there are no methods available that can do this with sufficient accuracy.  Most commonly used techniques to assess a patient's mortality risk draw primarily on administrative data and other structured data fields in electronic medical records. This approach leaves out a large amount of information that is only available in narrative documents such as provider notes, radiology reports, etc. In this project we propose to test the hypothesis that application of two novel technologies - artificial intelligence technique Dynamic Logic and natural language processing (NLP) - could leverage the information in narrative electronic documents to significantly improve the accuracy of identification of patients with low life expectancy.  Dynamic Logic allows to circumvent the challenge of combinatorial complexity that limits the number of variables and their combinations that can be considered as predictors of an outcome by most currently used analytical methods. Dynamic Logic makes use of a limited number of iterative approximations to reduce the complexity of a problem with multiple predictor variables from exponential to approximately linear. Utilization of Dynamic Logic will allow us to greatly increase the richness of the models for identification of patients with low life expectancy and ultimately improve their accuracy.  Information such as the patient's functional status that is usually only found in narrative documents may be critical to improving accuracy of identifying frail patients at high mortality risk. Modern NLP techniques can effectively identify key concepts in medical text but until now analytical methods allowed consideration of only a few of pre-selected concepts in prediction models. Combining NLP with Dynamic Logic will allow us to greatly expand the number of concepts from narrative text that could be included in the life expectancy prediction model, likely leading to a considerable improvement in accuracy.  In the proposed translational multidisciplinary project our team that will include experts on artificial intelligence, natural language processing, analysis of data in electronic medical records, and geriatrics will test whether a combination of Dynamic Logic and NLP improves identification of patients at high risk of death. PUBLIC HEALTH RELEVANCE: Identification of patients with limited life expectancy is important for accurate measurement of quality of care delivered by clinicians, clinical decision support in electronic medical records and research that compares different treatment options for patients. However, currently available methods suffer from low accuracy or use information that is not widely available. In this project we propose to combine two advanced computational technologies - artificial intelligence technique Dynamic Logic and natural language processing - to improve accuracy of identification of patients with low life expectancy.",Identification of Patients with Low Life Expectancy,9275946,R01HS024090,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2017,227900,0.04019309786649032
"Extended Methods and Software Development for Health NLP PROJECT SUMMARY There is a deluge of health-related texts in many genres, from the clinical narrative to newswire and social media. These texts are diverse in content, format, and style, and yet they represent complementary facets of biomedical and health knowledge. Natural Language Processing (NLP) holds much promise to extract, understand, and distill valuable information from these overwhelming large and complex streams of data, with the ultimate goal to advance biomedicine and impact the health and wellbeing of patients. There have been a number of success stories in various biomedical NLP applications, but the NLP methods investigated are usually tailored to one specific phenotype and one institution, thus reducing portability and scalability. Moreover, while there has been much work in the processing of clinical texts, other genres of health texts, like narratives and posts authored by health consumers and patients, are lacking solutions to marshal and make sense of the health information they contain. Robust NLP solutions that answer the needs of biomedicine and health in general have not been fully investigated yet. A unified, data-science approach to health NLP enables the exploration of methods and solutions unprecedented up to now.  Our vision is to unravel the information buried in the health narratives by advancing text-processing methods in a unified way across all the genres of texts. The crosscutting theme is the investigation of methods for health NLP (hNLP) made possible by big data, fused with health knowledge. Our proposal moves the field into exploring semi-supervised and fully unsupervised methods, which only succeed when very large amounts of data are leveraged and knowledge is injected into the methods with care. Our hNLP proposal also targets a key challenge of current hNLP research: the lack of shared software. We seek to provide a clearinghouse for software created under this proposal, and as such all developed tools will be disseminated. Starting from the data characteristics of health texts and information needs of stakeholders, we will develop and evaluate methods for information extraction, information understanding. We will translate our research into the publicly available NLP software platform cTAKES, through robust modules for extraction and understanding across all genres of health texts. We will also demonstrate impact of our methods and tools through several use cases, ranging from clinical point of care to public health, to translational and precision medicine, to participatory medicine. Finally, we will disseminate our work through community activities, such as challenges to advance the state of the art in health natural language processing. PROJECT NARRATIVE  There is a deluge of health texts. Natural Language Processing (NLP) holds much promise to unravel valuable information from these large data streams with the goal to advance medicine and the wellbeing of patients. We will advance state-of-the-art NLP by designing robust, scalable methods that leverage health big data, demonstrating relevance on high-impact use cases, and disseminating NLP tools for the research community and public at large.",Extended Methods and Software Development for Health NLP,9199581,R01GM114355,"['Apache', 'Benchmarking', 'Big Data', 'Caring', 'Characteristics', 'Clinical', 'Communication', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Event', 'Foundations', 'Goals', 'Gold', 'Health', 'Information Resources', 'Injectable', 'Institution', 'International', 'Investigation', 'Knowledge', 'Knowledge Extraction', 'Link', 'Literature', 'Marshal', 'Medicine', 'Methods', 'Names', 'Natural Language Processing', 'Ontology', 'Patients', 'Phenotype', 'Philosophy', 'Public Health', 'Research', 'Semantics', 'Solid', 'Standardization', 'Stream', 'Supervision', 'System', 'Terminology', 'Text', 'Translating', 'Translational Research', 'Vision', 'Work', 'commercialization', 'design', 'health knowledge', 'improved', 'information organization', 'method development', 'novel', 'point of care', 'portability', 'precision medicine', 'programs', 'social media', 'software development', 'success', 'syntax', 'tool', 'translational medicine']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2017,798827,0.089182480191455
"Improving Cardiovascular Drug Safety With Automated Bleeding Classification PROJECT SUMMARY Atrial fibrillation (AF) treatment often includes drug therapy with oral anticoagulants (OAC) to prevent stroke. Bleeding, however, is a common complication of these drugs, affecting up to one in four patients. The Center for Medicare and Medicaid Services recently prioritized OAC-related drug safety as a key quality measure. Currently, however, no method exists to accurately identify bleeding events and severity in large populations. Prior methods use diagnoses codes, which lack sensitivity and clinical detail, or manual chart review, which cannot be implemented in large populations. The proposed research aims address this knowledge gap by applying a natural language processing (NLP)-based approach to identify bleeding events and classify severity in a real-world AF population. The tools will be validated in patients treated at a different institution, to ensure reproducibility across provider settings. In addition, we will apply the bleeding classification tool to evaluate the association between bleeding severity and mortality. Dr. Shah is an emerging young investigator whose career development plan is focused on acquiring the biomedical informatics skills to needed to accurately identify and reduce patient harm. Her training plan focuses on learning core competencies in natural language processing, with the goal of turning the wealth of data in the electronic medical record into useable knowledge. She will combine mentorship from established experts and targeted coursework to acquire skills in biomedical informatics, data science, advanced analytic methods, and research leadership. Completion of these research and training aims will create a platform for future R01 proposals by: (i) enabling safety focused comparative effectiveness research in AF (ii) setting the stage to identify bleeding complications in other cardiovascular diseases and (iii) developing a skill set that allows leadership of a multidisciplinary research team. Through this career development plan, Dr. Shah will build upon her prior training in clinical cardiology and research methodology and lay a strong foundation for a high impact research career. PUBLIC HEALTH RELEVANCE Atrial fibrillation affects six million US adults, and the prevalence is expected to double by 2030. As new drug and device treatments for stroke prevention emerge, patients and providers increasingly need accurate methods to monitor and improve patient safety.",Improving Cardiovascular Drug Safety With Automated Bleeding Classification,9294543,K08HL136850,"['Accounting', 'Address', 'Adult', 'Adverse event', 'Affect', 'Age', 'Algorithms', 'Anticoagulants', 'Atrial Fibrillation', 'Biomedical Research', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Code', 'Competence', 'Complication', 'Computer Assisted', 'Computerized Medical Record', 'Data', 'Data Science', 'Development Plans', 'Devices', 'Diagnosis', 'Ensure', 'Event', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Hemorrhage', 'Incidence', 'Institution', 'Interdisciplinary Study', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Letters', 'Link', 'Manuals', 'Measures', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Oral', 'Patients', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Pragmatic clinical trial', 'Prevalence', 'Provider', 'Quality of life', 'Race', 'Records', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Research Training', 'Risk', 'Safety', 'Severities', 'Site', 'Standardization', 'Stroke', 'Stroke prevention', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'United States Centers for Medicare and Medicaid Services', 'Utah', 'Validation', 'Vital Status', 'acute coronary syndrome', 'aging population', 'analytical method', 'base', 'biomedical informatics', 'career', 'career development', 'clinically relevant', 'cohort', 'comparative effectiveness', 'effectiveness research', 'improved', 'interdisciplinary approach', 'medication compliance', 'mortality', 'neglect', 'novel', 'novel therapeutics', 'patient safety', 'personalized medicine', 'portability', 'public health relevance', 'screening', 'sex', 'skills', 'stroke treatment', 'tool']",NHLBI,UNIVERSITY OF UTAH,K08,2017,163080,0.015669787272953595
"Dynamic learning for post-vaccine event prediction using temporal information in VAERS Project Summary Vaccines have been one of the most successful public health interventions to date. They are, however, pharmaceutical products that carry risks. Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine- preventable illnesses. The CDC/FDA Vaccine Adverse Event Reporting System (VAERS) contains up to 30,000 reports per year over the past 25 years. VAERS reports include both structured data (e.g., vaccination date, first onset date, age, and gender) and unstructured narratives that often provide detailed clinical information about the clinical events and the temporal relationship of the series of event occurrences post vaccination. The structured data only provide one onsite date whereas temporal information of the sequence of events post vaccination is contained in the unstructured narratives. Current status –While structured data in the VAERS are widely used, the narratives are generally ignored because of the challenges inherent in working with unstructured data. Without these narratives, potentially valuable information is lost. Goals - In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Specifically, built upon the state-of-art ontology and natural language processing technologies, we will develop and validate a Temporal Information Modeling, Extraction and Reasoning system for Vaccine data (TIMER-V), which will automatically extract post-vaccination events and their temporal relationships from VAERS reports, semantically infer temporal relations, and integrate the exacted unstructured data with the structured data. Furthermore, we will provide and maintain a publicly available data access interface to query the new integrated data repository, which will facilitate vaccine safety research, casual inference, and other temporal related discovery. We will also develop and validate models to predict severe AEs using the co-occurrence or temporal patterns of the series of AEs post vaccination. To the best of our knowledge, this is the first attempt to make use of the unstructured narratives in the VAERS reports to facilitate the temporal related discovery to a broad community of investigators in pharmacology, pharmacoepidemiology, vaccine safety research, among others. Project Narrative Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine-preventable illnesses. In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. Currently the FDA/CDC Vaccine Adverse Event Reporting System (VAERS) only includes one onsite date in its database. The textual narratives in the reports are generally ignored primarily due to their unstructured nature. These narratives, however, contain more detailed information about the series of events that happened after vaccination, which could be valuable for more informed clinical studies. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Our new methods, their applications to VAERS database, and their dissemination will facilitate the entire research network for pursuing temporal related discovery with high methodological rigor.",Dynamic learning for post-vaccine event prediction using temporal information in VAERS,9290660,R01AI130460,"['Abbreviations', 'Address', 'Adverse event', 'Age', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Development', 'Evaluation', 'Event', 'Frequencies', 'Funding', 'Gender', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Informatics', 'Learning', 'Manuals', 'Measles-Mumps-Rubella Vaccine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Pattern', 'Performance', 'Pharmacoepidemiology', 'Pharmacologic Substance', 'Pharmacology', 'Process', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Series', 'Severe Adverse Event', 'Severities', 'Signal Transduction', 'Source', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Validation', 'base', 'data access', 'flexibility', 'improved', 'influenza virus vaccine', 'information model', 'novel', 'predictive modeling', 'public health intervention', 'response', 'vaccine safety']",NIAID,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2017,644888,0.02260942934884757
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk. PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9249484,R01AI117011,"['Animals', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Knowledge', 'Link', 'Literature', 'Location', 'Manuals', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'Zoonoses', 'improved', 'information model', 'interest', 'journal article', 'molecular sequence database', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2017,461012,0.046724346907224205
"Optimal Methods for Notifying Clinicians about Epilepsy Surgery Patients Project Summary Epilepsy is one of the leading neurological disorders in the United States, affecting more than 479,000 children and over 2 million adults. Approximately 30% of epileptic patients have poor seizure control despite antiepileptic medications and are potential candidates for neurosurgical intervention. Early identification and referral of children who are potential surgical candidates is complex and while relevant guidelines exist, there is no standard process to efficiently identify those patients meeting criteria for neurosurgical intervention. Given the large corpus of note-based data available in the electronic health record (EHR), it is challenging for providers to efficiently retain and process all the pertinent patient information. Natural Language Processing (NLP) and machine learning techniques have been successfully used to evaluate clinical notes and make recommendations in the research setting. However, NLP techniques are rarely integrated into practice to provide real-time clinical decision support. We developed and retrospectively evaluated a NLP system to help identify those patients who meet neurosurgical criteria and therefore enable surgical consults and evaluations to occur sooner. Knowing clinical decision support can improve outcomes of care, our proposed research will implement NLP into clinical practice and develop a decision support mechanism to improve the time to surgery for eligible patients. The objective of this project is to implement NLP directly into clinical care and determine the most effective decision support mechanism for provider adherence to epilepsy surgical consult recommendations. The long- term goal of this project is to reduce the time to initial surgery evaluation for patients with intractable epilepsy by integrating NLP-classification criteria into clinical practice. This project is one of the first in the field to study the integration of NLP recommendations into clinical care. We will use a human factors engineering framework to design and to analyze two different alerting methodologies for the best-fit for clinical workflow to produce the optimum provider adherence while reducing alert fatigue. Epilepsy progress notes can be classified across hospitals, and if successful, the system will be implemented in additional pediatric institutions around the United States. Project Narrative Epilepsy affects more than 479,000 children; 30% of those are not controlled with medication and may require surgery. We developed a novel Natural Language Processing (NLP) algorithm that can be integrated into neurology practice to detect patients who may be eligible for epilepsy surgical consults. Once implemented, this research can help to drive successful implementations of NLP and identify and use ideal alerting mechanisms in neurological care.",Optimal Methods for Notifying Clinicians about Epilepsy Surgery Patients,9355161,R21HS024977,[' '],AHRQ,CINCINNATI CHILDRENS HOSP MED CTR,R21,2017,159334,0.047559327561991604
"Improved Disease Stratification Using Electronic Health Records ABSTRACT This Career Development Application describes targeted coursework and mentored research for progression to independent research in the use of electronic health record data for disease subtyping. Electronic health records have demonstrated great promise as a scalable source of data for biomedical research to enable “precision medicine.” Use of natural language processing techniques has enabled computational analysis of specific terms found in free text clinical notes. An improved ability to extract symptom information from clinical notes would improve researchers’ ability to use de-identified data from patient records for discovery of disease subtypes. Symptom-related terms are particularly important in the context of mental health, but also harder to detect in notes than other terms like diseases or drug names.  The research aims of this proposal present a novel approach to scalable extension of biomedical terminologies and improved detection of those terms and their modifiers (e.g. severe, familial, absent). The richer dataset that can be extracted using these enhanced approaches is then used to define patient cohorts and to detect disease subtypes and predictors of response to specific pharmaceutical intervention.  Resulting patient stratification will be compared to groupings made without the enriched data and validated on an independent data set. The overarching hypothesis of this work is that enhanced mining of clinical notes will enable statistically significant and clinically relevant symptom-based stratification of psychiatric disorders. In order to test this hypothesis, I will: Aim 1: Develop a semi-automated pipeline for domain-specific terminology extension Aim 2: Define and stratify patient cohorts through use of enhanced term extraction Aim 3: Evaluate the validity and utility of the richer set of data obtained through Aims 1 and 2  One area of greatest need for more evidence-based disease stratification, and also of greatest challenge for a number of reasons, is that of mental health. Mental health disorders account for 30% of non-fatal disease burden world-wide, and pose an economic burden of trillions of dollars and climbing. Moreover, mental health symptoms are generally subjective and self-reported, with few objectively measurable signs. The impact of this proposal is that it will dramatically improve our ability to use EHR data to stratify patients in this drastically underserved area of health and healthcare.  The major innovations of this project are the adaptation and application of a semi-supervised pattern learning pipeline to augment mental health terminologies, and a novel approach to disease stratification using a significantly underutilized source of biomedical data, namely clinical notes.  This work addresses a major challenge for mining clinical notes in rapidly evolving biomedical domains and leverages a valuable source of medical evidence that is largely untapped and underutilized. Together, these methods for enhanced use of clinical notes will enable identification of distinct patient subgroups using data that is sitting idle in EHRs. Project Narrative Electronic health records have demonstrated great promise as a scalable source of data for biomedical research to enable “precision medicine.” Natural language processing has enabled the use of information from clinical notes in addition to structured data like diagnoses and lab values. This work aims to improve our ability to extract useful information from electronic health records to enable disease subtyping, particularly in the area of mental health.",Improved Disease Stratification Using Electronic Health Records,9453180,K01LM012529,"['Address', 'Area', 'Biological', 'Biomedical Research', 'Categories', 'Classification', 'Clinical', 'Clinical Data', 'Complement', 'Computer Analysis', 'Data', 'Data Set', 'Data Sources', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease stratification', 'Economic Burden', 'Electrocardiogram', 'Electronic Health Record', 'Genotype', 'Grouping', 'Health', 'Healthcare', 'Intervention', 'Learning', 'Measurable', 'Medical', 'Mental Health', 'Mental disorders', 'Mentors', 'Methods', 'Mining', 'Names', 'Natural Language Processing', 'Outcome', 'Patient Self-Report', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phenotype', 'Records', 'Research', 'Research Personnel', 'Rheumatology', 'Severities', 'Source', 'Statistical Models', 'Stratification', 'Structure', 'Subgroup', 'Supervision', 'Symptoms', 'Techniques', 'Terminology', 'Testing', 'Text', 'Work', 'base', 'burden of illness', 'career development', 'clinical decision-making', 'clinically actionable', 'clinically relevant', 'cohort', 'disease classification', 'disorder subtype', 'electronic data', 'evidence base', 'improved', 'innovation', 'novel strategies', 'patient stratification', 'phenotypic data', 'precision medicine', 'predicting response', 'response', 'treatment response']",NLM,DUKE UNIVERSITY,K01,2017,189054,-0.0033848740547602475
"Secondary use of EMRs for surgical complication surveillance DESCRIPTION (provided by applicant):  Recent statistics indicate that worldwide almost 234 million major surgical procedures are performed each year with the rates of major postsurgical complications (PSCs) range from 3% to 16% and rates of permanent disability or death range from 0.4% to 0.8%. Early detection of PSCs is crucial since early intervention could be lifesaving. Meanwhile, with the rapid adoption of electronic medical records (EMRs) and the accelerated advance of health information technology (HIT), detection of PSCs by applying advanced analytics on EMRs makes it possible for near real-time PSC surveillance. We have developed a rule-based PSC surveillance system to detect most frequent colorectal PSCs near real-time from EMRs where a pattern-based natural language processing (NLP) engine is used to extract PSC related information from text and a set of expert rules is used to detect PSCs. Two challenges are identified. First, it is very challenging to integrate a diverse set of relevant data using expert rules. In the past, probabilistic approaches such as Bayesian Network which can integrate a diverse set of relevant data have become popular in clinical decision support and disease outbreak surveillance. Can we implement probabilistic approaches for PSC surveillance? Secondly, a large portion of the clinical information is embedded in text and it has been quite expensive to manually obtain the patterns used in the NLP system since it requires team effort of subject matter experts and NLP specialists. In the research field, statistical NLP has been quite popular. However, decision making in clinical practice demands tractable evidences while models for statistical NLP are not human interpretable. Can we incorporate statistical NLP to accelerate the NLP knowledge engineering process? We hypothesize that a probabilistic approach for PSC surveillance can be developed for improved case detection which can integrate multiple evidences from structured as well as unstructured EMR data. We also hypothesize that empirical NLP can accelerate the knowledge engineering process needed for building pattern- based NLP systems used in practice. Specific aims include: i) developing and evaluating an innovative Bayesian PSC surveillance system that incorporates evidences from both structured and unstructured EMR data; and ii) incorporating and evaluating statistical NLP in accelerating the NLP knowledge engineering process of pattern-based NLP for PSC surveillance. Given the significance of HIT, our study results will advance the science in developing practical NLP systems that can be translated to meet NLP needs in health care practice. Additionally, given the significance of PSCs, our study results will address significant patient safety and quality issues in surgical practice. Utilizing automated methods to detect postsurgical complications will enable early detection of complications compared to other methods and therefore have great potential of improving patient safety and health care quality while reducing cost. The results could lead to large scale PSC surveillance and quality improvement towards safer and better health care. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to unprecedented opportunities to use EMRs for clinical practice and research. We explore the use of EMRs for near real-time postsurgical complication surveillance with the aim of improving health care quality and reducing health care cost through enhanced analytics towards surgical excellence.",Secondary use of EMRs for surgical complication surveillance,9251814,R01EB019403,"['Abscess', 'Address', 'Adoption', 'Age', 'Anesthetics', 'Area', 'Bayesian Method', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Research', 'Colorectal', 'Complex', 'Complication', 'Computerized Medical Record', 'Data', 'Decision Making', 'Detection', 'Development', 'Disease Outbreaks', 'Early Diagnosis', 'Early Intervention', 'Educational workshop', 'Engineering', 'Goals', 'Health Care Costs', 'Healthcare', 'Hemorrhage', 'Human', 'Ileus', 'Knowledge', 'Lead', 'Manuals', 'Methods', 'Minor', 'Motivation', 'Natural Language Processing', 'Nature', 'Nutritional', 'Operative Surgical Procedures', 'Output', 'Patients', 'Pattern', 'Perioperative', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Reporting', 'Research', 'Risk Factors', 'Science', 'Severities', 'Specialist', 'Statistical Models', 'Structure', 'Surgeon', 'Surgical complication', 'System', 'Testing', 'Text', 'Time', 'Translating', 'Uncertainty', 'Work', 'Wound Infection', 'base', 'clinical practice', 'computer based statistical methods', 'cost', 'disability', 'health care quality', 'health information technology', 'improved', 'innovation', 'patient safety', 'public health relevance', 'rapid growth', 'statistics']",NIBIB,MAYO CLINIC ROCHESTER,R01,2017,300000,0.06872473409625994
"Community Surveillance of Coronary Heart Disease Project Summary/Abstract  Preventing the onset of acute myocardial infarction (AMI) and its recurrence, and reducing the morbidity and mortality associated with AMI, remain of significant public health and clinical concern. Monitoring contemporary trends in AMI incidence, treatment, and in-hospital and long-term outcomes is of considerable importance given periodic national updates of treatment guidelines, emphasis on reducing hospital readmissions, and revised definitions and classifications of AMI. Continuously supported by the NHLBI, we have conducted more than 35 years of population-based surveillance of AMI incidence and attack rates, hospital management practices, and the in-hospital and long-term prognosis associated with AMI among residents of central MA hospitalized at all central MA medical centers. We have a highly experienced team of cardiologists, epidemiologists, clinical informatics, and health services researchers who will build on multi- decade long trends (1975-2011) in our principal study endpoints examined previously in this study to the two new study years of patients hospitalized with AMI at all central MA medical centers in 2014 and 2017.  To sustain our efforts into the era of electronic medical records (EMRs), and after implementation of the ICD-10 system in 2015, we will develop a new automated AMI surveillance system that efficiently utilizes EMRs by taking advantage of state-of-art natural language processing (NLP) methods that will be compatible with ICD-10 (Aim 1). We will use the new NLP method to streamline traditional chart review-based collection of socio-demographic, clinical, treatment, and hospital and post-discharge outcomes data in patients hospitalized with AMI at all 11 central MA medical centers in 2014 and 2017. The data extracted from NLP-streamlined chart reviews will be used to validate and refine the NLP system. Issues related to changes from ICD-9 to ICD- 10 will be carefully addressed. The new NLP-enriched EMR-based surveillance system will eventually be implemented in all participating central MA hospitals. Using the NLP-enriched and EMR-based surveillance data, we will monitor the contemporary clinical epidemiology of AMI, and out-of-hospital deaths due to coronary disease, and changing landscape, over a more than 40 year period (1975-2017) (Aim 2).  The new EMR-based and NLP-enriched system will enhance the population-based surveillance of acute coronary disease. This new system will be cost-effective, more efficient and near-real time, have greater accuracy and precision, and can be readily updated to accommodate changes in information technologies and broadly applicable to other hospital systems. It will support our continued efforts to provide unique community- based observational data on several populations that are often excluded from clinical trials, and that are increasing in numbers, namely the elderly and patients with multiple morbidities. Furthermore, it will generate critical data to inform more national clinical guidelines on the enhanced prevention and management of AMI. If successful, the system can serve as a model and be implemented statewide in MA and elsewhere in the US. Project Narrative  The results of the proposed community-based study will provide data about 40 year trends with regards to the changing magnitude of, and outcomes associated with, heart attacks in residents of a large central New England community. The results of this investigation will also provide contemporary insights on how patients who experience heart attacks in the community are treated by physicians.",Community Surveillance of Coronary Heart Disease,9215012,R01HL135219,"['Acute', 'Acute myocardial infarction', 'Address', 'Adoption', 'American', 'Area', 'Big Data', 'Caring', 'Case Fatality Rates', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Informatics', 'Clinical Treatment', 'Clinical Trials', 'Code', 'Collection', 'Communities', 'Computerized Medical Record', 'Coronary', 'Coronary heart disease', 'Data', 'Data Sources', 'Development', 'Diagnostic', 'Epidemiologist', 'Fostering', 'Future', 'Guidelines', 'Harvest', 'Health Care Costs', 'Health Policy', 'Health Services', 'Hospital Administration', 'Hospitals', 'ICD-9', 'Impairment', 'Incidence', 'Information Technology', 'Inpatients', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Investigation', 'Life Expectancy', 'Manuals', 'Medical', 'Medical center', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Myocardial Infarction', 'Myocardial Reperfusion', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'New England', 'Outcome', 'Participant', 'Patients', 'Periodicity', 'Physicians', 'Policy Maker', 'Population', 'Population Study', 'Practice Management', 'Prevention', 'Public Health', 'Quality of life', 'Recurrence', 'Research', 'Research Personnel', 'Research Support', 'Rural', 'Secondary Prevention', 'Symptoms', 'System', 'Technology', 'Time', 'Update', 'Work', 'base', 'clinical epidemiology', 'clinically relevant', 'cost effective', 'electronic data', 'experience', 'functional disability', 'health disparity', 'high risk population', 'hospital readmission', 'innovation', 'insight', 'metropolitan', 'mortality', 'older patient', 'outcome forecast', 'population based', 'prevent', 'socioeconomics', 'surveillance data', 'trend']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2017,836858,0.02135866565566336
"Enabling large-scale research on autism spectrum disorders through automated processing of EHR using natural language understanding Project Summary/Abstract  With increasing use of electronic medical records for a variety of patients, a large investment is being made in a resource still vastly underused. Especially in mental health, where problems are highly individualized, requiring personalized intervention, and often accompanied by rich data not easily captured in structured templates, the need for extracting information from free text in existing records for use as large-scale stand- alone datasets or in combination with other data is real. Without scalable and effective computational approaches to capture this data, much time, effort and money is used to create limited-use records that instead could be leveraged into precious data sources to inform existing research and lead to new insights, progress and treatments. Our broad, long-term goal is processing free text in EHR in mental health. We focus on Autism Spectrum Disorders (ASD), a particularly interesting example of both shortcomings and opportunities.  ASD’s prevalence has increased over the years, and estimates range from 1 in 150 in 2000 to 1 in 68 in 2010(1-5). These numbers are based on surveillance using electronic health records. The increasing prevalence is not well understood, and hypotheses range from changing diagnostic criteria to environmental factors. The lines of inquiry used to find cures are similarly broad and range from brain scans and genetics, resulting in large structured datasets, to highly individualized therapies, resulting in rich but unstructured data. Currently the text information in the electronic records is not being leveraged on a large scale.  The proposed project continues our preliminary work and uses a data-driven approach to create human- interpretable models that allow automated extraction of relevant structured data from free text. The Diagnostic and Statistical Manual of Mental Disorders (DSM) is the starting point for identifying features. A database of thousands of records is leveraged to design and test the algorithms. The two specific aims are: 1) design and test natural language processing (NLP) algorithms to detect DSM criteria for ASD in free text in EHR, and 2) demonstrate feasibility and usefulness of the models for large-scale analysis of ASD cases, which is inconceivable today with current approaches. Our methods include analysis of free text in electronic records and end-user annotations to create a large gold standard of instances of DSM criteria for ASD, application of machine learning and rule-based approaches to create human-interpretable models for automated annotation of diagnostic patterns in textual records, and demonstrate usefulness with new research (e.g., Automatically detect ASD vs. no-ASD status for challenging cases; evaluate prevalence of symptoms over time). Through NLP algorithms, this project has the potential to significantly shift away from the current paradigm of attempting to understand ASD by relying on small-scale data from individual interventions and lack of integration between different data sources, to leveraging information from existing large-scale data sources to propose novel analyses and hypotheses. Project Narrative  Lack of sophisticated tools to extract relevant diagnostic patterns from free text from the increasingly large number of electronic medical/health records is a critical barrier in the field of mental health to leverage and utilize the already available data. Natural language processing (NLP) algorithms designed specifically for mental health can make new data analysis and integration with other sources possible at a scale previously unseen. Using a data-driven process, this project will design NLP algorithms to annotate free text with criteria from the Diagnostic and Statistical Manual of Mental Disorders (DSM) and demonstrate scope, feasibility and usefulness by focusing on Autism Spectrum Disorders (ASD) where prevalence is increasing and much rich clinical text is stored in electronic health records (EHR).",Enabling large-scale research on autism spectrum disorders through automated processing of EHR using natural language understanding,9381416,R21HS024988,[' '],AHRQ,UNIVERSITY OF ARIZONA,R21,2017,146202,-0.04988165897278136
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005). PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9352296,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2017,249995,0.02180469763923141
"NLP to Identify and Rank Clinically Relevant Information for EHRs in the Emergency Department Summary Timely identification of relevant or “need to know” clinical information about a patient’s history in the acute care setting can be critical for patient safety and medical decision-making. Often the relevant information is buried in unstructured or free-text narratives within the Electronic Health Record (EHR), making it difficult to access in a timely fashion. Currently, it is estimated that over 50% of the EHR is free-text. EHR search tools today are often inefficient, simplistic, and unable to rank or evoke the relevance of information for a particular problem or complaint. This is compounded by the fact that EHRs are amassing clinical information at an exponential rate. While the benefits of having a wealth of information at a provider’s fingertips seem obvious, the time and energy cost of culling through enormous amounts of data creates new issues of decreased efficiency and information overload for providers seeking to identify the most pertinent and relevant information about their patients. In the emergency department, where patients can present with life threatening conditions, timely unlocking of clinically relevant information for a patient’s problem or complaint at the point of care can be critical to medical decision-making and patient safety. In this study, we plan to address this challenge through the development of a sophisticated natural language processing (NLP) search tool to automatically identify and rank clinically relevant information based on the patients presenting complaint. We will accomplish this through the following specific aims: 1) identify and define complaint- specific information elements within a patient’s history and 2) develop and test an NLP-based information retrieval tool. Narrative Patient safety hinges on having right information about the right patient and the right time. Often the relevant information is buried in unstructured or free-text narratives within the Electronic Health Record (EHR), making it difficult to access in a timely fashion. The purpose of this study is to develop and evaluate a sophisticated natural language processing (NLP) search tool to automatically identify and rank clinically relevant information from EHRs that providers rely upon to make medical decisions for their patients. This study comes at an important time where data in the EHRs is increasing at an exponential rate, creating a new problem for clinicians, that of finding all the relevant information for patient’s particular problem. This is particularly true in the emergency department setting where providers have limited if any prior relationship and often have to make quick decisions for patients with life threatening conditions. This search tool will provide a snapshot of clinically relevant information that the providers can view alongside the structured information already in the EHR. We believe this approach has the potential to increase clinician efficiency, decrease healthcare costs by avoiding duplicate diagnostic tests, and provide clinicians with the tools they need to make well-informed medical decisions, thereby improving patient safety and reducing suffering.",NLP to Identify and Rank Clinically Relevant Information for EHRs in the Emergency Department,9342692,R21HS024541,[' '],AHRQ,UNIVERSITY OF COLORADO DENVER,R21,2017,148922,-0.01017900895776612
"Interactive machine learning methods for clinical natural language processing DESCRIPTION (provided by applicant): Growing deployments of electronic health records (EHRs) systems have made massive clinical data available electronically. However, much of detailed clinical information of patients is embedded in narrative text and is not directly accessible for computerized clinical applications. Therefore, natural language processing (NLP) technologies, which can unlock information in narrative document, have received great attention in the medical domain. Current state-of-the-art NLP approaches often involve building probabilistic models. However, the wide adoption of statistical methods in clinical NLP faces two grand challenges: 1) the lack of large annotated clinical corpora; and 2) the lack of methodologies that can efficiently integrate linguistic and domain knowledge with statistical learning. High-performance statistical NLP methods rely on large scale and high quality annotations of clinical text, but it is time-consuming and costly to create large annotated clinica corpora as it often requires manual review by physicians. Moreover, the medical domain is knowledge intensive. To achieve optimal performance, probabilistic models need to leverage medical domain knowledge. Therefore, methods that can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost would be highly desirable for clinical text processing.    In this study, we propose to investigate interactive machine learning (IML) methods to address the above challenges in clinical NLP. An IML system builds a classification model in an iterative process, which can actively select informative samples for annotation based on models built on previously annotated samples, thus reducing the annotation cost for model development. More importantly, an IML system also involves human inputs to the learning process (e.g., an expert can specify important features for a classification task based on domain knowledge). Thus, IML is an ideal framework for efficiently integrating rule-based (via domain experts specifying features) and statistics-based (via different learning algorithms) approaches to clinical NLP. To achieve our goal, we propose three specific aims. In Aim 1, we plan to investigate different aspects of IML for word sense disambiguation, including developing new active learning algorithms and conducting cognitive usability analysis for efficient feature annotation by users. To demonstrate the broad uses of IML, we further extend IML approaches to two other important clinical NLP classification tasks: named entity recognition and clinical phenoytping in Aim 2. Finally we propose to disseminate the IML methods and tools to the biomedical research community in Aim 3. Project Narrative In this project, we propose to develop interactive machine learning methods to process clinical text stored in electronic health records (EHRs) systems. Such methods can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost, thus improving performance of text processors. This technology will allow more accurate data extraction from clinical documents, thus to facilitate clinical research that rely on EHRs data.",Interactive machine learning methods for clinical natural language processing,9132834,R01LM010681,"['Abbreviations', 'Active Learning', 'Address', 'Adoption', 'Algorithms', 'Attention', 'Biomedical Research', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electronic Health Record', 'Face', 'Goals', 'Grant', 'Human', 'Hybrids', 'Knowledge', 'Label', 'Learning', 'Linguistics', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Research', 'Research Personnel', 'Research Priority', 'Resources', 'Sampling', 'Source', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'System', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'base', 'clinical application', 'clinical phenotype', 'cohort', 'computer human interaction', 'computerized', 'cost', 'experience', 'improved', 'learning strategy', 'model building', 'model development', 'novel', 'open source', 'real world application', 'statistics', 'success', 'tool', 'usability']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2016,463405,0.04384407401116556
"Natural language processing for characterizing psychopathology ﻿    DESCRIPTION (provided by applicant):  Convergent genetic and epidemiologic evidence suggests the importance of understanding psychiatric illness from a dimensional rather than solely a categorical perspective. The limitations of traditional diagnostic categories motivated a major NIMH-supported effort to identify measures of psychopathology that more closely align with underlying disease biology.  At present, however, the available large clinical data sets, whether health claims, registries, or electronic health records, do not include such dimensional measures. Even with the integration of structure clinician and patient-reported outcomes, generating such cohorts could require a decade or more. Moreover, coded data does not systematically capture clinically-important concepts such as health behaviors or stressors.  While such cohorts are developed, natural language processing can facilitate the application of existing electronic health records to enable precision medicine in psychiatry. Specifically, while traditional natural language tools focus on extracting individual terms, emerging methods including those in development by the investigators allow extraction of concepts and dimensions.  The present investigation proposes to develop a toolkit for natural language processing of narrative patient notes to extract measures of psychopathology, including estimated RDoC domains. In preliminary investigations in a large health system, these tools have demonstrated both face validity and predictive validity. This toolkit also allows extraction o complex concepts from narrative notes, such as stressors and health behaviors.  In the proposed study, these natural language processing tools will be applied to a large psychiatric inpatient data set as well as a large general medical inpatient data set, to derive measures of psychopathology and other topics. The resulting measures will then be used in combination with coded data to build regression and machine-learning-based models to predict clinical outcomes including length of hospital stay and risk of readmission. The models will then be validated in independent clinical cohorts.  By combining expertise in longitudinal clinical investigation, natural language processing, and machine learning, the proposed study brings together a team with the needed skills to develop a critical toolkit for understanding health records dimensionally The resulting models can be applied to facilitate investigation of dimensions of psychopathology and related topics, allowing stratification of clinical risk to enable development of targeted interventions.         PUBLIC HEALTH RELEVANCE:  Public health significance many aspects of psychiatric illness are not adequately captured by diagnostic codes. This study will apply natural language processing and machine learning to electronic health records from large health systems. The resulting symptom dimensions will allow better stratification of risk for clinically-important outcomes, including prolonged hospital stays and early readmissions.            ",Natural language processing for characterizing psychopathology,9105846,R01MH106577,"['Admission activity', 'Antidepressive Agents', 'Applaud', 'Area', 'Back', 'Biology', 'Categories', 'Clinical', 'Clinical Data', 'Code', 'Complex', 'DSM-IV', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electronic Health Record', 'Electronics', 'Epidemiology', 'Face', 'Genetic', 'Health', 'Health behavior', 'Health system', 'Healthcare Systems', 'Hospitals', 'Individual', 'Inpatients', 'Intervention', 'Interview', 'Investigation', 'Length of Stay', 'Machine Learning', 'Measures', 'Medical', 'Mental Depression', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Moods', 'National Institute of Mental Health', 'Natural Language Processing', 'New England', 'Outcome', 'Patient Outcomes Assessments', 'Patients', 'Penetration', 'Pharmaceutical Preparations', 'Process', 'Psychiatric Diagnosis', 'Psychiatry', 'Psychopathology', 'Public Health', 'Registries', 'Reporting', 'Research Domain Criteria', 'Research Personnel', 'Resources', 'Risk', 'Severities', 'Stratification', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Text', 'United States National Academy of Sciences', 'Work', 'base', 'clinical investigation', 'clinical risk', 'cohort', 'health data', 'health record', 'hospital readmission', 'improved', 'natural language', 'neuropsychiatric symptom', 'novel', 'outcome prediction', 'precision medicine', 'predict clinical outcome', 'public health relevance', 'skills', 'stressor', 'success', 'terabyte', 'tool', 'trend']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,R01,2016,413500,0.019263596652595357
"Semi-structured Information Retrieval in Clinical Text for Cohort Identification DESCRIPTION (provided by applicant):  Natural Language Processing (NLP) techniques have shown promise for extracting data from the free text of electronic health records (EHRs), but studies have consistently found that techniques do not readily generalize across application settings. Unfortunately, most of the focus in applying NLP to real use cases has remained on a paradigm of single, well-defined application settings, so that generalizability to unseen use cases remains implicitly unaddressed. We propose to explicitly account for unseen application settings by adopting an information retrieval (IR) perspective with the objective of patient-level cohort identification. To do so, we introduce layered language models, an IR framework that enables the reuse of NLP-produced artifacts. Our long term goal is to accelerate investigations of patient health and disease by providing robust, user- centric tools that are necessary to process, retrieve, and utilize the free text of EHRs. The main goal of this proposal is to accurately retrieve ad hoc, realistic cohorts from clinical text at Mayo Clinic and OHSU, establishing methods, resources, and evaluation for patient-level IR. We hypothesize that cohort identification can be addressed in a generalizable fashion by a new IR framework: layered language models. We will test this hypothesis through four specific aims. In Aim 1, we will make medical NLP artifacts searchable in our layered language IR framework. This involves storing and indexing the NLP artifacts, as well as using statistical language models to retrieve documents based on text and its associated NLP artifacts. In Aim 2, we deal with the practical setting of ad hoc cohort identification, moving to patient-level (rather than document-level) IR. To accurately handle patient cohorts in which qualifying evidence may be spread over multiple documents, we will develop and implement patient-level retrieval models that account for cross- document relational and temporal combinations of events. In Aim 3, we will construct parallel IR test collections using EHR data from two sites; a diverse set of cohort queries written by multiple people toward various clinical or epidemiological ends; and assessments of which patients are relevant to which queries at both sites. Finally, in Aim 4, we refine and evaluate patient-level layered language IR on the ad hoc cohort identification task, making comparisons across the users, queries, optimization metrics, and institutions. We will draw additional extrinsic comparisons with pre-existing techniques, e.g., for cohorts from the Electronic Medical Records and Genonmics network. The expected outcomes of the proposed work are: (i) An open-source cohort identification tool, usable by clinicians and epidemiologists, that makes principled use of NLP artifacts for unseen queries; ii) A parallel test collection for cohort identification, includig two intra-institutional document collections, diverse test topics and user-produced text queries, and patient-level judgments of relevance to each query; and (iii) Validation of the reusability of medical NLP via the task of retrieving patient cohorts. PUBLIC HEALTH RELEVANCE:  With the widespread adoption of electronic medical records, one might expect that it would be simple for a medical expert to find things like ""patients in the community who suffer from asthma."" Unfortunately, on top of lab tests, medications, and demographic information, there are observations that a physician writes down as text - which are difficult for a computer to understand. Therefore, we aim to process text so that a computer can understand enough of it, and then search that text along with the rest of a patient's medical record; this will allow clinicians or researchers to find and study patients groups of interest.",Semi-structured Information Retrieval in Clinical Text for Cohort Identification,9115996,R01LM011934,"['Accounting', 'Address', 'Adopted', 'Adoption', 'Asthma', 'Clinic', 'Clinical', 'Collection', 'Communities', 'Computerized Medical Record', 'Computers', 'Data', 'Dictionary', 'Disease', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Goals', 'Health', 'Information Retrieval', 'Information Retrieval Systems', 'Institution', 'Interest Group', 'Investigation', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Morphologic artifacts', 'Names', 'Natural Language Processing', 'Outcome', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Process', 'Publishing', 'Qualifying', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Retrieval', 'Sampling', 'Semantics', 'Site', 'Smoke', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'Weight', 'Work', 'Writing', 'asthmatic patient', 'base', 'cohort', 'improved', 'indexing', 'novel', 'open source', 'profiles in patients', 'query optimization', 'syntax', 'text searching', 'tool']",NLM,MAYO CLINIC ROCHESTER,R01,2016,387966,0.0519060278373671
"EMR Adverse Drug Event Detection for Pharmacovigilance DESCRIPTION (provided by applicant): Adverse drug events (ADEs) result in substantial patient morbidity and lead to over 100,000 deaths yearly. The timely identification of previously unknown toxicities of cancer drugs is an important, unsolved problem. In the United States, 20% of the 548 drugs introduced into the market between 1975 and 1999 were either withdrawn or acquired a new ""black box"" warning during the 25-year period following initial approval by the Food and Drug Administration. Adverse drug events are an important cause of morbidity and mortality in patients, yet 95% of ADEs are unreported, leading to delays in the detection of previously unknown ADEs and underestimation of the risk to known ADEs. It is known that Electronic Medical Record (EMR), discharge summaries, and lab results contain ADE information and biomedical natural language processing (BioNLP) provides automated tools that facilitate chart review and thus improve patient surveillance and post-marketing pharmacovigilance. The objectives for this proposal are to develop ""intelligent"" BioNLP approaches to extract disease, medication, and structured ADE information from EMRs, and then evaluate extracted ADEs for detecting known ADE types as well as clinically unrecognized or novel ADEs whose pattern or effect have not been previously identified. PUBLIC HEALTH RELEVANCE: EMR Adverse Drug Event Detection This project proposes innovative intelligent biomedical natural language process approaches to automatically extract adverse drug event from the Electronic Medical Record. It is anticipated that the data resources, algorithms, and the Pharmacovigilance Toolkit developed will significantly enhance ADE detection, patient surveillance and post marketing pharmacovigilance.",EMR Adverse Drug Event Detection for Pharmacovigilance,9123554,U01CA180975,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Antineoplastic Agents', 'Biology', 'Boxing', 'Cancer Center', 'Cancer Research Network', 'Cessation of life', 'Clinical', 'Clinical Oncology', 'Clinical Trials', 'Collaborations', 'Common Terminology Criteria for Adverse Events', 'Comprehensive Cancer Center', 'Computerized Medical Record', 'Data', 'Detection', 'Discipline of Nursing', 'Disease', 'Drug toxicity', 'Elements', 'Future', 'Goals', 'Health Promotion', 'Hematology', 'Hospitals', 'Informatics', 'Injury', 'Inpatients', 'Intervention', 'Knowledge', 'Language', 'Lead', 'Left', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Marketing', 'Massachusetts', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Outpatients', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Prevention', 'Public Health', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Safety', 'Severities', 'Signal Transduction', 'Structure', 'System', 'Terminology', 'Testing', 'Therapeutic', 'Toxic effect', 'United States', 'United States Food and Drug Administration', 'Universities', 'Weight', 'Work', 'abstracting', 'disorder prevention', 'firewall', 'improved', 'innovation', 'insight', 'lenalidomide', 'medical schools', 'mortality', 'novel', 'oncology', 'open source', 'patient safety', 'post-market', 'public health relevance', 'response', 'tool']",NCI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U01,2016,336980,-0.02176217462767972
"EHR Anticoagulants Pharmacovigilance DESCRIPTION (provided by applicant): The timely identification of previously unknown toxicities of cardiovascular drugs is an important, unsolved problem. In the United States, 20% of the 548 drugs introduced into the market between 1975 and 1999 were either withdrawn or acquired a new ""black box"" warning during the 25-year period following initial approval by the Food and Drug Administration. Adverse drug events (ADEs) are an important cause of morbidity and mortality in patients, yet 95% of ADEs are unreported, leading to delays in the detection of previously unknown ADEs and underestimation of the risk to known ADEs. It is known that Electronic Health Record (EHR) notes and lab results contain ADE information and biomedical natural language processing (NLP) provides automated tools that facilitate chart review and thus improve patient surveillance and post-marketing pharmacovigilance. Optimal use of anticoagulants requires accurate and timely detection of ADEs from EHRs. The objectives for this proposal are to develop ""intelligent"" NLP approaches to extract disease, medication, and structured ADE information from EHRs, and then evaluate extracted ADEs for detecting known ADE types as well as clinically unrecognized or novel ADEs whose pattern or effect have not been previously identified. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page PUBLIC HEALTH RELEVANCE: This project proposes innovative intelligent biomedical natural language process approaches to detect ADEs in patients' electronic health records (EHRs), an important step towards ADE surveillance and pharmacovigilance. The focus of this proposal will bon the national priority area of anticoagulants ADE detection, although the algorithms and systems we develop can be extended to other drug classes and diseases. It is anticipated that the data resources, algorithms, and the Anticoagulant Pharmacovigilance Toolkit (APVTK) developed will significantly enhance ADE detection, patient surveillance and post marketing pharmacovigilance.",EHR Anticoagulants Pharmacovigilance,9275795,R01HL125089,"['Accounting', 'Adult', 'Adverse drug event', 'Adverse event', 'Algorithms', 'Anticoagulant therapy', 'Anticoagulants', 'Anticoagulation', 'Area', 'Atrial Fibrillation', 'Biometry', 'Boxing', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Controlled Vocabulary', 'Data', 'Data Sources', 'Detection', 'Development', 'Disease', 'Electronic Health Record', 'Elements', 'Epidemiologic Studies', 'Etiology', 'Frequencies', 'Health', 'Health Promotion', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Illinois', 'Injury', 'Institutes', 'Intervention', 'Lead', 'Length of Stay', 'Linguistics', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Marketing', 'Massachusetts', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Ontology', 'Outcomes Research', 'Package Insert', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Prevention', 'Product Packaging', 'Regimen', 'Reporting', 'Resources', 'Risk', 'Risk Factors', 'Route', 'Safety', 'Schedule', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Structure', 'System', 'Systems Analysis', 'Text', 'Thromboembolism', 'Time', 'Toxic effect', 'United States', 'United States Food and Drug Administration', 'Universities', 'Venous', 'Work', 'cost', 'disorder prevention', 'dosage', 'high risk', 'improved', 'innovation', 'mortality', 'novel', 'open source', 'patient safety', 'post-market', 'predictive modeling', 'tool']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2016,305257,0.03298865761211533
"EHR Anticoagulants Pharmacovigilance DESCRIPTION (provided by applicant): The timely identification of previously unknown toxicities of cardiovascular drugs is an important, unsolved problem. In the United States, 20% of the 548 drugs introduced into the market between 1975 and 1999 were either withdrawn or acquired a new ""black box"" warning during the 25-year period following initial approval by the Food and Drug Administration. Adverse drug events (ADEs) are an important cause of morbidity and mortality in patients, yet 95% of ADEs are unreported, leading to delays in the detection of previously unknown ADEs and underestimation of the risk to known ADEs. It is known that Electronic Health Record (EHR) notes and lab results contain ADE information and biomedical natural language processing (NLP) provides automated tools that facilitate chart review and thus improve patient surveillance and post-marketing pharmacovigilance. Optimal use of anticoagulants requires accurate and timely detection of ADEs from EHRs. The objectives for this proposal are to develop ""intelligent"" NLP approaches to extract disease, medication, and structured ADE information from EHRs, and then evaluate extracted ADEs for detecting known ADE types as well as clinically unrecognized or novel ADEs whose pattern or effect have not been previously identified. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page PUBLIC HEALTH RELEVANCE: This project proposes innovative intelligent biomedical natural language process approaches to detect ADEs in patients' electronic health records (EHRs), an important step towards ADE surveillance and pharmacovigilance. The focus of this proposal will bon the national priority area of anticoagulants ADE detection, although the algorithms and systems we develop can be extended to other drug classes and diseases. It is anticipated that the data resources, algorithms, and the Anticoagulant Pharmacovigilance Toolkit (APVTK) developed will significantly enhance ADE detection, patient surveillance and post marketing pharmacovigilance.",EHR Anticoagulants Pharmacovigilance,8976618,R01HL125089,"['Accounting', 'Adult', 'Adverse drug event', 'Adverse event', 'Algorithms', 'Anticoagulant therapy', 'Anticoagulants', 'Anticoagulation', 'Area', 'Atrial Fibrillation', 'Biometry', 'Boxing', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Controlled Vocabulary', 'Data', 'Data Sources', 'Detection', 'Development', 'Disease', 'Electronic Health Record', 'Elements', 'Epidemiologic Studies', 'Etiology', 'Frequencies', 'Health', 'Health Promotion', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Illinois', 'Injury', 'Institutes', 'Intervention', 'Lead', 'Length of Stay', 'Linguistics', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Marketing', 'Massachusetts', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Ontology', 'Outcomes Research', 'Package Insert', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Prevention', 'Product Packaging', 'Regimen', 'Reporting', 'Resources', 'Risk', 'Risk Factors', 'Route', 'Safety', 'Schedule', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Structure', 'System', 'Systems Analysis', 'Text', 'Thromboembolism', 'Time', 'Toxic effect', 'United States', 'United States Food and Drug Administration', 'Universities', 'Venous', 'Work', 'cost', 'disorder prevention', 'dosage', 'high risk', 'improved', 'innovation', 'mortality', 'novel', 'open source', 'patient safety', 'post-market', 'predictive modeling', 'tool']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2016,832278,0.03298865761211533
"Measuring and Improving Colonoscopy Quality Using Natural Language Processing DESCRIPTION (provided by applicant):  Colonoscopy is the predominant method for screening for colorectal cancer in the US. Yet, its effectiveness in screening is limited by variation in performance. For example, the rate at which physicians detect cancer precursors called adenomas during a colonoscopy has been shown to vary three-fold from one physician to another. A patient whose colonoscopy is performed by a physician with a low adenoma detection rate has a higher risk of subsequent colorectal cancer.  Our proposal centers on measuring, understanding, and improving colonoscopy quality. The major innovation of this work is to use natural language processing (NLP) to measure the quality of colonoscopy. NLP is a field of computer science in which a computer is trained to ""read"" text to identify relevant data We developed and validated the first NLP-based computer software application (C-QUAL) that analyzes colonoscopy and associated pathology reports. Our primary quality measure is adenoma detection rate because it is a common, validated measure that is linked to colorectal cancer incidence. However, we also use a number of secondary quality measures. We applied C-QUAL to almost 25,000 colonoscopy reports in one health system and found large variation in physician's performance on the quality measures. Building on this prior work, our goal is to use C-QUAL to measure colonoscopy quality across a spectrum of US practice environments, to understand what drives variation in colonoscopy quality, and to improve colonoscopy quality. In Aim 1, we propose to use the C-QUAL tool to measure performance in 4 diverse health care systems. This will be one of the largest assessments of the variation in adenoma detection rates and will span different geographic regions, payment systems, and practice settings. In Aim 2, we seek to understand why there is variation in quality. We will survey providers at the 4 health care systems about factors that might affect quality. We will link those survey results to the adenoma detection rates assessed in Aim 1 and look for key associations. It is assumed, but not proven, that providing feedback to physicians on colonoscopy quality will improve care. In Aim 3, we assess whether feedback to physicians does drive quality improvement and, building on Aim 2, explore which types of physicians may respond to feedback. Our proposal is the first to use this innovative method to measure colonoscopy quality and to use the quality scores to decrease the variation in colonoscopy performance. Together the results of the 3 aims are consistent with the NCI's focus on improving the quality of colorectal cancer screening. PUBLIC HEALTH RELEVANCE: Prior research has shown that the quality of colonoscopy varies from physician to physician. It is important to address low quality because patients who see physicians with lower quality colonoscopies have a higher risk of developing colorectal cancer in the future. To help address poor quality of colonoscopy, we propose to use a computer software program to measure colonoscopy quality, survey physicians to understand why there is variation in quality, and provide physician feedback to stimulate quality improvement.",Measuring and Improving Colonoscopy Quality Using Natural Language Processing,9025565,R01CA168959,"['Address', 'Affect', 'Cancer Etiology', 'Caring', 'Cecum', 'Cessation of life', 'Colon', 'Colonoscopy', 'Colorectal Cancer', 'Computer software', 'Computers', 'Data', 'Detection', 'Effectiveness', 'Environment', 'Feedback', 'Future', 'Gastroenterology', 'Geographic Locations', 'Goals', 'Health', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidence', 'Knowledge', 'Link', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Monitor', 'Natural Language Processing', 'Pathology Report', 'Patients', 'Performance', 'Physicians', 'Provider', 'Reading', 'Recommendation', 'Records', 'Reporting', 'Research', 'Risk', 'Science', 'Site', 'Societies', 'Sum', 'Surveys', 'System', 'Testing', 'Text', 'Training', 'United States National Institutes of Health', 'Variant', 'Work', 'adenoma', 'base', 'colorectal cancer screening', 'computer science', 'cost effective', 'high risk', 'improved', 'innovation', 'medical specialties', 'novel', 'payment', 'programs', 'screening', 'symposium', 'tool', 'trend']",NCI,HARVARD MEDICAL SCHOOL,R01,2016,409330,0.014249434711725763
"Natural language processing for clinical and translational research DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. This growth is being fueled by recent federal legislation that provides generous financial incentives to institutions demonstrating aggressive application and ""meaningful use"" of comprehensive EMRs. Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large scale studies of disease onset and treatment outcome, specifically within the context of routine clinical care. However, a well-known challenge for secondary use of EMR data for clinical and translational research is that much of detailed patient information is embedded in narrative text. Natural Language Processing (NLP) technologies, which are able to convert unstructured clinical text into coded data, have been introduced into the biomedical domain and have demonstrated promising results. Researchers have used NLP systems to identify clinical syndromes and common biomedical concepts from radiology reports, discharge summaries, problem lists, nursing documentation, and medical education documents. Different NLP systems have been developed at different institutions and utilized to convert clinical narrative text into structured data that may be used for other clinical applications and studies. Successful stories in applying NLP to clinical and translational research have been reported widely. However, institutions often deploy different NLP systems, which produce various types of output formats and make it difficult to exchange information between sites. Therefore, the lack of interoperability among different clinical NLP systems becomes a bottleneck for efficient multi-site studies. In addition, many successful studies often require a strong interdisciplinary team where informaticians and clinicians have to work very closely to iteratively define optimal algorithms for clinical phenotypes. As intensive informatics support may not be available to every clinical researcher, the usability of NLP systems for end users is another important issue. The proposed project builds upon first-hand knowledge and experience across the research team in the use of NLP for clinical and translational research projects. There are several big informatics initiatives for clinical and translational research but those initiatives generally assume one shoe fits all and follow top-down approaches to develop NLP solutions. Complementary to those initiatives, we will use a bottom-up approach to handle interoperability and usability: i) we will obtain a common NLP data model and exchange format through empirical analysis of existing NLP systems and NLP results; ii) we will develop a user-centric NLP front end interface for NLP systems wrapped to be consistent with the proposed NLP data model and exchange format incorporating usability analysis into the agile development process. All deliverables will be distributed through the open health NLP (OHNLP) consortium which we intend to make it more open and inclusive. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. We propose the development of a novel framework to enable the use of clinical information embedded in clinical narratives for clinical and translational research.",Natural language processing for clinical and translational research,9033918,R01GM102282,"['Acceleration', 'Adopted', 'Adoption', 'Adverse drug effect', 'Algorithms', 'Architecture', 'Attention', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'DNA Databases', 'Data', 'Data Set', 'Development', 'Dictionary', 'Discipline of Nursing', 'Disease', 'Documentation', 'Elements', 'Exclusion Criteria', 'Genes', 'Genomics', 'Goals', 'Growth', 'Health', 'Informatics', 'Institution', 'Knowledge', 'Link', 'Logical Observation Identifiers Names and Codes', 'Manuals', 'Medical Education', 'Modeling', 'Natural Language Processing', 'Onset of illness', 'Output', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Play', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Role', 'SNOMED Clinical Terms', 'Semantics', 'Shoes', 'Site', 'Statutes and Laws', 'Structure', 'Syndrome', 'System', 'Technology', 'Text', 'Translational Research', 'Treatment outcome', 'Work', 'base', 'clinical application', 'clinical care', 'clinical phenotype', 'computer human interaction', 'data exchange', 'data modeling', 'experience', 'financial incentive', 'flexibility', 'human centered computing', 'interoperability', 'novel', 'open source', 'patient safety', 'rapid growth', 'success', 'tool', 'usability', 'user-friendly']",NIGMS,MAYO CLINIC ROCHESTER,R01,2016,562809,0.0822176457303292
"Identification of Patients with Low Life Expectancy ﻿    DESCRIPTION (provided by applicant): It is increasingly recognized that optimal treatment is not the same for every patient - it depends on the individual patient's circumstances. One important factor that determines the optimal clinical management is the patient's life expectancy, which determines the temporal horizon within which medical decisions have to operate. For example, while adding an extra diabetes medication may save a 40-year-old individual from going blind or developing kidney failure 20 years later, it will not bring any benefits to an 80-yer old with a metastatic malignancy who is expected to live only a few months.  Consequently, it is critical that when we measure quality of care, suggest treatment options to clinicians through clinical decision support or compare different treatment strategies, we take into account the patient's life expectancy. However, currently there are no methods available that can do this with sufficient accuracy.  Most commonly used techniques to assess a patient's mortality risk draw primarily on administrative data and other structured data fields in electronic medical records. This approach leaves out a large amount of information that is only available in narrative documents such as provider notes, radiology reports, etc. In this project we propose to test the hypothesis that application of two novel technologies - artificial intelligence technique Dynamic Logic and natural language processing (NLP) - could leverage the information in narrative electronic documents to significantly improve the accuracy of identification of patients with low life expectancy.  Dynamic Logic allows to circumvent the challenge of combinatorial complexity that limits the number of variables and their combinations that can be considered as predictors of an outcome by most currently used analytical methods. Dynamic Logic makes use of a limited number of iterative approximations to reduce the complexity of a problem with multiple predictor variables from exponential to approximately linear. Utilization of Dynamic Logic will allow us to greatly increase the richness of the models for identification of patients with low life expectancy and ultimately improve their accuracy.  Information such as the patient's functional status that is usually only found in narrative documents may be critical to improving accuracy of identifying frail patients at high mortality risk. Modern NLP techniques can effectively identify key concepts in medical text but until now analytical methods allowed consideration of only a few of pre-selected concepts in prediction models. Combining NLP with Dynamic Logic will allow us to greatly expand the number of concepts from narrative text that could be included in the life expectancy prediction model, likely leading to a considerable improvement in accuracy.  In the proposed translational multidisciplinary project our team that will include experts on artificial intelligence, natural language processing, analysis of data in electronic medical records, and geriatrics will test whether a combination of Dynamic Logic and NLP improves identification of patients at high risk of death. PUBLIC HEALTH RELEVANCE: Identification of patients with limited life expectancy is important for accurate measurement of quality of care delivered by clinicians, clinical decision support in electronic medical records and research that compares different treatment options for patients. However, currently available methods suffer from low accuracy or use information that is not widely available. In this project we propose to combine two advanced computational technologies - artificial intelligence technique Dynamic Logic and natural language processing - to improve accuracy of identification of patients with low life expectancy.",Identification of Patients with Low Life Expectancy,9115065,R01HS024090,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2016,227891,0.04019309786649032
"Utilizing social media as a resource for mental health surveillance DESCRIPTION (provided by applicant):  Major depressive disorder is one of the most common debilitating illnesses in the United States, with a lifetime prevalence of 16.2%. Currently, nationwide mental health surveillance takes the form of large-scale telephone- based surveys. These surveys have high running costs and require teams of human telephone operators. Even the largest system, the Behavioral Risk Factor Surveillance System, reaches only 0.13% of the US population. Twitter (and other microblog services) offers a rich, if terse, multilingual source of real time data for public health surveillance. Natural Language Processing (NLP) provides techniques and resources to ""unlock"" data from text. We propose using Twitter and NLP as a cost-effective and flexible approach to augmenting current telephone- based surveillance methods for population level depression monitoring.         This grant application has two major strands. First, investigating ethical issues and challenges to privacy that emerge with the use of Twitter data for public health surveillance (Aim One). Second, developing techniques and resources for real-time public health surveillance for mental illness from Twitter (Aim Two &Aim Three). Aim One seeks to investigate and codify our responsibilities as researchers towards Twitter users by engaging with those users directly. With Aim Two, we will build and evaluate Natural Language Processing resources - algorithms, lexicons and taxonomies - to support the identification of depression symptoms in Twitter data. For Aim Three, we will build and evaluate Natural Language Processing modules and services that use Twitter as a data source for monitoring depression levels in the community. The significance of the proposed work lies in three areas. First, our investigations - both empirical and theoretical - will explore ethical issues in the use of Twitter for public health surveillance. This work has the potential to guide future research in the area. Second, in developing and evaluating algorithms and resources for identifying depression from tweets, we are contributing foundational work to the field of NLP. Third, developing these algorithms and resources will provide the bedrock for building social media based surveillance systems which will provide a cost effective means of augmenting current mental health surveillance practice. This proposal is innovative in both its application area (microblogs have not been used before for mental health surveillance), its focus on using NLP to identify depressive symptoms for public health, and in the central role that qualitative bioethical research will play in guiding the work. Project Narrative The proposed research focuses on using advanced Natural Language Processing methods to mine microblog data - in this case, Twitter - for mental health surveillance (specifically, depression surveillance), in order to augment current telephone-based mental health surveillance systems. The research has public health at its core.",Utilizing social media as a resource for mental health surveillance,9127812,R00LM011393,"['Algorithms', 'Applications Grants', 'Area', 'Attitude', 'Behavioral Risk Factor Surveillance System', 'Broadcast Media', 'Cities', 'Cognitive', 'Communities', 'County', 'Data', 'Data Sources', 'Dental', 'Development', 'Disasters', 'Earthquakes', 'Electronic Health Record', 'Epidemiology', 'Ethical Issues', 'Ethics', 'Exercise', 'Guidelines', 'Health', 'Human', 'Influenza A Virus, H1N1 Subtype', 'Interview', 'Investigation', 'Linguistics', 'Location', 'Major Depressive Disorder', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Methods', 'Mining', 'Monitor', 'Natural Language Processing', 'Participant', 'Phase', 'Play', 'Population', 'Population Surveillance', 'Prevalence', 'Privacy', 'Process', 'Psyche structure', 'Public Health', 'Recruitment Activity', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Scheme', 'Services', 'Smoking Status', 'Source', 'Surveillance Methods', 'Surveys', 'System', 'Taxonomy', 'Techniques', 'Telephone', 'Text', 'Time', 'United States', 'Update', 'Work', 'base', 'center for epidemiological studies depression scale', 'cost', 'cost effective', 'depressive symptoms', 'flexibility', 'innovation', 'lexical', 'social media', 'syndromic surveillance', 'text searching', 'tool', 'ward']",NLM,UNIVERSITY OF UTAH,R00,2016,225884,0.060727121737152515
"Extended Methods and Software Development for Health NLP PROJECT SUMMARY There is a deluge of health-related texts in many genres, from the clinical narrative to newswire and social media. These texts are diverse in content, format, and style, and yet they represent complementary facets of biomedical and health knowledge. Natural Language Processing (NLP) holds much promise to extract, understand, and distill valuable information from these overwhelming large and complex streams of data, with the ultimate goal to advance biomedicine and impact the health and wellbeing of patients. There have been a number of success stories in various biomedical NLP applications, but the NLP methods investigated are usually tailored to one specific phenotype and one institution, thus reducing portability and scalability. Moreover, while there has been much work in the processing of clinical texts, other genres of health texts, like narratives and posts authored by health consumers and patients, are lacking solutions to marshal and make sense of the health information they contain. Robust NLP solutions that answer the needs of biomedicine and health in general have not been fully investigated yet. A unified, data-science approach to health NLP enables the exploration of methods and solutions unprecedented up to now.  Our vision is to unravel the information buried in the health narratives by advancing text-processing methods in a unified way across all the genres of texts. The crosscutting theme is the investigation of methods for health NLP (hNLP) made possible by big data, fused with health knowledge. Our proposal moves the field into exploring semi-supervised and fully unsupervised methods, which only succeed when very large amounts of data are leveraged and knowledge is injected into the methods with care. Our hNLP proposal also targets a key challenge of current hNLP research: the lack of shared software. We seek to provide a clearinghouse for software created under this proposal, and as such all developed tools will be disseminated. Starting from the data characteristics of health texts and information needs of stakeholders, we will develop and evaluate methods for information extraction, information understanding. We will translate our research into the publicly available NLP software platform cTAKES, through robust modules for extraction and understanding across all genres of health texts. We will also demonstrate impact of our methods and tools through several use cases, ranging from clinical point of care to public health, to translational and precision medicine, to participatory medicine. Finally, we will disseminate our work through community activities, such as challenges to advance the state of the art in health natural language processing. PROJECT NARRATIVE  There is a deluge of health texts. Natural Language Processing (NLP) holds much promise to unravel valuable information from these large data streams with the goal to advance medicine and the wellbeing of patients. We will advance state-of-the-art NLP by designing robust, scalable methods that leverage health big data, demonstrating relevance on high-impact use cases, and disseminating NLP tools for the research community and public at large.",Extended Methods and Software Development for Health NLP,9029656,R01GM114355,"['Apache', 'Benchmarking', 'Big Data', 'Caring', 'Characteristics', 'Clinical', 'Communication', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Event', 'Foundations', 'Goals', 'Gold', 'Health', 'Information Resources', 'Institution', 'International', 'Investigation', 'Knowledge', 'Knowledge Extraction', 'Link', 'Literature', 'Marshal', 'Medicine', 'Methods', 'Names', 'Natural Language Processing', 'Ontology', 'Patients', 'Phenotype', 'Philosophy', 'Process', 'Public Health', 'Research', 'Semantics', 'Solid', 'Stream', 'System', 'Terminology', 'Text', 'Translating', 'Translational Research', 'Vision', 'Work', 'commercialization', 'design', 'health knowledge', 'improved', 'information organization', 'novel', 'point of care', 'portability', 'precision medicine', 'programs', 'social media', 'software development', 'success', 'syntax', 'tool', 'translational medicine']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2016,844963,0.089182480191455
"InfoMediator: Weaving Clinical Expertise in Online Health Communities DESCRIPTION (provided by applicant): Chronic illness can be overwhelming to patients because it impacts many areas of their daily lives. Accordingly, many patients turn to online health support groups to get social support. In face-to-face patient support groups (F2F), health professional moderators provide clinical expertise within the context of peer-patients' sharing of experience. However, in online health community settings, because health professionals' time and resources are expensive, it is challenging to get health professionals' opinions for thousands of messages posted each day. To solve this problem, I propose to develop methods and techniques that maximize the use of already available clinical expertise online for online peer-patient conversation threads by developing a system, InfoMediator. The InfoMediator will semi-automatically weave health professionals' existing answers to patients' questions into peer-patient conversations by using Natural Language Processing (NLP) techniques complemented by user feedback. As the career development component of this proposal, I deepen my skills and knowledge in NLP necessary for proposed work and patient education. As I develop the NLP techniques and knowledge in patient education, I and my mentors will develop methods and techniques that address weaving clinical expertise within peer-patient conversations by designing, implementing, and evaluating socio-technical aspects of the InfoMediator. The training opportunities provided by this NIH NLM K01 grant, together with the supportive research environment at Michigan State University, will help further extend my existing expertise in human-computer interaction, design, and health informatics to establish my independent informatics research program in patient-centered technologies. Focusing on persons with diabetes, the outcomes of the proposed research will help us understand how to empower persons with diabetes to improve self- efficacy and self-care, while increasing the quality of online health information environment. 8. Project Narrative Because health professionals' time and resources are expensive, it is challenging for online health communities to engage health professionals in thousands of peer-patient conversations in the same way that health professionals moderate patient conversations in face-to-face patient support groups. To address this challenge I propose developing methods and techniques that maximize the use of already available health professionals' expertise online for online health communities and help patients improve self-efficacy and self-care toward managing chronic disease.",InfoMediator: Weaving Clinical Expertise in Online Health Communities,9145272,K01LM011980,"['Address', 'Area', 'Chronic Disease', 'Classification', 'Clinical', 'Community Health', 'Complement', 'Data', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Environment', 'Exposure to', 'Feedback', 'Funding', 'Goals', 'Grant', 'Guidelines', 'Health', 'Health Professional', 'Informatics', 'Interview', 'Knowledge', 'Medical', 'Mentors', 'Methods', 'Michigan', 'Natural Language Processing', 'Nursing Informatics', 'Outcome', 'Output', 'Paper', 'Participant', 'Patient Education', 'Patient-Focused Outcomes', 'Patients', 'Persons', 'Problem Solving', 'Public Health Informatics', 'Reading', 'Research', 'Resources', 'Retrieval', 'Self Care', 'Self Efficacy', 'Social support', 'Specific qualifier value', 'Support Groups', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'biomedical informatics', 'career', 'career development', 'community setting', 'computer human interaction', 'design', 'empowered', 'experience', 'follow-up', 'improved', 'online community', 'patient oriented', 'peer', 'programs', 'prototype', 'search engine', 'skills', 'training opportunity', 'user centered design']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K01,2016,156557,0.015859949697107332
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk.         PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.                ",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9065021,R01AI117011,"['Accounting', 'Animals', 'Applied Research', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Knowledge', 'Link', 'Literature', 'Location', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Taxon', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'improved', 'information model', 'interest', 'journal article', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2016,479735,0.046724346907224205
"Optimal Methods for Notifying Clinicians about Epilepsy Surgery Patients Project Summary Epilepsy is one of the leading neurological disorders in the United States, affecting more than 479,000 children and over 2 million adults. Approximately 30% of epileptic patients have poor seizure control despite antiepileptic medications and are potential candidates for neurosurgical intervention. Early identification and referral of children who are potential surgical candidates is complex and while relevant guidelines exist, there is no standard process to efficiently identify those patients meeting criteria for neurosurgical intervention. Given the large corpus of note-based data available in the electronic health record (EHR), it is challenging for providers to efficiently retain and process all the pertinent patient information. Natural Language Processing (NLP) and machine learning techniques have been successfully used to evaluate clinical notes and make recommendations in the research setting. However, NLP techniques are rarely integrated into practice to provide real-time clinical decision support. We developed and retrospectively evaluated a NLP system to help identify those patients who meet neurosurgical criteria and therefore enable surgical consults and evaluations to occur sooner. Knowing clinical decision support can improve outcomes of care, our proposed research will implement NLP into clinical practice and develop a decision support mechanism to improve the time to surgery for eligible patients. The objective of this project is to implement NLP directly into clinical care and determine the most effective decision support mechanism for provider adherence to epilepsy surgical consult recommendations. The long- term goal of this project is to reduce the time to initial surgery evaluation for patients with intractable epilepsy by integrating NLP-classification criteria into clinical practice. This project is one of the first in the field to study the integration of NLP recommendations into clinical care. We will use a human factors engineering framework to design and to analyze two different alerting methodologies for the best-fit for clinical workflow to produce the optimum provider adherence while reducing alert fatigue. Epilepsy progress notes can be classified across hospitals, and if successful, the system will be implemented in additional pediatric institutions around the United States. Project Narrative Epilepsy affects more than 479,000 children; 30% of those are not controlled with medication and may require surgery. We developed a novel Natural Language Processing (NLP) algorithm that can be integrated into neurology practice to detect patients who may be eligible for epilepsy surgical consults. Once implemented, this research can help to drive successful implementations of NLP and identify and use ideal alerting mechanisms in neurological care.",Optimal Methods for Notifying Clinicians about Epilepsy Surgery Patients,9222109,R21HS024977,[' '],AHRQ,CINCINNATI CHILDRENS HOSP MED CTR,R21,2016,137233,0.047559327561991604
"Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data Adverse drug reactions (ADRs) are a major burden for patients and healthcare, causing preventable  hospitalizations and deaths, and incurring a huge cost. The long-term objective of this proposal is to advance  patient safety and reduce costs by discovering novel serious ADRs through use of automated methods that  combine information from large and varied patient populations as well as from the literature. There have been  considerable advances in pharmacovigilance, but more work is needed. For example, Vioxx, a commonly used  drug, was recently found to cause at least 88,000 occurrences of myocardial infarction, highlighting the  insufficiency of current methods. To date, methods have mainly depended on the use of single sources of data,  primarily from the Federal Food and Drug Administration Adverse Event Reporting System (FAERS) and from  electronic health records (EHRS). Although important, each of the sources has different limitations and  advantages, and therefore, combining the data across them should lead to more effective drug safety  surveillance by increasing the statistical power, and also by allowing each data source to complement the other  sources. We already have developed methods associated with each of the single sources, and therefore, this  is an excellent opportunity to build upon our research accomplishments to advance the state of the art in  pharmacovigilance.   More specifically, we will a) acquire and combine comprehensive clinical data from the electronic health  records (EHRs) of two different health care sites serving diverse populations by utilizing natural language  processing (NLP) to obtain vast quantities of fine-grained data, and then by developing data mining  methodologies on the clinical data to detect novel ADR signals, b) analyze differences in therapy-related risk  factors between the two EHR populations, such as racial and ethnic differences, c) detect ADR signals in the  FAERS database using an established methodology, d) develop improved methods to acquire ADR signals  based on information in the literature, and e) develop methods that utilize the results from the above sources to  maximize effectiveness. We will focus on eight serious ADRs, and collect a high-quality reference standard for  those ADRs so that we will be able to evaluate and compare performance of the different detection methods  individually as well as the methods that combine the sources.   This proposal is well positioned to overcome problems associated with existing automated methods, which  are primarily based on use of individual sources of data. We are confident the methods will be effective  because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in  this proposal presents an excellent chance to leverage heterogeneous data sources to dramatically improve  patient safety and reduce costs. Adverse drug reactions (ADRs) are a major burden for patients and health care, causing preventable  hospitalizations and deaths, and incurring huge costs, and, therefore, continuous post-marketing surveillance  is crucial for patient safety. This proposal aims to improve patient safety and reduce health care costs by  developing effective methods to discover new adverse drug reactions through the combination of information in  the FDA's Adverse Event Reporting System, the literature, and comprehensive clinical data from electronic  health records of two different sites with diverse populations, thereby overcoming limitations that rely mainly on  use of one data source.",Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data,9109047,R01LM010016,"['Academic Medical Centers', 'Address', 'Adverse event', 'Adverse reactions', 'Cereals', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Data', 'Complement', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Drug usage', 'Effectiveness', 'Electronic Health Record', 'Evaluation', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Hospitals', 'Individual', 'Knowledge', 'Lead', 'Literature', 'Logistic Regressions', 'Medical Care Costs', 'Methodology', 'Methods', 'Modeling', 'Myocardial Infarction', 'Natural Language Processing', 'New York', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Population Heterogeneity', 'Positioning Attribute', 'Presbyterian Church', 'Probability', 'Process', 'PubMed', 'Publications', 'Reaction', 'Reference Standards', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Factors', 'Rofecoxib', 'Safety', 'Signal Transduction', 'Site', 'Source', 'Structure', 'System', 'Techniques', 'United States Food and Drug Administration', 'Work', 'base', 'chemical property', 'conditioning', 'cost', 'data mining', 'ethnic difference', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'prevent', 'racial difference', 'research and development', 'text searching']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2016,430717,0.011448944552632533
"Secondary use of EMRs for surgical complication surveillance DESCRIPTION (provided by applicant):  Recent statistics indicate that worldwide almost 234 million major surgical procedures are performed each year with the rates of major postsurgical complications (PSCs) range from 3% to 16% and rates of permanent disability or death range from 0.4% to 0.8%. Early detection of PSCs is crucial since early intervention could be lifesaving. Meanwhile, with the rapid adoption of electronic medical records (EMRs) and the accelerated advance of health information technology (HIT), detection of PSCs by applying advanced analytics on EMRs makes it possible for near real-time PSC surveillance. We have developed a rule-based PSC surveillance system to detect most frequent colorectal PSCs near real-time from EMRs where a pattern-based natural language processing (NLP) engine is used to extract PSC related information from text and a set of expert rules is used to detect PSCs. Two challenges are identified. First, it is very challenging to integrate a diverse set of relevant data using expert rules. In the past, probabilistic approaches such as Bayesian Network which can integrate a diverse set of relevant data have become popular in clinical decision support and disease outbreak surveillance. Can we implement probabilistic approaches for PSC surveillance? Secondly, a large portion of the clinical information is embedded in text and it has been quite expensive to manually obtain the patterns used in the NLP system since it requires team effort of subject matter experts and NLP specialists. In the research field, statistical NLP has been quite popular. However, decision making in clinical practice demands tractable evidences while models for statistical NLP are not human interpretable. Can we incorporate statistical NLP to accelerate the NLP knowledge engineering process? We hypothesize that a probabilistic approach for PSC surveillance can be developed for improved case detection which can integrate multiple evidences from structured as well as unstructured EMR data. We also hypothesize that empirical NLP can accelerate the knowledge engineering process needed for building pattern- based NLP systems used in practice. Specific aims include: i) developing and evaluating an innovative Bayesian PSC surveillance system that incorporates evidences from both structured and unstructured EMR data; and ii) incorporating and evaluating statistical NLP in accelerating the NLP knowledge engineering process of pattern-based NLP for PSC surveillance. Given the significance of HIT, our study results will advance the science in developing practical NLP systems that can be translated to meet NLP needs in health care practice. Additionally, given the significance of PSCs, our study results will address significant patient safety and quality issues in surgical practice. Utilizing automated methods to detect postsurgical complications will enable early detection of complications compared to other methods and therefore have great potential of improving patient safety and health care quality while reducing cost. The results could lead to large scale PSC surveillance and quality improvement towards safer and better health care. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to unprecedented opportunities to use EMRs for clinical practice and research. We explore the use of EMRs for near real-time postsurgical complication surveillance with the aim of improving health care quality and reducing health care cost through enhanced analytics towards surgical excellence.",Secondary use of EMRs for surgical complication surveillance,9050675,R01EB019403,"['Abscess', 'Address', 'Adoption', 'Age', 'Anesthetics', 'Area', 'Bayesian Method', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Research', 'Colorectal', 'Complex', 'Complication', 'Computerized Medical Record', 'Data', 'Decision Making', 'Detection', 'Development', 'Disease Outbreaks', 'Early Diagnosis', 'Early Intervention', 'Educational workshop', 'Engineering', 'Goals', 'Health', 'Health Care Costs', 'Healthcare', 'Hemorrhage', 'Human', 'Ileus', 'Knowledge', 'Lead', 'Manuals', 'Methods', 'Minor', 'Motivation', 'Natural Language Processing', 'Nature', 'Nutritional', 'Operative Surgical Procedures', 'Output', 'Patients', 'Pattern', 'Perioperative', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Reporting', 'Research', 'Risk Factors', 'Science', 'Severities', 'Specialist', 'Statistical Models', 'Structure', 'Surgeon', 'Surgical complication', 'System', 'Testing', 'Text', 'Time', 'Translating', 'Uncertainty', 'Work', 'Wound Infection', 'abstracting', 'base', 'clinical practice', 'computer based statistical methods', 'cost', 'disability', 'health care quality', 'health information technology', 'improved', 'innovation', 'meetings', 'patient safety', 'rapid growth', 'statistics']",NIBIB,MAYO CLINIC ROCHESTER,R01,2016,300000,0.06872473409625994
"Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing ﻿    DESCRIPTION (provided by applicant): The adoption of Electronic Health Record (EHR) systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format, with tremendous potentials, but also equally growing concern for patient confidentiality breaches. Secondary use of clinical data is essential to fulfill the potentials for high quality healthcare, improved healthcare management, and effective clinical research. NIH expects that larger research projects share their research data in a way that protects the confidentiality of the research subjects. De-identification of patient data has been proposed as a solution to both facilitate secondary uses of clinical data, and protect patient data confidentiality. The majority of clinical data found in the EHR is represented as narrative text clinical notes, and de-identification of clinical text is a tedious ad costly manual endeavor. Automated approaches based on Natural Language Processing have been implemented and evaluated, allowing for much faster de- identification than manual approaches. Clinacuity, Inc. proposes to develop a new system to automatically de-identify clinical notes found in the EHR, to then improve the availability of clinical text for secondary uses, as well as ameliorate the protection of patient data confidentiality. To establish the merit and feasibility of such a system, we will work on the following objectives: 1) create a reference standard for training and testing the text de-identification application, a reference standard that will include a random sample of clinical narratives with protected health information annotated by domain experts; 2) develop a prototype to automatically de-identify clinical text in near real-time, a prototype that will implement a novel stepwise hybrid approach to maximize sensitivity first (our priority for de-identification), and then filter out false positives to enhance positive predictive value, and also replace PHI with realistic substitutes for improved protection; and 3) test the prototype with the aforementioned reference standard, using a cross-validation approach for training and testing, and also train and test the prototype with a reference standard from another healthcare organization. Commercial application: To strengthen patient information confidentiality protection, the HITECH Act heightened financial penalties incurred for breaches of PHI, even introducing criminal penalties. These new severe consequences for violation of patient information confidentiality render protection requirements even more obvious, and automatic high-accuracy clinical text de-identification, as offered by the system Clinacuity proposes, will strongly help healthcare and clinical research organizations avoid such consequences. This system has potential commercial applications in clinical research and in healthcare settings. It will improve access to richer, more detailed, and more accurate clinical data (in clinical text) for clinical researchers. It will also ease research data sharing, and help healthcare organizations protect patient data confidentiality. PUBLIC HEALTH RELEVANCE: The adoption of Electronic Health Record systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical daa becoming available in electronic format, with tremendous potentials, but also equally growing concern for patient confidentiality breaches. Secondary use of clinical data is essential to fulfil the potentials for high quality healthcare and effective clinical research. De-identification of patient data has been proposed as a solution to both facilitate secondary uses of clinical data, and protect patient data confidentiality. The majority of clinical data found in the EHR is represented as narrative text clinical notes that have been dictated and transcribed or directly typed in, and de-identification of clinical text is a tedious and costly manual endeavor. Automated approaches based on Natural Language Processing have been implemented and evaluated, allowing for much faster de-identification than manual approaches. The overall goal of this project is to develop a new system to automatically de-identify clinical narrative text in he Electronic Health Record, to then improve the availability of clinical text for secondary uses, as well as ameliorate the protection of patient data confidentiality.",Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing,8982010,R41GM116479,"['Abbreviations', 'Adoption', 'Affect', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Confidentiality of Patient Information', 'Consent', 'Data', 'Direct Costs', 'Electronic Health Record', 'Electronics', 'Enrollment', 'Eponyms', 'Gilbert Disease', 'Goals', 'Growth', 'Health', 'Health Care Research', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Hybrids', 'Improve Access', 'Incentives', 'Informed Consent', 'Manuals', 'Measurement', 'Measures', 'Methods', 'National Institute of General Medical Sciences', 'Natural Language Processing', 'Patient Care', 'Patient Data Privacy', 'Patients', 'Population', 'Predictive Value', 'Privacy', 'Reference Standards', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Retrospective Studies', 'Risk', 'Sampling', 'Structure', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'United States Centers for Medicare and Medicaid Services', 'United States National Institutes of Health', 'Validation', 'Vision', 'Work', 'base', 'commercial application', 'common rule', 'data sharing', 'flexibility', 'health care quality', 'improved', 'novel', 'patient privacy', 'payment', 'prototype', 'statistics', 'text searching']",NIGMS,"CLINACUITY,INC.",R41,2016,223924,-0.02045741802584097
"Challenges in Natural Language Processing in Clinical Text No abstract available Challenges in Natural Language Processing for Clinical Narratives Narrative: This project aims to organize a series of shared task challenges that open electronic health records to the research community for advancing the state of the art in natural language processing in clinical records. The proposed shared tasks are complemented by workshops, conference proceedings, and journal special issues that aim to disseminate the knowledge generated by the challenges.",Challenges in Natural Language Processing in Clinical Text,9597333,R13LM011411,[' '],NLM,GEORGE MASON UNIVERSITY,R13,2016,20000,0.02305625349180746
"Encoding and Processing Patient Allergy Information in EHRs DESCRIPTION (provided by applicant): Allergies affect one in five Americans and are the 5th leading chronic disease in the U.S. Each year, allergies account for more than 17 million outpatient office visits. Although documenting and exchanging allergy information in electronic health records (EHRs) is becoming increasingly important, we still face multiple challenges. These include: lack of well-adopted standard terminologies for representing allergies, frequent entry of allergy information as free-text, and no existing process for reconciling allergy information. In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) conduct analyses on standard terminologies and a large allergy repository to build a comprehensive knowledge base for representing allergy information; 2) design, develop and evaluate a natural language processing (NLP) module for extracting and encoding free-text allergy information and integrate it with an existing NLP system; 3) measure the feasibility and efficiency of the proposed NLP system for the new process of allergy reconciliation; and 4) distribute our methods and tool, so they are widely available to other researchers and healthcare institutions for non-commercial use. PUBLIC HEALTH RELEVANCE: Managing allergy information within the electronic health record (EHR) is vital to ensuring patient safety. The goal of this study is to propose a comprehensive solution to assess existing terminology standards and knowledge bases for representing allergy information, develop and evaluate a natural language processing (NLP) system for extracting and encoding allergy information from free-text clinical documents, and finally measure the feasibility of using NLP output to facilitate the allergy reconciliation proces.",Encoding and Processing Patient Allergy Information in EHRs,9142280,R01HS022728,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2016,491053,0.0852415444145766
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005). PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9146893,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2016,249994,0.02180469763923141
"NLP to Identify and Rank Clinically Relevant Information for EHRs in the Emergency Department Summary Timely identification of relevant or “need to know” clinical information about a patient’s history in the acute care setting can be critical for patient safety and medical decision-making. Often the relevant information is buried in unstructured or free-text narratives within the Electronic Health Record (EHR), making it difficult to access in a timely fashion. Currently, it is estimated that over 50% of the EHR is free-text. EHR search tools today are often inefficient, simplistic, and unable to rank or evoke the relevance of information for a particular problem or complaint. This is compounded by the fact that EHRs are amassing clinical information at an exponential rate. While the benefits of having a wealth of information at a provider’s fingertips seem obvious, the time and energy cost of culling through enormous amounts of data creates new issues of decreased efficiency and information overload for providers seeking to identify the most pertinent and relevant information about their patients. In the emergency department, where patients can present with life threatening conditions, timely unlocking of clinically relevant information for a patient’s problem or complaint at the point of care can be critical to medical decision-making and patient safety. In this study, we plan to address this challenge through the development of a sophisticated natural language processing (NLP) search tool to automatically identify and rank clinically relevant information based on the patients presenting complaint. We will accomplish this through the following specific aims: 1) identify and define complaint- specific information elements within a patient’s history and 2) develop and test an NLP-based information retrieval tool. Narrative Patient safety hinges on having right information about the right patient and the right time. Often the relevant information is buried in unstructured or free-text narratives within the Electronic Health Record (EHR), making it difficult to access in a timely fashion. The purpose of this study is to develop and evaluate a sophisticated natural language processing (NLP) search tool to automatically identify and rank clinically relevant information from EHRs that providers rely upon to make medical decisions for their patients. This study comes at an important time where data in the EHRs is increasing at an exponential rate, creating a new problem for clinicians, that of finding all the relevant information for patient’s particular problem. This is particularly true in the emergency department setting where providers have limited if any prior relationship and often have to make quick decisions for patients with life threatening conditions. This search tool will provide a snapshot of clinically relevant information that the providers can view alongside the structured information already in the EHR. We believe this approach has the potential to increase clinician efficiency, decrease healthcare costs by avoiding duplicate diagnostic tests, and provide clinicians with the tools they need to make well-informed medical decisions, thereby improving patient safety and reducing suffering.",NLP to Identify and Rank Clinically Relevant Information for EHRs in the Emergency Department,9245525,R21HS024541,[' '],AHRQ,UNIVERSITY OF COLORADO DENVER,R21,2016,148922,-0.01017900895776612
"Interactive machine learning methods for clinical natural language processing DESCRIPTION (provided by applicant): Growing deployments of electronic health records (EHRs) systems have made massive clinical data available electronically. However, much of detailed clinical information of patients is embedded in narrative text and is not directly accessible for computerized clinical applications. Therefore, natural language processing (NLP) technologies, which can unlock information in narrative document, have received great attention in the medical domain. Current state-of-the-art NLP approaches often involve building probabilistic models. However, the wide adoption of statistical methods in clinical NLP faces two grand challenges: 1) the lack of large annotated clinical corpora; and 2) the lack of methodologies that can efficiently integrate linguistic and domain knowledge with statistical learning. High-performance statistical NLP methods rely on large scale and high quality annotations of clinical text, but it is time-consuming and costly to create large annotated clinica corpora as it often requires manual review by physicians. Moreover, the medical domain is knowledge intensive. To achieve optimal performance, probabilistic models need to leverage medical domain knowledge. Therefore, methods that can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost would be highly desirable for clinical text processing.    In this study, we propose to investigate interactive machine learning (IML) methods to address the above challenges in clinical NLP. An IML system builds a classification model in an iterative process, which can actively select informative samples for annotation based on models built on previously annotated samples, thus reducing the annotation cost for model development. More importantly, an IML system also involves human inputs to the learning process (e.g., an expert can specify important features for a classification task based on domain knowledge). Thus, IML is an ideal framework for efficiently integrating rule-based (via domain experts specifying features) and statistics-based (via different learning algorithms) approaches to clinical NLP. To achieve our goal, we propose three specific aims. In Aim 1, we plan to investigate different aspects of IML for word sense disambiguation, including developing new active learning algorithms and conducting cognitive usability analysis for efficient feature annotation by users. To demonstrate the broad uses of IML, we further extend IML approaches to two other important clinical NLP classification tasks: named entity recognition and clinical phenoytping in Aim 2. Finally we propose to disseminate the IML methods and tools to the biomedical research community in Aim 3. Project Narrative In this project, we propose to develop interactive machine learning methods to process clinical text stored in electronic health records (EHRs) systems. Such methods can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost, thus improving performance of text processors. This technology will allow more accurate data extraction from clinical documents, thus to facilitate clinical research that rely on EHRs data.",Interactive machine learning methods for clinical natural language processing,8936515,R01LM010681,"['Abbreviations', 'Active Learning', 'Address', 'Adoption', 'Algorithms', 'Attention', 'Biomedical Research', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electronic Health Record', 'Face', 'Goals', 'Grant', 'Human', 'Hybrids', 'Knowledge', 'Label', 'Learning', 'Linguistics', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Research', 'Research Personnel', 'Research Priority', 'Resources', 'Sampling', 'Solutions', 'Source', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'System', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'base', 'clinical application', 'clinical phenotype', 'cohort', 'computer human interaction', 'computerized', 'cost', 'experience', 'improved', 'model building', 'model development', 'novel', 'open source', 'statistics', 'success', 'tool', 'usability']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2015,448348,0.04384407401116556
"IGF::CL::IGF  MEETING SUPPORT FOR THE NATURAL LANGUAGE PROCESSING WORKSHOP OCTOBER 8, 2015. MEETING SUPPORT FOR THE NATURAL LANGUAGE PROCESSING WORKSHOP OCTOBER 8, 2015 n/a","IGF::CL::IGF  MEETING SUPPORT FOR THE NATURAL LANGUAGE PROCESSING WORKSHOP OCTOBER 8, 2015.",9162846,61201400011I,"['Educational workshop', 'Natural Language Processing', 'meetings']",NCI,"SCIENTIFIC CONSULTING GROUP, INC.",N03,2015,16100,-0.012406860367127953
"Challenges in Natural Language Processing for Clinical Narratives DESCRIPTION (provided by applicant): Narratives of electronic health records (EHRs) contain useful information that is difficult to automatically extract, index, search, or interpret. Clinical natural language processing (NLP) technologies for automatic extraction, indexing, searching, and interpretation of EHRs are in development; however, due to privacy concerns related to EHRs, such technologies are usually developed by teams that have privileged access to EHRs in a specific institution. Technologies that are tailored to a specific set of data from a given institution generate inspiring results on that data; however, they can fail to generalize to similar data from other institutions and even other departments from the same institution. Therefore, learning from these technologies and building on them becomes difficult.          In order to improve NLP in EHRs, there is need for head-to-head comparison of approaches that can address a given task on the same data set. Shared-tasks provide one way of conducting systematic head-to- head comparisons. This proposal describes a series of shared-task challenges and conferences, spread over a five year period, that promote the development and evaluation of cutting edge clinical NLP systems by distributing de-identified EHRs to the broad research community, under data use agreements, so that:      *	the state-of-the-art in clinical NLP technologies can be identified and advanced,      *	a set of technologies that enable the use of the information contained in EHR narratives becomes available, and      *	the information from EHR narratives can be made more accessible, for example, for clinical and medical research.          The scientific activities supporting the organization of the shared-task challenges are sponsored in part by Informatics for Integrating Biology and the Bedside (i2b2), grant number U54-LM008748, PI: Kohane.          This proposal aims to organize a series of workshops, conference proceedings, and journal special issues that will accompany the shared-task challenges in order to disseminate the knowledge generated by the challenges. Public health relevance: this proposal will address two main challenges related to the use of clinical narratives for research: availability of clinical records for research and identification of the state of the art in clinical natural language processing (NLP) technologies so that we can push the state of the art forward and so that future work can build on the past. Progress in clinical NLP will improve access to electronic health records for research, and for clinical applications, benefiting healthcare and public health.",Challenges in Natural Language Processing for Clinical Narratives,8913773,R13LM011411,"['Access to Information', 'Address', 'Agreement', 'Biology', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Set', 'Development', 'Distributed Systems', 'Educational workshop', 'Electronic Health Record', 'Evaluation', 'Future', 'Goals', 'Gold', 'Grant', 'Hand', 'Healthcare', 'Improve Access', 'Informatics', 'Institution', 'Journals', 'Knowledge', 'Learning', 'Medical Research', 'Natural Language Processing', 'Privacy', 'Public Health', 'Publications', 'Records', 'Research', 'Rest', 'Series', 'System', 'Systems Development', 'Targeted Research', 'Technology', 'Work', 'clinical application', 'data sharing', 'head-to-head comparison', 'improved', 'indexing', 'practical application', 'public health relevance', 'symposium']",NLM,STATE UNIVERSITY OF NEW YORK AT ALBANY,R13,2015,19800,0.06801985663564217
"Semi-structured Information Retrieval in Clinical Text for Cohort Identification DESCRIPTION (provided by applicant):  Natural Language Processing (NLP) techniques have shown promise for extracting data from the free text of electronic health records (EHRs), but studies have consistently found that techniques do not readily generalize across application settings. Unfortunately, most of the focus in applying NLP to real use cases has remained on a paradigm of single, well-defined application settings, so that generalizability to unseen use cases remains implicitly unaddressed. We propose to explicitly account for unseen application settings by adopting an information retrieval (IR) perspective with the objective of patient-level cohort identification. To do so, we introduce layered language models, an IR framework that enables the reuse of NLP-produced artifacts. Our long term goal is to accelerate investigations of patient health and disease by providing robust, user- centric tools that are necessary to process, retrieve, and utilize the free text of EHRs. The main goal of this proposal is to accurately retrieve ad hoc, realistic cohorts from clinical text at Mayo Clinic and OHSU, establishing methods, resources, and evaluation for patient-level IR. We hypothesize that cohort identification can be addressed in a generalizable fashion by a new IR framework: layered language models. We will test this hypothesis through four specific aims. In Aim 1, we will make medical NLP artifacts searchable in our layered language IR framework. This involves storing and indexing the NLP artifacts, as well as using statistical language models to retrieve documents based on text and its associated NLP artifacts. In Aim 2, we deal with the practical setting of ad hoc cohort identification, moving to patient-level (rather than document-level) IR. To accurately handle patient cohorts in which qualifying evidence may be spread over multiple documents, we will develop and implement patient-level retrieval models that account for cross- document relational and temporal combinations of events. In Aim 3, we will construct parallel IR test collections using EHR data from two sites; a diverse set of cohort queries written by multiple people toward various clinical or epidemiological ends; and assessments of which patients are relevant to which queries at both sites. Finally, in Aim 4, we refine and evaluate patient-level layered language IR on the ad hoc cohort identification task, making comparisons across the users, queries, optimization metrics, and institutions. We will draw additional extrinsic comparisons with pre-existing techniques, e.g., for cohorts from the Electronic Medical Records and Genonmics network. The expected outcomes of the proposed work are: (i) An open-source cohort identification tool, usable by clinicians and epidemiologists, that makes principled use of NLP artifacts for unseen queries; ii) A parallel test collection for cohort identification, includig two intra-institutional document collections, diverse test topics and user-produced text queries, and patient-level judgments of relevance to each query; and (iii) Validation of the reusability of medical NLP via the task of retrieving patient cohorts. PUBLIC HEALTH RELEVANCE:  With the widespread adoption of electronic medical records, one might expect that it would be simple for a medical expert to find things like ""patients in the community who suffer from asthma."" Unfortunately, on top of lab tests, medications, and demographic information, there are observations that a physician writes down as text - which are difficult for a computer to understand. Therefore, we aim to process text so that a computer can understand enough of it, and then search that text along with the rest of a patient's medical record; this will allow clinicians or researchers to find and study patients groups of interest.",Semi-structured Information Retrieval in Clinical Text for Cohort Identification,8928647,R01LM011934,"['Accounting', 'Address', 'Adopted', 'Adoption', 'Asthma', 'Clinic', 'Clinical', 'Collection', 'Communities', 'Computerized Medical Record', 'Computers', 'Data', 'Dictionary', 'Disease', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Goals', 'Health', 'Information Retrieval', 'Information Retrieval Systems', 'Institution', 'Interest Group', 'Investigation', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Morphologic artifacts', 'Names', 'Natural Language Processing', 'Outcome', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Process', 'Publishing', 'Qualifying', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Retrieval', 'Sampling', 'Semantics', 'Site', 'Smoke', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'Weight', 'Work', 'Writing', 'asthmatic patient', 'base', 'cohort', 'improved', 'indexing', 'novel', 'open source', 'query optimization', 'syntax', 'text searching', 'tool']",NLM,MAYO CLINIC ROCHESTER,R01,2015,376327,0.0519060278373671
"EMR Adverse Drug Event Detection for Pharmacovigilance DESCRIPTION (provided by applicant): Adverse drug events (ADEs) result in substantial patient morbidity and lead to over 100,000 deaths yearly. The timely identification of previously unknown toxicities of cancer drugs is an important, unsolved problem. In the United States, 20% of the 548 drugs introduced into the market between 1975 and 1999 were either withdrawn or acquired a new ""black box"" warning during the 25-year period following initial approval by the Food and Drug Administration. Adverse drug events are an important cause of morbidity and mortality in patients, yet 95% of ADEs are unreported, leading to delays in the detection of previously unknown ADEs and underestimation of the risk to known ADEs. It is known that Electronic Medical Record (EMR), discharge summaries, and lab results contain ADE information and biomedical natural language processing (BioNLP) provides automated tools that facilitate chart review and thus improve patient surveillance and post-marketing pharmacovigilance. The objectives for this proposal are to develop ""intelligent"" BioNLP approaches to extract disease, medication, and structured ADE information from EMRs, and then evaluate extracted ADEs for detecting known ADE types as well as clinically unrecognized or novel ADEs whose pattern or effect have not been previously identified. PUBLIC HEALTH RELEVANCE: EMR Adverse Drug Event Detection This project proposes innovative intelligent biomedical natural language process approaches to automatically extract adverse drug event from the Electronic Medical Record. It is anticipated that the data resources, algorithms, and the Pharmacovigilance Toolkit developed will significantly enhance ADE detection, patient surveillance and post marketing pharmacovigilance.",EMR Adverse Drug Event Detection for Pharmacovigilance,8913078,U01CA180975,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Antineoplastic Agents', 'Biology', 'Boxing', 'Cancer Center', 'Cancer Research Network', 'Cessation of life', 'Clinical', 'Clinical Oncology', 'Clinical Trials', 'Collaborations', 'Common Terminology Criteria for Adverse Events', 'Comprehensive Cancer Center', 'Computerized Medical Record', 'Data', 'Detection', 'Discipline of Nursing', 'Disease', 'Drug toxicity', 'Elements', 'Future', 'Goals', 'Health Promotion', 'Hematology', 'Hospitals', 'Informatics', 'Injury', 'Inpatients', 'Intervention', 'Knowledge', 'Language', 'Lead', 'Left', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Marketing', 'Massachusetts', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Outpatients', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Prevention', 'Public Health', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Safety', 'Severities', 'Signal Transduction', 'Structure', 'System', 'Terminology', 'Testing', 'Therapeutic', 'Toxic effect', 'United States', 'United States Food and Drug Administration', 'Universities', 'Weight', 'Work', 'abstracting', 'disorder prevention', 'firewall', 'improved', 'innovation', 'insight', 'lenalidomide', 'medical schools', 'mortality', 'novel', 'oncology', 'open source', 'patient safety', 'post-market', 'public health relevance', 'response', 'tool']",NCI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U01,2015,339117,-0.02176217462767972
"Measuring and Improving Colonoscopy Quality Using Natural Language Processing DESCRIPTION (provided by applicant):  Colonoscopy is the predominant method for screening for colorectal cancer in the US. Yet, its effectiveness in screening is limited by variation in performance. For example, the rate at which physicians detect cancer precursors called adenomas during a colonoscopy has been shown to vary three-fold from one physician to another. A patient whose colonoscopy is performed by a physician with a low adenoma detection rate has a higher risk of subsequent colorectal cancer.  Our proposal centers on measuring, understanding, and improving colonoscopy quality. The major innovation of this work is to use natural language processing (NLP) to measure the quality of colonoscopy. NLP is a field of computer science in which a computer is trained to ""read"" text to identify relevant data We developed and validated the first NLP-based computer software application (C-QUAL) that analyzes colonoscopy and associated pathology reports. Our primary quality measure is adenoma detection rate because it is a common, validated measure that is linked to colorectal cancer incidence. However, we also use a number of secondary quality measures. We applied C-QUAL to almost 25,000 colonoscopy reports in one health system and found large variation in physician's performance on the quality measures. Building on this prior work, our goal is to use C-QUAL to measure colonoscopy quality across a spectrum of US practice environments, to understand what drives variation in colonoscopy quality, and to improve colonoscopy quality. In Aim 1, we propose to use the C-QUAL tool to measure performance in 4 diverse health care systems. This will be one of the largest assessments of the variation in adenoma detection rates and will span different geographic regions, payment systems, and practice settings. In Aim 2, we seek to understand why there is variation in quality. We will survey providers at the 4 health care systems about factors that might affect quality. We will link those survey results to the adenoma detection rates assessed in Aim 1 and look for key associations. It is assumed, but not proven, that providing feedback to physicians on colonoscopy quality will improve care. In Aim 3, we assess whether feedback to physicians does drive quality improvement and, building on Aim 2, explore which types of physicians may respond to feedback. Our proposal is the first to use this innovative method to measure colonoscopy quality and to use the quality scores to decrease the variation in colonoscopy performance. Together the results of the 3 aims are consistent with the NCI's focus on improving the quality of colorectal cancer screening. PUBLIC HEALTH RELEVANCE: Prior research has shown that the quality of colonoscopy varies from physician to physician. It is important to address low quality because patients who see physicians with lower quality colonoscopies have a higher risk of developing colorectal cancer in the future. To help address poor quality of colonoscopy, we propose to use a computer software program to measure colonoscopy quality, survey physicians to understand why there is variation in quality, and provide physician feedback to stimulate quality improvement.",Measuring and Improving Colonoscopy Quality Using Natural Language Processing,8830936,R01CA168959,"['Address', 'Affect', 'Cancer Etiology', 'Caring', 'Cecum', 'Cessation of life', 'Colon', 'Colonoscopy', 'Colorectal Cancer', 'Computer software', 'Computers', 'Data', 'Detection', 'Effectiveness', 'Environment', 'Feedback', 'Future', 'Gastroenterology', 'Geographic Locations', 'Goals', 'Health', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidence', 'Knowledge', 'Link', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Monitor', 'Natural Language Processing', 'Pathology Report', 'Patients', 'Performance', 'Physicians', 'Provider', 'Reading', 'Recommendation', 'Records', 'Reporting', 'Research', 'Risk', 'Science', 'Site', 'Societies', 'Sum', 'Surveys', 'System', 'Testing', 'Text', 'Training', 'United States National Institutes of Health', 'Variant', 'Work', 'adenoma', 'base', 'colorectal cancer screening', 'computer science', 'cost effective', 'high risk', 'improved', 'innovation', 'medical specialties', 'novel', 'payment', 'programs', 'screening', 'symposium', 'tool', 'trend']",NCI,HARVARD MEDICAL SCHOOL,R01,2015,494533,0.014249434711725763
"EHR Anticoagulants Pharmacovigilance     DESCRIPTION (provided by applicant): The timely identification of previously unknown toxicities of cardiovascular drugs is an important, unsolved problem. In the United States, 20% of the 548 drugs introduced into the market between 1975 and 1999 were either withdrawn or acquired a new ""black box"" warning during the 25-year period following initial approval by the Food and Drug Administration. Adverse drug events (ADEs) are an important cause of morbidity and mortality in patients, yet 95% of ADEs are unreported, leading to delays in the detection of previously unknown ADEs and underestimation of the risk to known ADEs. It is known that Electronic Health Record (EHR) notes and lab results contain ADE information and biomedical natural language processing (NLP) provides automated tools that facilitate chart review and thus improve patient surveillance and post-marketing pharmacovigilance. Optimal use of anticoagulants requires accurate and timely detection of ADEs from EHRs. The objectives for this proposal are to develop ""intelligent"" NLP approaches to extract disease, medication, and structured ADE information from EHRs, and then evaluate extracted ADEs for detecting known ADE types as well as clinically unrecognized or novel ADEs whose pattern or effect have not been previously identified. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page         PUBLIC HEALTH RELEVANCE: This project proposes innovative intelligent biomedical natural language process approaches to detect ADEs in patients' electronic health records (EHRs), an important step towards ADE surveillance and pharmacovigilance. The focus of this proposal will bon the national priority area of anticoagulants ADE detection, although the algorithms and systems we develop can be extended to other drug classes and diseases. It is anticipated that the data resources, algorithms, and the Anticoagulant Pharmacovigilance Toolkit (APVTK) developed will significantly enhance ADE detection, patient surveillance and post marketing pharmacovigilance.            ",EHR Anticoagulants Pharmacovigilance,8791564,R01HL125089,"['Accounting', 'Adult', 'Adverse drug event', 'Adverse event', 'Algorithms', 'Anticoagulant therapy', 'Anticoagulants', 'Anticoagulation', 'Area', 'Atrial Fibrillation', 'Biometry', 'Boxing', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Controlled Vocabulary', 'Data', 'Data Sources', 'Detection', 'Development', 'Disease', 'Electronic Health Record', 'Elements', 'Epidemiologic Studies', 'Etiology', 'Frequencies', 'Health', 'Health Promotion', 'Healthcare', 'Hemorrhage', 'Hospitals', 'Illinois', 'Injury', 'Institutes', 'Intervention', 'Lead', 'Length of Stay', 'Linguistics', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Marketing', 'Massachusetts', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Ontology', 'Outcomes Research', 'Package Insert', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Prevention', 'Product Packaging', 'Regimen', 'Reporting', 'Resources', 'Risk', 'Risk Factors', 'Route', 'Safety', 'Schedule', 'Severities', 'Signal Transduction', 'Specific qualifier value', 'Structure', 'System', 'Systems Analysis', 'Text', 'Thromboembolism', 'Time', 'Toxic effect', 'United States', 'United States Food and Drug Administration', 'Universities', 'Venous', 'Work', 'cost', 'disorder prevention', 'dosage', 'high risk', 'improved', 'innovation', 'mortality', 'novel', 'open source', 'patient safety', 'post-market', 'predictive modeling', 'tool']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2015,864744,0.03298865761211533
"Natural language processing for clinical and translational research DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. This growth is being fueled by recent federal legislation that provides generous financial incentives to institutions demonstrating aggressive application and ""meaningful use"" of comprehensive EMRs. Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large scale studies of disease onset and treatment outcome, specifically within the context of routine clinical care. However, a well-known challenge for secondary use of EMR data for clinical and translational research is that much of detailed patient information is embedded in narrative text. Natural Language Processing (NLP) technologies, which are able to convert unstructured clinical text into coded data, have been introduced into the biomedical domain and have demonstrated promising results. Researchers have used NLP systems to identify clinical syndromes and common biomedical concepts from radiology reports, discharge summaries, problem lists, nursing documentation, and medical education documents. Different NLP systems have been developed at different institutions and utilized to convert clinical narrative text into structured data that may be used for other clinical applications and studies. Successful stories in applying NLP to clinical and translational research have been reported widely. However, institutions often deploy different NLP systems, which produce various types of output formats and make it difficult to exchange information between sites. Therefore, the lack of interoperability among different clinical NLP systems becomes a bottleneck for efficient multi-site studies. In addition, many successful studies often require a strong interdisciplinary team where informaticians and clinicians have to work very closely to iteratively define optimal algorithms for clinical phenotypes. As intensive informatics support may not be available to every clinical researcher, the usability of NLP systems for end users is another important issue. The proposed project builds upon first-hand knowledge and experience across the research team in the use of NLP for clinical and translational research projects. There are several big informatics initiatives for clinical and translational research but those initiatives generally assume one shoe fits all and follow top-down approaches to develop NLP solutions. Complementary to those initiatives, we will use a bottom-up approach to handle interoperability and usability: i) we will obtain a common NLP data model and exchange format through empirical analysis of existing NLP systems and NLP results; ii) we will develop a user-centric NLP front end interface for NLP systems wrapped to be consistent with the proposed NLP data model and exchange format incorporating usability analysis into the agile development process. All deliverables will be distributed through the open health NLP (OHNLP) consortium which we intend to make it more open and inclusive. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. We propose the development of a novel framework to enable the use of clinical information embedded in clinical narratives for clinical and translational research.",Natural language processing for clinical and translational research,8826771,R01GM102282,"['Acceleration', 'Adopted', 'Adoption', 'Adverse drug effect', 'Algorithms', 'Architecture', 'Attention', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'DNA Databases', 'Data', 'Data Set', 'Development', 'Dictionary', 'Discipline of Nursing', 'Disease', 'Documentation', 'Elements', 'Exclusion Criteria', 'Genes', 'Genomics', 'Goals', 'Growth', 'Health', 'Informatics', 'Institution', 'Knowledge', 'Link', 'Logical Observation Identifiers Names and Codes', 'Manuals', 'Medical Education', 'Modeling', 'Natural Language Processing', 'Onset of illness', 'Output', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Play', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Role', 'SNOMED Clinical Terms', 'Semantics', 'Shoes', 'Site', 'Solutions', 'Statutes and Laws', 'Structure', 'Syndrome', 'System', 'Technology', 'Text', 'Translational Research', 'Treatment outcome', 'Work', 'base', 'clinical application', 'clinical care', 'clinical phenotype', 'computer human interaction', 'data exchange', 'data modeling', 'experience', 'financial incentive', 'flexibility', 'human centered computing', 'interoperability', 'novel', 'open source', 'patient safety', 'rapid growth', 'success', 'tool', 'usability', 'user-friendly']",NIGMS,MAYO CLINIC ROCHESTER,R01,2015,571551,0.0822176457303292
"IGF::OT::IGF EXPANDING SEER TO INCLUDE MOLECULAR PROFILING IN NON-SMALL CELL LUNG CANCER (NSCLC) This study seeks to leverage electronic pathology (ePath) reports through NLP and machine learning methods to automate the annotation of NSCLC lung cases with results from EGFR and ALK gene mutation testing.  Objectives:  1)	Develop and implement machine-learned predictive NLP models to automatically process ePath reports to ascertain the use of and reported results of EGFR and ALK testing in stage IV non-squamous NSCLC cases.   2)	Conduct a multiphase validation study of the NLP algorithms initially involving cases included in the Kentucky SEER registry, and posteriorly validating the algorithms for cases in the Seattle_Puget Sound SEER registry. 3)	Develop and evaluate an open source, distributable software implementation of the NLP algorithms, an accompanying application programming interface (API), and documentation that can be integrated into SEER*DMS and other registry software applications. n/a",IGF::OT::IGF EXPANDING SEER TO INCLUDE MOLECULAR PROFILING IN NON-SMALL CELL LUNG CANCER (NSCLC),9161889,61201300013I,"['ALK gene', 'Algorithms', 'Automated Annotation', 'Computer software', 'Documentation', 'EGFR gene', 'Electronics', 'Epidermal Growth Factor Receptor', 'Gene Mutation', 'Kentucky', 'Lung', 'Machine Learning', 'Methods', 'Modeling', 'Molecular Profiling', 'Non-Small-Cell Lung Carcinoma', 'Pathology Report', 'Process', 'Registries', 'Reporting', 'Staging', 'Testing', 'c-erbB-1 Proto-Oncogenes', 'open source', 'programs', 'sound', 'validation studies']",NCI,UNIVERSITY OF KENTUCKY,N01,2015,87813,-0.024193238496970613
"IGF::OT::IGF EXPANDING SEER TO INCLUDE MOLECULAR PROFILING IN NON-SMALL CELL LUNG CANCER (NSCLC). The overarching goal of this research proposal is to develop and validate Natural Language Processing (NLP) algorithms for ascertainment of use, results, and techniques employed for EGFR and ALK testing, respectively, from SEER electronic pathology reports of stage IV non-squamous NSCLC cases.  Successful achievement of this goal will occur through the accomplishment of the study objectives outlined below.  Objectives: 1)	 Develop Natural Language Processing (NLP) algorithms to ascertain use, results, and techniques employed for EGFR and ALK testing, respectively, from SEER electronic pathology reports of stage IV non-squamous NSCLC registry cases diagnosed between 09/01/2011 and 12/31/2013. 2)	Conduct a multiphase validation study of NLP algorithms for ascertainment of EGFR and ALK testing initially involving cases included in the Seattle Puget-Sound SEER registry, and posteriorly validating the NLP algorithms in the Kentucky SEER registry. n/a",IGF::OT::IGF EXPANDING SEER TO INCLUDE MOLECULAR PROFILING IN NON-SMALL CELL LUNG CANCER (NSCLC).,9161888,61201300012I,"['Achievement', 'Algorithms', 'Cells', 'Diagnosis', 'Electronics', 'Epidermal Growth Factor Receptor', 'Goals', 'Kentucky', 'Lung', 'Molecular Profiling', 'Natural Language Processing', 'Non-Small-Cell Lung Carcinoma', 'Pathology Report', 'Registries', 'Research Proposals', 'Staging', 'Techniques', 'Testing', 'neoplasm registry', 'sound', 'validation studies']",NCI,FRED HUTCHINSON CANCER RESEARCH CENTER,N01,2015,156435,0.06221866939225699
"Identification of Patients with Low Life Expectancy ﻿    DESCRIPTION (provided by applicant): It is increasingly recognized that optimal treatment is not the same for every patient - it depends on the individual patient's circumstances. One important factor that determines the optimal clinical management is the patient's life expectancy, which determines the temporal horizon within which medical decisions have to operate. For example, while adding an extra diabetes medication may save a 40-year-old individual from going blind or developing kidney failure 20 years later, it will not bring any benefits to an 80-yer old with a metastatic malignancy who is expected to live only a few months.  Consequently, it is critical that when we measure quality of care, suggest treatment options to clinicians through clinical decision support or compare different treatment strategies, we take into account the patient's life expectancy. However, currently there are no methods available that can do this with sufficient accuracy.  Most commonly used techniques to assess a patient's mortality risk draw primarily on administrative data and other structured data fields in electronic medical records. This approach leaves out a large amount of information that is only available in narrative documents such as provider notes, radiology reports, etc. In this project we propose to test the hypothesis that application of two novel technologies - artificial intelligence technique Dynamic Logic and natural language processing (NLP) - could leverage the information in narrative electronic documents to significantly improve the accuracy of identification of patients with low life expectancy.  Dynamic Logic allows to circumvent the challenge of combinatorial complexity that limits the number of variables and their combinations that can be considered as predictors of an outcome by most currently used analytical methods. Dynamic Logic makes use of a limited number of iterative approximations to reduce the complexity of a problem with multiple predictor variables from exponential to approximately linear. Utilization of Dynamic Logic will allow us to greatly increase the richness of the models for identification of patients with low life expectancy and ultimately improve their accuracy.  Information such as the patient's functional status that is usually only found in narrative documents may be critical to improving accuracy of identifying frail patients at high mortality risk. Modern NLP techniques can effectively identify key concepts in medical text but until now analytical methods allowed consideration of only a few of pre-selected concepts in prediction models. Combining NLP with Dynamic Logic will allow us to greatly expand the number of concepts from narrative text that could be included in the life expectancy prediction model, likely leading to a considerable improvement in accuracy.  In the proposed translational multidisciplinary project our team that will include experts on artificial intelligence, natural language processing, analysis of data in electronic medical records, and geriatrics will test whether a combination of Dynamic Logic and NLP improves identification of patients at high risk of death.         PUBLIC HEALTH RELEVANCE: Identification of patients with limited life expectancy is important for accurate measurement of quality of care delivered by clinicians, clinical decision support in electronic medical records and research that compares different treatment options for patients. However, currently available methods suffer from low accuracy or use information that is not widely available. In this project we propose to combine two advanced computational technologies - artificial intelligence technique Dynamic Logic and natural language processing - to improve accuracy of identification of patients with low life expectancy.            ",Identification of Patients with Low Life Expectancy,8942806,R01HS024090,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2015,232521,0.04019309786649032
"Utilizing social media as a resource for mental health surveillance DESCRIPTION (provided by applicant):  Major depressive disorder is one of the most common debilitating illnesses in the United States, with a lifetime prevalence of 16.2%. Currently, nationwide mental health surveillance takes the form of large-scale telephone- based surveys. These surveys have high running costs and require teams of human telephone operators. Even the largest system, the Behavioral Risk Factor Surveillance System, reaches only 0.13% of the US population. Twitter (and other microblog services) offers a rich, if terse, multilingual source of real time data for public health surveillance. Natural Language Processing (NLP) provides techniques and resources to ""unlock"" data from text. We propose using Twitter and NLP as a cost-effective and flexible approach to augmenting current telephone- based surveillance methods for population level depression monitoring.         This grant application has two major strands. First, investigating ethical issues and challenges to privacy that emerge with the use of Twitter data for public health surveillance (Aim One). Second, developing techniques and resources for real-time public health surveillance for mental illness from Twitter (Aim Two &Aim Three). Aim One seeks to investigate and codify our responsibilities as researchers towards Twitter users by engaging with those users directly. With Aim Two, we will build and evaluate Natural Language Processing resources - algorithms, lexicons and taxonomies - to support the identification of depression symptoms in Twitter data. For Aim Three, we will build and evaluate Natural Language Processing modules and services that use Twitter as a data source for monitoring depression levels in the community. The significance of the proposed work lies in three areas. First, our investigations - both empirical and theoretical - will explore ethical issues in the use of Twitter for public health surveillance. This work has the potential to guide future research in the area. Second, in developing and evaluating algorithms and resources for identifying depression from tweets, we are contributing foundational work to the field of NLP. Third, developing these algorithms and resources will provide the bedrock for building social media based surveillance systems which will provide a cost effective means of augmenting current mental health surveillance practice. This proposal is innovative in both its application area (microblogs have not been used before for mental health surveillance), its focus on using NLP to identify depressive symptoms for public health, and in the central role that qualitative bioethical research will play in guiding the work. Project Narrative The proposed research focuses on using advanced Natural Language Processing methods to mine microblog data - in this case, Twitter - for mental health surveillance (specifically, depression surveillance), in order to augment current telephone-based mental health surveillance systems. The research has public health at its core.",Utilizing social media as a resource for mental health surveillance,8911360,R00LM011393,"['Algorithms', 'Applications Grants', 'Area', 'Attitude', 'Behavioral Risk Factor Surveillance System', 'Broadcast Media', 'Cities', 'Cognitive', 'Communities', 'County', 'Data', 'Data Sources', 'Dental', 'Development', 'Disasters', 'Earthquakes', 'Electronic Health Record', 'Epidemiology', 'Ethical Issues', 'Ethics', 'Exercise', 'Guidelines', 'Health', 'Human', 'Influenza A Virus, H1N1 Subtype', 'Interview', 'Investigation', 'Linguistics', 'Location', 'Major Depressive Disorder', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Methods', 'Mining', 'Monitor', 'Natural Language Processing', 'Participant', 'Phase', 'Play', 'Population', 'Population Surveillance', 'Prevalence', 'Privacy', 'Process', 'Psyche structure', 'Public Health', 'Recruitment Activity', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Scheme', 'Services', 'Smoking Status', 'Source', 'Surveillance Methods', 'Surveys', 'System', 'Taxonomy', 'Techniques', 'Telephone', 'Text', 'Time', 'United States', 'Update', 'Work', 'base', 'center for epidemiological studies depression scale', 'cost', 'cost effective', 'depressive symptoms', 'flexibility', 'innovation', 'lexical', 'social', 'syndromic surveillance', 'text searching', 'tool', 'ward']",NLM,UNIVERSITY OF UTAH,R00,2015,217377,0.060727121737152515
"InfoMediator: Weaving Clinical Expertise in Online Health Communities DESCRIPTION (provided by applicant): Chronic illness can be overwhelming to patients because it impacts many areas of their daily lives. Accordingly, many patients turn to online health support groups to get social support. In face-to-face patient support groups (F2F), health professional moderators provide clinical expertise within the context of peer-patients' sharing of experience. However, in online health community settings, because health professionals' time and resources are expensive, it is challenging to get health professionals' opinions for thousands of messages posted each day. To solve this problem, I propose to develop methods and techniques that maximize the use of already available clinical expertise online for online peer-patient conversation threads by developing a system, InfoMediator. The InfoMediator will semi-automatically weave health professionals' existing answers to patients' questions into peer-patient conversations by using Natural Language Processing (NLP) techniques complemented by user feedback. As the career development component of this proposal, I deepen my skills and knowledge in NLP necessary for proposed work and patient education. As I develop the NLP techniques and knowledge in patient education, I and my mentors will develop methods and techniques that address weaving clinical expertise within peer-patient conversations by designing, implementing, and evaluating socio-technical aspects of the InfoMediator. The training opportunities provided by this NIH NLM K01 grant, together with the supportive research environment at Michigan State University, will help further extend my existing expertise in human-computer interaction, design, and health informatics to establish my independent informatics research program in patient-centered technologies. Focusing on persons with diabetes, the outcomes of the proposed research will help us understand how to empower persons with diabetes to improve self- efficacy and self-care, while increasing the quality of online health information environment. 8. Project Narrative Because health professionals' time and resources are expensive, it is challenging for online health communities to engage health professionals in thousands of peer-patient conversations in the same way that health professionals moderate patient conversations in face-to-face patient support groups. To address this challenge I propose developing methods and techniques that maximize the use of already available health professionals' expertise online for online health communities and help patients improve self-efficacy and self-care toward managing chronic disease.",InfoMediator: Weaving Clinical Expertise in Online Health Communities,8929297,K01LM011980,"['Address', 'Area', 'Chronic Disease', 'Classification', 'Clinical', 'Communities', 'Community Health', 'Complement', 'Data', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Environment', 'Exposure to', 'Feedback', 'Funding', 'Goals', 'Grant', 'Guidelines', 'Health', 'Health Professional', 'Informatics', 'Interview', 'Knowledge', 'Medical', 'Mentors', 'Methods', 'Michigan', 'Natural Language Processing', 'Nursing Informatics', 'Outcome', 'Output', 'Paper', 'Participant', 'Patient Education', 'Patients', 'Persons', 'Problem Solving', 'Public Health Informatics', 'Reading', 'Research', 'Resources', 'Retrieval', 'Self Care', 'Self Efficacy', 'Social support', 'Specific qualifier value', 'Support Groups', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'biomedical informatics', 'career', 'career development', 'community setting', 'computer human interaction', 'design', 'empowered', 'experience', 'follow-up', 'improved', 'patient oriented', 'peer', 'programs', 'prototype', 'skills', 'user centered design']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K01,2015,154413,0.015859949697107332
"Using NLP to Extract Clinically Important Recommendations from Radiology Reports  Abstract Communication of clinically important follow-up recommendations when abnormalities are identified on imaging studies is prone to error. The absence of an automated system to identify and track radiology follow-up recommendations is an important barrier to ensuring timely follow-up of patients, especially for non-acute but potentially life threatening and unexpected findings. The primary goal of this proposal is to develop a Natural Language Processing (NLP) system to extract clinically important recommendation information from free-text radiology reports. Each radiology report will be preprocessed at the structural, syntactic, and semantic level to generate features that will be used to extract the boundaries of sentences that include recommendation information as well as the details of reason for recommendation, requested imaging test, and recommendation time frame. We will use a large corpus of free-text radiology reports represented by a mixture of modalities (e.g., radiography, computed tomography, ultrasound, and magnetic resonance imaging (MRI)) from three different institutions. Using this dataset we will perform the following specific aims: Aim 1. Create a multi- institutional radiology report corpus annotated for clinically important recommendation information; Aim 2. Develop a novel NLP system to extract clinically important recommendations in radiology reports. The proposed research is innovative because it will generate a new text processing approach that can be used to flag reports visually and electronically so that separate workflow processes can be initiated to reduce the chance that necessary investigations or interventions suggested in the report are missed by clinicians. The proposed set of tools will be disseminated to the biomedical informatics community as open source tools. PUBLIC HEALTH RELEVANCE: Communication of recommendations for necessary investigations and interventions when abnormalities are identified on imaging studies is prone to error. When recommendations are not systematically identified and promptly communicated to referrers, poor patient outcomes can result. We propose to build natural language processing tools to automatically extract clinically important recommendation information from radiology reports.                ",Using NLP to Extract Clinically Important Recommendations from Radiology Reports,8804856,R21EB016872,"['Academic Medical Centers', 'Address', 'Adopted', 'Characteristics', 'Clinical', 'Communication', 'Communities', 'Computerized Medical Record', 'Data Set', 'Dependency', 'Diagnostic', 'Diagnostic radiologic examination', 'Ensure', 'Funding', 'Future', 'Goals', 'Gold', 'Growth', 'Guidelines', 'Hand', 'Health', 'Image', 'Imaging technology', 'Incidental Findings', 'Institution', 'Intervention', 'Investigation', 'Knowledge', 'Life', 'Lung nodule', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Measures', 'Medical center', 'Methods', 'Modality', 'Natural Language Processing', 'Outcome', 'Output', 'Patient Care', 'Patients', 'Persons', 'Process', 'Provider', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Reporting', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Risk', 'Safety', 'Semantics', 'Shapes', 'Societies', 'Specific qualifier value', 'Speech', 'System', 'Telephone', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Trauma', 'Ultrasonography', 'Unified Medical Language System', 'Washington', 'Writing', 'X-Ray Computed Tomography', 'biomedical informatics', 'cancer care', 'care delivery', 'design', 'falls', 'follow-up', 'health care delivery', 'imaging modality', 'improved', 'innovation', 'novel', 'open source', 'phrases', 'public health relevance', 'radiologist', 'screening', 'syntax', 'tool']",NIBIB,UNIVERSITY OF WASHINGTON,R21,2015,217500,0.04272576736944496
"Probabilistic Disease Surveillance DESCRIPTION (provided by applicant):         The proposed research will further develop and evaluate a probabilistic approach to disease surveillance. In this approach, a probabilistic case detection system (CDS) uses Bayesian diagnostic networks to compute the likelihoods of patient findings for each of a set of infectious diseases for every patient in a monitored population. CDS computes these likelihoods from data in electronic medical records, including information derived from free-text reports by natural language processing. CDS makes those estimates available to a probabilistic outbreak detection and characterization component (ODCS).             ODCS also utilizes a Bayesian approach to compute the probability that an outbreak is ongoing for each of a set of infectious diseases of interest, given information from CDS. ODCS also computes probability distributions over the current and future size of a detected outbreak and other characteristics such as incubation period used by public health officials when responding to an outbreak.                        The proposed research will extend the approach, which we have already developed and evaluated for the disease influenza to six additional respiratory infectious diseases. The research will also extend the capabilities of ODCS to utilize non-EMR data, detect an unknown disease, and detect and characterize concurrent outbreaks. The planned evaluations will measure the accuracy of both CDS and ODCS using historical surveillance data from two regions and simulated outbreak data, which we will create by adding outbreak cases generated by an agent-based epidemic simulator to real baseline surveillance data from non-outbreak periods.                        The innovation being advanced by this research is a novel, integrated, Bayesian approach for the early and accurate detection of cases of diseases that threaten health and for the detection and characterization of outbreaks of diseases that threaten public health. The proposed approach has significant potential to improve the information available to public health officials and physicians, which can be expected to improve clinical and public health decision making, and ultimately to improve population health. Project Relevance  The proposed research will improve the ability of public health officials and physicians to estimate the current incidence of influenza and other infectious diseases and to predict the future course of epidemics of those diseases. The improved information will better support decisions made by health departments to control epidemics, which is expected to reduce morbidity and mortality from epidemic diseases.",Probabilistic Disease Surveillance,8875053,R01LM011370,"['Accident and Emergency department', 'Advanced Development', 'Area', 'Bayesian Method', 'Characteristics', 'Clinical', 'Code', 'Communicable Diseases', 'Complex', 'Computer Systems', 'Computerized Medical Record', 'County', 'Data', 'Data Sources', 'Decision Making', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Outbreaks', 'Disease model', 'Epidemic', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Future', 'Health', 'Healthcare', 'Healthcare Systems', 'Incidence', 'Individual', 'Influenza', 'Intervention', 'Knowledge', 'Laboratories', 'Lung diseases', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Public Health', 'Public Health Practice', 'Publications', 'ROC Curve', 'Reporting', 'Research', 'Schools', 'Sensitivity and Specificity', 'Severities', 'Sodium Chloride', 'Structure', 'Support System', 'System', 'Systems Integration', 'Testing', 'Text', 'Time', 'Topaz', 'Universities', 'Utah', 'Vaccination', 'advanced system', 'base', 'computer code', 'diagnostic accuracy', 'disorder control', 'follow-up', 'improved', 'influenza outbreak', 'innovation', 'interest', 'knowledge base', 'mortality', 'novel', 'novel strategies', 'operation', 'pandemic disease', 'population health', 'portability', 'reproductive', 'respiratory', 'surveillance data']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2015,531035,0.015932838197633584
"Secondary use of EMRs for surgical complication surveillance     DESCRIPTION (provided by applicant):  Recent statistics indicate that worldwide almost 234 million major surgical procedures are performed each year with the rates of major postsurgical complications (PSCs) range from 3% to 16% and rates of permanent disability or death range from 0.4% to 0.8%. Early detection of PSCs is crucial since early intervention could be lifesaving. Meanwhile, with the rapid adoption of electronic medical records (EMRs) and the accelerated advance of health information technology (HIT), detection of PSCs by applying advanced analytics on EMRs makes it possible for near real-time PSC surveillance. We have developed a rule-based PSC surveillance system to detect most frequent colorectal PSCs near real-time from EMRs where a pattern-based natural language processing (NLP) engine is used to extract PSC related information from text and a set of expert rules is used to detect PSCs. Two challenges are identified. First, it is very challenging to integrate a diverse set of relevant data using expert rules. In the past, probabilistic approaches such as Bayesian Network which can integrate a diverse set of relevant data have become popular in clinical decision support and disease outbreak surveillance. Can we implement probabilistic approaches for PSC surveillance? Secondly, a large portion of the clinical information is embedded in text and it has been quite expensive to manually obtain the patterns used in the NLP system since it requires team effort of subject matter experts and NLP specialists. In the research field, statistical NLP has been quite popular. However, decision making in clinical practice demands tractable evidences while models for statistical NLP are not human interpretable. Can we incorporate statistical NLP to accelerate the NLP knowledge engineering process? We hypothesize that a probabilistic approach for PSC surveillance can be developed for improved case detection which can integrate multiple evidences from structured as well as unstructured EMR data. We also hypothesize that empirical NLP can accelerate the knowledge engineering process needed for building pattern- based NLP systems used in practice. Specific aims include: i) developing and evaluating an innovative Bayesian PSC surveillance system that incorporates evidences from both structured and unstructured EMR data; and ii) incorporating and evaluating statistical NLP in accelerating the NLP knowledge engineering process of pattern-based NLP for PSC surveillance. Given the significance of HIT, our study results will advance the science in developing practical NLP systems that can be translated to meet NLP needs in health care practice. Additionally, given the significance of PSCs, our study results will address significant patient safety and quality issues in surgical practice. Utilizing automated methods to detect postsurgical complications will enable early detection of complications compared to other methods and therefore have great potential of improving patient safety and health care quality while reducing cost. The results could lead to large scale PSC surveillance and quality improvement towards safer and better health care.         PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to unprecedented opportunities to use EMRs for clinical practice and research. We explore the use of EMRs for near real-time postsurgical complication surveillance with the aim of improving health care quality and reducing health care cost through enhanced analytics towards surgical excellence.                ",Secondary use of EMRs for surgical complication surveillance,8798027,R01EB019403,"['Abscess', 'Address', 'Adoption', 'Age', 'Anesthetics', 'Area', 'Bayesian Method', 'Cessation of life', 'Clinic', 'Clinical', 'Clinical Research', 'Colorectal', 'Complex', 'Complication', 'Computerized Medical Record', 'Data', 'Decision Making', 'Detection', 'Development', 'Disease Outbreaks', 'Early Diagnosis', 'Early Intervention', 'Educational workshop', 'Engineering', 'Goals', 'Health Care Costs', 'Healthcare', 'Hemorrhage', 'Human', 'Ileus', 'Knowledge', 'Lead', 'Manuals', 'Methods', 'Minor', 'Motivation', 'Natural Language Processing', 'Nature', 'Nutritional', 'Operative Surgical Procedures', 'Output', 'Patients', 'Pattern', 'Perioperative', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Registries', 'Reporting', 'Research', 'Risk Factors', 'Science', 'Severities', 'Specialist', 'Statistical Models', 'Structure', 'Surgeon', 'Surgical complication', 'System', 'Testing', 'Text', 'Time', 'Translating', 'Uncertainty', 'Work', 'Wound Infection', 'abstracting', 'base', 'clinical practice', 'computer based statistical methods', 'cost', 'disability', 'health care quality', 'health information technology', 'improved', 'innovation', 'meetings', 'patient safety', 'public health relevance', 'rapid growth', 'statistics']",NIBIB,MAYO CLINIC ROCHESTER,R01,2015,299888,0.06872473409625994
"Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data Adverse drug reactions (ADRs) are a major burden for patients and healthcare, causing preventable  hospitalizations and deaths, and incurring a huge cost. The long-term objective of this proposal is to advance  patient safety and reduce costs by discovering novel serious ADRs through use of automated methods that  combine information from large and varied patient populations as well as from the literature. There have been  considerable advances in pharmacovigilance, but more work is needed. For example, Vioxx, a commonly used  drug, was recently found to cause at least 88,000 occurrences of myocardial infarction, highlighting the  insufficiency of current methods. To date, methods have mainly depended on the use of single sources of data,  primarily from the Federal Food and Drug Administration Adverse Event Reporting System (FAERS) and from  electronic health records (EHRS). Although important, each of the sources has different limitations and  advantages, and therefore, combining the data across them should lead to more effective drug safety  surveillance by increasing the statistical power, and also by allowing each data source to complement the other  sources. We already have developed methods associated with each of the single sources, and therefore, this  is an excellent opportunity to build upon our research accomplishments to advance the state of the art in  pharmacovigilance.   More specifically, we will a) acquire and combine comprehensive clinical data from the electronic health  records (EHRs) of two different health care sites serving diverse populations by utilizing natural language  processing (NLP) to obtain vast quantities of fine-grained data, and then by developing data mining  methodologies on the clinical data to detect novel ADR signals, b) analyze differences in therapy-related risk  factors between the two EHR populations, such as racial and ethnic differences, c) detect ADR signals in the  FAERS database using an established methodology, d) develop improved methods to acquire ADR signals  based on information in the literature, and e) develop methods that utilize the results from the above sources to  maximize effectiveness. We will focus on eight serious ADRs, and collect a high-quality reference standard for  those ADRs so that we will be able to evaluate and compare performance of the different detection methods  individually as well as the methods that combine the sources.   This proposal is well positioned to overcome problems associated with existing automated methods, which  are primarily based on use of individual sources of data. We are confident the methods will be effective  because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in  this proposal presents an excellent chance to leverage heterogeneous data sources to dramatically improve  patient safety and reduce costs. Adverse drug reactions (ADRs) are a major burden for patients and health care, causing preventable  hospitalizations and deaths, and incurring huge costs, and, therefore, continuous post-marketing surveillance  is crucial for patient safety. This proposal aims to improve patient safety and reduce health care costs by  developing effective methods to discover new adverse drug reactions through the combination of information in  the FDA's Adverse Event Reporting System, the literature, and comprehensive clinical data from electronic  health records of two different sites with diverse populations, thereby overcoming limitations that rely mainly on  use of one data source.",Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data,8882546,R01LM010016,"['Academic Medical Centers', 'Address', 'Adverse event', 'Adverse reactions', 'Cereals', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Data', 'Complement', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Drug usage', 'Effectiveness', 'Electronic Health Record', 'Evaluation', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Hospitals', 'Individual', 'Knowledge', 'Lead', 'Literature', 'Logistic Regressions', 'Medical Care Costs', 'Methodology', 'Methods', 'Modeling', 'Myocardial Infarction', 'Natural Language Processing', 'New York', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Population Heterogeneity', 'Positioning Attribute', 'Presbyterian Church', 'Probability', 'Process', 'PubMed', 'Publications', 'Reaction', 'Reference Standards', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Factors', 'Rofecoxib', 'Safety', 'Signal Transduction', 'Site', 'Source', 'Structure', 'System', 'Techniques', 'United States Food and Drug Administration', 'Work', 'base', 'chemical property', 'conditioning', 'cost', 'data mining', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'prevent', 'racial/ethnic difference', 'research and development', 'text searching']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2015,417794,0.011448944552632533
"Encoding and Processing Patient Allergy Information in EHRs DESCRIPTION (provided by applicant): Allergies affect one in five Americans and are the 5th leading chronic disease in the U.S. Each year, allergies account for more than 17 million outpatient office visits. Although documenting and exchanging allergy information in electronic health records (EHRs) is becoming increasingly important, we still face multiple challenges. These include: lack of well-adopted standard terminologies for representing allergies, frequent entry of allergy information as free-text, and no existing process for reconciling allergy information. In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) conduct analyses on standard terminologies and a large allergy repository to build a comprehensive knowledge base for representing allergy information; 2) design, develop and evaluate a natural language processing (NLP) module for extracting and encoding free-text allergy information and integrate it with an existing NLP system; 3) measure the feasibility and efficiency of the proposed NLP system for the new process of allergy reconciliation; and 4) distribute our methods and tool, so they are widely available to other researchers and healthcare institutions for non-commercial use. PUBLIC HEALTH RELEVANCE: Managing allergy information within the electronic health record (EHR) is vital to ensuring patient safety. The goal of this study is to propose a comprehensive solution to assess existing terminology standards and knowledge bases for representing allergy information, develop and evaluate a natural language processing (NLP) system for extracting and encoding allergy information from free-text clinical documents, and finally measure the feasibility of using NLP output to facilitate the allergy reconciliation proces.",Encoding and Processing Patient Allergy Information in EHRs,8920540,R01HS022728,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2015,488893,0.0852415444145766
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005).         PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.            ",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9004939,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2015,250000,0.02180469763923141
"Improving Accuracy of Electronic Notes Using A Faster, Simpler Approach DESCRIPTION (provided by applicant): Physician progress notes contain information essential to patient care, including findings from history and physical exam, interpretation of tests, assessment and treatment plans. However in the transition from paper to electronic physician notes, many physicians spend more time creating them, which has led to the use of time-saving measures such as copy/paste and templates that have degraded note accuracy and quality. This threatens the usefulness of notes not only for their most important use-patient care-but also for research, quality improvement, and in supporting reimbursement. To address these problems, we propose a project with the following specific aims: 1. To refine and implement a new voice-generated enhanced electronic note system (VGEENS) integrating voice recognition with natural language processing and links to the electronic medical record (EMR) to improve note accuracy and timeliness. 2. To evaluate VGEENS using a randomized trial with 30 internal medicine physicians in each arm to assess electronic note accuracy, quality, timeliness, and user satisfaction. Intervention physicians will use VGEENS, while the control physicians will continue with note creation as they normally would. This novel approach has the potential to improve note accuracy while reducing delays in making progress notes in EMRs available to other clinicians. It leverages rapidly improving voice recognition and NLP technologies to permit physicians to use a natural, fast method-human voice-to convey their observation and thoughts into the EMR record. PUBLIC HEALTH RELEVANCE: Physician documentation of a patient visit contains information that is used in that patient's care. This information includes findings from a patient' history and physical exam, interpretation of necessary tests, the problem assessment and treatment plan. However, in the transition from paper to electronic physician notes, many physicians are spending more time creating these notes. This has led to use of time-saving measures that have degraded the accuracy and ease of use of patient notes. By the end of this project, we expect to have developed, used and evaluated a new method for creating electronic physician notes that both improve accuracy and timely availability of inpatient progress notes.","Improving Accuracy of Electronic Notes Using A Faster, Simpler Approach",8928601,R21HS023631,[' '],AHRQ,UNIVERSITY OF WASHINGTON,R21,2015,139245,-0.025353862929716607
"NLP-enabled decision support for cervical cancer screening and surveillance DESCRIPTION (provided by applicant): Although cervical cancer is preventable, it still continues to be a leading cause of death. Following the evidence- based guidelines for cervical cancer prevention is challenging for healthcare providers, due to which many patients do not receive the optimal preventive care. Clinical decision support (CDS) systems can potentially improve the care delivery. However, the current CDS systems only identify patients overdue for screening, and do not suggest the optimal screening interval. Moreover they do not help with surveillance of patients with abnormal screening results. This is because the existing systems lack the capability to process free- text clinical reports that contain information needed for applying the guidelines. Hence there is a critical need for natural language processing (NLP)-enabled CDS systems that can utilize discrete as well as free-text patient information for enhancing the decision support. Our long-term goal is to improve healthcare delivery of cervical cancer prevention with guideline based reminders. The central hypothesis is that NLP- enabled CDS system will significantly improve the quality of care delivery for cervical cancer prevention. The rationale is that use of NLP will improve granularity of the guideline implementation, which will in-turn enhance care delivery. As preliminary work we have developed an NLP-enabled CDS system that automatically interprets the patient information from the electronic health record and applies the national guidelines to compute the optimal recommendation for screening and surveillance. We have performed validation of the system in a non-clinical setting.1 In this application we will proceed towards deployment of the system in the clinical setting, and will carry out studies for measuring the impact on the quality of care delivery. In ai one, we will validate the system in the clinical setting and will optimize its usability and workflw integration. In aim two, we will test the hypothesis that reminders from the NLP-enabled CDS system to primary care providers will improve the quality of care delivery, by performing a one year intervention control study across four sites of a primary care practice. In aim three, we will test the hypothesis that reminders to non-adherent high-risk patients will improve their surveillance rates, by performing a randomized intervention study for three months. In this study, care coordinators will utilize the CDS system for sending reminders to patients that are non-adherent and at high risk due to abnormal screenings. The main contribution of this project will be knowledge about the effectiveness of NLP in enhancing the impact of CDS systems for cervical cancer prevention, and for clinical practice in general. This project is innovative because the CDS system will utilize NLP to generate screening reminders for normal patients and surveillance reminders for patients with abnormal findings. This is a major advancement over existing systems that can only identify patients for screening. PUBLIC HEALTH RELEVANCE: The proposed research is relevant to public health because it will yield knowledge about the effectiveness of natural language processing (NLP) to enhance impact of CDS systems for cervical cancer prevention, and for clinical practice in general. This research will foster implementation of similar CDS systems across the nation for cervical cancer prevention and for other decision problems, which will improve the quality of healthcare delivery. Thus, the proposed research is relevant to AHRQ's mission to improve the quality, safety, efficiency, and effectiveness of health care for all Americans.",NLP-enabled decision support for cervical cancer screening and surveillance,8934087,R21HS022911,[' '],AHRQ,MAYO CLINIC ROCHESTER,R21,2015,145229,0.04038837038858005
"Interactive machine learning methods for clinical natural language processing     DESCRIPTION (provided by applicant): Growing deployments of electronic health records (EHRs) systems have made massive clinical data available electronically. However, much of detailed clinical information of patients is embedded in narrative text and is not directly accessible for computerized clinical applications. Therefore, natural language processing (NLP) technologies, which can unlock information in narrative document, have received great attention in the medical domain. Current state-of-the-art NLP approaches often involve building probabilistic models. However, the wide adoption of statistical methods in clinical NLP faces two grand challenges: 1) the lack of large annotated clinical corpora; and 2) the lack of methodologies that can efficiently integrate linguistic and domain knowledge with statistical learning. High-performance statistical NLP methods rely on large scale and high quality annotations of clinical text, but it is time-consuming and costly to create large annotated clinica corpora as it often requires manual review by physicians. Moreover, the medical domain is knowledge intensive. To achieve optimal performance, probabilistic models need to leverage medical domain knowledge. Therefore, methods that can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost would be highly desirable for clinical text processing.    In this study, we propose to investigate interactive machine learning (IML) methods to address the above challenges in clinical NLP. An IML system builds a classification model in an iterative process, which can actively select informative samples for annotation based on models built on previously annotated samples, thus reducing the annotation cost for model development. More importantly, an IML system also involves human inputs to the learning process (e.g., an expert can specify important features for a classification task based on domain knowledge). Thus, IML is an ideal framework for efficiently integrating rule-based (via domain experts specifying features) and statistics-based (via different learning algorithms) approaches to clinical NLP. To achieve our goal, we propose three specific aims. In Aim 1, we plan to investigate different aspects of IML for word sense disambiguation, including developing new active learning algorithms and conducting cognitive usability analysis for efficient feature annotation by users. To demonstrate the broad uses of IML, we further extend IML approaches to two other important clinical NLP classification tasks: named entity recognition and clinical phenoytping in Aim 2. Finally we propose to disseminate the IML methods and tools to the biomedical research community in Aim 3.             Project Narrative In this project, we propose to develop interactive machine learning methods to process clinical text stored in electronic health records (EHRs) systems. Such methods can efficiently integrate domain and expert knowledge with machine learning processes to quickly build high-quality probabilistic models with minimum annotation cost, thus improving performance of text processors. This technology will allow more accurate data extraction from clinical documents, thus to facilitate clinical research that rely on EHRs data.",Interactive machine learning methods for clinical natural language processing,8818096,R01LM010681,"['Abbreviations', 'Active Learning', 'Address', 'Adoption', 'Algorithms', 'Attention', 'Biomedical Research', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Cognitive', 'Communities', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electronic Health Record', 'Face', 'Goals', 'Grant', 'Human', 'Hybrids', 'Knowledge', 'Label', 'Learning', 'Linguistics', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Patients', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Research', 'Research Personnel', 'Research Priority', 'Resources', 'Sampling', 'Solutions', 'Source', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'System', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'base', 'clinical application', 'clinical phenotype', 'cohort', 'computer human interaction', 'computerized', 'cost', 'experience', 'improved', 'model development', 'novel', 'open source', 'statistics', 'success', 'tool', 'usability']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2014,558372,0.04384407401116556
"Challenges in Natural Language Processing for Clinical Narratives     DESCRIPTION (provided by applicant): Narratives of electronic health records (EHRs) contain useful information that is difficult to automatically extract, index, search, or interpret. Clinical natural language processing (NLP) technologies for automatic extraction, indexing, searching, and interpretation of EHRs are in development; however, due to privacy concerns related to EHRs, such technologies are usually developed by teams that have privileged access to EHRs in a specific institution. Technologies that are tailored to a specific set of data from a given institution generate inspiring results on that data; however, they can fail to generalize to similar data from other institutions and even other departments from the same institution. Therefore, learning from these technologies and building on them becomes difficult.          In order to improve NLP in EHRs, there is need for head-to-head comparison of approaches that can address a given task on the same data set. Shared-tasks provide one way of conducting systematic head-to- head comparisons. This proposal describes a series of shared-task challenges and conferences, spread over a five year period, that promote the development and evaluation of cutting edge clinical NLP systems by distributing de-identified EHRs to the broad research community, under data use agreements, so that:      *	the state-of-the-art in clinical NLP technologies can be identified and advanced,      *	a set of technologies that enable the use of the information contained in EHR narratives becomes available, and      *	the information from EHR narratives can be made more accessible, for example, for clinical and medical research.          The scientific activities supporting the organization of the shared-task challenges are sponsored in part by Informatics for Integrating Biology and the Bedside (i2b2), grant number U54-LM008748, PI: Kohane.          This proposal aims to organize a series of workshops, conference proceedings, and journal special issues that will accompany the shared-task challenges in order to disseminate the knowledge generated by the challenges.                 Public health relevance: this proposal will address two main challenges related to the use of clinical narratives for research: availability of clinical records for research and identification of the state of the art in clinical natural language processing (NLP) technologies so that we can push the state of the art forward and so that future work can build on the past. Progress in clinical NLP will improve access to electronic health records for research, and for clinical applications, benefiting healthcare and public health.",Challenges in Natural Language Processing for Clinical Narratives,8722031,R13LM011411,"['Access to Information', 'Address', 'Agreement', 'Biology', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Set', 'Development', 'Distributed Systems', 'Educational workshop', 'Electronic Health Record', 'Evaluation', 'Future', 'Goals', 'Gold', 'Grant', 'Hand', 'Healthcare', 'Improve Access', 'Informatics', 'Institution', 'Journals', 'Knowledge', 'Learning', 'Medical Research', 'Natural Language Processing', 'Privacy', 'Public Health', 'Publications', 'Records', 'Research', 'Rest', 'Series', 'System', 'Systems Development', 'Targeted Research', 'Technology', 'Work', 'clinical application', 'data sharing', 'head-to-head comparison', 'improved', 'indexing', 'practical application', 'public health relevance', 'symposium']",NLM,STATE UNIVERSITY OF NEW YORK AT ALBANY,R13,2014,19998,0.06801985663564217
"Semi-structured Information Retrieval in Clinical Text for Cohort Identification     DESCRIPTION (provided by applicant):  Natural Language Processing (NLP) techniques have shown promise for extracting data from the free text of electronic health records (EHRs), but studies have consistently found that techniques do not readily generalize across application settings. Unfortunately, most of the focus in applying NLP to real use cases has remained on a paradigm of single, well-defined application settings, so that generalizability to unseen use cases remains implicitly unaddressed. We propose to explicitly account for unseen application settings by adopting an information retrieval (IR) perspective with the objective of patient-level cohort identification. To do so, we introduce layered language models, an IR framework that enables the reuse of NLP-produced artifacts. Our long term goal is to accelerate investigations of patient health and disease by providing robust, user- centric tools that are necessary to process, retrieve, and utilize the free text of EHRs. The main goal of this proposal is to accurately retrieve ad hoc, realistic cohorts from clinical text at Mayo Clinic and OHSU, establishing methods, resources, and evaluation for patient-level IR. We hypothesize that cohort identification can be addressed in a generalizable fashion by a new IR framework: layered language models. We will test this hypothesis through four specific aims. In Aim 1, we will make medical NLP artifacts searchable in our layered language IR framework. This involves storing and indexing the NLP artifacts, as well as using statistical language models to retrieve documents based on text and its associated NLP artifacts. In Aim 2, we deal with the practical setting of ad hoc cohort identification, moving to patient-level (rather than document-level) IR. To accurately handle patient cohorts in which qualifying evidence may be spread over multiple documents, we will develop and implement patient-level retrieval models that account for cross- document relational and temporal combinations of events. In Aim 3, we will construct parallel IR test collections using EHR data from two sites; a diverse set of cohort queries written by multiple people toward various clinical or epidemiological ends; and assessments of which patients are relevant to which queries at both sites. Finally, in Aim 4, we refine and evaluate patient-level layered language IR on the ad hoc cohort identification task, making comparisons across the users, queries, optimization metrics, and institutions. We will draw additional extrinsic comparisons with pre-existing techniques, e.g., for cohorts from the Electronic Medical Records and Genonmics network. The expected outcomes of the proposed work are: (i) An open-source cohort identification tool, usable by clinicians and epidemiologists, that makes principled use of NLP artifacts for unseen queries; ii) A parallel test collection for cohort identification, includig two intra-institutional document collections, diverse test topics and user-produced text queries, and patient-level judgments of relevance to each query; and (iii) Validation of the reusability of medical NLP via the task of retrieving patient cohorts.         PUBLIC HEALTH RELEVANCE:  With the widespread adoption of electronic medical records, one might expect that it would be simple for a medical expert to find things like ""patients in the community who suffer from asthma."" Unfortunately, on top of lab tests, medications, and demographic information, there are observations that a physician writes down as text - which are difficult for a computer to understand. Therefore, we aim to process text so that a computer can understand enough of it, and then search that text along with the rest of a patient's medical record; this will allow clinicians or researchers to find and study patients groups of interest.                ",Semi-structured Information Retrieval in Clinical Text for Cohort Identification,8811565,R01LM011934,"['Accounting', 'Address', 'Adopted', 'Adoption', 'Asthma', 'Clinic', 'Clinical', 'Collection', 'Communities', 'Computerized Medical Record', 'Computers', 'Data', 'Dictionary', 'Disease', 'Electronic Health Record', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Goals', 'Health', 'Information Retrieval', 'Information Retrieval Systems', 'Institution', 'Interest Group', 'Investigation', 'Judgment', 'Language', 'Learning', 'Machine Learning', 'Measures', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Modification', 'Morphologic artifacts', 'Names', 'Natural Language Processing', 'Outcome', 'Patient Recruitments', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Physicians', 'Process', 'Publishing', 'Qualifying', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Retrieval', 'Sampling', 'Semantics', 'Site', 'Smoke', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'Weight', 'Work', 'Writing', 'asthmatic patient', 'base', 'cohort', 'improved', 'indexing', 'novel', 'open source', 'public health relevance', 'syntax', 'text searching', 'tool']",NLM,MAYO CLINIC ROCHESTER,R01,2014,460688,0.0519060278373671
"EMR Adverse Drug Event Detection for Pharmacovigilance     DESCRIPTION (provided by applicant): Adverse drug events (ADEs) result in substantial patient morbidity and lead to over 100,000 deaths yearly. The timely identification of previously unknown toxicities of cancer drugs is an important, unsolved problem. In the United States, 20% of the 548 drugs introduced into the market between 1975 and 1999 were either withdrawn or acquired a new ""black box"" warning during the 25-year period following initial approval by the Food and Drug Administration. Adverse drug events are an important cause of morbidity and mortality in patients, yet 95% of ADEs are unreported, leading to delays in the detection of previously unknown ADEs and underestimation of the risk to known ADEs. It is known that Electronic Medical Record (EMR), discharge summaries, and lab results contain ADE information and biomedical natural language processing (BioNLP) provides automated tools that facilitate chart review and thus improve patient surveillance and post-marketing pharmacovigilance. The objectives for this proposal are to develop ""intelligent"" BioNLP approaches to extract disease, medication, and structured ADE information from EMRs, and then evaluate extracted ADEs for detecting known ADE types as well as clinically unrecognized or novel ADEs whose pattern or effect have not been previously identified.         PUBLIC HEALTH RELEVANCE: EMR Adverse Drug Event Detection This project proposes innovative intelligent biomedical natural language process approaches to automatically extract adverse drug event from the Electronic Medical Record. It is anticipated that the data resources, algorithms, and the Pharmacovigilance Toolkit developed will significantly enhance ADE detection, patient surveillance and post marketing pharmacovigilance.            ",EMR Adverse Drug Event Detection for Pharmacovigilance,8772667,U01CA180975,"['Adverse event', 'Algorithms', 'Antineoplastic Agents', 'Biology', 'Boxing', 'Cancer Center', 'Cancer Research Network', 'Cessation of life', 'Clinical', 'Clinical Oncology', 'Clinical Trials', 'Collaborations', 'Common Terminology Criteria for Adverse Events', 'Comprehensive Cancer Center', 'Computerized Medical Record', 'Data', 'Detection', 'Discipline of Nursing', 'Disease', 'Drug toxicity', 'Elements', 'Event', 'Future', 'Goals', 'Health Promotion', 'Hematology', 'Hospitals', 'Informatics', 'Injury', 'Inpatients', 'Intervention', 'Knowledge', 'Language', 'Lead', 'Left', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Marketing', 'Massachusetts', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Outpatients', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Prevention', 'Public Health', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Safety', 'Severities', 'Signal Transduction', 'Structure', 'System', 'Terminology', 'Testing', 'Therapeutic', 'Toxic effect', 'United States', 'United States Food and Drug Administration', 'Universities', 'Weight', 'Work', 'abstracting', 'disorder prevention', 'firewall', 'improved', 'innovation', 'insight', 'lenalidomide', 'medical schools', 'mortality', 'novel', 'oncology', 'open source', 'patient safety', 'post-market', 'public health relevance', 'response', 'tool']",NCI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U01,2014,374997,-0.02176217462767972
"Measuring and Improving Colonoscopy Quality Using Natural Language Processing     DESCRIPTION (provided by applicant):  Colonoscopy is the predominant method for screening for colorectal cancer in the US. Yet, its effectiveness in screening is limited by variation in performance. For example, the rate at which physicians detect cancer precursors called adenomas during a colonoscopy has been shown to vary three-fold from one physician to another. A patient whose colonoscopy is performed by a physician with a low adenoma detection rate has a higher risk of subsequent colorectal cancer.  Our proposal centers on measuring, understanding, and improving colonoscopy quality. The major innovation of this work is to use natural language processing (NLP) to measure the quality of colonoscopy. NLP is a field of computer science in which a computer is trained to ""read"" text to identify relevant data We developed and validated the first NLP-based computer software application (C-QUAL) that analyzes colonoscopy and associated pathology reports. Our primary quality measure is adenoma detection rate because it is a common, validated measure that is linked to colorectal cancer incidence. However, we also use a number of secondary quality measures. We applied C-QUAL to almost 25,000 colonoscopy reports in one health system and found large variation in physician's performance on the quality measures. Building on this prior work, our goal is to use C-QUAL to measure colonoscopy quality across a spectrum of US practice environments, to understand what drives variation in colonoscopy quality, and to improve colonoscopy quality. In Aim 1, we propose to use the C-QUAL tool to measure performance in 4 diverse health care systems. This will be one of the largest assessments of the variation in adenoma detection rates and will span different geographic regions, payment systems, and practice settings. In Aim 2, we seek to understand why there is variation in quality. We will survey providers at the 4 health care systems about factors that might affect quality. We will link those survey results to the adenoma detection rates assessed in Aim 1 and look for key associations. It is assumed, but not proven, that providing feedback to physicians on colonoscopy quality will improve care. In Aim 3, we assess whether feedback to physicians does drive quality improvement and, building on Aim 2, explore which types of physicians may respond to feedback. Our proposal is the first to use this innovative method to measure colonoscopy quality and to use the quality scores to decrease the variation in colonoscopy performance. Together the results of the 3 aims are consistent with the NCI's focus on improving the quality of colorectal cancer screening.           PUBLIC HEALTH RELEVANCE: Prior research has shown that the quality of colonoscopy varies from physician to physician. It is important to address low quality because patients who see physicians with lower quality colonoscopies have a higher risk of developing colorectal cancer in the future. To help address poor quality of colonoscopy, we propose to use a computer software program to measure colonoscopy quality, survey physicians to understand why there is variation in quality, and provide physician feedback to stimulate quality improvement.                  ",Measuring and Improving Colonoscopy Quality Using Natural Language Processing,8641674,R01CA168959,"['Address', 'Affect', 'Cancer Etiology', 'Caring', 'Cecum', 'Cessation of life', 'Colon', 'Colonoscopy', 'Colorectal', 'Colorectal Cancer', 'Computer software', 'Computers', 'Data', 'Detection', 'Effectiveness', 'Environment', 'Feedback', 'Future', 'Gastroenterology', 'Geographic Locations', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidence', 'Knowledge', 'Link', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Metric', 'Monitor', 'Natural Language Processing', 'Pathology Report', 'Patients', 'Performance', 'Physicians', 'Provider', 'Reading', 'Recommendation', 'Records', 'Reporting', 'Research', 'Risk', 'Science', 'Site', 'Societies', 'Sum', 'Surveys', 'System', 'Testing', 'Text', 'Training', 'United States National Institutes of Health', 'Variant', 'Work', 'adenoma', 'base', 'colorectal cancer screening', 'computer science', 'cost effective', 'high risk', 'improved', 'innovation', 'medical specialties', 'novel', 'payment', 'public health relevance', 'screening', 'symposium', 'tool', 'trend']",NCI,HARVARD MEDICAL SCHOOL,R01,2014,547410,0.014249434711725763
"Natural language processing for clinical and translational research DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. This growth is being fueled by recent federal legislation that provides generous financial incentives to institutions demonstrating aggressive application and ""meaningful use"" of comprehensive EMRs. Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large scale studies of disease onset and treatment outcome, specifically within the context of routine clinical care. However, a well-known challenge for secondary use of EMR data for clinical and translational research is that much of detailed patient information is embedded in narrative text. Natural Language Processing (NLP) technologies, which are able to convert unstructured clinical text into coded data, have been introduced into the biomedical domain and have demonstrated promising results. Researchers have used NLP systems to identify clinical syndromes and common biomedical concepts from radiology reports, discharge summaries, problem lists, nursing documentation, and medical education documents. Different NLP systems have been developed at different institutions and utilized to convert clinical narrative text into structured data that may be used for other clinical applications and studies. Successful stories in applying NLP to clinical and translational research have been reported widely. However, institutions often deploy different NLP systems, which produce various types of output formats and make it difficult to exchange information between sites. Therefore, the lack of interoperability among different clinical NLP systems becomes a bottleneck for efficient multi-site studies. In addition, many successful studies often require a strong interdisciplinary team where informaticians and clinicians have to work very closely to iteratively define optimal algorithms for clinical phenotypes. As intensive informatics support may not be available to every clinical researcher, the usability of NLP systems for end users is another important issue. The proposed project builds upon first-hand knowledge and experience across the research team in the use of NLP for clinical and translational research projects. There are several big informatics initiatives for clinical and translational research but those initiatives generally assume one shoe fits all and follow top-down approaches to develop NLP solutions. Complementary to those initiatives, we will use a bottom-up approach to handle interoperability and usability: i) we will obtain a common NLP data model and exchange format through empirical analysis of existing NLP systems and NLP results; ii) we will develop a user-centric NLP front end interface for NLP systems wrapped to be consistent with the proposed NLP data model and exchange format incorporating usability analysis into the agile development process. All deliverables will be distributed through the open health NLP (OHNLP) consortium which we intend to make it more open and inclusive. PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. We propose the development of a novel framework to enable the use of clinical information embedded in clinical narratives for clinical and translational research.",Natural language processing for clinical and translational research,8920720,R01GM102282,"['Acceleration', 'Adopted', 'Adoption', 'Adverse drug effect', 'Algorithms', 'Architecture', 'Attention', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'DNA Databases', 'Data', 'Data Set', 'Development', 'Dictionary', 'Discipline of Nursing', 'Disease Association', 'Documentation', 'Elements', 'Exclusion Criteria', 'Genes', 'Genomics', 'Goals', 'Growth', 'Health', 'Informatics', 'Institution', 'Knowledge', 'Link', 'Logical Observation Identifiers Names and Codes', 'Manuals', 'Medical Education', 'Modeling', 'Natural Language Processing', 'Onset of illness', 'Output', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Play', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Role', 'SNOMED Clinical Terms', 'Semantics', 'Shoes', 'Site', 'Solutions', 'Statutes and Laws', 'Structure', 'Syndrome', 'System', 'Technology', 'Text', 'Translational Research', 'Treatment outcome', 'Work', 'base', 'clinical application', 'clinical care', 'clinical phenotype', 'computer human interaction', 'data exchange', 'data modeling', 'experience', 'financial incentive', 'flexibility', 'human centered computing', 'interoperability', 'novel', 'open source', 'patient safety', 'rapid growth', 'success', 'tool', 'usability', 'user-friendly']",NIGMS,MAYO CLINIC ROCHESTER,R01,2014,160000,0.0822176457303292
"Natural language processing for clinical and translational research     DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. This growth is being fueled by recent federal legislation that provides generous financial incentives to institutions demonstrating aggressive application and ""meaningful use"" of comprehensive EMRs. Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large scale studies of disease onset and treatment outcome, specifically within the context of routine clinical care. However, a well-known challenge for secondary use of EMR data for clinical and translational research is that much of detailed patient information is embedded in narrative text. Natural Language Processing (NLP) technologies, which are able to convert unstructured clinical text into coded data, have been introduced into the biomedical domain and have demonstrated promising results. Researchers have used NLP systems to identify clinical syndromes and common biomedical concepts from radiology reports, discharge summaries, problem lists, nursing documentation, and medical education documents. Different NLP systems have been developed at different institutions and utilized to convert clinical narrative text into structured data that may be used for other clinical applications and studies. Successful stories in applying NLP to clinical and translational research have been reported widely. However, institutions often deploy different NLP systems, which produce various types of output formats and make it difficult to exchange information between sites. Therefore, the lack of interoperability among different clinical NLP systems becomes a bottleneck for efficient multi-site studies. In addition, many successful studies often require a strong interdisciplinary team where informaticians and clinicians have to work very closely to iteratively define optimal algorithms for clinical phenotypes. As intensive informatics support may not be available to every clinical researcher, the usability of NLP systems for end users is another important issue. The proposed project builds upon first-hand knowledge and experience across the research team in the use of NLP for clinical and translational research projects. There are several big informatics initiatives for clinical and translational research but those initiatives generally assume one shoe fits all and follow top-down approaches to develop NLP solutions. Complementary to those initiatives, we will use a bottom-up approach to handle interoperability and usability: i) we will obtain a common NLP data model and exchange format through empirical analysis of existing NLP systems and NLP results; ii) we will develop a user-centric NLP front end interface for NLP systems wrapped to be consistent with the proposed NLP data model and exchange format incorporating usability analysis into the agile development process. All deliverables will be distributed through the open health NLP (OHNLP) consortium which we intend to make it more open and inclusive.         PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. We propose the development of a novel framework to enable the use of clinical information embedded in clinical narratives for clinical and translational research.                ",Natural language processing for clinical and translational research,8640959,R01GM102282,"['Acceleration', 'Adopted', 'Adoption', 'Adverse drug effect', 'Algorithms', 'Architecture', 'Attention', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'DNA Databases', 'Data', 'Data Set', 'Development', 'Dictionary', 'Discipline of Nursing', 'Disease Association', 'Documentation', 'Elements', 'Exclusion Criteria', 'Genes', 'Genomics', 'Goals', 'Growth', 'Health', 'Informatics', 'Institution', 'Knowledge', 'Link', 'Logical Observation Identifiers Names and Codes', 'Manuals', 'Medical Education', 'Modeling', 'Natural Language Processing', 'Onset of illness', 'Output', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Play', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Role', 'SNOMED Clinical Terms', 'Semantics', 'Shoes', 'Site', 'Solutions', 'Statutes and Laws', 'Structure', 'Syndrome', 'System', 'Technology', 'Text', 'Translational Research', 'Treatment outcome', 'Work', 'base', 'clinical application', 'clinical care', 'clinical phenotype', 'computer human interaction', 'data exchange', 'data modeling', 'experience', 'financial incentive', 'flexibility', 'human centered computing', 'interoperability', 'novel', 'open source', 'patient safety', 'public health relevance', 'rapid growth', 'success', 'tool', 'usability', 'user-friendly']",NIGMS,MAYO CLINIC ROCHESTER,R01,2014,580082,0.0822176457303292
"Utilizing social media as a resource for mental health surveillance DESCRIPTION (provided by applicant):  Major depressive disorder is one of the most common debilitating illnesses in the United States, with a lifetime prevalence of 16.2%. Currently, nationwide mental health surveillance takes the form of large-scale telephone- based surveys. These surveys have high running costs and require teams of human telephone operators. Even the largest system, the Behavioral Risk Factor Surveillance System, reaches only 0.13% of the US population. Twitter (and other microblog services) offers a rich, if terse, multilingual source of real time data for public health surveillance. Natural Language Processing (NLP) provides techniques and resources to ""unlock"" data from text. We propose using Twitter and NLP as a cost-effective and flexible approach to augmenting current telephone- based surveillance methods for population level depression monitoring.         This grant application has two major strands. First, investigating ethical issues and challenges to privacy that emerge with the use of Twitter data for public health surveillance (Aim One). Second, developing techniques and resources for real-time public health surveillance for mental illness from Twitter (Aim Two &Aim Three). Aim One seeks to investigate and codify our responsibilities as researchers towards Twitter users by engaging with those users directly. With Aim Two, we will build and evaluate Natural Language Processing resources - algorithms, lexicons and taxonomies - to support the identification of depression symptoms in Twitter data. For Aim Three, we will build and evaluate Natural Language Processing modules and services that use Twitter as a data source for monitoring depression levels in the community. The significance of the proposed work lies in three areas. First, our investigations - both empirical and theoretical - will explore ethical issues in the use of Twitter for public health surveillance. This work has the potential to guide future research in the area. Second, in developing and evaluating algorithms and resources for identifying depression from tweets, we are contributing foundational work to the field of NLP. Third, developing these algorithms and resources will provide the bedrock for building social media based surveillance systems which will provide a cost effective means of augmenting current mental health surveillance practice. This proposal is innovative in both its application area (microblogs have not been used before for mental health surveillance), its focus on using NLP to identify depressive symptoms for public health, and in the central role that qualitative bioethical research will play in guiding the work. Project Narrative The proposed research focuses on using advanced Natural Language Processing methods to mine microblog data - in this case, Twitter - for mental health surveillance (specifically, depression surveillance), in order to augment current telephone-based mental health surveillance systems. The research has public health at its core.",Utilizing social media as a resource for mental health surveillance,8894203,R00LM011393,"['Algorithms', 'Applications Grants', 'Area', 'Attitude', 'Behavioral Risk Factor Surveillance System', 'Broadcast Media', 'Cities', 'Cognitive', 'Communities', 'County', 'Data', 'Data Sources', 'Dental', 'Development', 'Disasters', 'Earthquakes', 'Electronic Health Record', 'Epidemiology', 'Ethical Issues', 'Ethics', 'Exercise', 'Guidelines', 'Health', 'Human', 'Influenza A Virus, H1N1 Subtype', 'Interview', 'Investigation', 'Linguistics', 'Location', 'Major Depressive Disorder', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Methods', 'Mining', 'Monitor', 'Natural Language Processing', 'Participant', 'Phase', 'Play', 'Population', 'Population Surveillance', 'Prevalence', 'Privacy', 'Process', 'Psyche structure', 'Public Health', 'Recruitment Activity', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Scheme', 'Services', 'Smoking Status', 'Source', 'Surveillance Methods', 'Surveys', 'System', 'Taxonomy', 'Techniques', 'Telephone', 'Text', 'Time', 'United States', 'Update', 'Work', 'base', 'center for epidemiological studies depression scale', 'cost', 'cost effective', 'depressive symptoms', 'flexibility', 'innovation', 'lexical', 'social', 'syndromic surveillance', 'text searching', 'tool', 'ward']",NLM,UNIVERSITY OF UTAH,R00,2014,224100,0.060727121737152515
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8714052,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,UNIVERSITY OF UTAH,R01,2014,579144,0.03382327740148679
"InfoMediator: Weaving Clinical Expertise in Online Health Communities     DESCRIPTION (provided by applicant): Chronic illness can be overwhelming to patients because it impacts many areas of their daily lives. Accordingly, many patients turn to online health support groups to get social support. In face-to-face patient support groups (F2F), health professional moderators provide clinical expertise within the context of peer-patients' sharing of experience. However, in online health community settings, because health professionals' time and resources are expensive, it is challenging to get health professionals' opinions for thousands of messages posted each day. To solve this problem, I propose to develop methods and techniques that maximize the use of already available clinical expertise online for online peer-patient conversation threads by developing a system, InfoMediator. The InfoMediator will semi-automatically weave health professionals' existing answers to patients' questions into peer-patient conversations by using Natural Language Processing (NLP) techniques complemented by user feedback. As the career development component of this proposal, I deepen my skills and knowledge in NLP necessary for proposed work and patient education. As I develop the NLP techniques and knowledge in patient education, I and my mentors will develop methods and techniques that address weaving clinical expertise within peer-patient conversations by designing, implementing, and evaluating socio-technical aspects of the InfoMediator. The training opportunities provided by this NIH NLM K01 grant, together with the supportive research environment at Michigan State University, will help further extend my existing expertise in human-computer interaction, design, and health informatics to establish my independent informatics research program in patient-centered technologies. Focusing on persons with diabetes, the outcomes of the proposed research will help us understand how to empower persons with diabetes to improve self- efficacy and self-care, while increasing the quality of online health information environment.             8. Project Narrative Because health professionals' time and resources are expensive, it is challenging for online health communities to engage health professionals in thousands of peer-patient conversations in the same way that health professionals moderate patient conversations in face-to-face patient support groups. To address this challenge I propose developing methods and techniques that maximize the use of already available health professionals' expertise online for online health communities and help patients improve self-efficacy and self-care toward managing chronic disease.",InfoMediator: Weaving Clinical Expertise in Online Health Communities,8759393,K01LM011980,"['Address', 'Area', 'Chronic Disease', 'Classification', 'Clinical', 'Communities', 'Community Health', 'Complement', 'Data', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Environment', 'Exposure to', 'Feedback', 'Funding', 'Goals', 'Grant', 'Guidelines', 'Health', 'Health Professional', 'Informatics', 'Interview', 'Knowledge', 'Medical', 'Mentors', 'Methods', 'Michigan', 'Natural Language Processing', 'Nursing Informatics', 'Outcome', 'Output', 'Paper', 'Participant', 'Patient Education', 'Patients', 'Persons', 'Problem Solving', 'Public Health Informatics', 'Reading', 'Research', 'Resources', 'Retrieval', 'Self Care', 'Self Efficacy', 'Social support', 'Specific qualifier value', 'Support Groups', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'biomedical informatics', 'career', 'career development', 'community setting', 'computer human interaction', 'design', 'empowered', 'experience', 'follow-up', 'improved', 'patient oriented', 'peer', 'programs', 'prototype', 'skills', 'user centered design']",NLM,MICHIGAN STATE UNIVERSITY,K01,2014,155271,0.015859949697107332
"Using NLP to Extract Clinically Important Recommendations from Radiology Reports  Abstract Communication of clinically important follow-up recommendations when abnormalities are identified on imaging studies is prone to error. The absence of an automated system to identify and track radiology follow-up recommendations is an important barrier to ensuring timely follow-up of patients, especially for non-acute but potentially life threatening and unexpected findings. The primary goal of this proposal is to develop a Natural Language Processing (NLP) system to extract clinically important recommendation information from free-text radiology reports. Each radiology report will be preprocessed at the structural, syntactic, and semantic level to generate features that will be used to extract the boundaries of sentences that include recommendation information as well as the details of reason for recommendation, requested imaging test, and recommendation time frame. We will use a large corpus of free-text radiology reports represented by a mixture of modalities (e.g., radiography, computed tomography, ultrasound, and magnetic resonance imaging (MRI)) from three different institutions. Using this dataset we will perform the following specific aims: Aim 1. Create a multi- institutional radiology report corpus annotated for clinically important recommendation information; Aim 2. Develop a novel NLP system to extract clinically important recommendations in radiology reports. The proposed research is innovative because it will generate a new text processing approach that can be used to flag reports visually and electronically so that separate workflow processes can be initiated to reduce the chance that necessary investigations or interventions suggested in the report are missed by clinicians. The proposed set of tools will be disseminated to the biomedical informatics community as open source tools. PUBLIC HEALTH RELEVANCE: Communication of recommendations for necessary investigations and interventions when abnormalities are identified on imaging studies is prone to error. When recommendations are not systematically identified and promptly communicated to referrers, poor patient outcomes can result. We propose to build natural language processing tools to automatically extract clinically important recommendation information from radiology reports.                ",Using NLP to Extract Clinically Important Recommendations from Radiology Reports,8635902,R21EB016872,"['Academic Medical Centers', 'Address', 'Adopted', 'Characteristics', 'Clinical', 'Communication', 'Communities', 'Computerized Medical Record', 'Data Set', 'Dependency', 'Diagnostic', 'Diagnostic radiologic examination', 'Ensure', 'Funding', 'Future', 'Goals', 'Gold', 'Growth', 'Guidelines', 'Hand', 'Health', 'Image', 'Imaging technology', 'Incidental Findings', 'Institution', 'Intervention', 'Investigation', 'Knowledge', 'Life', 'Lung nodule', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Measures', 'Medical center', 'Methods', 'Modality', 'Natural Language Processing', 'Outcome', 'Output', 'Patient Care', 'Patients', 'Persons', 'Process', 'Provider', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Reporting', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Risk', 'Safety', 'Semantics', 'Shapes', 'Societies', 'Specific qualifier value', 'Speech', 'System', 'Telephone', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Trauma', 'Ultrasonography', 'Unified Medical Language System', 'Washington', 'Writing', 'X-Ray Computed Tomography', 'biomedical informatics', 'cancer care', 'care delivery', 'design', 'falls', 'follow-up', 'health care delivery', 'imaging modality', 'improved', 'innovation', 'novel', 'open source', 'phrases', 'public health relevance', 'radiologist', 'screening', 'syntax', 'tool']",NIBIB,UNIVERSITY OF WASHINGTON,R21,2014,257300,0.04272576736944496
"Probabilistic Disease Surveillance     DESCRIPTION (provided by applicant):         The proposed research will further develop and evaluate a probabilistic approach to disease surveillance. In this approach, a probabilistic case detection system (CDS) uses Bayesian diagnostic networks to compute the likelihoods of patient findings for each of a set of infectious diseases for every patient in a monitored population. CDS computes these likelihoods from data in electronic medical records, including information derived from free-text reports by natural language processing. CDS makes those estimates available to a probabilistic outbreak detection and characterization component (ODCS).             ODCS also utilizes a Bayesian approach to compute the probability that an outbreak is ongoing for each of a set of infectious diseases of interest, given information from CDS. ODCS also computes probability distributions over the current and future size of a detected outbreak and other characteristics such as incubation period used by public health officials when responding to an outbreak.                        The proposed research will extend the approach, which we have already developed and evaluated for the disease influenza to six additional respiratory infectious diseases. The research will also extend the capabilities of ODCS to utilize non-EMR data, detect an unknown disease, and detect and characterize concurrent outbreaks. The planned evaluations will measure the accuracy of both CDS and ODCS using historical surveillance data from two regions and simulated outbreak data, which we will create by adding outbreak cases generated by an agent-based epidemic simulator to real baseline surveillance data from non-outbreak periods.                        The innovation being advanced by this research is a novel, integrated, Bayesian approach for the early and accurate detection of cases of diseases that threaten health and for the detection and characterization of outbreaks of diseases that threaten public health. The proposed approach has significant potential to improve the information available to public health officials and physicians, which can be expected to improve clinical and public health decision making, and ultimately to improve population health.                  Project Relevance  The proposed research will improve the ability of public health officials and physicians to estimate the current incidence of influenza and other infectious diseases and to predict the future course of epidemics of those diseases. The improved information will better support decisions made by health departments to control epidemics, which is expected to reduce morbidity and mortality from epidemic diseases.",Probabilistic Disease Surveillance,8708209,R01LM011370,"['Accident and Emergency department', 'Advanced Development', 'Area', 'Bayesian Method', 'Characteristics', 'Clinical', 'Code', 'Communicable Diseases', 'Complex', 'Computer Systems', 'Computerized Medical Record', 'County', 'Data', 'Data Sources', 'Decision Making', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Outbreaks', 'Disease model', 'Epidemic', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Future', 'Health', 'Healthcare', 'Healthcare Systems', 'Incidence', 'Individual', 'Influenza', 'Intervention', 'Knowledge', 'Laboratories', 'Lung diseases', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Public Health', 'Public Health Practice', 'Publications', 'ROC Curve', 'Reporting', 'Research', 'Schools', 'Sensitivity and Specificity', 'Severities', 'Simulate', 'Sodium Chloride', 'Structure', 'Support System', 'System', 'Systems Integration', 'Testing', 'Text', 'Time', 'Topaz', 'Universities', 'Utah', 'Vaccination', 'advanced system', 'base', 'computer code', 'diagnostic accuracy', 'disorder control', 'follow-up', 'improved', 'influenza outbreak', 'innovation', 'interest', 'knowledge base', 'mortality', 'novel', 'novel strategies', 'operation', 'pandemic disease', 'population health', 'portability', 'reproductive', 'respiratory', 'surveillance data']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2014,535841,0.015932838197633584
"Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data Adverse drug reactions (ADRs) are a major burden for patients and healthcare, causing preventable  hospitalizations and deaths, and incurring a huge cost. The long-term objective of this proposal is to advance  patient safety and reduce costs by discovering novel serious ADRs through use of automated methods that  combine information from large and varied patient populations as well as from the literature. There have been  considerable advances in pharmacovigilance, but more work is needed. For example, Vioxx, a commonly used  drug, was recently found to cause at least 88,000 occurrences of myocardial infarction, highlighting the  insufficiency of current methods. To date, methods have mainly depended on the use of single sources of data,  primarily from the Federal Food and Drug Administration Adverse Event Reporting System (FAERS) and from  electronic health records (EHRS). Although important, each of the sources has different limitations and  advantages, and therefore, combining the data across them should lead to more effective drug safety  surveillance by increasing the statistical power, and also by allowing each data source to complement the other  sources. We already have developed methods associated with each of the single sources, and therefore, this  is an excellent opportunity to build upon our research accomplishments to advance the state of the art in  pharmacovigilance.   More specifically, we will a) acquire and combine comprehensive clinical data from the electronic health  records (EHRs) of two different health care sites serving diverse populations by utilizing natural language  processing (NLP) to obtain vast quantities of fine-grained data, and then by developing data mining  methodologies on the clinical data to detect novel ADR signals, b) analyze differences in therapy-related risk  factors between the two EHR populations, such as racial and ethnic differences, c) detect ADR signals in the  FAERS database using an established methodology, d) develop improved methods to acquire ADR signals  based on information in the literature, and e) develop methods that utilize the results from the above sources to  maximize effectiveness. We will focus on eight serious ADRs, and collect a high-quality reference standard for  those ADRs so that we will be able to evaluate and compare performance of the different detection methods  individually as well as the methods that combine the sources.   This proposal is well positioned to overcome problems associated with existing automated methods, which  are primarily based on use of individual sources of data. We are confident the methods will be effective  because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in  this proposal presents an excellent chance to leverage heterogeneous data sources to dramatically improve  patient safety and reduce costs. Adverse drug reactions (ADRs) are a major burden for patients and health care, causing preventable  hospitalizations and deaths, and incurring huge costs, and, therefore, continuous post-marketing surveillance  is crucial for patient safety. This proposal aims to improve patient safety and reduce health care costs by  developing effective methods to discover new adverse drug reactions through the combination of information in  the FDA's Adverse Event Reporting System, the literature, and comprehensive clinical data from electronic  health records of two different sites with diverse populations, thereby overcoming limitations that rely mainly on  use of one data source.",Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data,8660067,R01LM010016,"['Academic Medical Centers', 'Address', 'Adverse event', 'Adverse reactions', 'Cereals', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Data', 'Complement', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Drug usage', 'Effectiveness', 'Electronic Health Record', 'Evaluation', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Hospitals', 'Individual', 'Knowledge', 'Lead', 'Literature', 'Logistic Regressions', 'Medical Care Costs', 'Methodology', 'Methods', 'Modeling', 'Myocardial Infarction', 'Natural Language Processing', 'New York', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Population Heterogeneity', 'Positioning Attribute', 'Presbyterian Church', 'Probability', 'Process', 'PubMed', 'Publications', 'Reaction', 'Reference Standards', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Factors', 'Rofecoxib', 'Safety', 'Signal Transduction', 'Site', 'Source', 'Structure', 'System', 'Techniques', 'United States Food and Drug Administration', 'Work', 'base', 'chemical property', 'conditioning', 'cost', 'data mining', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'prevent', 'racial/ethnic difference', 'research and development', 'text searching']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2014,417795,0.011448944552632533
"Encoding and Processing Patient Allergy Information in EHRs     DESCRIPTION (provided by applicant): Allergies affect one in five Americans and are the 5th leading chronic disease in the U.S. Each year, allergies account for more than 17 million outpatient office visits. Although documenting and exchanging allergy information in electronic health records (EHRs) is becoming increasingly important, we still face multiple challenges. These include: lack of well-adopted standard terminologies for representing allergies, frequent entry of allergy information as free-text, and no existing process for reconciling allergy information. In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) conduct analyses on standard terminologies and a large allergy repository to build a comprehensive knowledge base for representing allergy information; 2) design, develop and evaluate a natural language processing (NLP) module for extracting and encoding free-text allergy information and integrate it with an existing NLP system; 3) measure the feasibility and efficiency of the proposed NLP system for the new process of allergy reconciliation; and 4) distribute our methods and tool, so they are widely available to other researchers and healthcare institutions for non-commercial use.         PUBLIC HEALTH RELEVANCE: Managing allergy information within the electronic health record (EHR) is vital to ensuring patient safety. The goal of this study is to propose a comprehensive solution to assess existing terminology standards and knowledge bases for representing allergy information, develop and evaluate a natural language processing (NLP) system for extracting and encoding allergy information from free-text clinical documents, and finally measure the feasibility of using NLP output to facilitate the allergy reconciliation proces.            ",Encoding and Processing Patient Allergy Information in EHRs,8741955,R01HS022728,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2014,489854,0.0852415444145766
"Improving Accuracy of Electronic Notes Using A Faster, Simpler Approach     DESCRIPTION (provided by applicant): Physician progress notes contain information essential to patient care, including findings from history and physical exam, interpretation of tests, assessment and treatment plans. However in the transition from paper to electronic physician notes, many physicians spend more time creating them, which has led to the use of time-saving measures such as copy/paste and templates that have degraded note accuracy and quality. This threatens the usefulness of notes not only for their most important use-patient care-but also for research, quality improvement, and in supporting reimbursement. To address these problems, we propose a project with the following specific aims: 1. To refine and implement a new voice-generated enhanced electronic note system (VGEENS) integrating voice recognition with natural language processing and links to the electronic medical record (EMR) to improve note accuracy and timeliness. 2. To evaluate VGEENS using a randomized trial with 30 internal medicine physicians in each arm to assess electronic note accuracy, quality, timeliness, and user satisfaction. Intervention physicians will use VGEENS, while the control physicians will continue with note creation as they normally would. This novel approach has the potential to improve note accuracy while reducing delays in making progress notes in EMRs available to other clinicians. It leverages rapidly improving voice recognition and NLP technologies to permit physicians to use a natural, fast method-human voice-to convey their observation and thoughts into the EMR record.         PUBLIC HEALTH RELEVANCE: Physician documentation of a patient visit contains information that is used in that patient's care. This information includes findings from a patient' history and physical exam, interpretation of necessary tests, the problem assessment and treatment plan. However, in the transition from paper to electronic physician notes, many physicians are spending more time creating these notes. This has led to use of time-saving measures that have degraded the accuracy and ease of use of patient notes. By the end of this project, we expect to have developed, used and evaluated a new method for creating electronic physician notes that both improve accuracy and timely availability of inpatient progress notes.            ","Improving Accuracy of Electronic Notes Using A Faster, Simpler Approach",8805997,R21HS023631,[' '],AHRQ,UNIVERSITY OF WASHINGTON,R21,2014,154347,-0.025353862929716607
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453          PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,8722026,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2014,342375,-0.021726057007146297
"NLP-enabled decision support for cervical cancer screening and surveillance     DESCRIPTION (provided by applicant): Although cervical cancer is preventable, it still continues to be a leading cause of death. Following the evidence- based guidelines for cervical cancer prevention is challenging for healthcare providers, due to which many patients do not receive the optimal preventive care. Clinical decision support (CDS) systems can potentially improve the care delivery. However, the current CDS systems only identify patients overdue for screening, and do not suggest the optimal screening interval. Moreover they do not help with surveillance of patients with abnormal screening results. This is because the existing systems lack the capability to process free- text clinical reports that contain information needed for applying the guidelines. Hence there is a critical need for natural language processing (NLP)-enabled CDS systems that can utilize discrete as well as free-text patient information for enhancing the decision support. Our long-term goal is to improve healthcare delivery of cervical cancer prevention with guideline based reminders. The central hypothesis is that NLP- enabled CDS system will significantly improve the quality of care delivery for cervical cancer prevention. The rationale is that use of NLP will improve granularity of the guideline implementation, which will in-turn enhance care delivery. As preliminary work we have developed an NLP-enabled CDS system that automatically interprets the patient information from the electronic health record and applies the national guidelines to compute the optimal recommendation for screening and surveillance. We have performed validation of the system in a non-clinical setting.1 In this application we will proceed towards deployment of the system in the clinical setting, and will carry out studies for measuring the impact on the quality of care delivery. In ai one, we will validate the system in the clinical setting and will optimize its usability and workflw integration. In aim two, we will test the hypothesis that reminders from the NLP-enabled CDS system to primary care providers will improve the quality of care delivery, by performing a one year intervention control study across four sites of a primary care practice. In aim three, we will test the hypothesis that reminders to non-adherent high-risk patients will improve their surveillance rates, by performing a randomized intervention study for three months. In this study, care coordinators will utilize the CDS system for sending reminders to patients that are non-adherent and at high risk due to abnormal screenings. The main contribution of this project will be knowledge about the effectiveness of NLP in enhancing the impact of CDS systems for cervical cancer prevention, and for clinical practice in general. This project is innovative because the CDS system will utilize NLP to generate screening reminders for normal patients and surveillance reminders for patients with abnormal findings. This is a major advancement over existing systems that can only identify patients for screening.         PUBLIC HEALTH RELEVANCE: The proposed research is relevant to public health because it will yield knowledge about the effectiveness of natural language processing (NLP) to enhance impact of CDS systems for cervical cancer prevention, and for clinical practice in general. This research will foster implementation of similar CDS systems across the nation for cervical cancer prevention and for other decision problems, which will improve the quality of healthcare delivery. Thus, the proposed research is relevant to AHRQ's mission to improve the quality, safety, efficiency, and effectiveness of health care for all Americans.            ",NLP-enabled decision support for cervical cancer screening and surveillance,8678798,R21HS022911,[' '],AHRQ,MAYO CLINIC ROCHESTER,R21,2014,145229,0.04038837038858005
"Challenges in Natural Language Processing for Clinical Narratives     DESCRIPTION (provided by applicant): Narratives of electronic health records (EHRs) contain useful information that is difficult to automatically extract, index, search, or interpret. Clinical natural language processing (NLP) technologies for automatic extraction, indexing, searching, and interpretation of EHRs are in development; however, due to privacy concerns related to EHRs, such technologies are usually developed by teams that have privileged access to EHRs in a specific institution. Technologies that are tailored to a specific set of data from a given institution generate inspiring results on that data; however, they can fail to generalize to similar data from other institutions and even other departments from the same institution. Therefore, learning from these technologies and building on them becomes difficult.          In order to improve NLP in EHRs, there is need for head-to-head comparison of approaches that can address a given task on the same data set. Shared-tasks provide one way of conducting systematic head-to- head comparisons. This proposal describes a series of shared-task challenges and conferences, spread over a five year period, that promote the development and evaluation of cutting edge clinical NLP systems by distributing de-identified EHRs to the broad research community, under data use agreements, so that:      *	the state-of-the-art in clinical NLP technologies can be identified and advanced,      *	a set of technologies that enable the use of the information contained in EHR narratives becomes available, and      *	the information from EHR narratives can be made more accessible, for example, for clinical and medical research.          The scientific activities supporting the organization of the shared-task challenges are sponsored in part by Informatics for Integrating Biology and the Bedside (i2b2), grant number U54-LM008748, PI: Kohane.          This proposal aims to organize a series of workshops, conference proceedings, and journal special issues that will accompany the shared-task challenges in order to disseminate the knowledge generated by the challenges.                  Public health relevance: this proposal will address two main challenges related to the use of clinical narratives for research: availability of clinical records for research and identification of the state of the art in clinical natural language processing (NLP) technologies so that we can push the state of the art forward and so that future work can build on the past. Progress in clinical NLP will improve access to electronic health records for research, and for clinical applications, benefiting healthcare and public health.",Challenges in Natural Language Processing for Clinical Narratives,8538500,R13LM011411,"['Access to Information', 'Address', 'Agreement', 'Biology', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Set', 'Development', 'Distributed Systems', 'Educational workshop', 'Electronic Health Record', 'Evaluation', 'Future', 'Goals', 'Gold', 'Grant', 'Hand', 'Healthcare', 'Improve Access', 'Informatics', 'Institution', 'Journals', 'Knowledge', 'Learning', 'Medical Research', 'Natural Language Processing', 'Privacy', 'Public Health', 'Publications', 'Records', 'Research', 'Rest', 'Series', 'System', 'Systems Development', 'Targeted Research', 'Technology', 'Work', 'clinical application', 'data sharing', 'head-to-head comparison', 'improved', 'indexing', 'practical application', 'public health relevance', 'symposium']",NLM,STATE UNIVERSITY OF NEW YORK AT ALBANY,R13,2013,18400,0.06801985663564217
"Natural language processing for clinical and translational research     DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. This growth is being fueled by recent federal legislation that provides generous financial incentives to institutions demonstrating aggressive application and ""meaningful use"" of comprehensive EMRs. Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large scale studies of disease onset and treatment outcome, specifically within the context of routine clinical care. However, a well-known challenge for secondary use of EMR data for clinical and translational research is that much of detailed patient information is embedded in narrative text. Natural Language Processing (NLP) technologies, which are able to convert unstructured clinical text into coded data, have been introduced into the biomedical domain and have demonstrated promising results. Researchers have used NLP systems to identify clinical syndromes and common biomedical concepts from radiology reports, discharge summaries, problem lists, nursing documentation, and medical education documents. Different NLP systems have been developed at different institutions and utilized to convert clinical narrative text into structured data that may be used for other clinical applications and studies. Successful stories in applying NLP to clinical and translational research have been reported widely. However, institutions often deploy different NLP systems, which produce various types of output formats and make it difficult to exchange information between sites. Therefore, the lack of interoperability among different clinical NLP systems becomes a bottleneck for efficient multi-site studies. In addition, many successful studies often require a strong interdisciplinary team where informaticians and clinicians have to work very closely to iteratively define optimal algorithms for clinical phenotypes. As intensive informatics support may not be available to every clinical researcher, the usability of NLP systems for end users is another important issue. The proposed project builds upon first-hand knowledge and experience across the research team in the use of NLP for clinical and translational research projects. There are several big informatics initiatives for clinical and translational research but those initiatives generally assume one shoe fits all and follow top-down approaches to develop NLP solutions. Complementary to those initiatives, we will use a bottom-up approach to handle interoperability and usability: i) we will obtain a common NLP data model and exchange format through empirical analysis of existing NLP systems and NLP results; ii) we will develop a user-centric NLP front end interface for NLP systems wrapped to be consistent with the proposed NLP data model and exchange format incorporating usability analysis into the agile development process. All deliverables will be distributed through the open health NLP (OHNLP) consortium which we intend to make it more open and inclusive.         PUBLIC HEALTH RELEVANCE: Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for clinical and translational research. We propose the development of a novel framework to enable the use of clinical information embedded in clinical narratives for clinical and translational research.                ",Natural language processing for clinical and translational research,8505753,R01GM102282,"['Acceleration', 'Adopted', 'Adoption', 'Adverse drug effect', 'Algorithms', 'Architecture', 'Attention', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'DNA Databases', 'Data', 'Data Set', 'Development', 'Dictionary', 'Discipline of Nursing', 'Disease Association', 'Documentation', 'Elements', 'Exclusion Criteria', 'Genes', 'Genomics', 'Goals', 'Growth', 'Health', 'Informatics', 'Institution', 'Knowledge', 'Link', 'Logical Observation Identifiers Names and Codes', 'Manuals', 'Medical Education', 'Modeling', 'Natural Language Processing', 'Onset of illness', 'Output', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Play', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Role', 'SNOMED Clinical Terms', 'Semantics', 'Shoes', 'Site', 'Solutions', 'Statutes and Laws', 'Structure', 'Syndrome', 'System', 'Technology', 'Text', 'Translational Research', 'Treatment outcome', 'Work', 'base', 'clinical application', 'clinical care', 'clinical phenotype', 'computer human interaction', 'data exchange', 'data modeling', 'experience', 'financial incentive', 'flexibility', 'human centered computing', 'interoperability', 'novel', 'open source', 'patient safety', 'public health relevance', 'rapid growth', 'success', 'tool', 'usability', 'user-friendly']",NIGMS,MAYO CLINIC ROCHESTER,R01,2013,630706,0.0822176457303292
"Utilizing social media as a resource for mental health surveillance     DESCRIPTION (provided by applicant):  Major depressive disorder is one of the most common debilitating illnesses in the United States, with a lifetime prevalence of 16.2%. Currently, nationwide mental health surveillance takes the form of large-scale telephone- based surveys. These surveys have high running costs and require teams of human telephone operators. Even the largest system, the Behavioral Risk Factor Surveillance System, reaches only 0.13% of the US population. Twitter (and other microblog services) offers a rich, if terse, multilingual source of real time data for public health surveillance. Natural Language Processing (NLP) provides techniques and resources to ""unlock"" data from text. We propose using Twitter and NLP as a cost-effective and flexible approach to augmenting current telephone- based surveillance methods for population level depression monitoring.         This grant application has two major strands. First, investigating ethical issues and challenges to privacy that emerge with the use of Twitter data for public health surveillance (Aim One). Second, developing techniques and resources for real-time public health surveillance for mental illness from Twitter (Aim Two &Aim Three). Aim One seeks to investigate and codify our responsibilities as researchers towards Twitter users by engaging with those users directly. With Aim Two, we will build and evaluate Natural Language Processing resources - algorithms, lexicons and taxonomies - to support the identification of depression symptoms in Twitter data. For Aim Three, we will build and evaluate Natural Language Processing modules and services that use Twitter as a data source for monitoring depression levels in the community. The significance of the proposed work lies in three areas. First, our investigations - both empirical and theoretical - will explore ethical issues in the use of Twitter for public health surveillance. This work has the potential to guide future research in the area. Second, in developing and evaluating algorithms and resources for identifying depression from tweets, we are contributing foundational work to the field of NLP. Third, developing these algorithms and resources will provide the bedrock for building social media based surveillance systems which will provide a cost effective means of augmenting current mental health surveillance practice. This proposal is innovative in both its application area (microblogs have not been used before for mental health surveillance), its focus on using NLP to identify depressive symptoms for public health, and in the central role that qualitative bioethical research will play in guiding the work.              Project Narrative The proposed research focuses on using advanced Natural Language Processing methods to mine microblog data - in this case, Twitter - for mental health surveillance (specifically, depression surveillance), in order to augment current telephone-based mental health surveillance systems. The research has public health at its core.",Utilizing social media as a resource for mental health surveillance,8510292,K99LM011393,"['Algorithms', 'Applications Grants', 'Area', 'Attitude', 'Behavioral Risk Factor Surveillance System', 'Broadcast Media', 'Cities', 'Cognitive', 'Communities', 'County', 'Data', 'Data Sources', 'Dental', 'Development', 'Disasters', 'Earthquakes', 'Electronic Health Record', 'Epidemiology', 'Ethical Issues', 'Ethics', 'Exercise', 'Guidelines', 'Health', 'Human', 'Influenza A Virus, H1N1 Subtype', 'Interview', 'Investigation', 'Linguistics', 'Location', 'Major Depressive Disorder', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Methods', 'Mining', 'Monitor', 'Natural Language Processing', 'Participant', 'Phase', 'Play', 'Population', 'Population Surveillance', 'Prevalence', 'Privacy', 'Process', 'Psyche structure', 'Public Health', 'Recruitment Activity', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Scheme', 'Services', 'Smoking Status', 'Source', 'Surveillance Methods', 'Surveys', 'System', 'Taxonomy', 'Techniques', 'Telephone', 'Text', 'Time', 'United States', 'Update', 'Work', 'base', 'center for epidemiological studies depression scale', 'cost', 'cost effective', 'depressive symptoms', 'flexibility', 'innovation', 'lexical', 'social', 'syndromic surveillance', 'text searching', 'tool', 'ward']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2013,82800,0.060727121737152515
"Measuring and Improving Colonoscopy Quality Using Natural Language Processing No abstract available  Prior research has shown that the quality of colonoscopy varies from physician to physician. It is important to address low quality because patients who see physicians with lower quality colonoscopies have a higher risk of developing colorectal cancer in the future. To help address poor quality of colonoscopy, we propose to use a computer software program to measure colonoscopy quality, survey physicians to understand why there is variation in quality, and test methods of physician feedback to stimulate quality improvement.                 ",Measuring and Improving Colonoscopy Quality Using Natural Language Processing,8752179,R01CA168959,"['Address', 'Affect', 'Cancer Etiology', 'Cecum', 'Cessation of life', 'Colon', 'Colonoscopy', 'Colorectal', 'Colorectal Cancer', 'Computer software', 'Computers', 'Data', 'Detection', 'Effectiveness', 'Environment', 'Feedback', 'Future', 'Gastroenterology', 'Geographic Locations', 'Goals', 'Gold', 'Health system', 'Healthcare', 'Healthcare Systems', 'Hospitals', 'Incidence', 'Institution', 'Knowledge', 'Link', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Metric', 'Monitor', 'Names', 'Natural Language Processing', 'Pathology Report', 'Patients', 'Performance', 'Physicians', 'Provider', 'Randomized', 'Reading', 'Recommendation', 'Records', 'Reporting', 'Research', 'Risk', 'Science', 'Societies', 'Sum', 'Surveys', 'System', 'Testing', 'Text', 'Training', 'United States National Institutes of Health', 'Variant', 'Work', 'adenoma', 'base', 'colorectal cancer screening', 'computer science', 'cost effective', 'high risk', 'improved', 'innovation', 'medical specialties', 'novel', 'payment', 'peer', 'screening', 'symposium', 'tool']",NCI,HARVARD MEDICAL SCHOOL,R01,2013,634795,0.01446974032517302
"Annotation, development and evaluation for clinical information extraction    DESCRIPTION (provided by applicant): Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible.       PUBLIC HEALTH RELEVANCE: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.         ","Annotation, development and evaluation for clinical information extraction",8501543,R01GM090187,"['Address', 'Algorithms', 'Automated Annotation', 'Clinical', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'Consensus', 'Country', 'Data Set', 'Development', 'Disease', 'Evaluation', 'Goals', 'Gold', 'Guidelines', 'Individual', 'Judgment', 'Knowledge', 'Linguistics', 'Manuals', 'Medical Records', 'Methodology', 'Methods', 'Metric', 'Natural Language Processing', 'Performance', 'Reliance', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Signs and Symptoms', 'System', 'Technology', 'Terminology', 'Text', 'Training', 'Translational Research', 'Translations', 'base', 'clinical care', 'cost', 'design', 'flexibility', 'innovation', 'knowledge translation', 'phrases', 'prevent', 'public health relevance', 'research clinical testing', 'research study', 'tool']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,370221,0.09115466932292769
"Text Processing and Geospatial Uncertainty for Phylogeography of Zoonotic Viruses     DESCRIPTION (provided by applicant): Phylogeography of zoonotic viruses studies the geographical spread and genetic lineages of viruses that are transmittable between animals and humans such as avian influenza and rabies. This science can help state public health and agriculture agencies identify the animal hosts that most impact virus propagation in a particular geographic region, the migration path of the virus including its origin, and the patterns of infection in various host populations, including humans, over time. The National Center for Biotechnology Information (NCBI), specifically GenBank, provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. However, geospatial metadata such as host location is inconsistently represented and sparse across GenBank entries, with our preliminary studies showing only about 20% of the GenBank records contain specific information such as a county, town, or region within a state. While this detailed geospatial information might be included in the corresponding journal article, it is not available for immediate use in a bioinformatics or GIS application unless it is manually extracted and linked back to the appropriate sequence. Absence of precise sampling locations from easily-computable secondary data sources such as GenBank increases the difficulty of achieving accurate phylogeographic models of virus migration. We propose an infrastructure to improve phylogeographic models of virus migration by linking relevant geospatial data from the literature. This work represents the first effort to use automatically extracted geospatial data present in journal articles corresponding to GenBank records in order to enhance modeling of virus migration. Our research will extend phylogeography and zoonotic surveillance by: creating a Natural Language Processing (NLP) infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the data extracted in Aim 1 with adequate biostatistical models (Aim 2), and evaluating the impact of our approach for phylogeography and surveillance of zoonotic viruses (Aim 3). Thus, this work will provide researchers with a framework for population surveillance using an integrated biomedical informatics approach including NLP, biostatistics, bioinformatics, and database design.           We will create Natural Language Processing Infrastructure and novel phylogeographic models of zoonotic viruses that will allow state public health and agriculture agencies and other researchers to study virus migration. This will enhance population health surveillance including identification of the animal hosts that most impact virus propagation in a particular geographic region, the migration path of zoonotic pathogens, and the patterns of infection in various host populations over time, including humans. This resource will enable state agencies to implement improved public health control measures that will reduce morbidity and mortality of animals and humans from zoonotic diseases.                ",Text Processing and Geospatial Uncertainty for Phylogeography of Zoonotic Viruses,8698542,R56AI102559,"['Accounting', 'Address', 'Agriculture', 'Animals', 'Applied Research', 'Avian Influenza', 'Back', 'Bioinformatics', 'Biometry', 'Biotechnology', 'China', 'Computer software', 'Country', 'County', 'Data', 'Data Sources', 'Databases', 'Development', 'Disease', 'Epidemiologist', 'Evaluation', 'Event', 'Foundations', 'Funding', 'Genbank', 'Generations', 'Genes', 'Genetic', 'Genomics', 'Geographic Locations', 'Goals', 'Gold', 'Habitats', 'Hantavirus', 'Human', 'Infection', 'Influenza', 'Information Systems', 'Label', 'Link', 'Literature', 'Location', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Natural Language Processing', 'Pattern', 'Population', 'Population Surveillance', 'Process', 'Public Health', 'Publications', 'Rabies', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Scientist', 'Solutions', 'Surveillance Modeling', 'System', 'Techniques', 'Text', 'Time', 'Trees', 'Uncertainty', 'Vertebrates', 'Viral', 'Viral Genome', 'Virus', 'Work', 'animal mortality', 'biomedical informatics', 'data modeling', 'database design', 'disease transmission', 'improved', 'journal article', 'migration', 'mortality', 'novel', 'pathogen', 'population health', 'web site']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R56,2013,451478,0.03693598179907826
"Probabilistic Disease Surveillance     DESCRIPTION (provided by applicant):         The proposed research will further develop and evaluate a probabilistic approach to disease surveillance. In this approach, a probabilistic case detection system (CDS) uses Bayesian diagnostic networks to compute the likelihoods of patient findings for each of a set of infectious diseases for every patient in a monitored population. CDS computes these likelihoods from data in electronic medical records, including information derived from free-text reports by natural language processing. CDS makes those estimates available to a probabilistic outbreak detection and characterization component (ODCS).             ODCS also utilizes a Bayesian approach to compute the probability that an outbreak is ongoing for each of a set of infectious diseases of interest, given information from CDS. ODCS also computes probability distributions over the current and future size of a detected outbreak and other characteristics such as incubation period used by public health officials when responding to an outbreak.                        The proposed research will extend the approach, which we have already developed and evaluated for the disease influenza to six additional respiratory infectious diseases. The research will also extend the capabilities of ODCS to utilize non-EMR data, detect an unknown disease, and detect and characterize concurrent outbreaks. The planned evaluations will measure the accuracy of both CDS and ODCS using historical surveillance data from two regions and simulated outbreak data, which we will create by adding outbreak cases generated by an agent-based epidemic simulator to real baseline surveillance data from non-outbreak periods.                        The innovation being advanced by this research is a novel, integrated, Bayesian approach for the early and accurate detection of cases of diseases that threaten health and for the detection and characterization of outbreaks of diseases that threaten public health. The proposed approach has significant potential to improve the information available to public health officials and physicians, which can be expected to improve clinical and public health decision making, and ultimately to improve population health.                  Project Relevance  The proposed research will improve the ability of public health officials and physicians to estimate the current incidence of influenza and other infectious diseases and to predict the future course of epidemics of those diseases. The improved information will better support decisions made by health departments to control epidemics, which is expected to reduce morbidity and mortality from epidemic diseases.",Probabilistic Disease Surveillance,8578484,R01LM011370,"['Accident and Emergency department', 'Advanced Development', 'Area', 'Characteristics', 'Clinical', 'Code', 'Communicable Diseases', 'Complex', 'Computer Systems', 'Computerized Medical Record', 'County', 'Data', 'Data Sources', 'Decision Making', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Outbreaks', 'Disease model', 'Epidemic', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Future', 'Health', 'Healthcare', 'Healthcare Systems', 'Incidence', 'Individual', 'Influenza', 'Intervention', 'Knowledge', 'Laboratories', 'Lung diseases', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Public Health', 'Public Health Practice', 'Publications', 'ROC Curve', 'Reporting', 'Research', 'Schools', 'Sensitivity and Specificity', 'Severities', 'Simulate', 'Sodium Chloride', 'Structure', 'Support System', 'System', 'Systems Integration', 'Testing', 'Text', 'Time', 'Topaz', 'Universities', 'Utah', 'Vaccination', 'advanced system', 'base', 'computer code', 'diagnostic accuracy', 'disorder control', 'follow-up', 'improved', 'influenza outbreak', 'innovation', 'interest', 'knowledge base', 'mortality', 'novel', 'novel strategies', 'operation', 'pandemic disease', 'population health', 'portability', 'reproductive', 'respiratory', 'surveillance data']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2013,590023,0.015932838197633584
"POET-2: High-performance computing for advanced clinical narrative preprocessing    DESCRIPTION (provided by applicant):       This project focuses on clinical natural language processing (cNLP), a field of emerging importance in informatics. Starting with the Linguistic String Project's Medical Language Processor (New York University) in the 1970s, researchers have made steady gains in cNLP through empirical studies and by building sophisticated high-level cNLP software applications (e.g., Columbia's MedLEE). There are no fewer than four scientific conferences devoted exclusively to biomedical/clinical NLP. The cNLP literature has been growing over the past decade, and this will gain momentum as more clinical text repositories are released, such as the MIMIC II and University of Pittsburgh BLU Lab corpora.       However, sustained success in the field of cNLP is hampered by the reality that clinical texts have a far more noise than do texts traditionally studied in NLP, such as newswire articles, biomedical abstracts, and discharge summaries. Noise in this context is defined by the parseability characteristics of the language and the linguistic structures that appear in text. Clinical texts come in a striking variety of note types, with the best studied types being discharge summaries, radiology reports, and pathology reports. These note types share an important feature: they are written to communicate care issues between healthcare providers and hence typically are well-composed, well-edited, and often are dictated. But the vast majority of notes in the electronic health record are written primarily to document care issues. They communicate as well, of course, but much less care is used in their creation than with discharge summaries and reports. As a result they are often ungrammatical; are composed of short, telegraphic phrases; are replete with misspellings and shorthand (e.g., abbreviations); are ill-formatted with templates and liberal use of white space; and are embedded with ""non-prose"" (e.g., strings of laboratory values). All of these sources of noise complicate otherwise straightforward NLP tasks like tokenization, sentence segmentation, and ultimately information extraction itself.       We propose a systematic study of ways to increase the signal-to-noise ratio in clinical narratives to improve cNLP. This work extends our preliminary research (under the POET project) and has the following aims:        o Develop and implement a suite of parseability improvement tools designed for all clinical note types from multiple healthcare institutions.     o Evaluate the empirical and the functional success of the parseability improvement tools.     o Design and implement a HIPAA-compliant UlMA-based pipeline cNLP framework for use in a typical high-performance, multi-processor computing environment.              Project Narrative We can see in the multi-billion dollar investment in electronic health records (EHRs) by the ARRA that mining clinical data electronically will continue to be essential to informatics research. Most data in the EHR resides as unstructured text, and POET2 provides a means to unlock that data through combining a new, HIPAA- complaint high-performance computing architecture with sophisticated text preprocessing.",POET-2: High-performance computing for advanced clinical narrative preprocessing,8536940,R01LM010981,"['Abbreviations', 'Active Learning', 'Address', 'Architecture', 'Area', 'Authorization documentation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Data', 'Computer software', 'Data', 'Electronic Health Record', 'Electronics', 'Employee Strikes', 'Ensure', 'Environment', 'Evaluation', 'Face', 'Gold', 'Growth', 'Health Care Reform', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'High Performance Computing', 'Informatics', 'Inpatients', 'Institution', 'Institutional Review Boards', 'Investments', 'Laboratories', 'Language', 'Linguistics', 'Literature', 'Maps', 'Medical', 'Mining', 'Modeling', 'Natural Language Processing', 'New York', 'Noise', 'Occupations', 'Outpatients', 'Paper', 'Pathology', 'Pathology Report', 'Patients', 'Performance', 'Proliferating', 'Publishing', 'Radiology Specialty', 'Records', 'Report (document)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Support', 'Resolution', 'Series', 'Shorthand', 'Signal Transduction', 'Source', 'Structure', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'Voting', 'Work', 'Writing', 'abstracting', 'base', 'cluster computing', 'data mining', 'design', 'improved', 'meetings', 'novel', 'phrases', 'pressure', 'repaired', 'repository', 'research study', 'success', 'symposium', 'tool', 'web services']",NLM,UNIVERSITY OF UTAH,R01,2013,292186,0.030413375966211624
"Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data Adverse drug reactions (ADRs) are a major burden for patients and healthcare, causing preventable  hospitalizations and deaths, and incurring a huge cost. The long-term objective of this proposal is to advance  patient safety and reduce costs by discovering novel serious ADRs through use of automated methods that  combine information from large and varied patient populations as well as from the literature. There have been  considerable advances in pharmacovigilance, but more work is needed. For example, Vioxx, a commonly used  drug, was recently found to cause at least 88,000 occurrences of myocardial infarction, highlighting the  insufficiency of current methods. To date, methods have mainly depended on the use of single sources of data,  primarily from the Federal Food and Drug Administration Adverse Event Reporting System (FAERS) and from  electronic health records (EHRS). Although important, each of the sources has different limitations and  advantages, and therefore, combining the data across them should lead to more effective drug safety  surveillance by increasing the statistical power, and also by allowing each data source to complement the other  sources. We already have developed methods associated with each of the single sources, and therefore, this  is an excellent opportunity to build upon our research accomplishments to advance the state of the art in  pharmacovigilance.   More specifically, we will a) acquire and combine comprehensive clinical data from the electronic health  records (EHRs) of two different health care sites serving diverse populations by utilizing natural language  processing (NLP) to obtain vast quantities of fine-grained data, and then by developing data mining  methodologies on the clinical data to detect novel ADR signals, b) analyze differences in therapy-related risk  factors between the two EHR populations, such as racial and ethnic differences, c) detect ADR signals in the  FAERS database using an established methodology, d) develop improved methods to acquire ADR signals  based on information in the literature, and e) develop methods that utilize the results from the above sources to  maximize effectiveness. We will focus on eight serious ADRs, and collect a high-quality reference standard for  those ADRs so that we will be able to evaluate and compare performance of the different detection methods  individually as well as the methods that combine the sources.   This proposal is well positioned to overcome problems associated with existing automated methods, which  are primarily based on use of individual sources of data. We are confident the methods will be effective  because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in  this proposal presents an excellent chance to leverage heterogeneous data sources to dramatically improve  patient safety and reduce costs. Adverse drug reactions (ADRs) are a major burden for patients and health care, causing preventable  hospitalizations and deaths, and incurring huge costs, and, therefore, continuous post-marketing surveillance  is crucial for patient safety. This proposal aims to improve patient safety and reduce health care costs by  developing effective methods to discover new adverse drug reactions through the combination of information in  the FDA's Adverse Event Reporting System, the literature, and comprehensive clinical data from electronic  health records of two different sites with diverse populations, thereby overcoming limitations that rely mainly on  use of one data source.",Pharmacovigilance Methods: Leveraging Heterogeneous Adverse Drug Reaction Data,8438732,R01LM010016,"['Academic Medical Centers', 'Address', 'Adverse event', 'Adverse reactions', 'Cereals', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Data', 'Complement', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Drug usage', 'Effectiveness', 'Electronic Health Record', 'Evaluation', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Hospitals', 'Individual', 'Knowledge', 'Lead', 'Literature', 'Logistic Regressions', 'Medical Care Costs', 'Methodology', 'Methods', 'Modeling', 'Myocardial Infarction', 'Natural Language Processing', 'New York', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Population Heterogeneity', 'Positioning Attribute', 'Presbyterian Church', 'Probability', 'Process', 'PubMed', 'Publications', 'Reaction', 'Reference Standards', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Factors', 'Rofecoxib', 'Safety', 'Signal Transduction', 'Site', 'Source', 'Structure', 'System', 'Techniques', 'United States Food and Drug Administration', 'Work', 'base', 'chemical property', 'conditioning', 'cost', 'data mining', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'prevent', 'racial/ethnic difference', 'research and development', 'text searching']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2013,440117,0.011448944552632533
"Integration of an NLP-based application to support medication management     DESCRIPTION (provided by applicant): An accurate and complete medication list on a patient's electronic health record (EHR) is critical to prevent prescribing and administration errors. Stage 1 of Meaningful Use requires certified EHRs to be capable of providing a user with the ability to perform medication reconciliation. However, most previous studies have taken place in the inpatient setting, while medication reconciliation in the outpatient setting is importnt and challenging. In addition, clinical notes contain critical medication information that also need to be reconciled. Our goal of this study is to develop novel methods and a system using natural language processing (NLP) and other technologies to facilitate the medication reconciliation process in the ambulatory setting. Our specific aims are to : 1) identify the requirements, use cases, work flow issues, barriers to and facilitators of using clinical notes and a NLP-based system in the medication reconciliation process; 2) design a generic system architecture and an application that integrates an NLP system and a web-based user interface within an existing medication reconciliation system; 3) pilot this study in two primary care clinics and measure the utilization, usability, performance and feasibility of the proposed methods and the tool; and 4) distribute our methods and the tool and to make them widely available to other researchers and healthcare institutions for non-commercial use.          An accurate and complete medication list on a patient's electronic health record (EHR) is critical to prevent prescribing and administration errors. In thi study, we will develop novel methods and a tool using natural language processing and other technologies to facilitate the medication reconciliation process. We will implement the system and evaluate our approach in the outpatient setting.            ",Integration of an NLP-based application to support medication management,8496045,R21HS021544,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R21,2013,148223,0.032415569210707
"Annotation, development and evaluation for clinical information extraction (transfer) Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible. In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.","Annotation, development and evaluation for clinical information extraction (transfer)",8868500,R01GM090187,[' '],NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2013,297936,0.09111453825096591
"Real-time Disambiguation of Abbreviations in Clinical Notes    DESCRIPTION (provided by applicant): A key prerequisite for high-quality healthcare delivery is effective communication within and across healthcare settings. However, communication can be hampered by the pervasive use of abbreviations in clinical notes. Clinicians use abbreviations to save time during documentation. While abbreviations may seem unambiguous to their authors, they often cause confusion to other readers, including healthcare providers, patients, and natural language processing (NLP) systems attempting to extract clinical terms from text. While the understanding that abbreviations can cause errors is widespread, few have deployed pragmatic solutions for this important problem. The proposed project will develop, evaluate, and share a systematic approach to Clinical Abbreviation Recognition and Disambiguation (CARD), and in doing so substantially aims to benefit existing NLP systems and to improve computer-based documentation systems by reducing ambiguities in electronic records in real-time. The study includes the following five Specific Aims: 1) Develop automated methods to detect abbreviations and their senses from clinical text corpora and build a comprehensive knowledge base of clinical abbreviations; 2) Develop and evaluate three automated word sense disambiguation (WSD) classifiers, and establish methods to combine those classifiers to maximize both their performance and coverage; 3) Develop the CARD system, and demonstrate its effectiveness by integrating it with two established NLP systems (MedLEE and KnowledgeMap); 4) Integrate CARD with an institutional clinical documentation system (Vanderbilt's StarNotes) and evaluate its ability to expand abbreviations in real-time as clinicians generate records; 5) Distribute the CARD knowledge base and software for non-commercial uses.              Project Narrative Abbreviations are widely used throughout all types of clinical documents and they cause confusion to both healthcare providers and patients and limit effective communications within and across care settings. This proposed study will develop informatics methods to automatically detect abbreviations and their possible meanings from large clinical text and to disambiguate abbreviations that have multiple meanings. We will also integrate those methods with clinical documentation systems so that abbreviations will be expanded in real-time when physicians entering clinical notes, thus to improve the quality of health records.",Real-time Disambiguation of Abbreviations in Clinical Notes,8305149,R01LM010681,"['Abbreviations', 'Algorithms', 'Architecture', 'Caring', 'Cessation of life', 'Clinical', 'Communication', 'Computer Systems', 'Computer software', 'Computers', 'Confusion', 'Coronary Arteriosclerosis', 'Databases', 'Detection', 'Disease', 'Documentation', 'Effectiveness', 'Electronics', 'Equipment and supply inventories', 'Frequencies', 'Health Personnel', 'Healthcare', 'Individual', 'Informatics', 'Joints', 'Machine Learning', 'Manuals', 'Medical Records', 'Methods', 'Names', 'Natural Language Processing', 'Nitroglycerin', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Provider', 'Reader', 'Records', 'Serious Adverse Event', 'Solutions', 'System', 'Technology', 'Text', 'Time', 'Work', 'Writing', 'acronyms', 'base', 'health care delivery', 'health record', 'improved', 'innovation', 'insight', 'knowledge base', 'novel', 'phrases', 'satisfaction']",NLM,VANDERBILT UNIVERSITY,R01,2012,129035,0.05220306626605401
"Challenges in Natural Language Processing for Clinical Narratives     DESCRIPTION (provided by applicant): Narratives of electronic health records (EHRs) contain useful information that is difficult to automatically extract, index, search, or interpret. Clinical natural language processing (NLP) technologies for automatic extraction, indexing, searching, and interpretation of EHRs are in development; however, due to privacy concerns related to EHRs, such technologies are usually developed by teams that have privileged access to EHRs in a specific institution. Technologies that are tailored to a specific set of data from a given institution generate inspiring results on that data; however, they can fail to generalize to similar data from other institutions and even other departments from the same institution. Therefore, learning from these technologies and building on them becomes difficult.          In order to improve NLP in EHRs, there is need for head-to-head comparison of approaches that can address a given task on the same data set. Shared-tasks provide one way of conducting systematic head-to- head comparisons. This proposal describes a series of shared-task challenges and conferences, spread over a five year period, that promote the development and evaluation of cutting edge clinical NLP systems by distributing de-identified EHRs to the broad research community, under data use agreements, so that:      *	the state-of-the-art in clinical NLP technologies can be identified and advanced,      *	a set of technologies that enable the use of the information contained in EHR narratives becomes available, and      *	the information from EHR narratives can be made more accessible, for example, for clinical and medical research.          The scientific activities supporting the organization of the shared-task challenges are sponsored in part by Informatics for Integrating Biology and the Bedside (i2b2), grant number U54-LM008748, PI: Kohane.          This proposal aims to organize a series of workshops, conference proceedings, and journal special issues that will accompany the shared-task challenges in order to disseminate the knowledge generated by the challenges.                  Public health relevance: this proposal will address two main challenges related to the use of clinical narratives for research: availability of clinical records for research and identification of the state of the art in clinical natural language processing (NLP) technologies so that we can push the state of the art forward and so that future work can build on the past. Progress in clinical NLP will improve access to electronic health records for research, and for clinical applications, benefiting healthcare and public health.",Challenges in Natural Language Processing for Clinical Narratives,8400218,R13LM011411,"['Access to Information', 'Address', 'Agreement', 'Biology', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Set', 'Development', 'Distributed Systems', 'Educational workshop', 'Electronic Health Record', 'Evaluation', 'Future', 'Goals', 'Gold', 'Grant', 'Hand', 'Healthcare', 'Improve Access', 'Informatics', 'Institution', 'Journals', 'Knowledge', 'Learning', 'Medical Research', 'Natural Language Processing', 'Privacy', 'Public Health', 'Publications', 'Records', 'Research', 'Rest', 'Series', 'System', 'Systems Development', 'Targeted Research', 'Technology', 'Work', 'clinical application', 'data sharing', 'head-to-head comparison', 'improved', 'indexing', 'practical application', 'public health relevance', 'symposium']",NLM,STATE UNIVERSITY OF NEW YORK AT ALBANY,R13,2012,20000,0.06801985663564217
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",8318253,R01LM010016,"['Adverse event', 'Affect', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Evaluation', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2012,327503,0.038551481675468305
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8333306,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,592423,0.03382327740148679
"Annotation, development and evaluation for clinical information extraction    DESCRIPTION (provided by applicant): Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible.      PUBLIC HEALTH RELEVANCE: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.           Project narrative: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.","Annotation, development and evaluation for clinical information extraction",8288078,R01GM090187,"['Address', 'Algorithms', 'Automated Annotation', 'Clinical', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'Consensus', 'Country', 'Data Set', 'Development', 'Disease', 'Evaluation', 'Goals', 'Gold', 'Guidelines', 'Individual', 'Judgment', 'Knowledge', 'Linguistics', 'Manuals', 'Medical Records', 'Methodology', 'Methods', 'Metric', 'Natural Language Processing', 'Performance', 'Reliance', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Signs and Symptoms', 'System', 'Technology', 'Terminology', 'Text', 'Training', 'Translational Research', 'Translations', 'base', 'clinical care', 'cost', 'design', 'flexibility', 'innovation', 'knowledge translation', 'phrases', 'prevent', 'public health relevance', 'research clinical testing', 'research study', 'tool']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,663130,0.09632777001042818
"POET-2: High-performance computing for advanced clinical narrative preprocessing    DESCRIPTION (provided by applicant):       This project focuses on clinical natural language processing (cNLP), a field of emerging importance in informatics. Starting with the Linguistic String Project's Medical Language Processor (New York University) in the 1970s, researchers have made steady gains in cNLP through empirical studies and by building sophisticated high-level cNLP software applications (e.g., Columbia's MedLEE). There are no fewer than four scientific conferences devoted exclusively to biomedical/clinical NLP. The cNLP literature has been growing over the past decade, and this will gain momentum as more clinical text repositories are released, such as the MIMIC II and University of Pittsburgh BLU Lab corpora.       However, sustained success in the field of cNLP is hampered by the reality that clinical texts have a far more noise than do texts traditionally studied in NLP, such as newswire articles, biomedical abstracts, and discharge summaries. Noise in this context is defined by the parseability characteristics of the language and the linguistic structures that appear in text. Clinical texts come in a striking variety of note types, with the best studied types being discharge summaries, radiology reports, and pathology reports. These note types share an important feature: they are written to communicate care issues between healthcare providers and hence typically are well-composed, well-edited, and often are dictated. But the vast majority of notes in the electronic health record are written primarily to document care issues. They communicate as well, of course, but much less care is used in their creation than with discharge summaries and reports. As a result they are often ungrammatical; are composed of short, telegraphic phrases; are replete with misspellings and shorthand (e.g., abbreviations); are ill-formatted with templates and liberal use of white space; and are embedded with ""non-prose"" (e.g., strings of laboratory values). All of these sources of noise complicate otherwise straightforward NLP tasks like tokenization, sentence segmentation, and ultimately information extraction itself.       We propose a systematic study of ways to increase the signal-to-noise ratio in clinical narratives to improve cNLP. This work extends our preliminary research (under the POET project) and has the following aims:        o Develop and implement a suite of parseability improvement tools designed for all clinical note types from multiple healthcare institutions.     o Evaluate the empirical and the functional success of the parseability improvement tools.     o Design and implement a HIPAA-compliant UlMA-based pipeline cNLP framework for use in a typical high-performance, multi-processor computing environment.              Project Narrative We can see in the multi-billion dollar investment in electronic health records (EHRs) by the ARRA that mining clinical data electronically will continue to be essential to informatics research. Most data in the EHR resides as unstructured text, and POET2 provides a means to unlock that data through combining a new, HIPAA- complaint high-performance computing architecture with sophisticated text preprocessing.",POET-2: High-performance computing for advanced clinical narrative preprocessing,8326648,R01LM010981,"['Abbreviations', 'Active Learning', 'Address', 'Architecture', 'Area', 'Authorization documentation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Data', 'Computer software', 'Data', 'Electronic Health Record', 'Electronics', 'Employee Strikes', 'Ensure', 'Environment', 'Evaluation', 'Face', 'Gold', 'Growth', 'Health Care Reform', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'High Performance Computing', 'Informatics', 'Inpatients', 'Institution', 'Institutional Review Boards', 'Investments', 'Laboratories', 'Language', 'Linguistics', 'Literature', 'Maps', 'Medical', 'Mining', 'Modeling', 'Natural Language Processing', 'New York', 'Noise', 'Occupations', 'Outpatients', 'Paper', 'Pathology', 'Pathology Report', 'Patients', 'Performance', 'Proliferating', 'Publishing', 'Radiology Specialty', 'Records', 'Report (document)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Support', 'Resolution', 'Series', 'Shorthand', 'Signal Transduction', 'Source', 'Structure', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'Voting', 'Work', 'Writing', 'abstracting', 'base', 'cluster computing', 'data mining', 'design', 'improved', 'meetings', 'novel', 'phrases', 'pressure', 'repaired', 'repository', 'research study', 'success', 'symposium', 'tool', 'web services']",NLM,UNIVERSITY OF UTAH,R01,2012,318393,0.030413375966211624
"Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach  Abstract The long-term objective of this research is to increase the clinical trial enrollment of US patients via a semi- automated, Natural Language Processing (NLP) based, interactive and patient-centered informatics application. The study design is prospective observational study. Scope is limited to cancer patients. There are three specific aims for this project. The first aim is to identify concepts that overlap between the electronic medical record's (EMR) clinical notes and the free text of clinical trial announcements. The PI will use the concepts to develop mapping frames that connect concepts in the text of trial announcements to those found in clinical notes in the medical record. When he has the mapping frames he will build the NLP module for the application. In the software development work he will utilize as many publicly available software components as possible. He will experiment with UIMA, GATE, MetaMap, Stanford Parser, NegEx algorithm and others. The PI will develop the tool around the National Library of Medicine's Unified Medical Language System knowledgebase. He will use Java for programming. The second aim is to create an algorithm that automatically generates questions to request information directly from the patient if the information is not available or accessible in the records. The third aim is to evaluate the in-vitro, laboratory performance of the application. For performance evaluation purposes the PI will recruit cancer care specialists to generate the gold standard lists of eligible clinical trials for study patients. He will publicly release the developed code at the end of the grant period. This K99/R00 project will serve the foundation for future R01 grant applications. The PI is fully committed to become faculty in the Clinical Research Informatics domain with a specialization in biomedical NLP. The support of the K99/R00 grant will enable him to acquire substantial formal training in Computational Linguistics while contributing to the body of knowledge of the Clinical Research Informatics field. The five-year grant support will ensure success in his endeavor. The proposed work is highly significant because the dismal clinical trial accrual rates (2-4 % nationally) hampers timely development of new drugs. In addition, studies show that physicians have statistically significant bias against elderly and minority patients to invite participation in clinical trials. The proposed project is synergistic with physician-centered efforts but the goal is to provide individualized, EMR based clinical trial recommendations directly to the patients. The results of this research will empower the patients and elevate their role in the decision making process.  Relevance The long-term objective of this research is to increase the clinical trial enrollment of US patients via a semi- automated, Natural Language Processing (NLP) based, interactive and patient-centered informatics application. The proposed work is highly significant because the dismal clinical trial accrual rates (2-4 % nationally) hampers timely development of new drugs. In addition, studies show that physicians have statistically significant bias against elderly and minority patients to invite participation in clinical trials. The proposed project is synergistic with physician-centered efforts but the goal is to provide individualized, electronic medical record based clinical trial recommendations directly to the patients. The results of this research will empower patients and elevate their role in the decision making process.",Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach,8331381,R00LM010227,"['Adult', 'Age', 'Algorithms', 'Applications Grants', 'Biomedical Research', 'Cancer Patient', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical trial protocol document', 'Code', 'Commit', 'Complement', 'Computer software', 'Computerized Medical Record', 'Decision Making', 'Development', 'Elderly', 'Elements', 'Eligibility Determination', 'Enrollment', 'Ensure', 'Equation', 'Evaluation', 'Faculty', 'Foundations', 'Future', 'Goals', 'Gold', 'Grant', 'In Vitro', 'Informatics', 'Java', 'Knowledge', 'Laboratories', 'Linguistics', 'Malignant Neoplasms', 'Maps', 'Medical Records', 'Medicine', 'Methods', 'Minority', 'Modification', 'Natural Language Processing', 'Newly Diagnosed', 'Observational Study', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Protocols documentation', 'Public Health Informatics', 'Publishing', 'Recommendation', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Role', 'Screening for cancer', 'Screening procedure', 'Specialist', 'Surgeon', 'Text', 'Training', 'Unified Medical Language System', 'United States National Library of Medicine', 'Work', 'abstracting', 'base', 'cancer care', 'empowered', 'ethnic minority population', 'information organization', 'knowledge base', 'novel', 'older patient', 'patient oriented', 'programs', 'prospective', 'research study', 'software development', 'success', 'tool']",NLM,CINCINNATI CHILDRENS HOSP MED CTR,R00,2012,238944,0.013694330037076874
"Integration of an NLP-based application to support medication management     DESCRIPTION (provided by applicant): An accurate and complete medication list on a patient's electronic health record (EHR) is critical to prevent prescribing and administration errors. Stage 1 of Meaningful Use requires certified EHRs to be capable of providing a user with the ability to perform medication reconciliation. However, most previous studies have taken place in the inpatient setting, while medication reconciliation in the outpatient setting is importnt and challenging. In addition, clinical notes contain critical medication information that also need to be reconciled. Our goal of this study is to develop novel methods and a system using natural language processing (NLP) and other technologies to facilitate the medication reconciliation process in the ambulatory setting. Our specific aims are to : 1) identify the requirements, use cases, work flow issues, barriers to and facilitators of using clinical notes and a NLP-based system in the medication reconciliation process; 2) design a generic system architecture and an application that integrates an NLP system and a web-based user interface within an existing medication reconciliation system; 3) pilot this study in two primary care clinics and measure the utilization, usability, performance and feasibility of the proposed methods and the tool; and 4) distribute our methods and the tool and to make them widely available to other researchers and healthcare institutions for non-commercial use.        PUBLIC HEALTH RELEVANCE: An accurate and complete medication list on a patient's electronic health record (EHR) is critical to prevent prescribing and administration errors. In thi study, we will develop novel methods and a tool using natural language processing and other technologies to facilitate the medication reconciliation process. We will implement the system and evaluate our approach in the outpatient setting.              An accurate and complete medication list on a patient's electronic health record (EHR) is critical to prevent prescribing and administration errors. In thi study, we will develop novel methods and a tool using natural language processing and other technologies to facilitate the medication reconciliation process. We will implement the system and evaluate our approach in the outpatient setting.            ",Integration of an NLP-based application to support medication management,8354008,R21HS021544,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R21,2012,149342,0.0247767970872995
"Real-time Disambiguation of Abbreviations in Clinical Notes    DESCRIPTION (provided by applicant): A key prerequisite for high-quality healthcare delivery is effective communication within and across healthcare settings. However, communication can be hampered by the pervasive use of abbreviations in clinical notes. Clinicians use abbreviations to save time during documentation. While abbreviations may seem unambiguous to their authors, they often cause confusion to other readers, including healthcare providers, patients, and natural language processing (NLP) systems attempting to extract clinical terms from text. While the understanding that abbreviations can cause errors is widespread, few have deployed pragmatic solutions for this important problem. The proposed project will develop, evaluate, and share a systematic approach to Clinical Abbreviation Recognition and Disambiguation (CARD), and in doing so substantially aims to benefit existing NLP systems and to improve computer-based documentation systems by reducing ambiguities in electronic records in real-time. The study includes the following five Specific Aims: 1) Develop automated methods to detect abbreviations and their senses from clinical text corpora and build a comprehensive knowledge base of clinical abbreviations; 2) Develop and evaluate three automated word sense disambiguation (WSD) classifiers, and establish methods to combine those classifiers to maximize both their performance and coverage; 3) Develop the CARD system, and demonstrate its effectiveness by integrating it with two established NLP systems (MedLEE and KnowledgeMap); 4) Integrate CARD with an institutional clinical documentation system (Vanderbilt's StarNotes) and evaluate its ability to expand abbreviations in real-time as clinicians generate records; 5) Distribute the CARD knowledge base and software for non-commercial uses.              Project Narrative Abbreviations are widely used throughout all types of clinical documents and they cause confusion to both healthcare providers and patients and limit effective communications within and across care settings. This proposed study will develop informatics methods to automatically detect abbreviations and their possible meanings from large clinical text and to disambiguate abbreviations that have multiple meanings. We will also integrate those methods with clinical documentation systems so that abbreviations will be expanded in real-time when physicians entering clinical notes, thus to improve the quality of health records.",Real-time Disambiguation of Abbreviations in Clinical Notes,8589822,R01LM010681,[' '],NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2012,237877,0.05220306626605401
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453           PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,8319670,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2012,352514,-0.021726057007146297
"Near Miss Narratives from the Fire Service: A Bayesian Analysis    DESCRIPTION (provided by applicant): This study will analyze the narrative text fields in all National Fire Fighter Near-Miss Reporting System (NFFNMRS) reports submitted since the system was created in 2005 (3,695 reports as of September 30, 2009). Near miss reporting systems have made major contributions to safety in such industries as aviation, nuclear power, petrochemical processing, steel production, and military operations, because the same patterns of causes of failure and their relations precede both adverse events and near misses. However, firefighters and researchers lack a scientific system to fully analyze the near miss data collected each year. This innovative effort will advance knowledge in firefighter safety by applying novel Bayesian methods of analysis to the narrative text fields of a new data source that has not yet been rigorously investigated. The proposal has 3 aims:  I. to use recently developed auto coding methods to characterize firefighter near miss narratives and classify these narratives into mechanisms of risk/injury. This analysis will apply the International Classification of External Cause of Injuries (ICECI) using Bayesian machine learning techniques to identify the various mechanisms captured in the near miss narratives and their relative prevalence.  II. To identify the correlation between each mechanism of risk/injury and each of the ""Contributing Factors"" listed on the NFFNMRS reporting form. The results will reveal any patterns and trends in the distribution of the contributing factors among the mechanisms, creating a deeper understanding of near miss circumstances, as well as a basis for improving the quality of future near miss data collection.  III. To use manual coding to identify actual injury incidents contained within a random sample of 1,000 near miss narratives and correlate these injuries with the ""Loss Potential"" categories on the NFFNMRS reporting form. The results will demonstrate how actual injuries are distributed within the reporting form's ""Loss Potential"" categories. This proposed study of the near miss narrative text in combination with coded data has the potential to reveal new insights that can strengthen firefighter safety through primary prevention. This study addresses a major gap in firefighter safety knowledge, i.e. the insufficient understanding of near miss events, and will have a high impact on efforts to improve the occupational health and safety of firefighters.         This study will analyze the narrative text fields in all National Fire Fighter Near-Miss Reporting System (NFFNMRS) reports submitted since the system was created in 2005 (3,695 reports as of September 30, 2009). Near miss reporting systems have made major contributions to safety in such industries as aviation, nuclear power, petrochemical processing, steel production, and military operations, because the same patterns of causes of failure and their relations precede both adverse events and near misses. However, firefighters and researchers lack a scientific system to fully analyze the near miss data collected each year. This innovative effort will advance knowledge in firefighter safety by applying novel Bayesian methods of analysis to the narrative text fields of a new data source, the NFFNMRS that has not yet been rigorously investigated. This proposed study of the near miss narrative text in combination with coded data has the potential to reveal new insights that can strengthen firefighter safety through primary prevention.         ",Near Miss Narratives from the Fire Service: A Bayesian Analysis,8325335,R03OH009984,[' '],NIOSH,DREXEL UNIVERSITY,R03,2012,76332,0.008031425518927241
"USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING    DESCRIPTION (provided by applicant): Understanding mechanisms of action is key to improving psychosocial interventions for cancer and other chronic disease conditions. In cancer, emotional expression has been identified as one possible mediator of the effect of psychosocial intervention on patient-reported outcomes. However, scientific evaluations of psychological mechanisms of adjustment to cancer and other chronic diseases are constrained by limitations associated with self-report measures. Because self-care resources, peer-to-peer networks, and more recent forms of psychosocial intervention are increasingly being delivered online, linguistic and behavioral data can be used to characterize internal coping processes, social interactions, and other manifest behaviors. Few tools are currently available for harnessing text as a potential data source, and signal detection indices of existing tools leave room for considerable improvement in these methodologies (Bantum & Owen, 2009). In the present study, natural language processing and other tools of computational linguistics will be used to develop a machine-learning classifier to identify emotional expression in electronic text data. The aims of the study are: 1) to annotate a large text corpus from cancer survivors using an objective and reliable emotion-coding procedure, 2) to incorporate linguistic and psychological features into a machine-learning classification method and identify which of these features are most strongly associated with codes assigned by trained human raters, and 3) to develop combined psychological and natural language processing (NLP) methods for identifying linguistic markers of emotional coping behaviors. To accomplish these aims, a comprehensive corpus of emotionally-laden cancer communications will be developed from 5 existing linguistic datasets. Five raters will be selected and undergo a rigorous training procedure for coding emotional expression using an emotion-coding system previously developed by the research. Coding will take place using an Internet-based coding interface that will allow the investigators to continuously monitor inter-rater reliability. Simultaneous with the coding process, the investigators will link the electronic text data with key linguistic and psychological features, including Linguistic Inquiry and Word Count (LIWC), Affective Norms for English Words (ANEW), WordNet, part of speech tags, patterns of capitalization and punctuation, emoticons, and textual context. A machine-learning classifier, using tools of natural language processing, will then be applied to the text/feature data and validated against human-rated emotion codes. The long-term objective of this research is to advance a methodology for objectively identifying coping behavior, particularly emotional expression, in order to supplement self-report measures and improve scientific understanding of adjustment to chronic disease, trauma, or other psychological conditions. This work is essential for identifying mechanisms of action in psychosocial interventions for cancer survivors and others and has significance for the fields of medicine, psychology, computational linguistics, and artificial intelligence.      PUBLIC HEALTH RELEVANCE: Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.           Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.",USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING,8120220,R21CA143642,"['Affective', 'Artificial Intelligence', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Cancer Intervention', 'Cancer Survivor', 'Categories', 'Characteristics', 'Chronic Disease', 'Classification', 'Code', 'Cognitive', 'Communication', 'Coping Behavior', 'Coping Skills', 'Data', 'Data Set', 'Data Sources', 'Decision Making', 'Detection', 'Distress', 'Educational process of instructing', 'Effectiveness of Interventions', 'Electronics', 'Emotional', 'Emotions', 'Goals', 'Health', 'Health behavior', 'Heart Rate', 'Human', 'Hydrocortisone', 'Individual', 'Internet', 'Intervention', 'Intervention Studies', 'Left', 'Life', 'Linguistics', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediator of activation protein', 'Medicine', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Pattern', 'Personal Satisfaction', 'Physiological', 'Predictive Value', 'Problem Solving', 'Procedures', 'Process', 'Psychological adjustment', 'Psychology', 'Publishing', 'Quality of life', 'Recommendation', 'Recording of previous events', 'Recovery', 'Regulation', 'Relative (related person)', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Sampling', 'Scientific Evaluation', 'Screening procedure', 'Self Care', 'Signal Transduction', 'Social Interaction', 'Social support', 'Specificity', 'Speech', 'Survey Methodology', 'Sympathetic Nervous System', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trauma', 'Treatment/Psychosocial Effects', 'Work', 'anticancer research', 'base', 'behavior observation', 'computerized', 'computerized tools', 'coping', 'effective intervention', 'emotional experience', 'experience', 'improved', 'indexing', 'innovation', 'lexical', 'natural language', 'peer', 'programs', 'psychologic', 'psychosocial', 'public health relevance', 'showing emotion', 'skills', 'skills training', 'symptom management', 'tool']",NCI,LOMA LINDA UNIVERSITY,R21,2011,161797,0.009060417684531686
"Real-time Disambiguation of Abbreviations in Clinical Notes    DESCRIPTION (provided by applicant): A key prerequisite for high-quality healthcare delivery is effective communication within and across healthcare settings. However, communication can be hampered by the pervasive use of abbreviations in clinical notes. Clinicians use abbreviations to save time during documentation. While abbreviations may seem unambiguous to their authors, they often cause confusion to other readers, including healthcare providers, patients, and natural language processing (NLP) systems attempting to extract clinical terms from text. While the understanding that abbreviations can cause errors is widespread, few have deployed pragmatic solutions for this important problem. The proposed project will develop, evaluate, and share a systematic approach to Clinical Abbreviation Recognition and Disambiguation (CARD), and in doing so substantially aims to benefit existing NLP systems and to improve computer-based documentation systems by reducing ambiguities in electronic records in real-time. The study includes the following five Specific Aims: 1) Develop automated methods to detect abbreviations and their senses from clinical text corpora and build a comprehensive knowledge base of clinical abbreviations; 2) Develop and evaluate three automated word sense disambiguation (WSD) classifiers, and establish methods to combine those classifiers to maximize both their performance and coverage; 3) Develop the CARD system, and demonstrate its effectiveness by integrating it with two established NLP systems (MedLEE and KnowledgeMap); 4) Integrate CARD with an institutional clinical documentation system (Vanderbilt's StarNotes) and evaluate its ability to expand abbreviations in real-time as clinicians generate records; 5) Distribute the CARD knowledge base and software for non-commercial uses.              Project Narrative Abbreviations are widely used throughout all types of clinical documents and they cause confusion to both healthcare providers and patients and limit effective communications within and across care settings. This proposed study will develop informatics methods to automatically detect abbreviations and their possible meanings from large clinical text and to disambiguate abbreviations that have multiple meanings. We will also integrate those methods with clinical documentation systems so that abbreviations will be expanded in real-time when physicians entering clinical notes, thus to improve the quality of health records.",Real-time Disambiguation of Abbreviations in Clinical Notes,8077875,R01LM010681,"['Abbreviations', 'Algorithms', 'Architecture', 'Caring', 'Cessation of life', 'Clinical', 'Communication', 'Computer Systems', 'Computer software', 'Computers', 'Confusion', 'Coronary Arteriosclerosis', 'Databases', 'Detection', 'Disease', 'Documentation', 'Effectiveness', 'Electronics', 'Equipment and supply inventories', 'Frequencies', 'Health Personnel', 'Healthcare', 'Individual', 'Informatics', 'Joints', 'Machine Learning', 'Manuals', 'Medical Records', 'Methods', 'Names', 'Natural Language Processing', 'Nitroglycerin', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Provider', 'Reader', 'Records', 'Serious Adverse Event', 'Solutions', 'System', 'Technology', 'Text', 'Time', 'Work', 'Writing', 'acronyms', 'base', 'health care delivery', 'health record', 'improved', 'innovation', 'insight', 'knowledge base', 'novel', 'phrases', 'satisfaction']",NLM,VANDERBILT UNIVERSITY,R01,2011,374000,0.05220306626605401
"Adapting Natural Language Processing Tools for Biosurveillance    DESCRIPTION (provided by applicant):       Early detection of disease outbreaks can decrease patient morbidity and mortality and minimize the spread of diseases. Early detection requires accurate classification of patient symptoms early in the course of their illness. One approach is biosurveillance, in which electronic symptom data are captured early in the course of illness, and analyzed for signals that might indicate an outbreak requiring investigation and response by the public health system. Emergency department (ED) patient records are particularly useful for biosurveillance, given their timely, electronic availability. ED data elements used in surveillance systems include the chief complaint (a brief description of the patient's primary symptom(s)), and triage nurses' note (also known as history of present illness).The chief complaint is the most widely used ED data element, because it is recorded electronically by the majority of EDs. One study showed that adding triage notes increased the sensitivity of biosurveillance case detection. The increased sensitivity is because the triage note increases the amount of data available: instead of one symptom in a chief complaint (e.g., fever), triage notes may contain multiple symptoms (e.g., ""fever, cough & shortness of breath for 12 hours""). Surveillance efforts are hampered, however, by the wide variability of free text data in ED chief complaints and triage notes. They often include misspellings, abbreviations, acronyms and other lexical and semantic variants that are difficult to group into symptom clusters (e.g., fever, temp 104, fvr, febrile). Tools are needed to address the lexical and semantic variation in symptom terms in ED data in order to improve biosurveillance. Natural language processing tools have been shown to facilitate concept extraction from more structured clinical data such as radiology reports, but there has been limited application of these techniques to free text ED triage notes. The project team developed the Emergency Medical Text Processor (EMT-P) to preprocess the chief complaint. EMT-P cleans and normalizes brief chief complaint entries and then extracts standardized concepts, but it is not sufficient in its current state to preprocess longer, more complex text passages such as triage notes. This proposed project will further strengthen biosurveillance by adapting EMT-P and other statistical and classical natural language processing tools to develop a system that extracts concepts from triage notes for biosurveillance.           Project Narrative Relevance: The public health system is responsible for monitoring large amounts of timely, electronic health data and needs more sophisticated tools to faciliate detection of, and response to, emerging infectious diseases and potential bioterrorism threats. The proposed project addresses this need by developing a system to extract relevant information from emergency department records.",Adapting Natural Language Processing Tools for Biosurveillance,8144459,G08LM009787,"['Abbreviations', 'Accident and Emergency department', 'Acute', 'Address', 'American', 'Avian Influenza', 'Bioterrorism', 'Bird Flu vaccine', 'Breathing', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Coughing', 'Country', 'Data', 'Data Element', 'Data Quality', 'Detection', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Early Diagnosis', 'Electronic Health Record', 'Electronics', 'Emergency Medicine', 'Emergency Situation', 'Emerging Communicable Diseases', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Fever', 'Genetic Transcription', 'Goals', 'Gold', 'Health', 'Health Services', 'Health system', 'Hour', 'Intervention', 'Investigation', 'Manuals', 'Measures', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'North Carolina', 'Nurses', 'Patients', 'Performance', 'Pertussis', 'Physicians', 'Predictive Value', 'Process', 'Public Health', 'Radiology Specialty', 'Recording of previous events', 'Records', 'Reporting', 'Sampling', 'Screening procedure', 'Semantics', 'Sensitivity and Specificity', 'Shortness of Breath', 'Signal Transduction', 'Smallpox', 'Structure', 'Symptoms', 'System', 'Techniques', 'Temperature', 'Text', 'Translating', 'Triage', 'Variant', 'acronyms', 'base', 'experience', 'improved', 'lexical', 'mortality', 'pandemic disease', 'population based', 'prototype', 'research to practice', 'response', 'satisfaction', 'syntax', 'tool']",NLM,UNIV OF NORTH CAROLINA CHAPEL HILL,G08,2011,147161,0.054751237238870425
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",8105502,R01LM010016,"['Adverse event', 'Affect', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Evaluation', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2011,333575,0.038551481675468305
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,8055880,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2011,328942,-0.00900700261742695
"Annotation, development and evaluation for clinical information extraction    DESCRIPTION (provided by applicant): Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible.      PUBLIC HEALTH RELEVANCE: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.           Project narrative: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.","Annotation, development and evaluation for clinical information extraction",8133360,R01GM090187,"['Address', 'Algorithms', 'Automated Annotation', 'Clinical', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'Consensus', 'Country', 'Data Set', 'Development', 'Disease', 'Evaluation', 'Goals', 'Gold', 'Guidelines', 'Individual', 'Judgment', 'Knowledge', 'Linguistics', 'Manuals', 'Medical Records', 'Methodology', 'Methods', 'Metric', 'Natural Language Processing', 'Performance', 'Reliance', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Signs and Symptoms', 'System', 'Technology', 'Terminology', 'Text', 'Training', 'Translational Research', 'Translations', 'base', 'clinical care', 'cost', 'design', 'flexibility', 'innovation', 'knowledge translation', 'phrases', 'prevent', 'public health relevance', 'research clinical testing', 'research study', 'tool']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,664617,0.09632777001042818
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8022026,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,591195,0.03382327740148679
"POET-2: High-performance computing for advanced clinical narrative preprocessing    DESCRIPTION (provided by applicant):       This project focuses on clinical natural language processing (cNLP), a field of emerging importance in informatics. Starting with the Linguistic String Project's Medical Language Processor (New York University) in the 1970s, researchers have made steady gains in cNLP through empirical studies and by building sophisticated high-level cNLP software applications (e.g., Columbia's MedLEE). There are no fewer than four scientific conferences devoted exclusively to biomedical/clinical NLP. The cNLP literature has been growing over the past decade, and this will gain momentum as more clinical text repositories are released, such as the MIMIC II and University of Pittsburgh BLU Lab corpora.       However, sustained success in the field of cNLP is hampered by the reality that clinical texts have a far more noise than do texts traditionally studied in NLP, such as newswire articles, biomedical abstracts, and discharge summaries. Noise in this context is defined by the parseability characteristics of the language and the linguistic structures that appear in text. Clinical texts come in a striking variety of note types, with the best studied types being discharge summaries, radiology reports, and pathology reports. These note types share an important feature: they are written to communicate care issues between healthcare providers and hence typically are well-composed, well-edited, and often are dictated. But the vast majority of notes in the electronic health record are written primarily to document care issues. They communicate as well, of course, but much less care is used in their creation than with discharge summaries and reports. As a result they are often ungrammatical; are composed of short, telegraphic phrases; are replete with misspellings and shorthand (e.g., abbreviations); are ill-formatted with templates and liberal use of white space; and are embedded with ""non-prose"" (e.g., strings of laboratory values). All of these sources of noise complicate otherwise straightforward NLP tasks like tokenization, sentence segmentation, and ultimately information extraction itself.       We propose a systematic study of ways to increase the signal-to-noise ratio in clinical narratives to improve cNLP. This work extends our preliminary research (under the POET project) and has the following aims:        o Develop and implement a suite of parseability improvement tools designed for all clinical note types from multiple healthcare institutions.     o Evaluate the empirical and the functional success of the parseability improvement tools.     o Design and implement a HIPAA-compliant UlMA-based pipeline cNLP framework for use in a typical high-performance, multi-processor computing environment.              Project Narrative We can see in the multi-billion dollar investment in electronic health records (EHRs) by the ARRA that mining clinical data electronically will continue to be essential to informatics research. Most data in the EHR resides as unstructured text, and POET2 provides a means to unlock that data through combining a new, HIPAA- complaint high-performance computing architecture with sophisticated text preprocessing.",POET-2: High-performance computing for advanced clinical narrative preprocessing,8182025,R01LM010981,"['Abbreviations', 'Active Learning', 'Address', 'Architecture', 'Area', 'Authorization documentation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Data', 'Computer software', 'Data', 'Electronic Health Record', 'Electronics', 'Employee Strikes', 'Ensure', 'Environment', 'Evaluation', 'Face', 'Gold', 'Growth', 'Health Care Reform', 'Health Insurance Portability and Accountability Act', 'Health Personnel', 'Healthcare', 'High Performance Computing', 'Informatics', 'Inpatients', 'Institution', 'Institutional Review Boards', 'Investments', 'Laboratories', 'Language', 'Linguistics', 'Literature', 'Maps', 'Medical', 'Mining', 'Modeling', 'Natural Language Processing', 'New York', 'Noise', 'Occupations', 'Outpatients', 'Paper', 'Pathology', 'Pathology Report', 'Patients', 'Performance', 'Proliferating', 'Publishing', 'Radiology Specialty', 'Records', 'Report (document)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Support', 'Resolution', 'Series', 'Shorthand', 'Signal Transduction', 'Source', 'Structure', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'Voting', 'Work', 'Writing', 'abstracting', 'base', 'cluster computing', 'data mining', 'design', 'improved', 'meetings', 'novel', 'phrases', 'pressure', 'repaired', 'repository', 'research study', 'success', 'symposium', 'tool', 'web services']",NLM,UNIVERSITY OF UTAH,R01,2011,325163,0.030413375966211624
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453           PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,8138590,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2011,356340,-0.021726057007146297
"Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach No abstract available  Relevance The long-term objective of this research is to increase the clinical trial enrollment of US patients via a semi- automated, Natural Language Processing (NLP) based, interactive and patient-centered informatics application. The proposed work is highly significant because the dismal clinical trial accrual rates (2-4 % nationally) hampers timely development of new drugs. In addition, studies show that physicians have statistically significant bias against elderly and minority patients to invite participation in clinical trials. The proposed project is synergistic with physician-centered efforts but the goal is to provide individualized, electronic medical record based clinical trial recommendations directly to the patients. The results of this research will empower patients and elevate their role in the decision making process.",Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach,8215715,R00LM010227,"['Adult', 'Age', 'Algorithms', 'Applications Grants', 'Biomedical Research', 'Cancer Patient', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical trial protocol document', 'Code', 'Commit', 'Complement', 'Computer software', 'Computerized Medical Record', 'Decision Making', 'Development', 'Elderly', 'Elements', 'Eligibility Determination', 'Enrollment', 'Ensure', 'Equation', 'Evaluation', 'Faculty', 'Foundations', 'Future', 'Goals', 'Gold', 'Grant', 'In Vitro', 'Informatics', 'Java', 'Knowledge', 'Laboratories', 'Linguistics', 'Malignant Neoplasms', 'Maps', 'Medical Records', 'Medicine', 'Methods', 'Minority', 'Modification', 'Natural Language Processing', 'Newly Diagnosed', 'Observational Study', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Protocols documentation', 'Public Health Informatics', 'Publishing', 'Recommendation', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Role', 'Screening for cancer', 'Screening procedure', 'Specialist', 'Surgeon', 'Text', 'Training', 'Unified Medical Language System', 'United States National Library of Medicine', 'Work', 'abstracting', 'base', 'cancer care', 'empowered', 'ethnic minority population', 'information organization', 'knowledge base', 'novel', 'older patient', 'patient oriented', 'programs', 'prospective', 'research study', 'software development', 'success', 'tool']",NLM,CINCINNATI CHILDRENS HOSP MED CTR,R00,2011,239040,0.031524762020850736
"Near Miss Narratives from the Fire Service: A Bayesian Analysis    DESCRIPTION (provided by applicant): This study will analyze the narrative text fields in all National Fire Fighter Near-Miss Reporting System (NFFNMRS) reports submitted since the system was created in 2005 (3,695 reports as of September 30, 2009). Near miss reporting systems have made major contributions to safety in such industries as aviation, nuclear power, petrochemical processing, steel production, and military operations, because the same patterns of causes of failure and their relations precede both adverse events and near misses. However, firefighters and researchers lack a scientific system to fully analyze the near miss data collected each year. This innovative effort will advance knowledge in firefighter safety by applying novel Bayesian methods of analysis to the narrative text fields of a new data source that has not yet been rigorously investigated. The proposal has 3 aims:  I. to use recently developed auto coding methods to characterize firefighter near miss narratives and classify these narratives into mechanisms of risk/injury. This analysis will apply the International Classification of External Cause of Injuries (ICECI) using Bayesian machine learning techniques to identify the various mechanisms captured in the near miss narratives and their relative prevalence.  II. To identify the correlation between each mechanism of risk/injury and each of the ""Contributing Factors"" listed on the NFFNMRS reporting form. The results will reveal any patterns and trends in the distribution of the contributing factors among the mechanisms, creating a deeper understanding of near miss circumstances, as well as a basis for improving the quality of future near miss data collection.  III. To use manual coding to identify actual injury incidents contained within a random sample of 1,000 near miss narratives and correlate these injuries with the ""Loss Potential"" categories on the NFFNMRS reporting form. The results will demonstrate how actual injuries are distributed within the reporting form's ""Loss Potential"" categories. This proposed study of the near miss narrative text in combination with coded data has the potential to reveal new insights that can strengthen firefighter safety through primary prevention. This study addresses a major gap in firefighter safety knowledge, i.e. the insufficient understanding of near miss events, and will have a high impact on efforts to improve the occupational health and safety of firefighters.      PUBLIC HEALTH RELEVANCE:  This study will analyze the narrative text fields in all National Fire Fighter Near-Miss Reporting System (NFFNMRS) reports submitted since the system was created in 2005 (3,695 reports as of September 30, 2009). Near miss reporting systems have made major contributions to safety in such industries as aviation, nuclear power, petrochemical processing, steel production, and military operations, because the same patterns of causes of failure and their relations precede both adverse events and near misses. However, firefighters and researchers lack a scientific system to fully analyze the near miss data collected each year. This innovative effort will advance knowledge in firefighter safety by applying novel Bayesian methods of analysis to the narrative text fields of a new data source, the NFFNMRS that has not yet been rigorously investigated. This proposed study of the near miss narrative text in combination with coded data has the potential to reveal new insights that can strengthen firefighter safety through primary prevention.            This study will analyze the narrative text fields in all National Fire Fighter Near-Miss Reporting System (NFFNMRS) reports submitted since the system was created in 2005 (3,695 reports as of September 30, 2009). Near miss reporting systems have made major contributions to safety in such industries as aviation, nuclear power, petrochemical processing, steel production, and military operations, because the same patterns of causes of failure and their relations precede both adverse events and near misses. However, firefighters and researchers lack a scientific system to fully analyze the near miss data collected each year. This innovative effort will advance knowledge in firefighter safety by applying novel Bayesian methods of analysis to the narrative text fields of a new data source, the NFFNMRS that has not yet been rigorously investigated. This proposed study of the near miss narrative text in combination with coded data has the potential to reveal new insights that can strengthen firefighter safety through primary prevention.         ",Near Miss Narratives from the Fire Service: A Bayesian Analysis,8206110,R03OH009984,[' '],NIOSH,DREXEL UNIVERSITY,R03,2011,74075,0.007919528245089916
"USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING    DESCRIPTION (provided by applicant): Understanding mechanisms of action is key to improving psychosocial interventions for cancer and other chronic disease conditions. In cancer, emotional expression has been identified as one possible mediator of the effect of psychosocial intervention on patient-reported outcomes. However, scientific evaluations of psychological mechanisms of adjustment to cancer and other chronic diseases are constrained by limitations associated with self-report measures. Because self-care resources, peer-to-peer networks, and more recent forms of psychosocial intervention are increasingly being delivered online, linguistic and behavioral data can be used to characterize internal coping processes, social interactions, and other manifest behaviors. Few tools are currently available for harnessing text as a potential data source, and signal detection indices of existing tools leave room for considerable improvement in these methodologies (Bantum & Owen, 2009). In the present study, natural language processing and other tools of computational linguistics will be used to develop a machine-learning classifier to identify emotional expression in electronic text data. The aims of the study are: 1) to annotate a large text corpus from cancer survivors using an objective and reliable emotion-coding procedure, 2) to incorporate linguistic and psychological features into a machine-learning classification method and identify which of these features are most strongly associated with codes assigned by trained human raters, and 3) to develop combined psychological and natural language processing (NLP) methods for identifying linguistic markers of emotional coping behaviors. To accomplish these aims, a comprehensive corpus of emotionally-laden cancer communications will be developed from 5 existing linguistic datasets. Five raters will be selected and undergo a rigorous training procedure for coding emotional expression using an emotion-coding system previously developed by the research. Coding will take place using an Internet-based coding interface that will allow the investigators to continuously monitor inter-rater reliability. Simultaneous with the coding process, the investigators will link the electronic text data with key linguistic and psychological features, including Linguistic Inquiry and Word Count (LIWC), Affective Norms for English Words (ANEW), WordNet, part of speech tags, patterns of capitalization and punctuation, emoticons, and textual context. A machine-learning classifier, using tools of natural language processing, will then be applied to the text/feature data and validated against human-rated emotion codes. The long-term objective of this research is to advance a methodology for objectively identifying coping behavior, particularly emotional expression, in order to supplement self-report measures and improve scientific understanding of adjustment to chronic disease, trauma, or other psychological conditions. This work is essential for identifying mechanisms of action in psychosocial interventions for cancer survivors and others and has significance for the fields of medicine, psychology, computational linguistics, and artificial intelligence.      PUBLIC HEALTH RELEVANCE: Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.           Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.",USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING,7991498,R21CA143642,"['Affective', 'Artificial Intelligence', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Cancer Intervention', 'Cancer Survivor', 'Categories', 'Characteristics', 'Chronic Disease', 'Classification', 'Code', 'Cognitive', 'Communication', 'Coping Behavior', 'Coping Skills', 'Data', 'Data Set', 'Data Sources', 'Decision Making', 'Detection', 'Distress', 'Educational process of instructing', 'Effectiveness of Interventions', 'Electronics', 'Emotional', 'Emotions', 'Goals', 'Health', 'Health behavior', 'Heart Rate', 'Human', 'Hydrocortisone', 'Individual', 'Internet', 'Intervention', 'Intervention Studies', 'Left', 'Life', 'Linguistics', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediator of activation protein', 'Medicine', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Pattern', 'Personal Satisfaction', 'Physiological', 'Predictive Value', 'Problem Solving', 'Procedures', 'Process', 'Psychological adjustment', 'Psychology', 'Publishing', 'Quality of life', 'Recommendation', 'Recording of previous events', 'Recovery', 'Regulation', 'Relative (related person)', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Sampling', 'Scientific Evaluation', 'Screening procedure', 'Self Care', 'Signal Transduction', 'Social Interaction', 'Social support', 'Specificity', 'Speech', 'Survey Methodology', 'Sympathetic Nervous System', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trauma', 'Treatment/Psychosocial Effects', 'Work', 'anticancer research', 'base', 'behavior observation', 'computerized', 'computerized tools', 'coping', 'effective intervention', 'emotional experience', 'experience', 'improved', 'indexing', 'innovation', 'lexical', 'natural language', 'peer', 'programs', 'psychologic', 'psychosocial', 'public health relevance', 'showing emotion', 'skills', 'skills training', 'symptom management', 'tool']",NCI,LOMA LINDA UNIVERSITY,R21,2010,223207,0.009060417684531686
"A Hybrid General Natural Language Processing Architecture    DESCRIPTION (provided by applicant): Electronic medical records and exchanges offer new opportunities for the analysis of population health data; however, new methods in natural language processing (NLP) must first be developed to structure and codify these records, since most medical data is in the form of free text which cannot be stored and manipulated by computers. Once this is accomplished, population health data can be analyzed which will lead to better treatment guidelines, targeted drug therapy, and more cost effective care. Logical Semantics, Inc. (LSI) proposes to develop new statistical NLP methods for analyzing large scale medical domains. These methods will leverage LSI's semantic annotation technology, which has created the largest semantically annotated clinical corpus in the world. LSI's goal is to semantically index large medical record repositories accurately against propositions arranged in knowledge ontologies and make these indices available for text mining applications. The phase one research is focused on three specific aims that will lead to breakthroughs in the science of NLP: (1) Develop new statistical NLP algorithms employing a large semantically annotated medical corpus, (2) Semi-automate knowledge ontology generation, and (3) Develop and combine rule based with statistical NLP algorithms to create a superior hybrid NLP system. The achievement of these aims will result in computer systems that can extract the meaning from free text medical records so researchers, policy makers, and clinicians can use health analytics to improve healthcare.      PUBLIC HEALTH RELEVANCE: Natural language processing (NLP) has been successful in extracting specific findings and diagnoses from free text medical records. However, for NLP to be useful in health analytics, methods must be devised to capture most of the findings in a medical record. Logical Semantics, Inc. (LSI) proposes to build new statistical algorithms that can scale against the numerous complex findings in medical reports. LSI will leverage its advanced semantic annotation technology which employs corpus linguistics and sentential logic to build these new algorithms. The goal is to abstract over 80% of a free text records into computer readable form so that researchers can develop new treatment guidelines, improve decision support, and deliver more cost effective care.           Project Narrative Natural language processing (NLP) has been successful in extracting specific findings and diagnoses from free text medical records. However, for NLP to be useful in health analytics, methods must be devised to capture most of the findings in a medical record. Logical Semantics, Inc. (LSI) proposes to build new statistical algorithms that can scale against the numerous complex findings in medical reports. LSI will leverage its advanced semantic annotation technology which employs corpus linguistics and sentential logic to build these new algorithms. The goal is to abstract over 80% of a free text records into computer readable form so that researchers can develop new treatment guidelines, improve decision support, and deliver more cost effective care.",A Hybrid General Natural Language Processing Architecture,7996937,R43LM010846,"['Achievement', 'Address', 'Algorithms', 'Architecture', 'Businesses', 'Caring', 'Clinical', 'Communities', 'Complex', 'Computer Systems', 'Computerized Medical Record', 'Computers', 'Data', 'Diagnosis', 'Discipline', 'Generations', 'Goals', 'Guidelines', 'Health', 'Healthcare', 'Hybrids', 'Knowledge', 'Lead', 'Legal patent', 'Linguistics', 'Logic', 'Measures', 'Medical', 'Medical Records', 'Methods', 'Metric', 'Mining', 'Natural Language Processing', 'Ontology', 'Pattern', 'Pharmacotherapy', 'Phase', 'Policy Maker', 'Process', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Services', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Work', 'abstracting', 'base', 'cost effective', 'improved', 'indexing', 'knowledge base', 'operation', 'phrases', 'population health', 'public health relevance', 'repository', 'stem', 'success', 'text searching', 'tool']",NLM,"LOGICAL SEMANTICS, INC.",R43,2010,148180,0.0526204896323171
"Real-time Disambiguation of Abbreviations in Clinical Notes    DESCRIPTION (provided by applicant): A key prerequisite for high-quality healthcare delivery is effective communication within and across healthcare settings. However, communication can be hampered by the pervasive use of abbreviations in clinical notes. Clinicians use abbreviations to save time during documentation. While abbreviations may seem unambiguous to their authors, they often cause confusion to other readers, including healthcare providers, patients, and natural language processing (NLP) systems attempting to extract clinical terms from text. While the understanding that abbreviations can cause errors is widespread, few have deployed pragmatic solutions for this important problem. The proposed project will develop, evaluate, and share a systematic approach to Clinical Abbreviation Recognition and Disambiguation (CARD), and in doing so substantially aims to benefit existing NLP systems and to improve computer-based documentation systems by reducing ambiguities in electronic records in real-time. The study includes the following five Specific Aims: 1) Develop automated methods to detect abbreviations and their senses from clinical text corpora and build a comprehensive knowledge base of clinical abbreviations; 2) Develop and evaluate three automated word sense disambiguation (WSD) classifiers, and establish methods to combine those classifiers to maximize both their performance and coverage; 3) Develop the CARD system, and demonstrate its effectiveness by integrating it with two established NLP systems (MedLEE and KnowledgeMap); 4) Integrate CARD with an institutional clinical documentation system (Vanderbilt's StarNotes) and evaluate its ability to expand abbreviations in real-time as clinicians generate records; 5) Distribute the CARD knowledge base and software for non-commercial uses.              Project Narrative Abbreviations are widely used throughout all types of clinical documents and they cause confusion to both healthcare providers and patients and limit effective communications within and across care settings. This proposed study will develop informatics methods to automatically detect abbreviations and their possible meanings from large clinical text and to disambiguate abbreviations that have multiple meanings. We will also integrate those methods with clinical documentation systems so that abbreviations will be expanded in real-time when physicians entering clinical notes, thus to improve the quality of health records.",Real-time Disambiguation of Abbreviations in Clinical Notes,7866149,R01LM010681,"['Abbreviations', 'Algorithms', 'Architecture', 'Caring', 'Cessation of life', 'Clinical', 'Communication', 'Computer Systems', 'Computer software', 'Computers', 'Confusion', 'Coronary Arteriosclerosis', 'Databases', 'Detection', 'Disease', 'Documentation', 'Effectiveness', 'Electronics', 'Equipment and supply inventories', 'Frequencies', 'Health Personnel', 'Healthcare', 'Individual', 'Informatics', 'Joints', 'Machine Learning', 'Manuals', 'Medical Records', 'Methods', 'Names', 'Natural Language Processing', 'Nitroglycerin', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Provider', 'Reader', 'Records', 'Serious Adverse Event', 'Solutions', 'System', 'Technology', 'Text', 'Time', 'Work', 'Writing', 'acronyms', 'base', 'health care delivery', 'health record', 'improved', 'innovation', 'insight', 'knowledge base', 'novel', 'phrases', 'satisfaction']",NLM,VANDERBILT UNIVERSITY,R01,2010,387500,0.05220306626605401
"Natural Language Processing to Study Epidemiology of Statin Side Effects    DESCRIPTION (provided by applicant): This application addresses broad Challenge Area (10) Information Technology for Processing Health Care Data and specific Challenge Topic 10-LM-101: Informatics for post-marketing surveillance. The overall goal of this study is to develop a generalizable framework for studying medication side effects recorded in narrative medical documents. We will implement and test this system on the example of epidemiologic characterization of side effects of HMG-CoA reductase inhibitors (a.k.a. statins). Statins are the most commonly used class of medications for treatment of hypercholesterolemia in the U.S. In randomized clinical trials statins are associated only with a slight increase in adverse reactions and no increase in discontinuation of treatment compared to placebo. However, in clinical practice the rates of side effects and discontinuation appear significantly higher and represent a major barrier to a critical, potentially lifesaving therapy. For example, myalgias are reported to be relatively rare in clinical trials but are thought to be more common in clinical practice. Additionally, a number of other statin-associated complaints reported anecdotally but not well elucidated in clinical trials include depression, irritability, and memory loss among others. Most of these have been poorly epidemiologically characterized and their prevalence and risk factors remain unknown. Structured electronic medical record (EMR) and administrative data have been used to study medication side effects. However, structured data have important limitations. They may not contain temporal or causative information necessary to link particular problems to medications and may not be sufficiently granular to identify specific adverse reactions. Narrative EMR data, such as provider notes, can provide documentation of causative links between medication and adverse events at high levels of granularity. Natural language processing (NLP) is an emerging technology that enables computational abstraction of information from narrative medical documents. In prior work we have successfully applied natural language processing to abstract medication information from narrative provider notes, including medication intensification, medication non-adherence and medication discontinuation. We will leverage these tools and the extensive EMR infrastructure at Partners HealthCare to develop and test a natural language processing system to study medication side effects. We will validate this system on the example of studying epidemiology of adverse reactions to statins. The findings of this project will lay the foundation for an open-source system that can be used for post-marketing surveillance of medication side effects using narrative EMR data.       PUBLIC HEALTH RELEVANCE (provided by applicant): Frequency and risk factors for side effects of statins (medications used to treat high cholesterol) in everyday medical practice (as opposed to research studies) are not known. In this project we will design a system for analyzing the information about statin side effects in the electronic medical records. If successful, this approach can be subsequently generalized to study side effects of many other medications.                 Natural Language Processing to Study Epidemiology of Statin Side Effects  Project Narrative  Frequency and risk factors for side effects of statins (medications used to treat high cholesterol) in everyday medical practice (as opposed to research studies) are not known. In this project we will design a system for analyzing the information about statin side effects in the electronic medical records. If successful, this approach can be subsequently generalized to study side effects of many other medications.",Natural Language Processing to Study Epidemiology of Statin Side Effects,7936999,RC1LM010460,"['Address', 'Adverse effects', 'Adverse event', 'Adverse reactions', 'Area', 'Cholesterol', 'Clinical Trials', 'Computerized Medical Record', 'Data', 'Documentation', 'Emerging Technologies', 'Epidemiology', 'Foundations', 'Frequencies', 'Goals', 'Healthcare', 'Hydroxymethylglutaryl-CoA Reductase Inhibitors', 'Hypersensitivity', 'Incidence', 'Informatics', 'Information Technology', 'Link', 'Medical', 'Memory Loss', 'Mental Depression', 'Myalgia', 'Natural Language Processing', 'Pharmaceutical Preparations', 'Placebos', 'Prevalence', 'Process', 'Provider', 'Randomized Clinical Trials', 'Reaction', 'Records', 'Reporting', 'Research Infrastructure', 'Risk Factors', 'Semantics', 'Side', 'Structure', 'System', 'Systems Analysis', 'Testing', 'Text', 'Work', 'abstracting', 'clinical practice', 'design', 'epidemiology study', 'hypercholesterolemia', 'medication compliance', 'open source', 'post-market', 'public health relevance', 'repository', 'research study', 'tool']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,RC1,2010,499697,0.003348905475044559
"Adapting Natural Language Processing Tools for Biosurveillance    DESCRIPTION (provided by applicant):       Early detection of disease outbreaks can decrease patient morbidity and mortality and minimize the spread of diseases. Early detection requires accurate classification of patient symptoms early in the course of their illness. One approach is biosurveillance, in which electronic symptom data are captured early in the course of illness, and analyzed for signals that might indicate an outbreak requiring investigation and response by the public health system. Emergency department (ED) patient records are particularly useful for biosurveillance, given their timely, electronic availability. ED data elements used in surveillance systems include the chief complaint (a brief description of the patient's primary symptom(s)), and triage nurses' note (also known as history of present illness).The chief complaint is the most widely used ED data element, because it is recorded electronically by the majority of EDs. One study showed that adding triage notes increased the sensitivity of biosurveillance case detection. The increased sensitivity is because the triage note increases the amount of data available: instead of one symptom in a chief complaint (e.g., fever), triage notes may contain multiple symptoms (e.g., ""fever, cough & shortness of breath for 12 hours""). Surveillance efforts are hampered, however, by the wide variability of free text data in ED chief complaints and triage notes. They often include misspellings, abbreviations, acronyms and other lexical and semantic variants that are difficult to group into symptom clusters (e.g., fever, temp 104, fvr, febrile). Tools are needed to address the lexical and semantic variation in symptom terms in ED data in order to improve biosurveillance. Natural language processing tools have been shown to facilitate concept extraction from more structured clinical data such as radiology reports, but there has been limited application of these techniques to free text ED triage notes. The project team developed the Emergency Medical Text Processor (EMT-P) to preprocess the chief complaint. EMT-P cleans and normalizes brief chief complaint entries and then extracts standardized concepts, but it is not sufficient in its current state to preprocess longer, more complex text passages such as triage notes. This proposed project will further strengthen biosurveillance by adapting EMT-P and other statistical and classical natural language processing tools to develop a system that extracts concepts from triage notes for biosurveillance.           Project Narrative Relevance: The public health system is responsible for monitoring large amounts of timely, electronic health data and needs more sophisticated tools to faciliate detection of, and response to, emerging infectious diseases and potential bioterrorism threats. The proposed project addresses this need by developing a system to extract relevant information from emergency department records.",Adapting Natural Language Processing Tools for Biosurveillance,7921455,G08LM009787,"['Abbreviations', 'Accident and Emergency department', 'Acute', 'Address', 'American', 'Avian Influenza', 'Bioterrorism', 'Bird Flu vaccine', 'Breathing', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Coughing', 'Country', 'Data', 'Data Element', 'Data Quality', 'Detection', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Early Diagnosis', 'Electronic Health Record', 'Electronics', 'Emergency Medicine', 'Emergency Situation', 'Emerging Communicable Diseases', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Fever', 'Genetic Transcription', 'Goals', 'Gold', 'Health', 'Health Services', 'Health system', 'Hour', 'Intervention', 'Investigation', 'Manuals', 'Measures', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'North Carolina', 'Nurses', 'Patients', 'Performance', 'Pertussis', 'Physicians', 'Predictive Value', 'Process', 'Public Health', 'Radiology Specialty', 'Recording of previous events', 'Records', 'Reporting', 'Sampling', 'Screening procedure', 'Semantics', 'Sensitivity and Specificity', 'Shortness of Breath', 'Signal Transduction', 'Smallpox', 'Structure', 'Symptoms', 'System', 'Techniques', 'Temperature', 'Text', 'Translating', 'Triage', 'Variant', 'acronyms', 'base', 'experience', 'improved', 'lexical', 'mortality', 'pandemic disease', 'population based', 'prototype', 'research to practice', 'response', 'satisfaction', 'syntax', 'tool']",NLM,UNIV OF NORTH CAROLINA CHAPEL HILL,G08,2010,148350,0.054751237238870425
"Natural Language Processing for Cancer Research Network Surveillance Studies    DESCRIPTION (provided by applicant): This application addresses Broad Challenge Area: (10) Information Technology for Processing Health Care Data and specific Challenge Topic: 10-CA-107 Expand Spectrum of Cancer Surveillance through Informatics Approaches. The proposed project launches a collaborative effort to advance adoption within the HMO Cancer Research Network (CRN) of ""industrial-strength"" natural language processing (NLP) systems useful for mining valuable, research-grade information from unstructured clinical text. Such text is available for processing, now in the electronic medical record (EMR) systems of affiliated CRN health plans. The proposed NLP methods   will create ongoing capacity to tap what has recently been described as ""a treasure trove of historical   unstructured data that provides essential information for the study of disease progression, treatment   effectiveness and long-term outcomes"" (5). The vision of advancing widespread NLP capacity across the CRN, as well as the approach we present here for implementing it, grew out of an in-depth strategic planning effort we completed in December 2008. That effort involved participants from six CRN sites guided by a blue-ribbon panel of NLP experts from three of the nation's leading centers of clinical NLP research: University of Pittsburgh Medical Center, Vanderbilt University, and Mayo Clinic. The vision is to deploy a powerful NLP system locally, manage it with newly hired and trained local NLP technical staff, and conduct NLP-based research projects initiated by local investigators, in consultation with higher-level external NLP experts. Our planning efforts suggest this collaborative model is feasible; we will test the model in the context of the proposed project. An important development in April 2009 yielded what we believe is a potentially transformative opportunity to accelerate adoption of NLP capacity in applied research settings: release of the open-source Clinical Text Analysis and Knowledge Extraction System (cTAKES) software. This software was the result of a collaborative effort between IBM and Mayo Clinic. Built on the same framework Mayo Clinic currently uses to process its repository of over 40 million clinical documents, cTAKES dramatically lowers the cost of adopting a comprehensive and flexible NLP system. Deployment and use of such systems was previously only feasible in institutions with large, academically-oriented biomedical informatics research programs.   Still, other deployment challenges and the need to acquire NLP training for local staff present residual   barriers to adopting comprehensive NLP systems such as cTAKES. In collaboration with five other CRN sites the proposed project mitigates these challenges in two ways: 1) it develops configurable open-source software modules needed to streamline and therefore reduce the cost of deploying cTAKES, and 2) it presents and tests a model for training local staff through hands-on NLP projects overseen by outside NLP expert consultants. The potential impact of this project is evident most clearly in the vast untapped opportunities for text mining represented in CRN-affiliated health plans, where EMR systems have been in place since at least 2005, and whose patients represent 4% of the U.S. population. Clinical text mining offers the potential to provide new or improved data elements for cancer surveillance and other types of research requiring information about patient functional status, medication side-effects, details of therapeutic approaches, and differential information about clinical findings. Another significant impact of this project is its plan to integrate into the cTAKES system   an open-source de-identification tool based on state of the art, best of breed NLP approaches developed by the MITRE Corporation. De-identification of clinical text will make it easier for researchers to get access to clinical text, and will also facilitate multi-site collaborations while protecting patient privacy. Finally, if successful, the NLP algorithm we propose as a proof-of-principle project at Group Health-which will classify sets of patient charts as either containing or not containing a diagnosis of recurrent breast cancer-could dramatically reduce the cost of research in this area; currently all recurrent breast cancer endpoints must be established through costly manual chart abstraction.   Novel aspects of the proposed project include its talented and transdisciplinary research team,   including national experts in NLP, and its resourceful strategy for building the technical resources and ""human capital"" needed to support an ongoing program of applied NLP research. Natural language processing is itself a highly innovative technology; when successfully established in multiple CRN in the future it will represent a watershed moment in the CRN's already impressive history of exploiting data systems to support innovative research. Newly hired staff positions total approximately 2.0 FTE in each project year, most of which we anticipate will be supported by ongoing new research programs after the proposed project concludes. Project narrative The proposed project develops new measurement technologies for extracting information about disease processes and treatment, currently documented only in clinical text, based on natural language processing approaches. Because these methods are generic they will potentially contribute to public health by advancing research in a wide variety of areas. The ""proof of principle"" algorithm developed in the project to identify recurrent breast cancer diagnoses will advance epidemiologic and clinical research pertaining to the 2.5 million women currently living with breast cancer.           Project narrative The proposed project develops new measurement technologies for extracting information about disease processes and treatment, currently documented only in clinical text, based on natural language processing approaches. Because these methods are generic they will potentially contribute to public health by advancing research in a wide variety of areas. The ""proof of principle"" algorithm developed in the project to identify recurrent breast cancer diagnoses will advance epidemiologic and clinical research pertaining to the 2.5 million women currently living with breast cancer.",Natural Language Processing for Cancer Research Network Surveillance Studies,7944035,RC1CA146917,"['Address', 'Adopted', 'Adoption', 'Adverse effects', 'Algorithms', 'Applied Research', 'Area', 'Arts', 'Bioinformatics', 'Breeding', 'Cancer Research Network', 'Charge', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Complex', 'Comprehensive Health Care', 'Computer software', 'Computerized Medical Record', 'Consultations', 'Data', 'Data Element', 'Data Quality', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Doctor of Philosophy', 'Environment', 'Epidemiology', 'Exercise', 'Future', 'Generic Drugs', 'Hand', 'Health', 'Health Planning', 'Health system plans', 'Healthcare', 'Human Resources', 'Individual', 'Informatics', 'Information Systems', 'Information Technology', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Learning', 'Licensing', 'Life', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Mining', 'Modeling', 'NCI Center for Cancer Research', 'Natural Language Processing', 'Operating System', 'Outcome', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Positioning Attribute', 'Process', 'Public Health', 'Recording of previous events', 'Recurrence', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Resources', 'Risk', 'Site', 'Solutions', 'Strategic Planning', 'System', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Training', 'Treatment Effectiveness', 'Universities', 'Vision', 'Woman', 'base', 'biomedical informatics', 'breast cancer diagnosis', 'cost', 'design', 'experience', 'feeding', 'firewall', 'flexibility', 'functional status', 'human capital', 'improved', 'innovation', 'innovative technologies', 'malignant breast neoplasm', 'novel', 'open source', 'patient privacy', 'programs', 'repository', 'skills', 'software systems', 'surveillance study', 'text searching', 'tool']",NCI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,RC1,2010,494477,0.05129238784276336
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",7779983,R01LM010016,"['Adverse event', 'Affect', 'Arts', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,343397,0.038551481675468305
"Integrated discovery and hypothesis testing of new associations in rare diseases    DESCRIPTION (provided by applicant): Rare diseases are studied in isolated laboratories, forgotten by main stream pharmacological companies, and considered almost academic curiosities. Finding variables that correlate/cause rare diseases (a condition is rare when it affects less than 1 person per 2,000) is a difficult task. The low number of cases and the sparse nature of the reports make it difficult to obtain significant/meaningful statistical results. There are two ways to avoid these problems. The first is to integrate reported cases and associations to generate enough statistical power. The second way is to have an independent data set, big enough to cover rare cases. Each of the two methods has intrinsic problems. For instance, the search in the literature puts together different studies, each of them with their own biases in population, methodology and objectives. On the other hand, blind searches for associations in big databases introduce a large number of false positives due to multiple hypothesis testing.       These problems could be avoided by developing innovative methods that allow the integration of information and methodologies in the literature and longitudinal databases. To achieve this goal, we propose a team that combines expertise in natural language processing systems (Carol Friedman), electronic health records (George Hripcsak), statistics in combined databases and computational virology (Raul Rabadan). This team will generate an interdisciplinary approach to mine and integrate the literature and the dataset collected at Columbia/New York Presbyterian hospital. Identifying unusual correlations in rare diseases is the first step to understanding the origin of the diseases and to finding a cure for them. We hypothesize that we will develop effective methods aimed at improving our understanding of rare diseases by combining hypothesis testing and hypothesis discovery, and by integrating information from the literature and from the patient record to obtain increased statistical power. This will involve using natural language processing and statistical methods to mine both the literature and the electronic health record (EHR).           Project Narrative We will test reported associations in rare diseases and discover new ones by integrating information from the literature and from Electronic Health Records in hospitals.",Integrated discovery and hypothesis testing of new associations in rare diseases,8142701,R01LM010140,"['Affect', 'Case Study', 'Curiosities', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Goals', 'Hand', 'Hospitals', 'Laboratories', 'Literature', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Nature', 'New York', 'Patients', 'Persons', 'Population', 'Presbyterian Church', 'Rare Diseases', 'Reporting', 'Statistical Methods', 'Stream', 'System', 'Testing', 'blind', 'forgetting', 'improved', 'innovation', 'interdisciplinary approach', 'longitudinal database', 'statistics', 'virology']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,10000,0.01150187302422643
"Integrated discovery and hypothesis testing of new associations in rare diseases    DESCRIPTION (provided by applicant): Rare diseases are studied in isolated laboratories, forgotten by main stream pharmacological companies, and considered almost academic curiosities. Finding variables that correlate/cause rare diseases (a condition is rare when it affects less than 1 person per 2,000) is a difficult task. The low number of cases and the sparse nature of the reports make it difficult to obtain significant/meaningful statistical results. There are two ways to avoid these problems. The first is to integrate reported cases and associations to generate enough statistical power. The second way is to have an independent data set, big enough to cover rare cases. Each of the two methods has intrinsic problems. For instance, the search in the literature puts together different studies, each of them with their own biases in population, methodology and objectives. On the other hand, blind searches for associations in big databases introduce a large number of false positives due to multiple hypothesis testing.       These problems could be avoided by developing innovative methods that allow the integration of information and methodologies in the literature and longitudinal databases. To achieve this goal, we propose a team that combines expertise in natural language processing systems (Carol Friedman), electronic health records (George Hripcsak), statistics in combined databases and computational virology (Raul Rabadan). This team will generate an interdisciplinary approach to mine and integrate the literature and the dataset collected at Columbia/New York Presbyterian hospital. Identifying unusual correlations in rare diseases is the first step to understanding the origin of the diseases and to finding a cure for them. We hypothesize that we will develop effective methods aimed at improving our understanding of rare diseases by combining hypothesis testing and hypothesis discovery, and by integrating information from the literature and from the patient record to obtain increased statistical power. This will involve using natural language processing and statistical methods to mine both the literature and the electronic health record (EHR).           Project Narrative We will test reported associations in rare diseases and discover new ones by integrating information from the literature and from Electronic Health Records in hospitals.",Integrated discovery and hypothesis testing of new associations in rare diseases,7828239,R01LM010140,"['Affect', 'Case Study', 'Curiosities', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Goals', 'Hand', 'Hospitals', 'Laboratories', 'Literature', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Nature', 'New York', 'Patients', 'Persons', 'Population', 'Presbyterian Church', 'Rare Diseases', 'Reporting', 'Statistical Methods', 'Stream', 'System', 'Testing', 'blind', 'forgetting', 'improved', 'innovation', 'interdisciplinary approach', 'longitudinal database', 'statistics', 'virology']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,531496,0.01150187302422643
"Sematic Relatedness for Active Medication Safety and Outcomes Surveillance    DESCRIPTION (provided by applicant):       Medication-related morbidity and mortality in ambulatory care in the United States results in estimated 100,000 deaths and $177 billion spending annually. Post-marketing passive surveillance of outcomes associated with medication use has been recognized as a necessary component in drug safety monitoring to overcome the limitations of pre- marketing clinical trials. Information technology applied to the patient's electronic medical and therapeutic record holds promise to improve this situation by detecting alarming trends in signs and symptoms in patient populations exposed to the same medication. Currently, much of the information necessary for active drug safety surveillance is ""locked"" in the unstructured text of electronic records. Our long-term goal is to develop information technology to recognize and prevent drug therapy related adverse events. Sophisticated natural language processing systems have been developed to find medical terms and their synonyms in the unstructured text and use them to retrieve information. In order to monitor alarming trends in symptoms in medical records, we need mechanisms that will allow not only accurate term and concept identification but also grouping of semantically related concepts that may not necessarily be synonymous. Measures of semantic relatedness rely on existing ontologies of domain knowledge as well as large textual corpora to compute a numeric score indicating the strength of relatedness between two concepts. Our central hypothesis is that such measures will be able to make fine-grained distinctions among concepts in the biomedical text, and provide a foundation upon which to organize concepts into meaningful groups automatically. In particular, this proposal seeks to develop methods that leverage the medical knowledge contained within Unified Medical Language System (UMLS) and corpora of clinical text. Our short-term goals are 1) develop new methods, specific to clinical text, for computing semantic relatedness 2) integrate these specific methods for computing semantic relatedness into more general methods of natural language processing 3) integrate semantic relatedness into methods for identifying labeled semantic relations in clinical text. Labeled relations significantly enhance the ability of natural language processing to support accurate automatic analysis of medical information for improving patient safety. Our next step will be to develop and validate a generalizable active medication safety surveillance system that will automatically track medication exposure and alarming trends in signs and symptoms in ambulatory and hospitalized populations for a broad range of diseases.           This project will a) create and validate a common open-source platform for developing and testing semantic relatedness measures, b) determine the validity of electronic medical records with respect to identification of symptoms associated with medication- related problems and c) develop a novel methodology to aggregate adverse reaction terms used to code spontaneous post-marketing drug safety surveillance reports. The results of this project will enable more effective medication safety surveillance efforts and thus will improve patient safety.",Sematic Relatedness for Active Medication Safety and Outcomes Surveillance,7942766,R01LM009623,"['Address', 'Adverse effects', 'Adverse event', 'Adverse reactions', 'Ambulatory Care', 'Angina Pectoris', 'Area', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical assessments', 'Code', 'Computational Technique', 'Computerized Medical Record', 'Databases', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Effectiveness', 'Electronics', 'Exposure to', 'Foundations', 'Generic Drugs', 'Goals', 'Group Identifications', 'Grouping', 'Health', 'Healthcare', 'Heart failure', 'Information Technology', 'Knowledge', 'Label', 'Linguistics', 'Mandatory Reporting', 'Manuals', 'Maps', 'Marketing', 'Measures', 'Medical', 'Medical Electronics', 'Medical History', 'Medical Informatics', 'Medical Records', 'Methodology', 'Methods', 'Minnesota', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Nature', 'One-Step dentin bonding system', 'Ontology', 'Outcome', 'Patients', 'Pharmaceutical Cares', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Pharmacy facility', 'Physicians', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Procedures', 'Process', 'Reaction', 'Records', 'Reference Standards', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Safety', 'Semantics', 'Signs and Symptoms', 'Statistical Models', 'Symptoms', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Therapeutic Effect', 'Time', 'Training', 'Unified Medical Language System', 'United States', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Validation', 'Work', 'Writing', 'base', 'computer science', 'data mining', 'design', 'experience', 'flexibility', 'improved', 'information organization', 'knowledge base', 'metathesaurus', 'mortality', 'novel', 'open source', 'patient population', 'patient safety', 'phrases', 'post-market', 'practice-based research network', 'prevent', 'social', 'treatment planning', 'trend']",NLM,UNIVERSITY OF MINNESOTA,R01,2010,320155,0.0432793134044701
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,7784533,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,341606,-0.00900700261742695
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,8056227,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,177422,-0.00900700261742695
"Annotation, development and evaluation for clinical information extraction    DESCRIPTION (provided by applicant): Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible.      PUBLIC HEALTH RELEVANCE: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.           Project narrative: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.","Annotation, development and evaluation for clinical information extraction",7985218,R01GM090187,"['Address', 'Algorithms', 'Automated Annotation', 'Caring', 'Clinical', 'Clinical Research', 'Code', 'Communities', 'Computerized Medical Record', 'Consensus', 'Country', 'Data Set', 'Development', 'Disease', 'Evaluation', 'Goals', 'Gold', 'Guidelines', 'Individual', 'Judgment', 'Knowledge', 'Linguistics', 'Manuals', 'Medical Records', 'Methodology', 'Methods', 'Metric', 'Natural Language Processing', 'Performance', 'Reliance', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Signs and Symptoms', 'System', 'Technology', 'Terminology', 'Text', 'Training', 'Translational Research', 'Translations', 'base', 'cost', 'design', 'flexibility', 'innovation', 'knowledge translation', 'phrases', 'prevent', 'public health relevance', 'research clinical testing', 'research study', 'tool']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2010,1,0.09632777001042818
"Enhancing Clinical Effectiveness Research with Natural Language Processing of EMR    DESCRIPTION (provided by applicant): To successfully use large linked clinical databases for comparative effectiveness research (CER) requires addressing some key informatics challenges associated with distributed, heterogeneous clinical data. Electronic networks of researchers are part of the solution because they can bridge the physical and organizational divides created by distinct health systems' individual electronic medical records (EMRs). In addition, informatics research has demonstrated the feasibility of automatically coding clinical text, enhancing the capacity to integrate both unstructured and non-standardized clinical data from EMRs. With this study, we propose to develop CER infrastructure, make broadly available the proven MediClass technology for automated classification of EMRs containing both coded data and text clinical notes, and demonstrate the potential of this infrastructure for addressing CER questions within the asthma and tobacco-using patient populations of 6 diverse health systems. Asthma and smoking each impose huge and modifiable burdens on the healthcare system, and multiple morbidities related to asthma and smoking have been targeted by the IOM and AHRQ as priority areas in efforts to improve the healthcare system through comparative effectiveness research. We propose to develop, deploy, operate and evaluate the CER HUB, an Internet-based platform for conducting CER, and to demonstrate its utility in studying clinical interventions in asthma and smoking. Researchers who register to use the HUB, beginning with the research team from the 6 participating study sites, will be able to use a secure website to configure and download MediClass applications addressing CER questions within their respective healthcare organizations, to contribute these IRB-approved, processed datasets back to a centralized data coordinating center to be pooled with data similarly processed from other healthcare organizations, and to use the pooled database to answer diverse comparative effectiveness questions of large, real-world populations. A central function of the CER HUB will be facilitating (through online, interactive tools) development of a shared library of MediClass knowledge modules that afford uniform, standardized coding of EMR data. This shared library of knowledge modules could permit researchers to assess effectiveness in multiple areas of healthcare and gain access to data otherwise locked away in text clinical notes. A goal of the CER HUB is to accelerate creation of standardized knowledge used to normalize heterogeneous EMR data as representations of clinical events for CER. During the project period we will conduct 2 studies using this infrastructure to address the effectiveness of interventions for asthmatics and tobacco users across the 6 participating health systems. As an ongoing resource, the HUB will provide a collaborative development platform for enhancing comparative effectiveness research in potentially any health care domain.      CER researchers can build software applications that will process their EMRs, creating standardized datasets permitting CER using a secure website to configure and download MediClass applications addressing CER questions within their respective healthcare organizations, to contribute these IRB-approved, processed datasets back to a centralized data coordinating center to be pooled with data similarly processed from other healthcare organizations, and to use the pooled database to answer diverse comparative effectiveness questions of large, real-world populations      PUBLIC HEALTH RELEVANCE: Comparative effectiveness research (CER) requires that clinical data be in standard forms allowing multiple, large databases to be efficiently combined, and requires that all of the data be coded so that automated summarization of the data is possible. However, much of the clinical data necessary for CER is in the text clinical notes written by clinicians when caring for patients. We will build a centralized website where CER researchers can build software applications that will process their electronic medical records, including both the text and coded data, creating standardized datasets permitting comparative effectiveness research. We will demonstrate the utility of this infrastructure by conducting CER studies investigating the effectiveness of interventions in asthma and smoking, across the 6 participating health systems.           PROJECT NARRATIVE Comparative effectiveness research (CER) requires that clinical data be in standard forms allowing multiple, large databases to be efficiently combined, and requires that all of the data be coded so that automated summarization of the data is possible. However, much of the clinical data necessary for CER is in the text clinical notes written by clinicians when caring for patients. We will build a centralized website where CER researchers can build software applications that will process their electronic medical records, including both the text and coded data, creating standardized datasets permitting comparative effectiveness research. We will demonstrate the utility of this infrastructure by conducting CER studies investigating the effectiveness of interventions in asthma and smoking, across the 6 participating health systems.",Enhancing Clinical Effectiveness Research with Natural Language Processing of EMR,8032928,R01HS019828,[' '],AHRQ,KAISER FOUNDATION RESEARCH INSTITUTE,R01,2010,8696942,-0.0665084244620218
"Improving Outpatient Medication Lists Using Temporal Reasoning and Clinical Texts    DESCRIPTION (provided by applicant):  Accurate and complete medication lists are critical inputs to effective medication reconciliation to prevent medication prescribing and administration errors. Previous research aggregated structured medication data form multiple sources to generate and maintain a reconciled medication list. Medications documented in clinical texts also need to be reconciled. However, most reconciliation methods currently have limited capability to process textual data and temporal information (e.g., dates, duration and status). Our goal is to pilot and test methodologies and applications in the fields of natural language processing (NLP) and temporal reasoning to facilitate the use of electronic clinical texts in order to improve the ""correctness"" and ""completeness"" of medication lists. Clinic notes and free-text ""comments"" fields in medication lists in an ambulatory electronic medical record system will be considered in the study. An NLP system and a temporal reasoning system will be adapted to automatically extract medication and associated temporal information from clinical texts and encode the medications using a controlled terminology. Multiple knowledge bases will be used to develop a mechanism to represent the timing of medication use, detect the changes (e.g., active or inactive), and then to organize medications into appropriate groups (e.g., by ingredient or by status). The feasibility and efficiency of the proposed methods and tools in improving the process of medication   reconciliation will be assessed. Domain experts will serve as judges to assess the success of capturing, coding, and organizing the medications and temporal information and also to evaluate whether our methods are complementary to those currently used for medication management.           Accurate and complete medication information at the point of care is crucial for delivery of high-quality care and prevention of adverse events. Most previous studies aggregated structured medication data from EMR and CPOE (Computerized Physician Order Entry) systems to generate and maintain a reconciled medication list. However, medications in non-structured narrative sources (such as clinic notes and free-text comments) must also be reconciled. Structured data presented in a standard, predictable form can be easily processed by a computer. By contrast, narrative data does not have a well-defined structure, so processing such data is very challenging. Our goal is to pilot and test methodologies and applications in the fields of natural language processing (any system that manipulates text) and temporal reasoning (e.g., identifying the timing of medication use) to facilitate the use of electronic clinical texts in order to improve the ""correctness"" and ""completeness"" of medication lists. The feasibility and efficiency of the proposed methods and tools in improving the process of medication reconciliation will be assessed.",Improving Outpatient Medication Lists Using Temporal Reasoning and Clinical Texts,7935475,R03HS018288,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R03,2010,50100,0.04124427680297896
"Annotation, development and evaluation for clinical information extraction    DESCRIPTION (provided by applicant): Much of the clinical information required for accurate clinical research, active decision support, and broad-coverage surveillance is locked in text files in an electronic medical record (EMR). The only feasible way to leverage this information for translational science is to extract and encode the information using natural language processing (NLP). Over the last two decades, several research groups have developed NLP tools for clinical notes, but a major bottleneck preventing progress in clinical NLP is the lack of standard, annotated data sets for training and evaluating NLP applications. Without these standards, individual NLP applications abound without the ability to train different algorithms on standard annotations, share and integrate NLP modules, or compare performance. We propose to develop standards and infrastructure that can enable technology to extract scientific information from textual medical records, and we propose the research as a collaborative effort involving NLP experts across the U.S. To accomplish this goal, we will address three specific aims: Aim 1: Extend existing standards and develop new consensus standards for annotating clinical text in a way that is interoperable, extensible, and usable. Aim 2: Apply existing methods and tools, and develop new methods and tools where necessary for manually annotating a set of publicly available clinical texts in a way that is efficient and accurate. Aim 3: Develop a publicly available toolkit for automatically annotating clinical text and perform a shared evaluation to evaluate the toolkit, using evaluation metrics that are multidimensional and flexible.      PUBLIC HEALTH RELEVANCE: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.           Project narrative: In this project, we will develop a publicly available corpus of annotated clinical texts for NLP research. We will experiment with methods for increasing the efficiency of annotation and will annotate de-identified reports of nine types for linguistic and clinical information. In addition, we will create an NLP toolkit that can be shared and will evaluate it against other NLP systems in a shared task evaluation with the community.","Annotation, development and evaluation for clinical information extraction",8231171,R01GM090187,[' '],NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2010,642650,0.09632777001042818
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453           PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,7946175,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Arts', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2010,370693,-0.021726057007146297
"Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach No abstract available  Relevance The long-term objective of this research is to increase the clinical trial enrollment of US patients via a semi- automated, Natural Language Processing (NLP) based, interactive and patient-centered informatics application. The proposed work is highly significant because the dismal clinical trial accrual rates (2-4 % nationally) hampers timely development of new drugs. In addition, studies show that physicians have statistically significant bias against elderly and minority patients to invite participation in clinical trials. The proposed project is synergistic with physician-centered efforts but the goal is to provide individualized, electronic medical record based clinical trial recommendations directly to the patients. The results of this research will empower patients and elevate their role in the decision making process.",Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach,8145098,R00LM010227,"['Adult', 'Age', 'Algorithms', 'Applications Grants', 'Arts', 'Biomedical Research', 'Cancer Patient', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical trial protocol document', 'Code', 'Commit', 'Complement', 'Computer software', 'Computerized Medical Record', 'Decision Making', 'Development', 'Elderly', 'Elements', 'Eligibility Determination', 'Enrollment', 'Ensure', 'Equation', 'Evaluation', 'Faculty', 'Foundations', 'Future', 'Goals', 'Gold', 'Grant', 'Hand', 'In Vitro', 'Informatics', 'Java', 'Knowledge', 'Laboratories', 'Linguistics', 'Malignant Neoplasms', 'Maps', 'Medical Records', 'Medicine', 'Methods', 'Minority', 'Modification', 'Natural Language Processing', 'Newly Diagnosed', 'Observational Study', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Protocols documentation', 'Public Health Informatics', 'Publishing', 'Recommendation', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Role', 'Screening for cancer', 'Screening procedure', 'Specialist', 'Surgeon', 'Text', 'Training', 'Unified Medical Language System', 'United States National Library of Medicine', 'Work', 'abstracting', 'base', 'cancer care', 'empowered', 'ethnic minority population', 'information organization', 'knowledge base', 'novel', 'older patient', 'patient oriented', 'programs', 'prospective', 'research study', 'software development', 'success', 'tool']",NLM,CINCINNATI CHILDRENS HOSP MED CTR,R00,2010,249000,0.031524762020850736
"Discovering and Applying Knowledge in Clinical Databases    DESCRIPTION (provided by applicant):  The ongoing goal of our project, ""Discovering and applying knowledge in clinical databases,"" is to develop and apply methods to exploit electronic medical record data for decision support, with an emphasis on narrative data. Since the inception of our project as an R29 in 1994, we have been developing methods for preparing raw electronic medical record data, applying and evaluating natural language processing, developing data mining techniques including machine learning, and putting the results to use for clinical care and research.       In this competing continuation, we propose to address the temporal information in the electronic medical record and to apply natural language processing and temporal processing to the task of syndromic surveillance in collaboration with the New York City Department of Health and Mental Hygiene (NYC DOHMH).       We have begun work on a temporal processing system. It extracts temporal assertions stated in narrative reports, uses the MedLEE natural language processor to parse the non-temporal information, infers implicit temporal assertions based on a knowledge base, and produces the information in the form of a simple temporal constraint satisfaction problem. The latter can be used to answer questions about the time of events and the temporal relation between pairs of events. We propose to complete the system, expand the knowledge base, speed computation, address the uncertainty of temporal assertions, incorporate temporal information from structured data, and evaluate the system.       NYC DOHMH has a mature syndromic surveillance system that watches over almost eight million persons, and it has as-yet unexploited data sources in the form of narrative and structured electronic medical records. We propose to apply natural language processing and our proposed temporal processing to convert the data to a form appropriate for surveillance. We will evaluate the incremental benefit of structured data, narrative data, and temporally processed narrative data.           n/a",Discovering and Applying Knowledge in Clinical Databases,7933293,R01LM006910,"['Address', 'Area', 'Cities', 'Clinical', 'Code', 'Collaborations', 'Computerized Medical Record', 'Data', 'Data Sources', 'Databases', 'Event', 'Goals', 'Health', 'Healthcare', 'Knowledge', 'Machine Learning', 'Mental Health', 'Methods', 'Natural Language Processing', 'New York City', 'Persons', 'Preparation', 'Process', 'Reporting', 'Research', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Uncertainty', 'Work', 'base', 'clinical care', 'data mining', 'improved', 'knowledge base', 'natural language', 'satisfaction', 'syndromic surveillance']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,152617,0.03528039648750205
"Extracting Semantic Knowledge from Clinical Reports    DESCRIPTION (provided by applicant): Analyzing and processing free-text medical reports for data mining and clinical data interchange is one of the most challenging problems in medical informatics, yet it is crucial for continued research advances and improvements in clinical care. Natural language processing (NLP) is an important enabling technology, but has been held back because it is difficult to understand human language, since it requires extensive domain knowledge. In Phase I, we developed new statistical and machine learning methods that apply domain specific knowledge to the semantic analysis of free-text radiology reports. The methods enabled the creation of two new prototype applications - a SNOMED CT (Systematized Nomenclature of Medicine--Clinical Terms) coding service called SnomedCoder, and a text mining tool for analyzing a large corpus of medical reports, called DataMiner. In Phase II, we will accomplish the following specific aims: 1) Improve the semantic extraction methods developed in Phase I, 2) Expand the semantic knowledge base and classify at least two million new unique sentences from multiple medical institutions, 3) Provide a SNOMED CT auto coding service (alpha service) to participating Indiana Health Information Exchange hospitals, and 4) Build a commercial version of the DataMiner software, and test its functionality using researchers at the Regenstrief Institute.       These scientific innovations will revolutionize the ability of health care researchers to analyze vast repositories of clinical information currently locked up in electronic medical records, and correlate this data with new biomedical discoveries in proteonomics and genomics. The ability to codify text rapidly will extend the potential for clinical decision support beyond its narrow base of numeric and structured medical data, and enable SNOMED CT to become a useful coding standard. Phase III will offer coding and data mining services to healthcare payers (both private and government), pharmaceuticals, and academic researchers. A key advantage of our approach over other NLP systems is that we attempt to codify all the information in the report and not just a limited subset, and insist on expert validation which provides a high degree of confidence in the accuracy of the coded data.Project Narrative           n/a",Extracting Semantic Knowledge from Clinical Reports,7554153,R44RR024929,"['Address', 'Algorithms', 'Back', 'Bioinformatics', 'Body of uterus', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Code', 'Collection', 'Computer software', 'Computerized Medical Record', 'Data', 'Decision Making', 'Effectiveness', 'Genomics', 'Goals', 'Government', 'Health', 'Healthcare', 'Hospitals', 'Human', 'Indiana', 'Institutes', 'Institution', 'Journals', 'Knowledge', 'Language', 'Longitudinal Studies', 'Machine Learning', 'Medical', 'Medical Informatics', 'Medical Records', 'Methods', 'Natural Language Processing', 'Paper', 'Pharmacologic Substance', 'Phase', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Services', 'Speed', 'Structure', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Testing', 'Text', 'Trees', 'Trust', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Validation', 'base', 'clinical care', 'computerized', 'data mining', 'health care quality', 'improved', 'indexing', 'innovation', 'knowledge base', 'novel strategies', 'patient safety', 'prototype', 'repository', 'research and development', 'success', 'text searching', 'tool']",NCRR,"LOGICAL SEMANTICS, INC.",R44,2009,422728,0.0054949989220600265
"Natural Language Processing to Study Epidemiology of Statin Side Effects    DESCRIPTION (provided by applicant): This application addresses broad Challenge Area (10) Information Technology for Processing Health Care Data and specific Challenge Topic 10-LM-101: Informatics for post-marketing surveillance. The overall goal of this study is to develop a generalizable framework for studying medication side effects recorded in narrative medical documents. We will implement and test this system on the example of epidemiologic characterization of side effects of HMG-CoA reductase inhibitors (a.k.a. statins). Statins are the most commonly used class of medications for treatment of hypercholesterolemia in the U.S. In randomized clinical trials statins are associated only with a slight increase in adverse reactions and no increase in discontinuation of treatment compared to placebo. However, in clinical practice the rates of side effects and discontinuation appear significantly higher and represent a major barrier to a critical, potentially lifesaving therapy. For example, myalgias are reported to be relatively rare in clinical trials but are thought to be more common in clinical practice. Additionally, a number of other statin-associated complaints reported anecdotally but not well elucidated in clinical trials include depression, irritability, and memory loss among others. Most of these have been poorly epidemiologically characterized and their prevalence and risk factors remain unknown. Structured electronic medical record (EMR) and administrative data have been used to study medication side effects. However, structured data have important limitations. They may not contain temporal or causative information necessary to link particular problems to medications and may not be sufficiently granular to identify specific adverse reactions. Narrative EMR data, such as provider notes, can provide documentation of causative links between medication and adverse events at high levels of granularity. Natural language processing (NLP) is an emerging technology that enables computational abstraction of information from narrative medical documents. In prior work we have successfully applied natural language processing to abstract medication information from narrative provider notes, including medication intensification, medication non-adherence and medication discontinuation. We will leverage these tools and the extensive EMR infrastructure at Partners HealthCare to develop and test a natural language processing system to study medication side effects. We will validate this system on the example of studying epidemiology of adverse reactions to statins. The findings of this project will lay the foundation for an open-source system that can be used for post-marketing surveillance of medication side effects using narrative EMR data.       PUBLIC HEALTH RELEVANCE (provided by applicant): Frequency and risk factors for side effects of statins (medications used to treat high cholesterol) in everyday medical practice (as opposed to research studies) are not known. In this project we will design a system for analyzing the information about statin side effects in the electronic medical records. If successful, this approach can be subsequently generalized to study side effects of many other medications.                 Natural Language Processing to Study Epidemiology of Statin Side Effects  Project Narrative  Frequency and risk factors for side effects of statins (medications used to treat high cholesterol) in everyday medical practice (as opposed to research studies) are not known. In this project we will design a system for analyzing the information about statin side effects in the electronic medical records. If successful, this approach can be subsequently generalized to study side effects of many other medications.",Natural Language Processing to Study Epidemiology of Statin Side Effects,7834605,RC1LM010460,"['Address', 'Adverse effects', 'Adverse event', 'Adverse reactions', 'Area', 'Cholesterol', 'Clinical Trials', 'Computerized Medical Record', 'Data', 'Documentation', 'Emerging Technologies', 'Foundations', 'Frequencies', 'Goals', 'Healthcare', 'Hydroxymethylglutaryl-CoA Reductase Inhibitors', 'Hypersensitivity', 'Incidence', 'Informatics', 'Information Technology', 'Link', 'Medical', 'Memory Loss', 'Myalgia', 'Natural Language Processing', 'Pharmaceutical Preparations', 'Placebos', 'Prevalence', 'Process', 'Provider', 'Randomized Clinical Trials', 'Reaction', 'Records', 'Reporting', 'Research Infrastructure', 'Risk Factors', 'Semantics', 'Side', 'Structure', 'System', 'Systems Analysis', 'Testing', 'Text', 'Work', 'abstracting', 'clinical practice', 'depression', 'design', 'epidemiology study', 'hypercholesterolemia', 'medication compliance', 'open source', 'post-market', 'public health relevance', 'repository', 'research study', 'tool']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,RC1,2009,499818,0.003348905475044559
"Adapting Natural Language Processing Tools for Biosurveillance    DESCRIPTION (provided by applicant):       Early detection of disease outbreaks can decrease patient morbidity and mortality and minimize the spread of diseases. Early detection requires accurate classification of patient symptoms early in the course of their illness. One approach is biosurveillance, in which electronic symptom data are captured early in the course of illness, and analyzed for signals that might indicate an outbreak requiring investigation and response by the public health system. Emergency department (ED) patient records are particularly useful for biosurveillance, given their timely, electronic availability. ED data elements used in surveillance systems include the chief complaint (a brief description of the patient's primary symptom(s)), and triage nurses' note (also known as history of present illness).The chief complaint is the most widely used ED data element, because it is recorded electronically by the majority of EDs. One study showed that adding triage notes increased the sensitivity of biosurveillance case detection. The increased sensitivity is because the triage note increases the amount of data available: instead of one symptom in a chief complaint (e.g., fever), triage notes may contain multiple symptoms (e.g., ""fever, cough & shortness of breath for 12 hours""). Surveillance efforts are hampered, however, by the wide variability of free text data in ED chief complaints and triage notes. They often include misspellings, abbreviations, acronyms and other lexical and semantic variants that are difficult to group into symptom clusters (e.g., fever, temp 104, fvr, febrile). Tools are needed to address the lexical and semantic variation in symptom terms in ED data in order to improve biosurveillance. Natural language processing tools have been shown to facilitate concept extraction from more structured clinical data such as radiology reports, but there has been limited application of these techniques to free text ED triage notes. The project team developed the Emergency Medical Text Processor (EMT-P) to preprocess the chief complaint. EMT-P cleans and normalizes brief chief complaint entries and then extracts standardized concepts, but it is not sufficient in its current state to preprocess longer, more complex text passages such as triage notes. This proposed project will further strengthen biosurveillance by adapting EMT-P and other statistical and classical natural language processing tools to develop a system that extracts concepts from triage notes for biosurveillance.           Project Narrative Relevance: The public health system is responsible for monitoring large amounts of timely, electronic health data and needs more sophisticated tools to faciliate detection of, and response to, emerging infectious diseases and potential bioterrorism threats. The proposed project addresses this need by developing a system to extract relevant information from emergency department records.",Adapting Natural Language Processing Tools for Biosurveillance,7693117,G08LM009787,"['Abbreviations', 'Accident and Emergency department', 'Acute', 'Address', 'American', 'Avian Influenza', 'Bioterrorism', 'Bird Flu vaccine', 'Breathing', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Coughing', 'Country', 'Data', 'Data Element', 'Data Quality', 'Detection', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Early Diagnosis', 'Electronic Health Record', 'Electronics', 'Emergency Medicine', 'Emergency Situation', 'Emerging Communicable Diseases', 'Epidemiologist', 'Evaluation', 'Event', 'Fever', 'Genetic Transcription', 'Goals', 'Gold', 'Health', 'Health Services', 'Health system', 'Hour', 'Intervention', 'Investigation', 'Manuals', 'Measures', 'Medical', 'Methods', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'North Carolina', 'Nurses', 'Patients', 'Performance', 'Pertussis', 'Physicians', 'Predictive Value', 'Process', 'Public Health', 'Radiology Specialty', 'Recording of previous events', 'Records', 'Reporting', 'Sampling', 'Screening procedure', 'Semantics', 'Sensitivity and Specificity', 'Shortness of Breath', 'Signal Transduction', 'Smallpox', 'Structure', 'Symptoms', 'System', 'Techniques', 'Temperature', 'Text', 'Translating', 'Triage', 'Variant', 'acronyms', 'base', 'experience', 'improved', 'lexical', 'mortality', 'pandemic disease', 'population based', 'prototype', 'research to practice', 'response', 'satisfaction', 'syntax', 'tool']",NLM,UNIV OF NORTH CAROLINA CHAPEL HILL,G08,2009,145926,0.054751237238870425
"Natural Language Processing for Cancer Research Network Surveillance Studies    DESCRIPTION (provided by applicant): This application addresses Broad Challenge Area: (10) Information Technology for Processing Health Care Data and specific Challenge Topic: 10-CA-107 Expand Spectrum of Cancer Surveillance through Informatics Approaches. The proposed project launches a collaborative effort to advance adoption within the HMO Cancer Research Network (CRN) of ""industrial-strength"" natural language processing (NLP) systems useful for mining valuable, research-grade information from unstructured clinical text. Such text is available for processing, now in the electronic medical record (EMR) systems of affiliated CRN health plans. The proposed NLP methods   will create ongoing capacity to tap what has recently been described as ""a treasure trove of historical   unstructured data that provides essential information for the study of disease progression, treatment   effectiveness and long-term outcomes"" (5). The vision of advancing widespread NLP capacity across the CRN, as well as the approach we present here for implementing it, grew out of an in-depth strategic planning effort we completed in December 2008. That effort involved participants from six CRN sites guided by a blue-ribbon panel of NLP experts from three of the nation's leading centers of clinical NLP research: University of Pittsburgh Medical Center, Vanderbilt University, and Mayo Clinic. The vision is to deploy a powerful NLP system locally, manage it with newly hired and trained local NLP technical staff, and conduct NLP-based research projects initiated by local investigators, in consultation with higher-level external NLP experts. Our planning efforts suggest this collaborative model is feasible; we will test the model in the context of the proposed project. An important development in April 2009 yielded what we believe is a potentially transformative opportunity to accelerate adoption of NLP capacity in applied research settings: release of the open-source Clinical Text Analysis and Knowledge Extraction System (cTAKES) software. This software was the result of a collaborative effort between IBM and Mayo Clinic. Built on the same framework Mayo Clinic currently uses to process its repository of over 40 million clinical documents, cTAKES dramatically lowers the cost of adopting a comprehensive and flexible NLP system. Deployment and use of such systems was previously only feasible in institutions with large, academically-oriented biomedical informatics research programs.   Still, other deployment challenges and the need to acquire NLP training for local staff present residual   barriers to adopting comprehensive NLP systems such as cTAKES. In collaboration with five other CRN sites the proposed project mitigates these challenges in two ways: 1) it develops configurable open-source software modules needed to streamline and therefore reduce the cost of deploying cTAKES, and 2) it presents and tests a model for training local staff through hands-on NLP projects overseen by outside NLP expert consultants. The potential impact of this project is evident most clearly in the vast untapped opportunities for text mining represented in CRN-affiliated health plans, where EMR systems have been in place since at least 2005, and whose patients represent 4% of the U.S. population. Clinical text mining offers the potential to provide new or improved data elements for cancer surveillance and other types of research requiring information about patient functional status, medication side-effects, details of therapeutic approaches, and differential information about clinical findings. Another significant impact of this project is its plan to integrate into the cTAKES system   an open-source de-identification tool based on state of the art, best of breed NLP approaches developed by the MITRE Corporation. De-identification of clinical text will make it easier for researchers to get access to clinical text, and will also facilitate multi-site collaborations while protecting patient privacy. Finally, if successful, the NLP algorithm we propose as a proof-of-principle project at Group Health-which will classify sets of patient charts as either containing or not containing a diagnosis of recurrent breast cancer-could dramatically reduce the cost of research in this area; currently all recurrent breast cancer endpoints must be established through costly manual chart abstraction.   Novel aspects of the proposed project include its talented and transdisciplinary research team,   including national experts in NLP, and its resourceful strategy for building the technical resources and ""human capital"" needed to support an ongoing program of applied NLP research. Natural language processing is itself a highly innovative technology; when successfully established in multiple CRN in the future it will represent a watershed moment in the CRN's already impressive history of exploiting data systems to support innovative research. Newly hired staff positions total approximately 2.0 FTE in each project year, most of which we anticipate will be supported by ongoing new research programs after the proposed project concludes. Project narrative The proposed project develops new measurement technologies for extracting information about disease processes and treatment, currently documented only in clinical text, based on natural language processing approaches. Because these methods are generic they will potentially contribute to public health by advancing research in a wide variety of areas. The ""proof of principle"" algorithm developed in the project to identify recurrent breast cancer diagnoses will advance epidemiologic and clinical research pertaining to the 2.5 million women currently living with breast cancer.           Project narrative The proposed project develops new measurement technologies for extracting information about disease processes and treatment, currently documented only in clinical text, based on natural language processing approaches. Because these methods are generic they will potentially contribute to public health by advancing research in a wide variety of areas. The ""proof of principle"" algorithm developed in the project to identify recurrent breast cancer diagnoses will advance epidemiologic and clinical research pertaining to the 2.5 million women currently living with breast cancer.",Natural Language Processing for Cancer Research Network Surveillance Studies,7839706,RC1CA146917,"['Address', 'Adopted', 'Adoption', 'Adverse effects', 'Algorithms', 'Applied Research', 'Area', 'Arts', 'Bioinformatics', 'Breeding', 'Cancer Research Network', 'Charge', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Complex', 'Comprehensive Health Care', 'Computer software', 'Computerized Medical Record', 'Consultations', 'Data', 'Data Element', 'Data Quality', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Doctor of Philosophy', 'Environment', 'Exercise', 'Future', 'Generic Drugs', 'Hand', 'Health', 'Health Planning', 'Health system plans', 'Healthcare', 'Human Resources', 'Individual', 'Informatics', 'Information Systems', 'Information Technology', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Learning', 'Licensing', 'Life', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Mining', 'Modeling', 'NCI Center for Cancer Research', 'Natural Language Processing', 'Operating System', 'Outcome', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Positioning Attribute', 'Process', 'Public Health', 'Recording of previous events', 'Recurrence', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Resources', 'Risk', 'Site', 'Solutions', 'Strategic Planning', 'System', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Training', 'Treatment Effectiveness', 'Universities', 'Vision', 'Woman', 'base', 'biomedical informatics', 'breast cancer diagnosis', 'cost', 'design', 'experience', 'feeding', 'firewall', 'flexibility', 'functional status', 'human capital', 'improved', 'innovation', 'innovative technologies', 'malignant breast neoplasm', 'novel', 'open source', 'patient privacy', 'programs', 'repository', 'skills', 'software systems', 'surveillance study', 'text searching', 'tool']",NCI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,RC1,2009,497857,0.05129238784276336
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",7870862,R01LM010016,"['Adverse event', 'Affect', 'Arts', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,172278,0.038551481675468305
"Investigating the generalizability of natural language processing of EMR data    DESCRIPTION (provided by applicant):       The electronic medical record (EMR) offers impressive opportunities for increasing care quality, but challenges stand in the way of realizing this vision. For example, coded EMR data readily available for analysis typically are incomplete (due to the prevalence of free-text clinical notes in EMR implementations), and data from different EMRs are often incommensurate due to differences in standard vocabularies and system implementations. While informatics research has shown the feasibility of automatically coding specific aspects of clinical text using Natural Language Processing (NLP), challenges remain for translating these informatics developments into large-scale care quality assessments. To date, successful NLP solutions for automated quality assessment have tended to be applications that are specific to (a) the target problem or clinical focus, (b) the EMR data system, and (c) the person or team that implements the NLP solution. In this study, we propose to begin addressing the problem of implementation team specificity by developing, evaluating, and making freely available a generalizable NLP development tool suite. The tools will enable widespread adoption of NLP systems to extract and code data from free text clinical notes. The Knowledge Editing Toolkit will simplify development of problem-specific knowledge by helping the user define the rules, concepts, and terms that constitute a domain-specific knowledge module, thus allowing any informaticist to develop an NLP application. The NLP Application Validation Toolkit will allow rapid testing and evaluation of the application against a gold standard of independently-coded test records from any EMR. To evaluate the effects of the toolkits on NLP generalizability, we will have three clinical informaticists each build two NLP applications (for a total of six distinct applications). One of their applications will identify a constellation of common clinical signs or symptoms (e.g., ""persistent cough"") that are relatively discrete concepts using simple language terms for many different clinical purposes. Their second application will assess behavioral counseling (e.g., ""alcohol counseling""), which uses complex language constructs for dedicated clinical purposes. We will describe and evaluate the accuracy of the solutions against independently coded test sets of medical records. We will quantify and compare the difficulty of creating these solutions as measured by the time, number of iterations required to build the applications, and the number of concepts and rules employed, as well as analyze variability in content and accuracy of the solutions created. In addition, we will use qualitative techniques to assess the ease of using the development tools; the difficulty in learning the tools; and specific types of problems, limitations, and bugs encountered. Such an NLP development tool suite has the potential to allow simple, elegant, and reliably good NLP solutions regardless of the clinical problem domain or the person developing the solution.           n/a",Investigating the generalizability of natural language processing of EMR data,7691692,R21LM009728,"['Address', 'Adopted', 'Adoption', 'Affect', 'Alcohols', 'Architecture', 'Behavioral', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Code', 'Complex', 'Computer Systems Development', 'Computerized Medical Record', 'Coughing', 'Counseling', 'Data', 'Databases', 'Development', 'Event', 'Gold', 'Healthcare Systems', 'Human Resources', 'Informatics', 'Information Systems', 'Information Technology', 'Institute of Medicine (U.S.)', 'Knowledge', 'Language', 'Learning', 'Measures', 'Medical Records', 'Modification', 'Natural Language Processing', 'Performance', 'Persons', 'Positioning Attribute', 'Prevalence', 'Process', 'Proliferating', 'Quality of Care', 'Records', 'Research', 'Running', 'Solutions', 'Source', 'Specificity', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Translating', 'United States National Academy of Sciences', 'Ursidae Family', 'Validation', 'Vision', 'Vocabulary', 'care delivery', 'cost', 'design', 'evaluation/testing', 'experience', 'health care quality', 'stem', 'success', 'tool', 'tool development', 'web site']",NLM,KAISER FOUNDATION RESEARCH INSTITUTE,R21,2009,177750,0.01193339100631269
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",7631876,R01LM010016,"['Adverse event', 'Affect', 'Arts', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,344239,0.038551481675468305
"Pharmacovigilence using Natural Language Processing, Statistics, and the EHR    DESCRIPTION (provided by applicant):       The long-term objective of this proposal is to advance patient safety and reduce the cost of medical care by discovering novel adverse drug events (ADEs) through use of automated methods. We will utilize natural language processing (NLP) and data mining methodologies on vast quantities of clinical data in electronic health records (EHRs) to detect novel ADE signals. ADEs are major problems world-wide and cause hospitalizations, deaths, and incur a huge cost to health care. Therefore, continued post-marketing surveillance encompassing large and varied patient populations is crucial for patient safety. EHRs contain a comprehensive amount of clinical information, which if harnessed properly, would be invaluable for pharmacovigilance. We have already demonstrated that we can accurately encode information in clinical reports using the NLP system MedLEE, and that we can accurately detect associations among clinical events using statistical methods that we developed. Therefore, this is an excellent opportunity to continue our research accomplishments and to advance the state of the art in pharmacovigilance.       More specifically, MedLEE will be used to map comprehensive clinical information in the EHR to codified data, and then statistical methods will be used to generate an extensive knowledge base of disease-symptom, disease-drug, drug-drug, and drug-symptom associations, which will be used to discover new ADEs. Additionally, we will develop methods to determine the correct sequence of drug, disease, and symptom events, which is critical for detecting ADEs. We will also develop methods to map fine-grained concepts into higher level concepts, which is important for optimizing the statistical methods. The performance of our discovery methods will be evaluated by testing the methods using drugs currently in use with known ADEs, and also by using historical rollback. We will first focus on discovery of short-term events using inpatient records, and then longer-term events using outpatient office visits.       This proposal is well positioned to overcome problems associated with existing automated methods based on spontaneous reporting databases and administrative databases. We are confident the methods will be effective because a strong infrastructure is in place for us to build upon. Most importantly, the methodology developed in this proposal presents an excellent chance to dramatically improve patient safety and reduce costs.               This proposal aims to improve patient safety and reduce health care costs by developing effective methods for the discovery of new adverse drug events. The use of natural language processing on vast quantities of EHR records will result in the harnessing of comprehensive clinical information for this purpose, overcoming some of the limitations of current methods that rely on spontaneous reporting and administrative databases.","Pharmacovigilence using Natural Language Processing, Statistics, and the EHR",7937173,R01LM010016,"['Adverse event', 'Affect', 'Arts', 'Back', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Data', 'Databases', 'Detection', 'Disease', 'Drug usage', 'Electronic Health Record', 'Event', 'Grouping', 'Health', 'Health Care Costs', 'Healthcare', 'Hospitalization', 'Inpatients', 'Knowledge', 'Maps', 'Marketing', 'MeSH Thesaurus', 'Medical', 'Medical Care Costs', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Office Visits', 'Outpatients', 'Output', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Signal Transduction', 'Source', 'Statistical Methods', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Unified Medical Language System', 'United States National Library of Medicine', 'administrative database', 'base', 'cost', 'data mining', 'drug testing', 'improved', 'knowledge base', 'novel', 'patient population', 'patient safety', 'post-market', 'statistics', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,172210,0.038551481675468305
"Investigating the generalizability of natural language processing of EMR data    DESCRIPTION (provided by applicant):       The electronic medical record (EMR) offers impressive opportunities for increasing care quality, but challenges stand in the way of realizing this vision. For example, coded EMR data readily available for analysis typically are incomplete (due to the prevalence of free-text clinical notes in EMR implementations), and data from different EMRs are often incommensurate due to differences in standard vocabularies and system implementations. While informatics research has shown the feasibility of automatically coding specific aspects of clinical text using Natural Language Processing (NLP), challenges remain for translating these informatics developments into large-scale care quality assessments. To date, successful NLP solutions for automated quality assessment have tended to be applications that are specific to (a) the target problem or clinical focus, (b) the EMR data system, and (c) the person or team that implements the NLP solution. In this study, we propose to begin addressing the problem of implementation team specificity by developing, evaluating, and making freely available a generalizable NLP development tool suite. The tools will enable widespread adoption of NLP systems to extract and code data from free text clinical notes. The Knowledge Editing Toolkit will simplify development of problem-specific knowledge by helping the user define the rules, concepts, and terms that constitute a domain-specific knowledge module, thus allowing any informaticist to develop an NLP application. The NLP Application Validation Toolkit will allow rapid testing and evaluation of the application against a gold standard of independently-coded test records from any EMR. To evaluate the effects of the toolkits on NLP generalizability, we will have three clinical informaticists each build two NLP applications (for a total of six distinct applications). One of their applications will identify a constellation of common clinical signs or symptoms (e.g., ""persistent cough"") that are relatively discrete concepts using simple language terms for many different clinical purposes. Their second application will assess behavioral counseling (e.g., ""alcohol counseling""), which uses complex language constructs for dedicated clinical purposes. We will describe and evaluate the accuracy of the solutions against independently coded test sets of medical records. We will quantify and compare the difficulty of creating these solutions as measured by the time, number of iterations required to build the applications, and the number of concepts and rules employed, as well as analyze variability in content and accuracy of the solutions created. In addition, we will use qualitative techniques to assess the ease of using the development tools; the difficulty in learning the tools; and specific types of problems, limitations, and bugs encountered. Such an NLP development tool suite has the potential to allow simple, elegant, and reliably good NLP solutions regardless of the clinical problem domain or the person developing the solution.           n/a",Investigating the generalizability of natural language processing of EMR data,7850343,R21LM009728,"['Address', 'Adopted', 'Adoption', 'Affect', 'Alcohols', 'Architecture', 'Behavioral', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Code', 'Complex', 'Computer Systems Development', 'Computerized Medical Record', 'Coughing', 'Counseling', 'Data', 'Databases', 'Development', 'Event', 'Gold', 'Healthcare Systems', 'Human Resources', 'Informatics', 'Information Systems', 'Information Technology', 'Institute of Medicine (U.S.)', 'Knowledge', 'Language', 'Learning', 'Measures', 'Medical Records', 'Modification', 'Natural Language Processing', 'Performance', 'Persons', 'Positioning Attribute', 'Prevalence', 'Process', 'Proliferating', 'Quality of Care', 'Records', 'Research', 'Running', 'Solutions', 'Source', 'Specificity', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Translating', 'United States National Academy of Sciences', 'Ursidae Family', 'Validation', 'Vision', 'Vocabulary', 'care delivery', 'cost', 'design', 'evaluation/testing', 'experience', 'health care quality', 'stem', 'success', 'tool', 'tool development', 'web site']",NLM,KAISER FOUNDATION RESEARCH INSTITUTE,R21,2009,99971,0.01193339100631269
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7672256,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Readability', 'Reader', 'Reading', 'Self Care', 'Self Management', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'fourth grade', 'improved', 'instrument', 'literacy', 'ninth grade', 'patient oriented', 'prevent', 'programs', 'tenth grade', 'tool', 'web site']",NIDDK,UNIVERSITY OF UTAH,R01,2009,398216,0.029693007478622035
"Sematic Relatedness for Active Medication Safety and Outcomes Surveillance    DESCRIPTION (provided by applicant):       Medication-related morbidity and mortality in ambulatory care in the United States results in estimated 100,000 deaths and $177 billion spending annually. Post-marketing passive surveillance of outcomes associated with medication use has been recognized as a necessary component in drug safety monitoring to overcome the limitations of pre- marketing clinical trials. Information technology applied to the patient's electronic medical and therapeutic record holds promise to improve this situation by detecting alarming trends in signs and symptoms in patient populations exposed to the same medication. Currently, much of the information necessary for active drug safety surveillance is ""locked"" in the unstructured text of electronic records. Our long-term goal is to develop information technology to recognize and prevent drug therapy related adverse events. Sophisticated natural language processing systems have been developed to find medical terms and their synonyms in the unstructured text and use them to retrieve information. In order to monitor alarming trends in symptoms in medical records, we need mechanisms that will allow not only accurate term and concept identification but also grouping of semantically related concepts that may not necessarily be synonymous. Measures of semantic relatedness rely on existing ontologies of domain knowledge as well as large textual corpora to compute a numeric score indicating the strength of relatedness between two concepts. Our central hypothesis is that such measures will be able to make fine-grained distinctions among concepts in the biomedical text, and provide a foundation upon which to organize concepts into meaningful groups automatically. In particular, this proposal seeks to develop methods that leverage the medical knowledge contained within Unified Medical Language System (UMLS) and corpora of clinical text. Our short-term goals are 1) develop new methods, specific to clinical text, for computing semantic relatedness 2) integrate these specific methods for computing semantic relatedness into more general methods of natural language processing 3) integrate semantic relatedness into methods for identifying labeled semantic relations in clinical text. Labeled relations significantly enhance the ability of natural language processing to support accurate automatic analysis of medical information for improving patient safety. Our next step will be to develop and validate a generalizable active medication safety surveillance system that will automatically track medication exposure and alarming trends in signs and symptoms in ambulatory and hospitalized populations for a broad range of diseases.           This project will a) create and validate a common open-source platform for developing and testing semantic relatedness measures, b) determine the validity of electronic medical records with respect to identification of symptoms associated with medication- related problems and c) develop a novel methodology to aggregate adverse reaction terms used to code spontaneous post-marketing drug safety surveillance reports. The results of this project will enable more effective medication safety surveillance efforts and thus will improve patient safety.",Sematic Relatedness for Active Medication Safety and Outcomes Surveillance,7691699,R01LM009623,"['Address', 'Adverse effects', 'Adverse event', 'Adverse reactions', 'Ambulatory Care', 'Angina Pectoris', 'Area', 'Body of uterus', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical assessments', 'Code', 'Computational Technique', 'Computerized Medical Record', 'Databases', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Effectiveness', 'Electronics', 'Exposure to', 'Foundations', 'Generic Drugs', 'Goals', 'Group Identifications', 'Grouping', 'Health', 'Healthcare', 'Heart failure', 'Information Technology', 'Knowledge', 'Label', 'Linguistics', 'Mandatory Reporting', 'Manuals', 'Maps', 'Marketing', 'Measures', 'Medical', 'Medical Electronics', 'Medical History', 'Medical Informatics', 'Medical Records', 'Methodology', 'Methods', 'Minnesota', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Nature', 'One-Step dentin bonding system', 'Ontology', 'Outcome', 'Patients', 'Pharmaceutical Cares', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Pharmacy facility', 'Physicians', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Procedures', 'Process', 'Reaction', 'Records', 'Reference Standards', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Safety', 'Semantics', 'Signs and Symptoms', 'Statistical Models', 'Symptoms', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Therapeutic Effect', 'Time', 'Training', 'Unified Medical Language System', 'United States', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Validation', 'Work', 'Writing', 'base', 'computer science', 'data mining', 'design', 'experience', 'flexibility', 'improved', 'information organization', 'knowledge base', 'metathesaurus', 'mortality', 'novel', 'open source', 'patient population', 'patient safety', 'phrases', 'post-market', 'practice-based research network', 'prevent', 'social', 'treatment planning', 'trend']",NLM,UNIVERSITY OF MINNESOTA,R01,2009,311821,0.0432793134044701
"Sematic Relatedness for Active Medication Safety and Outcomes Surveillance    DESCRIPTION (provided by applicant):       Medication-related morbidity and mortality in ambulatory care in the United States results in estimated 100,000 deaths and $177 billion spending annually. Post-marketing passive surveillance of outcomes associated with medication use has been recognized as a necessary component in drug safety monitoring to overcome the limitations of pre- marketing clinical trials. Information technology applied to the patient's electronic medical and therapeutic record holds promise to improve this situation by detecting alarming trends in signs and symptoms in patient populations exposed to the same medication. Currently, much of the information necessary for active drug safety surveillance is ""locked"" in the unstructured text of electronic records. Our long-term goal is to develop information technology to recognize and prevent drug therapy related adverse events. Sophisticated natural language processing systems have been developed to find medical terms and their synonyms in the unstructured text and use them to retrieve information. In order to monitor alarming trends in symptoms in medical records, we need mechanisms that will allow not only accurate term and concept identification but also grouping of semantically related concepts that may not necessarily be synonymous. Measures of semantic relatedness rely on existing ontologies of domain knowledge as well as large textual corpora to compute a numeric score indicating the strength of relatedness between two concepts. Our central hypothesis is that such measures will be able to make fine-grained distinctions among concepts in the biomedical text, and provide a foundation upon which to organize concepts into meaningful groups automatically. In particular, this proposal seeks to develop methods that leverage the medical knowledge contained within Unified Medical Language System (UMLS) and corpora of clinical text. Our short-term goals are 1) develop new methods, specific to clinical text, for computing semantic relatedness 2) integrate these specific methods for computing semantic relatedness into more general methods of natural language processing 3) integrate semantic relatedness into methods for identifying labeled semantic relations in clinical text. Labeled relations significantly enhance the ability of natural language processing to support accurate automatic analysis of medical information for improving patient safety. Our next step will be to develop and validate a generalizable active medication safety surveillance system that will automatically track medication exposure and alarming trends in signs and symptoms in ambulatory and hospitalized populations for a broad range of diseases.           This project will a) create and validate a common open-source platform for developing and testing semantic relatedness measures, b) determine the validity of electronic medical records with respect to identification of symptoms associated with medication- related problems and c) develop a novel methodology to aggregate adverse reaction terms used to code spontaneous post-marketing drug safety surveillance reports. The results of this project will enable more effective medication safety surveillance efforts and thus will improve patient safety.",Sematic Relatedness for Active Medication Safety and Outcomes Surveillance,7908950,R01LM009623,"['Address', 'Adverse effects', 'Adverse event', 'Adverse reactions', 'Ambulatory Care', 'Angina Pectoris', 'Area', 'Body of uterus', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical assessments', 'Code', 'Computational Technique', 'Computerized Medical Record', 'Databases', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Effectiveness', 'Electronics', 'Exposure to', 'Foundations', 'Generic Drugs', 'Goals', 'Group Identifications', 'Grouping', 'Health', 'Healthcare', 'Heart failure', 'Information Technology', 'Knowledge', 'Label', 'Linguistics', 'Mandatory Reporting', 'Manuals', 'Maps', 'Marketing', 'Measures', 'Medical', 'Medical Electronics', 'Medical History', 'Medical Informatics', 'Medical Records', 'Methodology', 'Methods', 'Minnesota', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Nature', 'One-Step dentin bonding system', 'Ontology', 'Outcome', 'Patients', 'Pharmaceutical Cares', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Pharmacy facility', 'Physicians', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Procedures', 'Process', 'Reaction', 'Records', 'Reference Standards', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Safety', 'Semantics', 'Signs and Symptoms', 'Statistical Models', 'Symptoms', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Therapeutic Effect', 'Time', 'Training', 'Unified Medical Language System', 'United States', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Validation', 'Work', 'Writing', 'base', 'computer science', 'data mining', 'design', 'experience', 'flexibility', 'improved', 'information organization', 'knowledge base', 'metathesaurus', 'mortality', 'novel', 'open source', 'patient population', 'patient safety', 'phrases', 'post-market', 'practice-based research network', 'prevent', 'social', 'treatment planning', 'trend']",NLM,UNIVERSITY OF MINNESOTA,R01,2009,17240,0.0432793134044701
"Integrated discovery and hypothesis testing of new associations in rare diseases    DESCRIPTION (provided by applicant): Rare diseases are studied in isolated laboratories, forgotten by main stream pharmacological companies, and considered almost academic curiosities. Finding variables that correlate/cause rare diseases (a condition is rare when it affects less than 1 person per 2,000) is a difficult task. The low number of cases and the sparse nature of the reports make it difficult to obtain significant/meaningful statistical results. There are two ways to avoid these problems. The first is to integrate reported cases and associations to generate enough statistical power. The second way is to have an independent data set, big enough to cover rare cases. Each of the two methods has intrinsic problems. For instance, the search in the literature puts together different studies, each of them with their own biases in population, methodology and objectives. On the other hand, blind searches for associations in big databases introduce a large number of false positives due to multiple hypothesis testing.       These problems could be avoided by developing innovative methods that allow the integration of information and methodologies in the literature and longitudinal databases. To achieve this goal, we propose a team that combines expertise in natural language processing systems (Carol Friedman), electronic health records (George Hripcsak), statistics in combined databases and computational virology (Raul Rabadan). This team will generate an interdisciplinary approach to mine and integrate the literature and the dataset collected at Columbia/New York Presbyterian hospital. Identifying unusual correlations in rare diseases is the first step to understanding the origin of the diseases and to finding a cure for them. We hypothesize that we will develop effective methods aimed at improving our understanding of rare diseases by combining hypothesis testing and hypothesis discovery, and by integrating information from the literature and from the patient record to obtain increased statistical power. This will involve using natural language processing and statistical methods to mine both the literature and the electronic health record (EHR).           Project Narrative We will test reported associations in rare diseases and discover new ones by integrating information from the literature and from Electronic Health Records in hospitals.",Integrated discovery and hypothesis testing of new associations in rare diseases,7727710,R01LM010140,"['Acquired Immunodeficiency Syndrome', 'Affect', 'Case Study', 'Cells', 'Clinical', 'Code', 'Computer software', 'Curiosities', 'Data', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Electronics', 'Environmental Risk Factor', 'Evaluation', 'Frequencies', 'Goals', 'Hand', 'Hospitals', 'Immunocompromised Host', 'Incidence', 'Individual', 'Inequality', 'Informatics', 'Information Theory', 'Kaposi Sarcoma', 'Kidney Diseases', 'Laboratories', 'Link', 'Literature', 'Liver diseases', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Nature', 'New York', 'Patients', 'Pattern', 'Persons', 'Population', 'Presbyterian Church', 'Process', 'PubMed', 'Rare Diseases', 'Records', 'Reporting', 'Research', 'Source', 'Statistical Methods', 'Stratification', 'Stream', 'Stress', 'System', 'Techniques', 'Testing', 'Text', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Virus', 'Work', 'Writing', 'abstracting', 'base', 'blind', 'data mining', 'forgetting', 'improved', 'innovation', 'interdisciplinary approach', 'longitudinal database', 'novel', 'pathogen', 'repository', 'research study', 'statistics', 'text searching', 'tool', 'virology', 'web site']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,533007,0.01150187302422643
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,7653874,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Databases', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,357875,-0.00900700261742695
"MOMENT (Monitoring for Outpatient Medication Effects and New Toxicities) in TIME    DESCRIPTION (provided by applicant):       The MOMENT (Monitoring for Outpatient Medication Effects and New Toxicities) in TIME project will extend the in-progress TIME (Tools for Inpatient Monitoring using Evidence) for Safe and Appropriate Testing grant, #5 R01 LM007995-03, by developing sophisticated text-mining and data extraction tools to examine adverse drug effects in patients presenting for emergency and hospital care.       Using a UMLS-based concept identifier enhanced with natural language processing, the MOMENT project will detect: (a) individual clinical manifestations (symptoms and physical exam findings) that are potentially drug related; (b) clinical syndromes (e.g., hepatoxicity, myopathy, renal insufficiency, glucose intolerance); and, (c) and clinical diseases (such as systemic lupus induced by hydralazine or procainamide, or acute myocardial infarction associated with rofecoxib). Innovative aspects of the proposed work include use of advanced natural language processing techniques to abstract concepts representing potential drug effects from patient history and physical examination records and ancillary test reports, the combination of evidence based templates and diagnostic algorithms to detect complex patterns of drug toxicity (e.g., hepatocelluar damage, pulmonary fibrosis, or acute coronary syndrome), advanced statistical methods for determining medication effects in large populations, and utilization of a clinical database containing data on more than 100,000 patients.      Public Statement      The MOMENT project (Monitoring for Outpatient Medication Effects and New Toxicities) will demonstrate the feasibility of combining informatics applications increasingly present in many healthcare institutions - electronic medical record systems and care provider order entry (CPOE) systems - to create an advanced detection and monitoring system for adverse medication effects. The project will develop tools and algorithms to detect the patient symptoms, physical exam signs, test results, and medical diagnoses that may indicate drug-induced injury.          n/a",MOMENT (Monitoring for Outpatient Medication Effects and New Toxicities) in TIME,7638001,R01LM007995,"['Abnormal Laboratory Test Result', 'Accreditation', 'Acute myocardial infarction', 'Admission activity', 'Adverse drug effect', 'Adverse effects', 'Algorithms', 'Ambulatory Monitoring', 'Basic Science', 'Caring', 'Chest Pain', 'Classification', 'Clinical', 'Code', 'Complex', 'Computer Assisted', 'Computerized Medical Record', 'Congenital Abnormality', 'Coronary heart disease', 'Data', 'Databases', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Disease', 'Diuretics', 'Drug Delivery Systems', 'Drug Exposure', 'Drug Prescriptions', 'Drug Utilization', 'Drug toxicity', 'Drug usage', 'Edema', 'Electronic Health Record', 'Emergency Situation', 'Evaluation', 'Event', 'Frequencies', 'Glucose Intolerance', 'Grant', 'Healthcare', 'Hepatotoxicity', 'Hormones', 'Hospitalization', 'Hospitals', 'Hour', 'Hydralazine', 'Image', 'Inappropriate ADH Syndrome', 'Individual', 'Informatics', 'Injury', 'Inpatients', 'Institution', 'Joints', 'Kidney Failure', 'Knowledge', 'Laboratories', 'Letters', 'Link', 'Literature', 'Liver Cirrhosis', 'Lower Extremity', 'Lupus', 'Manufacturer Name', 'Marketing', 'Medical', 'Methods', 'Methotrexate', 'Modality', 'Monitor', 'Myopathy', 'Natural Language Processing', 'Non-Prescription Drugs', 'Outcome', 'Outpatients', 'Pathology', 'Patients', 'Pattern', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Phase II/III Trial', 'Physical Examination', 'Population', 'Predictive Value', 'Procainamide', 'Process', 'Provider', 'Pulmonary Fibrosis', 'Recording of previous events', 'Records', 'Registries', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Rofecoxib', 'Sampling', 'Screening procedure', 'Sentinel', 'Severities', 'Site', 'Statistical Methods', 'Symptoms', 'Syndrome', 'System', 'Techniques', 'Test Result', 'Testing', 'Text', 'Time', 'TimeLine', 'Toxic effect', 'United States Food and Drug Administration', 'Universities', 'Vocabulary', 'Work', 'abstracting', 'acute coronary syndrome', 'base', 'care systems', 'clinical practice', 'evidence base', 'healthy volunteer', 'impression', 'innovation', 'knowledge base', 'member', 'mortality', 'novel', 'patient population', 'post-market', 'programs', 'telithromycin', 'text searching', 'tool']",NLM,VANDERBILT UNIVERSITY,R01,2009,374185,0.018820019521353735
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7908946,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'meetings', 'natural language', 'population based', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2009,71200,0.01447404390852661
"Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach    DESCRIPTION (provided by applicant):       The long-term objective of this research is to increase the clinical trial enrollment of US patients via a semi- automated, Natural Language Processing (NLP) based, interactive and patient-centered informatics application. The study design is prospective observational study. Scope is limited to cancer patients. There are three specific aims for this project. The first aim is to identify concepts that overlap between the electronic medical record's (EMR) clinical notes and the free text of clinical trial announcements. The PI will use the concepts to develop mapping frames that connect concepts in the text of trial announcements to those found in clinical notes in the medical record. When he has the mapping frames he will build the NLP module for the application. In the software development work he will utilize as many publicly available software components as possible. He will experiment with UIMA, GATE, MetaMap, Stanford Parser, NegEx algorithm and others. The PI will develop the tool around the National Library of Medicine's Unified Medical Language System knowledgebase. He will use Java for programming. The second aim is to create an algorithm that automatically generates questions to request information directly from the patient if the information is not available or accessible in the records. The third aim is to evaluate the in-vitro, laboratory performance of the application. For performance evaluation purposes the PI will recruit cancer care specialists to generate the gold standard lists of eligible clinical trials for study patients. He will publicly release the developed code at the end of the grant period. This K99/R00 project will serve the foundation for future R01 grant applications. The PI is fully committed to become faculty in the Clinical Research Informatics domain with a specialization in biomedical NLP. The support of the K99/R00 grant will enable him to acquire substantial formal training in Computational Linguistics while contributing to the body of knowledge of the Clinical Research Informatics field. The five-year grant support will ensure success in his endeavor. The proposed work is highly significant because the dismal clinical trial accrual rates (2-4 % nationally) hampers timely development of new drugs. In addition, studies show that physicians have statistically significant bias against elderly and minority patients to invite participation in clinical trials. The proposed project is synergistic with physician-centered efforts but the goal is to provide individualized, EMR based clinical trial recommendations directly to the patients. The results of this research will empower the patients and elevate their role in the decision making process.           Relevance The long-term objective of this research is to increase the clinical trial enrollment of US patients via a semi- automated, Natural Language Processing (NLP) based, interactive and patient-centered informatics application. The proposed work is highly significant because the dismal clinical trial accrual rates (2-4 % nationally) hampers timely development of new drugs. In addition, studies show that physicians have statistically significant bias against elderly and minority patients to invite participation in clinical trials. The proposed project is synergistic with physician-centered efforts but the goal is to provide individualized, electronic medical record based clinical trial recommendations directly to the patients. The results of this research will empower patients and elevate their role in the decision making process.",Increasing Clinical Trial Enrollment: A Semi-Automated Patient Centered Approach,7770648,K99LM010227,"['Adult', 'Age', 'Algorithms', 'Applications Grants', 'Arts', 'Biomedical Research', 'Cancer Patient', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical trial protocol document', 'Code', 'Commit', 'Complement', 'Computer software', 'Computerized Medical Record', 'Decision Making', 'Development', 'Elderly', 'Elements', 'Eligibility Determination', 'Enrollment', 'Ensure', 'Equation', 'Evaluation', 'Faculty', 'Foundations', 'Future', 'Goals', 'Gold', 'Grant', 'Hand', 'In Vitro', 'Informatics', 'Java', 'Knowledge', 'Laboratories', 'Linguistics', 'Malignant Neoplasms', 'Maps', 'Medical Records', 'Medicine', 'Methods', 'Minority', 'Modification', 'Natural Language Processing', 'Newly Diagnosed', 'Observational Study', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Protocols documentation', 'Public Health Informatics', 'Publishing', 'Recommendation', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Role', 'Screening for cancer', 'Screening procedure', 'Specialist', 'Surgeon', 'Text', 'Training', 'Unified Medical Language System', 'United States National Library of Medicine', 'Work', 'base', 'cancer care', 'empowered', 'ethnic minority population', 'information organization', 'novel', 'older patient', 'patient oriented', 'programs', 'prospective', 'research study', 'software development', 'success', 'tool']",NLM,UNIVERSITY OF WASHINGTON,K99,2009,84306,0.013075958305654119
"NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports    DESCRIPTION (provided by applicant):       Many NLP applications have been successfully developed to extract information from text. Most of the   applications have focused on identifying individual clinical conditions in textual records, which is the first step in making the conditions available to computerized applications. However, identifying individual instances of clinical conditions is not sufficient for many medical informatics tasks - the context surrounding the condition is crucial for integrating the information within the text to determine the clinical state of a patient. We propose to perform in-depth studies on NLP issues requiring knowledge of the context of clinical conditions in clinical records. We will focus our research by using syndromic surveillance from emergency department (ED) reports as a case study.      For this proposal, we will test the following hypothesis: An NLP system that indexes clinical concepts and integrates contextual information modifying the concepts can identify acute clinical conditions from ED reports as well as physicians can.      We will identify clinical concepts necessary for surveillance of seven syndromes, including respiratory,   gastrointestinal, neurological, rash, hemorrhagic, constitutional, and botulinic. To evaluate the hypothesis, we will perform the following specific aims:      Aim 1. Perform in-depth, foundational studies on four NLP topics to gain a deeper understanding of the      pertinent NLP research capabilities required for identification of acute clinical conditions from ED reports, including negation, uncertainty, temporal discrimination, and finding validation;      Aim 2. Apply the knowledge learned from the foundational studies to develop and evaluate an automated application for ED reports that will determine the values for clinical variables relevant to identifying patients with any of seven syndromes.      The research is innovative, because it will generate an in-depth study of multiple NLP topics crucial to   understanding a patient's clinical state from textual records and will focus on contextual understanding and analysis. The research will be guided by linguistic principles, by the semantics and discourse structure of ED reports, and by the application area of biosurveillance. Because we will develop research methods and tools that are customized to a particular domain, we will constrain the research space, which will provide direction and enhance the chance for success. However, the methods and tools generated by this research should be extensible to other clinical report types and to other domain applications, because we will explicitly specify and study NLP concepts and relationships that are common to many application areas.             n/a",NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports,7908086,R01LM009427,"['Accident and Emergency department', 'Acute', 'Area', 'Case Study', 'Clinical', 'Clinical Data', 'Constitutional', 'Detection', 'Discrimination', 'Exanthema', 'Exhibits', 'Individual', 'Informatics', 'Knowledge', 'Learning', 'Linguistics', 'Medical Informatics', 'Medical Informatics Applications', 'Methods', 'Natural Language Processing', 'Neurologic', 'Ontology', 'Patients', 'Physicians', 'Records', 'Reporting', 'Research', 'Research Methodology', 'Semantics', 'Specific qualifier value', 'Structure', 'Syndrome', 'System', 'Techniques', 'Testing', 'Text', 'Uncertainty', 'Validation', 'computerized', 'gastrointestinal', 'indexing', 'innovation', 'quality assurance', 'respiratory', 'success', 'syndromic surveillance', 'tool']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,130902,0.07500989678566054
"NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports    DESCRIPTION (provided by applicant):       Many NLP applications have been successfully developed to extract information from text. Most of the   applications have focused on identifying individual clinical conditions in textual records, which is the first step in making the conditions available to computerized applications. However, identifying individual instances of clinical conditions is not sufficient for many medical informatics tasks - the context surrounding the condition is crucial for integrating the information within the text to determine the clinical state of a patient. We propose to perform in-depth studies on NLP issues requiring knowledge of the context of clinical conditions in clinical records. We will focus our research by using syndromic surveillance from emergency department (ED) reports as a case study.      For this proposal, we will test the following hypothesis: An NLP system that indexes clinical concepts and integrates contextual information modifying the concepts can identify acute clinical conditions from ED reports as well as physicians can.      We will identify clinical concepts necessary for surveillance of seven syndromes, including respiratory,   gastrointestinal, neurological, rash, hemorrhagic, constitutional, and botulinic. To evaluate the hypothesis, we will perform the following specific aims:      Aim 1. Perform in-depth, foundational studies on four NLP topics to gain a deeper understanding of the      pertinent NLP research capabilities required for identification of acute clinical conditions from ED reports, including negation, uncertainty, temporal discrimination, and finding validation;      Aim 2. Apply the knowledge learned from the foundational studies to develop and evaluate an automated application for ED reports that will determine the values for clinical variables relevant to identifying patients with any of seven syndromes.      The research is innovative, because it will generate an in-depth study of multiple NLP topics crucial to   understanding a patient's clinical state from textual records and will focus on contextual understanding and analysis. The research will be guided by linguistic principles, by the semantics and discourse structure of ED reports, and by the application area of biosurveillance. Because we will develop research methods and tools that are customized to a particular domain, we will constrain the research space, which will provide direction and enhance the chance for success. However, the methods and tools generated by this research should be extensible to other clinical report types and to other domain applications, because we will explicitly specify and study NLP concepts and relationships that are common to many application areas.             n/a",NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports,7660312,R01LM009427,"['Accident and Emergency department', 'Acute', 'Area', 'Case Study', 'Clinical', 'Clinical Data', 'Constitutional', 'Detection', 'Discrimination', 'Exanthema', 'Exhibits', 'Individual', 'Informatics', 'Knowledge', 'Learning', 'Linguistics', 'Medical Informatics', 'Medical Informatics Applications', 'Methods', 'Natural Language Processing', 'Neurologic', 'Ontology', 'Patients', 'Physicians', 'Records', 'Reporting', 'Research', 'Research Methodology', 'Semantics', 'Specific qualifier value', 'Structure', 'Syndrome', 'System', 'Techniques', 'Testing', 'Text', 'Uncertainty', 'Validation', 'computerized', 'gastrointestinal', 'indexing', 'innovation', 'quality assurance', 'respiratory', 'success', 'syndromic surveillance', 'tool']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,362514,0.07500989678566054
"Improving Outpatient Medication Lists Using Temporal Reasoning and Clinical Texts    DESCRIPTION (provided by applicant):  Accurate and complete medication lists are critical inputs to effective medication reconciliation to prevent medication prescribing and administration errors. Previous research aggregated structured medication data form multiple sources to generate and maintain a reconciled medication list. Medications documented in clinical texts also need to be reconciled. However, most reconciliation methods currently have limited capability to process textual data and temporal information (e.g., dates, duration and status). Our goal is to pilot and test methodologies and applications in the fields of natural language processing (NLP) and temporal reasoning to facilitate the use of electronic clinical texts in order to improve the ""correctness"" and ""completeness"" of medication lists. Clinic notes and free-text ""comments"" fields in medication lists in an ambulatory electronic medical record system will be considered in the study. An NLP system and a temporal reasoning system will be adapted to automatically extract medication and associated temporal information from clinical texts and encode the medications using a controlled terminology. Multiple knowledge bases will be used to develop a mechanism to represent the timing of medication use, detect the changes (e.g., active or inactive), and then to organize medications into appropriate groups (e.g., by ingredient or by status). The feasibility and efficiency of the proposed methods and tools in improving the process of medication   reconciliation will be assessed. Domain experts will serve as judges to assess the success of capturing, coding, and organizing the medications and temporal information and also to evaluate whether our methods are complementary to those currently used for medication management.           Accurate and complete medication information at the point of care is crucial for delivery of high-quality care and prevention of adverse events. Most previous studies aggregated structured medication data from EMR and CPOE (Computerized Physician Order Entry) systems to generate and maintain a reconciled medication list. However, medications in non-structured narrative sources (such as clinic notes and free-text comments) must also be reconciled. Structured data presented in a standard, predictable form can be easily processed by a computer. By contrast, narrative data does not have a well-defined structure, so processing such data is very challenging. Our goal is to pilot and test methodologies and applications in the fields of natural language processing (any system that manipulates text) and temporal reasoning (e.g., identifying the timing of medication use) to facilitate the use of electronic clinical texts in order to improve the ""correctness"" and ""completeness"" of medication lists. The feasibility and efficiency of the proposed methods and tools in improving the process of medication reconciliation will be assessed.",Improving Outpatient Medication Lists Using Temporal Reasoning and Clinical Texts,7774682,R03HS018288,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R03,2009,48782,0.04124427680297896
"Discovering and Applying Knowledge in Clinical Databases    DESCRIPTION (provided by applicant):  The ongoing goal of our project, ""Discovering and applying knowledge in clinical databases,"" is to develop and apply methods to exploit electronic medical record data for decision support, with an emphasis on narrative data. Since the inception of our project as an R29 in 1994, we have been developing methods for preparing raw electronic medical record data, applying and evaluating natural language processing, developing data mining techniques including machine learning, and putting the results to use for clinical care and research.       In this competing continuation, we propose to address the temporal information in the electronic medical record and to apply natural language processing and temporal processing to the task of syndromic surveillance in collaboration with the New York City Department of Health and Mental Hygiene (NYC DOHMH).       We have begun work on a temporal processing system. It extracts temporal assertions stated in narrative reports, uses the MedLEE natural language processor to parse the non-temporal information, infers implicit temporal assertions based on a knowledge base, and produces the information in the form of a simple temporal constraint satisfaction problem. The latter can be used to answer questions about the time of events and the temporal relation between pairs of events. We propose to complete the system, expand the knowledge base, speed computation, address the uncertainty of temporal assertions, incorporate temporal information from structured data, and evaluate the system.       NYC DOHMH has a mature syndromic surveillance system that watches over almost eight million persons, and it has as-yet unexploited data sources in the form of narrative and structured electronic medical records. We propose to apply natural language processing and our proposed temporal processing to convert the data to a form appropriate for surveillance. We will evaluate the incremental benefit of structured data, narrative data, and temporally processed narrative data.           n/a",Discovering and Applying Knowledge in Clinical Databases,7495030,R01LM006910,"['Address', 'Area', 'Caring', 'Cities', 'Clinical', 'Code', 'Collaborations', 'Computerized Medical Record', 'Data', 'Data Sources', 'Databases', 'Event', 'Goals', 'Health', 'Healthcare', 'Knowledge', 'Language', 'Machine Learning', 'Medical Surveillance', 'Mental Health', 'Methods', 'Natural Language Processing', 'New York City', 'Persons', 'Preparation', 'Process', 'Reporting', 'Research', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Uncertainty', 'Work', 'base', 'data mining', 'improved', 'knowledge base', 'satisfaction', 'syndromic surveillance']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2008,338600,0.03528039648750205
"Extracting Semantic Knowledge from Clinical Reports    DESCRIPTION (provided by applicant): Analyzing and processing free-text medical reports for data mining and clinical data interchange is one of the most challenging problems in medical informatics, yet it is crucial for continued research advances and improvements in clinical care. Natural language processing (NLP) is an important enabling technology, but has been held back because it is difficult to understand human language, since it requires extensive domain knowledge. In Phase I, we developed new statistical and machine learning methods that apply domain specific knowledge to the semantic analysis of free-text radiology reports. The methods enabled the creation of two new prototype applications - a SNOMED CT (Systematized Nomenclature of Medicine--Clinical Terms) coding service called SnomedCoder, and a text mining tool for analyzing a large corpus of medical reports, called DataMiner. In Phase II, we will accomplish the following specific aims: 1) Improve the semantic extraction methods developed in Phase I, 2) Expand the semantic knowledge base and classify at least two million new unique sentences from multiple medical institutions, 3) Provide a SNOMED CT auto coding service (alpha service) to participating Indiana Health Information Exchange hospitals, and 4) Build a commercial version of the DataMiner software, and test its functionality using researchers at the Regenstrief Institute.       These scientific innovations will revolutionize the ability of health care researchers to analyze vast repositories of clinical information currently locked up in electronic medical records, and correlate this data with new biomedical discoveries in proteonomics and genomics. The ability to codify text rapidly will extend the potential for clinical decision support beyond its narrow base of numeric and structured medical data, and enable SNOMED CT to become a useful coding standard. Phase III will offer coding and data mining services to healthcare payers (both private and government), pharmaceuticals, and academic researchers. A key advantage of our approach over other NLP systems is that we attempt to codify all the information in the report and not just a limited subset, and insist on expert validation which provides a high degree of confidence in the accuracy of the coded data.Project Narrative           n/a",Extracting Semantic Knowledge from Clinical Reports,7394699,R44RR024929,"['Address', 'Algorithms', 'Back', 'Bioinformatics', 'Body of uterus', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Code', 'Collection', 'Computer software', 'Computerized Medical Record', 'Data', 'Data Reporting', 'Decision Making', 'Effectiveness', 'Genomics', 'Goals', 'Government', 'Health', 'Healthcare', 'Hospitals', 'Human', 'Indiana', 'Institutes', 'Institution', 'Journals', 'Knowledge', 'Language', 'Longitudinal Studies', 'Machine Learning', 'Medical', 'Medical Informatics', 'Medical Records', 'Methods', 'Natural Language Processing', 'Paper', 'Pharmacologic Substance', 'Phase', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Services', 'Speed', 'Standards of Weights and Measures', 'Structure', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Testing', 'Text', 'Thinking', 'Trees', 'Trust', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Validation', 'base', 'computerized', 'data mining', 'health care quality', 'improved', 'indexing', 'innovation', 'knowledge base', 'novel strategies', 'patient safety', 'prototype', 'repository', 'research and development', 'success', 'text searching', 'tool']",NCRR,"LOGICAL SEMANTICS, INC.",R44,2008,429955,0.0054949989220600265
"A Biomedical Natural Language Processing Resource DESCRIPTION:    The long-term aim of this project is to advance clinical care and biomedical research by establishing a natural language processing (NLP) resource for the biomedical community. A major bottleneck for development of automated tools for clinical applications and biomedical research is that most of the data and knowledge occur in the form of text, resulting in a lack of coded data. This NLP resource will make possible a host of automated applications by enabling high throughput access to coded biomedical knowledge and data. The foundation of this resource will be the MedLEE NLP system, which has been used operationally for almost a decade in healthcare settings for a broad range of applications that have proven to be valuable for clinical care. The NLP resource will also include BioMedLEE (a derivative of MedLEE), which encodes genotypic-phenotypic (GP) relations in the scientific literature. It currently focuses on GP relations associated with cancer and infectious diseases, and is being used to organize the extracted information to facilitate research, curation, and ontological development within model organism databases. This proposal will enable us to 1) disseminate our NLP resource to the community, 2) conduct technological research and development (R&D) to facilitate expansion and adaptation of the resource to new applications and specialties, 3) conduct R&D of tools that facilitate use of the extracted data and knowledge after coding, and 4) promote the resource, and provide service to users in the form of technical support, documentation, and tutorials. MedLEE and BioMedLEE are extendable systems that encompass the clinical and scientific communities. The dissemination of a proven NLP system that is applicable to the entire biomedical community provides an exceptional opportunity for multiple developers and researchers to work to unleash the true potential of NLP technology, increasing development of applications that aim to enhance scientific research and improve all levels of health. n/a",A Biomedical Natural Language Processing Resource,7429768,R01LM008635,"['Address', 'Alzheimer&apos', 's Disease', 'Autistic Disorder', 'Biological', 'Biomedical Research', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Communicable Diseases', 'Communities', 'Computerized Medical Record', 'Condition', 'Data', 'Databases', 'Detection', 'Development', 'Discipline', 'Disease', 'Documentation', 'Educational workshop', 'Fostering', 'Foundations', 'Health Status', 'Healthcare', 'Human', 'Imagery', 'Improve Access', 'Internet', 'Knowledge', 'Literature', 'Malignant Neoplasms', 'Measures', 'Medical', 'Medical Errors', 'Medical Surveillance', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Numbers', 'Ontology', 'Organism', 'Output', 'Partner in relationship', 'Patient Care', 'Performance', 'Phenotype', 'Postdoctoral Fellow', 'Process', 'Range', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Services', 'Side', 'Source', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Terminology', 'Text', 'Universities', 'Work', 'biological research', 'biomedical resource', 'clinical application', 'concept', 'high throughput technology', 'improved', 'knowledge base', 'medical specialties', 'model organisms databases', 'open source', 'research and development', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2008,534463,0.018666939980149726
"Feasibility of a Natural Language Processing-based Dental Charting Application    DESCRIPTION (provided by applicant): The absence of a flexible, robust, and accurate natural language interface is a significant barrier to the direct use of computer-based patient records by dental clinicians. While providing patient care, dentists, hygienists and assistants are handicapped in using a keyboard and mouse to interact with a computer, primarily because of infection control concerns. The objective of this proposal is to develop and evaluate a prototype dental charting system with a speech-driven interface that will allow the dentist to chart dental conditions using natural language. The system will use Natural Language Processing (NLP) to extract the key concepts associated with 16 dental conditions from transcribed dental examinations. These concepts, coded using the standardized terminologies, would provide a structured summary of a patient's initial dental exam. The proposal has two aims: 1) evaluate the accuracy of speech recognition technology for clinical dental examinations; and 2) develop and evaluate an NLP application for mapping transcribed text to a structured dental chart. This proposal describes a new, exploratory and innovative research project that could radically impact the practice of dental charting. Expected outcomes for this proposal include: 1) an understanding of the accuracy of speech recognition for real-time dictated dental exams; and 2) NLP-based tools to automatically chart restorative and periodontal conditions for each tooth into a structured dental chart. This developmental work will provide a strong foundation for developing a chairside NLP-based dental charting application that would automatically generate a structured dental chart suitable for chairside decision support.          n/a",Feasibility of a Natural Language Processing-based Dental Charting Application,7478824,R21DE018158,"['Caring', 'Clinical', 'Clinical Decision Support Systems', 'Code', 'Computerized Patient Records', 'Computers', 'Condition', 'Data', 'Dental', 'Dental Dictionaries', 'Dental General Practice', 'Dental Hygienists', 'Dental Informatics', 'Dental Offices', 'Dental Records', 'Dentistry', 'Dentists', 'Development', 'Devices', 'Disabled Persons', 'Documentation', 'Evaluation', 'Foundations', 'Goals', 'Human Resources', 'Infection Control', 'Language', 'Manuals', 'Maps', 'Measurement', 'Medical', 'Medical Transcription', 'Mus', 'Natural Language Processing', 'Numbers', 'Outcome', 'Patient Care', 'Patients', 'Performance', 'Positioning Attribute', 'Process', 'Reference Standards', 'Research', 'Research Project Grants', 'Services', 'Speech', 'Speech Recognition Software', 'Structure', 'Surveys', 'System', 'Technology', 'Terminology', 'Testing', 'Text', 'Time', 'Tooth structure', 'Training', 'Transcript', 'Universities', 'Vocabulary', 'Work', 'base', 'biomedical informatics', 'concept', 'dental structure', 'design', 'digital', 'experience', 'handicapping condition', 'improved', 'innovation', 'prevent', 'prototype', 'restoration', 'speech recognition', 'tool']",NIDCR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2008,177729,0.021387722655361308
"Investigating the generalizability of natural language processing of EMR data    DESCRIPTION (provided by applicant):       The electronic medical record (EMR) offers impressive opportunities for increasing care quality, but challenges stand in the way of realizing this vision. For example, coded EMR data readily available for analysis typically are incomplete (due to the prevalence of free-text clinical notes in EMR implementations), and data from different EMRs are often incommensurate due to differences in standard vocabularies and system implementations. While informatics research has shown the feasibility of automatically coding specific aspects of clinical text using Natural Language Processing (NLP), challenges remain for translating these informatics developments into large-scale care quality assessments. To date, successful NLP solutions for automated quality assessment have tended to be applications that are specific to (a) the target problem or clinical focus, (b) the EMR data system, and (c) the person or team that implements the NLP solution. In this study, we propose to begin addressing the problem of implementation team specificity by developing, evaluating, and making freely available a generalizable NLP development tool suite. The tools will enable widespread adoption of NLP systems to extract and code data from free text clinical notes. The Knowledge Editing Toolkit will simplify development of problem-specific knowledge by helping the user define the rules, concepts, and terms that constitute a domain-specific knowledge module, thus allowing any informaticist to develop an NLP application. The NLP Application Validation Toolkit will allow rapid testing and evaluation of the application against a gold standard of independently-coded test records from any EMR. To evaluate the effects of the toolkits on NLP generalizability, we will have three clinical informaticists each build two NLP applications (for a total of six distinct applications). One of their applications will identify a constellation of common clinical signs or symptoms (e.g., ""persistent cough"") that are relatively discrete concepts using simple language terms for many different clinical purposes. Their second application will assess behavioral counseling (e.g., ""alcohol counseling""), which uses complex language constructs for dedicated clinical purposes. We will describe and evaluate the accuracy of the solutions against independently coded test sets of medical records. We will quantify and compare the difficulty of creating these solutions as measured by the time, number of iterations required to build the applications, and the number of concepts and rules employed, as well as analyze variability in content and accuracy of the solutions created. In addition, we will use qualitative techniques to assess the ease of using the development tools; the difficulty in learning the tools; and specific types of problems, limitations, and bugs encountered. Such an NLP development tool suite has the potential to allow simple, elegant, and reliably good NLP solutions regardless of the clinical problem domain or the person developing the solution.           n/a",Investigating the generalizability of natural language processing of EMR data,7529967,R21LM009728,"['Address', 'Adopted', 'Adoption', 'Affect', 'Alcohols', 'Architecture', 'Behavioral', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Code', 'Complex', 'Computer Systems Development', 'Computerized Medical Record', 'Coughing', 'Counseling', 'Data', 'Databases', 'Development', 'Event', 'Gold', 'Healthcare Systems', 'Human Resources', 'Informatics', 'Information Systems', 'Information Technology', 'Institute of Medicine (U.S.)', 'Knowledge', 'Language', 'Learning', 'Measures', 'Medical Records', 'Modification', 'Natural Language Processing', 'Numbers', 'Performance', 'Persons', 'Positioning Attribute', 'Prevalence', 'Process', 'Proliferating', 'Purpose', 'Quality of Care', 'Records', 'Research', 'Running', 'Solutions', 'Source', 'Specificity', 'Standards of Weights and Measures', 'Structure', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Translating', 'United States National Academy of Sciences', 'Ursidae Family', 'Validation', 'Vision', 'Vocabulary', 'care delivery', 'concept', 'cost', 'design', 'evaluation/testing', 'experience', 'health care quality', 'stem', 'success', 'tool', 'tool development']",NLM,KAISER FOUNDATION RESEARCH INSTITUTE,R21,2008,213300,0.01193339100631269
"Using Natural Language Processing to Monitor Product Claims Compliance for FDA    DESCRIPTION (provided by applicant): Linguastat, Inc. proposes to develop a means to automate the process of monitoring and identifying companies engaged in false advertising and deceptive practices in the marketing of drugs, dietary supplements, and/or food products. By leveraging state of the art approaches in computational linguistics such as Information Extraction and Natural Language Processing, it should be feasible, with some adaptation, to use this technology to: 1) automatically and continuously monitor the websites, TV transcripts, press releases and other electronic marketing text communications of tens of thousands of companies for various claims and product information 2) automatically ""red-flag"" instances in which claims have a high likelihood of potential harm to consumers, according to FDA priorities 3) automatically identify and extract the companies, products and claims embedded in electronic product information and electronic promotional materials to create a database easily searchable by the FDA and 4) automatically capture web-based or other electronic content for human review and store it as ""evidence."" Such automated technology would enable the FDA to significantly stretch its limited human resource to more effectively and comprehensively identify noncompliant product information, detect deceptive ads and other illegal practices, successfully prosecute offenders, and prevent harm to American consumers. For this Phase I SBIR project we propose to assess the feasibility of automated claims monitoring in three steps: In the first step, we will train information extraction and natural language processing algorithms to extract product marketing claims from text. In the second, step we will apply data mining and rules-based algorithms to assess which claims are likely to be non-compliant and merit further attention by FDA staff. In the third step, we will design and build a database of product claims that allows analysts to search, organize, and prioritize product claims based on the type of claim (e.g. what ailments does the product claim to treat), the type of product, and the likelihood of non- compliance. This technology will enable regulators and consumers to better monitor and detect cases of false, misleading, or deceptive advertising and product information. By enabling more effective enforcement of FDA regulations and giving consumers tools to make better buying decisions, the public health can be better protected by minimizing the impact of products that cause harm, give false hope, or entice consumers to forgo conventional remedies.          n/a",Using Natural Language Processing to Monitor Product Claims Compliance for FDA,7677599,R43FD003406,[' '],FDA,"LINGUASTAT, INC.",R43,2008,20000,0.017315594313683676
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7475712,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2008,438476,0.029693007478622035
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7671784,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2008,48482,0.029693007478622035
"Sematic Relatedness for Active Medication Safety and Outcomes Surveillance    DESCRIPTION (provided by applicant):       Medication-related morbidity and mortality in ambulatory care in the United States results in estimated 100,000 deaths and $177 billion spending annually. Post-marketing passive surveillance of outcomes associated with medication use has been recognized as a necessary component in drug safety monitoring to overcome the limitations of pre- marketing clinical trials. Information technology applied to the patient's electronic medical and therapeutic record holds promise to improve this situation by detecting alarming trends in signs and symptoms in patient populations exposed to the same medication. Currently, much of the information necessary for active drug safety surveillance is ""locked"" in the unstructured text of electronic records. Our long-term goal is to develop information technology to recognize and prevent drug therapy related adverse events. Sophisticated natural language processing systems have been developed to find medical terms and their synonyms in the unstructured text and use them to retrieve information. In order to monitor alarming trends in symptoms in medical records, we need mechanisms that will allow not only accurate term and concept identification but also grouping of semantically related concepts that may not necessarily be synonymous. Measures of semantic relatedness rely on existing ontologies of domain knowledge as well as large textual corpora to compute a numeric score indicating the strength of relatedness between two concepts. Our central hypothesis is that such measures will be able to make fine-grained distinctions among concepts in the biomedical text, and provide a foundation upon which to organize concepts into meaningful groups automatically. In particular, this proposal seeks to develop methods that leverage the medical knowledge contained within Unified Medical Language System (UMLS) and corpora of clinical text. Our short-term goals are 1) develop new methods, specific to clinical text, for computing semantic relatedness 2) integrate these specific methods for computing semantic relatedness into more general methods of natural language processing 3) integrate semantic relatedness into methods for identifying labeled semantic relations in clinical text. Labeled relations significantly enhance the ability of natural language processing to support accurate automatic analysis of medical information for improving patient safety. Our next step will be to develop and validate a generalizable active medication safety surveillance system that will automatically track medication exposure and alarming trends in signs and symptoms in ambulatory and hospitalized populations for a broad range of diseases.           This project will a) create and validate a common open-source platform for developing and testing semantic relatedness measures, b) determine the validity of electronic medical records with respect to identification of symptoms associated with medication- related problems and c) develop a novel methodology to aggregate adverse reaction terms used to code spontaneous post-marketing drug safety surveillance reports. The results of this project will enable more effective medication safety surveillance efforts and thus will improve patient safety.",Sematic Relatedness for Active Medication Safety and Outcomes Surveillance,7579478,R01LM009623,"['Address', 'Adverse effects', 'Adverse event', 'Adverse reactions', 'Ambulatory Care', 'Angina Pectoris', 'Area', 'Body of uterus', 'Cereals', 'Cessation of life', 'Clinical', 'Clinical Informatics', 'Clinical Research', 'Clinical Trials', 'Clinical assessments', 'Computational Technique', 'Computerized Medical Record', 'Count', 'Databases', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Effectiveness', 'Electronics', 'Exposure to', 'Foundations', 'Generic Drugs', 'Goals', 'Group Identifications', 'Grouping', 'Health', 'Healthcare', 'Heart failure', 'Information Technology', 'Knowledge', 'Label', 'Linguistics', 'Mandatory Reporting', 'Manuals', 'Maps', 'Marketing', 'Measures', 'Medical', 'Medical Electronics', 'Medical History', 'Medical Informatics', 'Medical Records', 'Medical Surveillance', 'Methods', 'Minnesota', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Nature', 'Numbers', 'One-Step dentin bonding system', 'Ontology', 'Outcome', 'Patients', 'Pharmaceutical Cares', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Pharmacy facility', 'Physicians', 'Pliability', 'Population', 'Positioning Attribute', 'Practice based research', 'Primary Health Care', 'Procedures', 'Process', 'Purpose', 'Range', 'Reaction', 'Records', 'Reference Standards', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Role', 'Safety', 'Score', 'Semantics', 'Signs and Symptoms', 'Statistical Models', 'Symptoms', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Therapeutic Effect', 'Time', 'Training', 'Unified Medical Language System', 'United States', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Validation', 'Work', 'Writing', 'base', 'computer science', 'concept', 'data mining', 'design', 'experience', 'improved', 'information organization', 'knowledge base', 'metathesaurus', 'mortality', 'novel', 'open source', 'patient safety', 'post-market', 'prevent', 'social', 'treatment planning', 'trend']",NLM,UNIVERSITY OF MINNESOTA,R01,2008,299619,0.0432793134044701
"MOMENT (Monitoring for Outpatient Medication Effects and New Toxicities) in TIME    DESCRIPTION (provided by applicant):       The MOMENT (Monitoring for Outpatient Medication Effects and New Toxicities) in TIME project will extend the in-progress TIME (Tools for Inpatient Monitoring using Evidence) for Safe and Appropriate Testing grant, #5 R01 LM007995-03, by developing sophisticated text-mining and data extraction tools to examine adverse drug effects in patients presenting for emergency and hospital care.       Using a UMLS-based concept identifier enhanced with natural language processing, the MOMENT project will detect: (a) individual clinical manifestations (symptoms and physical exam findings) that are potentially drug related; (b) clinical syndromes (e.g., hepatoxicity, myopathy, renal insufficiency, glucose intolerance); and, (c) and clinical diseases (such as systemic lupus induced by hydralazine or procainamide, or acute myocardial infarction associated with rofecoxib). Innovative aspects of the proposed work include use of advanced natural language processing techniques to abstract concepts representing potential drug effects from patient history and physical examination records and ancillary test reports, the combination of evidence based templates and diagnostic algorithms to detect complex patterns of drug toxicity (e.g., hepatocelluar damage, pulmonary fibrosis, or acute coronary syndrome), advanced statistical methods for determining medication effects in large populations, and utilization of a clinical database containing data on more than 100,000 patients.      Public Statement      The MOMENT project (Monitoring for Outpatient Medication Effects and New Toxicities) will demonstrate the feasibility of combining informatics applications increasingly present in many healthcare institutions - electronic medical record systems and care provider order entry (CPOE) systems - to create an advanced detection and monitoring system for adverse medication effects. The project will develop tools and algorithms to detect the patient symptoms, physical exam signs, test results, and medical diagnoses that may indicate drug-induced injury.          n/a",MOMENT (Monitoring for Outpatient Medication Effects and New Toxicities) in TIME,7448662,R01LM007995,"['Abnormal Laboratory Test Result', 'Accreditation', 'Acute myocardial infarction', 'Admission activity', 'Adverse drug effect', 'Adverse effects', 'Algorithms', 'Ambulatory Monitoring', 'Appendix', 'Basic Science', 'Caring', 'Chest Pain', 'Classification', 'Clinical', 'Code', 'Complex', 'Computer Assisted', 'Computerized Medical Record', 'Condition', 'Congenital Abnormality', 'Coronary heart disease', 'Data', 'Databases', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Disease', 'Diuretics', 'Drug Delivery Systems', 'Drug Exposure', 'Drug Prescriptions', 'Drug Utilization', 'Drug toxicity', 'Drug usage', 'Edema', 'Electronic Health Record', 'Emergency Situation', 'Evaluation', 'Event', 'Frequencies', 'Glucose Intolerance', 'Grant', 'Healthcare', 'Hepatotoxicity', 'Hormones', 'Hospitalization', 'Hospitals', 'Hour', 'Hydralazine', 'Image', 'Inappropriate ADH Syndrome', 'Individual', 'Informatics', 'Injury', 'Inpatients', 'Institution', 'Joints', 'Kidney Failure', 'Knowledge', 'Laboratories', 'Letters', 'Link', 'Literature', 'Liver Cirrhosis', 'Lower Extremity', 'Lupus', 'Manufacturer Name', 'Marketing', 'Medical', 'Medical Surveillance', 'Methods', 'Methotrexate', 'Modality', 'Monitor', 'Myopathy', 'Natural Language Processing', 'Numbers', 'Outcome', 'Outpatients', 'Pathology', 'Patients', 'Pattern', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Phase II/III Trial', 'Physical Examination', 'Population', 'Predictive Value', 'Procainamide', 'Process', 'Provider', 'Pulmonary Fibrosis', 'Purpose', 'Recording of previous events', 'Records', 'Registries', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Rofecoxib', 'Sampling', 'Score', 'Screening procedure', 'Sentinel', 'Severities', 'Site', 'Standards of Weights and Measures', 'Statistical Methods', 'Symptoms', 'Syndrome', 'System', 'Techniques', 'Test Result', 'Testing', 'Text', 'Time', 'TimeLine', 'Toxic effect', 'United States Food and Drug Administration', 'Universities', 'Vocabulary', 'Work', 'abstracting', 'acute coronary syndrome', 'base', 'care systems', 'concept', 'healthy volunteer', 'impression', 'innovation', 'knowledge base', 'member', 'mortality', 'novel', 'post-market', 'programs', 'telithromycin', 'text searching', 'tool']",NLM,VANDERBILT UNIVERSITY,R01,2008,383338,0.018820019521353735
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7414601,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Language', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Population', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Today', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2008,142400,0.01447404390852661
"NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports    DESCRIPTION (provided by applicant):       Many NLP applications have been successfully developed to extract information from text. Most of the   applications have focused on identifying individual clinical conditions in textual records, which is the first step in making the conditions available to computerized applications. However, identifying individual instances of clinical conditions is not sufficient for many medical informatics tasks - the context surrounding the condition is crucial for integrating the information within the text to determine the clinical state of a patient. We propose to perform in-depth studies on NLP issues requiring knowledge of the context of clinical conditions in clinical records. We will focus our research by using syndromic surveillance from emergency department (ED) reports as a case study.      For this proposal, we will test the following hypothesis: An NLP system that indexes clinical concepts and integrates contextual information modifying the concepts can identify acute clinical conditions from ED reports as well as physicians can.      We will identify clinical concepts necessary for surveillance of seven syndromes, including respiratory,   gastrointestinal, neurological, rash, hemorrhagic, constitutional, and botulinic. To evaluate the hypothesis, we will perform the following specific aims:      Aim 1. Perform in-depth, foundational studies on four NLP topics to gain a deeper understanding of the      pertinent NLP research capabilities required for identification of acute clinical conditions from ED reports, including negation, uncertainty, temporal discrimination, and finding validation;      Aim 2. Apply the knowledge learned from the foundational studies to develop and evaluate an automated application for ED reports that will determine the values for clinical variables relevant to identifying patients with any of seven syndromes.      The research is innovative, because it will generate an in-depth study of multiple NLP topics crucial to   understanding a patient's clinical state from textual records and will focus on contextual understanding and analysis. The research will be guided by linguistic principles, by the semantics and discourse structure of ED reports, and by the application area of biosurveillance. Because we will develop research methods and tools that are customized to a particular domain, we will constrain the research space, which will provide direction and enhance the chance for success. However, the methods and tools generated by this research should be extensible to other clinical report types and to other domain applications, because we will explicitly specify and study NLP concepts and relationships that are common to many application areas.             n/a",NLP Foundational Studies & Ontologies for Syndromic Surveillance from ED Reports,7469551,R01LM009427,"['Accident and Emergency department', 'Acute', 'Area', 'Case Study', 'Clinical', 'Clinical Data', 'Condition', 'Constitutional', 'Depth', 'Detection', 'Discrimination', 'Exanthema', 'Exhibits', 'Individual', 'Informatics', 'Knowledge', 'Learning', 'Linguistics', 'Medical Informatics', 'Medical Informatics Applications', 'Medical Surveillance', 'Methods', 'Natural Language Processing', 'Neurologic', 'Ontology', 'Patients', 'Physicians', 'Records', 'Reporting', 'Research', 'Research Methodology', 'Semantics', 'Specific qualifier value', 'Structure', 'Syndrome', 'System', 'Techniques', 'Testing', 'Text', 'Uncertainty', 'Validation', 'computerized', 'concept', 'gastrointestinal', 'indexing', 'innovation', 'quality assurance', 'respiratory', 'success', 'syndromic surveillance', 'tool']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2008,392337,0.07500989678566054
"Statistical NLP Analysis of Cross-discipline Clinical Text emerging trend in computational linguistics is melding natural language processing (NLP) and machine learning (ML) to help computers make sense of human-generated free text. The blending of these disciplines is relatively rare in biomedical inforrnatics. Past medical NLP/ML research work is biased heavily towards linguistic methods that attempt to reason about grammar and syntax aided by a domain-focal knowledge base (e.g., one for radiology or one for clinical pathology). The aim of the work proposed here takes a different tack: exploring the utility of a statistical approach to clinical NLP, one augmented by machine learning and concentrating on general progress notes from across multiple clinical domains. The specific clinical goal will be to identify adverse drug events described implicitly or explicitly in inpatient progress notes. Rather than relying on a narrow domain focus to provide enough context restriction to make text interpretation tractable, this approach will use statistical patterns in note author information (e.g., profession, note type, treating ward) and patient information (e.g., admit diagnosis, procedures performed, temporal note relationships) for context restriction. The research component of this proposal is divided into two categories: three small-scale projects designed to rapidly hone new skills developed under the training component, and a large-scale project that assesses the feasibility of cross-discipline clinical text analysis. n/a",Statistical NLP Analysis of Cross-discipline Clinical Text,6944955,F38LM008478,"['Categories', 'Clinical', 'Clinical Pathology', 'Computers', 'Coupled', 'Diagnosis', 'Discipline', 'Event', 'Fellowship', 'Goals', 'Human', 'Inpatients', 'Linguistics', 'Machine Learning', 'Medical', 'Methods', 'Natural Language Processing', 'Patients', 'Pattern', 'Personal Satisfaction', 'Pharmaceutical Preparations', 'Procedures', 'Radiology Specialty', 'Research', 'Statistical Study', 'Text', 'Training', 'Work', 'Writing', 'design', 'experience', 'knowledge base', 'skills', 'syntax', 'theories', 'tool', 'trend', 'ward']",NLM,UNIVERSITY OF UTAH,F38,2007,38768,0.03703558776512632
"Discovering and Applying Knowledge in Clinical Databases    DESCRIPTION (provided by applicant):  The ongoing goal of our project, ""Discovering and applying knowledge in clinical databases,"" is to develop and apply methods to exploit electronic medical record data for decision support, with an emphasis on narrative data. Since the inception of our project as an R29 in 1994, we have been developing methods for preparing raw electronic medical record data, applying and evaluating natural language processing, developing data mining techniques including machine learning, and putting the results to use for clinical care and research.       In this competing continuation, we propose to address the temporal information in the electronic medical record and to apply natural language processing and temporal processing to the task of syndromic surveillance in collaboration with the New York City Department of Health and Mental Hygiene (NYC DOHMH).       We have begun work on a temporal processing system. It extracts temporal assertions stated in narrative reports, uses the MedLEE natural language processor to parse the non-temporal information, infers implicit temporal assertions based on a knowledge base, and produces the information in the form of a simple temporal constraint satisfaction problem. The latter can be used to answer questions about the time of events and the temporal relation between pairs of events. We propose to complete the system, expand the knowledge base, speed computation, address the uncertainty of temporal assertions, incorporate temporal information from structured data, and evaluate the system.       NYC DOHMH has a mature syndromic surveillance system that watches over almost eight million persons, and it has as-yet unexploited data sources in the form of narrative and structured electronic medical records. We propose to apply natural language processing and our proposed temporal processing to convert the data to a form appropriate for surveillance. We will evaluate the incremental benefit of structured data, narrative data, and temporally processed narrative data.           n/a",Discovering and Applying Knowledge in Clinical Databases,7288319,R01LM006910,"['Address', 'Area', 'Caring', 'Cities', 'Clinical', 'Code', 'Collaborations', 'Computerized Medical Record', 'Data', 'Data Sources', 'Databases', 'Event', 'Goals', 'Health', 'Healthcare', 'Knowledge', 'Language', 'Machine Learning', 'Medical Surveillance', 'Mental Health', 'Methods', 'Natural Language Processing', 'New York City', 'Persons', 'Preparation', 'Process', 'Reporting', 'Research', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Uncertainty', 'Work', 'base', 'data mining', 'improved', 'knowledge base', 'satisfaction', 'syndromic surveillance']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2007,345833,0.03528039648750205
"A Biomedical Natural Language Processing Resource DESCRIPTION:    The long-term aim of this project is to advance clinical care and biomedical research by establishing a natural language processing (NLP) resource for the biomedical community. A major bottleneck for development of automated tools for clinical applications and biomedical research is that most of the data and knowledge occur in the form of text, resulting in a lack of coded data. This NLP resource will make possible a host of automated applications by enabling high throughput access to coded biomedical knowledge and data. The foundation of this resource will be the MedLEE NLP system, which has been used operationally for almost a decade in healthcare settings for a broad range of applications that have proven to be valuable for clinical care. The NLP resource will also include BioMedLEE (a derivative of MedLEE), which encodes genotypic-phenotypic (GP) relations in the scientific literature. It currently focuses on GP relations associated with cancer and infectious diseases, and is being used to organize the extracted information to facilitate research, curation, and ontological development within model organism databases. This proposal will enable us to 1) disseminate our NLP resource to the community, 2) conduct technological research and development (R&D) to facilitate expansion and adaptation of the resource to new applications and specialties, 3) conduct R&D of tools that facilitate use of the extracted data and knowledge after coding, and 4) promote the resource, and provide service to users in the form of technical support, documentation, and tutorials. MedLEE and BioMedLEE are extendable systems that encompass the clinical and scientific communities. The dissemination of a proven NLP system that is applicable to the entire biomedical community provides an exceptional opportunity for multiple developers and researchers to work to unleash the true potential of NLP technology, increasing development of applications that aim to enhance scientific research and improve all levels of health. n/a",A Biomedical Natural Language Processing Resource,7257857,R01LM008635,"['Address', 'Alzheimer&apos', 's Disease', 'Autistic Disorder', 'Biological', 'Biomedical Research', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Communicable Diseases', 'Communities', 'Computerized Medical Record', 'Condition', 'Data', 'Databases', 'Detection', 'Development', 'Discipline', 'Disease', 'Documentation', 'Educational workshop', 'Fostering', 'Foundations', 'Health Status', 'Healthcare', 'Human', 'Imagery', 'Improve Access', 'Internet', 'Knowledge', 'Literature', 'Malignant Neoplasms', 'Measures', 'Medical', 'Medical Errors', 'Medical Surveillance', 'Methodology', 'Methods', 'Mining', 'Natural Language Processing', 'Numbers', 'Ontology', 'Organism', 'Output', 'Partner in relationship', 'Patient Care', 'Performance', 'Phenotype', 'Postdoctoral Fellow', 'Process', 'Range', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Services', 'Side', 'Source', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Terminology', 'Text', 'Universities', 'Work', 'biological research', 'biomedical resource', 'clinical application', 'concept', 'high throughput technology', 'improved', 'knowledge base', 'medical specialties', 'model organisms databases', 'open source', 'research and development', 'tool']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2007,529014,0.018666939980149726
"Feasibility of a Natural Language Processing-based Dental Charting Application    DESCRIPTION (provided by applicant): The absence of a flexible, robust, and accurate natural language interface is a significant barrier to the direct use of computer-based patient records by dental clinicians. While providing patient care, dentists, hygienists and assistants are handicapped in using a keyboard and mouse to interact with a computer, primarily because of infection control concerns. The objective of this proposal is to develop and evaluate a prototype dental charting system with a speech-driven interface that will allow the dentist to chart dental conditions using natural language. The system will use Natural Language Processing (NLP) to extract the key concepts associated with 16 dental conditions from transcribed dental examinations. These concepts, coded using the standardized terminologies, would provide a structured summary of a patient's initial dental exam. The proposal has two aims: 1) evaluate the accuracy of speech recognition technology for clinical dental examinations; and 2) develop and evaluate an NLP application for mapping transcribed text to a structured dental chart. This proposal describes a new, exploratory and innovative research project that could radically impact the practice of dental charting. Expected outcomes for this proposal include: 1) an understanding of the accuracy of speech recognition for real-time dictated dental exams; and 2) NLP-based tools to automatically chart restorative and periodontal conditions for each tooth into a structured dental chart. This developmental work will provide a strong foundation for developing a chairside NLP-based dental charting application that would automatically generate a structured dental chart suitable for chairside decision support.          n/a",Feasibility of a Natural Language Processing-based Dental Charting Application,7305430,R21DE018158,"['Caring', 'Clinical', 'Clinical Decision Support Systems', 'Code', 'Computerized Patient Records', 'Computers', 'Condition', 'Data', 'Dental', 'Dental Dictionaries', 'Dental General Practice', 'Dental Hygienists', 'Dental Informatics', 'Dental Offices', 'Dental Records', 'Dentistry', 'Dentists', 'Development', 'Devices', 'Disabled Persons', 'Documentation', 'Evaluation', 'Foundations', 'Goals', 'Human Resources', 'Infection Control', 'Language', 'Manuals', 'Maps', 'Measurement', 'Medical', 'Medical Transcription', 'Mus', 'Natural Language Processing', 'Numbers', 'Outcome', 'Patient Care', 'Patients', 'Performance', 'Positioning Attribute', 'Process', 'Reference Standards', 'Research', 'Research Project Grants', 'Services', 'Speech', 'Speech Recognition Software', 'Structure', 'Surveys', 'System', 'Technology', 'Terminology', 'Testing', 'Text', 'Time', 'Tooth structure', 'Training', 'Transcript', 'Universities', 'Vocabulary', 'Work', 'base', 'biomedical informatics', 'concept', 'dental structure', 'design', 'digital', 'experience', 'handicapping condition', 'improved', 'innovation', 'prevent', 'prototype', 'restoration', 'speech recognition', 'tool']",NIDCR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2007,229030,0.021387722655361308
"Using Natural Language Processing to Monitor Product Claims Compliance for FDA    DESCRIPTION (provided by applicant): Linguastat, Inc. proposes to develop a means to automate the process of monitoring and identifying companies engaged in false advertising and deceptive practices in the marketing of drugs, dietary supplements, and/or food products. By leveraging state of the art approaches in computational linguistics such as Information Extraction and Natural Language Processing, it should be feasible, with some adaptation, to use this technology to: 1) automatically and continuously monitor the websites, TV transcripts, press releases and other electronic marketing text communications of tens of thousands of companies for various claims and product information 2) automatically ""red-flag"" instances in which claims have a high likelihood of potential harm to consumers, according to FDA priorities 3) automatically identify and extract the companies, products and claims embedded in electronic product information and electronic promotional materials to create a database easily searchable by the FDA and 4) automatically capture web-based or other electronic content for human review and store it as ""evidence."" Such automated technology would enable the FDA to significantly stretch its limited human resource to more effectively and comprehensively identify noncompliant product information, detect deceptive ads and other illegal practices, successfully prosecute offenders, and prevent harm to American consumers. For this Phase I SBIR project we propose to assess the feasibility of automated claims monitoring in three steps: In the first step, we will train information extraction and natural language processing algorithms to extract product marketing claims from text. In the second, step we will apply data mining and rules-based algorithms to assess which claims are likely to be non-compliant and merit further attention by FDA staff. In the third step, we will design and build a database of product claims that allows analysts to search, organize, and prioritize product claims based on the type of claim (e.g. what ailments does the product claim to treat), the type of product, and the likelihood of non- compliance. This technology will enable regulators and consumers to better monitor and detect cases of false, misleading, or deceptive advertising and product information. By enabling more effective enforcement of FDA regulations and giving consumers tools to make better buying decisions, the public health can be better protected by minimizing the impact of products that cause harm, give false hope, or entice consumers to forgo conventional remedies.          n/a",Using Natural Language Processing to Monitor Product Claims Compliance for FDA,7326883,R43FD003406,[' '],FDA,"LINGUASTAT, INC.",R43,2007,99773,0.017315594313683676
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7303652,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2007,435682,0.029693007478622035
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance  existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of  health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted  readability levels with no critical information loss, using statistical natural language processing  techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive  impact on reader comprehension. We will use as a test bed for our system a general internal medicine  clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public. n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7492453,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2007,47853,0.029219328966228194
"MOMENT (Monitoring for Outpatient Medication Effects and New Toxicities) in TIME    DESCRIPTION (provided by applicant):       The MOMENT (Monitoring for Outpatient Medication Effects and New Toxicities) in TIME project will extend the in-progress TIME (Tools for Inpatient Monitoring using Evidence) for Safe and Appropriate Testing grant, #5 R01 LM007995-03, by developing sophisticated text-mining and data extraction tools to examine adverse drug effects in patients presenting for emergency and hospital care.       Using a UMLS-based concept identifier enhanced with natural language processing, the MOMENT project will detect: (a) individual clinical manifestations (symptoms and physical exam findings) that are potentially drug related; (b) clinical syndromes (e.g., hepatoxicity, myopathy, renal insufficiency, glucose intolerance); and, (c) and clinical diseases (such as systemic lupus induced by hydralazine or procainamide, or acute myocardial infarction associated with rofecoxib). Innovative aspects of the proposed work include use of advanced natural language processing techniques to abstract concepts representing potential drug effects from patient history and physical examination records and ancillary test reports, the combination of evidence based templates and diagnostic algorithms to detect complex patterns of drug toxicity (e.g., hepatocelluar damage, pulmonary fibrosis, or acute coronary syndrome), advanced statistical methods for determining medication effects in large populations, and utilization of a clinical database containing data on more than 100,000 patients.      Public Statement      The MOMENT project (Monitoring for Outpatient Medication Effects and New Toxicities) will demonstrate the feasibility of combining informatics applications increasingly present in many healthcare institutions - electronic medical record systems and care provider order entry (CPOE) systems - to create an advanced detection and monitoring system for adverse medication effects. The project will develop tools and algorithms to detect the patient symptoms, physical exam signs, test results, and medical diagnoses that may indicate drug-induced injury.          n/a",MOMENT (Monitoring for Outpatient Medication Effects and New Toxicities) in TIME,7262635,R01LM007995,"['Abnormal Laboratory Test Result', 'Accreditation', 'Acute myocardial infarction', 'Admission activity', 'Adverse drug effect', 'Adverse effects', 'Algorithms', 'Ambulatory Monitoring', 'Appendix', 'Basic Science', 'Caring', 'Chest Pain', 'Classification', 'Clinical', 'Code', 'Complex', 'Computer Assisted', 'Computerized Medical Record', 'Condition', 'Congenital Abnormality', 'Coronary heart disease', 'Data', 'Databases', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Disease', 'Diuretics', 'Drug Delivery Systems', 'Drug Exposure', 'Drug Prescriptions', 'Drug Utilization', 'Drug toxicity', 'Drug usage', 'Edema', 'Electronic Health Record', 'Emergency Situation', 'Evaluation', 'Event', 'Frequencies', 'Glucose Intolerance', 'Grant', 'Healthcare', 'Hepatotoxicity', 'Hormones', 'Hospitalization', 'Hospitals', 'Hour', 'Hydralazine', 'Image', 'Inappropriate ADH Syndrome', 'Individual', 'Informatics', 'Injury', 'Inpatients', 'Institution', 'Joints', 'Kidney Failure', 'Knowledge', 'Laboratories', 'Letters', 'Link', 'Literature', 'Liver Cirrhosis', 'Lower Extremity', 'Lupus', 'Manufacturer Name', 'Marketing', 'Medical', 'Medical Surveillance', 'Methods', 'Methotrexate', 'Modality', 'Monitor', 'Myopathy', 'Natural Language Processing', 'Numbers', 'Outcome', 'Outpatients', 'Pathology', 'Patients', 'Pattern', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Phase II/III Trial', 'Physical Examination', 'Population', 'Predictive Value', 'Procainamide', 'Process', 'Provider', 'Pulmonary Fibrosis', 'Purpose', 'Recording of previous events', 'Records', 'Registries', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Rofecoxib', 'Sampling', 'Score', 'Screening procedure', 'Sentinel', 'Severities', 'Site', 'Standards of Weights and Measures', 'Statistical Methods', 'Symptoms', 'Syndrome', 'System', 'Techniques', 'Test Result', 'Testing', 'Text', 'Time', 'TimeLine', 'Toxic effect', 'United States Food and Drug Administration', 'Universities', 'Vocabulary', 'Work', 'abstracting', 'acute coronary syndrome', 'base', 'care systems', 'concept', 'healthy volunteer', 'impression', 'innovation', 'knowledge base', 'member', 'mortality', 'novel', 'post-market', 'programs', 'telithromycin', 'text searching', 'tool']",NLM,VANDERBILT UNIVERSITY MED CTR,R01,2007,304785,0.018820019521353735
"TIME:(Tools for Inpatient Monitoring using Evidence)for Safe & AppropriateTesting The MOMENT (Monitoring for Outpatient Medication Effects and New Toxicities) in TIME project will extend the in-progress TIME (Tools for Inpatient Monitoring using Evidence) for Safe and Appropriate Testing grant, #5 R01 LM007995-03, by developing sophisticated text-mining and data extraction tools to examine adverse drug effects in patients presenting for emergency and hospital care. Using a UMLS-based concept identifier enhanced with natural language processing, the MOMENT project will detect: (a) individual clinical manifestations (symptoms and physical exam findings) that are potentially drug related; (b) clinical syndromes (e.g., hepatoxicity, myopathy, renal insufficiency, glucose intolerance); and, (c) and clinical diseases (such as systemic lupus induced by hydralazine or procainamide, or acute myocardial infarction associated with rofecoxib). Innovative aspects of the proposed work include use of advanced natural language processing techniques to abstract concepts representing potential drug effects from patient history and physical examination records and ancillary test reports, the combination of evidence- based templates and diagnostic algorithms to detect complex patterns of drug toxicity (e.g., hepatocelluar damage, pulmonary fibrosis, or acute coronary syndrome), advanced statistical methods for determining medication effects in large populations, and utilization of a clinical database containing data on more than 100,000 patients. Public Statement The MOMENT project (Monitoring for Outpatient Medication Effects and New Toxicities) will demonstrate the feasibility of combining informatics applications increasingly present in many healthcare institutions - electronic medical record systems and care provider order entry (CPOE) systems - to create an advanced detection and monitoring system for adverse medication effects. The project will develop tools and algorithms to detect the patient symptoms, physical exam signs, test results, and medical diagnoses that may indicate drug-induced injury. n/a",TIME:(Tools for Inpatient Monitoring using Evidence)for Safe & AppropriateTesting,7347232,R01LM007995,"['Abnormal Laboratory Test Result', 'Accreditation', 'Acute myocardial infarction', 'Admission activity', 'Adverse drug effect', 'Adverse effects', 'Algorithms', 'Ambulatory Monitoring', 'Appendix', 'Basic Science', 'Caring', 'Chest Pain', 'Classification', 'Clinical', 'Code', 'Complex', 'Computer Assisted', 'Computerized Medical Record', 'Condition', 'Congenital Abnormality', 'Coronary heart disease', 'Data', 'Databases', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Disease', 'Diuretics', 'Drug Delivery Systems', 'Drug Exposure', 'Drug Prescriptions', 'Drug Utilization', 'Drug toxicity', 'Drug usage', 'Edema', 'Electronic Health Record', 'Emergency Situation', 'Evaluation', 'Event', 'Frequencies', 'Glucose Intolerance', 'Grant', 'Healthcare', 'Hepatotoxicity', 'Hormones', 'Hospitalization', 'Hospitals', 'Hour', 'Hydralazine', 'Image', 'Inappropriate ADH Syndrome', 'Individual', 'Informatics', 'Injury', 'Inpatients', 'Institution', 'Joints', 'Kidney Failure', 'Knowledge', 'Laboratories', 'Letters', 'Link', 'Literature', 'Liver Cirrhosis', 'Lower Extremity', 'Lupus', 'Manufacturer Name', 'Marketing', 'Medical', 'Medical Surveillance', 'Methods', 'Methotrexate', 'Modality', 'Monitor', 'Myopathy', 'Natural Language Processing', 'Numbers', 'Outcome', 'Outpatients', 'Pathology', 'Patients', 'Pattern', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Phase II/III Trial', 'Physical Examination', 'Population', 'Predictive Value', 'Procainamide', 'Process', 'Provider', 'Pulmonary Fibrosis', 'Purpose', 'Recording of previous events', 'Records', 'Registries', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Rofecoxib', 'Sampling', 'Score', 'Screening procedure', 'Sentinel', 'Severities', 'Site', 'Standards of Weights and Measures', 'Statistical Methods', 'Symptoms', 'Syndrome', 'System', 'Techniques', 'Test Result', 'Testing', 'Text', 'Time', 'TimeLine', 'Toxic effect', 'United States Food and Drug Administration', 'Universities', 'Vocabulary', 'Work', 'abstracting', 'acute coronary syndrome', 'base', 'care systems', 'concept', 'healthy volunteer', 'impression', 'innovation', 'knowledge base', 'member', 'mortality', 'novel', 'post-market', 'programs', 'telithromycin', 'text searching', 'tool']",NLM,VANDERBILT UNIVERSITY MED CTR,R01,2007,55295,0.01744271154505516
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7195053,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Language', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Population', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Today', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2007,145510,0.01447404390852661
"A Context-Sensitive Teleconsultation Infrastructure DESCRIPTION (provided by applicant):  Consultation with appropriate specialists improves the quality of healthcare, particularly in patients with complicated cases or chronic illnesses. And for the majority of such patients, specialists use imaging studies (e.g., MR, CT) to objectively document the disease process (e.g., a cancer patient on chemotherapy). However, specialists are generally not available in all communities, tending to be concentrated in academic/specialty centers. Thus, to facilitate the routine use of teleconsultations for patients when specialists are not locally present: 1) the images captured to document the patient's condition must be incorporated into the medical record to enable proper review; and 2) the remote consultant should only receive pertinent parts of the medical record to streamline the consultation process. This proposal is focused on developing and testing a ""context-sensitive"" telehealth infrastructure based on: 1) automated incorporation of clinical context (patient presentation and referring physician hypothesis) to focus the consultation process; 2) a knowledge-base derived from data mining of natural language processing (NLP) results, mapping patient presentation to select an appropriate imaging study based on anatomical region and imaging parameters; and 3) automated selection of key anatomical structures in the acquired imaging study through the use of a contrast-customizable atlas and rigid body/deformable registration algorithms. Collectively, these technologies will allow context-sensitive, automated summarization of medical records for telehealth in a real-world environment. The proposed technologies will be implemented for neurological and musculoskeletal domains, two areas that are MR imaging intensive.      Technical evaluation will be performed with experts serving as the reference standard and will focus on measuring: 1) the accuracy of the corpus based, NLP-guided knowledge-base in selecting relevant anatomical structures; and 2) the accuracy of anatomical structure delineation using the customizable atlas registration methods. Clinical evaluation will be conducted in a real-world teleconsultation environment in a before/after study design using two performance metrics: 1) the time required for consultations; and 2) the effect on the quality of the consultations. n/a",A Context-Sensitive Teleconsultation Infrastructure,7272822,R01EB002247,"['Address', 'Algorithms', 'Anatomy', 'Area', 'Atlases', 'Body of uterus', 'Cancer Patient', 'Caregivers', 'Chronic', 'Chronic Disease', 'Clinical', 'Communication', 'Communities', 'Condition', 'Consultations', 'Data', 'Development', 'Diagnostic', 'Dimensions', 'Disease', 'Documentation', 'Eating', 'Environment', 'Evaluation', 'Functional Magnetic Resonance Imaging', 'Health Personnel', 'Healthcare', 'Image', 'Label', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medical Records', 'Methods', 'Metric', 'Modeling', 'Musculoskeletal', 'Musculoskeletal Pain', 'Natural Language Processing', 'Neurologic', 'Oncologist', 'Optics', 'Patients', 'Performance', 'Physicians', 'Primary Care Physician', 'Primary Health Care', 'Process', 'Quality of Care', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Slice', 'Specialist', 'Structure', 'Surgeon', 'System', 'Techniques', 'Technology', 'Teleconsultations', 'Telemedicine', 'Testing', 'Time', 'Upper arm', 'Work', 'base', 'chemotherapy', 'data mining', 'diagnostic accuracy', 'health care quality', 'image registration', 'improved', 'interest', 'knowledge base', 'medical specialties', 'novel', 'research clinical testing', 'size', 'telehealth']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2007,394864,-0.0036275802008840672
Biosurveillance Utilizing SNOMED-CT Based Natural Language Processing No abstract available n/a,Biosurveillance Utilizing SNOMED-CT Based Natural Language Processing,8449372,R01PH000022,[' '],PHPPO,MAYO CLINIC ROCHESTER,R01,2007,157257,-0.004123987064545239
"A Biomedical Natural Language Processing Resource DESCRIPTION:    The long-term aim of this project is to advance clinical care and biomedical research by establishing a natural language processing (NLP) resource for the biomedical community. A major bottleneck for development of automated tools for clinical applications and biomedical research is that most of the data and knowledge occur in the form of text, resulting in a lack of coded data. This NLP resource will make possible a host of automated applications by enabling high throughput access to coded biomedical knowledge and data. The foundation of this resource will be the MedLEE NLP system, which has been used operationally for almost a decade in healthcare settings for a broad range of applications that have proven to be valuable for clinical care. The NLP resource will also include BioMedLEE (a derivative of MedLEE), which encodes genotypic-phenotypic (GP) relations in the scientific literature. It currently focuses on GP relations associated with cancer and infectious diseases, and is being used to organize the extracted information to facilitate research, curation, and ontological development within model organism databases. This proposal will enable us to 1) disseminate our NLP resource to the community, 2) conduct technological research and development (R&D) to facilitate expansion and adaptation of the resource to new applications and specialties, 3) conduct R&D of tools that facilitate use of the extracted data and knowledge after coding, and 4) promote the resource, and provide service to users in the form of technical support, documentation, and tutorials. MedLEE and BioMedLEE are extendable systems that encompass the clinical and scientific communities. The dissemination of a proven NLP system that is applicable to the entire biomedical community provides an exceptional opportunity for multiple developers and researchers to work to unleash the true potential of NLP technology, increasing development of applications that aim to enhance scientific research and improve all levels of health. n/a",A Biomedical Natural Language Processing Resource,7075417,R01LM008635,"['artificial intelligence', 'automated medical record system', 'bioinformatics', 'biomedical automation', 'biomedical resource', 'clinical research', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data management', 'health science research', 'high throughput technology', 'human data']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,544814,0.018666939980149726
"Natural Language Processing for Respiratory Surveillance    DESCRIPTION (provided by applicant): The applicant's long-term career goal is to become an independent investigator in biomedical informatics research. She will dedicate her career to developing and evaluating methodologies for improving the processes and outcomes of healthcare using data locked in textual documents. This career development award will provide her with initial support for achieving her career goals.      The applicant has three goals for her career development over the next three years. First, she will compare the performance of different machine learning techniques at detecting patients with a respiratory syndrome. The proposed research will expand the state-of-the-art syndromic surveillance capabilities by integrating findings, symptoms, and diseases described in textual medical records. The product of the first research goal will be a model for respiratory syndromic case detection for monitoring natural and bioterrorism induced respiratory outbreaks. Second, she will apply existing methods and develop new techniques for extracting clinical conditions required for respiratory case detection from emergency department notes, contributing new knowledge to the medical language processing field using sentence and report level models that account for uncertainty, negation, and temporal occurrence. The product of the second goal will be a better understanding of the information required for accurate detection of respiratory related conditions from text and useful tools for automatically extracting that information. Third, she will teach, promote, and facilitate the use of natural language processing in the biomedical informatics field. The product of the third goal will be a graduate class surveying medical language processing methodology and applications and development of general tools sets for researchers who need encoded data from textual patient records.        The proposed research will focus on:   Aim 1. Development and evaluation of a respiratory case detection model;   Aim 2. Integration of existing natural language processing tools and development of new methodologies for extracting clinical conditions needed for respiratory case detection from textual records; and   Aim 3. Comparison of existing syndromic detection algorithms that use admit data against the same algorithms using conditions extracted from textual reports.         n/a",Natural Language Processing for Respiratory Surveillance,7076099,K22LM008301,"['automated data processing', 'automated medical record system', 'bioinformatics', 'bioterrorism /chemical warfare', 'clinical research', 'communicable disease diagnosis', 'computer assisted diagnosis', 'computer system design /evaluation', 'diagnosis design /evaluation', 'disease outbreaks', 'early diagnosis', 'hospital utilization', 'human data', 'language translation', 'respiratory disorder epidemiology', 'severe acute respiratory syndrome', 'sign /symptom']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K22,2006,162000,0.03261359107878053
"Discovering and Applying Knowledge in Clinical Databases    DESCRIPTION (provided by applicant):  The ongoing goal of our project, ""Discovering and applying knowledge in clinical databases,"" is to develop and apply methods to exploit electronic medical record data for decision support, with an emphasis on narrative data. Since the inception of our project as an R29 in 1994, we have been developing methods for preparing raw electronic medical record data, applying and evaluating natural language processing, developing data mining techniques including machine learning, and putting the results to use for clinical care and research.       In this competing continuation, we propose to address the temporal information in the electronic medical record and to apply natural language processing and temporal processing to the task of syndromic surveillance in collaboration with the New York City Department of Health and Mental Hygiene (NYC DOHMH).       We have begun work on a temporal processing system. It extracts temporal assertions stated in narrative reports, uses the MedLEE natural language processor to parse the non-temporal information, infers implicit temporal assertions based on a knowledge base, and produces the information in the form of a simple temporal constraint satisfaction problem. The latter can be used to answer questions about the time of events and the temporal relation between pairs of events. We propose to complete the system, expand the knowledge base, speed computation, address the uncertainty of temporal assertions, incorporate temporal information from structured data, and evaluate the system.       NYC DOHMH has a mature syndromic surveillance system that watches over almost eight million persons, and it has as-yet unexploited data sources in the form of narrative and structured electronic medical records. We propose to apply natural language processing and our proposed temporal processing to convert the data to a form appropriate for surveillance. We will evaluate the incremental benefit of structured data, narrative data, and temporally processed narrative data.           n/a",Discovering and Applying Knowledge in Clinical Databases,7147611,R01LM006910,"['artificial intelligence', 'classification', 'clinical research', 'computer assisted medical decision making', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'health care facility information system', 'human data', 'information system analysis', 'method development', 'vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,372106,0.03528039648750205
Biosurveillance Utilizing SNOMED-CT Based Natural Language Processing No abstract available n/a,Biosurveillance Utilizing SNOMED-CT Based Natural Language Processing,7119574,R01PH000022,[' '],PHPPO,MAYO CLINIC,R01,2006,834535,-0.004123987064545239
"A Context-Sensitive Teleconsultation Infrastructure DESCRIPTION (provided by applicant):  Consultation with appropriate specialists improves the quality of healthcare, particularly in patients with complicated cases or chronic illnesses. And for the majority of such patients, specialists use imaging studies (e.g., MR, CT) to objectively document the disease process (e.g., a cancer patient on chemotherapy). However, specialists are generally not available in all communities, tending to be concentrated in academic/specialty centers. Thus, to facilitate the routine use of teleconsultations for patients when specialists are not locally present: 1) the images captured to document the patient's condition must be incorporated into the medical record to enable proper review; and 2) the remote consultant should only receive pertinent parts of the medical record to streamline the consultation process. This proposal is focused on developing and testing a ""context-sensitive"" telehealth infrastructure based on: 1) automated incorporation of clinical context (patient presentation and referring physician hypothesis) to focus the consultation process; 2) a knowledge-base derived from data mining of natural language processing (NLP) results, mapping patient presentation to select an appropriate imaging study based on anatomical region and imaging parameters; and 3) automated selection of key anatomical structures in the acquired imaging study through the use of a contrast-customizable atlas and rigid body/deformable registration algorithms. Collectively, these technologies will allow context-sensitive, automated summarization of medical records for telehealth in a real-world environment. The proposed technologies will be implemented for neurological and musculoskeletal domains, two areas that are MR imaging intensive.      Technical evaluation will be performed with experts serving as the reference standard and will focus on measuring: 1) the accuracy of the corpus based, NLP-guided knowledge-base in selecting relevant anatomical structures; and 2) the accuracy of anatomical structure delineation using the customizable atlas registration methods. Clinical evaluation will be conducted in a real-world teleconsultation environment in a before/after study design using two performance metrics: 1) the time required for consultations; and 2) the effect on the quality of the consultations. n/a",A Context-Sensitive Teleconsultation Infrastructure,7115855,R01EB002247,"['anatomy', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'computer assisted patient care', 'computer data analysis', 'computer graphics /printing', 'computer system design /evaluation', 'diagnosis quality /standard', 'health care quality', 'health care referral /consultation', 'human data', 'image enhancement', 'image processing', 'magnetic resonance imaging', 'musculoskeletal disorder', 'nervous system disorder', 'statistics /biometry', 'telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2006,396341,-0.0036275802008840672
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7019753,G08LM008983,"['clinical research', 'public health']",NLM,SYRACUSE UNIVERSITY,G08,2006,149472,0.01447404390852661
"A Biomedical Natural Language Processing Resource DESCRIPTION:    The long-term aim of this project is to advance clinical care and biomedical research by establishing a natural language processing (NLP) resource for the biomedical community. A major bottleneck for development of automated tools for clinical applications and biomedical research is that most of the data and knowledge occur in the form of text, resulting in a lack of coded data. This NLP resource will make possible a host of automated applications by enabling high throughput access to coded biomedical knowledge and data. The foundation of this resource will be the MedLEE NLP system, which has been used operationally for almost a decade in healthcare settings for a broad range of applications that have proven to be valuable for clinical care. The NLP resource will also include BioMedLEE (a derivative of MedLEE), which encodes genotypic-phenotypic (GP) relations in the scientific literature. It currently focuses on GP relations associated with cancer and infectious diseases, and is being used to organize the extracted information to facilitate research, curation, and ontological development within model organism databases. This proposal will enable us to 1) disseminate our NLP resource to the community, 2) conduct technological research and development (R&D) to facilitate expansion and adaptation of the resource to new applications and specialties, 3) conduct R&D of tools that facilitate use of the extracted data and knowledge after coding, and 4) promote the resource, and provide service to users in the form of technical support, documentation, and tutorials. MedLEE and BioMedLEE are extendable systems that encompass the clinical and scientific communities. The dissemination of a proven NLP system that is applicable to the entire biomedical community provides an exceptional opportunity for multiple developers and researchers to work to unleash the true potential of NLP technology, increasing development of applications that aim to enhance scientific research and improve all levels of health. n/a",A Biomedical Natural Language Processing Resource,6899974,R01LM008635,"['artificial intelligence', 'automated medical record system', 'bioinformatics', 'biomedical automation', 'biomedical resource', 'clinical research', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data management', 'health science research', 'high throughput technology', 'human data']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,515359,0.018666939980149726
"Natural Language Processing for Respiratory Surveillance    DESCRIPTION (provided by applicant): The applicant's long-term career goal is to become an independent investigator in biomedical informatics research. She will dedicate her career to developing and evaluating methodologies for improving the processes and outcomes of healthcare using data locked in textual documents. This career development award will provide her with initial support for achieving her career goals.      The applicant has three goals for her career development over the next three years. First, she will compare the performance of different machine learning techniques at detecting patients with a respiratory syndrome. The proposed research will expand the state-of-the-art syndromic surveillance capabilities by integrating findings, symptoms, and diseases described in textual medical records. The product of the first research goal will be a model for respiratory syndromic case detection for monitoring natural and bioterrorism induced respiratory outbreaks. Second, she will apply existing methods and develop new techniques for extracting clinical conditions required for respiratory case detection from emergency department notes, contributing new knowledge to the medical language processing field using sentence and report level models that account for uncertainty, negation, and temporal occurrence. The product of the second goal will be a better understanding of the information required for accurate detection of respiratory related conditions from text and useful tools for automatically extracting that information. Third, she will teach, promote, and facilitate the use of natural language processing in the biomedical informatics field. The product of the third goal will be a graduate class surveying medical language processing methodology and applications and development of general tools sets for researchers who need encoded data from textual patient records.        The proposed research will focus on:   Aim 1. Development and evaluation of a respiratory case detection model;   Aim 2. Integration of existing natural language processing tools and development of new methodologies for extracting clinical conditions needed for respiratory case detection from textual records; and   Aim 3. Comparison of existing syndromic detection algorithms that use admit data against the same algorithms using conditions extracted from textual reports.         n/a",Natural Language Processing for Respiratory Surveillance,6898458,K22LM008301,"['automated data processing', 'automated medical record system', 'bioinformatics', 'bioterrorism /chemical warfare', 'clinical research', 'communicable disease diagnosis', 'computer assisted diagnosis', 'computer system design /evaluation', 'diagnosis design /evaluation', 'disease outbreaks', 'early diagnosis', 'hospital utilization', 'human data', 'language translation', 'respiratory disorder epidemiology', 'severe acute respiratory syndrome', 'sign /symptom']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K22,2005,162000,0.03261359107878053
"Discovering and Applying Knowledge in Clinical Databases DESCRIPTION (provided by applicant):     With the advent of improved clinical information system products (e.g., ambulatory systems, order entry  systems), improved data entry technologies (e.g., speech recognition, text processing techniques), and  further adoption of data interchange standards, more institutions are generating electronic medical records, and these records will expand in breadth, depth, and degree of coding in the future. The records are used mainly for individual patient care, but exploiting the records for clinical research and quality functions has lagged behind. Major challenges include the wide range of complex data and missing and inaccurate data.      We propose to continue our work to develop and test methods to mine a clinical data repository. A  special emphasis will be to exploit the vast amount of information in the repository (latent associations and knowledge) and to use computer intensive techniques and advances in data representation and manipulation to better interpret what is in the database and to overcome the challenges of complex, missing, and inaccurate data. We hypothesize that data mining techniques can be applied to a repository to generate accurate clinical interpretations. We further hypothesize that associations latent in a clinically rich repository can be used to improve the classification of cases in that repository.      We aim to develop methods to prepare data for mining; to characterize the information in the clinical data repository; to develop similarity measures based on manipulation of natural language processor output and on information retrieval techniques; to apply nearest neighbor technique and case-based reasoning to improve classification; to develop a statistically based method to improve classification of cases with incomplete or inaccurate data; and to apply our methods to real clinical research questions and carry out additional data mining research.      The researchers in the Department of Medical Informatics at Columbia University are uniquely positioned to carry out this research, given the experience of the team (data mining, statistics, health data organization, health knowledge representation, natural language processing), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data. n/a",Discovering and Applying Knowledge in Clinical Databases,6892934,R01LM006910,"['artificial intelligence', 'classification', 'clinical research', 'computer assisted medical decision making', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'health care facility information system', 'human data', 'information system analysis', 'method development', 'vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,384538,0.02771779533215385
"A Context-Sensitive Teleconsultation Infrastructure DESCRIPTION (provided by applicant):  Consultation with appropriate specialists improves the quality of healthcare, particularly in patients with complicated cases or chronic illnesses. And for the majority of such patients, specialists use imaging studies (e.g., MR, CT) to objectively document the disease process (e.g., a cancer patient on chemotherapy). However, specialists are generally not available in all communities, tending to be concentrated in academic/specialty centers. Thus, to facilitate the routine use of teleconsultations for patients when specialists are not locally present: 1) the images captured to document the patient's condition must be incorporated into the medical record to enable proper review; and 2) the remote consultant should only receive pertinent parts of the medical record to streamline the consultation process. This proposal is focused on developing and testing a ""context-sensitive"" telehealth infrastructure based on: 1) automated incorporation of clinical context (patient presentation and referring physician hypothesis) to focus the consultation process; 2) a knowledge-base derived from data mining of natural language processing (NLP) results, mapping patient presentation to select an appropriate imaging study based on anatomical region and imaging parameters; and 3) automated selection of key anatomical structures in the acquired imaging study through the use of a contrast-customizable atlas and rigid body/deformable registration algorithms. Collectively, these technologies will allow context-sensitive, automated summarization of medical records for telehealth in a real-world environment. The proposed technologies will be implemented for neurological and musculoskeletal domains, two areas that are MR imaging intensive.      Technical evaluation will be performed with experts serving as the reference standard and will focus on measuring: 1) the accuracy of the corpus based, NLP-guided knowledge-base in selecting relevant anatomical structures; and 2) the accuracy of anatomical structure delineation using the customizable atlas registration methods. Clinical evaluation will be conducted in a real-world teleconsultation environment in a before/after study design using two performance metrics: 1) the time required for consultations; and 2) the effect on the quality of the consultations. n/a",A Context-Sensitive Teleconsultation Infrastructure,6948251,R01EB002247,"['anatomy', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'computer assisted patient care', 'computer data analysis', 'computer graphics /printing', 'computer system design /evaluation', 'diagnosis quality /standard', 'health care quality', 'health care referral /consultation', 'human data', 'image enhancement', 'image processing', 'magnetic resonance imaging', 'musculoskeletal disorder', 'nervous system disorder', 'statistics /biometry', 'telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2005,399327,-0.0036275802008840672
"Natural Language Processing for Respiratory Surveillance    DESCRIPTION (provided by applicant): The applicant's long-term career goal is to become an independent investigator in biomedical informatics research. She will dedicate her career to developing and evaluating methodologies for improving the processes and outcomes of healthcare using data locked in textual documents. This career development award will provide her with initial support for achieving her career goals.      The applicant has three goals for her career development over the next three years. First, she will compare the performance of different machine learning techniques at detecting patients with a respiratory syndrome. The proposed research will expand the state-of-the-art syndromic surveillance capabilities by integrating findings, symptoms, and diseases described in textual medical records. The product of the first research goal will be a model for respiratory syndromic case detection for monitoring natural and bioterrorism induced respiratory outbreaks. Second, she will apply existing methods and develop new techniques for extracting clinical conditions required for respiratory case detection from emergency department notes, contributing new knowledge to the medical language processing field using sentence and report level models that account for uncertainty, negation, and temporal occurrence. The product of the second goal will be a better understanding of the information required for accurate detection of respiratory related conditions from text and useful tools for automatically extracting that information. Third, she will teach, promote, and facilitate the use of natural language processing in the biomedical informatics field. The product of the third goal will be a graduate class surveying medical language processing methodology and applications and development of general tools sets for researchers who need encoded data from textual patient records.        The proposed research will focus on:   Aim 1. Development and evaluation of a respiratory case detection model;   Aim 2. Integration of existing natural language processing tools and development of new methodologies for extracting clinical conditions needed for respiratory case detection from textual records; and   Aim 3. Comparison of existing syndromic detection algorithms that use admit data against the same algorithms using conditions extracted from textual reports.         n/a",Natural Language Processing for Respiratory Surveillance,6768325,K22LM008301,"['automated data processing', 'automated medical record system', 'bioinformatics', 'bioterrorism /chemical warfare', 'clinical research', 'communicable disease diagnosis', 'computer assisted diagnosis', 'computer system design /evaluation', 'diagnosis design /evaluation', 'disease outbreaks', 'early diagnosis', 'hospital utilization', 'human data', 'language translation', 'respiratory disorder epidemiology', 'severe acute respiratory syndrome', 'sign /symptom']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K22,2004,135000,0.03261359107878053
"Discovering and Applying Knowledge in Clinical Databases DESCRIPTION (provided by applicant):     With the advent of improved clinical information system products (e.g., ambulatory systems, order entry  systems), improved data entry technologies (e.g., speech recognition, text processing techniques), and  further adoption of data interchange standards, more institutions are generating electronic medical records, and these records will expand in breadth, depth, and degree of coding in the future. The records are used mainly for individual patient care, but exploiting the records for clinical research and quality functions has lagged behind. Major challenges include the wide range of complex data and missing and inaccurate data.      We propose to continue our work to develop and test methods to mine a clinical data repository. A  special emphasis will be to exploit the vast amount of information in the repository (latent associations and knowledge) and to use computer intensive techniques and advances in data representation and manipulation to better interpret what is in the database and to overcome the challenges of complex, missing, and inaccurate data. We hypothesize that data mining techniques can be applied to a repository to generate accurate clinical interpretations. We further hypothesize that associations latent in a clinically rich repository can be used to improve the classification of cases in that repository.      We aim to develop methods to prepare data for mining; to characterize the information in the clinical data repository; to develop similarity measures based on manipulation of natural language processor output and on information retrieval techniques; to apply nearest neighbor technique and case-based reasoning to improve classification; to develop a statistically based method to improve classification of cases with incomplete or inaccurate data; and to apply our methods to real clinical research questions and carry out additional data mining research.      The researchers in the Department of Medical Informatics at Columbia University are uniquely positioned to carry out this research, given the experience of the team (data mining, statistics, health data organization, health knowledge representation, natural language processing), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data. n/a",Discovering and Applying Knowledge in Clinical Databases,6754395,R01LM006910,"['artificial intelligence', 'classification', 'clinical research', 'computer assisted medical decision making', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'health care facility information system', 'human data', 'information system analysis', 'method development', 'vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,380979,0.02771779533215385
"Statistical NLP Analysis of Cross-discipline Clinical Text DESCRIPTION (provided by applicant):     An emerging trend in computational linguistics is melding natural language processing (NLP) and machine learning (ML) to help computers make sense of human-generated free text. The blending of these disciplines is relatively rare in biomedical informatics. Past medical NLP/ML research work is biased heavily towards linguistic methods that attempt to reason about grammar and syntax aided by a domain-focal knowledge base (e.g., one for radiology or one for clinical pathology). The aim of the work proposed here takes a different tack: exploring the utility of a statistical approach to clinical NLP, one augmented by machine learning and concentrating on general progress notes from across multiple clinical domains. The specific clinical goal will be to identify adverse drug events described implicitly or explicitly in inpatient progress notes. Rather than relying on a narrow domain focus to provide enough context restriction to make text interpretation tractable, this approach will use statistical patterns in note author information (e.g., profession, note type, treating ward) and patient information (e.g., admit diagnosis, procedures performed, temporal note relationships) for context restriction. The research component of this proposal is divided into two categories: three small-scale projects designed to rapidly hone new skills developed under the training component, and a large-scale project that assesses the feasibility of cross-discipline clinical text analysis. n/a",Statistical NLP Analysis of Cross-discipline Clinical Text,6836781,F38LM008478,"['bioinformatics', 'clinical research', 'computational biology', 'human data', 'library', 'mathematical model', 'public health', 'statistics /biometry']",NLM,UNIVERSITY OF UTAH,F38,2004,94545,0.03600085331414658
"A Context-Sensitive Teleconsultation Infrastructure DESCRIPTION (provided by applicant):  Consultation with appropriate specialists improves the quality of healthcare, particularly in patients with complicated cases or chronic illnesses. And for the majority of such patients, specialists use imaging studies (e.g., MR, CT) to objectively document the disease process (e.g., a cancer patient on chemotherapy). However, specialists are generally not available in all communities, tending to be concentrated in academic/specialty centers. Thus, to facilitate the routine use of teleconsultations for patients when specialists are not locally present: 1) the images captured to document the patient's condition must be incorporated into the medical record to enable proper review; and 2) the remote consultant should only receive pertinent parts of the medical record to streamline the consultation process. This proposal is focused on developing and testing a ""context-sensitive"" telehealth infrastructure based on: 1) automated incorporation of clinical context (patient presentation and referring physician hypothesis) to focus the consultation process; 2) a knowledge-base derived from data mining of natural language processing (NLP) results, mapping patient presentation to select an appropriate imaging study based on anatomical region and imaging parameters; and 3) automated selection of key anatomical structures in the acquired imaging study through the use of a contrast-customizable atlas and rigid body/deformable registration algorithms. Collectively, these technologies will allow context-sensitive, automated summarization of medical records for telehealth in a real-world environment. The proposed technologies will be implemented for neurological and musculoskeletal domains, two areas that are MR imaging intensive.      Technical evaluation will be performed with experts serving as the reference standard and will focus on measuring: 1) the accuracy of the corpus based, NLP-guided knowledge-base in selecting relevant anatomical structures; and 2) the accuracy of anatomical structure delineation using the customizable atlas registration methods. Clinical evaluation will be conducted in a real-world teleconsultation environment in a before/after study design using two performance metrics: 1) the time required for consultations; and 2) the effect on the quality of the consultations. n/a",A Context-Sensitive Teleconsultation Infrastructure,6802269,R01EB002247,"['anatomy', 'clinical research', 'computer assisted diagnosis', 'computer assisted medical decision making', 'computer assisted patient care', 'computer data analysis', 'computer graphics /printing', 'computer system design /evaluation', 'diagnosis quality /standard', 'health care quality', 'health care referral /consultation', 'human data', 'image enhancement', 'image processing', 'magnetic resonance imaging', 'musculoskeletal disorder', 'nervous system disorder', 'statistics /biometry', 'telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2004,387825,-0.0036275802008840672
"Discovering and Applying Knowledge in Clinical Databases DESCRIPTION (provided by applicant):     With the advent of improved clinical information system products (e.g., ambulatory systems, order entry  systems), improved data entry technologies (e.g., speech recognition, text processing techniques), and  further adoption of data interchange standards, more institutions are generating electronic medical records, and these records will expand in breadth, depth, and degree of coding in the future. The records are used mainly for individual patient care, but exploiting the records for clinical research and quality functions has lagged behind. Major challenges include the wide range of complex data and missing and inaccurate data.      We propose to continue our work to develop and test methods to mine a clinical data repository. A  special emphasis will be to exploit the vast amount of information in the repository (latent associations and knowledge) and to use computer intensive techniques and advances in data representation and manipulation to better interpret what is in the database and to overcome the challenges of complex, missing, and inaccurate data. We hypothesize that data mining techniques can be applied to a repository to generate accurate clinical interpretations. We further hypothesize that associations latent in a clinically rich repository can be used to improve the classification of cases in that repository.      We aim to develop methods to prepare data for mining; to characterize the information in the clinical data repository; to develop similarity measures based on manipulation of natural language processor output and on information retrieval techniques; to apply nearest neighbor technique and case-based reasoning to improve classification; to develop a statistically based method to improve classification of cases with incomplete or inaccurate data; and to apply our methods to real clinical research questions and carry out additional data mining research.      The researchers in the Department of Medical Informatics at Columbia University are uniquely positioned to carry out this research, given the experience of the team (data mining, statistics, health data organization, health knowledge representation, natural language processing), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data. n/a",Discovering and Applying Knowledge in Clinical Databases,6630735,R01LM006910,"['artificial intelligence', ' classification', ' clinical research', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' information system analysis', ' method development', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,377617,0.02771779533215385
"Mining Complex Clinical Data for Patient Safety Research Medical errors hurt patients, cost money, and undermine the health care system. The first step to reducing errors is detecting them, for what cannot be detected cannot be managed. A number of approaches have been applied to medical error detection, including mandatory event reporting, voluntary near-miss reporting, chart review, and automated surveillance using information systems. Automated surveillance promises large-scale detection, minimal labor, and, potentially, detection in real time to prevent or recover from errors. Unfortunately, large amounts of important clinical information lie locked in narrative reports, unavailable to automated decision support systems. A number of tools have emerged from medical informatics and computer science- natural language processing, visualization tools, and machine learning- as well as methods for understanding cognitive processes. We hypothesize that the electronic medical record contains information useful for detecting errors and that natural language processing and other tools will allow us to retrieve the information. We will assemble a team skilled in natural language processing, data mining, terminology, patient safety research, and health care. We will use a clinical repository with ten years of data on two million patients. It includes administrative, laboratory, and pharmacy coded information as well as a wide range of narrative reports including discharge summaries, operative reports, outpatient notes, autopsy reports, resident signout notes, nursing notes, and reports from numerous ancillary services (radiology, pathology, etc.). We will apply a proven natural language processor called MedLEE to code the information and measure the accuracy of automated queries to detect and characterize errors. We will target several areas: explicit error reporting in the medical record, NYPORTS mandatory event reporting, clinical conflicts in record, and other sources of error information. We will use a systems approach to errors and cognitive analysis to uncover cues to improve error detection. We will incorporate the system into the hospital's current event surveillance program and assess the impact on error detection. We will adhere to strict privacy policies and security procedures. This project represents a unique opportunity to apply the most advanced medical language processing system to a large, comprehensive clinical repository to advance patient safety research.  n/a",Mining Complex Clinical Data for Patient Safety Research,6657426,R18HS011806,"['automated medical record system', ' clinical research', ' computer data analysis', ' diagnosis quality /standard', ' health service demonstration project', ' human data', ' iatrogenic disease', ' patient care', ' patient safety /medical error']",AHRQ,COLUMBIA UNIVERSITY HEALTH SCIENCES,R18,2003,369494,0.0190758349336354
"A Context-Sensitive Teleconsultation Infrastructure DESCRIPTION (provided by applicant):  Consultation with appropriate specialists improves the quality of healthcare, particularly in patients with complicated cases or chronic illnesses. And for the majority of such patients, specialists use imaging studies (e.g., MR, CT) to objectively document the disease process (e.g., a cancer patient on chemotherapy). However, specialists are generally not available in all communities, tending to be concentrated in academic/specialty centers. Thus, to facilitate the routine use of teleconsultations for patients when specialists are not locally present: 1) the images captured to document the patient's condition must be incorporated into the medical record to enable proper review; and 2) the remote consultant should only receive pertinent parts of the medical record to streamline the consultation process. This proposal is focused on developing and testing a ""context-sensitive"" telehealth infrastructure based on: 1) automated incorporation of clinical context (patient presentation and referring physician hypothesis) to focus the consultation process; 2) a knowledge-base derived from data mining of natural language processing (NLP) results, mapping patient presentation to select an appropriate imaging study based on anatomical region and imaging parameters; and 3) automated selection of key anatomical structures in the acquired imaging study through the use of a contrast-customizable atlas and rigid body/deformable registration algorithms. Collectively, these technologies will allow context-sensitive, automated summarization of medical records for telehealth in a real-world environment. The proposed technologies will be implemented for neurological and musculoskeletal domains, two areas that are MR imaging intensive.      Technical evaluation will be performed with experts serving as the reference standard and will focus on measuring: 1) the accuracy of the corpus based, NLP-guided knowledge-base in selecting relevant anatomical structures; and 2) the accuracy of anatomical structure delineation using the customizable atlas registration methods. Clinical evaluation will be conducted in a real-world teleconsultation environment in a before/after study design using two performance metrics: 1) the time required for consultations; and 2) the effect on the quality of the consultations. n/a",A Context-Sensitive Teleconsultation Infrastructure,6725819,R01EB002247,"['anatomy', ' clinical research', ' computer assisted diagnosis', ' computer assisted medical decision making', ' computer assisted patient care', ' computer data analysis', ' computer graphics /printing', ' computer system design /evaluation', ' diagnosis quality /standard', ' health care quality', ' health care referral /consultation', ' human data', ' image enhancement', ' image processing', ' magnetic resonance imaging', ' musculoskeletal disorder', ' nervous system disorder', ' statistics /biometry', ' telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2003,383268,-0.0036275802008840672
"Mining Complex Clinical Data for Patient Safety Research Medical errors hurt patients, cost money, and undermine the health care system. The first step to reducing errors is detecting them, for what cannot be detected cannot be managed. A number of approaches have been applied to medical error detection, including mandatory event reporting, voluntary near-miss reporting, chart review, and automated surveillance using information systems. Automated surveillance promises large-scale detection, minimal labor, and, potentially, detection in real time to prevent or recover from errors. Unfortunately, large amounts of important clinical information lie locked in narrative reports, unavailable to automated decision support systems. A number of tools have emerged from medical informatics and computer science- natural language processing, visualization tools, and machine learning- as well as methods for understanding cognitive processes. We hypothesize that the electronic medical record contains information useful for detecting errors and that natural language processing and other tools will allow us to retrieve the information. We will assemble a team skilled in natural language processing, data mining, terminology, patient safety research, and health care. We will use a clinical repository with ten years of data on two million patients. It includes administrative, laboratory, and pharmacy coded information as well as a wide range of narrative reports including discharge summaries, operative reports, outpatient notes, autopsy reports, resident signout notes, nursing notes, and reports from numerous ancillary services (radiology, pathology, etc.). We will apply a proven natural language processor called MedLEE to code the information and measure the accuracy of automated queries to detect and characterize errors. We will target several areas: explicit error reporting in the medical record, NYPORTS mandatory event reporting, clinical conflicts in record, and other sources of error information. We will use a systems approach to errors and cognitive analysis to uncover cues to improve error detection. We will incorporate the system into the hospital's current event surveillance program and assess the impact on error detection. We will adhere to strict privacy policies and security procedures. This project represents a unique opportunity to apply the most advanced medical language processing system to a large, comprehensive clinical repository to advance patient safety research.  n/a",Mining Complex Clinical Data for Patient Safety Research,6528316,R18HS011806,"['automated medical record system', ' clinical research', ' computer data analysis', ' diagnosis quality /standard', ' health service demonstration project', ' human data', ' iatrogenic disease', ' patient care', ' patient safety /medical error']",AHRQ,COLUMBIA UNIVERSITY HEALTH SCIENCES,R18,2002,360015,0.0190758349336354
"Human Subject Research Enhancements Program We propose to enhance the data consistency and integrity of oversight and tracking systems for human subjects research at Mayo Foundation. Our specific aims include: 1) a comprehensive information modeling exercise to understand the interrelationships and dependencies of administrative and clinical data elements related to human subjects research oversight; 2) building common application components that will simplify the creation of research protocols, IRB application, research subject enrollment and consent, and administrative tracking; 3) providing full text and natural language processing based indices to project abstracts, applications, minutes, and administrative notes, to facilitate the authorized searching and retrieval of materials human subject related to human subject review; and 4) coordinating the information model, modular software tools, and textual indexing, as preliminary work for a competitive informatics proposal for adverse event recognition, pattern detection, and the consistent recording of drugs, devices and outcomes measures. n/a",Human Subject Research Enhancements Program,6591449,S07RR018225,"['abstracting', ' behavioral /social science research tag', ' clinical research', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' health science research support', ' human rights', ' information systems']",NCRR,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",S07,2002,1,0.010873553034304121
"UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING   DESCRIPTION (adapted from the Abstract):                                              The long-term aim of this project is to use natural language processing (NLP)        to help realize the full potential of the Electronic Medical Record (EMR).           Our research involves advanced NLP techniques to: 1) extract and encode              information in textual reports; 2) map terms to an authoritative vocabulary;         3) obtain comprehensive domain coverage based on the processing of domain            corpora; and 4) facilitate vocabulary development by providing visualization         tools using the Extensible Markup Language (XML).  It has already been               demonstrated that MedLEE, the NLP system we developed, accurately extracts and       codifies information in the EMR.  This current project builds upon our               experience with MedLEE and uses it to accomplish the latter three goals              concerning vocabulary development and standardization.                                                                                                                    More specifically, MedLEE will be used to map source terms to UMLS concepts.         MedLEE will process and structure the source terms and candidate UMLS                concepts.  Suitable matches will be found based on structural similarity             between components of the source term and candidate concepts.  This should           enhance current methods because knowledge of the type of modifiers that match        should improve the quality of the matches.  We will also use MedLEE to process       a large corpus and generate structured output in XML format.  Statistics based       on the structured output will be computed, and then clinically relevant              composite terms will be detected based on frequencies of the structures              containing the more elementary terms.  Our method differs from other discovery       methods because we use NLP techniques that identify semantic modifiers and           complex relations even if the terms are distant from each other, whereas other       methods use statistical co-occurrence data based on adjacency.  The individual       XML structures and statistics will be combined and mapped into a single XML          tree.  It will be possible to visualize the tree and frequencies using an XML        tree viewer, to navigate the tree, to manipulate the tree, and to reorganize         the tree according to different axes (i.e., procedure, body location,                finding).                                                                                                                                                                 The use of a sophisticated NLP system, such as MedLEE, is ideal as a                 foundation for our proposed work in vocabulary development and                       standardization; medical terminology is an integral part of medical language         and a state of the art NLP system is especially equipped to handle the               inherent complexities of language. n/a",UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING,6490773,R01LM006274,"['abstracting', ' automated medical record system', ' computer system design /evaluation', ' human data', ' information retrieval', ' method development', ' vocabulary development for information system']",NLM,QUEENS COLLEGE,R01,2002,288252,0.0512315127579946
"Mining Complex Clinical Data for Patient Safety Research Medical errors hurt patients, cost money, and undermine the health care system. The first step to reducing errors is detecting them, for what cannot be detected cannot be managed. A number of approaches have been applied to medical error detection, including mandatory event reporting, voluntary near-miss reporting, chart review, and automated surveillance using information systems. Automated surveillance promises large-scale detection, minimal labor, and, potentially, detection in real time to prevent or recover from errors. Unfortunately, large amounts of important clinical information lie locked in narrative reports, unavailable to automated decision support systems. A number of tools have emerged from medical informatics and computer science- natural language processing, visualization tools, and machine learning- as well as methods for understanding cognitive processes. We hypothesize that the electronic medical record contains information useful for detecting errors and that natural language processing and other tools will allow us to retrieve the information. We will assemble a team skilled in natural language processing, data mining, terminology, patient safety research, and health care. We will use a clinical repository with ten years of data on two million patients. It includes administrative, laboratory, and pharmacy coded information as well as a wide range of narrative reports including discharge summaries, operative reports, outpatient notes, autopsy reports, resident signout notes, nursing notes, and reports from numerous ancillary services (radiology, pathology, etc.). We will apply a proven natural language processor called MedLEE to code the information and measure the accuracy of automated queries to detect and characterize errors. We will target several areas: explicit error reporting in the medical record, NYPORTS mandatory event reporting, clinical conflicts in record, and other sources of error information. We will use a systems approach to errors and cognitive analysis to uncover cues to improve error detection. We will incorporate the system into the hospital's current event surveillance program and assess the impact on error detection. We will adhere to strict privacy policies and security procedures. This project represents a unique opportunity to apply the most advanced medical language processing system to a large, comprehensive clinical repository to advance patient safety research.  n/a",Mining Complex Clinical Data for Patient Safety Research,6448720,R18HS011806,"['automated medical record system', ' clinical research', ' computer data analysis', ' diagnosis quality /standard', ' health service demonstration project', ' human data', ' iatrogenic disease', ' patient care', ' patient safety /medical error']",AHRQ,COLUMBIA UNIVERSITY HEALTH SCIENCES,R18,2001,356099,0.0190758349336354
"UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING   DESCRIPTION (adapted from the Abstract):                                              The long-term aim of this project is to use natural language processing (NLP)        to help realize the full potential of the Electronic Medical Record (EMR).           Our research involves advanced NLP techniques to: 1) extract and encode              information in textual reports; 2) map terms to an authoritative vocabulary;         3) obtain comprehensive domain coverage based on the processing of domain            corpora; and 4) facilitate vocabulary development by providing visualization         tools using the Extensible Markup Language (XML).  It has already been               demonstrated that MedLEE, the NLP system we developed, accurately extracts and       codifies information in the EMR.  This current project builds upon our               experience with MedLEE and uses it to accomplish the latter three goals              concerning vocabulary development and standardization.                                                                                                                    More specifically, MedLEE will be used to map source terms to UMLS concepts.         MedLEE will process and structure the source terms and candidate UMLS                concepts.  Suitable matches will be found based on structural similarity             between components of the source term and candidate concepts.  This should           enhance current methods because knowledge of the type of modifiers that match        should improve the quality of the matches.  We will also use MedLEE to process       a large corpus and generate structured output in XML format.  Statistics based       on the structured output will be computed, and then clinically relevant              composite terms will be detected based on frequencies of the structures              containing the more elementary terms.  Our method differs from other discovery       methods because we use NLP techniques that identify semantic modifiers and           complex relations even if the terms are distant from each other, whereas other       methods use statistical co-occurrence data based on adjacency.  The individual       XML structures and statistics will be combined and mapped into a single XML          tree.  It will be possible to visualize the tree and frequencies using an XML        tree viewer, to navigate the tree, to manipulate the tree, and to reorganize         the tree according to different axes (i.e., procedure, body location,                finding).                                                                                                                                                                 The use of a sophisticated NLP system, such as MedLEE, is ideal as a                 foundation for our proposed work in vocabulary development and                       standardization; medical terminology is an integral part of medical language         and a state of the art NLP system is especially equipped to handle the               inherent complexities of language. n/a",UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING,6095940,R01LM006274,"['abstracting', ' automated medical record system', ' computer system design /evaluation', ' human data', ' information retrieval', ' method development', ' vocabulary development for information system']",NLM,QUEENS COLLEGE,R01,2001,303860,0.0512315127579946
"IMIA WG6 CONFERENCE The basic science of representing patient events, findings, interventions, and outcomes in a semantically consistent and logically reproducible way is medical concept representation.  It embodies principles of linguistics, logic, computer science, cognition, biology and clinical medicine to undertake this highly multidisciplinary activity. Much of this work is undertaken in experimental settings, which hypothesize practical extensions to existing models, and test their utility against standardized retrieval sets or clinical usability environments. The proposed conference intends to continue the tradition of the International Medical Informatics Association (IMIA), Working Group 6 on Medical Concept Representation, to provide a forum for the academic discussion of problems, issues, theories, and applications of natural language processing, knowledge representation, terminology development, and concept coordination to biomedicine and healthcare.  the proposed tracks at this time are: 1. Natural Language Processing  2. Clinical Classifications 3. Cognitive Evaluations  4. Terminology Models  5. Maintenance and Uptake Strategies.  n/a",IMIA WG6 CONFERENCE,6027283,R13LM006899,"['informatics', ' international health /scientific organization', ' meeting /conference /symposium', ' travel']",NLM,MAYO CLINIC ROCHESTER,R13,2000,20000,0.0023389096572341353
"A framework to enhance radiology structured report by invoking NLP and DL:  Models and Applications PROJECT SUMMARY/ABSTRACT  In radiology practices, timely and accurate formulation of reports is closely linked to patient satisfaction, physician productivity, and reimbursement. While the American College of Radiology and the Radiological Soci- ety of North America have recommended implementation of structured reporting to facilitate clear and consistent communication between radiologists and referring clinicians, cumbersome nature of current structured reporting systems made them unpopular amongst their users. Recently, the emerging techniques of deep learning have been widely and successfully applied in many different natural language processing tasks (NLP). However, when adopted in a certain speciﬁc domain, such as radiology, these techniques should be combined with extensive domain knowledge to improve efﬁciency and accuracy. There is, therefore, a critical need to take advantage of clinical NLP and deep learning to fundamentally change the radiology reporting. The long-term goal in this appli- cation is to improve the form, content, and quality of radiology reports and to facilitate rapid generation of radiol- ogy reports with consistent organization and standardized texts. The overall objective is to use radiology-speciﬁc ontology, NLP and computer vision techniques, and deep learning to construct a radiology-speciﬁc knowledge graph, which will then be used to build a reporting system that can assist radiologists to quickly generate struc- tured and standardized text reports. The rationale for this project is that through integration of new clinical NLP technologies, radiology-speciﬁc knowledge graphs, and development of new reporting system, we can build au- tomatous systems with a higher-level understanding of the radiological world. The speciﬁc aims of this project are to: (1) recognize and normalize named entities in radiology reports; (2) construct a radiology-speciﬁc knowledge graph from free-text and images; and (3) build a reporting system that can dynamically adjust templates based on radiologists' prior entries. The research proposed in this application is innovative, in the applicant's opinion, because it combines deep learning, NLP techniques, and domain knowledge in a single framework to construct comprehensive and accurate knowledge graphs that will enhance the workﬂow of the current reporting systems. The proposed research is signiﬁcant because a novel reporting system can expedite radiologists' workﬂow and acquire well-annotated datasets that facilitate machine learning and data science. To develop such a method, the candidate, Dr. Yifan Peng, requires additional training and mentoring in clinical NLP and radiology. During the K99 phase, Dr. Peng will conduct this research as a research fellow at the National Center for Biotechnology Information. He will be mentored by Dr. Zhiyong Lu, a leading text mining and deep learning researcher, and co- mentored by Dr. Ronald M. Summers, a leading radiologist and clinical informatics researcher. This application for the NIH Pathway to Independence Award (K99/R00) describes a career development plan that will allow Dr. Peng to achieve the career goals of becoming an independent investigator and leader in the study of clinical NLP. PROJECT NARRATIVE The proposed research is relevant to public health because it entails a new strategy to construct a radiology- speciﬁc knowledge graph to facilitate the development of a new reporting system that enables rapid generation of structured radiology reports. The proposed knowledge graph and reporting system will contribute to advancement in understanding of the radiological world, and promise to enhance clinical communication and patient-centric care. Thus, the proposed research is relevant to the part of the NLM's mission that pertains to applying deep knowledge of clinical terminology and natural language processing to improve clinical data science and health services.",A framework to enhance radiology structured report by invoking NLP and DL:  Models and Applications,10197509,R00LM013001,"['Address', 'Adopted', 'American College of Radiology', 'Award', 'Biotechnology', 'Caring', 'Client satisfaction', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Communication', 'Complex', 'Computer Vision Systems', 'Data Science', 'Data Set', 'Development', 'Development Plans', 'Formulation', 'Generations', 'Goals', 'Health Services', 'Hospitals', 'Hybrids', 'Image', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Medical', 'Mentors', 'Methods', 'Mission', 'Modeling', 'Mus', 'Names', 'Natural Language Processing', 'Nature', 'Nomenclature', 'North America', 'Ontology', 'Outcome', 'Pathway interactions', 'Patients', 'Phase', 'Physicians', 'Picture Archiving and Communication System', 'Process', 'Productivity', 'Public Health', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Resort', 'Societies', 'Standardization', 'Structure', 'System', 'Systems Development', 'Techniques', 'Technology', 'Terminology', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'Voice', 'Writing', 'base', 'career', 'career development', 'convolutional neural network', 'deep learning', 'deep neural network', 'impression', 'improved', 'innovation', 'knowledge graph', 'lexical', 'long short term memory', 'neural network', 'neural network architecture', 'novel', 'radiologist', 'repository', 'response', 'syntax', 'text searching']",NLM,WEILL MEDICAL COLL OF CORNELL UNIV,R00,2020,236549,0.05579778493008089
"CLAMP-CS: a Cloud-based, Service-oriented, high-performance Natural Language Processing Platform for Healthcare Project Summary Wide adoption of electronic health records (EHRs) has led to huge clinical databases, which enable the rapid growth of healthcare analytics market. One particular challenge for analyzing EHRs data is that much detailed patient information is embedded in clinical documents and not directly available for downstream analysis. Therefore, clinical natural language processing (NLP) technologies, which can unlock information embedded in clinical narratives, have received great attention, with an estimated global market of $2.65 billion by 2021 . In our previous work, we have developed CLAMP (Clinical Language Annotation, Modeling, and Processing), a clinical NLP tool with demonstrated superior performance through multiple international NLP challenges and a large user community (over 1,500 downloads by users from over 700 organizations). Commercialization of CLAMP by Melax Technologies Inc. has been successful (i.e., with a dozen licensed customers now); but it also reveals its limitations as a desktop application in the Cloud era. Therefore, we propose to extend CLAMP to a new Cloud- based, Service-oriented platform (called CLAMP-CS), which will address the identified challenges by: 1) improving clinical NLP performance and reducing annotation cost by leveraging the state-of-the-art algorithms such as deep learning, active learning and transfer learning and making them accessible to less experienced users; 2) following new service-oriented architectures to make CLAMP-CS available via SaaS and PaaS, ready for Cloud-based development and deployment; and 3) improving CLAMP-CS interoperability with downstream applications following two widely used standard representations: HL7 FHIR (Fast Healthcare Interoperability Resources) and OMOP CMD (Common Data Model), to support the use cases in clinical operations and research respectively. With these advanced features, we believe CLAMP-CS will be a leading clinical NLP system in the market and it will accelerate the adoption of NLP technology for diverse healthcare applications and clinical/translational research. Project Narrative In this study, we plan to develop a new clinical natural language processing (NLP) tool based on the existing widely used CLAMP (Clinical Language Annotation, Modeling, and Processing) system, to support enterprise development and deployment of NLP solutions in healthcare. We believe that the new generation of Cloud- based, service-oriented NLP tool will accelerate the adoption of NLP technology for diverse healthcare applications and clinical and translational research.","CLAMP-CS: a Cloud-based, Service-oriented, high-performance Natural Language Processing Platform for Healthcare",10011177,R44TR003254,"['Active Learning', 'Address', 'Adopted', 'Adoption', 'Algorithms', 'Architecture', 'Attention', 'Belief', 'Clinical', 'Clinical Research', 'Closure by clamp', 'Cloud Computing', 'Communities', 'Custom', 'Data', 'Development', 'Diagnosis', 'Electronic Health Record', 'Environment', 'Fast Healthcare Interoperability Resources', 'Generations', 'Grant', 'Growth', 'Health Sciences', 'Healthcare', 'Hospital Administration', 'International', 'Language', 'Licensing', 'Machine Learning', 'Medical', 'Modeling', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Operations Research', 'Output', 'Patients', 'Performance', 'Psychological Transfer', 'Records', 'Research', 'Services', 'System', 'Technology', 'Texas', 'Time', 'Translational Research', 'Universities', 'Work', 'active method', 'base', 'clinical application', 'clinical database', 'cloud based', 'commercialization', 'cost', 'data modeling', 'deep learning', 'deep learning algorithm', 'experience', 'improved', 'insight', 'interoperability', 'language training', 'learning algorithm', 'model building', 'next generation', 'novel', 'prevent', 'rapid growth', 'tool', 'user-friendly', 'web app']",NCATS,"MELAX TECHNOLOGIES, INC.",R44,2020,503546,0.09364155648099816
"Automated domain adaptation for clinical natural language processing Project Summary Automatic extraction of useful information from clinical texts enables new clinical research tasks and new technologies at the point of care. The natural language processing (NLP) systems that perform this extraction rely on supervised machine learning. The learning process uses manually labeled datasets that are limited in size and scope, and as a result, applying NLP systems to unseen datasets often results in severely degraded performance. Obtaining larger and broader datasets is unlikely due to the expense of the manual labeling process and the difficulty of sharing text data between multiple different institutions. Therefore, this project develops unsupervised domain adaptation algorithms to adapt NLP systems to new data. Domain adaptation describes the process of adapting a machine learning system to new data sources. The proposed methods are unsupervised in that they do not require manual labels for the new data. This project has three aims. The first aim makes use of multiple existing datasets for the same task to study the differences in domains, and uses this information to develop new domain adaptation algorithms. Evaluation uses standard machine learning metrics, and analysis of performance is tightly bounded by strong baselines from below and realistic upper bounds, both based on theoretical research on machine learning generalization. The second aim develops open source software tools to simplify the process of incorporating domain adaptation into clinical text processing workflows. This software will have input interfaces to connect to methods developed in Aim 1 and output interfaces to connect with Apache cTAKES, a widely used open- source NLP tool. Aim 3 tests these methods in an end-to-end use case, adverse drug event (ADE) extraction on a dataset of pediatric pulmonary hypertension notes. ADE extraction relies on multiple NLP systems, so this use case is able to show how broad improvements to NLP methods can improve downstream methods. This aim also creates new manual labels for the dataset for an end-to-end evaluation that directly measures how improvements to the NLP systems lead to improvement in ADE extraction. Project Narrative Software systems that use machine learning to understand clinical text often suffer severe performance loss when they are applied to new data that looks different than the data that they originally learned from. In this project, we develop and implement methods that allow these systems to automatically adapt to the characteristics of a new data source. We evaluate these methods on the clinical research task of adverse drug event detection, which relies on many different variables found in the text of electronic health records.",Automated domain adaptation for clinical natural language processing,9986899,R01LM012918,"['Adult', 'Adverse drug event', 'Algorithms', 'Apache', 'Area', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Informatics', 'Clinical Research', 'Colon Carcinoma', 'Communities', 'Computer software', 'Computers', 'Conceptions', 'Data', 'Data Set', 'Data Sources', 'Detection', 'Dimensions', 'Ecosystem', 'Educational process of instructing', 'Electronic Health Record', 'Evaluation', 'Human', 'Information Retrieval', 'Institution', 'Knowledge', 'Label', 'Language', 'Lead', 'Learning', 'Linguistics', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Manuals', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Network-based', 'Output', 'Pathology', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Pulmonary Hypertension', 'Radiology Specialty', 'Research', 'Software Tools', 'Source', 'Statistical Models', 'System', 'Testing', 'Text', 'TimeLine', 'Training', 'Update', 'Vision', 'Work', 'adaptation algorithm', 'base', 'case finding', 'improved', 'machine learning method', 'malignant breast neoplasm', 'method development', 'natural language', 'neural network', 'new technology', 'news', 'novel', 'open source', 'point of care', 'side effect', 'social media', 'software systems', 'statistics', 'structured data', 'supervised learning', 'tool', 'tumor', 'unsupervised learning']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2020,383874,0.057781738596264835
"Natural Language Processing and Automated Speech Recognition to Identify Older Adults with Cognitive Impairment Project Summary The purpose of this proposal is to develop two strategies, natural language processing (NLP) and automated speech analysis (ASA), to enable automated identification of patients with cognitive impairment (CI), from mild cognitive impairment (MCI) to Alzheimer’s Disease Related Dementias (ADRD) in clinical settings. The number of older adults in the United States with MCI and ADRD is increasing and yet the ability of clinicians and researchers to identify them at scale has advanced little over recent decades and screening with clinical assessments is done inconsistently. Alternative strategies using available data, like analysis of diagnostic codes in the clinical record or insurance claims, have very low sensitivity. NLP and ASA used with machine learning are technologies that could greatly increase ability to detect MCI and ADRD in clinical contexts. NLP automatically converts text in the electronic health record (EHR) into structured concepts suitable for analysis. Thus, clinicians’ documentation of signs and symptoms or orders of tests and services that reflect or address cognitive limitations can be efficiently captured, possibly long before the clinician uses an ADRD-related diagnostic code. ASA directly measures cognition by recognizing different features of cognition captured in speech. Extracting features through both NLP and ASA could thus provide a unique measure of cognition and its impact on the individual and their caregivers. Early detection of MCI and ADRD can help researchers identify appropriate patients for research and help clinicians and health systems target patients for preventive care and care coordination. For these reasons, more efficient, highly scalable strategies are needed to identify people with MCI and ADRD. The Specific Aims of this proposal are to (1) Develop and validate a ML algorithm using features extracted from the EHR with NLP to identify patients with CI, (2) Develop and validate a ML algorithm using features extracted from ASA of audio recordings of patient-provider encounters during routine primary care visits to identify patients with CI, (3) Develop and validate a ML algorithm using both NLP and ASA extracted features to create an integrated CI diagnostic algorithm. We will develop machine learning algorithms using NLP and ASA extracted features trained against neurocognitive assessment data on 800 primary care patients in New York City and validate them in an independent sample of 200 patients in Chicago. In secondary analyses we will train ML algorithms to identify MCI and its subtypes. This project will be the most rigorous development of NLP, ASA, and ML algorithms for CI yet performed, the first to test ASA in primary care settings, and the first to test NLP and ASA feature extraction strategies in combination. The multi-disciplinary team of clinicians, health services researchers, and neurocognitive and data scientists will apply machine learning to develop these highly scalable, automated technologies for identification of MCI and ADRD. 1 Project Narrative The ability of clinicians, health systems and researchers to identify patients with mild cognitive impairment (MCI) and Alzheimer’s Disease Related Dementias (ADRD) is limited. This project will apply machine learning to natural language processing (NLP) of electronic health record data and automated speech analysis (ASA) of patient-doctor conversations during primary care visits to identify patients with MCI and ADRD using automated and scalable procedures. The analytic algorithms will be developed with neurocognitive assessment data on 800 primary care patients in New York City and validated in an independent sample of 200 patients in Chicago. 1",Natural Language Processing and Automated Speech Recognition to Identify Older Adults with Cognitive Impairment,9998610,R01AG066471,"['Acoustics', 'Acute', 'Address', 'Algorithms', 'Alzheimer&apos', 's disease related dementia', 'Caregivers', 'Chicago', 'Clinical', 'Clinical assessments', 'Code', 'Cognition', 'Cognitive', 'Data', 'Data Analyses', 'Data Element', 'Data Scientist', 'Data Set', 'Development', 'Diagnosis', 'Diagnostic', 'Documentation', 'Early Diagnosis', 'Elderly', 'Electronic Health Record', 'Health Services', 'Health system', 'Impaired cognition', 'Individual', 'Insurance Carriers', 'Machine Learning', 'Measures', 'Mental disorders', 'Methods', 'Natural Language Processing', 'Neurocognitive', 'New York City', 'Parkinson Disease', 'Patient Care', 'Patients', 'Persons', 'Physicians', 'Population', 'Positioning Attribute', 'Preventive care', 'Primary Health Care', 'Procedures', 'Provider', 'Psychiatric Diagnosis', 'Reference Standards', 'Research', 'Research Personnel', 'Resource Allocation', 'Risk Factors', 'Sampling', 'Semantics', 'Sensitivity and Specificity', 'Services', 'Signs and Symptoms', 'Speech', 'Structure', 'Study Subject', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'United States', 'Validation', 'Visit', 'adverse event risk', 'aging population', 'automated speech recognition', 'base', 'care coordination', 'clinical encounter', 'cognitive function', 'cognitive testing', 'deep learning', 'demographics', 'electronic data', 'electronic structure', 'falls', 'feature extraction', 'financial incentive', 'health care settings', 'improved', 'insurance claims', 'learning classifier', 'machine learning algorithm', 'mental state', 'mild cognitive impairment', 'multidisciplinary', 'prevent', 'primary care setting', 'recruit', 'risk mitigation', 'screening', 'secondary analysis', 'structured data', 'success', 'testing services', 'tool', 'treatment choice', 'unstructured data']",NIA,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2020,855710,0.0378255689731314
"Natural Language Processing and Machine Learning for Cancer Surveillance The purpose of this call order is to provide support in the area of quality control and improvement of cancer data, specifically for Clinical Document Annotation and Processing Pipeline (CDAP), LabKey Software, and the development of annotation schema. n/a",Natural Language Processing and Machine Learning for Cancer Surveillance,10281318,6116004B91020F00002,"['Area', 'Automated Annotation', 'Clinical', 'Data', 'Machine Learning', 'Malignant Neoplasms', 'Natural Language Processing', 'Quality Control', 'software development']",NCI,"WESTAT, INC.",N02,2020,149865,0.01086624665572333
"Addressing variability in peripheral arterial disease outcomes using machine learning techniques Project Summary/Abstract: Peripheral arterial disease (PAD) is a major cause of morbidity and mortality in the United States, affecting over eight million Americans, of whom 100,000 a year suffer major amputation. Current guidelines dictate medical treatment and aggressive risk factor modification for all PAD patients, whether symptomatic or not, with revascularization attempts for patients with chronic limb threatening ischemia (CLTI) or lifestyle-limiting claudication. Despite strongly-worded standards of care, variability in PAD outcomes persists. Prior research has demonstrated that some demographic factors such as gender, race, and socioeconomic status are associated with worse PAD care and outcomes even when controlling for comorbidities. It is unknown what specific patient, provider, and healthcare system factors lead to these disparities. Efforts to understand which patients will suffer worse outcomes and disease progression have been hampered by contemporary outcomes research techniques. The majority of PAD outcomes research relies on administrative claims databases, procedural registries, or single center retrospective reviews. While each of these methods has some advantages, none offer the combination of patient- and disease-specific data, information about care provision on a provider and health-system level, and outcomes across a range of possible locations. Furthermore, use of any of these methods at the scale necessary to draw powerful conclusions is prohibitively time- and resource-intensive. The overall objective of this research is to use a novel natural language processing model to build a combined EHR/CMS database and to use that database to predict which PAD patients are at highest risk of poor outcomes with improved power and precision. This proposal contains plans for collaboration with Duke Forge, who bring expertise in natural language processing and machine learning in order to efficiently identify PAD patients within our EHR and efficiently abstract information about them. Once identified, these patients can be linked to their CMS outcomes, allowing for assessment of how patient-, physician-, and healthcare-specific factors affect PAD outcomes. Our central hypothesis is that natural language processing powered by machine learning will permit efficient identification of patients with PAD, thereby facilitating higher-powered and higher-quality investigation into disparities in PAD outcomes. This research will pave the way for future interventions targeting sources of outcome inequality, possibly including access to care, physician adherence to national guidelines, and patient preferences or health literacy. Project Narrative: Peripheral arterial disease affects more than eight million Americans, with over one hundred thousand amputations performed yearly. Despite the prevalence and morbidity of this disease, there is a lack of knowledge about which patients will require amputation, multiple surgeries or hospitalizations, or suffer cardiovascular- related death. This proposal uses natural language processing to improve on current research methodology in order to account for patient-specific, physician-specific, and system-specific factors in peripheral arterial disease care and outcomes.",Addressing variability in peripheral arterial disease outcomes using machine learning techniques,10066805,F32HL151181,"['Address', 'Adherence', 'Affect', 'American', 'Amputation', 'Blood Vessels', 'Cardiovascular system', 'Caring', 'Cessation of life', 'Chronic', 'Clinical', 'Clinical Trials', 'Code', 'Collaborations', 'Country', 'Data', 'Data Set', 'Databases', 'Demographic Factors', 'Disease', 'Disease Outcome', 'Disease Progression', 'Documentation', 'Elements', 'Failure', 'Female', 'Future', 'Gender', 'Goals', 'Guidelines', 'Health Services Accessibility', 'Health system', 'Healthcare', 'Healthcare Systems', 'Hospitalization', 'Inequality', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Intervention', 'Investigation', 'Ischemia', 'Knowledge', 'Lead', 'Life Style', 'Light', 'Limb structure', 'Link', 'Location', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'Modification', 'Morbidity - disease rate', 'Myocardial Infarction', 'Natural Language Processing', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Patient Care', 'Patient Preferences', 'Patients', 'Peripheral arterial disease', 'Physicians', 'Prevalence', 'Process', 'Provider', 'Race', 'Registries', 'Research', 'Research Methodology', 'Research Technics', 'Resources', 'Risk', 'Risk Factors', 'Socioeconomic Status', 'Source', 'Stroke', 'System', 'Techniques', 'Text', 'Time', 'Training', 'United States', 'Work', 'adjudicate', 'base', 'care outcomes', 'claudication', 'cohort', 'comorbidity', 'cost', 'demographics', 'design', 'effective intervention', 'health literacy', 'high risk', 'improved', 'low socioeconomic status', 'mortality', 'novel', 'patient subsets', 'tool', 'treatment effect']",NHLBI,DUKE UNIVERSITY,F32,2020,82476,-0.0028971376613396173
"Fine-grained spatial information extraction for radiology reports ABSTRACT Automated biomedical image classification has seen enormous improvements in performance over recent years, particularly in radiology. However, the machine learning (ML) methods that have achieved this remarkable performance often require enormous amounts of labeled data for training. An increasingly accepted means of acquiring this data is through the use of natural language processing (NLP) on the free-text reports associated with an image For example, take the following brain MRI report snippet:  There is evidence of left parietal encephalomalacia consistent with known history of prior stroke. Small  focal area of hemosiderin deposition along the lateral margins of the left lateral ventricle. Here, the associated MRI could be labeled for both Encephalomalacia and Hemosiderin. NLP methods to automatically label images in this way have been used to create several large image classification datasets However, as this example demonstrates, radiology reports often contain far more granular information than prior NLP methods attempted to extract. Both findings in the above example mention their anatomical location, which linguistically is referred to as a spatial grounding, as the location anchors the finding in a spatial reference. Further, the encephalomalacia finding is connected to the related diagnosis of stroke, while the hemosiderin finding provides a morphological description (small focal area). This granular information is important for image classification, as advanced deep learning methods are capable of utilizing highly granular structured data. This is logical, as for instance a lung tumor has a slightly different presentation than a liver tumor. If an ML algorithm can leverage both the coarse information (the general presentation of a tumor) while also recognizing the subtle granular differences, it can find an optimal balance between specificity and generalizability. From an imaging perspective, this can also be seen as a middle ground between image-level labels (which are cheap but require significant data for training—a typical dataset has thousands of images or more) and segmentation (which is expensive to obtain, but provides better training data—a typical dataset has 40 to 200 images), as the fine-grained spatial labels correspond to natural anatomical segments. Our fundamental hypothesis in this project is that if granular information can be extracted from radiology reports with NLP, this will improve downstream radiological image classification when training on a sufficiently large dataset. For radiology, the primary form of granularity is spatial (location, shape, orientation, etc.), so this will be the focus of our efforts. We further hypothesize that these NLP techniques will be generalizable to most types of radiology reports. For the purpose of this R21-scale project, however, we will focus on three distinct types of reports with different challenges: chest X-rays (one of the most-studied and largest-scale image classification types), extremity X-rays (which offer different findings than chest X-rays), and brain MRIs (which present a different image modality and the additional complexity of three dimensions). NARRATIVE This project is interested in developing natural language processing (NLP) methods for better understanding the spatial relationships described in the free text data within radiology reports found in electronic health record (EHR) systems. We will (i) develop an ontology, (ii) manually create a dataset for training NLP methods, (iii) develop automatic NLP methods compatible the ontology and corpus, and (iv) evaluate automatic image classification methods that use the output of the NLP system as image labels.",Fine-grained spatial information extraction for radiology reports,9957898,R21EB029575,"['3-Dimensional', 'Address', 'Algorithms', 'Anatomy', 'Architecture', 'Area', 'Brain', 'Classification', 'Data', 'Data Set', 'Deposition', 'Devices', 'Diagnosis', 'Electronic Health Record', 'Encephalomalacia', 'Equilibrium', 'Goals', 'Grain', 'Hemosiderin', 'Human', 'Image', 'Information Retrieval', 'Label', 'Lateral', 'Left', 'Limb structure', 'Linguistics', 'Liver neoplasms', 'Location', 'Lung Neoplasms', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Methods', 'Morphology', 'Natural Language Processing', 'Ontology', 'Output', 'Parietal', 'Performance', 'Radiology Specialty', 'Recording of previous events', 'Reporting', 'Research', 'Roentgen Rays', 'Shapes', 'Specificity', 'Stroke', 'System', 'Techniques', 'Text', 'Thoracic Radiography', 'Training', 'Trust', 'base', 'bioimaging', 'deep learning', 'design', 'imaging modality', 'improved', 'innovation', 'interest', 'large datasets', 'lateral ventricle', 'learning strategy', 'machine learning algorithm', 'machine learning method', 'radiological imaging', 'scale up', 'spatial relationship', 'structured data', 'tool', 'tumor']",NIBIB,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R21,2020,271250,0.06408923273262172
"Resource Curation and Evaluation for EHR Note Comprehension Title: Resource Curation and Evaluation for EHR Note Comprehension The American Diabetes Association estimates that over 25.8 million Americans have diabetes. Proper patient self-management is perhaps the most critical and under-exercised element for patients to achieve appropriate glycemic control and thus mitigate complications and comorbid conditions, and implement appropriate preventive strategies (e.g., vaccines, exercise, healthy diet). In previous work we have shown that educating patients by using content in their medical records improved diabetes outcomes and that NoteAid, a multi- module natural language processing system that links medical jargon in electronic health records to definitions, has improved self-reported comprehension. Here we propose to develop NoteAid with a high-quality resource of expert-curated lay definitions and evaluation methods for diabetes patients’ note comprehension. ! ! Title: Resource Curation and Evaluation for EHR Note Comprehension The American Diabetes Association estimates that over 25.8 million Americans have diabetes. In previous work we have shown that educating patients by using content in their medical records improved diabetes outcomes and that NoteAid, a natural language processing system that links medical jargon in electronic health records to definitions, has improved self-reported comprehension. Here we propose to develop NoteAid with a high-quality resource of expert-curated lay definitions and evaluation methods for diabetes patients’ note comprehension. ! !",Resource Curation and Evaluation for EHR Note Comprehension,9925807,R01LM012817,"['Adult', 'Age', 'American', 'Artificial Intelligence', 'Attitude', 'Back', 'Behavior', 'Caregivers', 'Caring', 'Characteristics', 'Clinical', 'Cognitive', 'Communication', 'Complex', 'Comprehension', 'Country', 'Data Science', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Elements', 'Ethnic Origin', 'Evaluation', 'Exercise', 'Eye diseases', 'General Population', 'Health', 'Healthcare', 'Heart Diseases', 'Hospitals', 'Individual', 'Informatics', 'Internet', 'Intervention', 'Kidney Diseases', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Link', 'Machine Learning', 'Massachusetts', 'Measurement', 'Mechanics', 'Medical', 'Medical Records', 'Medical Students', 'Methods', 'Modeling', 'Natural Language Processing', 'Nursing Students', 'Ontology', 'Outcome', 'Patient Education', 'Patient Self-Report', 'Patients', 'Physicians', 'Prevention strategy', 'Questionnaires', 'Race', 'Randomized', 'Reading', 'Resources', 'Risk', 'Self Management', 'Speed', 'Supervision', 'System', 'Techniques', 'Testing', 'Time', 'Trust', 'United States National Institutes of Health', 'Universities', 'Vaccines', 'Visual Aid', 'Vocabulary', 'Work', 'base', 'care outcomes', 'clinically relevant', 'comorbidity', 'cost', 'deep learning', 'design', 'diabetic patient', 'glycemic control', 'good diet', 'improved', 'innovation', 'instrument', 'response', 'sex', 'skills', 'usability']",NLM,UNIVERSITY OF MASSACHUSETTS LOWELL,R01,2020,335875,-0.020906415621700408
"Open Health Natural Language Processing Collaboratory Project Summary One of the major barriers in leveraging Electronic Health Record (EHR) data for clinical and translational science is the prevalent use of unstructured or semi-structured clinical narratives for documenting clinical information. Natural Language Processing (NLP), which extracts structured information from narratives, has received great attention and has played a critical role in enabling secondary use of EHRs for clinical and translational research. As demonstrated by large scale efforts such as ACT (Accrual of patients for Clinical Trials), eMERGE, and PCORnet, using EHR data for research rests on the capabilities of a robust data and informatics infrastructure that allows the structuring of clinical narratives and supports the extraction of clinical information for downstream applications. Current successful NLP use cases often require a strong informatics team (with NLP experts) to work with clinicians to supply their domain knowledge and build customized NLP engines iteratively. This requires close collaboration between NLP experts and clinicians, not feasible at institutions with limited informatics support. Additionally, the usability, portability, and generalizability of the NLP systems are still limited, partially due to the lack of access to EHRs across institutions to train the systems. The limited availability of EHR data limits the training available to improve the workforce competence in clinical NLP. We aim to address the above challenges by extending our existing collaboration among multiple CTSA hubs on open health natural language processing (OHNLP) to share distributional information of NLP artifacts (i.e., words, n-grams, phrases, sentences, concept mentions, concepts, and text segments) acquired from real EHRs across multiple institutions. We will leverage the advanced privacy-preserving computing infrastructure of iDASH (integrating Data for Analysis, Anonymization, and SHaring) for privacy- preserving data analysis models and will partner with diverse communities including Observational Health Data Sciences and Informatics (OHDSI), Precision Medicine Initiative (PMI), PCORnet, and Rare Diseases Clinical Research Network (RDCRN) to demonstrate the utility of NLP for translational research. This CTSA innovation award RFA provides us with a unique opportunity to address the challenges faced with clinical NLP and through strong partnership with multiple research communities and leadership roles of the research team in clinical NLP, we envision that the successful delivery of this project will broaden the utilization of clinical NLP across the research community. There are four aims planned: i) obtain PHI-suppressed NLP artifacts with retained distribution information across multiple institutions and assess the privacy risk of accessing PHI- suppressed artifacts, ii) generate a synthetic text corpus for exploratory analysis of clinical narratives and assess its utility in NLP tasks leveraging various NLP challenges, iii) develop privacy-preserving computational phenotyping models empowered with NLP, and iv) partner with diverse communities to demonstrate the utility of our project for translational research. Project Narratives The proposed project aims to broaden the secondary use of electronic health records (EHRs) across the research community by combining innovative privacy-preserving computing techniques and clinical natural language processing.",Open Health Natural Language Processing Collaboratory,10005506,U01TR002062,"['Address', 'Algorithms', 'Attention', 'Award', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Pooling', 'Data Science', 'Detection', 'Disease', 'Electronic Health Record', 'Ensure', 'Familial Hypercholesterolemia', 'Frequencies', 'Health', 'Hepatolenticular Degeneration', 'Individual', 'Informatics', 'Information Distribution', 'Infrastructure', 'Institution', 'Kidney Calculi', 'Knowledge', 'Leadership', 'Learning', 'Measures', 'Medical', 'Minnesota', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Natural Language Processing', 'Observational Study', 'Patients', 'Phenotype', 'Play', 'Precision Medicine Initiative', 'Privacy', 'Process', 'Rare Diseases', 'Research', 'Research Personnel', 'Rest', 'Risk', 'Role', 'Sampling', 'Security', 'Semantics', 'Site', 'Source', 'Structure', 'System', 'Talents', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Translational Research', 'Universities', 'Work', 'base', 'citizen science', 'cohort', 'collaboratory', 'data infrastructure', 'data registry', 'empowered', 'health data', 'improved', 'indexing', 'individual patient', 'informatics infrastructure', 'innovation', 'interest', 'novel', 'phenotypic data', 'phenotyping algorithm', 'phrases', 'portability', 'preservation', 'privacy preservation', 'recruit', 'statistics', 'tool', 'usability', 'virtual']",NCATS,MAYO CLINIC ROCHESTER,U01,2020,1500847,0.09652376214957706
"Prediction of therapist cultural competency using Natural Language Processing (NLP) models PROJECT SUMMARY  Racial-ethnic minorities (REM) and lesbian, gay, bisexual, transgender, and queer (LGBTQ) individuals experience high levels of psychological distress. Psychological treatments can be effective in addressing mental health concerns, but disparities in quality of care still exist. Although systemic and institutional factors contribute to disparities in care, mental health providers are also critical to examine. A primary focus of efforts to understand and reduce provider contributions to mental health care disparities has been to examine cultural competency (CC), which involves a provider’s ability to navigate the cultural aspects of clinical interactions. Patient ratings of CC are generally associated with treatment outcomes and therapeutic processes. While patient perceptions of provider CC are important, a reliance on retrospective patient ratings limits what we know about how cultural identities are discussed, and the language that constitutes culturally sensitive care. Many studies of provider CC also require observers or patients to make complex judgments based on internal provider characteristics that are not reliably observable (e.g. rate provider awareness of their own cultural values). More studies are needed that examine patient-provider interactions in treatment in order to assess the impact of specific provider behaviors, and how they relate to perceptions of provider CC. Recently, Natural Language Processing (NLP) models have been applied to psychotherapy conversations to automatically capture the use of evidence based treatments, topics of conversation, empathy, and emotional expression. Prior research demonstrating the feasibility of automatically identifying topics of conversation in psychotherapy suggest that NLP models could be trained to automatically identify specific moments in sessions where patients and providers are talking about cultural issues. NLP models could allow researchers to not only examine how specific patterns of provider-patient interactions drive CC, but might also provide rapid feedback to providers, and in turn help address disparities in care. The purpose of the current study is to do the foundational work to develop and evaluate NLP tools that capture the cultural content of provider-patient interactions among REM and LGBTQ patients. First, utilizing 32,436 labeled talk turns from 200 psychotherapy sessions we will evaluate the accuracy of NLP models in recognizing the discussion of cultural topics in psychotherapy. Second, we will use NLP models to explore differences in the content of 1,235 psychotherapy sessions that were rated as highly positive or negative on a measure of cultural competence. PROJECT NARRATIVE Although disparities in the quality of mental health treatment for racial-ethnic minority (REM) and lesbian, gay, bisexual, transgender, and queer (LGBTQ) patients are well known, to date there are no tools that can identify specific patterns of provider-patient interactions that drive disparities in care. This project will evaluate the ability of Natural Language Processing (NLP) models to recognize discussion of cultural topics in psychotherapy among REM and LGBTQ patients, and explore differences in patient-provider interactions with low and high patient ratings of provider cultural competency.",Prediction of therapist cultural competency using Natural Language Processing (NLP) models,9906653,F31MD014941,"['Address', 'Alcohol or Other Drugs use', 'Anxiety', 'Awareness', 'Behavior', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Code', 'Complex', 'Computerized Medical Record', 'Data', 'Discrimination', 'Empathy', 'Evidence based treatment', 'Face', 'Feedback', 'Foundations', 'Funding', 'Goals', 'Grant', 'Health Personnel', 'Healthcare', 'Individual', 'Judgment', 'Label', 'Language', 'Lesbian Gay Bisexual Transgender Queer', 'Machine Learning', 'Measures', 'Mental Depression', 'Mental Health', 'Methods', 'Modeling', 'National Institute on Alcohol Abuse and Alcoholism', 'Natural Language Processing', 'Outcome', 'Patients', 'Pattern', 'Perception', 'Performance', 'Process', 'Provider', 'Psychotherapy', 'Quality of Care', 'Reporting', 'Research', 'Research Personnel', 'Suicide', 'Technology', 'Text', 'Therapeutic', 'Training', 'Treatment outcome', 'Work', 'base', 'commercial application', 'community setting', 'cultural competence', 'cultural values', 'disparity reduction', 'effective intervention', 'ethnic minority population', 'experience', 'health care disparity', 'improved', 'psychologic', 'psychological distress', 'racial and ethnic', 'sexual identity', 'showing emotion', 'substance abuse treatment', 'symptomatic improvement', 'tool', 'treatment disparity', 'university student', 'willingness']",NIMHD,UNIVERSITY OF UTAH,F31,2020,45016,0.06799195719416656
"Nurses' documentation of patient diagnoses, symptoms and interventions for home care patients with Alzheimer's Disease and related dementias: A natural language processing study PROJECT SUMMARY Alzheimer's disease and related dementias (ADRD) represent a looming public health crisis, affecting roughly 5 million people and 11% of older adults in the United States. Studies on patient transitions across healthcare settings suggests that older adults with chronic conditions are vulnerable to inadequate transfer of information, putting them at risk for diminished quality of care, medication errors and potentially preventable complications. Patients at varied stages along the ADRD trajectory – especially those with multiple chronic conditions – may experience unique risks due to the loss of information across care settings. In a recent study, we developed a longitudinal dataset on a diverse cohort of 56,652 patients – mostly aged 65+ with multiple comorbidities – receiving home health care (HHC) services from a large non-profit home care provider. For patients admitted to HHC in 2010-2012, we identified subgroups of patients with ADRD diagnoses made prior to and after HHC admission. Outside the scope of this prior study remains a vast and largely unexplored data source – nurses’ free-text clinical notes captured in the electronic health record. With roughly 1 million entries of nurses’ free- text notes associated with the study population, there is a wealth of potential information from which to gain new insights and a need for innovative methods to analyze this unstructured data source. In this study, we propose to use natural language processing (NLP) techniques, a method for systematically analyzing free-text content that draws upon machine learning. Using the dataset developed in the prior study, this study aims to: 1. Expand and improve an existing NLP system to automatically identify the following information in the  nursing free-text notes: (i) knowledge of the patient having a previously established ADRD diagnosis; (ii)  observations of cognitive symptoms and related patient/caregiver needs; and (iii) mentions of interventions  to address these needs. 2. For patients who do not have an ADRD diagnosis prior to HHC admission, determine whether nurses’ free-  text documentation of cognitive symptoms identified in the NLP predict subsequent ADRD diagnoses  during the 4-year follow-up period. 3. Among patients diagnosed with ADRD prior to HHC admission, determine whether nurses’ free-text  documentation patterns (e.g. knowledge of the patient’s ADRD diagnostic status, observations of cognitive  symptoms, and interventions) predict: (i) service use; and (ii) adverse health outcomes for which ADRD  patients are at heightened risk (e.g. hospitalizations due to urinary tract infection, dehydration, falls). This study will allow us to examine HHC nurses’ practices, which are often difficult to observe systematically, and identify strategies to address the complex needs of their patients with ADRD and un-diagnosed patients who may be on a path toward ADRD diagnosis. The long-term goal of this research is to develop a home- based intervention that aims to improve quality of life for patients and caregivers along the ADRD trajectory. PROJECT NARRATIVE Alzheimer's disease and related dementias (ADRD) affect about 5 million people in the U.S. Home health care nurses provide care for many people with ADRD and document what they observe about their patients’ needs in the form of free-text notes. This study will use a method known as ‘natural language processing’ to gain new knowledge from nurses’ notes and identify ways to better support people with ADRD and their caregivers.","Nurses' documentation of patient diagnoses, symptoms and interventions for home care patients with Alzheimer's Disease and related dementias: A natural language processing study",10056750,R21AG065753,"['Address', 'Admission activity', 'Affect', 'Alzheimer&apos', 's disease patient', 'Alzheimer&apos', 's disease related dementia', 'Ambulatory Care', 'Assessment tool', 'Awareness', 'Care given by nurses', 'Caregivers', 'Caring', 'Chronic', 'Classification', 'Clinical', 'Clinical Nursing', 'Clinical assessments', 'Cognitive', 'Communication', 'Complex', 'Data', 'Data Set', 'Data Sources', 'Dehydration', 'Diagnosis', 'Diagnostic', 'Discipline of Nursing', 'Documentation', 'Early Diagnosis', 'Elderly', 'Electronic Health Record', 'Evaluation', 'Family', 'Family Caregiver', 'Foundations', 'Funding', 'Future', 'Goals', 'Health', 'Home Care Services', 'Home Health Care Agencies', 'Home environment', 'Hospitalization', 'Individual', 'Intervention', 'Knowledge', 'Link', 'Machine Learning', 'Medical Care Team', 'Medicare claim', 'Medication Errors', 'Methods', 'Natural Language Processing', 'Neurobehavioral Manifestations', 'New York', 'Nurses', 'Nursing Homes', 'Nursing Services', 'Outcome', 'Pathway interactions', 'Patients', 'Pattern', 'Performance', 'Population', 'Positioning Attribute', 'Provider', 'Public Health', 'Quality of Care', 'Quality of life', 'Research', 'Risk', 'Risk Management', 'Services', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'United States', 'Urinary tract infection', 'Visiting Nurse', 'Work', 'aged', 'base', 'care coordination', 'care providers', 'cohort', 'comorbidity', 'ethnic diversity', 'experience', 'falls', 'follow-up', 'health care service', 'health care settings', 'hospice environment', 'improved', 'innovation', 'insight', 'longitudinal dataset', 'multiple chronic conditions', 'patient home care', 'patient subsets', 'racial and ethnic', 'study population', 'unstructured data']",NIA,VISITING NURSE SERVICE OF NEW YORK,R21,2020,260660,0.03002438817114245
"Integrative data science approaches for rare disease discovery in health records ABSTRACT: There are nearly 7,000 diseases that have a prevalence of only one in 2,000 individuals or less. Yet, such rare diseases are estimated to collectively affect over 300 million people worldwide, representing a significant healthcare concern. Although rare diseases have predominantly genetic origins, nearly half of them do not manifest symptoms until adulthood and frequently confound discovery and diagnosis. Even in the case of early onset disorders, the sheer number of possible diagnoses can often overwhelm clinicians. As a result, rare diseases are often diagnosed with delay, misdiagnosed or even remain undiagnosed, not only disrupting patient lives but also hindering progress on our understanding of such diseases. Data science methods that mine large-scale retrospective health record data for phenotypic information will aid in timely and accurate diagnoses of rare diseases, especially when combined with additional data types, thus, having significant real- world impact. This proposal will integrate electronic health record (EHR) data sets with publicly available vocabularies and ontologies, and genomic data for the improved identification and characterization of patients with rare diseases, using approaches from machine learning, natural language processing (NLP) and basic bioinformatics. The work has three specific aims and will be carried out in two phases. During the mentored phase, the principal investigator (PI) will develop data-driven methods to extract standardized concepts related to rare diseases from clinical notes and infer the occurrence of each disease (Aim 1). He will also develop data science approaches to compare and contrast longitudinal patterns associated with patients' journeys through the healthcare system when seeking a diagnosis for a rare disease, and aid in clinical decision-making by leveraging these patterns (Aim 2). During the independent phase (Aim 3), computational methods will be developed for the integrated modeling and analysis of genotypic (from Aim 3) and phenotypic information (from Aims 1 and 2). Cohorts to be sequenced will cover diseases for which causal genes or disease definitions are unclear (discovery), as well as those for which these are well known (validation). This work will be carried out under the mentorship of four faculty members with complementary expertise in biomedical informatics, data science, NLP, and rare disease genomics at the University of Washington, the largest medical system in the Pacific Northwest (four million EHRs), world-renowned researchers in medical genetics, and a robust data science environment. In addition, under the direction of the mentoring team, the PI will complete advanced coursework, receive training in translational bioinformatics and clinical research informatics, submit manuscripts, and seek an independent research position. This proposal will yield preliminary results for subsequent studies on data-driven phenotyping and enable the realization of the PI's career goals by providing him with the necessary training to build on his machine learning and basic bioinformatics expertise to transition into an independent investigator in biomedical data science. PROJECT NARRATIVE Rare genetic diseases are estimated to affect the lives of 25 to 30 million Americans and their families, and present a significant economic burden on the healthcare system. Currently, our knowledge of the broad spectrum of the 7,000 observed rare diseases is limited to a few well-studied ones, hindering our ability to make correct and timely diagnoses. The objective of this study is to improve the identification of patients with rare diseases in healthcare systems by developing data science approaches that automatically recognize rare disease-related patterns in patient health records and correlate them with genomic data, thus, aiding in diagnosis and discovery.",Integrative data science approaches for rare disease discovery in health records,9884791,K99LM012992,"['Adult', 'Affect', 'American', 'Award', 'Basic Science', 'Behavioral', 'Bioinformatics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Research', 'Computing Methodologies', 'Consensus', 'Data', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostics Research', 'Disease', 'Economic Burden', 'Electronic Health Record', 'Environment', 'Faculty', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Manuscripts', 'Markov Chains', 'Medical', 'Medical Genetics', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Pacific Northwest', 'Patient Recruitments', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Principal Investigator', 'Rare Diseases', 'Recording of previous events', 'Research', 'Research Personnel', 'Standardization', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Vocabulary', 'Washington', 'Work', 'accurate diagnosis', 'base', 'biomedical data science', 'biomedical informatics', 'career', 'causal variant', 'clinical data warehouse', 'clinical decision-making', 'cohort', 'diagnostic accuracy', 'disease phenotype', 'early onset disorder', 'exome sequencing', 'gene discovery', 'genomic data', 'health care delivery', 'health data', 'health record', 'improved', 'member', 'multimodal data', 'novel', 'open source', 'patient health information', 'phenotypic data', 'prototype', 'psychologic', 'rare condition', 'rare genetic disorder', 'recruit', 'skills', 'software development', 'support tools', 'tool', 'trait']",NLM,UNIVERSITY OF WASHINGTON,K99,2020,92070,-0.01063990183129613
"National NLP Clinical Challenges (n2c2): Challenges in Natural Language Processing for Clinical Narratives Project Summary and Abstract Narratives of electronic health records (EHRs) contain useful information that is difficult to automatically extract, index, search, or interpret. Natural language processing (NLP) technologies can extract this information and convert it in to a structured format that is more readily accessible by computerized systems. However, the development of NLP systems is contingent on access to relevant data and EHRs are notoriously difficult to obtain because of privacy reasons. Despite the recent efforts to de-identify and release narrative EHRs for research, these data are still very rare. As a result, clinical NLP, as a field has lagged behind. To address this problem, since 2006, we organized thirteen shared tasks, accompanied with workshops and journal publications. Twelve of these shared tasks have focused on the development of clinical NLP systems and the remaining one on the usability of these systems. We have covered both depth and breadth in terms of shared tasks, preparing tasks that study cutting-edge NLP problems on a variety of EHR data from multiple institutions. Our shared tasks are the longest running series of clinical NLP shared tasks, with ever growing EHR data sets, tasks, and participation. Our most popular three data sets have been cited 495 (2010 data), 284 (2006 de-id data), and 274 (2009 data) times, respectively, representing hundreds of articles that have come out of these three data sets alone. Our goal in this proposal is to continue the efforts we started in 2006 under i2b2 shared task challenges (i2b2, NIH NLM U54LM008748, PI: Kohane and R13 LM011411, PI: Uzuner) to de-identify EHRs, annotate them with gold- standard annotations for clinical NLP tasks, and release them to the research community for the development and head-to-head comparison of clinical NLP systems, for the advancement of the state of the art. Continuing our efforts under National NLP Clinical Challenges (n2c2) based at the Health Data Science program of the newly established Department of Biomedical Informatics at Harvard Medical School, we aim to form partnerships with the community to grow the shared task efforts in several ways: (1) grow the available de-identified EHR data sets through partnerships that can contribute to the volume and variety of the data, and (2) grow the available gold-standard annotations in terms of depth and breadth of NLP tasks. Given these aims and partnerships, we plan to hold a series of shared tasks. We will complement these shared tasks with workshops that meet in conjunction with the Fall Symposium of the American Medical Informatics Association and with journal special issues so that advancement of the state of the art can be sped up and future generations can build on the past. Project Narrative We propose to organize a series of shared tasks, workshops, and journal publications for fostering the continuous development of clinical Natural Language Processing (NLP) technologies that can extract information from narratives of Electronic Health Records (EHRs). Our aim is to grow the annotated gold standard EHR data sets that are available to the research community through partnerships and to bring together clinical NLP researchers with informatics researchers for building collaborations. We will engage the community in shared tasks and disseminate the knowledge generated by these shared tasks through workshops and journal special issues for the advancement of the state of the art.",National NLP Clinical Challenges (n2c2): Challenges in Natural Language Processing for Clinical Narratives,9930152,R13LM013127,"['Access to Information', 'Address', 'American', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Community Developments', 'Complement', 'Data', 'Data Science', 'Data Set', 'Development', 'Educational workshop', 'Electronic Health Record', 'Evaluation', 'Fostering', 'Future', 'Future Generations', 'Goals', 'Gold', 'Grant', 'Growth', 'Hand', 'Head', 'Healthcare', 'Improve Access', 'Individual', 'Informatics', 'Institution', 'Israel', 'Journals', 'Knowledge', 'Measures', 'Medical Informatics', 'Medical center', 'Methodology', 'Natural Language Processing', 'Outcome', 'Paper', 'Peer Review', 'Performance', 'Privacy', 'Publications', 'Publishing', 'Records', 'Research', 'Research Personnel', 'Rest', 'Running', 'Series', 'Source', 'Structure', 'System', 'Systems Development', 'Targeted Research', 'Technology', 'Time', 'United States National Institutes of Health', 'Universities', 'base', 'biomedical informatics', 'clinical development', 'computerized', 'falls', 'head-to-head comparison', 'health data', 'indexing', 'medical schools', 'meetings', 'practical application', 'programs', 'symposium', 'usability', 'working group']",NLM,GEORGE MASON UNIVERSITY,R13,2020,20000,0.050121830226298576
"Leveraging Twitter to Monitor Nicotine and Tobacco Cancer Communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to Monitor Nicotine and Tobacco Cancer Communication,10111658,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Twitter', 'Work', 'automated analysis', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'data standards', 'data streams', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'machine learning algorithm', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tobacco products', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF ARKANSAS AT FAYETTEVILLE,R01,2020,491992,0.0033569954280755496
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,10237828,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Information Retrieval', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2020,500000,0.0025174994903373145
"Investigating the documentation of E-cigarette use in the VA EHR PROJECT SUMMARY Electronic cigarettes were developed in China in the early 2000s and first introduced to the US market in 2007. Once established in the US, the product experienced explosive growth, with the number of electronic cigarette users doubling every year between 2008 and 2012. In 2012, it was estimated that 75% of US adults had heard of electronic cigarettes, and 8% had tried them. While electronic cigarettes have been studied over the last sev- eral years, no scientific consensus has emerged regarding either the safety of electronic cigarettes, or their po- tential as a smoking cessation aid. With this proposal, we will investigate how electronic cigarette use is documented in the Veterans Association Electronic Health Record, focusing specifically on the relationship between electronic cigarette use and com- bustible tobacco use, with the goal of understanding both how electronic cigarette use is documented in the context of the United States’ only nationwide health system, and how electronic cigarette related information can be reliably extracted from narrative clinical text using fully automated Natural Language Processing meth- ods. PROJECT NARRATIVE The proposed research focuses on the use of Natural Language Processing methods to automatically extract mentions of electronic cigarette use from the Veterans Association Electronic Health Record. The research will provide insight into important, currently unresolved questions regarding how clinicians record electronic cigarette use in the context of a nationwide health system, and whether patients report the use of electronic cigarettes as a smoking cessation aid or use the devices in conjunction with combustible tobacco.",Investigating the documentation of E-cigarette use in the VA EHR,9852435,R03DA047577,"['Address', 'Adult', 'Algorithms', 'American', 'Area', 'China', 'Cities', 'Clinical', 'Consensus', 'Dangerousness', 'Data', 'Data Set', 'Development', 'Devices', 'Documentation', 'Electronic Health Record', 'Electronic Nicotine Delivery Systems', 'Electronic cigarette', 'Environment', 'Epidemiology', 'Evaluation', 'Geography', 'Goals', 'Government', 'Growth', 'Health', 'Health system', 'Healthcare Systems', 'Hearing', 'Individual', 'Methods', 'Natural Language Processing', 'Patients', 'Pattern', 'Professional Organizations', 'Public Health', 'Public Health Applications Research', 'Reporting', 'Research', 'Risk', 'Safety', 'Scheme', 'Smoking', 'Sodium Chloride', 'Source', 'Technology', 'Text', 'Tobacco', 'Tobacco use', 'United States', 'Universities', 'Utah', 'Variant', 'Veterans', 'Work', 'authority', 'electronic cigarette use', 'electronic cigarette user', 'electronic hookah', 'evidence base', 'experience', 'information model', 'innovation', 'insight', 'smoking cessation', 'structured data', 'success', 'systems research', 'tobacco control', 'tool', 'vape pens']",NIDA,UNIVERSITY OF UTAH,R03,2020,76250,-0.013386179314306818
"Improving Cardiovascular Drug Safety With Automated Bleeding Classification PROJECT SUMMARY Atrial fibrillation (AF) treatment often includes drug therapy with oral anticoagulants (OAC) to prevent stroke. Bleeding, however, is a common complication of these drugs, affecting up to one in four patients. The Center for Medicare and Medicaid Services recently prioritized OAC-related drug safety as a key quality measure. Currently, however, no method exists to accurately identify bleeding events and severity in large populations. Prior methods use diagnoses codes, which lack sensitivity and clinical detail, or manual chart review, which cannot be implemented in large populations. The proposed research aims address this knowledge gap by applying a natural language processing (NLP)-based approach to identify bleeding events and classify severity in a real-world AF population. The tools will be validated in patients treated at a different institution, to ensure reproducibility across provider settings. In addition, we will apply the bleeding classification tool to evaluate the association between bleeding severity and mortality. Dr. Shah is an emerging young investigator whose career development plan is focused on acquiring the biomedical informatics skills to needed to accurately identify and reduce patient harm. Her training plan focuses on learning core competencies in natural language processing, with the goal of turning the wealth of data in the electronic medical record into useable knowledge. She will combine mentorship from established experts and targeted coursework to acquire skills in biomedical informatics, data science, advanced analytic methods, and research leadership. Completion of these research and training aims will create a platform for future R01 proposals by: (i) enabling safety focused comparative effectiveness research in AF (ii) setting the stage to identify bleeding complications in other cardiovascular diseases and (iii) developing a skill set that allows leadership of a multidisciplinary research team. Through this career development plan, Dr. Shah will build upon her prior training in clinical cardiology and research methodology and lay a strong foundation for a high impact research career. PUBLIC HEALTH RELEVANCE Atrial fibrillation affects six million US adults, and the prevalence is expected to double by 2030. As new drug and device treatments for stroke prevention emerge, patients and providers increasingly need accurate methods to monitor and improve patient safety.",Improving Cardiovascular Drug Safety With Automated Bleeding Classification,9899862,K08HL136850,"['Accounting', 'Address', 'Adult', 'Adverse event', 'Affect', 'Age', 'Algorithms', 'Anticoagulants', 'Atrial Fibrillation', 'Biomedical Research', 'Cardiology', 'Cardiovascular Agents', 'Cardiovascular Diseases', 'Classification', 'Clinical', 'Code', 'Comparative Effectiveness Research', 'Competence', 'Complication', 'Computer Assisted', 'Computerized Medical Record', 'Data', 'Data Science', 'Development Plans', 'Devices', 'Diagnosis', 'Ensure', 'Event', 'Foundations', 'Future', 'Goals', 'Healthcare', 'Hemorrhage', 'Incidence', 'Institution', 'Interdisciplinary Study', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Letters', 'Link', 'Manuals', 'Measures', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Oral', 'Patients', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Pragmatic clinical trial', 'Prevalence', 'Provider', 'Quality of life', 'Race', 'Records', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Research Training', 'Risk', 'Safety', 'Severities', 'Site', 'Standardization', 'Stroke', 'Stroke prevention', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'United States Centers for Medicare and Medicaid Services', 'Utah', 'Validation', 'Vital Status', 'acute coronary syndrome', 'advanced analytics', 'aging population', 'analytical method', 'base', 'biomedical informatics', 'career', 'career development', 'clinically relevant', 'cohort', 'improved', 'interdisciplinary approach', 'medication compliance', 'medication safety', 'mortality', 'neglect', 'novel', 'novel therapeutics', 'patient safety', 'personalized medicine', 'portability', 'public health relevance', 'screening', 'sex', 'skills', 'stroke risk', 'stroke therapy', 'tool']",NHLBI,UNIVERSITY OF UTAH,K08,2020,163080,0.015669787272953595
"Epidemiology and clinical outcomes of diabetic macular edema Approximately 25% of the millions of veterans (est. 8.92 million FY 2013) enrolled for care in Veterans Health Administration (VHA) have diabetes mellitus, and diabetic macular edema (DME) is the leading cause of vision loss in the adult diabetic population world-wide. Although diabetic retinopathy has been well-studied, comparatively little is known about the burden of DME. In fact, only two national prevalence studies and no national study on the incidence of DME in persons with type 2 diabetes have been conducted. Similarly many risk factors have been characterized for DR, but no large studies have established predictors for DME.  Beyond the Medicare claims database, the VHA National Patient Care Database (NPCD) contains standardized administrative data for several aspects of patient care including diagnoses, procedures, medications, lab test results, vital signs, clinical text notes, and mortality. Because the VA uses teleretinal screening as routine clinical care for all patients with diabetes with these results included in the NPCD, the NPCD is an ideal source for studying the epidemiology of and risk factors for DME.  This study proposes to determine the burden of diabetic macular edema, establish risk factors, and examine treatment outcomes in a previously extracted dataset on 1.98 million veterans who have undergone diabetic retinopathy screening at least once since 2004. Currently invaluable ophthalmic data are encoded in unstructured clinical encounter notes in the Computerized Patient Record System (CPRS), and no validated automated extraction method exists to capture these data elements. An automated extraction method using natural language processing will be created and validated to unlock key ophthalmic variables. These text extraction methods will be applicable to extracting ophthalmology data from not only notes of patients with DME but also any ophthalmology clinical note. This will enable future large scale studies in ophthalmology using NPCD and be immediately valuable to the research community at large.  The candidate, Dr. Aaron Lee, MD MSCI, is an ophthalmologist with subspecialty training in retina surgery with a strong background in computer science and epidemiology. His career goal is to become an independent clinician scientist studying diabetic eye disease with large-scale electronic medical record extracted data. While he possesses the foundational skills, he seeks to gain training in advanced statistics and natural language processing to unlock the data captured in unstructured clinical encounter notes. He has assembled an outstanding mentorship team under the primary mentor, Dr. Edward Boyko, MD MPH. This mentorship team includes renowned experts in clinical epidemiology, health informatics, ophthalmology, and natural language processing. This K23 will provide Dr. Lee the structured coursework, mentorship, and applied learning needed to acquire new research skills. He will leverage key local resources to carry out the proposed research at the University of Washington and the VA Seattle Epidemiologic Research and Information Center. Despite the significant visual loss associated with diabetic macular edema, little is known about the frequency of its occurrence, its risk factors, and the real-world effectiveness of existing treatments. The purpose of this proposed research is to utilize the VA National Patient Care Database to extract relevant data elements to examine these three clinical questions: 1) what is the incidence and prevalence of diabetic macular edema, 2) what are the risk factors associated with its development, and 3) what is the comparative real-world effectiveness of its treatments, including intravitreal anti-VEGF therapy, intravitreal corticosteroid therapy and macular laser. The methods developed in this research proposal will not only further our understanding of DME but also generalize and enable future large-scale ophthalmic studies.",Epidemiology and clinical outcomes of diabetic macular edema,9995499,K23EY029246,"['Adopted', 'Adrenal Cortex Hormones', 'Adult', 'Affect', 'Age', 'Algorithms', 'Anemia', 'Blindness', 'Cardiovascular Diseases', 'Caring', 'Cataract Extraction', 'Clinic', 'Clinical', 'Clinical Data', 'Code', 'Communities', 'Computerized Medical Record', 'Computerized Patient Records', 'Data', 'Data Element', 'Data Set', 'Databases', 'Development', 'Diabetes Mellitus', 'Diabetic Retinopathy', 'Diagnosis', 'Dyslipidemias', 'Effectiveness', 'Enrollment', 'Epidemiology', 'Ethnic Origin', 'Exclusion Criteria', 'Eye', 'Eye diseases', 'Foundational Skills', 'Frequencies', 'Future', 'Goals', 'Handedness', 'Hypertension', 'Incidence', 'Information Centers', 'Injections', 'Intervention', 'Lasers', 'Lead', 'Light Coagulation', 'Manuals', 'Masks', 'Measures', 'Medicare claim', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Natural Language Processing', 'Non-Insulin-Dependent Diabetes Mellitus', 'Operative Surgical Procedures', 'Ophthalmologist', 'Ophthalmology', 'Outcome', 'Pathology', 'Patient Care', 'Patients', 'Pattern', 'Persons', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phase', 'Phenotype', 'Physicians', 'Population', 'Prediction of Response to Therapy', 'Prevalence', 'Prevalence Study', 'Procedures', 'Protocols documentation', 'Public Health Informatics', 'Randomized Controlled Trials', 'Regimen', 'Research', 'Research Proposals', 'Resources', 'Retina', 'Risk Factors', 'Scientist', 'Severities', 'Sleep Apnea Syndromes', 'Smoking Status', 'Source', 'Standardization', 'Structure', 'System', 'Test Result', 'Text', 'Time', 'Training', 'Treatment Protocols', 'Treatment outcome', 'Universities', 'Use Effectiveness', 'Validation', 'Vascular Endothelial Growth Factors', 'Veterans', 'Visual', 'Visual Acuity', 'Washington', 'automated algorithm', 'bevacizumab', 'career', 'clinical care', 'clinical encounter', 'clinical epidemiology', 'cohort', 'comparative', 'computer science', 'diabetes management', 'diabetic', 'epidemiology study', 'hands-on learning', 'health administration', 'intravitreal injection', 'kidney dysfunction', 'laser photocoagulation', 'macula', 'macular edema', 'mortality', 'proliferative diabetic retinopathy', 'screening', 'skills', 'statistics', 'therapy outcome', 'traditional therapy', 'treatment optimization', 'unstructured data']",NEI,UNIVERSITY OF WASHINGTON,K23,2020,238049,0.0020010652529038066
"Multicenter Study of the Emergency Department Trigger Tool Existing methods for surveillance of patient harm in the ED setting are inadequate, without any meaningful change in decades. Trigger tools, popularized by the Institute for Healthcare Improvement’s Global Trigger Tool, have been developed for multiple clinical areas and are used across the world, outperform traditional approaches for surveillance of adverse events. These tools use a two-tiered review process where a nurse screens records for triggers (predefined findings that make the presence of an AE more likely) and reviews records with triggers for AEs, discarding those without triggers. We developed a consensus-based ED trigger tool (EDTT) using a multicenter, transdisciplinary modified Delphi approach, subsequently pilot testing this in a multicenter fashion with encouraging results. This was followed by a recently completed, AHRQ-funded single center study to automate, refine and validate this tool. This study demonstrated that the EDTT is a high-yield and efficient instrument for identifying adverse events in the ED. The present study will evaluate the refined, automated EDTT), in a multicenter study. We will evaluate the EDTT’s generalizability and robustness at three sites with large emergency departments, with a planned in-depth review of 9,000 ED admissions. We will use natural language processing of electronic medical record narratives and machine learning to improve the EDTT efficiency in trigger detection and AE discovery. We will establish the basis for a wider use and prepare for scalability and usability of the tool, creating standardized, streamlined and free online training materials, and by evaluating the tool in a real-world manner consistent with intended use. Project Narrative Commonly used approaches in Emergency Departments to detect adverse events are low yield and have not changed in decades, providing inadequate surveillance for patient harm. The need for improved methodology is critical, given the evolving role of the emergency department in the health care system. Trigger tools, developed for use in many healthcare settings across the world, detect all-cause harm, helping direct resources by identifying areas of risk and allowing an assessment of the effectiveness of quality improvement efforts over time. Trigger tools involve screening of records by a nurse for triggers (findings that make an adverse event more likely) and a review of only records with triggers searching for adverse events. Any events identified undergo confirmatory physician review. We developed a trigger tool for the ED, applying rigorous methods to identify predictive triggers, to computerize the screen for triggers eliminating manual review and improving record selection to enhance yield and efficiency. This tool demonstrates superior performance for detecting adverse events. We will now test this tool in a multicenter project to evaluate its broad application, confirming its utility and to continue to improve its yield and efficiency in adverse event detection by applying natural language processing and machine learning techniques.",Multicenter Study of the Emergency Department Trigger Tool,10098792,R01HS027811,[' '],AHRQ,WASHINGTON UNIVERSITY,R01,2020,398528,0.024446198876396645
"Collaborative Research: Statistical algorithms for anomaly detection and patterns recognition in patient care and safety event reports Medical errors have been shown to be the third leading cause of death in the United States. The Institute of Medicine and several state legislatures have recommended the use of patient safety event reporting systems (PSRS) to better understand and improve safety hazards. Numerous healthcare providers have adopted these systems, which provide a framework for healthcare provlder staff to report patient safety events. Public databases like MAUDE and VAERS have also been created to collect and trend safety events across healthcare systems. A patient safety event (PSE) report generally consists of both structured and unstructured data elements. Structured data are pre-defined, fixed fields that solicit specific information about the event. The unstructured data fields generally include a free text field where the reporter can enter a text description of the event. The text descriptions are often a rich data source in that the reporter ls not constrained to limited categories or selection options and is able to freely descrlbe the details of the event. The goal of this project is to develop novel statistical methods to analyze unstructured text like patient safety event reports arising in healthcare, which can lead to significant improvements to patient safety and enable timely intervention strategies. We address three problems: (a) Building realistic and meaningful baseline models for near misses, and detecting systematic deterioration of adverse outcomes relative to such baselines; (b) Understanding critical factors that lead to near misses & quantifying severity of outcomes; and (c) ldentifylng document groups of interest. We will use novel statistical approaches that combine Natural Language Processing with Statistical Process Monitoring, Statistical Networks Analysis, and Spatio-temporal Modeling to build a generalizable toolbox that can address these issues in healthcare. An important advantage of our research team is the involvement of healthcare domain experts and access to frontline staff, and we will leverage this strength to develop our algorithms. A key feature of our work is the generalizability of our methods, which will be applicable to biomedical documents arising across a remarkable variety of areas, such as patient safety and equipment malfunction reports, electronic health records, adverse drug or vaccine reports, etc. We will also release open source software via R packages & GitHub, which will enable healthcare staff and researchers to execute our methods on their datasets. Estimates of preventable adverse events in healthcare are staggering, despite the frequently cited Institute of Medicine (IOM) report that first brought attention to the problem over ten years ago. Identifying temporal trends and patterns in the data is particularly important to improving patient safety and patient care. Using our algorithms to effectively analyze documents from reporting systems has the potential to dramatically improve the safety and quality of care by exposing possible weaknesses in the care process.",Collaborative Research: Statistical algorithms for anomaly detection and patterns recognition in patient care and safety event reports,10211805,R01LM013309,"['Address', 'Adopted', 'Adverse event', 'Algorithms', 'Area', 'Attention', 'Caring', 'Categories', 'Cause of Death', 'Computer software', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Databases', 'Detection', 'Deterioration', 'Electronic Health Record', 'Equipment Malfunction', 'Event', 'Goals', 'Health Personnel', 'Healthcare', 'Healthcare Systems', 'Institute of Medicine (U.S.)', 'Interest Group', 'Intervention', 'Lead', 'Medical Errors', 'Methods', 'Modeling', 'Monitor', 'Natural Language Processing', 'Nurses', 'Outcome', 'Pathway Analysis', 'Patient Care', 'Pattern', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Process', 'Quality of Care', 'Report (document)', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Safety', 'Severities', 'Statistical Algorithm', 'Statistical Computing', 'Statistical Methods', 'System', 'Text', 'Time', 'Time trend', 'United States', 'Vaccines', 'Work', 'adverse outcome', 'hazard', 'improved', 'novel', 'open source', 'patient safety', 'spatiotemporal', 'structured data', 'trend', 'unstructured data']",NLM,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R01,2020,278731,0.01607990587620123
"Automated Data Collection on Antimicrobial Use in Dogs and Cats in a Tertiary Hospital and Private Practices Judicious antimicrobial use in veterinary medicine is important because improper antimicrobial use can contribute to the evolution of antimicrobial resistance in bacterial pathogens, which makes subsequent use of these drugs less effective in both human and veterinary medicine. There is very little on-the-ground information about veterinary clinicians’ antimicrobial use (AMU) practices in companion animal practice in the US. veterinary medicine. To improve our understanding of antimicrobial use in dogs and cats, we propose to create a nationwide digital surveillance system to collect critical AMU data using existing electronic practice information management systems (PIMS) in collaboration with veterinary industry partners. The system will automatically harvest AMU and patient data from digital PIMS. The proposed system will harvest data collected in routine veterinary examinations from existing PIMS systems and therefore will not require any additional effort from practitioners to participate in the program. Natural language processing, a machine learning method used to classify unstructured text, will be used to review electronic medical records to determine patients’ diagnosis. We aim to prototype the system in our native digital PIMS at North Carolina State University’s College of Veterinary Medicine Teaching hospital. We will then enroll additional private veterinary practices, including general practice, specialty hospitals, and emergency clinics, as sentinels and collect the same detailed PIMS data from a more representative set of clinics. Working closely with the sentinel clinics will provide a deep understanding of how our system operates in private clinics, and in the final stage we aim to expand the fully automated system to PIMS nationwide. The combination of sentinel clinics with the nationwide survey of clinics will create a powerful broad and deep surveillance system for antimicrobial use in veterinary clinics. A broad suite of AMU parameters will be estimated from this data, and the results reported to the FDA in an annual report. Additionally, we will share the data with other researchers through an web-based portal and GitHub repositories. This system will provide the critical data and analysis to understand veterinary AMU in the US. NARRATIVE Veterinarians play a central role in protecting animal and human health by preserving the efficacy of the antibiotics that their use for their patients. We have created a partnership among a public university, private veterinary hospitals, and a leading industry partner to collect information on how antibiotics are being used in cat and dog practices across the country with no disruption to the participating hospitals. The data will support FDA’s commitment to promoting antimicrobial stewardship.",Automated Data Collection on Antimicrobial Use in Dogs and Cats in a Tertiary Hospital and Private Practices,10166402,U01FD007057,[' '],FDA,NORTH CAROLINA STATE UNIVERSITY RALEIGH,U01,2020,199999,0.003395388587718068
"Dynamic learning for post-vaccine event prediction using temporal information in VAERS Project Summary Vaccines have been one of the most successful public health interventions to date. They are, however, pharmaceutical products that carry risks. Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine- preventable illnesses. The CDC/FDA Vaccine Adverse Event Reporting System (VAERS) contains up to 30,000 reports per year over the past 25 years. VAERS reports include both structured data (e.g., vaccination date, first onset date, age, and gender) and unstructured narratives that often provide detailed clinical information about the clinical events and the temporal relationship of the series of event occurrences post vaccination. The structured data only provide one onsite date whereas temporal information of the sequence of events post vaccination is contained in the unstructured narratives. Current status –While structured data in the VAERS are widely used, the narratives are generally ignored because of the challenges inherent in working with unstructured data. Without these narratives, potentially valuable information is lost. Goals - In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Specifically, built upon the state-of-art ontology and natural language processing technologies, we will develop and validate a Temporal Information Modeling, Extraction and Reasoning system for Vaccine data (TIMER-V), which will automatically extract post-vaccination events and their temporal relationships from VAERS reports, semantically infer temporal relations, and integrate the exacted unstructured data with the structured data. Furthermore, we will provide and maintain a publicly available data access interface to query the new integrated data repository, which will facilitate vaccine safety research, casual inference, and other temporal related discovery. We will also develop and validate models to predict severe AEs using the co-occurrence or temporal patterns of the series of AEs post vaccination. To the best of our knowledge, this is the first attempt to make use of the unstructured narratives in the VAERS reports to facilitate the temporal related discovery to a broad community of investigators in pharmacology, pharmacoepidemiology, vaccine safety research, among others. Project Narrative Effective analyses of post-vaccination adverse events (AEs) is vital to assuring the safety of vaccines, a key public health intervention for reducing the frequency of vaccine-preventable illnesses. In response to the FOA, PA-15-312, this proposed project focuses on the specific objective on “creation/evaluation of statistical methodologies for analyzing data on vaccine safety, including data available from existing data sources such as passive reporting systems or healthcare databases”. Currently the FDA/CDC Vaccine Adverse Event Reporting System (VAERS) only includes one onsite date in its database. The textual narratives in the reports are generally ignored primarily due to their unstructured nature. These narratives, however, contain more detailed information about the series of events that happened after vaccination, which could be valuable for more informed clinical studies. We propose to develop a novel framework to extract and accurately interpret the temporal information contained in the narratives through informatics approaches, and to develop prediction models for risk of severe AEs. Our new methods, their applications to VAERS database, and their dissemination will facilitate the entire research network for pursuing temporal related discovery with high methodological rigor.",Dynamic learning for post-vaccine event prediction using temporal information in VAERS,9854882,R01AI130460,"['Abbreviations', 'Address', 'Adverse event', 'Age', 'Centers for Disease Control and Prevention (U.S.)', 'Clinic', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Data Analyses', 'Data Sources', 'Database Management Systems', 'Databases', 'Development', 'Evaluation', 'Event', 'Frequencies', 'Funding', 'Gender', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Informatics', 'Information Retrieval', 'Learning', 'Manuals', 'Measles-Mumps-Rubella Vaccine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Ontology', 'Patients', 'Pattern', 'Performance', 'Pharmacoepidemiology', 'Pharmacologic Substance', 'Pharmacology', 'Process', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Series', 'Serious Adverse Event', 'Severities', 'Signal Transduction', 'Source', 'System', 'Technology', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Validation', 'base', 'data access', 'data warehouse', 'flexibility', 'improved', 'influenza virus vaccine', 'information model', 'novel', 'predictive modeling', 'public health intervention', 'response', 'risk prediction model', 'structured data', 'unstructured data', 'vaccine safety']",NIAID,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2020,766226,0.02260942934884757
"Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing The adoption of Electronic Health Record (EHR) systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format with tremendous potential but an equally large concern for patient confidentiality breaches. Secondary use of clinical data is essential to fulfill the potential for high quality healthcare, improved healthcare management, and effective clinical research. NIH expects that larger research projects share their research data in a way that protects the confidentiality of research subjects. De-identification of patient data has been proposed as a solution to both facilitate secondary use of clinical data and protect patient data confidentiality. The majority of clinical data found in the EHR is represented as narrative text clinical notes, and de-identification of clinical text is a tedious and costly manual endeavor. Automated approaches based on Natural Language Processing have been implemented and evaluated, allowing for higher accuracy and much faster de- identification than manual approaches. Clinacuity, Inc. proposes to advance a text de-identification system from a prototype to an accurate, adaptable, and robust system, integrated into the research infrastructure at our implementation and testing site (Medical University of South Carolina, Charleston, SC), and ready for commercialization efforts. To accomplish this undertaking, we will focus on the following specific aims and related objectives, while continuing to prepare the commercialization of the integrated system, with detailed market analysis, commercial roadmap development, and modern media communication: 1) Enhance the text de-identification system performance, scalability, and quality to produce an enterprise-grade solution ready for deployment; 2) Enable use of structured data for enhanced text de-identification (when structured PII is available) and for complete patient records de-identification (i.e., records combining structured and unstructured data). This aim also includes implementing “one-way” pseudo-identifier cryptographic hashing to enable securely linking already de-identified patient records; 3) Integrate the text de-identification system with a research data capture and management system. This includes implementation of the de-identification system as a secure web service, with standards-based access and integration. This de-identification system has potential commercial applications in clinical research and in healthcare settings. It will improve access to richer, more detailed, and more accurate clinical data (in clinical text) for clinical researchers. It will ease research data sharing (as expected for larger NIH-funded research projects) and help healthcare organizations protect patient data confidentiality. Significant time-savings will also be offered, with a process at least 200-1000 times faster than manual de-identification. The adoption of Electronic Health Record systems is growing at a fast pace in the U.S., and this growth results in very large quantities of patient clinical data becoming available in electronic format, with tremendous potential, but also equally growing concern for patient confidentiality breaches. De-identification of patient data has been proposed as a solution to both facilitate secondary uses of clinical data and protect patient data confidentiality. This project will advance a text de-identification system from a prototype to an accurate, adaptable and robust system allowing for complete patient records de-identification, integrated in the research infrastructure at our implementation and testing site and ready for commercialization efforts. It will improve access to richer, more detailed, and more accurate clinical data for clinical researchers, ease research data sharing and help healthcare organizations protect patient data confidentiality. !",Clinical Text Automatic De-Identification to Support Large Scale Data Reuse and Sharing,9908962,R42GM116479,"['Adoption', 'Clinical', 'Clinical Data', 'Clinical Research', 'Code', 'Communications Media', 'Confidentiality of Patient Information', 'Data', 'Development', 'Electronic Health Record', 'Enrollment', 'Environment', 'Fast Healthcare Interoperability Resources', 'Funding', 'Growth', 'Health Insurance Portability and Accountability Act', 'Improve Access', 'Link', 'Manuals', 'Medical', 'Modernization', 'National Institute of General Medical Sciences', 'Natural Language Processing', 'Patient Care', 'Patient Data Privacy', 'Patients', 'Performance', 'Personally Identifiable Information', 'Phase', 'Privacy', 'Process', 'Records', 'Reference Standards', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Risk', 'Savings', 'Secure', 'Site', 'South Carolina', 'Speed', 'Structure', 'System', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'Universities', 'Visualization', 'base', 'commercial application', 'commercialization', 'cost', 'cryptography', 'data reuse', 'data sharing', 'health care quality', 'health care service organization', 'health care settings', 'health management', 'improved', 'large scale data', 'prototype', 'software development', 'standard measure', 'structured data', 'systems research', 'unstructured data', 'web services']",NIGMS,"CLINACUITY,INC.",R42,2020,759330,-0.016452660296283176
"Collaborative Research: Statistical Algorithms for Anomaly Detection and Patterns Recognition in Patient Care and Safety Event Reports Project Summary Medical errors have been shown to be the third leading cause of death in the United States. The Institute of Medicine and several state legislatures have recommended the use of patient safety event reporting systems (PSRS) to better understand and improve safety hazards. A patient safety event (PSE) report generally consists of both structured and unstructured data elements. Structured data are pre-defined, fixed fields that solicit specific information about the event. The unstructured data fields generally include a free text field where the reporter can enter a text description of the event. The text descriptions are often a rich data source in that the reporter is not constrained to limited categories or selection options and is able to freely describe the details of the event. The goal of this project is to develop novel statistical methods to analyze unstructured text like patient safety event reports arising in healthcare, which can lead to significant improvements to patient safety and enable timely intervention strategies. We address three problems: (a) Building realistic and meaningful baseline models for near misses, and detecting systematic deterioration of adverse outcomes relative to such baselines; (b) Understanding critical factors that lead to near misses & quantifying severity of outcomes; and (c) Identifying document groups of interest. We will use novel statistical approaches that combine Natural Language Processing with Statistical Process Monitoring, Statistical Networks Analysis, and Spatio-temporal Modeling to build a generalizable toolbox that can address these issues in healthcare. We will also release open source software via R packages & GitHub, which will enable healthcare staff and researchers to execute our methods on their datasets. The COVID-19 pandemic has resulted in increased patient volumes and increased patient acuity, leading to an excessive burden on many healthcare facilities across the United States. This greatly increases the risk of patient safety consequences arising from malfunctioning medical equipment or adverse reaction to medication. To ensure patient safety and the highest quality of healthcare during this crisis, we need a rapid response system to model and analyze COVID-specific safety issues at scale, and quickly disseminate the results to healthcare facilities, so that these risks can be mitigated at the point of care. In this supplement, we propose to do this by (a) mining public databases and EHRs to identify devices/medication being used for treating COVID and (b) applying our methods (based on NLP, SPC, and SPM) to understand risks associated with these items. This information will be disseminated nationally to all healthcare facilities so that it can be integrated into the EHR at the point of care to alert clinicians. Project Narrative Estimates of preventable adverse events in healthcare are staggering, and the risk is particularly high for COVID patients due to the rapidly increasing burden on healthcare facilities. Using our algorithms to identify temporal trends and analyze free text narratives from reporting systems can ensure the safety and quality of care for COVID patients by exposing and mitigating possible weaknesses in the care process.",Collaborative Research: Statistical Algorithms for Anomaly Detection and Patterns Recognition in Patient Care and Safety Event Reports,10254593,R01LM013309,"['Address', 'Adverse event', 'Adverse reactions', 'Algorithms', 'Architecture', 'Behavior', 'COVID-19 pandemic', 'Caring', 'Categories', 'Cause of Death', 'Computer software', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Detection', 'Deterioration', 'Devices', 'Disease Outbreaks', 'English Language', 'Ensure', 'Equipment', 'Equipment Malfunction', 'Event', 'Goals', 'Health', 'Health care facility', 'Healthcare', 'Institute of Medicine (U.S.)', 'Interest Group', 'Intervention', 'Investigation', 'Lead', 'Length', 'Measures', 'Medical', 'Medical Errors', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Monitor', 'Natural Language Processing', 'Operative Surgical Procedures', 'Outcome', 'Pathway Analysis', 'Pathway interactions', 'Patient Care', 'Patients', 'Pattern Recognition', 'Pharmaceutical Preparations', 'Prevalence', 'Process', 'Property', 'Quality of Care', 'Records', 'Reporter', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Safety', 'Severities', 'Side', 'Site', 'Statistical Algorithm', 'Statistical Computing', 'Statistical Methods', 'Statistical Models', 'Supervision', 'System', 'Techniques', 'Text', 'Time', 'Time trend', 'United States', 'Variant', 'Vocabulary', 'adverse outcome', 'base', 'checkup examination', 'cluster computing', 'coronavirus disease', 'dosage', 'hazard', 'health care quality', 'health care service', 'health care service organization', 'improved', 'insight', 'interest', 'mathematical model', 'novel', 'open source', 'patient safety', 'point of care', 'response', 'service delivery', 'spatiotemporal', 'structured data', 'trend', 'unstructured data']",NLM,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R01,2020,74963,0.01699523395924335
"Community Surveillance of Coronary Heart Disease Project Summary/Abstract  Preventing the onset of acute myocardial infarction (AMI) and its recurrence, and reducing the morbidity and mortality associated with AMI, remain of significant public health and clinical concern. Monitoring contemporary trends in AMI incidence, treatment, and in-hospital and long-term outcomes is of considerable importance given periodic national updates of treatment guidelines, emphasis on reducing hospital readmissions, and revised definitions and classifications of AMI. Continuously supported by the NHLBI, we have conducted more than 35 years of population-based surveillance of AMI incidence and attack rates, hospital management practices, and the in-hospital and long-term prognosis associated with AMI among residents of central MA hospitalized at all central MA medical centers. We have a highly experienced team of cardiologists, epidemiologists, clinical informatics, and health services researchers who will build on multi- decade long trends (1975-2011) in our principal study endpoints examined previously in this study to the two new study years of patients hospitalized with AMI at all central MA medical centers in 2014 and 2017.  To sustain our efforts into the era of electronic medical records (EMRs), and after implementation of the ICD-10 system in 2015, we will develop a new automated AMI surveillance system that efficiently utilizes EMRs by taking advantage of state-of-art natural language processing (NLP) methods that will be compatible with ICD-10 (Aim 1). We will use the new NLP method to streamline traditional chart review-based collection of socio-demographic, clinical, treatment, and hospital and post-discharge outcomes data in patients hospitalized with AMI at all 11 central MA medical centers in 2014 and 2017. The data extracted from NLP-streamlined chart reviews will be used to validate and refine the NLP system. Issues related to changes from ICD-9 to ICD- 10 will be carefully addressed. The new NLP-enriched EMR-based surveillance system will eventually be implemented in all participating central MA hospitals. Using the NLP-enriched and EMR-based surveillance data, we will monitor the contemporary clinical epidemiology of AMI, and out-of-hospital deaths due to coronary disease, and changing landscape, over a more than 40 year period (1975-2017) (Aim 2).  The new EMR-based and NLP-enriched system will enhance the population-based surveillance of acute coronary disease. This new system will be cost-effective, more efficient and near-real time, have greater accuracy and precision, and can be readily updated to accommodate changes in information technologies and broadly applicable to other hospital systems. It will support our continued efforts to provide unique community- based observational data on several populations that are often excluded from clinical trials, and that are increasing in numbers, namely the elderly and patients with multiple morbidities. Furthermore, it will generate critical data to inform more national clinical guidelines on the enhanced prevention and management of AMI. If successful, the system can serve as a model and be implemented statewide in MA and elsewhere in the US. Project Narrative  The results of the proposed community-based study will provide data about 40 year trends with regards to the changing magnitude of, and outcomes associated with, heart attacks in residents of a large central New England community. The results of this investigation will also provide contemporary insights on how patients who experience heart attacks in the community are treated by physicians.",Community Surveillance of Coronary Heart Disease,9838247,R01HL135219,"['Acute', 'Acute myocardial infarction', 'Address', 'Adoption', 'American', 'Area', 'Big Data', 'Caring', 'Case Fatality Rates', 'Cessation of life', 'Classification', 'Clinical', 'Clinical Informatics', 'Clinical Treatment', 'Clinical Trials', 'Code', 'Collection', 'Communities', 'Computerized Medical Record', 'Coronary', 'Coronary heart disease', 'Data', 'Development', 'Diagnostic', 'Disease Surveillance', 'Epidemiologist', 'Fostering', 'Future', 'Guidelines', 'Harvest', 'Health Care Costs', 'Health Policy', 'Health Services', 'Hospital Administration', 'Hospitals', 'ICD-9', 'Impairment', 'Incidence', 'Information Technology', 'Inpatients', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Investigation', 'Life Expectancy', 'Manuals', 'Medical', 'Medical center', 'Medicine', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Myocardial Infarction', 'Myocardial Reperfusion', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'New England', 'Outcome', 'Participant', 'Patients', 'Periodicity', 'Physicians', 'Policy Maker', 'Population', 'Population Study', 'Practice Management', 'Prevention', 'Public Health', 'Quality of life', 'Recurrence', 'Research', 'Research Personnel', 'Research Support', 'Rural', 'Secondary Prevention', 'Source', 'Symptoms', 'System', 'Technology', 'Time', 'Update', 'Work', 'base', 'clinical epidemiology', 'clinically relevant', 'cost effective', 'electronic data', 'experience', 'functional disability', 'health disparity', 'high risk population', 'hospital readmission', 'innovation', 'insight', 'metropolitan', 'mortality', 'older patient', 'optimal treatments', 'outcome forecast', 'patient health information', 'population based', 'prevent', 'sociodemographics', 'socioeconomics', 'surveillance data', 'treatment guidelines', 'trend']",NHLBI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2020,768763,0.02135866565566336
"Transforming Patient Safety Event Data into Actionable Insights through Advanced Analytics Abstract The objective of the proposed research is to develop an innovative algorithms and a software tool to reduce the burden of safety event report classification and analysis so that report data can be transformed to actionable insights. Making safety event data more actionable will support the proactive identification of safety hazards before patients are harmed. We will achieve our research objective through (1) the development of natural language processing algorithms to classify safety event reports into actionable medication error categories; (2) the development of prototype software that will automatically categorize and visualize safety event reports to support trend identification; and (3) the pilot testing of prototype software with hospital and patient safety organization safety analysts. This project utilizes the extensive expertise of the research team in human factors and safety science, including computer science, specifically regarding information retrieval and data classification. Our research team includes patient safety organizations and collaboration with the computer science department at Georgetown University. The proposal is directly aligned with AHRQ’s priority area of making health care safer. Contributions from this research will include an expansion of our understanding of natural language processing and its application to categorizing clinical text, advances in visual analytics, and the development of a software tool to support patient safety analysts. The outputs of this research will serve both healthcare organizations and patient safety organizations allowing them to more efficiently and effectively analyze safety report data. Project Narrative This project is relevant to public health because it applies human factors and computer science to develop software to improve the analysis of patient safety event report data to reduce safety hazards and prevent patient harm. Patient safety event report data will be analyzed using natural language processing algorithms to more efficiently classify events into error categories. Based on these algorithms, prototype software will be developed, tested, and disseminated with the goal of automatic categorization and visualization of safety event reports to identify important safety hazards.",Transforming Patient Safety Event Data into Actionable Insights through Advanced Analytics,9962801,R01HS026481,[' '],AHRQ,MEDSTAR HEALTH RESEARCH INSTITUTE,R01,2020,398080,0.02874587185521628
"Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign Project Summary/Abstract Written The objective of the proposed research is to reduce the patient safety hazards associated with electronic medication administration records (eMARs) by, (1) understanding current usability and safety gaps, and (2) creating design and development documents, wireframes, and prototypes to serve as the foundation for future eMARs that will eliminate these gaps. In particular, we focus on communication and information flow challenges between nurses, pharmacists, and physicians during medication administration and use of the eMAR. The proposed research is in direct response to special emphasis notice (NOT-HS-16-009). We will develop a broad understanding of usability and safety hazards associated with eMARs by analyzing a large dataset of 1.7 million patient safety event reports and detailed medication error related narratives. We will then conduct heuristic analyses of current eMARs, and interviews and observations of physicians, nurses, and pharmacists. These data will serve to inform the development of eMAR design documents, wireframes, and prototypes as the foundation for future development. This project utilizes the extensive expertise of the research team in human factors and safety science, health information technology (health IT), informatics, and data science. Our research team includes physicians, nurses, pharmacists, and human factors engineers, and experts in natural language processing. In addition, our partnerships include a patient safety organization and a health IT vendor. The proposal addresses fundamental aspects of the call for proposals by providing new insights on the safety of health IT and improves current practices by developing use cases and new prototypes for immediate use by healthIT vendors. Contributions from this research will include a fundamental understanding of the role of health IT during medication administration with a focus on communication and information flow, design, development and testing documents for vendors and providers, and eMAR wireframes and prototypes to improve development. Our research will also provide organizations like the Office of the National Coordinator with medication related test scenarios to assess current health IT systems. Project Narrative This project is relevant to public health because it applies the sciences of human factors and informatics to improve the usability and safety of the electronic medication administration records (eMARs), which will ultimately improve patient care. Patient safety event report data will be analyzed to identify usability and safety hazards in eMARs, and interviews and observations will be conducted to identify clinician needs. Based on this knowledge eMAR design documents, wireframes, and prototypes will be developed, tested, and disseminated with the goal of improving communication and information flow to reduce medication related errors.",Improving Patient Safety and Clinician Cognitive Support Through eMAR Redesign,9912124,R01HS025136,[' '],AHRQ,MEDSTAR HEALTH RESEARCH INSTITUTE,R01,2020,398492,0.029230258671862613

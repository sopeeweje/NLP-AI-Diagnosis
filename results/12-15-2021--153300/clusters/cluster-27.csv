text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Comprehensive but simple encoding of bioassays to accelerate translational drug discovery PROJECT SUMMARY Collaborative Drug Discovery, Inc. (CDD) proposes to extend its innovative software platform — BioAssay Express (BAE, version 1.0) — which helps biologists to quickly and easily encode their plain-text biological assay protocols into formats suitable for computational processing. In Phase 2B, CDD will further enhance BAE so that it can markup a bioassay with a comprehensive, detailed set of annotations that completely specifies the protocol, step by step. In its current state of development in Phase 2, BAE 1.0 enables scientists to summarize the English language text that specifies an experimental protocol, supplementing it with machine-interpretable descriptors. At the end of Phase 2B, BAE 2.0 will output a detailed machine- interpretable version that will be completely equivalent to, and can optionally substitute for, the traditional English language method description. The enhanced BAE 2.0 software will enable scientists engaged in early stage drug discovery to efficiently and accurately document experimental procedures and intelligently aggregate datasets across research groups. An important benefit will be facilitating improved access, use and reproducibility by (1) flagging differences between closely related assays; (2) correlating differences in protocols with differences in results; and (3) improving the reproducibility of experiments conducted in different laboratories, to highlight potential causes of divergences. The project will continue to leverage the related broad initiatives at NIH and elsewhere that are working to promote translational research, gather screening data via open repositories, and apply sophisticated ontologies to classify these datasets. Our project targets the intersection of these disparate initiatives and unifies them by making their complex standards and guidelines for praxis realistically accessible to researchers who want to focus on their scientific work To encourage adoption, the software will prioritize intuitive ease of use by scientists who are not informatics experts, harmonize with existing laboratory workflows, minimize the extra effort of annotation, integrate seamlessly into preclinical data management platforms (including but not limited to CDD’s own CDD Vault), and deliver clear and immediate benefits to the user as part of an integrated experience. This combination of new capabilities and extreme ease of use will accelerate translational drug discovery efforts by empowering software platforms that bridge the divide between biologists and medicinal chemists to apply sophisticated tools — long available on the chemistry side — for the first time also to the biological side, and thus across both domains. Existing software can already easily connect screening results to chemical structures. This new platform will further connect these data to the purpose and methodology of the screens. PROJECT NARRATIVE The proposed project will create novel computational tools that will help researchers to efficiently and accurately document experimental procedures, thereby facilitating improved access, use and reproducibility of data and assays. This fundamental capability ultimately accelerates translating new experimental discoveries into the development of novel and improved drugs against a wide range of diseases. These tools will particularly benefit networks of researchers working on diseases that leading pharmaceutical companies have largely ignored because they are not perceived as highly profitable opportunities, despite the fact that in many cases they afflict millions of people.",Comprehensive but simple encoding of bioassays to accelerate translational drug discovery,9608807,R44TR000185,"['Academia', 'Adoption', 'Algorithms', 'Awareness', 'Bioinformatics', 'Biological', 'Biological Assay', 'Chemical Structure', 'Chemistry', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Disease', 'English Language', 'Ensure', 'Experimental Designs', 'Government', 'Guidelines', 'Improve Access', 'Incentives', 'Industry', 'Institutes', 'Instruction', 'Intelligence', 'Intuition', 'Laboratories', 'Letters', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Ontology', 'Output', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Policies', 'Procedures', 'Protocols documentation', 'PubChem', 'Reproducibility', 'Research', 'Research Personnel', 'Retrieval', 'Robotics', 'Scientist', 'Screening Result', 'Semantics', 'Side', 'Specific qualifier value', 'Standardization', 'Structure', 'Technology', 'Testing', 'Texas', 'Text', 'Time', 'Training', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Work', 'biomedical ontology', 'computerized tools', 'data management', 'drug discovery', 'empowered', 'experience', 'experimental study', 'improved', 'innovation', 'novel', 'pre-clinical', 'prevent', 'prospective', 'repository', 'robotic system', 'scaffold', 'screening', 'tool']",NCATS,"COLLABORATIVE DRUG DISCOVERY, INC.",R44,2019,754441,0.02956801198828683
"Comprehensive but simple encoding of bioassays to accelerate translational drug discovery PROJECT SUMMARY Collaborative Drug Discovery, Inc. (CDD) proposes to extend its innovative software platform — BioAssay Express (BAE, version 1.0) — which helps biologists to quickly and easily encode their plain-text biological assay protocols into formats suitable for computational processing. In Phase 2B, CDD will further enhance BAE so that it can markup a bioassay with a comprehensive, detailed set of annotations that completely specifies the protocol, step by step. In its current state of development in Phase 2, BAE 1.0 enables scientists to summarize the English language text that specifies an experimental protocol, supplementing it with machine-interpretable descriptors. At the end of Phase 2B, BAE 2.0 will output a detailed machine- interpretable version that will be completely equivalent to, and can optionally substitute for, the traditional English language method description. The enhanced BAE 2.0 software will enable scientists engaged in early stage drug discovery to efficiently and accurately document experimental procedures and intelligently aggregate datasets across research groups. An important benefit will be facilitating improved access, use and reproducibility by (1) flagging differences between closely related assays; (2) correlating differences in protocols with differences in results; and (3) improving the reproducibility of experiments conducted in different laboratories, to highlight potential causes of divergences. The project will continue to leverage the related broad initiatives at NIH and elsewhere that are working to promote translational research, gather screening data via open repositories, and apply sophisticated ontologies to classify these datasets. Our project targets the intersection of these disparate initiatives and unifies them by making their complex standards and guidelines for praxis realistically accessible to researchers who want to focus on their scientific work To encourage adoption, the software will prioritize intuitive ease of use by scientists who are not informatics experts, harmonize with existing laboratory workflows, minimize the extra effort of annotation, integrate seamlessly into preclinical data management platforms (including but not limited to CDD’s own CDD Vault), and deliver clear and immediate benefits to the user as part of an integrated experience. This combination of new capabilities and extreme ease of use will accelerate translational drug discovery efforts by empowering software platforms that bridge the divide between biologists and medicinal chemists to apply sophisticated tools — long available on the chemistry side — for the first time also to the biological side, and thus across both domains. Existing software can already easily connect screening results to chemical structures. This new platform will further connect these data to the purpose and methodology of the screens. PROJECT NARRATIVE The proposed project will create novel computational tools that will help researchers to efficiently and accurately document experimental procedures, thereby facilitating improved access, use and reproducibility of data and assays. This fundamental capability ultimately accelerates translating new experimental discoveries into the development of novel and improved drugs against a wide range of diseases. These tools will particularly benefit networks of researchers working on diseases that leading pharmaceutical companies have largely ignored because they are not perceived as highly profitable opportunities, despite the fact that in many cases they afflict millions of people.",Comprehensive but simple encoding of bioassays to accelerate translational drug discovery,9464228,R44TR000185,"['Academia', 'Adoption', 'Algorithms', 'Awareness', 'Bioinformatics', 'Biological', 'Biological Assay', 'Chemical Structure', 'Chemistry', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Disease', 'English Language', 'Ensure', 'Experimental Designs', 'Government', 'Guidelines', 'Improve Access', 'Incentives', 'Industry', 'Institutes', 'Instruction', 'Intuition', 'Laboratories', 'Letters', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Ontology', 'Output', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Policies', 'Procedures', 'Protocols documentation', 'PubChem', 'Reproducibility', 'Research', 'Research Personnel', 'Retrieval', 'Robotics', 'Scientist', 'Screening Result', 'Semantics', 'Side', 'Specific qualifier value', 'Standardization', 'Structure', 'System', 'Technology', 'Testing', 'Texas', 'Text', 'Time', 'Training', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Work', 'biomedical ontology', 'computerized tools', 'data management', 'drug discovery', 'empowered', 'experience', 'experimental study', 'improved', 'innovation', 'novel', 'pre-clinical', 'prevent', 'prospective', 'repository', 'scaffold', 'screening', 'tool']",NCATS,"COLLABORATIVE DRUG DISCOVERY, INC.",R44,2018,744321,0.02956801198828683
"FLYBASE: A DROSOPHILA GENOMIC AND GENETIC DATABASE DESCRIPTION (provided by applicant): FlyBase is the core genomic / genetic Model Organism Database (MOD) for the important biomedical model, Drosophila melanogaster and related species of flies. Drosophila is one of the premiere animal research systems and can be used very cost-effectively to help understand the etiology of human genetic diseases and is the closest experimental model that care provide insights into the biology of insect vectors of human disease (such as mosquitos that carry major infectious diseases such as malaria. West Nile Virus. The goals are twofold: (1) to provide a centralized resource for Drosophila genetic/genomic data and experimental reagents in order to enable Drosophila research to advance as rapidly as possible and (2) to provide these results to the broader biomedical research community to further their own research. FlyBase captures data from the primary scientific literature and from large-scale genome analysis and functional genomics community resource projects through annotation by FlyBase curators, direct user submissions and automated text-mining. FlyBase and collaborating informatics resources extensively share data thereby leveraging the output of each group. These heterogeneous data are organized into coherent datasets and integrated in a central database according to a series of ontology organizing systems. On a bi-monthly basis, these data are extracted from the central FlyBase database and served to the entire scientific community through a freely accessible FlyBase website, which is accessed millions of times each month. In addition to providing the research community to ttie corpus of FlyBase-captured data in ways that can be readily interrogated and browsed, FlyBase also maintains millions of links to other biomedical websites providing other relevant information. Over the proposed 5-year grant period, FlyBase expects to contribute to the enhancement of MOD interoperability and to work with a broad range of informatics resources and journal publishers to provide as rich a set of data and data-mining tools as possible to the biomedical research community. RELEVANCE: FlyBase provides crucial data and informatic infrastructure to accelerate the pace of Drosophila research and for the broader biomedical sciences community to leverage these discoveries to further their own research. Drosophila is now a well-accepted experimental model for human disease research and is the closest genetic / genomic experimental model system for research on the insect vectors that carry human diseases.",FLYBASE: A DROSOPHILA GENOMIC AND GENETIC DATABASE,9250186,U41HG000739,"['Animal Experimentation', 'Biological', 'Biology', 'Biomedical Research', 'Caring', 'Communicable Diseases', 'Communities', 'Culicidae', 'Data', 'Data Set', 'Databases', 'Development', 'Documentation', 'Drosophila genome', 'Drosophila genus', 'Drosophila melanogaster', 'Drosophilidae', 'Educational workshop', 'Etiology', 'Evolution', 'Experimental Models', 'Freezing', 'Genbank', 'Gene Family', 'Generations', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Models', 'Genome', 'Genomics', 'Goals', 'Grant', 'Hereditary Disease', 'Human Genetics', 'Individual', 'Insect Vectors', 'Journals', 'Link', 'Literature', 'Macromolecular Complexes', 'Malaria', 'Manuals', 'Maps', 'Metadata', 'Modeling', 'Molecular Genetics', 'Ontology', 'Output', 'Paper', 'Pathway interactions', 'Phenotype', 'Procedures', 'Production', 'Proteins', 'Reagent', 'Reporting', 'Research', 'Resource Informatics', 'Resources', 'Role', 'Running', 'Science', 'Series', 'Site', 'Structure', 'Study models', 'Suggestion', 'System', 'Time', 'Training', 'Triage', 'Update', 'West Nile virus', 'Work', 'cost', 'data access', 'data integration', 'data integrity', 'data mining', 'data resource', 'data sharing', 'fly', 'functional genomics', 'genome analysis', 'genome-wide', 'genomic data', 'human disease', 'informatics infrastructure', 'insight', 'interoperability', 'model organisms databases', 'outreach', 'symposium', 'systems research', 'text searching', 'tool', 'web interface', 'web site']",NHGRI,HARVARD UNIVERSITY,U41,2017,4342750,0.09301087722589729
"FLYBASE: A DROSOPHILA GENOMIC AND GENETIC DATABASE DESCRIPTION (provided by applicant): FlyBase is the core genomic / genetic Model Organism Database (MOD) for the important biomedical model, Drosophila melanogaster and related species of flies. Drosophila is one of the premiere animal research systems and can be used very cost-effectively to help understand the etiology of human genetic diseases and is the closest experimental model that care provide insights into the biology of insect vectors of human disease (such as mosquitos that carry major infectious diseases such as malaria. West Nile Virus. The goals are twofold: (1) to provide a centralized resource for Drosophila genetic/genomic data and experimental reagents in order to enable Drosophila research to advance as rapidly as possible and (2) to provide these results to the broader biomedical research community to further their own research. FlyBase captures data from the primary scientific literature and from large-scale genome analysis and functional genomics community resource projects through annotation by FlyBase curators, direct user submissions and automated text-mining. FlyBase and collaborating informatics resources extensively share data thereby leveraging the output of each group. These heterogeneous data are organized into coherent datasets and integrated in a central database according to a series of ontology organizing systems. On a bi-monthly basis, these data are extracted from the central FlyBase database and served to the entire scientific community through a freely accessible FlyBase website, which is accessed millions of times each month. In addition to providing the research community to ttie corpus of FlyBase-captured data in ways that can be readily interrogated and browsed, FlyBase also maintains millions of links to other biomedical websites providing other relevant information. Over the proposed 5-year grant period, FlyBase expects to contribute to the enhancement of MOD interoperability and to work with a broad range of informatics resources and journal publishers to provide as rich a set of data and data-mining tools as possible to the biomedical research community. RELEVANCE: FlyBase provides crucial data and informatic infrastructure to accelerate the pace of Drosophila research and for the broader biomedical sciences community to leverage these discoveries to further their own research. Drosophila is now a well-accepted experimental model for human disease research and is the closest genetic / genomic experimental model system for research on the insect vectors that carry human diseases.",FLYBASE: A DROSOPHILA GENOMIC AND GENETIC DATABASE,9043157,U41HG000739,"['Ally', 'Animal Experimentation', 'Biological', 'Biological Models', 'Biology', 'Biomedical Research', 'Caring', 'Communicable Diseases', 'Communities', 'Culicidae', 'Data', 'Data Set', 'Databases', 'Development', 'Documentation', 'Drosophila genome', 'Drosophila genus', 'Drosophila melanogaster', 'Drosophilidae', 'Educational workshop', 'Etiology', 'Evolution', 'Experimental Models', 'Freezing', 'Genbank', 'Gene Family', 'Generations', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Models', 'Genome', 'Genomics', 'Goals', 'Grant', 'Hereditary Disease', 'Human Genetics', 'Individual', 'Insect Vectors', 'Journals', 'Link', 'Literature', 'Macromolecular Complexes', 'Malaria', 'Manuals', 'Maps', 'Metadata', 'Modeling', 'Molecular Genetics', 'Ontology', 'Output', 'Paper', 'Pathway interactions', 'Phenotype', 'Procedures', 'Production', 'Proteins', 'Reagent', 'Reporting', 'Research', 'Resource Informatics', 'Resources', 'Role', 'Running', 'Science', 'Series', 'Site', 'Structure', 'Study models', 'Suggestion', 'System', 'Time', 'Training', 'Triage', 'Update', 'West Nile virus', 'Work', 'base', 'cost', 'data access', 'data integration', 'data integrity', 'data mining', 'data sharing', 'fly', 'functional genomics', 'genome analysis', 'genomic data', 'human disease', 'informatics infrastructure', 'insight', 'interoperability', 'model organisms databases', 'outreach', 'symposium', 'systems research', 'text searching', 'tool', 'web interface', 'web site']",NHGRI,HARVARD UNIVERSITY,U41,2016,4342750,0.09301087722589729
"FLYBASE: A DROSOPHILA GENOMIC AND GENETIC DATABASE DESCRIPTION (provided by applicant): FlyBase is the core genomic / genetic Model Organism Database (MOD) for the important biomedical model, Drosophila melanogaster and related species of flies. Drosophila is one of the premiere animal research systems and can be used very cost-effectively to help understand the etiology of human genetic diseases and is the closest experimental model that care provide insights into the biology of insect vectors of human disease (such as mosquitos that carry major infectious diseases such as malaria. West Nile Virus. The goals are twofold: (1) to provide a centralized resource for Drosophila genetic/genomic data and experimental reagents in order to enable Drosophila research to advance as rapidly as possible and (2) to provide these results to the broader biomedical research community to further their own research. FlyBase captures data from the primary scientific literature and from large-scale genome analysis and functional genomics community resource projects through annotation by FlyBase curators, direct user submissions and automated text-mining. FlyBase and collaborating informatics resources extensively share data thereby leveraging the output of each group. These heterogeneous data are organized into coherent datasets and integrated in a central database according to a series of ontology organizing systems. On a bi-monthly basis, these data are extracted from the central FlyBase database and served to the entire scientific community through a freely accessible FlyBase website, which is accessed millions of times each month. In addition to providing the research community to ttie corpus of FlyBase-captured data in ways that can be readily interrogated and browsed, FlyBase also maintains millions of links to other biomedical websites providing other relevant information. Over the proposed 5-year grant period, FlyBase expects to contribute to the enhancement of MOD interoperability and to work with a broad range of informatics resources and journal publishers to provide as rich a set of data and data-mining tools as possible to the biomedical research community.         RELEVANCE: FlyBase provides crucial data and informatic infrastructure to accelerate the pace of Drosophila research and for the broader biomedical sciences community to leverage these discoveries to further their own research. Drosophila is now a well-accepted experimental model for human disease research and is the closest genetic / genomic experimental model system for research on the insect vectors that carry human diseases. n/a",FLYBASE: A DROSOPHILA GENOMIC AND GENETIC DATABASE,8902241,U41HG000739,"['Ally', 'Animal Experimentation', 'Biological', 'Biological Models', 'Biology', 'Biomedical Research', 'Caring', 'Communicable Diseases', 'Communities', 'Culicidae', 'Data', 'Data Set', 'Databases', 'Development', 'Drosophila genome', 'Drosophila genus', 'Drosophila melanogaster', 'Drosophilidae', 'Educational workshop', 'Etiology', 'Evolution', 'Experimental Models', 'Freezing', 'Genbank', 'Gene Family', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Models', 'Genome', 'Genomics', 'Goals', 'Grant', 'Hereditary Disease', 'Human Genetics', 'Individual', 'Informatics', 'Insect Vectors', 'Journals', 'Link', 'Literature', 'Macromolecular Complexes', 'Malaria', 'Manuals', 'Maps', 'Metadata', 'Modeling', 'Molecular Genetics', 'Ontology', 'Output', 'Paper', 'Pathway interactions', 'Phenotype', 'Procedures', 'Production', 'Proteins', 'Reagent', 'Reporting', 'Research', 'Research Infrastructure', 'Resource Informatics', 'Resources', 'Role', 'Running', 'Series', 'Site', 'Structure', 'Study models', 'Suggestion', 'System', 'Time', 'Training', 'Triage', 'Update', 'West Nile virus', 'Work', 'citizen science', 'cost', 'data integration', 'data integrity', 'data mining', 'data sharing', 'fly', 'functional genomics', 'genome analysis', 'human disease', 'insight', 'interoperability', 'model organisms databases', 'outreach', 'symposium', 'systems research', 'text searching', 'tool', 'web interface', 'web site']",NHGRI,HARVARD UNIVERSITY,U41,2015,4234181,0.09301087722589729
"FLYBASE: A DROSOPHILA GENOMIC AND GENETIC DATABASE     DESCRIPTION (provided by applicant): FlyBase is the core genomic / genetic Model Organism Database (MOD) for the important biomedical model, Drosophila melanogaster and related species of flies. Drosophila is one of the premiere animal research systems and can be used very cost-effectively to help understand the etiology of human genetic diseases and is the closest experimental model that care provide insights into the biology of insect vectors of human disease (such as mosquitos that carry major infectious diseases such as malaria. West Nile Virus. The goals are twofold: (1) to provide a centralized resource for Drosophila genetic/genomic data and experimental reagents in order to enable Drosophila research to advance as rapidly as possible and (2) to provide these results to the broader biomedical research community to further their own research. FlyBase captures data from the primary scientific literature and from large-scale genome analysis and functional genomics community resource projects through annotation by FlyBase curators, direct user submissions and automated text-mining. FlyBase and collaborating informatics resources extensively share data thereby leveraging the output of each group. These heterogeneous data are organized into coherent datasets and integrated in a central database according to a series of ontology organizing systems. On a bi-monthly basis, these data are extracted from the central FlyBase database and served to the entire scientific community through a freely accessible FlyBase website, which is accessed millions of times each month. In addition to providing the research community to ttie corpus of FlyBase-captured data in ways that can be readily interrogated and browsed, FlyBase also maintains millions of links to other biomedical websites providing other relevant information. Over the proposed 5-year grant period, FlyBase expects to contribute to the enhancement of MOD interoperability and to work with a broad range of informatics resources and journal publishers to provide as rich a set of data and data-mining tools as possible to the biomedical research community.         RELEVANCE: FlyBase provides crucial data and informatic infrastructure to accelerate the pace of Drosophila research and for the broader biomedical sciences community to leverage these discoveries to further their own research. Drosophila is now a well-accepted experimental model for human disease research and is the closest genetic / genomic experimental model system for research on the insect vectors that carry human diseases.                   n/a",FLYBASE: A DROSOPHILA GENOMIC AND GENETIC DATABASE,8665574,U41HG000739,"['Ally', 'Animal Experimentation', 'Biological', 'Biological Models', 'Biology', 'Biomedical Research', 'Caring', 'Communicable Diseases', 'Communities', 'Culicidae', 'Data', 'Data Set', 'Databases', 'Development', 'Drosophila genome', 'Drosophila genus', 'Drosophila melanogaster', 'Drosophilidae', 'Educational workshop', 'Etiology', 'Evolution', 'Experimental Models', 'Freezing', 'Genbank', 'Gene Family', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Models', 'Genome', 'Genomics', 'Goals', 'Grant', 'Hereditary Disease', 'Human Genetics', 'Individual', 'Informatics', 'Insect Vectors', 'Journals', 'Link', 'Literature', 'Macromolecular Complexes', 'Malaria', 'Manuals', 'Maps', 'Metadata', 'Modeling', 'Molecular Genetics', 'Ontology', 'Output', 'Paper', 'Pathway interactions', 'Phenotype', 'Procedures', 'Production', 'Proteins', 'Reagent', 'Reporting', 'Research', 'Research Infrastructure', 'Resource Informatics', 'Resources', 'Role', 'Running', 'Science', 'Series', 'Site', 'Structure', 'Study models', 'Suggestion', 'System', 'Time', 'Training', 'Triage', 'Update', 'West Nile virus', 'Work', 'cost', 'data integration', 'data integrity', 'data mining', 'data sharing', 'fly', 'functional genomics', 'genome analysis', 'human disease', 'insight', 'interoperability', 'model organisms databases', 'outreach', 'symposium', 'systems research', 'text searching', 'tool', 'web interface', 'web site']",NHGRI,HARVARD UNIVERSITY,U41,2014,3279871,0.09301087722589729
"WormBase: a core data resource for C. elegans and other nematodes Project Summary WormBase is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural signficance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes and biology. Most users access WormBase via the Internet (www.wormbase.org); some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate many nematode genome sequences, along with their annotations and core genetic information, as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information and develops ontologies; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information, as well as user outreach and education. Project Narrative Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs, and whole anima development, physiology and behavior to understand human disease. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this diverse and important group of animals.",WormBase: a core data resource for C. elegans and other nematodes,9750325,U24HG002223,"['Affect', 'Agriculture', 'Anatomy', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cells', 'ChIP-seq', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Complex', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Dependence', 'Development', 'Disease', 'Disease model', 'Documentation', 'Education and Outreach', 'Educational workshop', 'Electronic Mail', 'Elements', 'Ensure', 'FAIR principles', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Gold', 'Graph', 'Health', 'Human', 'Imagery', 'Incentives', 'Information Resources', 'Information Retrieval', 'Internet', 'Laboratories', 'Leadership', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Persons', 'Phenotype', 'Physiology', 'Play', 'Publications', 'Reagent', 'Reference Standards', 'Regulation', 'Regulatory Element', 'Regulatory Pathway', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Role', 'Scientist', 'Services', 'Site', 'Source', 'System', 'Techniques', 'Time', 'Training', 'Transcript', 'Validation', 'Variant', 'Visit', 'Work', 'Workplace', 'World Health', 'application programming interface', 'base', 'cloud based', 'comparative', 'data mining', 'data resource', 'data visualization', 'design', 'distributed data', 'functional genomics', 'gene function', 'gene interaction', 'gene product', 'genetic analysis', 'genetic information', 'genome-wide', 'genomic data', 'health economics', 'human disease', 'information display', 'interest', 'interoperability', 'light weight', 'member', 'novel', 'outreach', 'reference genome', 'relational database', 'social media', 'symposium', 'system architecture', 'text searching', 'tool', 'tool development', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U24,2018,1194375,0.09705948373033772
"WormBase: a core data resource for C. elegans and other nematodes Project Summary WormBase is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural signficance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes and biology. Most users access WormBase via the Internet (www.wormbase.org); some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate many nematode genome sequences, along with their annotations and core genetic information, as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information and develops ontologies; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information, as well as user outreach and education. Project Narrative Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs, and whole anima development, physiology and behavior to understand human disease. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this diverse and important group of animals.",WormBase: a core data resource for C. elegans and other nematodes,9572165,U24HG002223,"['Affect', 'Agriculture', 'Anatomy', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cells', 'ChIP-seq', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Complex', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Dependence', 'Development', 'Disease', 'Disease model', 'Documentation', 'Education and Outreach', 'Educational workshop', 'Electronic Mail', 'Elements', 'Ensure', 'FAIR principles', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Gold', 'Graph', 'Health', 'Human', 'Imagery', 'Incentives', 'Information Resources', 'Information Retrieval', 'Internet', 'Laboratories', 'Leadership', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Persons', 'Phenotype', 'Physiology', 'Play', 'Publications', 'Reagent', 'Reference Standards', 'Regulation', 'Regulatory Element', 'Regulatory Pathway', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Role', 'Scientist', 'Services', 'Site', 'Source', 'System', 'Techniques', 'Time', 'Training', 'Transcript', 'Validation', 'Variant', 'Visit', 'Work', 'Workplace', 'World Health', 'application programming interface', 'base', 'cloud based', 'comparative', 'data mining', 'data resource', 'data visualization', 'design', 'distributed data', 'functional genomics', 'gene function', 'gene interaction', 'gene product', 'genetic analysis', 'genetic information', 'genome-wide', 'genomic data', 'health economics', 'human disease', 'information display', 'interest', 'interoperability', 'light weight', 'member', 'novel', 'outreach', 'reference genome', 'relational database', 'social media', 'symposium', 'system architecture', 'text searching', 'tool', 'tool development', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U24,2018,2493700,0.09705948373033772
"WormBase:  a core data resource for C. elegans and other nematodes DESCRIPTION (provided by applicant): WormBase Is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural importance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes. Most users access WormBase via the Internet (www.wormbase.orq). While some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate hundreds of nematode genome sequences, annotations and core genetic information as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information.         RELEVANCE Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs and a whole animal's development, physiology and behavior. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this numerous and important group of animals. n/a",WormBase:  a core data resource for C. elegans and other nematodes,9330882,U41HG002223,"['Affect', 'Agriculture', 'Anatomy', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Caenorhabditis elegans', 'Cells', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dependency', 'Development', 'Elements', 'Ensure', 'Galaxy', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Human', 'Imagery', 'Incentives', 'Information Retrieval', 'Internet', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Modernization', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Online Systems', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Phenotype', 'Physiology', 'Proteins', 'RNA', 'RNA immunoprecipitation sequencing', 'Reagent', 'Regulation', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Set protein', 'Site', 'Source', 'System', 'Systems Biology', 'Technology', 'Terminology', 'Time', 'Transcript', 'Update', 'Workplace', 'World Health', 'application programming interface', 'base', 'biological research', 'comparative', 'data integration', 'data mining', 'data resource', 'distributed data', 'drug development', 'experimental study', 'functional genomics', 'gene function', 'gene interaction', 'gene product', 'genetic analysis', 'genetic information', 'genome-wide', 'genomic data', 'health economics', 'human disease', 'information display', 'interoperability', 'light weight', 'member', 'non-genomic', 'novel', 'reference genome', 'text searching', 'tool', 'transcriptome sequencing', 'web site', 'web-based tool']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U41,2017,4178190,0.0727047095586425
"WormBase: a core data resource for C. elegans and other nematodes Project Summary WormBase is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural signficance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes and biology. Most users access WormBase via the Internet (www.wormbase.org); some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate many nematode genome sequences, along with their annotations and core genetic information, as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information and develops ontologies; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information, as well as user outreach and education. Project Narrative Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs, and whole anima development, physiology and behavior to understand human disease. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this diverse and important group of animals.",WormBase: a core data resource for C. elegans and other nematodes,9976564,U24HG002223,"['Affect', 'Agriculture', 'Anatomy', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cells', 'ChIP-seq', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Data', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Dependence', 'Development', 'Disease', 'Disease model', 'Documentation', 'Education and Outreach', 'Educational workshop', 'Electronic Mail', 'Elements', 'Ensure', 'FAIR principles', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Gold', 'Graph', 'Health', 'Human', 'Incentives', 'Information Resources', 'Information Retrieval', 'Infrastructure', 'Internet', 'Laboratories', 'Leadership', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Persons', 'Phenotype', 'Physiology', 'Play', 'Publications', 'Reagent', 'Reference Standards', 'Regulation', 'Regulatory Element', 'Regulatory Pathway', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Role', 'Scientist', 'Services', 'Site', 'Source', 'System', 'Techniques', 'Time', 'Training', 'Transcript', 'Validation', 'Variant', 'Visit', 'Visualization', 'Work', 'Workplace', 'World Health', 'WormBase', 'application programming interface', 'base', 'cloud based', 'comparative', 'complex data ', 'data mining', 'data resource', 'data tools', 'data visualization', 'design', 'distributed data', 'functional genomics', 'gene function', 'gene interaction', 'gene product', 'genetic analysis', 'genetic information', 'genome resource', 'genome-wide', 'genomic data', 'health economics', 'human disease', 'information display', 'interest', 'interoperability', 'large scale data', 'light weight', 'member', 'novel', 'outreach', 'reference genome', 'relational database', 'social media', 'symposium', 'system architecture', 'text searching', 'tool', 'tool development', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U24,2020,2200324,0.09705948373033772
"WormBase: a core data resource for C. elegans and other nematodes Project Summary WormBase is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural signficance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes and biology. Most users access WormBase via the Internet (www.wormbase.org); some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate many nematode genome sequences, along with their annotations and core genetic information, as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information and develops ontologies; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information, as well as user outreach and education. Project Narrative Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs, and whole anima development, physiology and behavior to understand human disease. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this diverse and important group of animals.",WormBase: a core data resource for C. elegans and other nematodes,9776601,U24HG002223,"['Affect', 'Agriculture', 'Anatomy', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cells', 'ChIP-seq', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Complex', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Dependence', 'Development', 'Disease', 'Disease model', 'Documentation', 'Education and Outreach', 'Educational workshop', 'Electronic Mail', 'Elements', 'Ensure', 'FAIR principles', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Gold', 'Graph', 'Health', 'Human', 'Imagery', 'Incentives', 'Information Resources', 'Information Retrieval', 'Infrastructure', 'Internet', 'Laboratories', 'Leadership', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Persons', 'Phenotype', 'Physiology', 'Play', 'Publications', 'Reagent', 'Reference Standards', 'Regulation', 'Regulatory Element', 'Regulatory Pathway', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Role', 'Scientist', 'Services', 'Site', 'Source', 'System', 'Techniques', 'Time', 'Training', 'Transcript', 'Validation', 'Variant', 'Visit', 'Work', 'Workplace', 'World Health', 'application programming interface', 'base', 'cloud based', 'comparative', 'data mining', 'data resource', 'data visualization', 'design', 'distributed data', 'functional genomics', 'gene function', 'gene interaction', 'gene product', 'genetic analysis', 'genetic information', 'genome-wide', 'genomic data', 'health economics', 'human disease', 'information display', 'interest', 'interoperability', 'light weight', 'member', 'novel', 'outreach', 'reference genome', 'relational database', 'social media', 'symposium', 'system architecture', 'text searching', 'tool', 'tool development', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U24,2019,2347011,0.09705948373033772
"WormBase:  a core data resource for C. elegans and other nematodes     DESCRIPTION (provided by applicant): WormBase Is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural importance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes. Most users access WormBase via the Internet (www.wormbase.orq). While some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate hundreds of nematode genome sequences, annotations and core genetic information as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information.         RELEVANCE Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs and a whole animal's development, physiology and behavior. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this numerous and important group of animals.                  n/a",WormBase:  a core data resource for C. elegans and other nematodes,9130596,U41HG002223,"['Affect', 'Agriculture', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Caenorhabditis elegans', 'Cells', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dependency', 'Development', 'Elements', 'Ensure', 'Galaxy', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Housing', 'Human', 'Imagery', 'Information Retrieval', 'Internet', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Online Systems', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Phenotype', 'Physiology', 'Proteins', 'RNA', 'RNA immunoprecipitation sequencing', 'Reagent', 'Regulation', 'Regulator Genes', 'Research', 'Research Project Grants', 'Resources', 'Set protein', 'Site', 'Source', 'System', 'Systems Biology', 'Technology', 'Terminology', 'Transcript', 'Update', 'Workplace', 'World Health', 'application programming interface', 'base', 'biological research', 'comparative', 'data integration', 'data mining', 'distributed data', 'drug development', 'functional genomics', 'gene function', 'gene interaction', 'gene product', 'genetic analysis', 'genetic information', 'genome annotation', 'genome sequencing', 'genome-wide', 'genomic data', 'health economics', 'human disease', 'information display', 'interoperability', 'light weight', 'member', 'non-genomic', 'novel', 'reference genome', 'research study', 'text searching', 'tool', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U41,2016,2939165,0.0727047095586425
"WormBase:  a core data resource for C. elegans and other nematodes DESCRIPTION (provided by applicant): WormBase Is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural importance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes. Most users access WormBase via the Internet (www.wormbase.orq). While some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate hundreds of nematode genome sequences, annotations and core genetic information as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information.         RELEVANCE Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs and a whole animal's development, physiology and behavior. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this numerous and important group of animals. n/a",WormBase:  a core data resource for C. elegans and other nematodes,9336007,U41HG002223,"['Affect', 'Agriculture', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Caenorhabditis elegans', 'Cells', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dependency', 'Development', 'Elements', 'Ensure', 'Galaxy', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Housing', 'Human', 'Imagery', 'Information Retrieval', 'Internet', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Online Systems', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Phenotype', 'Physiology', 'Proteins', 'RNA', 'RNA immunoprecipitation sequencing', 'Reagent', 'Regulation', 'Regulator Genes', 'Research', 'Research Project Grants', 'Resources', 'Set protein', 'Site', 'Source', 'System', 'Systems Biology', 'Technology', 'Terminology', 'Transcript', 'Update', 'Workplace', 'World Health', 'application programming interface', 'base', 'biological research', 'comparative', 'data integration', 'data mining', 'distributed data', 'drug development', 'functional genomics', 'gene function', 'gene interaction', 'gene product', 'genetic analysis', 'genetic information', 'genome annotation', 'genome sequencing', 'genome-wide', 'genomic data', 'health economics', 'human disease', 'information display', 'interoperability', 'light weight', 'member', 'non-genomic', 'novel', 'reference genome', 'research study', 'text searching', 'tool', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U41,2016,83000,0.0727047095586425
"WormBase:  a core data resource for C. elegans and other nematodes DESCRIPTION (provided by applicant): WormBase Is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural importance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes. Most users access WormBase via the Internet (www.wormbase.orq). While some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate hundreds of nematode genome sequences, annotations and core genetic information as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information.         RELEVANCE Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs and a whole animal's development, physiology and behavior. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this numerous and important group of animals. n/a",WormBase:  a core data resource for C. elegans and other nematodes,9137332,U41HG002223,"['Affect', 'Agriculture', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Caenorhabditis elegans', 'Cells', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dependency', 'Development', 'Elements', 'Ensure', 'Galaxy', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Housing', 'Human', 'Imagery', 'Information Retrieval', 'Internet', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Online Systems', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Phenotype', 'Physiology', 'Proteins', 'RNA', 'RNA immunoprecipitation sequencing', 'Reagent', 'Regulation', 'Regulator Genes', 'Research', 'Research Project Grants', 'Resources', 'Set protein', 'Site', 'Source', 'System', 'Systems Biology', 'Technology', 'Terminology', 'Transcript', 'Update', 'Workplace', 'World Health', 'base', 'biological research', 'comparative', 'data integration', 'data mining', 'distributed data', 'drug development', 'functional genomics', 'gene function', 'gene interaction', 'genetic analysis', 'genetic information', 'genome annotation', 'genome sequencing', 'genome-wide', 'health economics', 'human disease', 'information display', 'interoperability', 'member', 'non-genomic', 'novel', 'programs', 'research study', 'text searching', 'tool', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U41,2015,451148,0.0727047095586425
"WormBase:  a core data resource for C. elegans and other nematodes DESCRIPTION (provided by applicant): WormBase Is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural importance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes. Most users access WormBase via the Internet (www.wormbase.orq). While some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate hundreds of nematode genome sequences, annotations and core genetic information as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information.         RELEVANCE Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs and a whole animal's development, physiology and behavior. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this numerous and important group of animals. n/a",WormBase:  a core data resource for C. elegans and other nematodes,9137322,U41HG002223,"['Affect', 'Agriculture', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Caenorhabditis elegans', 'Cells', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dependency', 'Development', 'Elements', 'Ensure', 'Galaxy', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Housing', 'Human', 'Imagery', 'Information Retrieval', 'Internet', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Online Systems', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Phenotype', 'Physiology', 'Proteins', 'RNA', 'RNA immunoprecipitation sequencing', 'Reagent', 'Regulation', 'Regulator Genes', 'Research', 'Research Project Grants', 'Resources', 'Set protein', 'Site', 'Source', 'System', 'Systems Biology', 'Technology', 'Terminology', 'Transcript', 'Update', 'Workplace', 'World Health', 'base', 'biological research', 'comparative', 'data integration', 'data mining', 'distributed data', 'drug development', 'functional genomics', 'gene function', 'gene interaction', 'genetic analysis', 'genetic information', 'genome annotation', 'genome sequencing', 'genome-wide', 'health economics', 'human disease', 'information display', 'interoperability', 'member', 'non-genomic', 'novel', 'programs', 'research study', 'text searching', 'tool', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U41,2015,206128,0.0727047095586425
"WormBase:  a core data resource for C. elegans and other nematodes     DESCRIPTION (provided by applicant): WormBase Is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural importance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes. Most users access WormBase via the Internet (www.wormbase.orq). While some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate hundreds of nematode genome sequences, annotations and core genetic information as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information.         RELEVANCE Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs and a whole animal's development, physiology and behavior. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this numerous and important group of animals.                  n/a",WormBase:  a core data resource for C. elegans and other nematodes,8886973,U41HG002223,"['Affect', 'Agriculture', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Caenorhabditis elegans', 'Cells', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dependency', 'Development', 'Elements', 'Ensure', 'Galaxy', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Housing', 'Human', 'Imagery', 'Information Retrieval', 'Internet', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Online Systems', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Phenotype', 'Physiology', 'Proteins', 'RNA', 'RNA immunoprecipitation sequencing', 'Reagent', 'Regulation', 'Regulator Genes', 'Research', 'Research Project Grants', 'Resources', 'Set protein', 'Site', 'Source', 'System', 'Systems Biology', 'Technology', 'Terminology', 'Transcript', 'Update', 'Workplace', 'World Health', 'base', 'biological research', 'comparative', 'data integration', 'data mining', 'distributed data', 'drug development', 'functional genomics', 'gene function', 'gene interaction', 'genetic analysis', 'genetic information', 'genome annotation', 'genome sequencing', 'genome-wide', 'health economics', 'human disease', 'information display', 'interoperability', 'member', 'non-genomic', 'novel', 'programs', 'research study', 'text searching', 'tool', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U41,2015,2864345,0.0727047095586425
"WormBase:  a core data resource for C. elegans and other nematodes     DESCRIPTION (provided by applicant): WormBase Is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural importance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes. Most users access WormBase via the Internet (www.wormbase.orq). While some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate hundreds of nematode genome sequences, annotations and core genetic information as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information.         RELEVANCE Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs and a whole animal's development, physiology and behavior. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this numerous and important group of animals.                  n/a",WormBase:  a core data resource for C. elegans and other nematodes,8735175,U41HG002223,"['Affect', 'Agriculture', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Caenorhabditis elegans', 'Cells', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dependency', 'Development', 'Elements', 'Ensure', 'Galaxy', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Housing', 'Human', 'Imagery', 'Information Retrieval', 'Internet', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Online Systems', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Phenotype', 'Physiology', 'Proteins', 'RNA', 'Reagent', 'Regulation', 'Regulator Genes', 'Research', 'Research Project Grants', 'Resources', 'Set protein', 'Site', 'Source', 'System', 'Systems Biology', 'Technology', 'Terminology', 'Transcript', 'Update', 'Workplace', 'World Health', 'base', 'biological research', 'comparative', 'data integration', 'data mining', 'distributed data', 'drug development', 'functional genomics', 'gene function', 'gene interaction', 'genetic analysis', 'genome annotation', 'genome sequencing', 'health economics', 'human disease', 'information display', 'interoperability', 'member', 'non-genomic', 'novel', 'programs', 'research study', 'text searching', 'tool', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U41,2014,2891903,0.0727047095586425
"WormBase:  a core data resource for C. elegans and other nematodes     DESCRIPTION (provided by applicant): WormBase Is the major publicly available database of information related to Caenorhabditis elegans, an important organism for basic biomedical research, and other nematodes of medical and agricultural importance. Although a crucial daily resource for members of the C. elegans research field, our users extend to the larger parasitology, biomedical, and bioinformatics research communities. WormBase acts as a central forum through which every research group can contribute to the global effort to comprehend nematode genomes. Most users access WormBase via the Internet (www.wormbase.orq). While some install the database locally. WormBase offers extensive coverage of C. elegans core genomic, genetic, anatomical and functional information, allowing the biomedical community to fully utilize the results of intensive molecular genetic analyses and functional genomic studies of this organism in the study of human disease. These data include all available nematode genomic data (such as genome sequence, transcripts and cis-regulatory sites prioritized by species), large-scale functional genomic datasets, the function and interactions of genes and gene products as they relate to development, physiology and behavior, and biological reagents and their source information. WormBase comprises a set of databases storing a wide range of biological information; a website that allows users to access stored information and precomputed analyses based on these data; and tools for programmatic access such as an application programming interface, a data mining platform, and bulk downloads. Curation activities include extraction and integration of information from the literature (assisted by the use of information retrieval tools), incorporation of large-scale datasets from a range of research projects, and gene model verification from experimental data. We will curate hundreds of nematode genome sequences, annotations and core genetic information as well as data on gene function, pathways and transcriptional regulatory networks for C. elegans and select other species. We will expand tools available for data mining, workflow management, visualization, and community annotation, and integrate, store and distribute data in a maintainable, interoperable and scalable system. The project team involves three sites: Caltech primarily curates functional information; EBI carries out sequence-based curation and builds databases for public release; and OICR develops and supports the web presence and visualization. The three sites work closely together and share tasks to ensure timely incorporation, storage and display of information.         RELEVANCE Nematodes (roundworms) are major parasites of humans, livestock and crops, and understanding how to control them is a major world health and economic challenge. One species of nematode, C. elegans, is extensively used for basic biomedical research to elucidate how networks of genes affect cells, organs and a whole animal's development, physiology and behavior. WormBase collects, stores and displays information about the genomes and genes of C. elegans and other nematodes to facilitate research about this numerous and important group of animals.                  n/a",WormBase:  a core data resource for C. elegans and other nematodes,8554533,U41HG002223,"['Affect', 'Agriculture', 'Animals', 'Architecture', 'Base Sequence', 'Behavior', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Caenorhabditis elegans', 'Cells', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dependency', 'Development', 'Elements', 'Ensure', 'Galaxy', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Housing', 'Human', 'Imagery', 'Information Retrieval', 'Internet', 'Literature', 'Livestock', 'Maintenance', 'Maps', 'Medical', 'Modeling', 'Molecular Genetics', 'Nematoda', 'Network-based', 'Online Systems', 'Ontology', 'Organ', 'Organism', 'Parasites', 'Parasitology', 'Pathway interactions', 'Phenotype', 'Physiology', 'Proteins', 'RNA', 'Reagent', 'Regulation', 'Regulator Genes', 'Research', 'Research Project Grants', 'Resources', 'Set protein', 'Site', 'Source', 'System', 'Systems Biology', 'Technology', 'Terminology', 'Transcript', 'Update', 'Workplace', 'World Health', 'base', 'biological research', 'comparative', 'data integration', 'data mining', 'distributed data', 'drug development', 'functional genomics', 'gene function', 'gene interaction', 'genetic analysis', 'genome annotation', 'genome sequencing', 'health economics', 'human disease', 'information display', 'interoperability', 'member', 'non-genomic', 'novel', 'programs', 'research study', 'text searching', 'tool', 'transcriptome sequencing', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,U41,2013,3106203,0.0727047095586425
"WormBase: a core data resource for C elegans and other nematodes    DESCRIPTION (provided by applicant):  Caenorhabditis elegans is a major model system for basic biological and biomedical research and the first animal for which there is a complete description of its genome, anatomy and development, and some information about each of its ~22,000 genes. Five years of funding is requested to maintain and expand WormBase, a Model Organism Database (MOD), with complete coverage of core genomic, genetic, anatomical and functional information about this and other nematodes. Such a database is necessary to allow the entire biomedical research community to make full use of nematode genomic sequences. The two top priorities will be intensive data curation and user interface improvement. WormBase will include up-to-date annotation of the genomic data, the current genetic and physical maps and many experimental data such as genome-scale datasets connected to the function and interactions of cells and genes, as well as development, physiology and behavior. Direct access to the sources of biological material, such as the strain collection of the Caenorhabditis Genetics Center and direct links to data sets maintained by others will be provided. Data will be recovered from the existing resources, from direct contribution of the individual laboratories, and from the literature. While WormBase will act as a central forum through which every laboratory will be able to contribute constructively to the global effort to fully comprehend this metazoan organism, WormBase professional curators will ensure detailed attribution of data sources and check consistency and integrity. To facilitate communication, WormBase will use technology, terminology and style concordant with other databases wherever possible. WormBase will maintain ontologies for nematode anatomy and phenotypes. WormBase will be Web-based and easy to use. Multiple relational databases will be used for data management; the object-based Acedb database system will be used for integration, and this integrated database plus ""slave"" relational databases will be used to drive the website. Coordination of the project and the main curation site will be at Caltech under the supervision of a C. elegans biologist. Curation and annotation of genomic sequence will take place at the centers - the Sanger Institute and Washington University - that generated the entire genome sequence. Oxford University will maintain genetic nomenclature.  Nematodes (roundworms) are major parasites of humans, livestock and crops, and extension of WormBase to broader coverage of nematode genomics will facilitate research into the diagnosis and treatment of nematode-based disease. Studies of C. elegans have informed us of basic principles of normal development and the molecular basis of aging, cancer, nicotine addiction, as well as a variety of fundamental biological processes such as cell migration, cell differentiation and cell death.              n/a",WormBase: a core data resource for C elegans and other nematodes,7502984,P41HG002223,"['Ablation', 'Age', 'Agriculture', 'Alleles', 'Anatomy', 'Animals', 'Antibodies', 'Architecture', 'Base Sequence', 'Behavior', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cell Communication', 'Cell Death', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Chromosome Mapping', 'Code', 'Collection', 'Communication', 'Communities', 'Comparative Anatomy', 'Compatible', 'DNA Sequence', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Ensure', 'Expressed Sequence Tags', 'Funding', 'Gene Expression Regulation', 'Gene Proteins', 'Gene Structure', 'Genes', 'Genetic', 'Genetic Processes', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Institutes', 'Internet', 'Knock-out', 'Knowledge', 'Laboratories', 'Link', 'Literature', 'Livestock', 'Longevity', 'Malignant Neoplasms', 'Maps', 'Medical', 'Metabolic', 'Methods', 'Molecular', 'Molecular Genetics', 'Mutation', 'Names', 'Natural Language Processing', 'Nature', 'Nematoda', 'Nicotine Dependence', 'Nomenclature', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Parasites', 'Parasitic nematode', 'Pathway interactions', 'Phenotype', 'Physical Chromosome Mapping', 'Physiology', 'Pliability', 'Process', 'Proteins', 'Proteomics', 'RNA Interference', 'Reagent', 'Regulation', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Secure', 'Site', 'Slave', 'Source', 'Subcellular Anatomy', 'Supervision', 'System', 'Techniques', 'Technology', 'Terminology', 'Tertiary Protein Structure', 'Transcript', 'Transgenes', 'Transgenic Organisms', 'Universities', 'Variant', 'Washington', 'Yeasts', 'base', 'cell motility', 'chromatin immunoprecipitation', 'comparative', 'comparative genomic hybridization', 'data integration', 'data management', 'data modeling', 'design', 'experience', 'functional genomics', 'gene function', 'genetic analysis', 'genome sequencing', 'improved', 'interoperability', 'member', 'migration', 'model organisms databases', 'programs', 'research study', 'small molecule', 'tool', 'transcription factor', 'usability', 'web interface', 'yeast two hybrid system']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,P41,2008,2750000,0.06868801106750388
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7941562,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,1038804,0.18590382236229352
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7581087,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,3437506,0.18590382236229352
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8061704,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'information model', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2011,2923298,0.18590382236229352
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8138946,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,745063,0.18590382236229352
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7780085,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,3513343,0.18590382236229352
"Gene Ontology Consortium PROJECT SUMMARY Because of the staggering complexity of biological systems, biomedical research is becoming increasingly dependent on knowledge stored in a computable form. The Gene Ontology (GO) is by far the largest knowledgebase of how genes function, and has become a critical component of the computational infrastructure enabling the genomic revolution. It has become nearly indispensible in the interpretation of large- scale molecular measurements in biological research. Crucially, for human health research, GO is also one of a suite of complementary ontologies constructed in such as way to maximally promote interoperability and comparability of data sets. It represents the gene functions and biological processes that are perturbed in human disease, e.g. via the links from Human Phenotype Ontology (HPO) class abnormality of lipid metabolism, defined in relation to the GO class lipid metabolic process (GO_0006629), researchers or clinicians can find the set of genes that are known to be involved in this process. GO is a knowledge resource that can be statistically mined, either standalone or in combination with data from other knowledge resources, which enables experts to discover connections and form new hypotheses from the biological networks GO represents. All knowledge in GO is represented using semantic web technologies and so is amenable to computational integration and consistency checking. The proposed GO knowledge environment will enable a wider community of scientists to contribute to, and to utilize, a common, computable representation of biology. To ensure the knowledge environment meets the requirements of biomedical researchers, we will: a) deliver a comprehensive, detailed, computable knowledgebase of gene function, encoded in the Gene Ontology and annotations (computer-readable statements about the how specific genes function), focusing on human biology; b) provide a “hub” for a broad community of scientists to collaboratively extend, correct and improve the knowledgebase; c) ensure the GO knowledge resource is of the highest quality with regards to depth, breadth and accuracy; d) facilitate the transfer of insights obtained from studies of non-human organisms, such as the mouse and zebrafish, to human biology; and e) enable the scientific community to use the knowledgebase in analyses of large-scale genetic and -omics data. Our aims reflect the essential requirements for realizing the overarching objectives for a biomedical data resource: efficiently capturing and integrating biological knowledge and adhering to the highest possible standard for accuracy and detail; constructing and providing a robust, flexible, powerful, and extensible technological infrastructure available not only for internal use but just as easily by the wider community; and lastly, leveraging state-of-the-art social media, web services and other technologies to disseminate the GO resource to the entire biomedical research community. PUBLIC HEALTH RELEVANCE This project aims to provide a complete and integrated picture of what every single gene in a human being does, thus allowing us to better understand the genetic and cellular workings of human health and disease. We do this by developing the Gene Ontology, a computational resource that collects biological knowledge into a large network structure that connects genes with the roles they play. Researchers, clinicians, and sophisticated computer programs use this network to interpret the massive amounts of biomedical and genomic data being generated in experiments and in studies designed to gain key insights into human health.",Gene Ontology Consortium,9930119,U41HG002273,"['Animal Model', 'Area', 'Automation', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Biomedical Research', 'Communities', 'Community Developments', 'Computer software', 'Computers', 'Data', 'Data Set', 'Disease', 'Documentation', 'Ensure', 'Environment', 'Family', 'Family member', 'Foundations', 'Gene Family', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Individual', 'Information Resources', 'Infrastructure', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Measurement', 'Medical Research', 'Medicine', 'Metabolism', 'Modeling', 'Modification', 'Mus', 'Network-based', 'Ontology', 'Organism', 'Orthologous Gene', 'Phenotype', 'Phylogenetic Analysis', 'Play', 'Process', 'Provider', 'PubMed', 'Readability', 'Recurrence', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Standardization', 'Structure', 'Technology', 'Testing', 'Training', 'Trees', 'Update', 'Work', 'Zebrafish', 'annotation  system', 'base', 'big biomedical data', 'biological research', 'biological systems', 'biomedical ontology', 'computer based Semantic Analysis', 'computer infrastructure', 'computer program', 'computing resources', 'data resource', 'design', 'experimental study', 'flexibility', 'gene function', 'genomic data', 'human disease', 'improved', 'insight', 'interest', 'interoperability', 'knowledge base', 'lipid metabolism', 'model organisms databases', 'molecular scale', 'ontology development', 'outreach', 'prevent', 'public health relevance', 'scale up', 'social media', 'software development', 'support network', 'text searching', 'tool', 'web services']",NHGRI,UNIVERSITY OF SOUTHERN CALIFORNIA,U41,2020,412500,0.16011774220137243
"Gene Ontology Consortium PROJECT SUMMARY Because of the staggering complexity of biological systems, biomedical research is becoming increasingly dependent on knowledge stored in a computable form. The Gene Ontology (GO) is by far the largest knowledgebase of how genes function, and has become a critical component of the computational infrastructure enabling the genomic revolution. It has become nearly indispensible in the interpretation of large- scale molecular measurements in biological research. Crucially, for human health research, GO is also one of a suite of complementary ontologies constructed in such as way to maximally promote interoperability and comparability of data sets. It represents the gene functions and biological processes that are perturbed in human disease, e.g. via the links from Human Phenotype Ontology (HPO) class abnormality of lipid metabolism, defined in relation to the GO class lipid metabolic process (GO_0006629), researchers or clinicians can find the set of genes that are known to be involved in this process. GO is a knowledge resource that can be statistically mined, either standalone or in combination with data from other knowledge resources, which enables experts to discover connections and form new hypotheses from the biological networks GO represents. All knowledge in GO is represented using semantic web technologies and so is amenable to computational integration and consistency checking. The proposed GO knowledge environment will enable a wider community of scientists to contribute to, and to utilize, a common, computable representation of biology. To ensure the knowledge environment meets the requirements of biomedical researchers, we will: a) deliver a comprehensive, detailed, computable knowledgebase of gene function, encoded in the Gene Ontology and annotations (computer-readable statements about the how specific genes function), focusing on human biology; b) provide a “hub” for a broad community of scientists to collaboratively extend, correct and improve the knowledgebase; c) ensure the GO knowledge resource is of the highest quality with regards to depth, breadth and accuracy; d) facilitate the transfer of insights obtained from studies of non-human organisms, such as the mouse and zebrafish, to human biology; and e) enable the scientific community to use the knowledgebase in analyses of large-scale genetic and -omics data. Our aims reflect the essential requirements for realizing the overarching objectives for a biomedical data resource: efficiently capturing and integrating biological knowledge and adhering to the highest possible standard for accuracy and detail; constructing and providing a robust, flexible, powerful, and extensible technological infrastructure available not only for internal use but just as easily by the wider community; and lastly, leveraging state-of-the-art social media, web services and other technologies to disseminate the GO resource to the entire biomedical research community. PUBLIC HEALTH RELEVANCE This project aims to provide a complete and integrated picture of what every single gene in a human being does, thus allowing us to better understand the genetic and cellular workings of human health and disease. We do this by developing the Gene Ontology, a computational resource that collects biological knowledge into a large network structure that connects genes with the roles they play. Researchers, clinicians, and sophisticated computer programs use this network to interpret the massive amounts of biomedical and genomic data being generated in experiments and in studies designed to gain key insights into human health.",Gene Ontology Consortium,9663327,U41HG002273,"['Animal Model', 'Area', 'Automation', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Biomedical Research', 'Communities', 'Community Developments', 'Computer software', 'Computers', 'Data', 'Data Set', 'Disease', 'Documentation', 'Ensure', 'Environment', 'Family', 'Family member', 'Foundations', 'Gene Family', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Individual', 'Information Resources', 'Infrastructure', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Measurement', 'Medical Research', 'Medicine', 'Metabolism', 'Modeling', 'Modification', 'Mus', 'Network-based', 'Ontology', 'Organism', 'Orthologous Gene', 'Phenotype', 'Phylogenetic Analysis', 'Play', 'Process', 'Provider', 'PubMed', 'Readability', 'Recurrence', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Standardization', 'Structure', 'Technology', 'Testing', 'Training', 'Trees', 'Update', 'Work', 'Zebrafish', 'annotation  system', 'base', 'big biomedical data', 'biological research', 'biological systems', 'biomedical ontology', 'computer based Semantic Analysis', 'computer infrastructure', 'computer program', 'computing resources', 'data resource', 'design', 'experimental study', 'flexibility', 'gene function', 'genomic data', 'human disease', 'improved', 'insight', 'interest', 'interoperability', 'knowledge base', 'lipid metabolism', 'model organisms databases', 'molecular scale', 'ontology development', 'outreach', 'prevent', 'public health relevance', 'scale up', 'social media', 'software development', 'support network', 'text searching', 'tool', 'web services']",NHGRI,UNIVERSITY OF SOUTHERN CALIFORNIA,U41,2019,2668358,0.16011774220137243
"Gene Ontology Consortium PROJECT SUMMARY Because of the staggering complexity of biological systems, biomedical research is becoming increasingly dependent on knowledge stored in a computable form. The Gene Ontology (GO) is by far the largest knowledgebase of how genes function, and has become a critical component of the computational infrastructure enabling the genomic revolution. It has become nearly indispensible in the interpretation of large- scale molecular measurements in biological research. Crucially, for human health research, GO is also one of a suite of complementary ontologies constructed in such as way to maximally promote interoperability and comparability of data sets. It represents the gene functions and biological processes that are perturbed in human disease, e.g. via the links from Human Phenotype Ontology (HPO) class abnormality of lipid metabolism, defined in relation to the GO class lipid metabolic process (GO_0006629), researchers or clinicians can find the set of genes that are known to be involved in this process. GO is a knowledge resource that can be statistically mined, either standalone or in combination with data from other knowledge resources, which enables experts to discover connections and form new hypotheses from the biological networks GO represents. All knowledge in GO is represented using semantic web technologies and so is amenable to computational integration and consistency checking. The proposed GO knowledge environment will enable a wider community of scientists to contribute to, and to utilize, a common, computable representation of biology. To ensure the knowledge environment meets the requirements of biomedical researchers, we will: a) deliver a comprehensive, detailed, computable knowledgebase of gene function, encoded in the Gene Ontology and annotations (computer-readable statements about the how specific genes function), focusing on human biology; b) provide a “hub” for a broad community of scientists to collaboratively extend, correct and improve the knowledgebase; c) ensure the GO knowledge resource is of the highest quality with regards to depth, breadth and accuracy; d) facilitate the transfer of insights obtained from studies of non-human organisms, such as the mouse and zebrafish, to human biology; and e) enable the scientific community to use the knowledgebase in analyses of large-scale genetic and -omics data. Our aims reflect the essential requirements for realizing the overarching objectives for a biomedical data resource: efficiently capturing and integrating biological knowledge and adhering to the highest possible standard for accuracy and detail; constructing and providing a robust, flexible, powerful, and extensible technological infrastructure available not only for internal use but just as easily by the wider community; and lastly, leveraging state-of-the-art social media, web services and other technologies to disseminate the GO resource to the entire biomedical research community. PUBLIC HEALTH RELEVANCE This project aims to provide a complete and integrated picture of what every single gene in a human being does, thus allowing us to better understand the genetic and cellular workings of human health and disease. We do this by developing the Gene Ontology, a computational resource that collects biological knowledge into a large network structure that connects genes with the roles they play. Researchers, clinicians, and sophisticated computer programs use this network to interpret the massive amounts of biomedical and genomic data being generated in experiments and in studies designed to gain key insights into human health.",Gene Ontology Consortium,9444478,U41HG002273,"['Animal Model', 'Area', 'Automation', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Biomedical Research', 'Communities', 'Community Developments', 'Computer software', 'Computers', 'Data', 'Data Set', 'Disease', 'Documentation', 'Ensure', 'Environment', 'Family', 'Family member', 'Foundations', 'Gene Family', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Individual', 'Information Resources', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Measurement', 'Medical Research', 'Medicine', 'Metabolism', 'Modeling', 'Modification', 'Mus', 'Network-based', 'Ontology', 'Organism', 'Orthologous Gene', 'Phenotype', 'Phylogenetic Analysis', 'Play', 'Process', 'Provider', 'PubMed', 'Readability', 'Recurrence', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Standardization', 'Structure', 'Technology', 'Testing', 'Training', 'Trees', 'Update', 'Work', 'Zebrafish', 'annotation  system', 'base', 'biological research', 'biological systems', 'biomedical ontology', 'computer based Semantic Analysis', 'computer infrastructure', 'computer program', 'computing resources', 'data resource', 'design', 'experimental study', 'flexibility', 'gene function', 'genomic data', 'human disease', 'improved', 'insight', 'interest', 'interoperability', 'knowledge base', 'lipid metabolism', 'model organisms databases', 'molecular scale', 'ontology development', 'outreach', 'prevent', 'public health relevance', 'scale up', 'social media', 'software development', 'support network', 'text searching', 'tool', 'web services']",NHGRI,UNIVERSITY OF SOUTHERN CALIFORNIA,U41,2018,2860911,0.16011774220137243
"Gene Ontology Consortium PROJECT SUMMARY Because of the staggering complexity of biological systems, biomedical research is becoming increasingly dependent on knowledge stored in a computable form. The Gene Ontology (GO) is by far the largest knowledgebase of how genes function, and has become a critical component of the computational infrastructure enabling the genomic revolution. It has become nearly indispensible in the interpretation of large- scale molecular measurements in biological research. Crucially, for human health research, GO is also one of a suite of complementary ontologies constructed in such as way to maximally promote interoperability and comparability of data sets. It represents the gene functions and biological processes that are perturbed in human disease, e.g. via the links from Human Phenotype Ontology (HPO) class abnormality of lipid metabolism, defined in relation to the GO class lipid metabolic process (GO_0006629), researchers or clinicians can find the set of genes that are known to be involved in this process. GO is a knowledge resource that can be statistically mined, either standalone or in combination with data from other knowledge resources, which enables experts to discover connections and form new hypotheses from the biological networks GO represents. All knowledge in GO is represented using semantic web technologies and so is amenable to computational integration and consistency checking. The proposed GO knowledge environment will enable a wider community of scientists to contribute to, and to utilize, a common, computable representation of biology. To ensure the knowledge environment meets the requirements of biomedical researchers, we will: a) deliver a comprehensive, detailed, computable knowledgebase of gene function, encoded in the Gene Ontology and annotations (computer-readable statements about the how specific genes function), focusing on human biology; b) provide a “hub” for a broad community of scientists to collaboratively extend, correct and improve the knowledgebase; c) ensure the GO knowledge resource is of the highest quality with regards to depth, breadth and accuracy; d) facilitate the transfer of insights obtained from studies of non-human organisms, such as the mouse and zebrafish, to human biology; and e) enable the scientific community to use the knowledgebase in analyses of large-scale genetic and -omics data. Our aims reflect the essential requirements for realizing the overarching objectives for a biomedical data resource: efficiently capturing and integrating biological knowledge and adhering to the highest possible standard for accuracy and detail; constructing and providing a robust, flexible, powerful, and extensible technological infrastructure available not only for internal use but just as easily by the wider community; and lastly, leveraging state-of-the-art social media, web services and other technologies to disseminate the GO resource to the entire biomedical research community. PUBLIC HEALTH RELEVANCE This project aims to provide a complete and integrated picture of what every single gene in a human being does, thus allowing us to better understand the genetic and cellular workings of human health and disease. We do this by developing the Gene Ontology, a computational resource that collects biological knowledge into a large network structure that connects genes with the roles they play. Researchers, clinicians, and sophisticated computer programs use this network to interpret the massive amounts of biomedical and genomic data being generated in experiments and in studies designed to gain key insights into human health.",Gene Ontology Consortium,9209989,U41HG002273,"['Animal Model', 'Area', 'Automation', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Biomedical Research', 'Communities', 'Community Developments', 'Computer software', 'Computers', 'Data', 'Data Set', 'Development', 'Disease', 'Documentation', 'Ensure', 'Environment', 'Family', 'Family member', 'Foundations', 'Gene Family', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Individual', 'Information Resources', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Measurement', 'Medical Research', 'Medicine', 'Metabolism', 'Modeling', 'Modification', 'Mus', 'Network-based', 'Ontology', 'Organism', 'Orthologous Gene', 'Phenotype', 'Phylogenetic Analysis', 'Play', 'Process', 'Provider', 'PubMed', 'Readability', 'Recurrence', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Standardization', 'Structure', 'Technology', 'Testing', 'Training', 'Trees', 'Update', 'Work', 'Zebrafish', 'annotation  system', 'base', 'biological research', 'biological systems', 'biomedical ontology', 'computer based Semantic Analysis', 'computer infrastructure', 'computer program', 'computing resources', 'data resource', 'design', 'experimental study', 'flexibility', 'gene function', 'genomic data', 'human disease', 'improved', 'insight', 'interest', 'interoperability', 'knowledge base', 'lipid metabolism', 'model organisms databases', 'molecular scale', 'outreach', 'prevent', 'public health relevance', 'scale up', 'social media', 'software development', 'support network', 'text searching', 'tool', 'web services']",NHGRI,UNIVERSITY OF SOUTHERN CALIFORNIA,U41,2017,3053465,0.16011774220137243
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7185305,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Biology, Other', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Computer information processing', 'Data', 'Databases', 'Depth', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'functional genomics', 'genetic element', 'genome database', 'human disease', 'interest', 'model organisms databases', 'repository', 'size', 'tool']",NHGRI,JACKSON LABORATORY,P41,2007,3146180,0.18590382236229352
"Predicting gene attributes from patterns of annotations. DESCRIPTION (provided by applicant): The Gene Ontology Consortium produces a controlled vocabulary for annotation of gene functions, which has been adopted by many organism-specific gene annotation databases. This allows the prediction of gene function based on partial annotation: if two attributes are strongly correlated in a database, then the presence of one attribute is evidence for the presence of the other. Recent ideas from machine learning, such as dependency networks, may allow more complicated interdependencies between genes and their attributes to be modeled efficiently, which should enable better predictions to be made. Cross-validation will be used to assess the performance of these models, in comparison with linear models and baseline models in which attributes are assumed to be independent. This approach will also be integrated with a probabilistic model of annotation-transfer based on sequence similarity. n/a",Predicting gene attributes from patterns of annotations.,6626288,F32HG002552,"['computer data analysis', ' computer graphics /printing', ' functional /structural genomics', ' genetic models', ' mathematical model', ' model design /development', ' molecular biology information system', ' postdoctoral investigator']",NHGRI,HARVARD UNIVERSITY (MEDICAL SCHOOL),F32,2003,46420,0.16267751010699513
"Predicting gene attributes from patterns of annotations. DESCRIPTION (provided by applicant): The Gene Ontology Consortium produces a controlled vocabulary for annotation of gene functions, which has been adopted by many organism-specific gene annotation databases. This allows the prediction of gene function based on partial annotation: if two attributes are strongly correlated in a database, then the presence of one attribute is evidence for the presence of the other. Recent ideas from machine learning, such as dependency networks, may allow more complicated interdependencies between genes and their attributes to be modeled efficiently, which should enable better predictions to be made. Cross-validation will be used to assess the performance of these models, in comparison with linear models and baseline models in which attributes are assumed to be independent. This approach will also be integrated with a probabilistic model of annotation-transfer based on sequence similarity. n/a",Predicting gene attributes from patterns of annotations.,6488035,F32HG002552,"['computer data analysis', ' computer graphics /printing', ' functional /structural genomics', ' genetic models', ' mathematical model', ' model design /development', ' molecular biology information system', ' postdoctoral investigator']",NHGRI,HARVARD UNIVERSITY (MEDICAL SCHOOL),F32,2002,38320,0.16267751010699513
"Development of a secure, cloud-based platform to improve record linkage & cross-agency collaboration for the public sector: using deep learning & scalable data integrations to combat the opioid crisis Project Summary/Abstract: This SBIR Phase I proposal aims to fund research and development for a new, multitenant secure cloud-based platform specifically tailored to provide local governmental agencies with tools to share datasets and link them accurately, at high quality and low cost. The OpenLattice platform will focus on reducing drug overdoses and making drug treatment less fractured. Individual-level datasets linked across medical providers and law enforcement can support analyses of prescribing pathways and treatment trajectories that precede opioid overdose, entry into treatment, disruption, and recovery. However, linking data at the individual level has proven to be a difficult and resource-intensive endeavor compared to use of aggregate-level data, with issues with deduplication plaguing many institutional databases. With 91 American deaths recorded daily from opioid overdoses and systems of care spread across multiple institutions, the need for greater and high-quality data sharing is undeniable. Our test partner for assessing the efficacy of proposed innovations is the Greater Portland Addiction Collaborative (GPAC) in Maine, a partnership of hospitals, a police department, jail, detox treatment centers and halfway houses already working together to reduce drug overdoses. This proposal aims to demonstrate proof of concept for (i) scaling high-quality data integrations across multiple governmental domains via a standardized entity data model, and (ii) improving record linkage using neural networks. Firstly, OpenLattice is developing an open source ontology and integration scripts to standardize integration of datasets into OpenLattice's database. As the individual customization requirements decline for onboarding customers and integrating new data into the platform, costs will be greatly slashed, removing a significant barrier to data solutions for smaller counties and cities across the country, who have historically faced custom integrations, system updates, data storage fees and add-ons at high cost. The OpenLattice platform also enables use of existing ETL tools and seamless integration with police dispatch systems, emergency medical calls, healthcare records, and online prescription systems across partners who have committed to data sharing and collaboration. Secondly, OpenLattice is developing a new, proprietary algorithm for record linkage that employs a promising but as-yet commercially untested technique: a multilayer perceptron neural network, more commonly known as deep learning. In pilot research, the linking algorithm has already demonstrated success rivaling—and sometimes exceeding—current state of the art linking technologies. In Phase I, OpenLattice will continue to improve ontologies, integration tools, and the deep learning neural network, and test on publicly available datasets with dissimilar data types and formats, with manual confirmation of results. When successful, these innovations will address critical barriers to improving clinical practice in treating opioid addiction by enabling a more comprehensive continuum of care for those in treatment. Project Narrative: Large-scale and coordinated responses to several of the US’s hot-button public health and criminal justice issues, such as the opioid epidemic and mass incarceration, are complicated by poor resource sharing and the US government’s highly fractured jurisdictional authority. This Small Business Innovation Research Phase I project aims to develop an efficient, scalable, cloud- based platform for hosting and linking highly sensitive state and local government databases at low cost, using (i) innovative data integration scripts and ontologies that standardize and scale capacity and (ii) technical advances in record de-duplication for linking databases. Data solutions would have tremendous societal impact on understandings of public health and the opioid epidemic by making drug treatment less fractured, saving lives and dramatically broadening contextual information, once data is broken out of silos.","Development of a secure, cloud-based platform to improve record linkage & cross-agency collaboration for the public sector: using deep learning & scalable data integrations to combat the opioid crisis",9622726,R43CE002937,[' '],NCIPC,"OPENLATTICE, INC.",R43,2018,225000,0.04727660071302797
"Computational Methods for Directed Functional Genomics DESCRIPTION (provided by applicant): Computational prediction of gene function (or phenotype) can reduce the scale of an experimental problem by focusing attention on a subset of possible experiments. Current function annotation databases-e.g., the Saccharomyces Genome Database (SGD) annotation of genes with Gene Ontology (GO) functions-are critically important resources, but were not designed to host computational predictions Although SGD and several other annotation databases label predictions as such, they provide no measures of confidence. The need exists for quantitative predictions, as distinct from qualitative ""somebody said so"" predictions. Probabilistic scoring systems, in which the score communicates the probability of veracity, are likely to be the most useful.  We will generate probabilistic predictions by developing probabilistic models for predicting function and phenotype. For reasons of data availability, we use S. cerevisiae and C. elegans as model systems.  We will also generate probabilistic models to predict protein and genetic interactions. We will exploit probabilistic networks of protein and genetic interaction in several ways. We will apply ideas from communication theory (2-terminal network reliability) to predict new members of protein complexes from probabilistic protein networks. We will develop computational methods to guide efficient discovery of genetic interactions in S. cerevisiae, as a model for guiding future high-throughput studies in metazoans. We will exploit probabilistic synthetic lethal interaction networks to identify drug mechanism of action.  We will disseminate predictions to the broader biomedical community. We propose a distributed quantitative prediction resource inspired by the DAS system of distributed genome annotation. We will adapt previously developed interfaces for browsing, searching, and retrieving probabilistic annotations to enhance their utility.  In Aim 1. we develop, apply, and validate methods for predicting function, phenotype, physical and genetic interaction in S. cerevisiae and C. elegans.  In Aim 2. we exploit probabilistic networks of protein and genetic interaction in S. cerevisiae to elucidate network structure, to guide functional genomic experiments, and to reveal drug mechanism of action.  In Aim, 3. we disseminate probabilistic predictions within a simple, generic, distributed software framework for sharing and browsing quantitative predictions. n/a",Computational Methods for Directed Functional Genomics,8099225,R01HG003224,"['Attention', 'Biological', 'Biological Models', 'Caenorhabditis elegans', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Databases', 'Dependency', 'Distributed Systems', 'Future', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Label', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Ontology', 'Phenotype', 'Probability', 'Proteins', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Source', 'Statistical Models', 'Structure', 'System', 'communication theory', 'design', 'drug mechanism', 'drug sensitivity', 'functional genomics', 'gene function', 'genome database', 'member', 'protein complex', 'research study', 'web based interface']",NHGRI,HARVARD MEDICAL SCHOOL,R01,2010,169176,0.12206358325526807
"Computational Methods for Directed Functional Genomics DESCRIPTION (provided by applicant): Computational prediction of gene function (or phenotype) can reduce the scale of an experimental problem by focusing attention on a subset of possible experiments. Current function annotation databases-e.g., the Saccharomyces Genome Database (SGD) annotation of genes with Gene Ontology (GO) functions-are critically important resources, but were not designed to host computational predictions Although SGD and several other annotation databases label predictions as such, they provide no measures of confidence. The need exists for quantitative predictions, as distinct from qualitative ""somebody said so"" predictions. Probabilistic scoring systems, in which the score communicates the probability of veracity, are likely to be the most useful.  We will generate probabilistic predictions by developing probabilistic models for predicting function and phenotype. For reasons of data availability, we use S. cerevisiae and C. elegans as model systems.  We will also generate probabilistic models to predict protein and genetic interactions. We will exploit probabilistic networks of protein and genetic interaction in several ways. We will apply ideas from communication theory (2-terminal network reliability) to predict new members of protein complexes from probabilistic protein networks. We will develop computational methods to guide efficient discovery of genetic interactions in S. cerevisiae, as a model for guiding future high-throughput studies in metazoans. We will exploit probabilistic synthetic lethal interaction networks to identify drug mechanism of action.  We will disseminate predictions to the broader biomedical community. We propose a distributed quantitative prediction resource inspired by the DAS system of distributed genome annotation. We will adapt previously developed interfaces for browsing, searching, and retrieving probabilistic annotations to enhance their utility.  In Aim 1. we develop, apply, and validate methods for predicting function, phenotype, physical and genetic interaction in S. cerevisiae and C. elegans.  In Aim 2. we exploit probabilistic networks of protein and genetic interaction in S. cerevisiae to elucidate network structure, to guide functional genomic experiments, and to reveal drug mechanism of action.  In Aim, 3. we disseminate probabilistic predictions within a simple, generic, distributed software framework for sharing and browsing quantitative predictions. n/a",Computational Methods for Directed Functional Genomics,7679365,R01HG003224,"['Attention', 'Biological', 'Biological Models', 'Caenorhabditis elegans', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Databases', 'Dependency', 'Distributed Systems', 'Future', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Label', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Ontology', 'Phenotype', 'Probability', 'Proteins', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Source', 'Statistical Models', 'Structure', 'System', 'communication theory', 'design', 'drug mechanism', 'drug sensitivity', 'functional genomics', 'gene function', 'genome database', 'member', 'protein complex', 'research study', 'web based interface']",NHGRI,HARVARD MEDICAL SCHOOL,R01,2009,365593,0.12206358325526807
"Computational Methods for Directed Functional Genomics DESCRIPTION (provided by applicant): Computational prediction of gene function (or phenotype) can reduce the scale of an experimental problem by focusing attention on a subset of possible experiments. Current function annotation databases-e.g., the Saccharomyces Genome Database (SGD) annotation of genes with Gene Ontology (GO) functions-are critically important resources, but were not designed to host computational predictions Although SGD and several other annotation databases label predictions as such, they provide no measures of confidence. The need exists for quantitative predictions, as distinct from qualitative ""somebody said so"" predictions. Probabilistic scoring systems, in which the score communicates the probability of veracity, are likely to be the most useful.  We will generate probabilistic predictions by developing probabilistic models for predicting function and phenotype. For reasons of data availability, we use S. cerevisiae and C. elegans as model systems.  We will also generate probabilistic models to predict protein and genetic interactions. We will exploit probabilistic networks of protein and genetic interaction in several ways. We will apply ideas from communication theory (2-terminal network reliability) to predict new members of protein complexes from probabilistic protein networks. We will develop computational methods to guide efficient discovery of genetic interactions in S. cerevisiae, as a model for guiding future high-throughput studies in metazoans. We will exploit probabilistic synthetic lethal interaction networks to identify drug mechanism of action.  We will disseminate predictions to the broader biomedical community. We propose a distributed quantitative prediction resource inspired by the DAS system of distributed genome annotation. We will adapt previously developed interfaces for browsing, searching, and retrieving probabilistic annotations to enhance their utility.  In Aim 1. we develop, apply, and validate methods for predicting function, phenotype, physical and genetic interaction in S. cerevisiae and C. elegans.  In Aim 2. we exploit probabilistic networks of protein and genetic interaction in S. cerevisiae to elucidate network structure, to guide functional genomic experiments, and to reveal drug mechanism of action.  In Aim, 3. we disseminate probabilistic predictions within a simple, generic, distributed software framework for sharing and browsing quantitative predictions. n/a",Computational Methods for Directed Functional Genomics,7487999,R01HG003224,"['Attention', 'Biological', 'Biological Models', 'Caenorhabditis elegans', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Databases', 'Dependency', 'Distributed Systems', 'Future', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Label', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Ontology', 'Phenotype', 'Probability', 'Proteins', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Score', 'Source', 'Statistical Models', 'Structure', 'System', 'communication theory', 'design', 'drug mechanism', 'drug sensitivity', 'functional genomics', 'gene function', 'genome database', 'member', 'research study', 'web based interface']",NHGRI,HARVARD MEDICAL SCHOOL,R01,2008,414441,0.12206358325526807
"Computational Methods for Directed Functional Genomics DESCRIPTION (provided by applicant): Computational prediction of gene function (or phenotype) can reduce the scale of an experimental problem by focusing attention on a subset of possible experiments. Current function annotation databases-e.g., the Saccharomyces Genome Database (SGD) annotation of genes with Gene Ontology (GO) functions-are critically important resources, but were not designed to host computational predictions Although SGD and several other annotation databases label predictions as such, they provide no measures of confidence. The need exists for quantitative predictions, as distinct from qualitative ""somebody said so"" predictions. Probabilistic scoring systems, in which the score communicates the probability of veracity, are likely to be the most useful.  We will generate probabilistic predictions by developing probabilistic models for predicting function and phenotype. For reasons of data availability, we use S. cerevisiae and C. elegans as model systems.  We will also generate probabilistic models to predict protein and genetic interactions. We will exploit probabilistic networks of protein and genetic interaction in several ways. We will apply ideas from communication theory (2-terminal network reliability) to predict new members of protein complexes from probabilistic protein networks. We will develop computational methods to guide efficient discovery of genetic interactions in S. cerevisiae, as a model for guiding future high-throughput studies in metazoans. We will exploit probabilistic synthetic lethal interaction networks to identify drug mechanism of action.  We will disseminate predictions to the broader biomedical community. We propose a distributed quantitative prediction resource inspired by the DAS system of distributed genome annotation. We will adapt previously developed interfaces for browsing, searching, and retrieving probabilistic annotations to enhance their utility.  In Aim 1. we develop, apply, and validate methods for predicting function, phenotype, physical and genetic interaction in S. cerevisiae and C. elegans.  In Aim 2. we exploit probabilistic networks of protein and genetic interaction in S. cerevisiae to elucidate network structure, to guide functional genomic experiments, and to reveal drug mechanism of action.  In Aim, 3. we disseminate probabilistic predictions within a simple, generic, distributed software framework for sharing and browsing quantitative predictions. n/a",Computational Methods for Directed Functional Genomics,7283832,R01HG003224,"['Attention', 'Biological', 'Biological Models', 'Caenorhabditis elegans', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Databases', 'Dependency', 'Distributed Systems', 'Future', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Label', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Ontology', 'Phenotype', 'Probability', 'Proteins', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Score', 'Source', 'Statistical Models', 'Structure', 'System', 'communication theory', 'design', 'drug mechanism', 'drug sensitivity', 'functional genomics', 'gene function', 'genome database', 'member', 'research study', 'web based interface']",NHGRI,HARVARD UNIVERSITY (MEDICAL SCHOOL),R01,2007,402371,0.12206358325526807
"Biomedical Ontology and Tools for Database Curation DESCRIPTION (provided by applicant): This proposal describes a new tool for text data mining-a biomedical language ontology and integrated natural-language-processing methods. Our long-term goal is to provide resources for biomedical knowledge discovery from text. Our immediate goal is to provide a knowledge discovery tool for the curation of organism databases such as the Genome Database (SGD). The proposed research not only serves the research needs of the SGD community, it also helps the broader biomedical community exploit the strengths of the comparative approach to biological research. The hypothesis of this proposal is that knowledge discovery from biomedical text requires a knowledge base that integrates both genomic and linguistic information. This hypothesis is based on two observations: (a) the language of biomedicine, like all natural language, is complex in structure and morphology (the basic units of meaning) and poses problems of synonymy (several terms having the same meaning), polysemy (a term having more than one meaning), hypernymy (one term being more general than another), hyponymy (one term being more specific than another), denotation (what a term refers to in contrast to what it means), and denotation and description (different ways of referring to the same thing); and (b) important biomedical knowledge sources, such as the Gene Ontology (GO), are expressed in natural language. The specific aims of the proposed project are to: 1. Extend an existing biomedical language ontology to include genomic and linguistic data from SGD; 2. Use this ontology to discover, in full-text articles made available by SGD, information about the molecular function of yeast gene products that can be inferred from direct experimental assays; 3. Evaluate the effectiveness of the new tool and methods by comparing its results to those of the SGD curators for gene products that have GO functional annotations with evidence code IDA (Inferred from Direct Assay). n/a",Biomedical Ontology and Tools for Database Curation,6885487,R43HG003600,"['computer program /software', 'computer system design /evaluation', 'fungal genetics', 'information retrieval', 'information system analysis', 'molecular biology information system', 'yeasts']",NHGRI,"CONVERSPEECH, LLC",R43,2005,99250,0.12082625485068271
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8541935,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2012,100000,0.13543006714526856
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8737919,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2014,468122,0.13543006714526856
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8541872,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2013,330290,0.13543006714526856
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8330927,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2012,246312,0.13543006714526856
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7910730,P41HG004059,[' '],NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2010,1093220,0.07185552300276828
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7669241,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,829379,0.07185552300276828
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7921192,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,250001,0.07185552300276828
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7293650,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CAN RES CTR,P41,2007,796910,0.07185552300276828
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8515555,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2012,290837,0.10941332376933499
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8034342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2011,332732,0.10941332376933499
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,7772342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2010,326303,0.10941332376933499
"Textpresso: information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso: information retrieval and extraction system for biological literature,7583249,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Body of uterus', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2009,320000,0.10941332376933499
"Extension of the Sequence Ontology:  Preparing for the (re) sequencing revolution    DESCRIPTION (provided by applicant): High throughput sequencing technologies have made possible both personal human genome sequencing and rapid re-sequencing of many organisms. The dramatic increase in the throughputs of these technologies demands equal progress in the technologies used to manage their outputs. Over the last decade ontologies have emerged as indispensible tools for the management of large biomedical datasets. The Sequence Ontology (SO) is world's most widely used ontology for describing sequence annotations. The advent of rapid genome re-sequencing has made it essential that SO also provide the means to describe sequence variants. This renewal submission thus has two broad goals: (1) extend SO into the realm of genomic variant annotation, and (2) harmonize SO with recent developments in the field of biomedical ontology and genomics. Both are essential if we are to meet the data management needs of researchers seeking to exchange, compare and analyze re-sequenced genomes and their variants in the context of existing gene annotations.        The gigantic datasets produced by personal human genome sequencing present daunting challenges for data management. This proposal seeks funds to extend tools for describing genome annotations, into the realm of sequence variation. Doing so will facilitate exchange, comparisons and analyses of re-sequenced genomes.         ",Extension of the Sequence Ontology:  Preparing for the (re) sequencing revolution,8462288,R01HG004341,"['Adoption', 'Animal Model', 'Biological', 'Biology', 'Computer software', 'Data', 'Data Set', 'Development', 'Ensure', 'Eye', 'Funding', 'Genes', 'Genome', 'Genomics', 'Goals', 'Human Genome', 'Information Management', 'Ontology', 'Organism', 'Output', 'Process', 'Property', 'Publications', 'Publishing', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Structure', 'Techniques', 'Technology', 'Terminology', 'Time', 'United States National Institutes of Health', 'Validation', 'Variant', 'Work', 'base', 'biomedical informatics', 'biomedical ontology', 'data management', 'file format', 'genome annotation', 'genome sequencing', 'information organization', 'meetings', 'novel', 'success', 'tool']",NHGRI,UNIVERSITY OF UTAH,R01,2013,279985,0.07651021990001339
"Extension of the Sequence Ontology:  Preparing for the (re) sequencing revolution    DESCRIPTION (provided by applicant): High throughput sequencing technologies have made possible both personal human genome sequencing and rapid re-sequencing of many organisms. The dramatic increase in the throughputs of these technologies demands equal progress in the technologies used to manage their outputs. Over the last decade ontologies have emerged as indispensible tools for the management of large biomedical datasets. The Sequence Ontology (SO) is world's most widely used ontology for describing sequence annotations. The advent of rapid genome re-sequencing has made it essential that SO also provide the means to describe sequence variants. This renewal submission thus has two broad goals: (1) extend SO into the realm of genomic variant annotation, and (2) harmonize SO with recent developments in the field of biomedical ontology and genomics. Both are essential if we are to meet the data management needs of researchers seeking to exchange, compare and analyze re-sequenced genomes and their variants in the context of existing gene annotations.        The gigantic datasets produced by personal human genome sequencing present daunting challenges for data management. This proposal seeks funds to extend tools for describing genome annotations, into the realm of sequence variation. Doing so will facilitate exchange, comparisons and analyses of re-sequenced genomes.         ",Extension of the Sequence Ontology:  Preparing for the (re) sequencing revolution,8309860,R01HG004341,"['Adoption', 'Animal Model', 'Biological', 'Biology', 'Computer software', 'Data', 'Data Set', 'Development', 'Ensure', 'Eye', 'Funding', 'Genes', 'Genome', 'Genomics', 'Goals', 'Human Genome', 'Information Management', 'Ontology', 'Organism', 'Output', 'Process', 'Property', 'Publications', 'Publishing', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Structure', 'Techniques', 'Technology', 'Terminology', 'Time', 'United States National Institutes of Health', 'Validation', 'Variant', 'Work', 'base', 'biomedical informatics', 'biomedical ontology', 'data management', 'file format', 'genome sequencing', 'information organization', 'meetings', 'novel', 'success', 'tool']",NHGRI,UNIVERSITY OF UTAH,R01,2012,294487,0.07651021990001339
"Extension of the Sequence Ontology:  Preparing for the (re) sequencing revolution    DESCRIPTION (provided by applicant): High throughput sequencing technologies have made possible both personal human genome sequencing and rapid re-sequencing of many organisms. The dramatic increase in the throughputs of these technologies demands equal progress in the technologies used to manage their outputs. Over the last decade ontologies have emerged as indispensible tools for the management of large biomedical datasets. The Sequence Ontology (SO) is world's most widely used ontology for describing sequence annotations. The advent of rapid genome re-sequencing has made it essential that SO also provide the means to describe sequence variants. This renewal submission thus has two broad goals: (1) extend SO into the realm of genomic variant annotation, and (2) harmonize SO with recent developments in the field of biomedical ontology and genomics. Both are essential if we are to meet the data management needs of researchers seeking to exchange, compare and analyze re-sequenced genomes and their variants in the context of existing gene annotations.      PUBLIC HEALTH RELEVANCE: The gigantic datasets produced by personal human genome sequencing present daunting challenges for data management. This proposal seeks funds to extend tools for describing genome annotations, into the realm of sequence variation. Doing so will facilitate exchange, comparisons and analyses of re-sequenced genomes.           The gigantic datasets produced by personal human genome sequencing present daunting challenges for data management. This proposal seeks funds to extend tools for describing genome annotations, into the realm of sequence variation. Doing so will facilitate exchange, comparisons and analyses of re-sequenced genomes.         ",Extension of the Sequence Ontology:  Preparing for the (re) sequencing revolution,8148118,R01HG004341,"['Adoption', 'Animal Model', 'Biological', 'Biology', 'Computer software', 'Data', 'Data Set', 'Development', 'Ensure', 'Eye', 'Funding', 'Genes', 'Genome', 'Genomics', 'Goals', 'Human Genome', 'Information Management', 'Ontology', 'Organism', 'Output', 'Process', 'Property', 'Publications', 'Publishing', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Structure', 'Techniques', 'Technology', 'Terminology', 'Time', 'United States National Institutes of Health', 'Validation', 'Variant', 'Work', 'base', 'biomedical informatics', 'biomedical ontology', 'data management', 'file format', 'genome sequencing', 'information organization', 'meetings', 'novel', 'success', 'tool']",NHGRI,UNIVERSITY OF UTAH,R01,2011,300000,0.06853474099084893
"Automated Integration of Biomedical Knowledge Today, ontologies are critical instruments for biomedical investigators, especially in those areas, such as cancer research, that require the command of a vast amount of information and a systemic approach to the design and interpretation of experiments. In fact, ontologies are proliferating in all areas of biomedical research, offering both challenges and opportunities. One of the principal challenges of this field stems from the fact that ontologies are developed in isolation, rendering it impossible to move, for instance, from genes to organisms, to diseases, to drugs. The National Center for Biomedical Ontology (NCBO) represents a fundamental endeavor in the collection, coordination and distribution of biomedical ontologies and offers an unparalleled opportunity to combine these biomedical ontologies into a single search space where genetic, anatomic, molecular and pharmacological information can be seamlessly explored and exploited as a holistic representation of biomedical knowledge. Unfortunately, ontology integration using standard means of manual curation is a labor intensive task, unable to scale up and keep up with the current growth rate of biomedical ontologies. We have developed a systematic framework for automated ontology engineering based on information theory, and we have successfully applied it to the analysis and engineering of Gene Ontology (GO), the development gene and protein databases, and the identification of peripheral biomarkers of disease progression and drug response. This project brings together a unique group of competences, ranging from ontology engineering, statistical signal processing, bioinformatics, cancer research, and clinical pharmacogenomics, to develop a principled method, grounded on the mathematics of information theory, to automatically combine and integrate biomedical ontologies and implement it as part of the NCBO architecture Ontologies are critical instruments for biomedical investigators especially in those areas, such as cancer research, that require a vast amount of information and a systemic approach to the design and interpretation of their experiments. In collaboration with the National Center for Biomedical Ontology (NCBO), this project will develop a principled method, grounded on the mathematics of information theory, to automatically combine biomedical ontologies. As a result, this project will integrate biomedical knowledge along dimensions that are today isolated and, in so doing, it will empower investigators with a new holistic understanding of disease, it will fast track the clinical  translation of biological discoveries, and it will change the approach to discovery, especially for those diseases that, like cancer, require a systemic view of their biological mechanisms.",Automated Integration of Biomedical Knowledge,7558468,R01HG004836,"['Anatomy', 'Architecture', 'Area', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Research', 'Classification', 'Clinical', 'Collaborations', 'Collection', 'Colorectal Cancer', 'Competence', 'Complex', 'Development', 'Dimensions', 'Disease', 'Disease Progression', 'Engineered Gene', 'Engineering', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Growth', 'Human', 'Information Theory', 'Internet', 'Java', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Mathematics', 'Methods', 'Molecular', 'Ontology', 'Organism', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Proliferating', 'Protein Databases', 'Research Infrastructure', 'Research Personnel', 'Services', 'Side', 'Structure', 'Testing', 'Text', 'Tissues', 'Translations', 'anticancer research', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'computerized data processing', 'design', 'empowered', 'graphical user interface', 'insight', 'instrument', 'open source', 'programs', 'repository', 'research study', 'response', 'scale up', 'statistics', 'stem']",NHGRI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2009,428078,0.17849935908755787
"Automated Integration of Biomedical Knowledge Today, ontologies are critical instruments for biomedical investigators, especially in those areas, such as cancer research, that require the command of a vast amount of information and a systemic approach to the design and interpretation of experiments. In fact, ontologies are proliferating in all areas of biomedical research, offering both challenges and opportunities. One of the principal challenges of this field stems from the fact that ontologies are developed in isolation, rendering it impossible to move, for instance, from genes to organisms, to diseases, to drugs. The National Center for Biomedical Ontology (NCBO) represents a fundamental endeavor in the collection, coordination and distribution of biomedical ontologies and offers an unparalleled opportunity to combine these biomedical ontologies into a single search space where genetic, anatomic, molecular and pharmacological information can be seamlessly explored and exploited as a holistic representation of biomedical knowledge. Unfortunately, ontology integration using standard means of manual curation is a labor intensive task, unable to scale up and keep up with the current growth rate of biomedical ontologies. We have developed a systematic framework for automated ontology engineering based on information theory, and we have successfully applied it to the analysis and engineering of Gene Ontology (GO), the development gene and protein databases, and the identification of peripheral biomarkers of disease progression and drug response. This project brings together a unique group of competences, ranging from ontology engineering, statistical signal processing, bioinformatics, cancer research, and clinical pharmacogenomics, to develop a principled method, grounded on the mathematics of information theory, to automatically combine and integrate biomedical ontologies and implement it as part of the NCBO architecture Ontologies are critical instruments for biomedical investigators especially in those areas, such as cancer research, that require a vast amount of information and a systemic approach to the design and interpretation of their experiments. In collaboration with the National Center for Biomedical Ontology (NCBO), this project will develop a principled method, grounded on the mathematics of information theory, to automatically combine biomedical ontologies. As a result, this project will integrate biomedical knowledge along dimensions that are today isolated and, in so doing, it will empower investigators with a new holistic understanding of disease, it will fast track the clinical  translation of biological discoveries, and it will change the approach to discovery, especially for those diseases that, like cancer, require a systemic view of their biological mechanisms.",Automated Integration of Biomedical Knowledge,7945368,R01HG004836,"['Anatomy', 'Area', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Research', 'Clinical', 'Collaborations', 'Collection', 'Colorectal Cancer', 'Communities', 'Competence', 'Complex', 'Computer software', 'Consultations', 'Controlled Vocabulary', 'Dana-Farber Cancer Institute', 'Data', 'Databases', 'Development', 'Dimensions', 'Disease', 'Disease Progression', 'Engineered Gene', 'Engineering', 'Fostering', 'Future', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Growth', 'Human', 'In Vitro', 'Information Theory', 'Internet', 'Java', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Mathematics', 'Methodology', 'Methods', 'Molecular', 'National Cancer Institute', 'Nature', 'Ontology', 'Organism', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Proliferating', 'Protein Databases', 'Research Infrastructure', 'Research Personnel', 'Services', 'Structure', 'Testing', 'Text', 'Tissues', 'Translations', 'Validation', 'anticancer research', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'design', 'empowered', 'gene function', 'graphical user interface', 'information organization', 'instrument', 'interoperability', 'novel', 'open source', 'repository', 'research study', 'response', 'scale up', 'sound', 'statistics', 'stem']",NHGRI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2010,428079,0.17849935908755787
"UniProt: A Centralized Protein Sequence and Function Resource    DESCRIPTION (provided by applicant): The specific aim of this UniProt Consortium is to provide a centralized protein sequence and function resource by enhancing the UniProt Knowledgebase (UniProtKB) and ensuring that the diverse information in UniProt is of use to a broad scientific user community by exploiting a range of dissemination strategies. The UniProtKB will include a variety of data types including, but not limited to, protein sequences, nomenclature, family classifications, and alternatively-spliced and modified forms. Relevant information on protein function will be included with potential protein interactions, expression patterns, pathways and controlled vocabularies of Gene Ontology (GO terms). Annotation methods applied in the UniProtKB will include extraction of information from the literature and computational analyses, as well as integrating and mining large-scale data sets. The types of evidence and methods of annotation for both experimental and computational data along with attribution of the source will be included. The UniProtKB will rely on high interoperability with other databases, while exploiting novel approaches to encourage community curation. To facilitate the use of UniProt, the UniProt Consortium will enhance its existing user-friendly interfaces and tools to allow for simple and complex queries and for retrieval of large datasets. Database records will be down-loadable in defined, parsable format. An efficient and responsive user support service will be provided. Finally, the UniProt Consortium will exert the flexibility and adaptability needed to respond to changing needs of the scientific community. The broad, long-term objectives of this project are:  To provide the scientific community with the Universal Protein Resource (UniProt) as a comprehensive, high-quality and freely accessible resource of protein sequence and functional information.  To enable scientists to identify and analyze products of protein-coding genes by making text- and sequence-based queries in the UniProt databases.  To provide efficient and unencumbered access to the databases produced by the UniProt Consortium.       RELEVANCE: The databases produced by the UniProt Consortium will provide researchers with an integrated access to protein sequence and function by gathering and enriching data from genomics and proteomics projects as well as the results published by individual researchers. This is a crucial step in making genomics and proteomics research results easily accessible to support biomedical research in academia and industry and hence facilitate the development of preventive and curative strategies for human health.           n/a",UniProt: A Centralized Protein Sequence and Function Resource,8252773,U41HG006104,"['Academia', 'Amino Acid Sequence', 'Base Sequence', 'Biological', 'Biomedical Research', 'Classification', 'Code', 'Communities', 'Complex', 'Consultations', 'Controlled Vocabulary', 'Data', 'Data Files', 'Data Set', 'Databases', 'Development', 'Documentation', 'Electronic Mail', 'Ensure', 'Extensible Markup Language', 'Family', 'Feedback', 'Genes', 'Genomics', 'Health', 'Human', 'Individual', 'Industry', 'Literature', 'Methods', 'Mining', 'Modification', 'National Human Genome Research Institute', 'Nomenclature', 'Ontology', 'Pathway interactions', 'Pattern', 'Peptide Sequence Determination', 'Preventive', 'Principal Investigator', 'Process', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'Publishing', 'RNA Splicing', 'Records', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Scientist', 'Services', 'Source', 'Structure', 'Text', 'Time', 'Training', 'Translating', 'Variant', 'Work', 'database design', 'flexibility', 'interest', 'interoperability', 'knowledge base', 'meetings', 'novel strategies', 'programs', 'protein function', 'tool', 'user-friendly', 'web site']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U41,2011,76164,0.05816474058209453
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,7069599,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2006,398762,0.05121572285517942
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6896406,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2005,403171,0.05121572285517942
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6774132,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2004,411436,0.05121572285517942
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6744998,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2003,191306,0.03763632933126951
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6363593,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2001,306158,0.03763632933126951
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6165092,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2000,297119,0.03763632933126951
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,2744854,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,1999,343110,0.03763632933126951
"A synthetic biology library for the Semantic Web    DESCRIPTION (provided by applicant): The long-term aims of this project are to improve the engineering practices of synthetic biologists, and thereby to hasten the pace of synthetic biology research. Synthetic biology is poised to make great contributions to health, medicine, energy, and the environment by building systems ranging from the production of anti-malarial drugs, to improved antibiotics, to tumor-tracking bacteria, to engineering new carbon-neutral energy sources, and to organisms that clean up environmental toxins and fight pollution. Our research approach is to combine well-established engineering principles with semantic web methods and technologies. More specifically, we will develop tools that support the engineering principles of modularity, reuse, and version control for the configuration of complete biological systems. The semantic web technologies that we apply to this domain include ontology development, the formal specification of semantics, and an OWL reasoning system to carry out validation, intelligent information retrieval, and configuration tasks. The specific goal of the proposed research is develop a prototype tool for the management of information throughout the synthetic biology research lifecycle. The capabilities of this tool will include version control, intelligent information retrieval about synthetic biology components, and configuration management to assist in assembling those components into working biological systems. We will design, develop, and test this tool in direct collaboration with working synthetic biologists from Dr. Sauro's laboratory. We divide our research work into three specific aims: (1) analyzing the work of synthetic biologists and extending our ontology to include configuration constraints and versioning knowledge for synthetic biology components; (2) developing version management capabilities for synthetic biologists; and (3) developing configuration and parts retrieval capabilities for synthetic biologists. To support both collaborative work and reuse of components, our tool and our research leverages the idea of an annotated library of synthetic biology components. Our work will use existing libraries, and our tool will add to these libraries through its use. To support the widest range of system reuse, our tool will be based on public, semantic web standards and will be developed as an open, modern web application. We envision an enterprise-wide use of our tool, which will allow for better communication, improved design, and better engineering of synthetic biological systems. In turn, these engineering improvements will help hasten the wide-scale industrial production, adoption, and use of synthetic biology constructs, leading to new advances in bioengineered therapeutic, energy, and food technologies.       The public health relevance of the proposed research is that improved engineering for the  synthetic biology enterprise will improve the quality and efficiency with which synthetic biological  constructs can be designed and built. These improvements will enable new scientific advances  that directly benefit the public-advances such as bioengineered therapies, improved antibiotics,  and inexpensive production of pharmaceutical agents.",A synthetic biology library for the Semantic Web,8334022,R42HG006737,"['Adoption', 'Antibiotics', 'Antimalarials', 'Bacteria', 'Biological', 'Biomedical Engineering', 'Cells', 'Characteristics', 'Collaborations', 'Communication', 'Complex', 'Development', 'Engineering', 'Food Technology', 'Future', 'Goals', 'Health', 'Imagination', 'Information Management', 'Information Retrieval', 'Knowledge', 'Laboratories', 'Libraries', 'Life', 'Logic', 'Methods', 'Ontology', 'Organism', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Process', 'Production', 'Public Health', 'Research', 'Research Personnel', 'Retrieval', 'Scientific Advances and Accomplishments', 'Scientist', 'Semantics', 'Software Engineering', 'Staging', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Validation', 'Work', 'base', 'biological systems', 'computer based Semantic Analysis', 'coping', 'design', 'design and construction', 'improved', 'information organization', 'innovation', 'novel', 'prototype', 'public health relevance', 'software development', 'synthetic biology', 'tool', 'tumor']",NHGRI,"CLARK AND PARSIA, LLC",R42,2012,379537,0.04619546115564443
"A synthetic biology library for the Semantic Web The long-term aims of this project are to improve the engineering practices of synthetic biologists, and thereby to hasten the pace of synthetic biology research. Synthetic biology is poised to make great contributions public health, by building systems designed to improve health by diverse means - from the production of anti-malarial drugs, to improved antibiotics, to tumor-tracking bacteria. Our research approach is to combine well-established engineering principles with semantic web methods and technologies. More specifically, we will develop tools that support the engineering principles of modularity, reuse, and version control for the configuration of complete biological systems. The semantic web technologies that we apply to this domain include ontology development, the formal specification of semantics, and an OWL reasoning system to carry out validation, intelligent information retrieval, and configuration tasks. The specific goal of the proposed research is develop a prototype tool for the management of information throughout the synthetic biology research lifecycle. The capabilities of this tool will include version control, intelligent information retrieval about synthetic biology components, and configuration management to assist in assembling those components into working biological systems. We will design, develop, and test this tool in direct collaboration with working synthetic biologists from Dr. Sauro's laboratory. In more detail, we divide our research work into three specific aims (1) analyzing the work of synthetic biologists and extending our ontology to include configuration constraints and versioning knowledge for synthetic biology components (2) developing version management capabilities for synthetic biologists, and (3) developing configuration and parts retrieval capabilities for synthetic biologists. To support both collaborative work, and reuse of components, our tool and our research leverages the idea of an annotated library of synthetic biology components. Our work will both use existing libraries, and our tool will add to these libraries through its use. We envision an enterprise-wide use of our tool, which will allow for better communication, improved design, and better engineering of synthetic biological systems. In turn, these engineering improvements will help hasten the wide-scale industrial production, adoption and use of synthetic biology constructs, leading to new advances in bioengineered therapeutic, energy, and food technologies. The public health relevance of the proposed research is that improved engineering for the  synthetic biology enterprise will improve the quality and efficiency with which synthetic biological  constructs can be designed and built. These improvements will enable new scientific advances  that directly benefit the public-advances such as bioengineered therapies, improved antibiotics,  and inexpensive production of pharmaceutical agents.",A synthetic biology library for the Semantic Web,8199523,R42HG006737,"['Adoption', 'Antibiotics', 'Antimalarials', 'Bacteria', 'Biological', 'Biology', 'Biomedical Engineering', 'Carbon', 'Cells', 'Characteristics', 'Collaborations', 'Communication', 'Complex', 'Development', 'Energy-Generating Resources', 'Engineering', 'Environment', 'Food Technology', 'Future', 'Goals', 'Health', 'Imagination', 'Information Management', 'Information Retrieval', 'Internet', 'Knowledge', 'Laboratories', 'Libraries', 'Life', 'Logic', 'Medicine', 'Methods', 'Ontology', 'Organism', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Pollution', 'Process', 'Production', 'Research', 'Research Personnel', 'Retrieval', 'Scientific Advances and Accomplishments', 'Scientist', 'Semantics', 'Software Engineering', 'Staging', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Toxic Environmental Substances', 'Validation', 'Work', 'base', 'biological systems', 'computer based Semantic Analysis', 'coping', 'design', 'design and construction', 'fighting', 'improved', 'information organization', 'innovation', 'novel', 'prototype', 'public health relevance', 'software development', 'synthetic biology', 'tool', 'tumor']",NHGRI,"CLARK AND PARSIA, LLC",R42,2011,299537,0.0491181015865115
"A synthetic biology library for the Semantic Web    DESCRIPTION (provided by applicant): The long-term aims of this project are to improve the engineering practices of synthetic biologists, and thereby to hasten the pace of synthetic biology research. Synthetic biology is poised to make great contributions to health, medicine, energy, and the environment by building systems ranging from the production of anti-malarial drugs, to improved antibiotics, to tumor-tracking bacteria, to engineering new carbon-neutral energy sources, and to organisms that clean up environmental toxins and fight pollution. Our research approach is to combine well-established engineering principles with semantic web methods and technologies. More specifically, we will develop tools that support the engineering principles of modularity, reuse, and version control for the configuration of complete biological systems. The semantic web technologies that we apply to this domain include ontology development, the formal specification of semantics, and an OWL reasoning system to carry out validation, intelligent information retrieval, and configuration tasks. The specific goal of the proposed research is develop a prototype tool for the management of information throughout the synthetic biology research lifecycle. The capabilities of this tool will include version control, intelligent information retrieval about synthetic biology components, and configuration management to assist in assembling those components into working biological systems. We will design, develop, and test this tool in direct collaboration with working synthetic biologists from Dr. Sauro's laboratory. We divide our research work into three specific aims: (1) analyzing the work of synthetic biologists and extending our ontology to include configuration constraints and versioning knowledge for synthetic biology components; (2) developing version management capabilities for synthetic biologists; and (3) developing configuration and parts retrieval capabilities for synthetic biologists. To support both collaborative work and reuse of components, our tool and our research leverages the idea of an annotated library of synthetic biology components. Our work will use existing libraries, and our tool will add to these libraries through its use. To support the widest range of system reuse, our tool will be based on public, semantic web standards and will be developed as an open, modern web application. We envision an enterprise-wide use of our tool, which will allow for better communication, improved design, and better engineering of synthetic biological systems. In turn, these engineering improvements will help hasten the wide-scale industrial production, adoption, and use of synthetic biology constructs, leading to new advances in bioengineered therapeutic, energy, and food technologies.       The public health relevance of the proposed research is that improved engineering for the  synthetic biology enterprise will improve the quality and efficiency with which synthetic biological  constructs can be designed and built. These improvements will enable new scientific advances  that directly benefit the public-advances such as bioengineered therapies, improved antibiotics,  and inexpensive production of pharmaceutical agents.",A synthetic biology library for the Semantic Web,8533504,R42HG006737,"['Adoption', 'Antibiotics', 'Antimalarials', 'Bacteria', 'Biological', 'Biomedical Engineering', 'Cells', 'Characteristics', 'Collaborations', 'Communication', 'Complex', 'Development', 'Engineering', 'Food Technology', 'Future', 'Goals', 'Health', 'Imagination', 'Information Management', 'Information Retrieval', 'Knowledge', 'Laboratories', 'Libraries', 'Life', 'Logic', 'Methods', 'Ontology', 'Organism', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Phase', 'Process', 'Production', 'Public Health', 'Research', 'Research Personnel', 'Retrieval', 'Scientific Advances and Accomplishments', 'Scientist', 'Semantics', 'Software Engineering', 'Staging', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Validation', 'Work', 'base', 'biological systems', 'computer based Semantic Analysis', 'coping', 'design', 'design and construction', 'improved', 'information organization', 'innovation', 'novel', 'prototype', 'public health relevance', 'software development', 'synthetic biology', 'tool', 'tumor']",NHGRI,"CLARK AND PARSIA, LLC",R42,2012,89000,0.04619546115564443
"Computational Gene Modeling and Genome Sequence Assembly    DESCRIPTION (provided by applicant):       This project addresses two major bioinformatics problems: the development of better software for finding genes in eukaryotic genome sequences, and the development of genome assemblers for large shotgun sequencing projects. The gene finding project will pursue two tracks: first, we will continue to improve our Generalized Hidden Markov Model and our Pair Hidden Markov Model gene finders, training them for new species as new genomes appear, and enhancing their capabilities to use related species as a guide to gene finding in a new species. Second, we will develop a new eukaryotic annotation pipeline, which will integrate the results from a wide range of sources, including gene finders, protein sequence alignments, cDNA and EST alignments, and other sequence features. This pipeline will be used to predict comprehensive gene sets for multiple species, focusing especially on species for which the available annotation is incomplete or outdated. The pipeline will also be available as a service to annotate genomes for other groups. The assembler project will include several major efforts. First, we will continue to build on our successful open source assembler project, AMOS, adding new modules to allow inter-operation with other assembly packages. Second, we will develop new assemblers that can handle pyrosequencing data, low coverage genome projects, and sequences collected from complex mixtures of species. Third, we will provide assembly services to genome sequencing centers and other collaborators, helping them to assemble genomes using the latest available assembly tools. These will include new sequencing projects as well as genomes that, although already sequenced, can be re-assembled more accurately using improved assembly software. For all of the software development projects, we will continue our practice of making all our source code freely available to investigators in the scientific research community worldwide.             n/a",Computational Gene Modeling and Genome Sequence Assembly,7858163,R01LM006845,"['Address', 'Amino Acid Sequence', 'Arts', 'Bioinformatics', 'Biological Models', 'Communities', 'Complementary DNA', 'Complex Mixtures', 'Computer software', 'Data', 'Development', 'Environment', 'Expressed Sequence Tags', 'Genes', 'Genome', 'Goals', 'Human Resources', 'Modeling', 'Peptide Sequence Determination', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Services', 'Shotgun Sequencing', 'Source', 'Source Code', 'System', 'Training', 'Trypanosoma cruzi', 'Work', 'experience', 'genome sequencing', 'improved', 'markov model', 'open source', 'operation', 'programs', 'repaired', 'software development', 'tool']",NLM,"UNIV OF MARYLAND, COLLEGE PARK",R01,2010,608513,0.044668957363137723
"Computational Gene Modeling and Genome Sequence Assembly    DESCRIPTION (provided by applicant):       This project addresses two major bioinformatics problems: the development of better software for finding genes in eukaryotic genome sequences, and the development of genome assemblers for large shotgun sequencing projects. The gene finding project will pursue two tracks: first, we will continue to improve our Generalized Hidden Markov Model and our Pair Hidden Markov Model gene finders, training them for new species as new genomes appear, and enhancing their capabilities to use related species as a guide to gene finding in a new species. Second, we will develop a new eukaryotic annotation pipeline, which will integrate the results from a wide range of sources, including gene finders, protein sequence alignments, cDNA and EST alignments, and other sequence features. This pipeline will be used to predict comprehensive gene sets for multiple species, focusing especially on species for which the available annotation is incomplete or outdated. The pipeline will also be available as a service to annotate genomes for other groups. The assembler project will include several major efforts. First, we will continue to build on our successful open source assembler project, AMOS, adding new modules to allow inter-operation with other assembly packages. Second, we will develop new assemblers that can handle pyrosequencing data, low coverage genome projects, and sequences collected from complex mixtures of species. Third, we will provide assembly services to genome sequencing centers and other collaborators, helping them to assemble genomes using the latest available assembly tools. These will include new sequencing projects as well as genomes that, although already sequenced, can be re-assembled more accurately using improved assembly software. For all of the software development projects, we will continue our practice of making all our source code freely available to investigators in the scientific research community worldwide.             n/a",Computational Gene Modeling and Genome Sequence Assembly,7630568,R01LM006845,"['Address', 'Amino Acid Sequence', 'Arts', 'Bioinformatics', 'Biological Models', 'Communities', 'Complementary DNA', 'Complex Mixtures', 'Computer software', 'Data', 'Development', 'Environment', 'Expressed Sequence Tags', 'Genes', 'Genome', 'Goals', 'Human Resources', 'Modeling', 'Operative Surgical Procedures', 'Peptide Sequence Determination', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Services', 'Shotgun Sequencing', 'Source', 'Source Code', 'System', 'Training', 'Trypanosoma cruzi', 'Work', 'experience', 'genome sequencing', 'improved', 'markov model', 'open source', 'programs', 'repaired', 'software development', 'tool']",NLM,"UNIV OF MARYLAND, COLLEGE PARK",R01,2009,603620,0.044668957363137723
"Computational Gene Modeling and Genome Sequence Assembly    DESCRIPTION (provided by applicant):       This project addresses two major bioinformatics problems: the development of better software for finding genes in eukaryotic genome sequences, and the development of genome assemblers for large shotgun sequencing projects. The gene finding project will pursue two tracks: first, we will continue to improve our Generalized Hidden Markov Model and our Pair Hidden Markov Model gene finders, training them for new species as new genomes appear, and enhancing their capabilities to use related species as a guide to gene finding in a new species. Second, we will develop a new eukaryotic annotation pipeline, which will integrate the results from a wide range of sources, including gene finders, protein sequence alignments, cDNA and EST alignments, and other sequence features. This pipeline will be used to predict comprehensive gene sets for multiple species, focusing especially on species for which the available annotation is incomplete or outdated. The pipeline will also be available as a service to annotate genomes for other groups. The assembler project will include several major efforts. First, we will continue to build on our successful open source assembler project, AMOS, adding new modules to allow inter-operation with other assembly packages. Second, we will develop new assemblers that can handle pyrosequencing data, low coverage genome projects, and sequences collected from complex mixtures of species. Third, we will provide assembly services to genome sequencing centers and other collaborators, helping them to assemble genomes using the latest available assembly tools. These will include new sequencing projects as well as genomes that, although already sequenced, can be re-assembled more accurately using improved assembly software. For all of the software development projects, we will continue our practice of making all our source code freely available to investigators in the scientific research community worldwide.             n/a",Computational Gene Modeling and Genome Sequence Assembly,7864735,R01LM006845,"['Address', 'Amino Acid Sequence', 'Arts', 'Bioinformatics', 'Biological Models', 'Communities', 'Complementary DNA', 'Complex Mixtures', 'Computer software', 'Data', 'Development', 'Environment', 'Expressed Sequence Tags', 'Genes', 'Genome', 'Goals', 'Human Resources', 'Modeling', 'Operative Surgical Procedures', 'Peptide Sequence Determination', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Services', 'Shotgun Sequencing', 'Source', 'Source Code', 'System', 'Training', 'Trypanosoma cruzi', 'Work', 'experience', 'genome sequencing', 'improved', 'markov model', 'open source', 'programs', 'repaired', 'software development', 'tool']",NLM,"UNIV OF MARYLAND, COLLEGE PARK",R01,2009,140994,0.044668957363137723
"Computational Gene Modeling and Genome Sequence Assembly    DESCRIPTION (provided by applicant):       This project addresses two major bioinformatics problems: the development of better software for finding genes in eukaryotic genome sequences, and the development of genome assemblers for large shotgun sequencing projects. The gene finding project will pursue two tracks: first, we will continue to improve our Generalized Hidden Markov Model and our Pair Hidden Markov Model gene finders, training them for new species as new genomes appear, and enhancing their capabilities to use related species as a guide to gene finding in a new species. Second, we will develop a new eukaryotic annotation pipeline, which will integrate the results from a wide range of sources, including gene finders, protein sequence alignments, cDNA and EST alignments, and other sequence features. This pipeline will be used to predict comprehensive gene sets for multiple species, focusing especially on species for which the available annotation is incomplete or outdated. The pipeline will also be available as a service to annotate genomes for other groups. The assembler project will include several major efforts. First, we will continue to build on our successful open source assembler project, AMOS, adding new modules to allow inter-operation with other assembly packages. Second, we will develop new assemblers that can handle pyrosequencing data, low coverage genome projects, and sequences collected from complex mixtures of species. Third, we will provide assembly services to genome sequencing centers and other collaborators, helping them to assemble genomes using the latest available assembly tools. These will include new sequencing projects as well as genomes that, although already sequenced, can be re-assembled more accurately using improved assembly software. For all of the software development projects, we will continue our practice of making all our source code freely available to investigators in the scientific research community worldwide.             n/a",Computational Gene Modeling and Genome Sequence Assembly,7435220,R01LM006845,"['Address', 'Altretamine', 'Amino Acid Sequence', 'Arts', 'Bioinformatics', 'Biological Models', 'Communities', 'Complementary DNA', 'Complex Mixtures', 'Computer software', 'Data', 'Development', 'Environment', 'Expressed Sequence Tags', 'Genes', 'Genome', 'Goals', 'Human Resources', 'Modeling', 'Operative Surgical Procedures', 'Peptide Sequence Determination', 'Range', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Services', 'Shotgun Sequencing', 'Source', 'Source Code', 'System', 'Training', 'Trypanosoma cruzi', 'Work', 'experience', 'genome sequencing', 'improved', 'markov model', 'open source', 'programs', 'repaired', 'software development', 'tool']",NLM,"UNIV OF MARYLAND, COLLEGE PARK",R01,2008,588371,0.044668957363137723
"Computational Gene Modeling and Genome Sequence Assembly    DESCRIPTION (provided by applicant):       This project addresses two major bioinformatics problems: the development of better software for finding genes in eukaryotic genome sequences, and the development of genome assemblers for large shotgun sequencing projects. The gene finding project will pursue two tracks: first, we will continue to improve our Generalized Hidden Markov Model and our Pair Hidden Markov Model gene finders, training them for new species as new genomes appear, and enhancing their capabilities to use related species as a guide to gene finding in a new species. Second, we will develop a new eukaryotic annotation pipeline, which will integrate the results from a wide range of sources, including gene finders, protein sequence alignments, cDNA and EST alignments, and other sequence features. This pipeline will be used to predict comprehensive gene sets for multiple species, focusing especially on species for which the available annotation is incomplete or outdated. The pipeline will also be available as a service to annotate genomes for other groups. The assembler project will include several major efforts. First, we will continue to build on our successful open source assembler project, AMOS, adding new modules to allow inter-operation with other assembly packages. Second, we will develop new assemblers that can handle pyrosequencing data, low coverage genome projects, and sequences collected from complex mixtures of species. Third, we will provide assembly services to genome sequencing centers and other collaborators, helping them to assemble genomes using the latest available assembly tools. These will include new sequencing projects as well as genomes that, although already sequenced, can be re-assembled more accurately using improved assembly software. For all of the software development projects, we will continue our practice of making all our source code freely available to investigators in the scientific research community worldwide.             n/a",Computational Gene Modeling and Genome Sequence Assembly,7260194,R01LM006845,"['Address', 'Altretamine', 'Amino Acid Sequence', 'Arts', 'Bioinformatics', 'Biological Models', 'Communities', 'Complementary DNA', 'Complex Mixtures', 'Computer software', 'Data', 'Development', 'Environment', 'Expressed Sequence Tags', 'Genes', 'Genome', 'Goals', 'Human Resources', 'Modeling', 'Operative Surgical Procedures', 'Peptide Sequence Determination', 'Range', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Services', 'Shotgun Sequencing', 'Source', 'Source Code', 'System', 'Training', 'Trypanosoma cruzi', 'Work', 'experience', 'genome sequencing', 'improved', 'markov model', 'open source', 'programs', 'repaired', 'software development', 'tool']",NLM,UNIVERSITY OF MARYLAND COLLEGE PK CAMPUS,R01,2007,556536,0.044668957363137723
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7651469,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'genome database', 'improved', 'interest', 'natural language', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2009,273993,0.05566622698067092
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7465580,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Class', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'RGD (sequence)', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'desire', 'genome database', 'improved', 'interest', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2008,273993,0.05566622698067092
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7264196,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Class', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'RGD (sequence)', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'desire', 'genome database', 'improved', 'interest', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2007,279300,0.05566622698067092
"Turning Data into Whole Cell Ontology Models for Functional Analysis     DESCRIPTION (provided by applicant): A holy grail of bioinformatics is the creation of whole-cell models with the ability to enhance human understanding and facilitate discovery. To this end, a successful and widely-used effort is the Gene Ontology (GO), a massive project to manually annotate genes into terms describing molecular functions, biological processes and cellular components and provide relationships between terms, e.g. capturing that ""small ribosomal subunit"" and ""large ribosomal subunit"" come together to make ""ribosome"". GO is widely used to understand the function of a gene or group of genes. Unfortunately, GO is limited by the effort required to create and update it by hand. It exists only for well-studied organisms and even then in only one, generic form per organism with limited overall genome coverage and a bias towards well-studied genes and functions. It is not possible to learn about an uncharacterized gene or discover a new function using GO, and one cannot quickly assemble an ontology model for a new organism, let alone a specific cell-type or disease-state.  This proposed research will change this state of affairs. Already, work has shown that large networks of gene and protein interactions in Saccharomyces cerevisiae can be used to computationally infer an ontology whose coverage and power are equivalent to those of the manually-curated GO Cellular Component ontology. Still, this first attempt was limited in the types of experimental data used and its ability to infer the more generally useful Biological Process ontology. Here machine learning approaches will be applied to integrate many types of experimental data into ontology model construction and analyze the type of biological information provided by each experiment, revealing those experiments most informative for capturing Biological Process information. Furthermore, the high-throughput experimental data to ontology paradigm explored here will be used to develop a computational tool to highlight novel types of hypotheses that are inaccessible by current high-throughput experimental data analysis methods.  Preliminary work has shown GO to be useful for prediction of synthetic lethal pairs of genes, i.e. genes that are individually non-essential but when knocked out together cause cell death. Given the high mutation rate in cancer, these pairs provide potential cancer drug targets, as a drug may target a gene product which is now essential in the mutated cancer cells but not other cells, thereby killing only cancer cells. Because data-driven ontologies are not as hindered by issues with bias and coverage and are specifically designed to capture only functional relationships, this proposal will explore the idea that data-driven ontologies will be better suited to help predict synthetic lethal pairs than GO. To this end, algorithms will be developed to construct a data-driven ontology of yeast DNA repair and use this ontology to predict synthetic lethal pairs of genes.  Overall, this proposal will develop the computational and experimental roadmap to construct a whole-cell model of gene function - an ontology - and use the model to discover useful biology - synthetic lethal pairs.         PUBLIC HEALTH RELEVANCE: In this proposal, a new framework for using the results of commonly performed, genome-wide experiments has the potential to create whole-cell models of gene function, similar to the widely-used Gene Ontology, directly from data without manual intervention. This will allow creation of useful models of cells from different organisms, tissues and diseases which researchers can use to discover the function of unstudied genes and to uncover new functions performed by the cell. Furthermore, this proposal will use these models for the discovery of new cancer drug targets called synthetic lethal pairs of genes.            ",Turning Data into Whole Cell Ontology Models for Functional Analysis,9145523,F30HG007618,"['Algorithms', 'Antineoplastic Agents', 'Bioinformatics', 'Biological', 'Biological Process', 'Biology', 'Cell Death', 'Cell model', 'Cell physiology', 'Cells', 'Cluster Analysis', 'Code', 'Collection', 'Coupled', 'DNA Repair', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Disease', 'Drug Targeting', 'Future', 'Gene Cluster', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Goals', 'Hand', 'Human', 'Individual', 'Intervention', 'Knock-out', 'Learning', 'Lighting', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Methods', 'Modeling', 'Molecular', 'Mutate', 'Mutation', 'Ontology', 'Organism', 'Pharmaceutical Preparations', 'Phosphotransferases', 'Processed Genes', 'Proteins', 'Research', 'Research Personnel', 'Ribosomes', 'Saccharomyces cerevisiae', 'Subgroup', 'System', 'Tissues', 'Update', 'Work', 'Yeasts', 'base', 'biological information processing', 'cancer cell', 'cell type', 'computerized tools', 'design', 'experimental analysis', 'functional group', 'gene function', 'gene product', 'genome-wide', 'improved', 'killings', 'novel', 'novel anticancer drug', 'prediction algorithm', 'public health relevance', 'research study', 'synthetic biology', 'tool']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",F30,2016,48576,0.16256288156966467
"Turning Data into Whole Cell Ontology Models for Functional Analysis     DESCRIPTION (provided by applicant): A holy grail of bioinformatics is the creation of whole-cell models with the ability to enhance human understanding and facilitate discovery. To this end, a successful and widely-used effort is the Gene Ontology (GO), a massive project to manually annotate genes into terms describing molecular functions, biological processes and cellular components and provide relationships between terms, e.g. capturing that ""small ribosomal subunit"" and ""large ribosomal subunit"" come together to make ""ribosome"". GO is widely used to understand the function of a gene or group of genes. Unfortunately, GO is limited by the effort required to create and update it by hand. It exists only for well-studied organisms and even then in only one, generic form per organism with limited overall genome coverage and a bias towards well-studied genes and functions. It is not possible to learn about an uncharacterized gene or discover a new function using GO, and one cannot quickly assemble an ontology model for a new organism, let alone a specific cell-type or disease-state.  This proposed research will change this state of affairs. Already, work has shown that large networks of gene and protein interactions in Saccharomyces cerevisiae can be used to computationally infer an ontology whose coverage and power are equivalent to those of the manually-curated GO Cellular Component ontology. Still, this first attempt was limited in the types of experimental data used and its ability to infer the more generally useful Biological Process ontology. Here machine learning approaches will be applied to integrate many types of experimental data into ontology model construction and analyze the type of biological information provided by each experiment, revealing those experiments most informative for capturing Biological Process information. Furthermore, the high-throughput experimental data to ontology paradigm explored here will be used to develop a computational tool to highlight novel types of hypotheses that are inaccessible by current high-throughput experimental data analysis methods.  Preliminary work has shown GO to be useful for prediction of synthetic lethal pairs of genes, i.e. genes that are individually non-essential but when knocked out together cause cell death. Given the high mutation rate in cancer, these pairs provide potential cancer drug targets, as a drug may target a gene product which is now essential in the mutated cancer cells but not other cells, thereby killing only cancer cells. Because data-driven ontologies are not as hindered by issues with bias and coverage and are specifically designed to capture only functional relationships, this proposal will explore the idea that data-driven ontologies will be better suited to help predict synthetic lethal pairs than GO. To this end, algorithms will be developed to construct a data-driven ontology of yeast DNA repair and use this ontology to predict synthetic lethal pairs of genes.  Overall, this proposal will develop the computational and experimental roadmap to construct a whole-cell model of gene function - an ontology - and use the model to discover useful biology - synthetic lethal pairs.         PUBLIC HEALTH RELEVANCE: In this proposal, a new framework for using the results of commonly performed, genome-wide experiments has the potential to create whole-cell models of gene function, similar to the widely-used Gene Ontology, directly from data without manual intervention. This will allow creation of useful models of cells from different organisms, tissues and diseases which researchers can use to discover the function of unstudied genes and to uncover new functions performed by the cell. Furthermore, this proposal will use these models for the discovery of new cancer drug targets called synthetic lethal pairs of genes.            ",Turning Data into Whole Cell Ontology Models for Functional Analysis,8951600,F30HG007618,"['Algorithms', 'Antineoplastic Agents', 'Bioinformatics', 'Biological', 'Biological Process', 'Biology', 'Cell Death', 'Cell model', 'Cell physiology', 'Cells', 'Cluster Analysis', 'Code', 'Collection', 'Coupled', 'DNA Repair', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Disease', 'Drug Targeting', 'Future', 'Gene Cluster', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Goals', 'Hand', 'Human', 'Individual', 'Intervention', 'Knock-out', 'Learning', 'Lighting', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Methods', 'Modeling', 'Molecular', 'Mutate', 'Mutation', 'Ontology', 'Organism', 'Pharmaceutical Preparations', 'Phosphotransferases', 'Processed Genes', 'Proteins', 'Research', 'Research Personnel', 'Ribosomes', 'Saccharomyces cerevisiae', 'Subgroup', 'System', 'Tissues', 'Update', 'Work', 'Yeasts', 'base', 'biological information processing', 'cancer cell', 'cell type', 'computerized tools', 'design', 'experimental analysis', 'functional group', 'gene function', 'genome-wide', 'improved', 'killings', 'novel', 'public health relevance', 'research study', 'synthetic biology', 'tool']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",F30,2015,39304,0.16256288156966467
"Turning Data into Whole Cell Ontology Models for Functional Analysis     DESCRIPTION (provided by applicant): A holy grail of bioinformatics is the creation of whole-cell models with the ability to enhance human understanding and facilitate discovery. To this end, a successful and widely-used effort is the Gene Ontology (GO), a massive project to manually annotate genes into terms describing molecular functions, biological processes and cellular components and provide relationships between terms, e.g. capturing that ""small ribosomal subunit"" and ""large ribosomal subunit"" come together to make ""ribosome"". GO is widely used to understand the function of a gene or group of genes. Unfortunately, GO is limited by the effort required to create and update it by hand. It exists only for well-studied organisms and even then in only one, generic form per organism with limited overall genome coverage and a bias towards well-studied genes and functions. It is not possible to learn about an uncharacterized gene or discover a new function using GO, and one cannot quickly assemble an ontology model for a new organism, let alone a specific cell-type or disease-state.  This proposed research will change this state of affairs. Already, work has shown that large networks of gene and protein interactions in Saccharomyces cerevisiae can be used to computationally infer an ontology whose coverage and power are equivalent to those of the manually-curated GO Cellular Component ontology. Still, this first attempt was limited in the types of experimental data used and its ability to infer the more generally useful Biological Process ontology. Here machine learning approaches will be applied to integrate many types of experimental data into ontology model construction and analyze the type of biological information provided by each experiment, revealing those experiments most informative for capturing Biological Process information. Furthermore, the high-throughput experimental data to ontology paradigm explored here will be used to develop a computational tool to highlight novel types of hypotheses that are inaccessible by current high-throughput experimental data analysis methods.  Preliminary work has shown GO to be useful for prediction of synthetic lethal pairs of genes, i.e. genes that are individually non-essential but when knocked out together cause cell death. Given the high mutation rate in cancer, these pairs provide potential cancer drug targets, as a drug may target a gene product which is now essential in the mutated cancer cells but not other cells, thereby killing only cancer cells. Because data-driven ontologies are not as hindered by issues with bias and coverage and are specifically designed to capture only functional relationships, this proposal will explore the idea that data-driven ontologies will be better suited to help predict synthetic lethal pairs than GO. To this end, algorithms will be developed to construct a data-driven ontology of yeast DNA repair and use this ontology to predict synthetic lethal pairs of genes.  Overall, this proposal will develop the computational and experimental roadmap to construct a whole-cell model of gene function - an ontology - and use the model to discover useful biology - synthetic lethal pairs.         PUBLIC HEALTH RELEVANCE: In this proposal, a new framework for using the results of commonly performed, genome-wide experiments has the potential to create whole-cell models of gene function, similar to the widely-used Gene Ontology, directly from data without manual intervention. This will allow creation of useful models of cells from different organisms, tissues and diseases which researchers can use to discover the function of unstudied genes and to uncover new functions performed by the cell. Furthermore, this proposal will use these models for the discovery of new cancer drug targets called synthetic lethal pairs of genes.            ",Turning Data into Whole Cell Ontology Models for Functional Analysis,8644512,F30HG007618,"['Algorithms', 'Antineoplastic Agents', 'Bioinformatics', 'Biological', 'Biological Process', 'Biology', 'Cell Death', 'Cell model', 'Cell physiology', 'Cells', 'Cluster Analysis', 'Code', 'Collection', 'Coupled', 'DNA Repair', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Disease', 'Drug Targeting', 'Future', 'Gene Cluster', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Goals', 'Hand', 'Human', 'Individual', 'Intervention', 'Knock-out', 'Learning', 'Lighting', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Methods', 'Modeling', 'Molecular', 'Mutate', 'Mutation', 'Ontology', 'Organism', 'Pharmaceutical Preparations', 'Phosphotransferases', 'Processed Genes', 'Proteins', 'Research', 'Research Personnel', 'Ribosomes', 'Saccharomyces cerevisiae', 'Subgroup', 'System', 'Tissues', 'Update', 'Work', 'Yeasts', 'base', 'biological information processing', 'cancer cell', 'cell type', 'computerized tools', 'design', 'experimental analysis', 'functional group', 'gene function', 'genome-wide', 'improved', 'killings', 'novel', 'public health relevance', 'research study', 'synthetic biology', 'tool']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",F30,2014,35110,0.16256288156966467
"Neural ElectroMagnetic Ontologies: ERP Knowledge Representation & Integration    Description (provided by applicant): Research in the design and implementation of ""Neural ElectroMagnetic Ontologies"" (NEMO) will address a critical need for tools to support representation, storage, and sharing of brain electromagnetic data. Electro- encephalography (EEG) and event-related potentials (ERP) are venerable techniques for cognitive and clinical research on human brain function. To realize their full potential, however, it will be necessary to address some long-standing challenges in comparing results across experiments and research laboratories. NEMO will address this need by providing ERP ontologies that can be used for meta-analysis of patterns across experiment contexts and research labs. Given the widespread use of EEG and ERP methods, and their clinical as well as research applications, development of such a system is both timely and significant. System design and implementation will rest on six specific aims. The first goal is to develop rigorous procedures for classification and labeling of electrophysiological patterns (event-related potentials, or ERPs) (Aim 1). The methods and tools that are developed initially for classification and labeling of surface (sensor- level) data will then be extended to support classification of data in source (anatomical) space (Aim 2). Next, we will represent the concepts that define ERP patterns as formal logics, or ""ontologies,"" and will use those concepts to describe the ERP patterns. Relational databases will be modeled based on the ontologies to support high-level questions about the nature of ERP patterns and the relationships between patterns that are associated with different lab, experiment, and analysis contexts (Aim 3). The application domain for our project is reading and language. We have established a consortium of experts in this area who will contribute EEG and ERP data from experimental studies and will collaborate with us on the design and testing, and evaluation of the tools developed for this project. The practical scientific aim will be to conduct meta-analyses of ERP patterns in reading and language. In addition to re-analyses of existing cross-lab data, new experiment paradigms (adapted from the fBIRN project) will be carried out across research sites to calibrate data acquisition and preprocessing methods, and to test the robustness of patterns across different experiment contexts (Aim 4). Initially, we will develop a different ontology for each representational space (e.g., sensor and source space) and each analysis method. Then, we will capture the semantic mappings between different sets of patterns (different ontologies) using data mining (Aim 5). To support this work, we will develop an integrated tool environment for storage and management of EEG and ERP data and meta-data, measure generation and labeling, ontology development, and meta-analysis. This environment will be web-accessible so that partners will have shared access to the project data, analysis tools, ontologies, and meta-analysis results (Aim 6). At the end of this project, the ontologies, annotated database, tools, and technologies will be made available to the larger research community. PUBLIC HEALTH RELEVANCE The practical goal for the NEMO project is to build an ontology database to support data sharing and meta- analysis of EEG and ERP results. The ability to describe brain electrophysiological patterns from different research laboratories and different experiment contexts within a common framework will have immediate benefits for the neuroscience community, as well as long-term benefits for neuroscience research and for scientific areas with similar requirements for robust data representation and integration, and data and resources sharing.            Relevance The practical goal for the NEMO project is to build an ontology database to support data sharing and meta- analysis of EEG and ERP results. The ability to describe brain electrophysiological patterns from different research laboratories and different experment contexts within a common framework will have immediate benefits for the neuroscience community, as well as long-term benefits for neuroscience research and for scientific areas with similar requirements for robust data representation and integration, and data and resources sharing. 1",Neural ElectroMagnetic Ontologies: ERP Knowledge Representation & Integration,8269994,R01EB007684,"['Address', 'Area', 'Auditory', 'Automatic Data Processing', 'Brain', 'Brain imaging', 'Brodmann&apos', 's area', 'Calibration', 'Classification', 'Clinical', 'Clinical Research', 'Code', 'Cognitive', 'Collaborations', 'Communities', 'Comprehension', 'Computer software', 'Data', 'Data Analyses', 'Data Provenance', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Electromagnetics', 'Environment', 'Equipment', 'Evaluation', 'Event-Related Potentials', 'Generations', 'Goals', 'Head', 'Health', 'Human', 'Internet', 'Knowledge', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Lead', 'Location', 'Logic', 'Maps', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Nature', 'Neuropsychology', 'Neurosciences', 'Neurosciences Research', 'Online Systems', 'Ontology', 'Pattern', 'Procedures', 'Process', 'Reading', 'Research', 'Research Methodology', 'Research Personnel', 'Resolution', 'Resource Sharing', 'Rest', 'Scalp structure', 'Scanning', 'Semantics', 'Series', 'Simulate', 'Site', 'Solutions', 'Source', 'Stream', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'To specify', 'Translations', 'Visual', 'Work', 'base', 'data acquisition', 'data integration', 'data management', 'data mining', 'data sharing', 'density', 'design', 'encephalography', 'evaluation/testing', 'information organization', 'interdisciplinary approach', 'interest', 'member', 'neurodevelopment', 'neuroinformatics', 'relating to nervous system', 'relational database', 'research study', 'sensor', 'spatiotemporal', 'success', 'task analysis', 'tool', 'tool development', 'web-accessible']",NIBIB,UNIVERSITY OF OREGON,R01,2012,487090,0.13811435433450764
"Neural ElectroMagnetic Ontologies: ERP Knowledge Representation & Integration    Description (provided by applicant): Research in the design and implementation of ""Neural ElectroMagnetic Ontologies"" (NEMO) will address a critical need for tools to support representation, storage, and sharing of brain electromagnetic data. Electro- encephalography (EEG) and event-related potentials (ERP) are venerable techniques for cognitive and clinical research on human brain function. To realize their full potential, however, it will be necessary to address some long-standing challenges in comparing results across experiments and research laboratories. NEMO will address this need by providing ERP ontologies that can be used for meta-analysis of patterns across experiment contexts and research labs. Given the widespread use of EEG and ERP methods, and their clinical as well as research applications, development of such a system is both timely and significant. System design and implementation will rest on six specific aims. The first goal is to develop rigorous procedures for classification and labeling of electrophysiological patterns (event-related potentials, or ERPs) (Aim 1). The methods and tools that are developed initially for classification and labeling of surface (sensor- level) data will then be extended to support classification of data in source (anatomical) space (Aim 2). Next, we will represent the concepts that define ERP patterns as formal logics, or ""ontologies,"" and will use those concepts to describe the ERP patterns. Relational databases will be modeled based on the ontologies to support high-level questions about the nature of ERP patterns and the relationships between patterns that are associated with different lab, experiment, and analysis contexts (Aim 3). The application domain for our project is reading and language. We have established a consortium of experts in this area who will contribute EEG and ERP data from experimental studies and will collaborate with us on the design and testing, and evaluation of the tools developed for this project. The practical scientific aim will be to conduct meta-analyses of ERP patterns in reading and language. In addition to re-analyses of existing cross-lab data, new experiment paradigms (adapted from the fBIRN project) will be carried out across research sites to calibrate data acquisition and preprocessing methods, and to test the robustness of patterns across different experiment contexts (Aim 4). Initially, we will develop a different ontology for each representational space (e.g., sensor and source space) and each analysis method. Then, we will capture the semantic mappings between different sets of patterns (different ontologies) using data mining (Aim 5). To support this work, we will develop an integrated tool environment for storage and management of EEG and ERP data and meta-data, measure generation and labeling, ontology development, and meta-analysis. This environment will be web-accessible so that partners will have shared access to the project data, analysis tools, ontologies, and meta-analysis results (Aim 6). At the end of this project, the ontologies, annotated database, tools, and technologies will be made available to the larger research community. PUBLIC HEALTH RELEVANCE The practical goal for the NEMO project is to build an ontology database to support data sharing and meta- analysis of EEG and ERP results. The ability to describe brain electrophysiological patterns from different research laboratories and different experiment contexts within a common framework will have immediate benefits for the neuroscience community, as well as long-term benefits for neuroscience research and for scientific areas with similar requirements for robust data representation and integration, and data and resources sharing.            Relevance The practical goal for the NEMO project is to build an ontology database to support data sharing and meta- analysis of EEG and ERP results. The ability to describe brain electrophysiological patterns from different research laboratories and different experment contexts within a common framework will have immediate benefits for the neuroscience community, as well as long-term benefits for neuroscience research and for scientific areas with similar requirements for robust data representation and integration, and data and resources sharing. 1",Neural ElectroMagnetic Ontologies: ERP Knowledge Representation & Integration,8069619,R01EB007684,"['Address', 'Area', 'Auditory', 'Automatic Data Processing', 'Brain', 'Brain imaging', 'Brodmann&apos', 's area', 'Calibration', 'Classification', 'Clinical', 'Clinical Research', 'Code', 'Cognitive', 'Collaborations', 'Communities', 'Comprehension', 'Computer software', 'Data', 'Data Analyses', 'Data Provenance', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Electromagnetics', 'Environment', 'Equipment', 'Evaluation', 'Event-Related Potentials', 'Generations', 'Goals', 'Head', 'Health', 'Human', 'Internet', 'Knowledge', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Lead', 'Location', 'Logic', 'Maps', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Nature', 'Neuropsychology', 'Neurosciences', 'Neurosciences Research', 'Online Systems', 'Ontology', 'Pattern', 'Procedures', 'Process', 'Reading', 'Research', 'Research Methodology', 'Research Personnel', 'Resolution', 'Resource Sharing', 'Rest', 'Scalp structure', 'Scanning', 'Semantics', 'Series', 'Simulate', 'Site', 'Solutions', 'Source', 'Stream', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'To specify', 'Translations', 'Visual', 'Work', 'base', 'data acquisition', 'data integration', 'data management', 'data mining', 'data sharing', 'density', 'design', 'encephalography', 'evaluation/testing', 'information organization', 'interdisciplinary approach', 'interest', 'member', 'neurodevelopment', 'neuroinformatics', 'relating to nervous system', 'relational database', 'research study', 'sensor', 'spatiotemporal', 'success', 'task analysis', 'tool', 'tool development', 'web-accessible']",NIBIB,UNIVERSITY OF OREGON,R01,2011,515495,0.13811435433450764
"Neural ElectroMagnetic Ontologies: ERP Knowledge Representation & Integration    Description (provided by applicant): Research in the design and implementation of ""Neural ElectroMagnetic Ontologies"" (NEMO) will address a critical need for tools to support representation, storage, and sharing of brain electromagnetic data. Electro- encephalography (EEG) and event-related potentials (ERP) are venerable techniques for cognitive and clinical research on human brain function. To realize their full potential, however, it will be necessary to address some long-standing challenges in comparing results across experiments and research laboratories. NEMO will address this need by providing ERP ontologies that can be used for meta-analysis of patterns across experiment contexts and research labs. Given the widespread use of EEG and ERP methods, and their clinical as well as research applications, development of such a system is both timely and significant. System design and implementation will rest on six specific aims. The first goal is to develop rigorous procedures for classification and labeling of electrophysiological patterns (event-related potentials, or ERPs) (Aim 1). The methods and tools that are developed initially for classification and labeling of surface (sensor- level) data will then be extended to support classification of data in source (anatomical) space (Aim 2). Next, we will represent the concepts that define ERP patterns as formal logics, or ""ontologies,"" and will use those concepts to describe the ERP patterns. Relational databases will be modeled based on the ontologies to support high-level questions about the nature of ERP patterns and the relationships between patterns that are associated with different lab, experiment, and analysis contexts (Aim 3). The application domain for our project is reading and language. We have established a consortium of experts in this area who will contribute EEG and ERP data from experimental studies and will collaborate with us on the design and testing, and evaluation of the tools developed for this project. The practical scientific aim will be to conduct meta-analyses of ERP patterns in reading and language. In addition to re-analyses of existing cross-lab data, new experiment paradigms (adapted from the fBIRN project) will be carried out across research sites to calibrate data acquisition and preprocessing methods, and to test the robustness of patterns across different experiment contexts (Aim 4). Initially, we will develop a different ontology for each representational space (e.g., sensor and source space) and each analysis method. Then, we will capture the semantic mappings between different sets of patterns (different ontologies) using data mining (Aim 5). To support this work, we will develop an integrated tool environment for storage and management of EEG and ERP data and meta-data, measure generation and labeling, ontology development, and meta-analysis. This environment will be web-accessible so that partners will have shared access to the project data, analysis tools, ontologies, and meta-analysis results (Aim 6). At the end of this project, the ontologies, annotated database, tools, and technologies will be made available to the larger research community. PUBLIC HEALTH RELEVANCE The practical goal for the NEMO project is to build an ontology database to support data sharing and meta- analysis of EEG and ERP results. The ability to describe brain electrophysiological patterns from different research laboratories and different experiment contexts within a common framework will have immediate benefits for the neuroscience community, as well as long-term benefits for neuroscience research and for scientific areas with similar requirements for robust data representation and integration, and data and resources sharing.            Relevance The practical goal for the NEMO project is to build an ontology database to support data sharing and meta- analysis of EEG and ERP results. The ability to describe brain electrophysiological patterns from different research laboratories and different experment contexts within a common framework will have immediate benefits for the neuroscience community, as well as long-term benefits for neuroscience research and for scientific areas with similar requirements for robust data representation and integration, and data and resources sharing. 1",Neural ElectroMagnetic Ontologies: ERP Knowledge Representation & Integration,7816664,R01EB007684,"['Address', 'Area', 'Arts', 'Auditory', 'Automatic Data Processing', 'Brain', 'Brain imaging', 'Brodmann&apos', 's area', 'Calibration', 'Classification', 'Clinical', 'Clinical Research', 'Code', 'Cognitive', 'Collaborations', 'Communities', 'Comprehension', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Electromagnetics', 'Environment', 'Equipment', 'Evaluation', 'Event-Related Potentials', 'Generations', 'Goals', 'Head', 'Human', 'Internet', 'Knowledge', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Lead', 'Location', 'Logic', 'Maps', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Nature', 'Neuropsychology', 'Neurosciences', 'Neurosciences Research', 'Online Systems', 'Ontology', 'Pattern', 'Procedures', 'Process', 'Reading', 'Research', 'Research Methodology', 'Research Personnel', 'Resolution', 'Resource Sharing', 'Rest', 'Scalp structure', 'Scanning', 'Semantics', 'Series', 'Simulate', 'Site', 'Solutions', 'Source', 'Stream', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'To specify', 'Translations', 'Visual', 'Work', 'base', 'data acquisition', 'data integration', 'data management', 'data mining', 'data sharing', 'density', 'design', 'encephalography', 'evaluation/testing', 'information organization', 'interdisciplinary approach', 'interest', 'member', 'neurodevelopment', 'neuroinformatics', 'public health relevance', 'relating to nervous system', 'relational database', 'research study', 'sensor', 'spatiotemporal', 'success', 'task analysis', 'tool', 'tool development', 'web-accessible', 'wiki']",NIBIB,UNIVERSITY OF OREGON,R01,2010,566478,0.13811435433450764
"Neural ElectroMagnetic Ontologies: ERP Knowledge Representation & Integration    Description (provided by applicant): Research in the design and implementation of ""Neural ElectroMagnetic Ontologies"" (NEMO) will address a critical need for tools to support representation, storage, and sharing of brain electromagnetic data. Electro- encephalography (EEG) and event-related potentials (ERP) are venerable techniques for cognitive and clinical research on human brain function. To realize their full potential, however, it will be necessary to address some long-standing challenges in comparing results across experiments and research laboratories. NEMO will address this need by providing ERP ontologies that can be used for meta-analysis of patterns across experiment contexts and research labs. Given the widespread use of EEG and ERP methods, and their clinical as well as research applications, development of such a system is both timely and significant. System design and implementation will rest on six specific aims. The first goal is to develop rigorous procedures for classification and labeling of electrophysiological patterns (event-related potentials, or ERPs) (Aim 1). The methods and tools that are developed initially for classification and labeling of surface (sensor- level) data will then be extended to support classification of data in source (anatomical) space (Aim 2). Next, we will represent the concepts that define ERP patterns as formal logics, or ""ontologies,"" and will use those concepts to describe the ERP patterns. Relational databases will be modeled based on the ontologies to support high-level questions about the nature of ERP patterns and the relationships between patterns that are associated with different lab, experiment, and analysis contexts (Aim 3). The application domain for our project is reading and language. We have established a consortium of experts in this area who will contribute EEG and ERP data from experimental studies and will collaborate with us on the design and testing, and evaluation of the tools developed for this project. The practical scientific aim will be to conduct meta-analyses of ERP patterns in reading and language. In addition to re-analyses of existing cross-lab data, new experiment paradigms (adapted from the fBIRN project) will be carried out across research sites to calibrate data acquisition and preprocessing methods, and to test the robustness of patterns across different experiment contexts (Aim 4). Initially, we will develop a different ontology for each representational space (e.g., sensor and source space) and each analysis method. Then, we will capture the semantic mappings between different sets of patterns (different ontologies) using data mining (Aim 5). To support this work, we will develop an integrated tool environment for storage and management of EEG and ERP data and meta-data, measure generation and labeling, ontology development, and meta-analysis. This environment will be web-accessible so that partners will have shared access to the project data, analysis tools, ontologies, and meta-analysis results (Aim 6). At the end of this project, the ontologies, annotated database, tools, and technologies will be made available to the larger research community. PUBLIC HEALTH RELEVANCE The practical goal for the NEMO project is to build an ontology database to support data sharing and meta- analysis of EEG and ERP results. The ability to describe brain electrophysiological patterns from different research laboratories and different experiment contexts within a common framework will have immediate benefits for the neuroscience community, as well as long-term benefits for neuroscience research and for scientific areas with similar requirements for robust data representation and integration, and data and resources sharing.            Relevance The practical goal for the NEMO project is to build an ontology database to support data sharing and meta- analysis of EEG and ERP results. The ability to describe brain electrophysiological patterns from different research laboratories and different experment contexts within a common framework will have immediate benefits for the neuroscience community, as well as long-term benefits for neuroscience research and for scientific areas with similar requirements for robust data representation and integration, and data and resources sharing. 1",Neural ElectroMagnetic Ontologies: ERP Knowledge Representation & Integration,7585137,R01EB007684,"['Address', 'Area', 'Arts', 'Auditory', 'Automatic Data Processing', 'Brain', 'Brain imaging', 'Brodmann&apos', 's area', 'Calibration', 'Classification', 'Clinical', 'Clinical Research', 'Code', 'Cognitive', 'Collaborations', 'Communities', 'Comprehension', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Electromagnetics', 'Environment', 'Equipment', 'Evaluation', 'Event-Related Potentials', 'Generations', 'Goals', 'Head', 'Human', 'Internet', 'Knowledge', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Lead', 'Location', 'Logic', 'Maps', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Nature', 'Neuropsychology', 'Neurosciences', 'Neurosciences Research', 'Online Systems', 'Ontology', 'Pattern', 'Procedures', 'Process', 'Reading', 'Research', 'Research Methodology', 'Research Personnel', 'Resolution', 'Resource Sharing', 'Rest', 'Scalp structure', 'Scanning', 'Semantics', 'Series', 'Simulate', 'Site', 'Solutions', 'Source', 'Stream', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'To specify', 'Translations', 'Visual', 'Work', 'base', 'data acquisition', 'data integration', 'data management', 'data mining', 'data sharing', 'density', 'design', 'encephalography', 'evaluation/testing', 'information organization', 'interdisciplinary approach', 'interest', 'member', 'neurodevelopment', 'neuroinformatics', 'public health relevance', 'relating to nervous system', 'research study', 'sensor', 'spatiotemporal', 'success', 'task analysis', 'tool', 'tool development', 'web-accessible', 'wiki']",NIBIB,UNIVERSITY OF OREGON,R01,2009,597692,0.13811435433450764
"UniProt: A centralized protein sequence and function resource Project summary The mission of the Universal Protein Resource (UniProt) is to support biomedical research by providing a freely available, stable, comprehensive, richly and accurately annotated protein sequence knowledgebase (www.uniprot.org). UniProt integrates, interprets and standardizes data from a multitude of sources to achieve the most comprehensive catalog of protein sequences and functional annotation available to date, providing information from hundreds of thousands of publications for tens of millions of proteins from tens of thousands of species. The activities proposed here will increase the utility of UniProt for biomedical research and precision medicine. The expert curated functional information provided by UniProt is widely acknowledged to be of exceptional quality and is continuously updated as new knowledge becomes available. Our first aim will be to continue to curate the scientific literature to ensure UniProt remains up to date. We will also work with the text-mining community to continue to improve curation efficiency. The curated records (0.5 million) are complemented by the (80 million) records for uncharacterized proteins. To ensure their usefulness for the community we will continue to develop our automatic annotation systems to annotate these proteins based on the knowledge of characterized proteins. Our third aim is to connect to and integrate protein data from resources around the world to make UniProt the worldwide global hub of protein information. The integration of clinical variation data as well as metabolomics information with proteins will help to support the multi-omics approaches of precision medicine. Our fourth aim describes the production of the resource to ensure that our data is freely available according to the FAIR principles. UniProt forms a foundation for hundreds of life sciences data resources. Continuous software development is needed to ensure delivery of this key component of the life science infrastructure. The UniProt website is used by hundreds of thousands of scientists every month. The final aim describes how we will enable this community to make best use of UniProt, through user training, outreach and improved user interfaces, driven by user testing. Project narrative UniProt is the world’s leading resource of protein sequence and functional information, covering all species including Homo sapiens, model organisms and pathogens. UniProt annotates protein function using community-standard ontologies to support the interpretation of genomic and other datasets on which biomedical research and precision medicine depend. The activities described here will increase the utility of UniProt for biomedical research and precision medicine, enhancing research efficiency and understanding of human disease.",UniProt: A centralized protein sequence and function resource,9948760,U24HG007822,"['Amino Acid Sequence', 'Animal Model', 'Biological Sciences', 'Biomedical Research', 'Catalogs', 'Clinical', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Education and Outreach', 'Ensure', 'FAIR principles', 'Foundations', 'Generations', 'Genes', 'Genomics', 'Growth', 'Health', 'Homo sapiens', 'Homologous Protein', 'Human', 'Industry', 'Infrastructure', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Metadata', 'Methods', 'Mission', 'Ontology', 'Orthologous Gene', 'Persons', 'Production', 'Proteins', 'Proteomics', 'Publications', 'Records', 'Research', 'Resources', 'Rhea', 'Role', 'Scientist', 'Services', 'Source', 'Source Code', 'Standardization', 'Structure', 'Test Result', 'Testing', 'Training', 'Triage', 'Update', 'Variant', 'Visualization', 'Work', 'annotation  system', 'base', 'biological systems', 'biomedical resource', 'data resource', 'data science resource', 'data standards', 'experience', 'feeding', 'genomic data', 'human disease', 'human pathogen', 'improved', 'interactive tool', 'interest', 'knowledge base', 'large scale data', 'metabolomics', 'multiple omics', 'outreach', 'pathogen', 'personalized approach', 'precision medicine', 'prediction algorithm', 'protein function', 'public repository', 'software development', 'text searching', 'tool development', 'usability', 'web services', 'web site', 'website development']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U24,2020,3000000,0.08487568987750066
"UniProt: A centralized protein sequence and function resource Project summary The mission of the Universal Protein Resource (UniProt) is to support biomedical research by providing a freely available, stable, comprehensive, richly and accurately annotated protein sequence knowledgebase (www.uniprot.org). UniProt integrates, interprets and standardizes data from a multitude of sources to achieve the most comprehensive catalog of protein sequences and functional annotation available to date, providing information from hundreds of thousands of publications for tens of millions of proteins from tens of thousands of species. The activities proposed here will increase the utility of UniProt for biomedical research and precision medicine. The expert curated functional information provided by UniProt is widely acknowledged to be of exceptional quality and is continuously updated as new knowledge becomes available. Our first aim will be to continue to curate the scientific literature to ensure UniProt remains up to date. We will also work with the text-mining community to continue to improve curation efficiency. The curated records (0.5 million) are complemented by the (80 million) records for uncharacterized proteins. To ensure their usefulness for the community we will continue to develop our automatic annotation systems to annotate these proteins based on the knowledge of characterized proteins. Our third aim is to connect to and integrate protein data from resources around the world to make UniProt the worldwide global hub of protein information. The integration of clinical variation data as well as metabolomics information with proteins will help to support the multi-omics approaches of precision medicine. Our fourth aim describes the production of the resource to ensure that our data is freely available according to the FAIR principles. UniProt forms a foundation for hundreds of life sciences data resources. Continuous software development is needed to ensure delivery of this key component of the life science infrastructure. The UniProt website is used by hundreds of thousands of scientists every month. The final aim describes how we will enable this community to make best use of UniProt, through user training, outreach and improved user interfaces, driven by user testing. Project narrative UniProt is the world’s leading resource of protein sequence and functional information, covering all species including Homo sapiens, model organisms and pathogens. UniProt annotates protein function using community-standard ontologies to support the interpretation of genomic and other datasets on which biomedical research and precision medicine depend. The activities described here will increase the utility of UniProt for biomedical research and precision medicine, enhancing research efficiency and understanding of human disease.",UniProt: A centralized protein sequence and function resource,9707857,U24HG007822,"['Amino Acid Sequence', 'Animal Model', 'Biological Sciences', 'Biomedical Research', 'Catalogs', 'Clinical', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Education and Outreach', 'Ensure', 'FAIR principles', 'Foundations', 'Generations', 'Genes', 'Genomics', 'Growth', 'Health', 'Homo sapiens', 'Homologous Protein', 'Human', 'Imagery', 'Industry', 'Infrastructure', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Metadata', 'Methods', 'Mission', 'Ontology', 'Orthologous Gene', 'Persons', 'Production', 'Proteins', 'Proteomics', 'Publications', 'Records', 'Research', 'Resources', 'Rhea', 'Role', 'Scientist', 'Services', 'Source', 'Source Code', 'Standardization', 'Structure', 'Test Result', 'Testing', 'Training', 'Triage', 'Update', 'Variant', 'Work', 'annotation  system', 'base', 'biological systems', 'biomedical resource', 'data resource', 'data science resource', 'experience', 'feeding', 'genomic data', 'human disease', 'human pathogen', 'improved', 'interactive tool', 'interest', 'knowledge base', 'metabolomics', 'multiple omics', 'outreach', 'pathogen', 'personalized approach', 'precision medicine', 'prediction algorithm', 'protein function', 'repository', 'software development', 'text searching', 'tool development', 'usability', 'web services', 'web site', 'website development']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U24,2019,3000000,0.08487568987750066
"UniProt: A centralized protein sequence and function resource Project summary The mission of the Universal Protein Resource (UniProt) is to support biomedical research by providing a freely available, stable, comprehensive, richly and accurately annotated protein sequence knowledgebase (www.uniprot.org). UniProt integrates, interprets and standardizes data from a multitude of sources to achieve the most comprehensive catalog of protein sequences and functional annotation available to date, providing information from hundreds of thousands of publications for tens of millions of proteins from tens of thousands of species. The activities proposed here will increase the utility of UniProt for biomedical research and precision medicine. The expert curated functional information provided by UniProt is widely acknowledged to be of exceptional quality and is continuously updated as new knowledge becomes available. Our first aim will be to continue to curate the scientific literature to ensure UniProt remains up to date. We will also work with the text-mining community to continue to improve curation efficiency. The curated records (0.5 million) are complemented by the (80 million) records for uncharacterized proteins. To ensure their usefulness for the community we will continue to develop our automatic annotation systems to annotate these proteins based on the knowledge of characterized proteins. Our third aim is to connect to and integrate protein data from resources around the world to make UniProt the worldwide global hub of protein information. The integration of clinical variation data as well as metabolomics information with proteins will help to support the multi-omics approaches of precision medicine. Our fourth aim describes the production of the resource to ensure that our data is freely available according to the FAIR principles. UniProt forms a foundation for hundreds of life sciences data resources. Continuous software development is needed to ensure delivery of this key component of the life science infrastructure. The UniProt website is used by hundreds of thousands of scientists every month. The final aim describes how we will enable this community to make best use of UniProt, through user training, outreach and improved user interfaces, driven by user testing. Project narrative UniProt is the world’s leading resource of protein sequence and functional information, covering all species including Homo sapiens, model organisms and pathogens. UniProt annotates protein function using community-standard ontologies to support the interpretation of genomic and other datasets on which biomedical research and precision medicine depend. The activities described here will increase the utility of UniProt for biomedical research and precision medicine, enhancing research efficiency and understanding of human disease.",UniProt: A centralized protein sequence and function resource,9527210,U24HG007822,"['Amino Acid Sequence', 'Animal Model', 'Biological Sciences', 'Biomedical Research', 'Catalogs', 'Clinical', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Education and Outreach', 'Ensure', 'FAIR principles', 'Foundations', 'Generations', 'Genes', 'Genomics', 'Growth', 'Health', 'Homo sapiens', 'Homologous Protein', 'Human', 'Imagery', 'Industry', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Metadata', 'Methods', 'Mission', 'Ontology', 'Orthologous Gene', 'Persons', 'Production', 'Proteins', 'Proteomics', 'Publications', 'Records', 'Research', 'Research Infrastructure', 'Resources', 'Rhea', 'Role', 'Scientist', 'Services', 'Source', 'Source Code', 'Standardization', 'Structure', 'Test Result', 'Testing', 'Training', 'Triage', 'Update', 'Variant', 'Work', 'annotation  system', 'base', 'biological systems', 'biomedical resource', 'data resource', 'experience', 'feeding', 'genomic data', 'human disease', 'improved', 'interactive tool', 'interest', 'knowledge base', 'metabolomics', 'multiple omics', 'outreach', 'pathogen', 'personalized approach', 'precision medicine', 'prediction algorithm', 'protein function', 'repository', 'software development', 'text searching', 'tool development', 'usability', 'web services', 'web site', 'website development']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U24,2018,200000,0.08487568987750066
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,8076789,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical ontology', 'biomedical scientist', 'design', 'information organization', 'innovation', 'knowledge base', 'meetings', 'member', 'next generation', 'open source', 'repository', 'research and development', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2010,956625,0.14810998836901051
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7660538,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical ontology', 'biomedical scientist', 'design', 'information organization', 'innovation', 'knowledge base', 'meetings', 'member', 'next generation', 'open source', 'repository', 'research and development', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2009,688362,0.14810998836901051
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7475421,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Class', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Facility Construction Funding Category', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Numbers', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical resource', 'concept', 'design', 'information organization', 'innovation', 'knowledge base', 'member', 'next generation', 'open source', 'repository', 'research and development', 'size', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2007,160000,0.14810998836901051
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7248464,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Class', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Facility Construction Funding Category', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Numbers', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical resource', 'concept', 'design', 'information organization', 'innovation', 'knowledge base', 'member', 'next generation', 'open source', 'repository', 'research and development', 'size', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2007,693808,0.14810998836901051
"Technology Development for a MolBio Knowledge-Base Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges adsing from the proliferation of high- throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent iadvances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB. n/a",Technology Development for a MolBio Knowledge-Base,7473405,R01LM008111,"['Address', 'Area', 'Biology', 'Class', 'Clinical', 'Collection', 'Data', 'Databases', 'Evaluation', 'Facility Construction Funding Category', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Human', 'Indium', 'Informatics', 'Information Resources', 'Information Resources Management', 'Investments', 'Knowledge', 'Knowledge Base (Computer)', 'Laboratories', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Metabolism', 'Methods', 'Metric System', 'Molecular', 'Molecular Biology', 'Mus', 'Names', 'Natural Language Processing', 'Numbers', 'Ontology', 'Persons', 'Pharmaceutical Preparations', 'Plug-in', 'Proteins', 'Purpose', 'Semantics', 'Source', 'SwissProt', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'base', 'concept', 'data integration', 'gene function', 'heuristics', 'instrumentation', 'knowledge base', 'success', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2007,234058,0.040820945697935755
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,7123058,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2006,611872,0.040820945697935755
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6953701,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2005,613495,0.040820945697935755
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6822280,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2004,577307,0.040820945697935755
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,7122136,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2006,39750,0.07164683411984726
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6955060,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2005,39150,0.07164683411984726
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6837265,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2004,38596,0.07164683411984726
"Bioinformatics approaches to characterizing amino acid function.    DESCRIPTION (provided by applicant): Identifying residues of importance in the protein products of genes is a challenging and important problem for informatics, genome annotation, molecular biology, biochemistry and drug discovery. Functional annotation of genes is inherently hierarchical; genes can be annotated at the level of genome sequence, transcript variant, protein product, protein domain, nucleotide or amino acid. Only a few resources annotate protein function at the level of the amino acid and language relating residue function and gene product sequence, structure and expression is challenging. To address this, I am investigating how sequence, evolutionary and structural descriptors can be used to quantify function. I am applying this knowledge to develop methods that can associate residues with known functional annotations, perform annotation transfer onto an experimentally determined or modeled protein structure, and determine the likely molecular effects of mutation, thus creating a framework for residue annotation. One of the greatest challenges for the computational biologist is identifying features (or attributes) that are useful for classification of genomic data. With this effort, we will continue our work describing novel features for classification of functional sites and we will test them using supervised machine learning tools. We will do this by, 1) testing the power of several diverse functional features for classification of catalytic residues in proteins, 2) applying these features to other important residue functional annotation problems, and 3) evaluate features based on homologous sequences. This research is important for understanding the molecular basis of diseases such as cancer and pharmacogenetics data from a molecular perspective. When completed, scientists will have a rich set of data and tools for basic health research.             n/a",Bioinformatics approaches to characterizing amino acid function.,7391729,K22LM009135,"['Address', 'Ally', 'Amino Acids', 'Biochemistry', 'Bioinformatics', 'Biology', 'Classification', 'Collaborations', 'Computational Biology', 'Computers', 'Data', 'Data Set', 'Databases', 'Descriptor', 'Disease', 'Elements', 'Evolution', 'Funding', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Hand', 'Health', 'Homologous Gene', 'Indium', 'Informatics', 'Information Systems', 'Knowledge', 'Laboratory Research', 'Language', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Mutation', 'Nucleic Acids', 'Nucleotides', 'Outcome', 'Pharmacogenetics', 'Positioning Attribute', 'Productivity', 'Proteins', 'Range', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Scientist', 'Sequence Homologs', 'Site', 'Structural Biologist', 'Structural Models', 'Structural Protein', 'Structure', 'Structure-Activity Relationship', 'Tertiary Protein Structure', 'Testing', 'Transcript', 'Variant', 'Work', 'base', 'comparative', 'computer science', 'drug discovery', 'experience', 'genome sequencing', 'innovation', 'knowledge base', 'multidisciplinary', 'novel', 'novel strategies', 'paralogous gene', 'programs', 'protein function', 'protein protein interaction', 'protein structure', 'research study', 'tool', 'web interface']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,K22,2008,158143,0.08059152311586616
"Bioinformatics approaches to characterizing amino acid function.    DESCRIPTION (provided by applicant): Identifying residues of importance in the protein products of genes is a challenging and important problem for informatics, genome annotation, molecular biology, biochemistry and drug discovery. Functional annotation of genes is inherently hierarchical; genes can be annotated at the level of genome sequence, transcript variant, protein product, protein domain, nucleotide or amino acid. Only a few resources annotate protein function at the level of the amino acid and language relating residue function and gene product sequence, structure and expression is challenging. To address this, I am investigating how sequence, evolutionary and structural descriptors can be used to quantify function. I am applying this knowledge to develop methods that can associate residues with known functional annotations, perform annotation transfer onto an experimentally determined or modeled protein structure, and determine the likely molecular effects of mutation, thus creating a framework for residue annotation. One of the greatest challenges for the computational biologist is identifying features (or attributes) that are useful for classification of genomic data. With this effort, we will continue our work describing novel features for classification of functional sites and we will test them using supervised machine learning tools. We will do this by, 1) testing the power of several diverse functional features for classification of catalytic residues in proteins, 2) applying these features to other important residue functional annotation problems, and 3) evaluate features based on homologous sequences. This research is important for understanding the molecular basis of diseases such as cancer and pharmacogenetics data from a molecular perspective. When completed, scientists will have a rich set of data and tools for basic health research.             n/a",Bioinformatics approaches to characterizing amino acid function.,7220054,K22LM009135,"['Address', 'Ally', 'Amino Acids', 'Biochemistry', 'Bioinformatics', 'Biology', 'Classification', 'Collaborations', 'Computational Biology', 'Computers', 'Data', 'Data Set', 'Databases', 'Descriptor', 'Disease', 'Elements', 'Evolution', 'Funding', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Hand', 'Health', 'Homologous Gene', 'Indium', 'Informatics', 'Information Systems', 'Knowledge', 'Laboratory Research', 'Language', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Mutation', 'Nucleic Acids', 'Nucleotides', 'Outcome', 'Pharmacogenetics', 'Positioning Attribute', 'Productivity', 'Proteins', 'Range', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Scientist', 'Sequence Homologs', 'Site', 'Structural Biologist', 'Structural Models', 'Structural Protein', 'Structure', 'Structure-Activity Relationship', 'Tertiary Protein Structure', 'Testing', 'Transcript', 'Variant', 'Work', 'base', 'comparative', 'computer science', 'drug discovery', 'experience', 'genome sequencing', 'innovation', 'knowledge base', 'multidisciplinary', 'novel', 'novel strategies', 'paralogous gene', 'programs', 'protein function', 'protein protein interaction', 'protein structure', 'research study', 'tool', 'web interface']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,K22,2007,155110,0.08059152311586616
"Bioinformatics approaches to characterizing amino acid function.    DESCRIPTION (provided by applicant): Identifying residues of importance in the protein products of genes is a challenging and important problem for informatics, genome annotation, molecular biology, biochemistry and drug discovery. Functional annotation of genes is inherently hierarchical; genes can be annotated at the level of genome sequence, transcript variant, protein product, protein domain, nucleotide or amino acid. Only a few resources annotate protein function at the level of the amino acid and language relating residue function and gene product sequence, structure and expression is challenging. To address this, I am investigating how sequence, evolutionary and structural descriptors can be used to quantify function. I am applying this knowledge to develop methods that can associate residues with known functional annotations, perform annotation transfer onto an experimentally determined or modeled protein structure, and determine the likely molecular effects of mutation, thus creating a framework for residue annotation. One of the greatest challenges for the computational biologist is identifying features (or attributes) that are useful for classification of genomic data. With this effort, we will continue our work describing novel features for classification of functional sites and we will test them using supervised machine learning tools. We will do this by, 1) testing the power of several diverse functional features for classification of catalytic residues in proteins, 2) applying these features to other important residue functional annotation problems, and 3) evaluate features based on homologous sequences. This research is important for understanding the molecular basis of diseases such as cancer and pharmacogenetics data from a molecular perspective. When completed, scientists will have a rich set of data and tools for basic health research.             n/a",Bioinformatics approaches to characterizing amino acid function.,7079558,K22LM009135,"['bioinformatics', 'classification', 'gene mutation', 'genes', 'genome', 'learning', 'model', 'protein protein interaction', 'protein structure', 'proteins']",NLM,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,K22,2006,152164,0.08059152311586616
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7840891,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,39294,0.12335107520645022
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7662449,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,167303,0.12335107520645022
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7906366,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,123630,0.12335107520645022
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7470146,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Rate', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'concept', 'drug discovery', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2008,278208,0.12335107520645022
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,8151670,R01LM009153,[' '],NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,14785,0.12335107520645022
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,8133305,R01LM009153,[' '],NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,109860,0.12335107520645022
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7260682,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Rate', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'concept', 'drug discovery', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2007,291350,0.12335107520645022
"Systematic, Genome-Scale Functional Characterization of Conserved smORFs PROJECT SUMMARY Short peptides (10-100aa) are important regulators of physiology, development and metabolism, however their detection is difficult due to size and abundance. A stunning 30% of annotated human smORF genes include disease-associated variants mapped within exons, compared to 15% of human genes in general. Further, many smORFs are conserved across the entire metazoan phylogeny from invertebrates to vertebrates including man. These ultra-conserved functional smORF genes we call the Conserved smORF Catalog or CSC. These genes have been conserved across more than 500myr of evolution, and yet we know almost nothing at all about their functions. Due to a century of genetic analysis, the genome of the model organism Drosophila melanogaster has the most complete functional annotation among metazoans. Functional annotations derived from Drosophila have been instrumental in hypothesis-based drug development for more than thirty years, and more recently have made possible the biological interpretation of hundreds of SNPs detected in genome-wide association studies (GWAS). Hence, functional annotations derived in fly for conserved genes are transferable to human and are of direct clinical relevance. Remarkably, less than 10% of smORFs in Drosophila have been studied functionally, or experimentally verified as generating peptides. A combination of genome engineering, computational, molecular, and functional studies will be used to systematically and comprehensively characterize the CSC, representing the first genome-scale characterization of smORFs in any organism providing a wealth of information on the biological functions of this poorly studied class of proteins. In total, we will characterize and functionally annotate ~400 conserved smORFs using CRISPR knockout followed by phenotyping and rescue assays. We will assess the phenotypes of the mutants, measuring viability, morphology, fecundity and fertility, lifespan, metabolism (sugar and lipid levels), and a number of behavioral phenotypes. For smORFs with robust phenotypes, we will then attempt to rescue a subset of these mutants in three ways: first, by inserting the whole deleted RNA; second, with a version of the RNA with the smORF(s) removed by the addition a stop codon; and lastly, using a micro- construct containing only the smORF and the endogenous promoter. We will generate direct evidence for translation using tagged expression analysis and targeted MS/MS to scan for predicted polypeptides in the whole embryo and tissue dissection samples. In addition to validating the existence of the predicted molecules, this dataset will provide a foundational gold standard for further development of tools for the computational prediction of functional micropeptides. These studies are directed toward the understanding of basic life processes and lay the foundation for promoting better human health. PROJECT NARRATIVE As a public resource, our studies will combine genome-scale phenotyping with detailed functional characterization that will assess the effects of evolutionary conserved small open reading frames (smORFs) on animal viability, development, fecundity, metabolism, longevity and behavior. We will apply state-of-the art methods in Ribosomal profiling, CRISPR genome engineering and targeted mass spectrometry together with the development of new computational tools and analyses to generate a foundational gold standard dataset for the study of smORFs and the prediction of functional smORFs in genome annotation. Many of the genes encoding these molecules have been found to play important roles in human diseases such as neurodegeneration, developmental disorders and cancer.","Systematic, Genome-Scale Functional Characterization of Conserved smORFs",9729028,R01HG009352,"['Adipose tissue', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Arthropods', 'Autoimmune Diseases', 'Behavior', 'Behavioral', 'Biological', 'Biological Assay', 'Biological Process', 'CRISPR/Cas technology', 'Catalogs', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'Codon Nucleotides', 'Collection', 'Computer Analysis', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Dissection', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Targeting', 'Evolution', 'Exons', 'Fertility', 'Foundations', 'Frameshift Mutation', 'Gene Transfer', 'Genes', 'Genetic Transcription', 'Genome', 'Genome engineering', 'Gold', 'Health', 'Human', 'Human Genome', 'Image', 'In Situ', 'Invertebrates', 'Knock-out', 'Life', 'Lipids', 'Literature', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Messenger RNA', 'Metabolism', 'Methods', 'Molecular', 'Morphology', 'Muscle', 'National Human Genome Research Institute', 'Nerve Degeneration', 'Nervous system structure', 'Neurodegenerative Disorders', 'Neurotransmitters', 'Ontology', 'Open Reading Frames', 'Organism', 'Peptides', 'Phenotype', 'Phylogeny', 'Physiology', 'Play', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Reproducibility', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Scanning', 'System', 'Technology', 'Terminator Codon', 'Time', 'Tissues', 'Translating', 'Translations', 'Variant', 'Vertebrates', 'adipokines', 'base', 'clinically relevant', 'computerized tools', 'developmental disease', 'drug development', 'drug resource', 'embryo tissue', 'fly', 'gene function', 'genetic analysis', 'genome annotation', 'genome wide association study', 'genome-wide', 'human disease', 'in situ imaging', 'insight', 'knock-down', 'man', 'mutant', 'novel', 'overexpression', 'polypeptide', 'promoter', 'ribosome profiling', 'sugar', 'tool', 'tool development', 'translational genomics', 'virtual']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,R01,2019,1002519,0.04063554255984629
"Systematic, Genome-Scale Functional Characterization of Conserved smORFs PROJECT SUMMARY Short peptides (10-100aa) are important regulators of physiology, development and metabolism, however their detection is difficult due to size and abundance. A stunning 30% of annotated human smORF genes include disease-associated variants mapped within exons, compared to 15% of human genes in general. Further, many smORFs are conserved across the entire metazoan phylogeny from invertebrates to vertebrates including man. These ultra-conserved functional smORF genes we call the Conserved smORF Catalog or CSC. These genes have been conserved across more than 500myr of evolution, and yet we know almost nothing at all about their functions. Due to a century of genetic analysis, the genome of the model organism Drosophila melanogaster has the most complete functional annotation among metazoans. Functional annotations derived from Drosophila have been instrumental in hypothesis-based drug development for more than thirty years, and more recently have made possible the biological interpretation of hundreds of SNPs detected in genome-wide association studies (GWAS). Hence, functional annotations derived in fly for conserved genes are transferable to human and are of direct clinical relevance. Remarkably, less than 10% of smORFs in Drosophila have been studied functionally, or experimentally verified as generating peptides. A combination of genome engineering, computational, molecular, and functional studies will be used to systematically and comprehensively characterize the CSC, representing the first genome-scale characterization of smORFs in any organism providing a wealth of information on the biological functions of this poorly studied class of proteins. In total, we will characterize and functionally annotate ~400 conserved smORFs using CRISPR knockout followed by phenotyping and rescue assays. We will assess the phenotypes of the mutants, measuring viability, morphology, fecundity and fertility, lifespan, metabolism (sugar and lipid levels), and a number of behavioral phenotypes. For smORFs with robust phenotypes, we will then attempt to rescue a subset of these mutants in three ways: first, by inserting the whole deleted RNA; second, with a version of the RNA with the smORF(s) removed by the addition a stop codon; and lastly, using a micro- construct containing only the smORF and the endogenous promoter. We will generate direct evidence for translation using tagged expression analysis and targeted MS/MS to scan for predicted polypeptides in the whole embryo and tissue dissection samples. In addition to validating the existence of the predicted molecules, this dataset will provide a foundational gold standard for further development of tools for the computational prediction of functional micropeptides. These studies are directed toward the understanding of basic life processes and lay the foundation for promoting better human health. PROJECT NARRATIVE As a public resource, our studies will combine genome-scale phenotyping with detailed functional characterization that will assess the effects of evolutionary conserved small open reading frames (smORFs) on animal viability, development, fecundity, metabolism, longevity and behavior. We will apply state-of-the art methods in Ribosomal profiling, CRISPR genome engineering and targeted mass spectrometry together with the development of new computational tools and analyses to generate a foundational gold standard dataset for the study of smORFs and the prediction of functional smORFs in genome annotation. Many of the genes encoding these molecules have been found to play important roles in human diseases such as neurodegeneration, developmental disorders and cancer.","Systematic, Genome-Scale Functional Characterization of Conserved smORFs",9548692,R01HG009352,"['Adipose tissue', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Arthropods', 'Autoimmune Diseases', 'Behavior', 'Behavioral', 'Biological', 'Biological Assay', 'Biological Process', 'CRISPR/Cas technology', 'Catalogs', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'Codon Nucleotides', 'Collection', 'Computer Analysis', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Dissection', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Targeting', 'Evolution', 'Exons', 'Fertility', 'Foundations', 'Frameshift Mutation', 'Gene Transfer', 'Genes', 'Genetic Transcription', 'Genome', 'Genome engineering', 'Gold', 'Health', 'Human', 'Human Genome', 'Image', 'In Situ', 'Invertebrates', 'Knock-out', 'Life', 'Lipids', 'Literature', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Messenger RNA', 'Metabolism', 'Methods', 'Molecular', 'Morphology', 'Muscle', 'National Human Genome Research Institute', 'Nerve Degeneration', 'Nervous system structure', 'Neurodegenerative Disorders', 'Neurotransmitters', 'Ontology', 'Open Reading Frames', 'Organism', 'Peptides', 'Phenotype', 'Phylogeny', 'Physiology', 'Play', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Reproducibility', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Scanning', 'System', 'Technology', 'Terminator Codon', 'Time', 'Tissues', 'Translating', 'Translations', 'Variant', 'Vertebrates', 'adipokines', 'base', 'clinically relevant', 'computerized tools', 'developmental disease', 'drug development', 'drug resource', 'embryo tissue', 'fly', 'gene function', 'genetic analysis', 'genome annotation', 'genome wide association study', 'genome-wide', 'human disease', 'in situ imaging', 'insight', 'knock-down', 'man', 'mutant', 'novel', 'overexpression', 'polypeptide', 'promoter', 'ribosome profiling', 'sugar', 'tool', 'tool development', 'translational genomics', 'virtual']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,R01,2018,1002519,0.04063554255984629
"Systematic, Genome-Scale Functional Characterization of Conserved smORFs PROJECT SUMMARY Short peptides (10-100aa) are important regulators of physiology, development and metabolism, however their detection is difficult due to size and abundance. A stunning 30% of annotated human smORF genes include disease-associated variants mapped within exons, compared to 15% of human genes in general. Further, many smORFs are conserved across the entire metazoan phylogeny from invertebrates to vertebrates including man. These ultra-conserved functional smORF genes we call the Conserved smORF Catalog or CSC. These genes have been conserved across more than 500myr of evolution, and yet we know almost nothing at all about their functions. Due to a century of genetic analysis, the genome of the model organism Drosophila melanogaster has the most complete functional annotation among metazoans. Functional annotations derived from Drosophila have been instrumental in hypothesis-based drug development for more than thirty years, and more recently have made possible the biological interpretation of hundreds of SNPs detected in genome-wide association studies (GWAS). Hence, functional annotations derived in fly for conserved genes are transferable to human and are of direct clinical relevance. Remarkably, less than 10% of smORFs in Drosophila have been studied functionally, or experimentally verified as generating peptides. A combination of genome engineering, computational, molecular, and functional studies will be used to systematically and comprehensively characterize the CSC, representing the first genome-scale characterization of smORFs in any organism providing a wealth of information on the biological functions of this poorly studied class of proteins. In total, we will characterize and functionally annotate ~400 conserved smORFs using CRISPR knockout followed by phenotyping and rescue assays. We will assess the phenotypes of the mutants, measuring viability, morphology, fecundity and fertility, lifespan, metabolism (sugar and lipid levels), and a number of behavioral phenotypes. For smORFs with robust phenotypes, we will then attempt to rescue a subset of these mutants in three ways: first, by inserting the whole deleted RNA; second, with a version of the RNA with the smORF(s) removed by the addition a stop codon; and lastly, using a micro- construct containing only the smORF and the endogenous promoter. We will generate direct evidence for translation using tagged expression analysis and targeted MS/MS to scan for predicted polypeptides in the whole embryo and tissue dissection samples. In addition to validating the existence of the predicted molecules, this dataset will provide a foundational gold standard for further development of tools for the computational prediction of functional micropeptides. These studies are directed toward the understanding of basic life processes and lay the foundation for promoting better human health. PROJECT NARRATIVE As a public resource, our studies will combine genome-scale phenotyping with detailed functional characterization that will assess the effects of evolutionary conserved small open reading frames (smORFs) on animal viability, development, fecundity, metabolism, longevity and behavior. We will apply state-of-the art methods in Ribosomal profiling, CRISPR genome engineering and targeted mass spectrometry together with the development of new computational tools and analyses to generate a foundational gold standard dataset for the study of smORFs and the prediction of functional smORFs in genome annotation. Many of the genes encoding these molecules have been found to play important roles in human diseases such as neurodegeneration, developmental disorders and cancer.","Systematic, Genome-Scale Functional Characterization of Conserved smORFs",9228843,R01HG009352,"['Adipose tissue', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Arthropods', 'Autoimmune Diseases', 'Behavior', 'Behavioral', 'Biological', 'Biological Assay', 'Biological Process', 'CRISPR/Cas technology', 'Catalogs', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'Codon Nucleotides', 'Collection', 'Computer Analysis', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Dissection', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Targeting', 'Evolution', 'Exons', 'Fertility', 'Foundations', 'Frameshift Mutation', 'Gene Transfer', 'Genes', 'Genetic Transcription', 'Genome', 'Genome engineering', 'Gold', 'Health', 'Human', 'Human Genome', 'Image', 'In Situ', 'Invertebrates', 'Knock-out', 'Life', 'Lipids', 'Literature', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Messenger RNA', 'Metabolism', 'Methods', 'Molecular', 'Morphology', 'Muscle', 'National Human Genome Research Institute', 'Nerve Degeneration', 'Nervous system structure', 'Neurodegenerative Disorders', 'Neurotransmitters', 'Ontology', 'Open Reading Frames', 'Organism', 'Peptides', 'Phenotype', 'Phylogeny', 'Physiology', 'Play', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Reproducibility', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Scanning', 'System', 'Technology', 'Terminator Codon', 'Time', 'Tissues', 'Translating', 'Translations', 'Variant', 'Vertebrates', 'adipokines', 'base', 'clinically relevant', 'computerized tools', 'developmental disease', 'drug development', 'drug resource', 'embryo tissue', 'fly', 'gene function', 'genetic analysis', 'genome annotation', 'genome wide association study', 'genome-wide', 'human disease', 'in situ imaging', 'insight', 'knock-down', 'man', 'mutant', 'novel', 'overexpression', 'polypeptide', 'promoter', 'ribosome profiling', 'sugar', 'tool', 'tool development', 'translational genomics', 'virtual']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,R01,2017,1002725,0.04063554255984629
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9534738,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Competence', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Manuals', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Readability', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'annotation  system', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'knowledge base', 'novel', 'ontology development', 'personalized medicine', 'repository', 'response', 'stem', 'tool', 'translational pipeline']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2018,431097,0.07732383791855606
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9360131,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Competence', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Manuals', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Readability', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'annotation  system', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'knowledge base', 'novel', 'permissiveness', 'personalized medicine', 'repository', 'response', 'stem', 'tool', 'translational pipeline']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2017,456710,0.07732383791855606
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9161167,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Maps', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'information processing', 'knowledge base', 'novel', 'personalized medicine', 'repository', 'response', 'stem', 'tool']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2016,471848,0.07732383791855606
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7673720,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2009,142851,0.03720088379682208
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7872692,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2009,66015,0.03720088379682208
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7495148,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Facility Construction Funding Category', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Numbers', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Rate', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Standards of Weights and Measures', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Today', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2008,132030,0.03720088379682208
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7301251,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Facility Construction Funding Category', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Numbers', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Rate', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Standards of Weights and Measures', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Today', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2007,130432,0.03720088379682208
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7693803,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Classification', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2009,280000,0.06424477549820892
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7467204,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Classification', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Personal Satisfaction', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Today', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'concept', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2008,280000,0.06424477549820892
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,8138486,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2011,266112,0.06424477549820892
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7929664,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2010,277200,0.06424477549820892
"Beyond information extraction: Identifying Gene Ontology concepts in text    DESCRIPTION (provided by applicant):       There has been growing interest in recent years in developing methods that automatically identify Gene Ontology (GO) concepts in the unstructured text of scientific articles. This interest is motivated in part by the need to automate the task of model-organism database curation. In addition, however, methods that automatically identify GO concepts in text will enable data mining tools that compile and interpret information extracted from text, tools that will benefit a large number of people across the scientific enterprise. This project builds on recently completed work in which we used the literature of S. cerevisiae and annotations in the Saccharomyces Genome Database (SGD) to develop methods that determine what molecular function claims are being made in an article and what experimental evidence there is in the article for those claims. The data generated in this project contains a wealth of information that could lead to greatly improved methods for identifying GO concepts in text. The specific aims of this project are: (1) to develop a representation for GO molecular function concepts that captures information not only about the language of a GO term but also the biomedical entity the term refers to; and (2) to analyze the results of the S. cerevisiae data mining project using the GO representations formulated in (1) to determine which are likely to produce improved GO term recognition. The analysis will be performed on 276 true positive results, 29,276 false positive results, and 336 false negative results to see if a new GO concept representation can reduce the number of false positives or false negatives without losing any true positives. The data mining tools of this proposal can be extended to ontologies other than GO, thereby leveraging the effort expended on ontology development.           n/a",Beyond information extraction: Identifying Gene Ontology concepts in text,7918188,R03LM009752,"['Biological Assay', 'Biological Process', 'Controlled Vocabulary', 'Data', 'Data Set', 'Databases', 'Development', 'Ensure', 'Frequencies', 'Genes', 'Individual', 'Information Retrieval', 'Language', 'Lead', 'Literature', 'Methods', 'Molecular', 'Ontology', 'Problem Solving', 'Proteins', 'Reporting', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Text', 'Work', 'base', 'biomedical scientist', 'data mining', 'gene correction', 'gene function', 'genome database', 'improved', 'interest', 'model organisms databases', 'natural language', 'phrases', 'success', 'text searching', 'tool']",NLM,"CONVERSPEECH, LLC",R03,2010,50000,0.08620244178325953
"Beyond information extraction: Identifying Gene Ontology concepts in text    DESCRIPTION (provided by applicant):       There has been growing interest in recent years in developing methods that automatically identify Gene Ontology (GO) concepts in the unstructured text of scientific articles. This interest is motivated in part by the need to automate the task of model-organism database curation. In addition, however, methods that automatically identify GO concepts in text will enable data mining tools that compile and interpret information extracted from text, tools that will benefit a large number of people across the scientific enterprise. This project builds on recently completed work in which we used the literature of S. cerevisiae and annotations in the Saccharomyces Genome Database (SGD) to develop methods that determine what molecular function claims are being made in an article and what experimental evidence there is in the article for those claims. The data generated in this project contains a wealth of information that could lead to greatly improved methods for identifying GO concepts in text. The specific aims of this project are: (1) to develop a representation for GO molecular function concepts that captures information not only about the language of a GO term but also the biomedical entity the term refers to; and (2) to analyze the results of the S. cerevisiae data mining project using the GO representations formulated in (1) to determine which are likely to produce improved GO term recognition. The analysis will be performed on 276 true positive results, 29,276 false positive results, and 336 false negative results to see if a new GO concept representation can reduce the number of false positives or false negatives without losing any true positives. The data mining tools of this proposal can be extended to ontologies other than GO, thereby leveraging the effort expended on ontology development.           n/a",Beyond information extraction: Identifying Gene Ontology concepts in text,7362877,R03LM009752,"['Biological Assay', 'Biological Process', 'Controlled Vocabulary', 'Data', 'Data Set', 'Databases', 'Development', 'Ensure', 'Frequencies', 'Genes', 'Individual', 'Information Retrieval', 'Language', 'Lead', 'Literature', 'Methods', 'Molecular', 'Ontology', 'Problem Solving', 'Proteins', 'Reporting', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Text', 'Work', 'base', 'biomedical scientist', 'data mining', 'gene correction', 'gene function', 'genome database', 'improved', 'interest', 'model organisms databases', 'natural language', 'phrases', 'success', 'text searching', 'tool']",NLM,"CONVERSPEECH, LLC",R03,2009,50000,0.08620244178325953
"Advancing Literature Mining through Image Processing and Analysis    DESCRIPTION (provided by applicant): We propose to advance biomedical literature mining by providing new technologies for searching and retrieving biomedical images. The content of these images is a good representation of the research results discussed in scientific articles, and often contains additional facts not explicitly mentioned in the article or image caption. While the inherent structure in biomedical images facilitates automated image content extraction, the extraction process can be made more accurate by concurrent processing of text and images. Text-enhanced image analysis results in richly annotated images, which open up new possibilities for locating images of interest. The specific aims are to 1) develop methods for extracting structured image content through image processing and analysis, to 2) design methods to boost accuracy of image understanding through concurrent processing of text and images, to 3) devise methods for searching across structured image content, and 4) to develop an image search tool that demonstrates the power of using structured Image content for accessing the biomedical literature.           We propose to advance our ability to search Millions of published research articles by looking into the images within those articles. Our methodology understands the text and layout within images, allowing for very precise image searches. The methodology should have a broad impact on our ability to access the biological and medical literature.",Advancing Literature Mining through Image Processing and Analysis,8138362,R01LM009956,"['Biological', 'Biological Sciences', 'Health Professional', 'Image', 'Image Analysis', 'Literature', 'Medical', 'Methodology', 'Methods', 'Performance', 'Process', 'Publishing', 'Research', 'Research Personnel', 'Structure', 'Text', 'bioimaging', 'design', 'image processing', 'improved', 'interest', 'new technology', 'text searching', 'tool']",NLM,YALE UNIVERSITY,R01,2011,321995,0.029109441369985105
"Advancing Literature Mining through Image Processing and Analysis    DESCRIPTION (provided by applicant): We propose to advance biomedical literature mining by providing new technologies for searching and retrieving biomedical images. The content of these images is a good representation of the research results discussed in scientific articles, and often contains additional facts not explicitly mentioned in the article or image caption. While the inherent structure in biomedical images facilitates automated image content extraction, the extraction process can be made more accurate by concurrent processing of text and images. Text-enhanced image analysis results in richly annotated images, which open up new possibilities for locating images of interest. The specific aims are to 1) develop methods for extracting structured image content through image processing and analysis, to 2) design methods to boost accuracy of image understanding through concurrent processing of text and images, to 3) devise methods for searching across structured image content, and 4) to develop an image search tool that demonstrates the power of using structured Image content for accessing the biomedical literature.           We propose to advance our ability to search Millions of published research articles by looking into the images within those articles. Our methodology understands the text and layout within images, allowing for very precise image searches. The methodology should have a broad impact on our ability to access the biological and medical literature.",Advancing Literature Mining through Image Processing and Analysis,7727854,R01LM009956,"['Biological', 'Biological Sciences', 'Health Professional', 'Image', 'Image Analysis', 'Literature', 'Medical', 'Methodology', 'Methods', 'Performance', 'Process', 'Publishing', 'Research', 'Research Personnel', 'Structure', 'Text', 'bioimaging', 'design', 'image processing', 'improved', 'interest', 'new technology', 'text searching', 'tool']",NLM,YALE UNIVERSITY,R01,2009,338835,0.029109441369985105
"Advancing Literature Mining through Image Processing and Analysis    DESCRIPTION (provided by applicant): We propose to advance biomedical literature mining by providing new technologies for searching and retrieving biomedical images. The content of these images is a good representation of the research results discussed in scientific articles, and often contains additional facts not explicitly mentioned in the article or image caption. While the inherent structure in biomedical images facilitates automated image content extraction, the extraction process can be made more accurate by concurrent processing of text and images. Text-enhanced image analysis results in richly annotated images, which open up new possibilities for locating images of interest. The specific aims are to 1) develop methods for extracting structured image content through image processing and analysis, to 2) design methods to boost accuracy of image understanding through concurrent processing of text and images, to 3) devise methods for searching across structured image content, and 4) to develop an image search tool that demonstrates the power of using structured Image content for accessing the biomedical literature.           We propose to advance our ability to search Millions of published research articles by looking into the images within those articles. Our methodology understands the text and layout within images, allowing for very precise image searches. The methodology should have a broad impact on our ability to access the biological and medical literature.",Advancing Literature Mining through Image Processing and Analysis,7921438,R01LM009956,"['Biological', 'Biological Sciences', 'Health Professional', 'Image', 'Image Analysis', 'Literature', 'Medical', 'Methodology', 'Methods', 'Performance', 'Process', 'Publishing', 'Research', 'Research Personnel', 'Structure', 'Text', 'bioimaging', 'design', 'image processing', 'improved', 'interest', 'new technology', 'text searching', 'tool']",NLM,YALE UNIVERSITY,R01,2010,335430,0.029109441369985105
"Onto-BioThesaurus: ontological representation of gene/protein names for biomedica    DESCRIPTION (provided by applicant):       The long-term goal of our research is to develop resources and tools for knowledge retrieval management in the biomedical domain. As the pace of biomedical research accelerates, researchers become more and more dependent on computers to manage the explosive amount of biomedical information being published. The high quality of many databases is guaranteed by database curators who extract and synthesize information stored in literature or other databases. It is important to accurately recognize biomedical entity names in text and map the identified names to corresponding records in biomedical databases. Usually, a biomedical database provides a list of names either entered by curators or extracted from other databases. Those names could be used to retrieve records from databases or map names to database records by NLP systems. However, there are several characteristics associated with biomedical entity names, namely: synonymy (i.e., different names refer to the same database entry), ambiguity (i.e., one name is associated with different entries), and novelty (i.e., names or entities are not present in databases or knowledge bases) which make the task of retrieving database records using names and the task of associating names in text to database records very daunting. Additionally, biomedical entities can appear in text as short forms (SFs) abbreviated from their long forms (LFs). The prevalent use of SFs representing biomedical entities is another challenge faced by end users and NLP applications because of the high ambiguity of SFs.       Recently, ontology-based knowledge management is becoming increasingly popular since ontologies provide formal, machine-processable, and human-interpretable representations of the biomedical entities and their relations. We hypothesize that biomedical ontologies can be used to reduce the difficulty associated with retrieving records using names or mapping names in text to database records. Specific aims and the corresponding hypotheses are: i) develop onto-BioThesaurus by enriching BioThesaurus with gene/protein-related ontologies (Hypothesis: aligning gene/protein names to gene/protein-related ontologies can reduce the complexity associated with gene/protein names); ii) harvest synonyms for gene/protein classes and entities from online resources and text (Hypothesis: harvesting synonyms especially gene/protein SFs is critical since SFs are frequently used to represent gene/protein entities); iii) build a web user interface for gene/protein names and entries search and query through ontology-enabled onto-BioThesaurus (Hypothesis: enhancing BioThesaurus with gene/protein-related ontologies would enable us to build heuristic rules to enable machine reasoning); and iv) evaluate and distribute research methods/outcome (Hypothesis: evaluating and distributing research methods/outcome are critical to advance both basic and applied biomedical science.            The proposed research is critical for biomedical knowledge retrieval and management. It serves as one of the foundation for storing, retrieving, and extracting knowledge and information in the biomedical domain. Additionally, the proposed research will benefit biomedical researchers and general community for understanding and managing biomedical text through web interfaces and automated systems.",Onto-BioThesaurus: ontological representation of gene/protein names for biomedica,7654995,R01LM009959,"['Abbreviations', 'Biomedical Research', 'Characteristics', 'Communities', 'Computers', 'Databases', 'Expert Opinion', 'Foundations', 'Gene Proteins', 'Genes', 'Goals', 'Harvest', 'Human', 'Information Resources', 'Information Resources Management', 'Internet', 'Investigation', 'Knowledge', 'Literature', 'Manuals', 'Maps', 'Names', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Outcome', 'Peer Review', 'Process', 'Proteins', 'Publishing', 'Records', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Retrieval', 'Review Literature', 'Science', 'Services', 'System', 'Techniques', 'Terminology', 'Text', 'Thesauri', 'Time', 'acronyms', 'base', 'biomedical ontology', 'heuristics', 'knowledge base', 'tool', 'web interface', 'web site']",NLM,GEORGETOWN UNIVERSITY,R01,2009,608650,0.09121833023936568
"Services to support the OBO foundry standards PROJECT SUMMARY/ABSTRACT The Open Biomedical Ontologies (OBO) Foundry is a community organized project that aims to facilitate the discovery, development, application, harmonization, interoperability and sharing of ontologies. OBO ontologies are widely used as a standard for knowledge representation in a broad range of domains relevant to the NIH basic, translational, and clinical research mission. They cover for example gene functions (Gene Ontology, GO), diseases and phenotypes (Disease Ontology, DO; Human Phenotype Ontology, HP; Uberon Anatomy Ontology), assay types (Ontology for Biomedical Investigations, OBI), and relationships connecting these entities, both within and across ontologies (Relationship Ontology, RO). For over a decade, OBO has provided a variety of widely used services to the community, including 1) the OBO registry, which collects standardized metadata for each member ontology such as domain, scope, license, point of contact, etc., thus enhancing findability of each ontology, 2) configurable Persistent URLs (PURLs), which provide an unchanging address for ontologies and ontology classes, enhancing accessibility and reuse, 3) optional content hosting for OBO ontologies that want to take advantage of centrally managed versioning, 4) standardized software tools that make it easy to follow best practices for ontology development, 5) OBO principles, guidelines and best practices, designed to enhance quality and interoperability, and 6) an outreach and coordination effort to mediate between different member ontologies with the goal to coordinate and reduce overlap, through a central email list and regular teleconferences. The present proposal will provide time-limited, catalytic support to increase robustness of existing services by upgrading critical infrastructure on which they are based, by defining and applying standards for metadata and PURL management, and by applying best practices from software engineering to increase maintainability of the codebase. Second, we will introduce new capabilities into our service infrastructure to add automated quality controls to ontologies (individually and in groups), enhance the integration between our services and standardized tools, and add monitoring and metrics. Third, we will perform outreach and training efforts to increase the ability of the community to participate in the OBO project. In accomplishing these aims we will continue to keep our operating costs extremely low, and to freely share the tools and resources we build. PROJECT NARRATIVE/PUBLIC HEALTH RELEVANCE Ontologies provide a precise language to describe scientific findings, and have successfully been used by researchers to share and compare their results. Different ontologies describe different aspects of knowledge, sometimes in an incompatible way. Here we propose to implement a set of services that allows different ontology producers to develop ontologies in a standardized fashion that will make it easier to use them together.",Services to support the OBO foundry standards,9568005,R24HG010032,"['Address', 'Anatomy', 'Basic Science', 'Biological Assay', 'Biological Process', 'Biology', 'Clinical Research', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Development', 'Disease', 'Documentation', 'Education and Outreach', 'Electronic Mail', 'Ensure', 'Funding', 'Gene Components', 'Genes', 'Goals', 'Guidelines', 'Human', 'Individual', 'Investigation', 'Investments', 'Knowledge', 'Language', 'Licensing', 'Maintenance', 'Measures', 'Mediating', 'Metadata', 'Mission', 'Molecular', 'Monitor', 'Ontology', 'Phenotype', 'Publishing', 'Quality Control', 'Registries', 'Reporting', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Services', 'Software Engineering', 'Software Tools', 'Standardization', 'System', 'Teleconferences', 'Testing', 'Time', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Update', 'base', 'biomedical ontology', 'cost', 'design', 'disease phenotype', 'experience', 'gene function', 'gene product', 'improved', 'information organization', 'interoperability', 'member', 'ontology development', 'open source', 'outreach', 'public health relevance', 'success', 'tool', 'volunteer']",NHGRI,LA JOLLA INSTITUTE FOR IMMUNOLOGY,R24,2018,464271,0.18693747055441925
"Services to support the OBO foundry standards PROJECT SUMMARY/ABSTRACT The Open Biomedical Ontologies (OBO) Foundry is a community organized project that aims to facilitate the discovery, development, application, harmonization, interoperability and sharing of ontologies. OBO ontologies are widely used as a standard for knowledge representation in a broad range of domains relevant to the NIH basic, translational, and clinical research mission. They cover for example gene functions (Gene Ontology, GO), diseases and phenotypes (Disease Ontology, DO; Human Phenotype Ontology, HP; Uberon Anatomy Ontology), assay types (Ontology for Biomedical Investigations, OBI), and relationships connecting these entities, both within and across ontologies (Relationship Ontology, RO). For over a decade, OBO has provided a variety of widely used services to the community, including 1) the OBO registry, which collects standardized metadata for each member ontology such as domain, scope, license, point of contact, etc., thus enhancing findability of each ontology, 2) configurable Persistent URLs (PURLs), which provide an unchanging address for ontologies and ontology classes, enhancing accessibility and reuse, 3) optional content hosting for OBO ontologies that want to take advantage of centrally managed versioning, 4) standardized software tools that make it easy to follow best practices for ontology development, 5) OBO principles, guidelines and best practices, designed to enhance quality and interoperability, and 6) an outreach and coordination effort to mediate between different member ontologies with the goal to coordinate and reduce overlap, through a central email list and regular teleconferences. The present proposal will provide time-limited, catalytic support to increase robustness of existing services by upgrading critical infrastructure on which they are based, by defining and applying standards for metadata and PURL management, and by applying best practices from software engineering to increase maintainability of the codebase. Second, we will introduce new capabilities into our service infrastructure to add automated quality controls to ontologies (individually and in groups), enhance the integration between our services and standardized tools, and add monitoring and metrics. Third, we will perform outreach and training efforts to increase the ability of the community to participate in the OBO project. In accomplishing these aims we will continue to keep our operating costs extremely low, and to freely share the tools and resources we build. PROJECT NARRATIVE/PUBLIC HEALTH RELEVANCE Ontologies provide a precise language to describe scientific findings, and have successfully been used by researchers to share and compare their results. Different ontologies describe different aspects of knowledge, sometimes in an incompatible way. Here we propose to implement a set of services that allows different ontology producers to develop ontologies in a standardized fashion that will make it easier to use them together.",Services to support the OBO foundry standards,9385259,R24HG010032,"['Address', 'Anatomy', 'Basic Science', 'Biological Assay', 'Biological Process', 'Biology', 'Clinical Research', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Development', 'Disease', 'Documentation', 'Education and Outreach', 'Electronic Mail', 'Ensure', 'Funding', 'Gene Components', 'Genes', 'Goals', 'Guidelines', 'Human', 'Individual', 'Investigation', 'Investments', 'Knowledge', 'Language', 'Licensing', 'Maintenance', 'Measures', 'Mediating', 'Metadata', 'Mission', 'Molecular', 'Monitor', 'Ontology', 'Phenotype', 'Publishing', 'Quality Control', 'Registries', 'Reporting', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Services', 'Software Engineering', 'Software Tools', 'Standardization', 'System', 'Teleconferences', 'Testing', 'Time', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Update', 'base', 'biomedical ontology', 'cost', 'design', 'disease phenotype', 'experience', 'gene function', 'gene product', 'improved', 'information organization', 'interoperability', 'member', 'open source', 'outreach', 'public health relevance', 'success', 'tool', 'volunteer']",NHGRI,LA JOLLA INSTITUTE FOR IMMUNOLOGY,R24,2017,484515,0.18693747055441925
"Services to support the OBO foundry standards PROJECT SUMMARY/ABSTRACT The Open Biomedical Ontologies (OBO) Foundry is a community organized project that aims to facilitate the discovery, development, application, harmonization, interoperability and sharing of ontologies. OBO ontologies are widely used as a standard for knowledge representation in a broad range of domains relevant to the NIH basic, translational, and clinical research mission. They cover for example gene functions (Gene Ontology, GO), diseases and phenotypes (Disease Ontology, DO; Human Phenotype Ontology, HP; Uberon Anatomy Ontology), assay types (Ontology for Biomedical Investigations, OBI), and relationships connecting these entities, both within and across ontologies (Relationship Ontology, RO). For over a decade, OBO has provided a variety of widely used services to the community, including 1) the OBO registry, which collects standardized metadata for each member ontology such as domain, scope, license, point of contact, etc., thus enhancing findability of each ontology, 2) configurable Persistent URLs (PURLs), which provide an unchanging address for ontologies and ontology classes, enhancing accessibility and reuse, 3) optional content hosting for OBO ontologies that want to take advantage of centrally managed versioning, 4) standardized software tools that make it easy to follow best practices for ontology development, 5) OBO principles, guidelines and best practices, designed to enhance quality and interoperability, and 6) an outreach and coordination effort to mediate between different member ontologies with the goal to coordinate and reduce overlap, through a central email list and regular teleconferences. The present proposal will provide time-limited, catalytic support to increase robustness of existing services by upgrading critical infrastructure on which they are based, by defining and applying standards for metadata and PURL management, and by applying best practices from software engineering to increase maintainability of the codebase. Second, we will introduce new capabilities into our service infrastructure to add automated quality controls to ontologies (individually and in groups), enhance the integration between our services and standardized tools, and add monitoring and metrics. Third, we will perform outreach and training efforts to increase the ability of the community to participate in the OBO project. In accomplishing these aims we will continue to keep our operating costs extremely low, and to freely share the tools and resources we build. PROJECT NARRATIVE/PUBLIC HEALTH RELEVANCE Ontologies provide a precise language to describe scientific findings, and have successfully been used by researchers to share and compare their results. Different ontologies describe different aspects of knowledge, sometimes in an incompatible way. Here we propose to implement a set of services that allows different ontology producers to develop ontologies in a standardized fashion that will make it easier to use them together.",Services to support the OBO foundry standards,9731614,R24HG010032,"['Address', 'Anatomy', 'Basic Science', 'Biological Assay', 'Biological Process', 'Biology', 'Clinical Research', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Development', 'Disease', 'Documentation', 'Education and Outreach', 'Electronic Mail', 'Ensure', 'Funding', 'Genes', 'Goals', 'Guidelines', 'Human', 'Individual', 'Infrastructure', 'Investigation', 'Investments', 'Knowledge', 'Language', 'Licensing', 'Maintenance', 'Measures', 'Mediating', 'Metadata', 'Mission', 'Molecular', 'Monitor', 'Ontology', 'Phenotype', 'Publishing', 'Quality Control', 'Registries', 'Reporting', 'Research Personnel', 'Resources', 'Running', 'Services', 'Software Engineering', 'Software Tools', 'Standardization', 'System', 'Teleconferences', 'Testing', 'Time', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Update', 'base', 'biomedical ontology', 'cost', 'design', 'disease phenotype', 'experience', 'gene function', 'gene product', 'improved', 'information organization', 'interoperability', 'member', 'ontology development', 'open source', 'outreach', 'public health relevance', 'success', 'tool', 'volunteer']",NHGRI,LA JOLLA INSTITUTE FOR IMMUNOLOGY,R24,2019,464271,0.18693747055441925
"Automated Literature Mining for Validation of High-Throughput Function Prediction    DESCRIPTION (provided by applicant): The function of millions of proteins remains unknown, and automated protein function prediction systems have a poor record of performance. We will test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques through a novel automated system that will mine the literature for targeted information relevant to those predictions. The impact of our work will be to enable large-scale, validated, annotation of protein function and in turn to facilitate progress in tackling drug discovery for treatment of diseases.       High-throughput experiments and bioinformatics techniques are creating an exploding volume of data with which we hope to transcribe the genetic blueprints of life. Targeted experiments are required to validate biomedical discoveries from these sources. Fortunately, the information to confirm or refute a prediction is often already available in an existing publication and the biologist can take advantage of this supporting evidence for validation. However, the sheer volume of predictions from high throughput methods exceeds the capacity of researchers to perform even the necessary literature searches. This gap in capacity must be addressed using automated literature mining methods that perform comparably to a human expert; indeed, development of such methods is a grand challenge of modern Biology.       We will mine the full text literature to validate computational predictions of functional sites in proteins. The innovations in our approach include: (1) using computational predictions as the context for a literature search; (2) information extraction of protein functional sites from full text journal publications; (3) high-throughput text mining; and (4) using primary information in protein databases to evaluate the methods.       Understanding of protein function is a critical bottleneck in the progress of biomedical research. It is time to truly integrate the biological literature into the protein function prediction problem. By doing so, we will enable a critical advance in high-throughput protein function prediction           Project Narrative The goals of this research are to test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques. Our approach is to develop a revolutionary system that will automatically mine the literature for targeted information relevant to those predictions. We will produce reliable protein functional site predictions that can in turn be exploited for in silico high- throughput drug design.",Automated Literature Mining for Validation of High-Throughput Function Prediction,7843633,R01LM010120,"['Address', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Computational Biology', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Drug Design', 'Genetic', 'Goals', 'Human', 'Journals', 'Life', 'Literature', 'Methods', 'Mining', 'Performance', 'Protein Databases', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Site', 'Source', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Validation', 'Work', 'drug discovery', 'innovation', 'novel', 'protein function', 'research study', 'text searching']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,711389,0.0527437445655401
"Automated Literature Mining for Validation of High-Throughput Function Prediction    DESCRIPTION (provided by applicant): The function of millions of proteins remains unknown, and automated protein function prediction systems have a poor record of performance. We will test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques through a novel automated system that will mine the literature for targeted information relevant to those predictions. The impact of our work will be to enable large-scale, validated, annotation of protein function and in turn to facilitate progress in tackling drug discovery for treatment of diseases.       High-throughput experiments and bioinformatics techniques are creating an exploding volume of data with which we hope to transcribe the genetic blueprints of life. Targeted experiments are required to validate biomedical discoveries from these sources. Fortunately, the information to confirm or refute a prediction is often already available in an existing publication and the biologist can take advantage of this supporting evidence for validation. However, the sheer volume of predictions from high throughput methods exceeds the capacity of researchers to perform even the necessary literature searches. This gap in capacity must be addressed using automated literature mining methods that perform comparably to a human expert; indeed, development of such methods is a grand challenge of modern Biology.       We will mine the full text literature to validate computational predictions of functional sites in proteins. The innovations in our approach include: (1) using computational predictions as the context for a literature search; (2) information extraction of protein functional sites from full text journal publications; (3) high-throughput text mining; and (4) using primary information in protein databases to evaluate the methods.       Understanding of protein function is a critical bottleneck in the progress of biomedical research. It is time to truly integrate the biological literature into the protein function prediction problem. By doing so, we will enable a critical advance in high-throughput protein function prediction           Project Narrative The goals of this research are to test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques. Our approach is to develop a revolutionary system that will automatically mine the literature for targeted information relevant to those predictions. We will produce reliable protein functional site predictions that can in turn be exploited for in silico high- throughput drug design.",Automated Literature Mining for Validation of High-Throughput Function Prediction,8144625,R01LM010120,"['Address', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Computational Biology', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Drug Design', 'Genetic', 'Goals', 'Human', 'Journals', 'Life', 'Literature', 'Methods', 'Mining', 'Performance', 'Protein Databases', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Site', 'Source', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Validation', 'Work', 'drug discovery', 'innovation', 'novel', 'protein function', 'research study', 'text searching']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,80113,0.0527437445655401
"Automated Literature Mining for Validation of High-Throughput Function Prediction    DESCRIPTION (provided by applicant): The function of millions of proteins remains unknown, and automated protein function prediction systems have a poor record of performance. We will test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques through a novel automated system that will mine the literature for targeted information relevant to those predictions. The impact of our work will be to enable large-scale, validated, annotation of protein function and in turn to facilitate progress in tackling drug discovery for treatment of diseases.       High-throughput experiments and bioinformatics techniques are creating an exploding volume of data with which we hope to transcribe the genetic blueprints of life. Targeted experiments are required to validate biomedical discoveries from these sources. Fortunately, the information to confirm or refute a prediction is often already available in an existing publication and the biologist can take advantage of this supporting evidence for validation. However, the sheer volume of predictions from high throughput methods exceeds the capacity of researchers to perform even the necessary literature searches. This gap in capacity must be addressed using automated literature mining methods that perform comparably to a human expert; indeed, development of such methods is a grand challenge of modern Biology.       We will mine the full text literature to validate computational predictions of functional sites in proteins. The innovations in our approach include: (1) using computational predictions as the context for a literature search; (2) information extraction of protein functional sites from full text journal publications; (3) high-throughput text mining; and (4) using primary information in protein databases to evaluate the methods.       Understanding of protein function is a critical bottleneck in the progress of biomedical research. It is time to truly integrate the biological literature into the protein function prediction problem. By doing so, we will enable a critical advance in high-throughput protein function prediction           Project Narrative The goals of this research are to test hypotheses about protein functional sites by validating high-throughput predictions derived from computational biology techniques. Our approach is to develop a revolutionary system that will automatically mine the literature for targeted information relevant to those predictions. We will produce reliable protein functional site predictions that can in turn be exploited for in silico high- throughput drug design.",Automated Literature Mining for Validation of High-Throughput Function Prediction,7724794,R01LM010120,"['Address', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Research', 'Computational Biology', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Drug Design', 'Genetic', 'Goals', 'Human', 'Journals', 'Life', 'Literature', 'Methods', 'Mining', 'Performance', 'Protein Databases', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Site', 'Source', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Validation', 'Work', 'drug discovery', 'innovation', 'novel', 'protein function', 'research study', 'text searching']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2009,721448,0.0527437445655401
"Dfam: sustainable growth, curation support, and improved quality for mobile element annotation Project Summary / Abstract Repetitive DNA, especially that due to transposable elements (TEs), makes up a large fraction of many genomes. Thorough and accurate annotation of repetitive content in genomes depends on a comprehensive database of known TEs, along with robust statistical and procedural methods for recognizing decayed instances of elements and disentangling their complex relationships.  Annotation of TE instances is usually performed using our RepeatMasker software, which compares a genome to a database containing representations of known repeat families. These have historically been consensus sequences, which generally approximate the sequences of the original TEs. The largest repository of such consensus sequences is Repbase, whose restrictive license and limited interface for curators has led to a lack of input from third parties and the creation of many unaffiliated, often organism-specific open databases. The parallel existence of these many databases has led to a divergence in nomenclature and repeat definition.  Our Dfam database is an open access collection of repetitive DNA families, in which each family is represented by a multiple sequence alignment and a profile hidden Markov model (HMM). We have demonstrated that profile HMMs support improved annotation sensitivity, and Dfam provides numerous aids to both curators of TE families and those who make use of the resulting annotations. In this proposal, we describe a plan to develop the infrastructure of Dfam to expand to 1000s of genomes, and to establish a self-sustaining TE Data Commons dependent on limited centralized curation. We further describe plans to improve the quality of repeat annotation through development of methods for more reliable alignment adjudication, to expand approaches to visualization of this complex data type, and to improve the modeling of TE subfamilies.  By further developing this open access database, we will provide a strong disincentive for the proliferation of unaffiliated non-standard repeat datasets and ease the burden of data management for those developing TE libraries. Project Narrative Most of the vertebrate genome finds its ultimate origin in transposable elements (TEs), and the thorough annotation of TEs is a critical aspect of genome annotation pipelines. Currently, the data used for this annotation is dominated by a single database, Repbase, whose restrictive license impedes coalescence on a central standardized resource. We have developed an innovative open access database (Dfam) that yields substantial gains in TE annotation quality and provides numerous novel features in support of community TE curation. We will grow and improve Dfam to create a sustainable, standardized, and open system for TE family data. To achieve this, we will develop Dfam's infrastructure to scale to 1000s of genomes, develop improved methods for computing and representing the complex relationships between TE instances, and develop a framework for aiding curators in developing and sharing their TE libraries.","Dfam: sustainable growth, curation support, and improved quality for mobile element annotation",9930626,U24HG010136,"['Animal Model', 'Architecture', 'Awareness', 'Biological databases', 'Collaborations', 'Collection', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consensus', 'Consensus Sequence', 'DNA', 'DNA Transposable Elements', 'Data', 'Data Commons', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disincentive', 'Educational workshop', 'Elements', 'Evolution', 'Family', 'Foundations', 'Funding', 'Generations', 'Genome', 'Growth', 'Human', 'Human Genome', 'Infrastructure', 'Knowledge', 'Libraries', 'Licensing', 'Medical', 'Metadata', 'Methods', 'Modeling', 'Movement', 'Mutation', 'Nomenclature', 'Organism', 'Paper', 'Production', 'Protocols documentation', 'Publications', 'Quality Control', 'Repetitive Sequence', 'Research', 'Research Institute', 'Research Personnel', 'Resources', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Standardization', 'System', 'Taxonomy', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'Update', 'Visualization', 'Work', 'adjudication', 'annotation  system', 'base', 'complex data ', 'data management', 'data modeling', 'expectation', 'experience', 'genetic information', 'genome annotation', 'genome browser', 'genome-wide', 'improved', 'innovation', 'markov model', 'meetings', 'method development', 'novel', 'outreach', 'reference genome', 'repository', 'vertebrate genome', 'whole genome']",NHGRI,INSTITUTE FOR SYSTEMS BIOLOGY,U24,2020,607378,0.04956269270538845
"Dfam: sustainable growth, curation support, and improved quality for mobile element annotation Project Summary / Abstract Repetitive DNA, especially that due to transposable elements (TEs), makes up a large fraction of many genomes. Thorough and accurate annotation of repetitive content in genomes depends on a comprehensive database of known TEs, along with robust statistical and procedural methods for recognizing decayed instances of elements and disentangling their complex relationships.  Annotation of TE instances is usually performed using our RepeatMasker software, which compares a genome to a database containing representations of known repeat families. These have historically been consensus sequences, which generally approximate the sequences of the original TEs. The largest repository of such consensus sequences is Repbase, whose restrictive license and limited interface for curators has led to a lack of input from third parties and the creation of many unaffiliated, often organism-specific open databases. The parallel existence of these many databases has led to a divergence in nomenclature and repeat definition.  Our Dfam database is an open access collection of repetitive DNA families, in which each family is represented by a multiple sequence alignment and a profile hidden Markov model (HMM). We have demonstrated that profile HMMs support improved annotation sensitivity, and Dfam provides numerous aids to both curators of TE families and those who make use of the resulting annotations. In this proposal, we describe a plan to develop the infrastructure of Dfam to expand to 1000s of genomes, and to establish a self-sustaining TE Data Commons dependent on limited centralized curation. We further describe plans to improve the quality of repeat annotation through development of methods for more reliable alignment adjudication, to expand approaches to visualization of this complex data type, and to improve the modeling of TE subfamilies.  By further developing this open access database, we will provide a strong disincentive for the proliferation of unaffiliated non-standard repeat datasets and ease the burden of data management for those developing TE libraries. Project Narrative Most of the vertebrate genome finds its ultimate origin in transposable elements (TEs), and the thorough annotation of TEs is a critical aspect of genome annotation pipelines. Currently, the data used for this annotation is dominated by a single database, Repbase, whose restrictive license impedes coalescence on a central standardized resource. We have developed an innovative open access database (Dfam) that yields substantial gains in TE annotation quality and provides numerous novel features in support of community TE curation. We will grow and improve Dfam to create a sustainable, standardized, and open system for TE family data. To achieve this, we will develop Dfam's infrastructure to scale to 1000s of genomes, develop improved methods for computing and representing the complex relationships between TE instances, and develop a framework for aiding curators in developing and sharing their TE libraries.","Dfam: sustainable growth, curation support, and improved quality for mobile element annotation",9764454,U24HG010136,"['Animal Model', 'Architecture', 'Awareness', 'Biological databases', 'Collaborations', 'Collection', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consensus', 'Consensus Sequence', 'DNA', 'DNA Transposable Elements', 'Data', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disincentive', 'Educational workshop', 'Elements', 'Evolution', 'Family', 'Foundations', 'Funding', 'Generations', 'Genome', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Infrastructure', 'Knowledge', 'Libraries', 'Licensing', 'Medical', 'Metadata', 'Methods', 'Modeling', 'Movement', 'Mutation', 'Nomenclature', 'Organism', 'Paper', 'Production', 'Protocols documentation', 'Publications', 'Quality Control', 'Repetitive Sequence', 'Research', 'Research Institute', 'Research Personnel', 'Resources', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Standardization', 'System', 'Taxonomy', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'Update', 'Work', 'adjudication', 'annotation  system', 'base', 'data management', 'data modeling', 'expectation', 'experience', 'genetic information', 'genome annotation', 'genome browser', 'genome-wide', 'improved', 'innovation', 'markov model', 'meetings', 'method development', 'novel', 'outreach', 'reference genome', 'repository', 'vertebrate genome', 'whole genome']",NHGRI,INSTITUTE FOR SYSTEMS BIOLOGY,U24,2019,603378,0.04956269270538845
"Dfam: sustainable growth, curation support, and improved quality for mobile element annotation Project Summary / Abstract Repetitive DNA, especially that due to transposable elements (TEs), makes up a large fraction of many genomes. Thorough and accurate annotation of repetitive content in genomes depends on a comprehensive database of known TEs, along with robust statistical and procedural methods for recognizing decayed instances of elements and disentangling their complex relationships.  Annotation of TE instances is usually performed using our RepeatMasker software, which compares a genome to a database containing representations of known repeat families. These have historically been consensus sequences, which generally approximate the sequences of the original TEs. The largest repository of such consensus sequences is Repbase, whose restrictive license and limited interface for curators has led to a lack of input from third parties and the creation of many unaffiliated, often organism-specific open databases. The parallel existence of these many databases has led to a divergence in nomenclature and repeat definition.  Our Dfam database is an open access collection of repetitive DNA families, in which each family is represented by a multiple sequence alignment and a profile hidden Markov model (HMM). We have demonstrated that profile HMMs support improved annotation sensitivity, and Dfam provides numerous aids to both curators of TE families and those who make use of the resulting annotations. In this proposal, we describe a plan to develop the infrastructure of Dfam to expand to 1000s of genomes, and to establish a self-sustaining TE Data Commons dependent on limited centralized curation. We further describe plans to improve the quality of repeat annotation through development of methods for more reliable alignment adjudication, to expand approaches to visualization of this complex data type, and to improve the modeling of TE subfamilies.  By further developing this open access database, we will provide a strong disincentive for the proliferation of unaffiliated non-standard repeat datasets and ease the burden of data management for those developing TE libraries. Project Narrative Most of the vertebrate genome finds its ultimate origin in transposable elements (TEs), and the thorough annotation of TEs is a critical aspect of genome annotation pipelines. Currently, the data used for this annotation is dominated by a single database, Repbase, whose restrictive license impedes coalescence on a central standardized resource. We have developed an innovative open access database (Dfam) that yields substantial gains in TE annotation quality and provides numerous novel features in support of community TE curation. We will grow and improve Dfam to create a sustainable, standardized, and open system for TE family data. To achieve this, we will develop Dfam's infrastructure to scale to 1000s of genomes, develop improved methods for computing and representing the complex relationships between TE instances, and develop a framework for aiding curators in developing and sharing their TE libraries.","Dfam: sustainable growth, curation support, and improved quality for mobile element annotation",9572485,U24HG010136,"['Animal Model', 'Architecture', 'Awareness', 'Biological databases', 'Collaborations', 'Collection', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consensus', 'Consensus Sequence', 'DNA', 'DNA Transposable Elements', 'Data', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disincentive', 'Educational workshop', 'Elements', 'Evolution', 'Family', 'Foundations', 'Funding', 'Generations', 'Genome', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Knowledge', 'Libraries', 'Licensing', 'Medical', 'Metadata', 'Methods', 'Modeling', 'Movement', 'Mutation', 'Nomenclature', 'Organism', 'Paper', 'Production', 'Protocols documentation', 'Publications', 'Quality Control', 'Repetitive Sequence', 'Research', 'Research Infrastructure', 'Research Institute', 'Research Personnel', 'Resources', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Standardization', 'System', 'Taxonomy', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'Update', 'Work', 'adjudication', 'annotation  system', 'base', 'data management', 'data modeling', 'expectation', 'experience', 'genetic information', 'genome annotation', 'genome browser', 'genome-wide', 'improved', 'innovation', 'markov model', 'meetings', 'method development', 'novel', 'outreach', 'reference genome', 'repository', 'vertebrate genome', 'whole genome']",NHGRI,INSTITUTE FOR SYSTEMS BIOLOGY,U24,2018,665174,0.04956269270538845
"Improved manuscript search through PubSeq    DESCRIPTION (provided by applicant): Most top-level searches of scientific literature include querying of structured fields such as author, subject, or affiliation. A free-text search of abstracts or full texts entries would be more flexible allowing queries with any word combination including ranges of names and identifiers. Unfortunately, free text searches usually yield incomplete and often erroneous results since the naming of biologically important molecules (genes, proteins, substrates) is not standardized. Unless a specific query issued to a retrieval service (e.g. PubMed) covers all possible aliases of a given protein or gene the results may be insufficient or simply wrong. The system proposed here translates the problem of looking up literature pertaining to a certain protein to the sequence level. By correlating existing identifiers, names, and synonyms of proteins with their sequences this lookup increases the accuracy and coverage of the results. A particular challenge that our system will uniquely address is the following. Increasingly structural and functional genomics projects bring up proteins for which nothing is known. If someone published some new experimental that will actually name such a protein, this important knowledge will likely be lost to the genomics investigator because PubMed alarms need to be activated by keywords and names. Our system could fill in the gap: users will be able to deposit sequences corresponding to proteins of unknown function/name. If experimental information will be published for the same or a related sequence the original investigator will be notified.    PUBLIC HEALTH RELEVANCE: The experimental and computational data appearing daily in publications is critical to the advancement of biological research. However, the sheer quantity and high frequency in which new data is published turns bench scientists into research librarians trying to sift through the flood of information while searching for relevant and reliable data. Furthermore, as biological research is increasingly driven by the study of proteins and genes that mostly lack annotations, or even an identifiers, there is a need to access the literature by using sequence data alone. By automating the process of searching and discovering relevant information as it becomes available, the proposed system promises to save time and increase the coverage of relevant and reliable data retrieved by a given search in an intuitive and ""easy to consume"" format.           Relevance: The experimental and computational data appearing daily in publications is critical to the advancement of biological research. However, the sheer quantity and high frequency in which new data is published turns bench scientists into research librarians trying to sift through the flood of information while searching for relevant and reliable data. Furthermore as biological research is increasingly driven by the study of proteins and genes that mostly lack annotations, or even an identifiers, there is a need to access the literature by using sequence data alone. By automating the process of searching and discovering relevant information as it becomes available, the proposed system promises to save time and increase the coverage of relevant and reliable data retrieved by a given search in an intuitive and ""easy to consume"" format.",Improved manuscript search through PubSeq,7748592,R43LM010156,"['Address', 'Amino Acid Sequence', 'Arts', 'Base Sequence', 'Blast Cell', 'Companions', 'DNA Sequence', 'Data', 'Databases', 'Deposition', 'Development', 'Drosophila genus', 'Floods', 'Frequencies', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Knowledge', 'Librarians', 'Literature', 'Malaria', 'Manuscripts', 'Maps', 'Monitor', 'Names', 'Notification', 'Peptide Sequence Determination', 'Process', 'Proteins', 'Protocols documentation', 'PubMed', 'Publications', 'Publishing', 'Records', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Services', 'Structure', 'System', 'Text', 'Time', 'Translating', 'Update', 'abstracting', 'base', 'biological research', 'flexibility', 'functional/structural genomics', 'improved', 'malignant breast neoplasm', 'public health relevance', 'repository', 'research study', 'text searching', 'tool']",NLM,"BIOSOF, LLC",R43,2009,97198,0.05800582682708816
"A synthetic biology library for the Semantic Web    DESCRIPTION (provided by applicant): The goal of the proposed research is to determine the feasibility of, and create prototype software for, a library of reusable synthetic biology components using Semantic Web technologies OWL, RDF, SPARQL and SWRL. This library will dramatically increase the availability of information about reusable components for synthetic biology, including machine-readable descriptions of the semantic meaning of the components. Such information, amenable to automated reasoning, intelligent retrieval, and other services, will increase the utility of the synthetic biology library for researchers as well as for commercial vendors engaged in designing synthetic biology products. Such a library will hasten the industrialization of synthetic biology, with a host of resulting benefits to public health, including better ""living machines"" designed and manufactured more quickly and efficiently.   In order to achieve these results, in Phase I we will extend BioBricks standard biological component descriptions with semantically-rich descriptions, using OWL ontologies, to support better integration, both with other components, via model checking and verification-style analysis services, as well as with the wealth of biological background knowledge that presently exists in OWL ontology form. We will extend OWL reasoning systems to interpret these semantically rich BioBrick descriptions, including Semantic Web query support via SPARQL and rule support via SWRL. Phase I deliverables include use case and requirements from bioengineering; gap analysis between these requirements and existing technologies; a final report detailing lessons learning and preliminary designs for Phase II; and a prototype library containing OWL-extended BioBrick descriptions and OWL, SPARQL, and SWRL reasoning and query services; and a faceted BioBrick browser for intuitive interaction with the library's entries. In short, better information sharing and analysis infrastructure will provide productivity increases for synthetic biology researchers and practitioners, which will in turn hasten the widescale industrial adoption and use of synthetic biology techniques, leading to new advances in bioengineered therapeutic, energy, and food technologies.      PUBLIC HEALTH RELEVANCE: The relevance of the proposed research to public health is that a Semantic Web-enabled library for synthetic biology will increase the design quality and efficiency-via computer-aided decision support and seamless information sharing-of synthetic biology R&D and industrial activity, thus hastening the advent of industrialscale synthetic biology. That advent will directly benefit public health by way of better ""living machines"" to address therapeutics, energy, food, and other vital areas.           Project Narrative The relevance of the proposed research to public health is that a Semantic Web-enabled library for synthetic biology will increase the design quality and efficiency-via computer-aided decision support and seamless in- formation sharing-of synthetic biology R&D and industrial activity, thus hastening the advent of industrial- scale synthetic biology. That advent will directly benefit public health by way of better ""living machines"" to ad- dress therapeutics, energy, food, and other vital areas.",A synthetic biology library for the Semantic Web,7910247,R41LM010745,"['Address', 'Adoption', 'Area', 'Arts', 'Bioinformatics', 'Biological', 'Biology', 'Biomedical Engineering', 'Communities', 'Computer Assisted', 'Computer software', 'DNA', 'DNA Library', 'Databases', 'Development', 'Devices', 'Engineering', 'Feedback', 'Food Energy', 'Food Technology', 'Genes', 'Genetic', 'Goals', 'Individual', 'Industrialization', 'Industry', 'Information Management', 'Information Resources', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Life', 'Logic', 'Marketing', 'Mechanics', 'Methods', 'Modeling', 'Ontology', 'Phase', 'Process', 'Production', 'Productivity', 'Public Health', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Retrieval', 'Semantics', 'Services', 'Solutions', 'Standardization', 'Sterile coverings', 'Strigiformes', 'Structure', 'Synthetic Genes', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tweens', 'Universities', 'Validation', 'Vendor', 'Washington', 'Work', 'base', 'biological systems', 'computer based Semantic Analysis', 'computer science', 'design', 'flexibility', 'improved', 'information organization', 'innovation', 'interoperability', 'meetings', 'model design', 'models and simulation', 'prototype', 'public health relevance', 'research and development', 'synthetic biology', 'task analysis', 'tool', 'web-enabled']",NLM,"CLARK AND PARSIA, LLC",R41,2010,99967,0.06070801678650015
"BioGRID: An Open Integrated Resource for Biological Interaction Data Project Summary/Abstract Complex protein and genetic interaction networks determine the properties of all biological systems and underlie human development, health and disease. Decades of biochemical, genetic and molecular biological experiments have identified myriad molecular processes that underpin specific biological functions, as documented in the primary biomedical literature. Recent technological innovations combined with complete genome sequence information have enabled a host of high-throughput (HTP) methods to generate protein and genetic interaction data on an unprecedented scale. Because human interaction networks are often directly analogous to networks in tractable model organisms, it is essential that the hundreds of thousands of biological interactions discovered across the major model organisms, as well as humans, are archived in a well- annotated manner that is amenable to rigorous analysis and computation. To capture, integrate, and interrogate this wealth of data from both the literature and HTP datasets, we developed the BioGRID database as an open repository for protein and genetic interactions (www.thebiogrid.org). BioGRID is widely used by the biological and biomedical research community, with on average over 16,555 unique visitors per month in 2015. Using the search and visualization tools in BioGRID, these users explore the 971,027 total interactions that have been directly traced to experimental data in 45,603 publications by our curators. In addition, the unique datasets in BioGRID are disseminated widely by a host of partner databases, meta-databases, and applications. Here, we propose to markedly enhance the data content, the database architecture, and the user interface of BioGRID. We will expand the amount and types of data available through BioGRID, with a focus on interactions of central biological processes that are frequently perturbed in human disease. We will use new ontologies to systematically capture new data types, including CRISPR-based genetic interactions, structured phenotypes across all species, chemical and drug interactions, and post-translational modifications. Text- mining algorithms will be incorporated into the curation pipeline to enhance curation rates, and thereby substantially expand the coverage of the database. User access to the large datasets in BioGRID will be facilitated by data-rich interfaces, user-defined search and display parameters, and multiple methods of visualization. All software will continue to be open source and engineered toward compatibility and complementary with other academic database and software development efforts. The BioGRID will provide interaction data and software tools to model organism databases and other interested parties without restriction. The BioGRID resource will enable the biomedical research community to access validated biological interaction datasets across model organisms and humans for hypothesis generation and network analysis, and thereby further the general mission of the NIH. Narrative The BioGRID database is a comprehensive resource that provides protein and genetic interaction data for the major model organism species and humans, along with user-oriented tools to explore this information. The BioGRID facilitates better understanding of human disease by enabling inference of gene and protein function through network context and the computational comparison of these gene and protein networks in human health and disease to analogous networks mapped in model organisms. The large amounts of data in the BioGRID are freely provided to many other databases and users, thus facilitating both fundamental and translational research.",BioGRID: An Open Integrated Resource for Biological Interaction Data,9177106,R01OD010929,"['Accounting', 'Algorithms', 'Alleles', 'Animal Model', 'Architecture', 'Archives', 'Area', 'Autophagocytosis', 'Behavior', 'Binding', 'Biochemical Genetics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'CRISPR/Cas technology', 'Caenorhabditis elegans', 'Cells', 'Chemicals', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Computer software', 'Custom', 'DNA Damage', 'Data', 'Data Set', 'Databases', 'Discipline', 'Disease', 'Drug Interactions', 'Engineering', 'Event', 'Fission Yeast', 'Funding', 'Gene Proteins', 'Generations', 'Genes', 'Genetic', 'Health', 'Housing', 'Human', 'Human Development', 'Imagery', 'International', 'Link', 'Literature', 'Maps', 'Methods', 'Mission', 'Molecular', 'Molecular Biology', 'Ontology', 'Organism', 'Pathway Analysis', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphoric Monoester Hydrolases', 'Phosphotransferases', 'Plants', 'Post-Translational Protein Processing', 'Process', 'Property', 'Protein Dynamics', 'Protein Structure Initiative', 'Proteins', 'Publications', 'Publishing', 'Reporting', 'Research', 'Resources', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Services', 'Signal Transduction', 'Software Tools', 'Source', 'Structure', 'System', 'Technology', 'Tissues', 'Translational Research', 'Ubiquitin', 'United States National Institutes of Health', 'Yeasts', 'abstracting', 'base', 'biological research', 'biological systems', 'cancer stem cell', 'chemical genetics', 'chromatin remodeling', 'cloud based', 'cohort', 'fundamental research', 'gene function', 'genome sequencing', 'human disease', 'improved', 'interest', 'model organisms databases', 'open source', 'protein complex', 'protein function', 'repository', 'research study', 'response', 'software development', 'technological innovation', 'text searching', 'tool', 'web site']",OD,SINAI HEALTH SYSTEM,R01,2016,895351,0.042880323872597936
"BioGRID: An Open Integrated Resource for Biological Interaction Data Project Summary/Abstract Complex protein and genetic interaction networks determine the properties of all biological systems and underlie human development, health and disease. Decades of biochemical, genetic and molecular biological experiments have identified myriad molecular processes that underpin specific biological functions, as documented in the primary biomedical literature. Recent technological innovations combined with complete genome sequence information have enabled a host of high-throughput (HTP) methods to generate protein and genetic interaction data on an unprecedented scale. Because human interaction networks are often directly analogous to networks in tractable model organisms, it is essential that the hundreds of thousands of biological interactions discovered across the major model organisms, as well as humans, are archived in a well- annotated manner that is amenable to rigorous analysis and computation. To capture, integrate, and interrogate this wealth of data from both the literature and HTP datasets, we developed the BioGRID database as an open repository for protein and genetic interactions (www.thebiogrid.org). BioGRID is widely used by the biological and biomedical research community, with on average over 16,555 unique visitors per month in 2015. Using the search and visualization tools in BioGRID, these users explore the 971,027 total interactions that have been directly traced to experimental data in 45,603 publications by our curators. In addition, the unique datasets in BioGRID are disseminated widely by a host of partner databases, meta-databases, and applications. Here, we propose to markedly enhance the data content, the database architecture, and the user interface of BioGRID. We will expand the amount and types of data available through BioGRID, with a focus on interactions of central biological processes that are frequently perturbed in human disease. We will use new ontologies to systematically capture new data types, including CRISPR-based genetic interactions, structured phenotypes across all species, chemical and drug interactions, and post-translational modifications. Text- mining algorithms will be incorporated into the curation pipeline to enhance curation rates, and thereby substantially expand the coverage of the database. User access to the large datasets in BioGRID will be facilitated by data-rich interfaces, user-defined search and display parameters, and multiple methods of visualization. All software will continue to be open source and engineered toward compatibility and complementary with other academic database and software development efforts. The BioGRID will provide interaction data and software tools to model organism databases and other interested parties without restriction. The BioGRID resource will enable the biomedical research community to access validated biological interaction datasets across model organisms and humans for hypothesis generation and network analysis, and thereby further the general mission of the NIH. Narrative The BioGRID database is a comprehensive resource that provides protein and genetic interaction data for the major model organism species and humans, along with user-oriented tools to explore this information. The BioGRID facilitates better understanding of human disease by enabling inference of gene and protein function through network context and the computational comparison of these gene and protein networks in human health and disease to analogous networks mapped in model organisms. The large amounts of data in the BioGRID are freely provided to many other databases and users, thus facilitating both fundamental and translational research.",BioGRID: An Open Integrated Resource for Biological Interaction Data,9494718,R01OD010929,"['Algorithms', 'Alleles', 'Animal Model', 'Architecture', 'Archives', 'Area', 'Autophagocytosis', 'Behavior', 'Binding', 'Biochemical Genetics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'CRISPR/Cas technology', 'Caenorhabditis elegans', 'Cells', 'Chemicals', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Computer software', 'Custom', 'DNA Damage', 'Data', 'Data Set', 'Databases', 'Discipline', 'Disease', 'Drug Interactions', 'Engineering', 'Event', 'Fission Yeast', 'Funding', 'Gene Proteins', 'Generations', 'Genes', 'Genetic', 'Genome', 'Health', 'Human', 'Human Development', 'Imagery', 'International', 'Intuition', 'Link', 'Literature', 'Maps', 'Methods', 'Mission', 'Molecular', 'Molecular Biology', 'Ontology', 'Organism', 'Pathway Analysis', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphoric Monoester Hydrolases', 'Phosphotransferases', 'Plants', 'Post-Translational Protein Processing', 'Process', 'Property', 'Protein Structure Initiative', 'Proteins', 'Publications', 'Publishing', 'Reporting', 'Research', 'Resources', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Services', 'Signal Transduction', 'Software Tools', 'Source', 'Stem cells', 'Structure', 'System', 'Technology', 'Tissues', 'Translational Research', 'Ubiquitin', 'United States National Institutes of Health', 'Visualization software', 'Yeasts', 'annotation  system', 'base', 'biological research', 'biological systems', 'cancer cell', 'chemical genetics', 'chromatin remodeling', 'cloud based', 'cohort', 'experimental study', 'flexibility', 'fundamental research', 'gene function', 'human disease', 'improved', 'interest', 'model organisms databases', 'open source', 'phenotypic data', 'protein complex', 'protein function', 'repository', 'response', 'software development', 'technological innovation', 'text searching', 'tool', 'web site']",OD,SINAI HEALTH SYSTEM,R01,2018,895351,0.042880323872597936
"BioGRID: An Open Integrated Resource for Biological Interaction Data Project Summary/Abstract Complex protein and genetic interaction networks determine the properties of all biological systems and underlie human development, health and disease. Decades of biochemical, genetic and molecular biological experiments have identified myriad molecular processes that underpin specific biological functions, as documented in the primary biomedical literature. Recent technological innovations combined with complete genome sequence information have enabled a host of high-throughput (HTP) methods to generate protein and genetic interaction data on an unprecedented scale. Because human interaction networks are often directly analogous to networks in tractable model organisms, it is essential that the hundreds of thousands of biological interactions discovered across the major model organisms, as well as humans, are archived in a well- annotated manner that is amenable to rigorous analysis and computation. To capture, integrate, and interrogate this wealth of data from both the literature and HTP datasets, we developed the BioGRID database as an open repository for protein and genetic interactions (www.thebiogrid.org). BioGRID is widely used by the biological and biomedical research community, with on average over 16,555 unique visitors per month in 2015. Using the search and visualization tools in BioGRID, these users explore the 971,027 total interactions that have been directly traced to experimental data in 45,603 publications by our curators. In addition, the unique datasets in BioGRID are disseminated widely by a host of partner databases, meta-databases, and applications. Here, we propose to markedly enhance the data content, the database architecture, and the user interface of BioGRID. We will expand the amount and types of data available through BioGRID, with a focus on interactions of central biological processes that are frequently perturbed in human disease. We will use new ontologies to systematically capture new data types, including CRISPR-based genetic interactions, structured phenotypes across all species, chemical and drug interactions, and post-translational modifications. Text- mining algorithms will be incorporated into the curation pipeline to enhance curation rates, and thereby substantially expand the coverage of the database. User access to the large datasets in BioGRID will be facilitated by data-rich interfaces, user-defined search and display parameters, and multiple methods of visualization. All software will continue to be open source and engineered toward compatibility and complementary with other academic database and software development efforts. The BioGRID will provide interaction data and software tools to model organism databases and other interested parties without restriction. The BioGRID resource will enable the biomedical research community to access validated biological interaction datasets across model organisms and humans for hypothesis generation and network analysis, and thereby further the general mission of the NIH. Narrative The BioGRID database is a comprehensive resource that provides protein and genetic interaction data for the major model organism species and humans, along with user-oriented tools to explore this information. The BioGRID facilitates better understanding of human disease by enabling inference of gene and protein function through network context and the computational comparison of these gene and protein networks in human health and disease to analogous networks mapped in model organisms. The large amounts of data in the BioGRID are freely provided to many other databases and users, thus facilitating both fundamental and translational research.",BioGRID: An Open Integrated Resource for Biological Interaction Data,9284537,R01OD010929,"['Algorithms', 'Alleles', 'Animal Model', 'Architecture', 'Archives', 'Area', 'Autophagocytosis', 'Behavior', 'Binding', 'Biochemical Genetics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'CRISPR/Cas technology', 'Caenorhabditis elegans', 'Cells', 'Chemicals', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Computer software', 'Custom', 'DNA Damage', 'Data', 'Data Set', 'Databases', 'Discipline', 'Disease', 'Drug Interactions', 'Engineering', 'Event', 'Fission Yeast', 'Funding', 'Gene Proteins', 'Generations', 'Genes', 'Genetic', 'Genome', 'Health', 'Human', 'Human Development', 'Imagery', 'International', 'Intuition', 'Link', 'Literature', 'Malignant Neoplasms', 'Maps', 'Methods', 'Mission', 'Molecular', 'Molecular Biology', 'Ontology', 'Organism', 'Pathway Analysis', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphoric Monoester Hydrolases', 'Phosphotransferases', 'Plants', 'Post-Translational Protein Processing', 'Process', 'Property', 'Protein Structure Initiative', 'Proteins', 'Publications', 'Publishing', 'Reporting', 'Research', 'Resources', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Services', 'Signal Transduction', 'Software Tools', 'Source', 'Stem cells', 'Structure', 'System', 'Technology', 'Tissues', 'Translational Research', 'Ubiquitin', 'United States National Institutes of Health', 'Visualization software', 'Yeasts', 'annotation  system', 'base', 'biological research', 'biological systems', 'chemical genetics', 'chromatin remodeling', 'cloud based', 'cohort', 'experimental study', 'flexibility', 'fundamental research', 'gene function', 'human disease', 'improved', 'interest', 'model organisms databases', 'open source', 'phenotypic data', 'protein complex', 'protein function', 'repository', 'response', 'software development', 'technological innovation', 'text searching', 'tool', 'web site']",OD,SINAI HEALTH SYSTEM,R01,2017,850582,0.042880323872597936
"BioGRID: An Open Integrated Resource for Biological Interaction Data Project Summary/Abstract Complex protein and genetic interaction networks determine the properties of all biological systems and underlie human development, health and disease. Decades of biochemical, genetic and molecular biological experiments have identified myriad molecular processes that underpin specific biological functions, as documented in the primary biomedical literature. Recent technological innovations combined with complete genome sequence information have enabled a host of high-throughput (HTP) methods to generate protein and genetic interaction data on an unprecedented scale. Because human interaction networks are often directly analogous to networks in tractable model organisms, it is essential that the hundreds of thousands of biological interactions discovered across the major model organisms, as well as humans, are archived in a well- annotated manner that is amenable to rigorous analysis and computation. To capture, integrate, and interrogate this wealth of data from both the literature and HTP datasets, we developed the BioGRID database as an open repository for protein and genetic interactions (www.thebiogrid.org). BioGRID is widely used by the biological and biomedical research community, with on average over 16,555 unique visitors per month in 2015. Using the search and visualization tools in BioGRID, these users explore the 971,027 total interactions that have been directly traced to experimental data in 45,603 publications by our curators. In addition, the unique datasets in BioGRID are disseminated widely by a host of partner databases, meta-databases, and applications. Here, we propose to markedly enhance the data content, the database architecture, and the user interface of BioGRID. We will expand the amount and types of data available through BioGRID, with a focus on interactions of central biological processes that are frequently perturbed in human disease. We will use new ontologies to systematically capture new data types, including CRISPR-based genetic interactions, structured phenotypes across all species, chemical and drug interactions, and post-translational modifications. Text- mining algorithms will be incorporated into the curation pipeline to enhance curation rates, and thereby substantially expand the coverage of the database. User access to the large datasets in BioGRID will be facilitated by data-rich interfaces, user-defined search and display parameters, and multiple methods of visualization. All software will continue to be open source and engineered toward compatibility and complementary with other academic database and software development efforts. The BioGRID will provide interaction data and software tools to model organism databases and other interested parties without restriction. The BioGRID resource will enable the biomedical research community to access validated biological interaction datasets across model organisms and humans for hypothesis generation and network analysis, and thereby further the general mission of the NIH. Narrative The BioGRID database is a comprehensive resource that provides protein and genetic interaction data for the major model organism species and humans, along with user-oriented tools to explore this information. The BioGRID facilitates better understanding of human disease by enabling inference of gene and protein function through network context and the computational comparison of these gene and protein networks in human health and disease to analogous networks mapped in model organisms. The large amounts of data in the BioGRID are freely provided to many other databases and users, thus facilitating both fundamental and translational research.",BioGRID: An Open Integrated Resource for Biological Interaction Data,9948007,R01OD010929,"['Algorithms', 'Alleles', 'Animal Model', 'Architecture', 'Archives', 'Area', 'Autophagocytosis', 'Behavior', 'Binding', 'Biochemical Genetics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'CRISPR/Cas technology', 'Caenorhabditis elegans', 'Cells', 'Chemicals', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Computer software', 'Custom', 'DNA Damage', 'Data', 'Data Set', 'Databases', 'Discipline', 'Disease', 'Drug Interactions', 'Engineering', 'Event', 'Fission Yeast', 'Funding', 'Gene Proteins', 'Generations', 'Genes', 'Genetic', 'Genome', 'Health', 'Human', 'Human Development', 'International', 'Intuition', 'Link', 'Literature', 'Maps', 'Methods', 'Mission', 'Molecular', 'Molecular Biology', 'Ontology', 'Organism', 'Pathway Analysis', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphoric Monoester Hydrolases', 'Phosphotransferases', 'Plants', 'Post-Translational Protein Processing', 'Process', 'Property', 'Protein Structure Initiative', 'Proteins', 'Publications', 'Publishing', 'Reporting', 'Research', 'Resources', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Services', 'Signal Transduction', 'Software Tools', 'Source', 'Structure', 'System', 'Technology', 'Tissues', 'Translational Research', 'Ubiquitin', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Yeasts', 'annotation  system', 'base', 'biological research', 'biological systems', 'cancer cell', 'chemical genetics', 'chromatin remodeling', 'cloud based', 'cohort', 'data dissemination', 'data tools', 'experimental study', 'flexibility', 'fundamental research', 'gene function', 'human disease', 'improved', 'interest', 'large datasets', 'model organisms databases', 'open source', 'phenotypic data', 'protein complex', 'protein function', 'repository', 'response', 'software development', 'stem cells', 'technological innovation', 'text searching', 'tool', 'web site']",OD,SINAI HEALTH SYSTEM,R01,2020,895351,0.042880323872597936
"BioGRID: An Open Integrated Resource for Biological Interaction Data Project Summary/Abstract Complex protein and genetic interaction networks determine the properties of all biological systems and underlie human development, health and disease. Decades of biochemical, genetic and molecular biological experiments have identified myriad molecular processes that underpin specific biological functions, as documented in the primary biomedical literature. Recent technological innovations combined with complete genome sequence information have enabled a host of high-throughput (HTP) methods to generate protein and genetic interaction data on an unprecedented scale. Because human interaction networks are often directly analogous to networks in tractable model organisms, it is essential that the hundreds of thousands of biological interactions discovered across the major model organisms, as well as humans, are archived in a well- annotated manner that is amenable to rigorous analysis and computation. To capture, integrate, and interrogate this wealth of data from both the literature and HTP datasets, we developed the BioGRID database as an open repository for protein and genetic interactions (www.thebiogrid.org). BioGRID is widely used by the biological and biomedical research community, with on average over 16,555 unique visitors per month in 2015. Using the search and visualization tools in BioGRID, these users explore the 971,027 total interactions that have been directly traced to experimental data in 45,603 publications by our curators. In addition, the unique datasets in BioGRID are disseminated widely by a host of partner databases, meta-databases, and applications. Here, we propose to markedly enhance the data content, the database architecture, and the user interface of BioGRID. We will expand the amount and types of data available through BioGRID, with a focus on interactions of central biological processes that are frequently perturbed in human disease. We will use new ontologies to systematically capture new data types, including CRISPR-based genetic interactions, structured phenotypes across all species, chemical and drug interactions, and post-translational modifications. Text- mining algorithms will be incorporated into the curation pipeline to enhance curation rates, and thereby substantially expand the coverage of the database. User access to the large datasets in BioGRID will be facilitated by data-rich interfaces, user-defined search and display parameters, and multiple methods of visualization. All software will continue to be open source and engineered toward compatibility and complementary with other academic database and software development efforts. The BioGRID will provide interaction data and software tools to model organism databases and other interested parties without restriction. The BioGRID resource will enable the biomedical research community to access validated biological interaction datasets across model organisms and humans for hypothesis generation and network analysis, and thereby further the general mission of the NIH. Narrative The BioGRID database is a comprehensive resource that provides protein and genetic interaction data for the major model organism species and humans, along with user-oriented tools to explore this information. The BioGRID facilitates better understanding of human disease by enabling inference of gene and protein function through network context and the computational comparison of these gene and protein networks in human health and disease to analogous networks mapped in model organisms. The large amounts of data in the BioGRID are freely provided to many other databases and users, thus facilitating both fundamental and translational research.",BioGRID: An Open Integrated Resource for Biological Interaction Data,9745709,R01OD010929,"['Algorithms', 'Alleles', 'Animal Model', 'Architecture', 'Archives', 'Area', 'Autophagocytosis', 'Behavior', 'Binding', 'Biochemical Genetics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'CRISPR/Cas technology', 'Caenorhabditis elegans', 'Cells', 'Chemicals', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Computer software', 'Custom', 'DNA Damage', 'Data', 'Data Set', 'Databases', 'Discipline', 'Disease', 'Drug Interactions', 'Engineering', 'Event', 'Fission Yeast', 'Funding', 'Gene Proteins', 'Generations', 'Genes', 'Genetic', 'Genome', 'Health', 'Human', 'Human Development', 'Imagery', 'International', 'Intuition', 'Link', 'Literature', 'Maps', 'Methods', 'Mission', 'Molecular', 'Molecular Biology', 'Ontology', 'Organism', 'Pathway Analysis', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphoric Monoester Hydrolases', 'Phosphotransferases', 'Plants', 'Post-Translational Protein Processing', 'Process', 'Property', 'Protein Structure Initiative', 'Proteins', 'Publications', 'Publishing', 'Reporting', 'Research', 'Resources', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Services', 'Signal Transduction', 'Software Tools', 'Source', 'Stem cells', 'Structure', 'System', 'Technology', 'Tissues', 'Translational Research', 'Ubiquitin', 'United States National Institutes of Health', 'Visualization software', 'Yeasts', 'annotation  system', 'base', 'biological research', 'biological systems', 'cancer cell', 'chemical genetics', 'chromatin remodeling', 'cloud based', 'cohort', 'experimental study', 'flexibility', 'fundamental research', 'gene function', 'human disease', 'improved', 'interest', 'model organisms databases', 'open source', 'phenotypic data', 'protein complex', 'protein function', 'repository', 'response', 'software development', 'technological innovation', 'text searching', 'tool', 'web site']",OD,SINAI HEALTH SYSTEM,R01,2019,895351,0.042880323872597936
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8333306,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,592423,0.0423346167623918
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8022026,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,591195,0.0423346167623918
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8714052,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,UNIVERSITY OF UTAH,R01,2014,579144,0.0423346167623918
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8714053,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2014,307471,0.067331695440723
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8535824,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2013,291730,0.067331695440723
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8326650,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2012,317212,0.067331695440723
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8202896,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2011,312599,0.067331695440723
"Utilizing Imaged-based Features in Biomedical Literature Classification DESCRIPTION (provided by applicant): The proposed research aims to support and improve effective access to the biomedical literature, by utilizing the rich, highly-informative image data within publications, in addition to text. The biomedical literature is expanding at a rate of about  1,000,000 new publications a year. Scientists and physicians, as part of their daily work, go through a myriad of publications searching for relevant information. The task is even more arduous for scientific database curators (bio- curators, in organizations such as FlyBase or UniProt), who have to identify the literature most relevant to the database area, locate within it high-quality evidence concerning genes, proteins, organisms, or disease, and curate the findings within a database entry, with references to the relevant literature. Notably, much of the evidence within publications lies in figures. Accordingly, images are used by scientists and database curators as indicators for relevance.  To assist and expedite the search for information within the literature, automated text-mining tools are being developed; still, several shared tasks and competitive challenges demonstrated that the need for more effective automated identification of relevant information in biomedical publications remains a bottleneck for bio-curation and for scientific discovery. While image analysis within and outside the biomedical domain is an active research area, most current work on biomedical image processing focuses on retrieval and understanding of images as a primary form of data. Likewise, most efforts on biomedical literature retrieval and mining focus on text alone. Little has been done so far to use images within publications, which provide important cues as to the relevance of information embedded in papers.  The hypothesis underlying our proposal is that useful information can be derived directly from images within publications and integrated with text-based methods, leading to improved identification of relevant publications and of informative portions within them. The proposed research comprises extensive comparative study of highly-informative features within images, development and identification of such image-features, development of tools that extract such features and information from images, and integration of image-based information into the textual articles-classification process, aiming to determine the publications' relevance to well-defined biomedical needs. The fundamental research tasks we shall address are: A) Identification and comparative study of useful features for image-representation, focusing on their utility for specific biomedical needs; B) Classification of biomedical images and biomedical documents based on image-data; C) Document classification through integration of text- and image-based classifiers. To ground the research in genuine needs, secure access to much image data, and ensure broad-applicability of the results, we shall work within three diverse areas for which we have secured access to expertise and data: Finding articles about cis-regulatory regions (Cyrene project at Brown University); Evidence for gene expression in the mouse (Jackson Lab's GXD); Experimental evidence for protein-protein interaction (Delaware's Protein Information Resource). The successful completion of the proposed project will provide integrated methods and tools, utilizing both image-based and text-based features, leading to more focused and effective retrieval and mining tools, thus better supporting data-intensive biomedical discovery. Physicians and bio-medical scientists rely on the vast published literature as their main source of information about current developments and findings, on which they base both patient treatment and ongoing research toward bio- medical discovery. The proposed research will support physicians and scientists, speed-up their search for information and improve their effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage not only of text but also of the highly-informative image data within publications. The successful outcome of this research will lead to the development of focused, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting targeted biomedical discovery including better understanding of disease mechanism, uncovering possible means for accurate diagnoses, and revealing potential new drugs and drug-targets.",Utilizing Imaged-based Features in Biomedical Literature Classification,8916181,R56LM011354,"['Address', 'Area', 'Bioinformatics', 'Biological', 'Biological Phenomena', 'Categories', 'Classification', 'Comparative Study', 'Computational Biology', 'Computer-Assisted Image Analysis', 'Computers', 'Coupled', 'Cues', 'Data', 'Data Set', 'Databases', 'Delaware', 'Development', 'Dimensions', 'Disease', 'Drug Targeting', 'Ensure', 'Entropy', 'Gene Expression', 'Gene Proteins', 'Generic Drugs', 'Goals', 'Gold', 'Gray unit of radiation dose', 'Harvest', 'Image', 'Image Analysis', 'Improve Access', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Knowledge', 'Lead', 'Literature', 'Measures', 'Medical', 'Methods', 'Mining', 'Modeling', 'Mus', 'Nucleic Acid Regulatory Sequences', 'Organism', 'Outcomes Research', 'Paper', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Property', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reader', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Retrieval', 'Scanning', 'Scientist', 'Secure', 'Series', 'Shapes', 'Source', 'Speed', 'System', 'Testing', 'Text', 'Textbooks', 'Texture', 'Training', 'Universities', 'Voting', 'Work', 'accurate diagnosis', 'authority', 'base', 'bioimaging', 'evaluation/testing', 'experience', 'fundamental research', 'image processing', 'improved', 'indexing', 'insight', 'mouse genome', 'protein protein interaction', 'research study', 'text searching', 'tool', 'tool development']",NLM,UNIVERSITY OF DELAWARE,R56,2015,280000,0.04416074793196246
"Utilizing Imaged-based Features in Biomedical Literature Classification DESCRIPTION (provided by applicant): The proposed research aims to support and improve effective access to the biomedical literature, by utilizing the rich, highly-informative image data within publications, in addition to text. The biomedical literature is expanding at a rate of about  1,000,000 new publications a year. Scientists and physicians, as part of their daily work, go through a myriad of publications searching for relevant information. The task is even more arduous for scientific database curators (bio- curators, in organizations such as FlyBase or UniProt), who have to identify the literature most relevant to the database area, locate within it high-quality evidence concerning genes, proteins, organisms, or disease, and curate the findings within a database entry, with references to the relevant literature. Notably, much of the evidence within publications lies in figures. Accordingly, images are used by scientists and database curators as indicators for relevance.  To assist and expedite the search for information within the literature, automated text-mining tools are being developed; still, several shared tasks and competitive challenges demonstrated that the need for more effective automated identification of relevant information in biomedical publications remains a bottleneck for bio-curation and for scientific discovery. While image analysis within and outside the biomedical domain is an active research area, most current work on biomedical image processing focuses on retrieval and understanding of images as a primary form of data. Likewise, most efforts on biomedical literature retrieval and mining focus on text alone. Little has been done so far to use images within publications, which provide important cues as to the relevance of information embedded in papers.  The hypothesis underlying our proposal is that useful information can be derived directly from images within publications and integrated with text-based methods, leading to improved identification of relevant publications and of informative portions within them. The proposed research comprises extensive comparative study of highly-informative features within images, development and identification of such image-features, development of tools that extract such features and information from images, and integration of image-based information into the textual articles-classification process, aiming to determine the publications' relevance to well-defined biomedical needs. The fundamental research tasks we shall address are: A) Identification and comparative study of useful features for image-representation, focusing on their utility for specific biomedical needs; B) Classification of biomedical images and biomedical documents based on image-data; C) Document classification through integration of text- and image-based classifiers. To ground the research in genuine needs, secure access to much image data, and ensure broad-applicability of the results, we shall work within three diverse areas for which we have secured access to expertise and data: Finding articles about cis-regulatory regions (Cyrene project at Brown University); Evidence for gene expression in the mouse (Jackson Lab's GXD); Experimental evidence for protein-protein interaction (Delaware's Protein Information Resource). The successful completion of the proposed project will provide integrated methods and tools, utilizing both image-based and text-based features, leading to more focused and effective retrieval and mining tools, thus better supporting data-intensive biomedical discovery. Physicians and bio-medical scientists rely on the vast published literature as their main source of information about current developments and findings, on which they base both patient treatment and ongoing research toward bio- medical discovery. The proposed research will support physicians and scientists, speed-up their search for information and improve their effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage not only of text but also of the highly-informative image data within publications. The successful outcome of this research will lead to the development of focused, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting targeted biomedical discovery including better understanding of disease mechanism, uncovering possible means for accurate diagnoses, and revealing potential new drugs and drug-targets.",Utilizing Imaged-based Features in Biomedical Literature Classification,8892560,R56LM011354,"['Address', 'Area', 'Bioinformatics', 'Biological', 'Biological Phenomena', 'Categories', 'Classification', 'Comparative Study', 'Computational Biology', 'Computer-Assisted Image Analysis', 'Computers', 'Coupled', 'Cues', 'Data', 'Data Set', 'Databases', 'Delaware', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Drug Targeting', 'Ensure', 'Entropy', 'Gene Expression', 'Gene Proteins', 'Generic Drugs', 'Goals', 'Gold', 'Gray unit of radiation dose', 'Harvest', 'Image', 'Image Analysis', 'Improve Access', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Knowledge', 'Lead', 'Literature', 'Measures', 'Medical', 'Methods', 'Metric', 'Mining', 'Modeling', 'Mus', 'Nucleic Acid Regulatory Sequences', 'Organism', 'Outcomes Research', 'Paper', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Property', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reader', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Retrieval', 'Scanning', 'Scientist', 'Secure', 'Series', 'Shapes', 'Source', 'Speed', 'System', 'Testing', 'Text', 'Textbooks', 'Texture', 'Training', 'Universities', 'Voting', 'Work', 'authority', 'base', 'bioimaging', 'evaluation/testing', 'experience', 'fundamental research', 'image processing', 'improved', 'indexing', 'insight', 'mouse genome', 'protein protein interaction', 'research study', 'text searching', 'tool', 'tool development']",NLM,UNIVERSITY OF DELAWARE,R56,2014,280000,0.04416074793196246
"HYBRID ONTOLOGY AND MACHINE LEARNING BASED METHODS USING EMR DATA FOR EFFECTIVE CLINICAL DECISION SUPPORT (CDS); A SEPSIS CASE STUDY USING ICU DATA ﻿    DESCRIPTION (provided by applicant):  Our research is focused on the use of advanced ontological models as a foundation for computerized Clinical Decision Support (CDS) systems that link hospitalized patient data routinely captured in electronic medical records (EMRs) with medical knowledge to effectively influence timely awareness and treatment choices by clinicians. Our phase 1 goal is to demonstrate how our advanced CDS technology has the potential to improve preventable mortality outcomes associated with sepsis in ICU patients. We distinguish two types of CDS algorithms available today for detecting and/or predicting sepsis using EMR data: 1) evidence-based ""knowledge-driven"" detection algorithms; and 2) data-driven ""predictive"" algorithms based on machine learning (ML) techniques. Recent studies indicate currently available CDS tools do not reduce risk of death in hospitalized patients. We believe this may be because diseases such as sepsis are time-sensitive, complex syndromes and also due to the challenges of computerized reuse of unstructured EMR data. Our sepsis ontology models this complexity to: a) provide enhanced knowledge-driven sepsis risk- stratified cohort classifications that help guide interventions; b) support accurate natural language processing (NLP) of free text clinical notes to enhance real-time sepsis risk detection; and c) improve the accuracy of data- driven prediction models when used in conjunction with ML training algorithms. Our CDS is based on proprietary cluster computing technology we call ""VFusion"" designed to efficiently deal with the generation and practical use of large, application domain-specific ontologies. Our sepsis ontology employs a family of upper level ontologies, combined with granular evidence-based domain ontologies, configurable rule sets (e.g. first order logic-based), and required components of reference terminologies. Our research will use openly available ICU patient data to establish statistical detection/prediction performance metrics using this ontology in 2 modes of use: 1) as a knowledge-based screening tool to detect subtle signs of sepsis in individualized hospitalized patients; 2) used in conjunction with ML to improve data-driven predictive performance. We will measure specificity/sensitivity, and positive/negative predictive power of our hybrid ontology-based technology to demonstrate dramatically improved performance over existing CDS algorithms. In Phase II we plan a retrospective demonstration with a much larger sample of patients to include non-ICU patients in collaboration with a major healthcare system. Our product vision is an early inpatient sepsis detection algorithm with high accuracy embodied as a ""plug-in application"" compatible with any modern EMR platform in use at a client hospital effective in both ICU and non-ICU care settings.         PUBLIC HEALTH RELEVANCE:  Even with the advent of powerful, new computer medical record technologies used in hospitals, the risk of death has not been reduced in hospitalized patients. The goal of this research effort is to combine medical record systems with advanced cognitive technologies that are commonly used today in products such as the iPhone ""Siri"" to improve clinician awareness and decisions that will dramatically reduce preventable mortality. Our initial use case will be the management of sepsis--=the leading cause of death in non-coronary intensive care units in hospitals                ",HYBRID ONTOLOGY AND MACHINE LEARNING BASED METHODS USING EMR DATA FOR EFFECTIVE CLINICAL DECISION SUPPORT (CDS); A SEPSIS CASE STUDY USING ICU DATA,9046860,R43LM012291,"['Address', 'Age', 'Algorithms', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Awareness', 'Caring', 'Case Study', 'Cause of Death', 'Cellular Phone', 'Cessation of life', 'Classification', 'Client', 'Clinical', 'Clinical Decision Support Systems', 'Code', 'Cognitive', 'Collaborations', 'Comorbidity', 'Complex', 'Computerized Medical Record', 'Computers', 'Data', 'Data Set', 'Detection', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Early treatment', 'Family', 'Foundations', 'Generations', 'Goals', 'Healthcare Systems', 'Hospitals', 'Hour', 'Hybrids', 'Infection', 'Inpatients', 'Intensive Care Units', 'Intervention', 'Knowledge', 'Laboratories', 'Letters', 'Link', 'Logic', 'Machine Learning', 'Manuals', 'Measurement', 'Measures', 'Medical', 'Medical Records', 'Methods', 'Modeling', 'Mortality Determinants', 'Natural Language Processing', 'Ontology', 'Organ', 'Outcome', 'Patients', 'Performance', 'Phase', 'Physiological', 'Plug-in', 'Protocols documentation', 'Recording of previous events', 'Research', 'Retrospective Studies', 'Risk', 'Risk Factors', 'Sampling', 'Semantics', 'Sensitivity and Specificity', 'Sepsis', 'Septic Shock', 'Severities', 'Source', 'Specificity', 'Syndrome', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Time', 'Training', 'Validation', 'Vendor', 'Vision', 'antimicrobial', 'base', 'cluster computing', 'cohort', 'computerized', 'design', 'diagnostic accuracy', 'effective therapy', 'evidence base', 'improved', 'knowledge base', 'mortality', 'predictive modeling', 'public health relevance', 'screening', 'tool']",NLM,"COMPUTER TECHNOLOGY ASSOCIATES, INC.",R43,2015,149100,0.101976529334914
"Incorporating Image-based Features into Biomedical Document Classification The proposed research aims to develop and advance tools for using image-data appearing in scientific publications, in addition to text, in order to support beneficial, targeted access to the biomedical literature. The number of biomedical publications grows at a rate of over one million new publications per year. Identifying relevant information requires scientists and physicians to scan daily through a myriad of papers. For scientific database curators (bio-curators, in organizations such as Jackson Labs or UniProt), the task is particularly onerous, as they must identify articles most significant to the database, locate within them high-quality evidence concerning disease, genes/proteins and mutations, and curate the findings in database entries along with references to relevant evidence in the articles. Notably, much of the evidence within publications lies in figures. Thus, images are rich and essential indicators for relevance.  While biomedical text mining tools are being developed to expedite search for information within publications, several competitive shared tasks underscored the need for more effective tools to overcome the bottleneck for bio-curation and for scientific discovery. Moreover, bio-curators point-out the importance of images as a key information source. While image analysis is an active research field, most current work on biomedical image processing focuses on image identification, understanding and indexing; Not on images as aids to document analysis. Similarly, most work on biomedical literature mining focuses on text alone. Thus, little has been done so far to utilize, in addition to text, images within publications that provide important cues about the relevance of the information embedded in articles.  Our premise, supported by bio-curators experience, is that information derived from images can (and should) be directly incorporated into biomedical document retrieval and classification, and will improve accurate identification of relevant articles (for a given user’s needs) while pin-pointing significant evidence within them. We will comprehensively identify, develop and compare informative image-features, develop methods and tools for representing both images and documents based on such features, and introduce means to effectively integrate image-based data into the text-based document classification process. The work will comprise the following fundamental tasks: A) Building robust tools for harvesting images from PDF articles and segmenting compound figures into individual image-panels; B) Identification and investigation of highly-informative features for biomedical image-representation, and categorization of biomedical images into significant types and classes; C) Effective representation of documents using text and image, and integration of text-based and image-based classifiers. We anchor our research in genuine needs, secure access to much image data, and strive for broad-applicability of the results, by working within several broad and diverse curation-areas within institutes with which we collaborate: Evidence for gene-expression & phenotypes in Mouse (Jackson Labs) and in worm (WormBase), and experimental evidence for protein-protein interaction (Protein Information Resource). The work on this project will result in new methods and tools that take advantage of both image- and text-data, facilitating more effective and focused retrieval and mining, thus better supporting bio-curation and data-intensive biomedical discovery. Published biomedical literature forms a vast information-source for biomedical scientists and physicians; both treatment decisions and research toward bio-medical discovery are based on such information. The proposed research aims to support and speed-up the search for information while improving effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage of the highly-informative image data within publications. The successful outcome of this research will lead to the development of well-targeted, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting focused biomedical discovery, including better understanding of the role of gene mutations in disease mechanisms, uncovering interactions among proteins, and revealing potential new drugs and drug-targets.",Incorporating Image-based Features into Biomedical Document Classification,9989894,R01LM012527,"['Address', 'Area', 'Biological Phenomena', 'Categories', 'Classification', 'Collaborations', 'Computer-Assisted Image Analysis', 'Cues', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Targeting', 'Failure', 'Fluorescence Microscopy', 'Foundations', 'Gel', 'Gene Expression', 'Gene Mutation', 'Gene Proteins', 'Genomics', 'Geometry', 'Goals', 'Grain', 'Harvest', 'Image', 'Image Analysis', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Letters', 'Literature', 'Medical', 'Methods', 'Mining', 'Modeling', 'Mus', 'Mutation', 'Outcomes Research', 'Paper', 'Phenotype', 'Physicians', 'Positioning Attribute', 'Process', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Publishing', 'Research', 'Resource Informatics', 'Retrieval', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Secure', 'Shapes', 'Solid', 'Source', 'Speed', 'System', 'Text', 'Texture', 'Training', 'Work', 'WormBase', 'base', 'bioimaging', 'biomedical scientist', 'decision research', 'evaluation/testing', 'experience', 'experimental study', 'feature extraction', 'image processing', 'improved', 'indexing', 'mouse genome', 'multimodality', 'new therapeutic target', 'novel', 'protein protein interaction', 'protein structure', 'text searching', 'tool']",NLM,UNIVERSITY OF DELAWARE,R01,2020,463024,0.013735493597835696
"Incorporating Image-based Features into Biomedical Document Classification The proposed research aims to develop and advance tools for using image-data appearing in scientific publications, in addition to text, in order to support beneficial, targeted access to the biomedical literature. The number of biomedical publications grows at a rate of over one million new publications per year. Identifying relevant information requires scientists and physicians to scan daily through a myriad of papers. For scientific database curators (bio-curators, in organizations such as Jackson Labs or UniProt), the task is particularly onerous, as they must identify articles most significant to the database, locate within them high-quality evidence concerning disease, genes/proteins and mutations, and curate the findings in database entries along with references to relevant evidence in the articles. Notably, much of the evidence within publications lies in figures. Thus, images are rich and essential indicators for relevance.  While biomedical text mining tools are being developed to expedite search for information within publications, several competitive shared tasks underscored the need for more effective tools to overcome the bottleneck for bio-curation and for scientific discovery. Moreover, bio-curators point-out the importance of images as a key information source. While image analysis is an active research field, most current work on biomedical image processing focuses on image identification, understanding and indexing; Not on images as aids to document analysis. Similarly, most work on biomedical literature mining focuses on text alone. Thus, little has been done so far to utilize, in addition to text, images within publications that provide important cues about the relevance of the information embedded in articles.  Our premise, supported by bio-curators experience, is that information derived from images can (and should) be directly incorporated into biomedical document retrieval and classification, and will improve accurate identification of relevant articles (for a given user’s needs) while pin-pointing significant evidence within them. We will comprehensively identify, develop and compare informative image-features, develop methods and tools for representing both images and documents based on such features, and introduce means to effectively integrate image-based data into the text-based document classification process. The work will comprise the following fundamental tasks: A) Building robust tools for harvesting images from PDF articles and segmenting compound figures into individual image-panels; B) Identification and investigation of highly-informative features for biomedical image-representation, and categorization of biomedical images into significant types and classes; C) Effective representation of documents using text and image, and integration of text-based and image-based classifiers. We anchor our research in genuine needs, secure access to much image data, and strive for broad-applicability of the results, by working within several broad and diverse curation-areas within institutes with which we collaborate: Evidence for gene-expression & phenotypes in Mouse (Jackson Labs) and in worm (WormBase), and experimental evidence for protein-protein interaction (Protein Information Resource). The work on this project will result in new methods and tools that take advantage of both image- and text-data, facilitating more effective and focused retrieval and mining, thus better supporting bio-curation and data-intensive biomedical discovery. Published biomedical literature forms a vast information-source for biomedical scientists and physicians; both treatment decisions and research toward bio-medical discovery are based on such information. The proposed research aims to support and speed-up the search for information while improving effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage of the highly-informative image data within publications. The successful outcome of this research will lead to the development of well-targeted, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting focused biomedical discovery, including better understanding of the role of gene mutations in disease mechanisms, uncovering interactions among proteins, and revealing potential new drugs and drug-targets.",Incorporating Image-based Features into Biomedical Document Classification,9762175,R01LM012527,"['Address', 'Area', 'Biological Phenomena', 'Categories', 'Classification', 'Collaborations', 'Computer-Assisted Image Analysis', 'Cues', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Targeting', 'Failure', 'Fluorescence Microscopy', 'Foundations', 'Gel', 'Gene Expression', 'Gene Mutation', 'Gene Proteins', 'Genomics', 'Geometry', 'Goals', 'Grain', 'Harvest', 'Image', 'Image Analysis', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Letters', 'Literature', 'Medical', 'Methods', 'Mining', 'Modeling', 'Mus', 'Mutation', 'Outcomes Research', 'Paper', 'Phenotype', 'Physicians', 'Positioning Attribute', 'Process', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Publishing', 'Research', 'Resource Informatics', 'Retrieval', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Secure', 'Shapes', 'Solid', 'Source', 'Speed', 'Structural Protein', 'System', 'Text', 'Texture', 'Training', 'Work', 'base', 'bioimaging', 'biomedical scientist', 'decision research', 'evaluation/testing', 'experience', 'experimental study', 'image processing', 'improved', 'indexing', 'mouse genome', 'multimodality', 'new therapeutic target', 'novel', 'protein protein interaction', 'protein structure', 'text searching', 'tool']",NLM,UNIVERSITY OF DELAWARE,R01,2019,463024,0.013735493597835696
"Incorporating Image-based Features into Biomedical Document Classification The proposed research aims to develop and advance tools for using image-data appearing in scientific publications, in addition to text, in order to support beneficial, targeted access to the biomedical literature. The number of biomedical publications grows at a rate of over one million new publications per year. Identifying relevant information requires scientists and physicians to scan daily through a myriad of papers. For scientific database curators (bio-curators, in organizations such as Jackson Labs or UniProt), the task is particularly onerous, as they must identify articles most significant to the database, locate within them high-quality evidence concerning disease, genes/proteins and mutations, and curate the findings in database entries along with references to relevant evidence in the articles. Notably, much of the evidence within publications lies in figures. Thus, images are rich and essential indicators for relevance.  While biomedical text mining tools are being developed to expedite search for information within publications, several competitive shared tasks underscored the need for more effective tools to overcome the bottleneck for bio-curation and for scientific discovery. Moreover, bio-curators point-out the importance of images as a key information source. While image analysis is an active research field, most current work on biomedical image processing focuses on image identification, understanding and indexing; Not on images as aids to document analysis. Similarly, most work on biomedical literature mining focuses on text alone. Thus, little has been done so far to utilize, in addition to text, images within publications that provide important cues about the relevance of the information embedded in articles.  Our premise, supported by bio-curators experience, is that information derived from images can (and should) be directly incorporated into biomedical document retrieval and classification, and will improve accurate identification of relevant articles (for a given user’s needs) while pin-pointing significant evidence within them. We will comprehensively identify, develop and compare informative image-features, develop methods and tools for representing both images and documents based on such features, and introduce means to effectively integrate image-based data into the text-based document classification process. The work will comprise the following fundamental tasks: A) Building robust tools for harvesting images from PDF articles and segmenting compound figures into individual image-panels; B) Identification and investigation of highly-informative features for biomedical image-representation, and categorization of biomedical images into significant types and classes; C) Effective representation of documents using text and image, and integration of text-based and image-based classifiers. We anchor our research in genuine needs, secure access to much image data, and strive for broad-applicability of the results, by working within several broad and diverse curation-areas within institutes with which we collaborate: Evidence for gene-expression & phenotypes in Mouse (Jackson Labs) and in worm (WormBase), and experimental evidence for protein-protein interaction (Protein Information Resource). The work on this project will result in new methods and tools that take advantage of both image- and text-data, facilitating more effective and focused retrieval and mining, thus better supporting bio-curation and data-intensive biomedical discovery. Published biomedical literature forms a vast information-source for biomedical scientists and physicians; both treatment decisions and research toward bio-medical discovery are based on such information. The proposed research aims to support and speed-up the search for information while improving effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage of the highly-informative image data within publications. The successful outcome of this research will lead to the development of well-targeted, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting focused biomedical discovery, including better understanding of the role of gene mutations in disease mechanisms, uncovering interactions among proteins, and revealing potential new drugs and drug-targets.",Incorporating Image-based Features into Biomedical Document Classification,9565648,R01LM012527,"['Address', 'Area', 'Biological Phenomena', 'Categories', 'Classification', 'Collaborations', 'Computer-Assisted Image Analysis', 'Cues', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Targeting', 'Failure', 'Fluorescence Microscopy', 'Foundations', 'Gel', 'Gene Expression', 'Gene Mutation', 'Gene Proteins', 'Genomics', 'Geometry', 'Goals', 'Grain', 'Gray unit of radiation dose', 'Harvest', 'Image', 'Image Analysis', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Letters', 'Literature', 'Medical', 'Methods', 'Mining', 'Modality', 'Modeling', 'Mus', 'Mutation', 'Outcomes Research', 'Paper', 'Phenotype', 'Physicians', 'Positioning Attribute', 'Process', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Publishing', 'Research', 'Resource Informatics', 'Retrieval', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Secure', 'Shapes', 'Solid', 'Source', 'Speed', 'System', 'Text', 'Texture', 'Training', 'Work', 'base', 'bioimaging', 'biomedical scientist', 'decision research', 'evaluation/testing', 'experience', 'experimental study', 'image processing', 'improved', 'indexing', 'mouse genome', 'new therapeutic target', 'novel', 'protein protein interaction', 'protein structure', 'text searching', 'tool']",NLM,UNIVERSITY OF DELAWARE,R01,2018,437876,0.013735493597835696
"Incorporating Image-based Features into Biomedical Document Classification The proposed research aims to develop and advance tools for using image-data appearing in scientific publications, in addition to text, in order to support beneficial, targeted access to the biomedical literature. The number of biomedical publications grows at a rate of over one million new publications per year. Identifying relevant information requires scientists and physicians to scan daily through a myriad of papers. For scientific database curators (bio-curators, in organizations such as Jackson Labs or UniProt), the task is particularly onerous, as they must identify articles most significant to the database, locate within them high-quality evidence concerning disease, genes/proteins and mutations, and curate the findings in database entries along with references to relevant evidence in the articles. Notably, much of the evidence within publications lies in figures. Thus, images are rich and essential indicators for relevance.  While biomedical text mining tools are being developed to expedite search for information within publications, several competitive shared tasks underscored the need for more effective tools to overcome the bottleneck for bio-curation and for scientific discovery. Moreover, bio-curators point-out the importance of images as a key information source. While image analysis is an active research field, most current work on biomedical image processing focuses on image identification, understanding and indexing; Not on images as aids to document analysis. Similarly, most work on biomedical literature mining focuses on text alone. Thus, little has been done so far to utilize, in addition to text, images within publications that provide important cues about the relevance of the information embedded in articles.  Our premise, supported by bio-curators experience, is that information derived from images can (and should) be directly incorporated into biomedical document retrieval and classification, and will improve accurate identification of relevant articles (for a given user’s needs) while pin-pointing significant evidence within them. We will comprehensively identify, develop and compare informative image-features, develop methods and tools for representing both images and documents based on such features, and introduce means to effectively integrate image-based data into the text-based document classification process. The work will comprise the following fundamental tasks: A) Building robust tools for harvesting images from PDF articles and segmenting compound figures into individual image-panels; B) Identification and investigation of highly-informative features for biomedical image-representation, and categorization of biomedical images into significant types and classes; C) Effective representation of documents using text and image, and integration of text-based and image-based classifiers. We anchor our research in genuine needs, secure access to much image data, and strive for broad-applicability of the results, by working within several broad and diverse curation-areas within institutes with which we collaborate: Evidence for gene-expression & phenotypes in Mouse (Jackson Labs) and in worm (WormBase), and experimental evidence for protein-protein interaction (Protein Information Resource). The work on this project will result in new methods and tools that take advantage of both image- and text-data, facilitating more effective and focused retrieval and mining, thus better supporting bio-curation and data-intensive biomedical discovery. Published biomedical literature forms a vast information-source for biomedical scientists and physicians; both treatment decisions and research toward bio-medical discovery are based on such information. The proposed research aims to support and speed-up the search for information while improving effective access to the most relevant part of the biomedical literature, by developing new methods and tools that take advantage of the highly-informative image data within publications. The successful outcome of this research will lead to the development of well-targeted, effective tools for finding information pertinent to biological phenomena and medical needs, thus expediting focused biomedical discovery, including better understanding of the role of gene mutations in disease mechanisms, uncovering interactions among proteins, and revealing potential new drugs and drug-targets.",Incorporating Image-based Features into Biomedical Document Classification,9457095,R01LM012527,"['Address', 'Area', 'Biological Phenomena', 'Categories', 'Cereals', 'Classification', 'Collaborations', 'Computer-Assisted Image Analysis', 'Cues', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Failure', 'Fluorescence Microscopy', 'Foundations', 'Gel', 'Gene Expression', 'Gene Mutation', 'Gene Proteins', 'Genomics', 'Geometry', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Image', 'Image Analysis', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Letters', 'Literature', 'Medical', 'Methods', 'Mining', 'Modality', 'Modeling', 'Mus', 'Mutation', 'Outcomes Research', 'Paper', 'Phenotype', 'Physicians', 'Positioning Attribute', 'Process', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Publishing', 'Research', 'Resource Informatics', 'Retrieval', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Secure', 'Shapes', 'Solid', 'Source', 'Speed', 'System', 'Text', 'Texture', 'Training', 'Work', 'base', 'bioimaging', 'biomedical scientist', 'decision research', 'evaluation/testing', 'experience', 'experimental study', 'image processing', 'improved', 'indexing', 'mouse genome', 'new therapeutic target', 'novel', 'novel therapeutics', 'protein protein interaction', 'protein structure', 'text searching', 'tool']",NLM,UNIVERSITY OF DELAWARE,R01,2017,488239,0.013735493597835696
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9979969,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Big Data Methods', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Estrogen receptor positive', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'data standards', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'public repository', 'repository', 'structured data', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2020,511367,0.15689584106020057
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9747967,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Big Data Methods', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2019,514129,0.15689584106020057
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9527186,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Methods', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2018,516810,0.15689584106020057
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9398728,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Development', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Methods', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2017,546372,0.15689584106020057
"Feasibility of a Therapeutic Intent Ontology Program Director/Principal Investigator (Last, First, Middle): Nelson, Stuart J. ABSTRACT Therapeutic intent can be thought of as the reason for commencing a therapy, not just a disease being treated, but the context in which the therapy is appropriate. We propose to investigate if it is possible to represent the therapeutic intent as seen in drug indications in the Structured Product Label approved by the FDA in a formal, computable manner, using standard controlled vocabularies and a model (an upper level application ontology). If successful, such a representation could be invaluable in clinical decision support, precision medicine, and in deepening our understanding of the uses of drugs. We propose studying the labels of 1000 RxNorm clinical drugs as a sample size, and basing a model on that sample. Then, by providing a web annotation tool, we would be able to evaluate the utility of the model by using both an internally developed set or criteria as well as outside use and review. OMB No. 0925-0001/0002 (Rev. 03/16 Approved Through 10/31/2018) Page 1 Continuation Format Page Program Director/Principal Investigator (Last, First, Middle): Nelson, Stuart J. NARRATIVE We propose to study if it is possible to represent the reasons for giving a medication in a way that a computer can process it. If possible, this should be the basis of improving decision support offered to a prescriber. It might also help us understand more about how and when to use medications. OMB No. 0925-0001/0002 (Rev. 03/16 Approved Through 10/31/2018) Page 1 Continuation Format Page",Feasibility of a Therapeutic Intent Ontology,9583840,R21LM012929,"['Anatomy', 'Awareness', 'Catalogs', 'Clinical', 'Complex', 'Computers', 'Controlled Vocabulary', 'Data', 'Development', 'Disease', 'Drug usage', 'Expert Opinion', 'FDA approved', 'Genetic', 'Goals', 'Health', 'Human', 'Information Resources Management', 'Internet', 'Label', 'Methods', 'Modeling', 'Nature', 'Ontology', 'Pain', 'Pathologic', 'Patient Care', 'Pharmaceutical Preparations', 'Physiological', 'Principal Investigator', 'Process', 'Product Labeling', 'Public Domains', 'Readability', 'Reproducibility', 'Research Infrastructure', 'Resources', 'Sample Size', 'Sampling', 'Semantics', 'Structure', 'Symptoms', 'Terminology', 'Therapeutic', 'Therapeutic Uses', 'Vocabulary', 'Work', 'Yang', 'annotation  system', 'base', 'cheminformatics', 'clinical decision support', 'digital', 'improved', 'information organization', 'precision medicine', 'programs', 'tool', 'usability', 'web site']",NLM,UNIVERSITY OF NEW MEXICO HEALTH SCIS CTR,R21,2018,48610,0.09313575220904421
"Feasibility of a Therapeutic Intent Ontology Program Director/Principal Investigator (Last, First, Middle): Nelson, Stuart J. ABSTRACT Therapeutic intent can be thought of as the reason for commencing a therapy, not just a disease being treated, but the context in which the therapy is appropriate. We propose to investigate if it is possible to represent the therapeutic intent as seen in drug indications in the Structured Product Label approved by the FDA in a formal, computable manner, using standard controlled vocabularies and a model (an upper level application ontology). If successful, such a representation could be invaluable in clinical decision support, precision medicine, and in deepening our understanding of the uses of drugs. We propose studying the labels of 1000 RxNorm clinical drugs as a sample size, and basing a model on that sample. Then, by providing a web annotation tool, we would be able to evaluate the utility of the model by using both an internally developed set or criteria as well as outside use and review. OMB No. 0925-0001/0002 (Rev. 03/16 Approved Through 10/31/2018) Page 1 Continuation Format Page Program Director/Principal Investigator (Last, First, Middle): Nelson, Stuart J. NARRATIVE We propose to study if it is possible to represent the reasons for giving a medication in a way that a computer can process it. If possible, this should be the basis of improving decision support offered to a prescriber. It might also help us understand more about how and when to use medications. OMB No. 0925-0001/0002 (Rev. 03/16 Approved Through 10/31/2018) Page 1 Continuation Format Page",Feasibility of a Therapeutic Intent Ontology,9767858,R21LM012929,"['Anatomy', 'Awareness', 'Catalogs', 'Clinical', 'Complex', 'Computers', 'Controlled Vocabulary', 'Data', 'Development', 'Disease', 'Drug usage', 'Expert Opinion', 'FDA approved', 'Genetic', 'Goals', 'Health', 'Human', 'Information Resources Management', 'Infrastructure', 'Internet', 'Label', 'Methods', 'Modeling', 'Nature', 'Ontology', 'Pain', 'Pathologic', 'Patient Care', 'Pharmaceutical Preparations', 'Physiological', 'Principal Investigator', 'Process', 'Product Labeling', 'Public Domains', 'Readability', 'Reproducibility', 'Resources', 'Sample Size', 'Sampling', 'Semantics', 'Structure', 'Symptoms', 'Terminology', 'Therapeutic', 'Therapeutic Uses', 'Vocabulary', 'Work', 'Yang', 'annotation  system', 'base', 'cheminformatics', 'clinical decision support', 'digital', 'improved', 'information organization', 'precision medicine', 'programs', 'tool', 'usability', 'web site']",NLM,GEORGE WASHINGTON UNIVERSITY,R21,2019,215325,0.09313575220904421
"Genomic Database for Candida Albicans    DESCRIPTION (provided by applicant): The goal of the Candida Genome Database (CGD) is to implement and develop a database containing comprehensive annotated information about the genome of the human fungal pathogen, Candida albicans. C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. It is thus vital that there is a comprehensive and up to date resource for researchers investigating the biology and pathogenesis of C. albicans, as such a resource accelerates their research. In this proposal we propose to improve the C. albicans' genome sequence and its annotation, the incorporation of the genome sequences of other closely related fungi, and curation of the research literature for those organisms. We also propose to incorporate tools for the visualization and searching of microarray data, and to add new protein pages to the database with comprehensive details about predicted proteins encoded by the C. albicans genome. Finally, we propose to continue our core mission of curating the C. albicans literature, extracting gene names, descriptions, phenotypes and Gene Ontology terms from the current literature on an ongoing basis. Together, successful completion of these aims will support and accelerate research into C. albicans, and thus have a positive impact on human health.      PUBLIC HEALTH RELEVANCE: C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. While many antifungal compounds do exist, these drugs are often of limited use because of their toxicity and side effects. In addition, there has been an emergence of antifungal resistance in the clinical setting. For example, significant resistance to the azole class of antifungal drugs has developed, especially in HIV-positive adults, where resistant strains are present in 21-32% of symptomatic patients. Thus, there is a need for alternative antifungal agents that are more specifically directed at the fungal cell and less toxic to human cells, and thus it is vital that C. albicans research continue as rapidly as possible. Successful completion of this project, to provide a curated and comprehensive Candida albicans database, will accelerate Candida research, and in doing so will aid in the fight against C. labicans infections, and thus significantly positively impact human health.           Project Narrative C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. While many antifungal compounds do exist, these drugs are often of limited use because of their toxicity and side effects. In addition, there has been an emergence of antifungal resistance in the clinical setting. For example, significant resistance to the azole class of antifungal drugs has developed, especially in HIV-positive adults, where resistant strains are present in 21-32% of symptomatic patients. Thus, there is a need for alternative antifungal agents that are more specifically directed at the fungal cell and less toxic to human cells, and thus it is vital that C. albicans research continue as rapidly as possible. Successful completion of this project, to provide a curated and comprehensive Candida albicans database, will accelerate Candida research, and in doing so will aid in the fight against C. labicans infections, and thus significantly positively impact human health.",Genomic Database for Candida Albicans,8270327,R01DE015873,"['Adopted', 'Adult', 'Adverse effects', 'Antifungal Agents', 'Azole resistance', 'Binding Sites', 'Biochemical Pathway', 'Biological', 'Biology', 'Blood Circulation', 'Candida', 'Candida albicans', 'Candidiasis', 'Cataloging', 'Catalogs', 'Cells', 'Characteristics', 'Chromatin Structure', 'Chromosomes', 'Clinical', 'Collaborations', 'Communities', 'Community Surveys', 'Comparative Genomic Analysis', 'Computer software', 'Consensus', 'Consultations', 'Data', 'Data Set', 'Databases', 'Development', 'Diploidy', 'Elements', 'Ensure', 'Faculty', 'Fungal Drug Resistance', 'Future', 'Genes', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'HIV Seropositivity', 'Haplotypes', 'Health', 'Health Services Accessibility', 'Human', 'Human Genome', 'Hydrophobicity', 'Imagery', 'Infection', 'Institutes', 'Internet', 'Isoelectric Point', 'Laboratories', 'Literature', 'Maintenance', 'Maps', 'Messenger RNA', 'Mission', 'Molecular Weight', 'Names', 'Ontology', 'Organism', 'Orthologous Gene', 'Other Genetics', 'Pathogenesis', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Postdoctoral Fellow', 'Property', 'Proteins', 'Quality Control', 'Reading', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Respondent', 'Scientist', 'Sequence Analysis', 'Services', 'Site', 'Source', 'Structure', 'Surveys', 'System', 'Techniques', 'Tertiary Protein Structure', 'Testing', 'Toxic effect', 'Unit of Measure', 'base', 'comparative', 'fight against', 'flexibility', 'fungus', 'gene discovery', 'genetic element', 'genome database', 'genome sequencing', 'genome-wide', 'graduate student', 'improved', 'interest', 'meetings', 'mortality', 'mutant', 'new technology', 'pathogen', 'protein function', 'public health relevance', 'repository', 'research study', 'resistant strain', 'text searching', 'tool', 'transcription factor', 'web site']",NIDCR,STANFORD UNIVERSITY,R01,2012,784195,-0.002441246005118734
"Genomic Database for Candida Albicans    DESCRIPTION (provided by applicant): The goal of the Candida Genome Database (CGD) is to implement and develop a database containing comprehensive annotated information about the genome of the human fungal pathogen, Candida albicans. C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. It is thus vital that there is a comprehensive and up to date resource for researchers investigating the biology and pathogenesis of C. albicans, as such a resource accelerates their research. In this proposal we propose to improve the C. albicans' genome sequence and its annotation, the incorporation of the genome sequences of other closely related fungi, and curation of the research literature for those organisms. We also propose to incorporate tools for the visualization and searching of microarray data, and to add new protein pages to the database with comprehensive details about predicted proteins encoded by the C. albicans genome. Finally, we propose to continue our core mission of curating the C. albicans literature, extracting gene names, descriptions, phenotypes and Gene Ontology terms from the current literature on an ongoing basis. Together, successful completion of these aims will support and accelerate research into C. albicans, and thus have a positive impact on human health.      PUBLIC HEALTH RELEVANCE: C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. While many antifungal compounds do exist, these drugs are often of limited use because of their toxicity and side effects. In addition, there has been an emergence of antifungal resistance in the clinical setting. For example, significant resistance to the azole class of antifungal drugs has developed, especially in HIV-positive adults, where resistant strains are present in 21-32% of symptomatic patients. Thus, there is a need for alternative antifungal agents that are more specifically directed at the fungal cell and less toxic to human cells, and thus it is vital that C. albicans research continue as rapidly as possible. Successful completion of this project, to provide a curated and comprehensive Candida albicans database, will accelerate Candida research, and in doing so will aid in the fight against C. labicans infections, and thus significantly positively impact human health.           Project Narrative C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. While many antifungal compounds do exist, these drugs are often of limited use because of their toxicity and side effects. In addition, there has been an emergence of antifungal resistance in the clinical setting. For example, significant resistance to the azole class of antifungal drugs has developed, especially in HIV-positive adults, where resistant strains are present in 21-32% of symptomatic patients. Thus, there is a need for alternative antifungal agents that are more specifically directed at the fungal cell and less toxic to human cells, and thus it is vital that C. albicans research continue as rapidly as possible. Successful completion of this project, to provide a curated and comprehensive Candida albicans database, will accelerate Candida research, and in doing so will aid in the fight against C. labicans infections, and thus significantly positively impact human health.",Genomic Database for Candida Albicans,8045455,R01DE015873,"['Adopted', 'Adult', 'Adverse effects', 'Antifungal Agents', 'Azole resistance', 'Binding Sites', 'Biochemical Pathway', 'Biological', 'Biology', 'Blood Circulation', 'Candida', 'Candida albicans', 'Candidiasis', 'Cataloging', 'Catalogs', 'Cells', 'Characteristics', 'Chromatin Structure', 'Chromosomes', 'Clinical', 'Collaborations', 'Communities', 'Community Surveys', 'Comparative Genomic Analysis', 'Computer software', 'Consensus', 'Consultations', 'Data', 'Data Set', 'Databases', 'Development', 'Diploidy', 'Elements', 'Ensure', 'Faculty', 'Fungal Drug Resistance', 'Future', 'Genes', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'HIV Seropositivity', 'Haplotypes', 'Health', 'Health Services Accessibility', 'Human', 'Human Genome', 'Hydrophobicity', 'Imagery', 'Infection', 'Institutes', 'Internet', 'Isoelectric Point', 'Laboratories', 'Literature', 'Maintenance', 'Maps', 'Messenger RNA', 'Mission', 'Molecular Weight', 'Names', 'Ontology', 'Organism', 'Orthologous Gene', 'Other Genetics', 'Pathogenesis', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Postdoctoral Fellow', 'Property', 'Proteins', 'Quality Control', 'Reading', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Respondent', 'Scientist', 'Sequence Analysis', 'Services', 'Site', 'Source', 'Structure', 'Surveys', 'System', 'Techniques', 'Tertiary Protein Structure', 'Testing', 'Toxic effect', 'Unit of Measure', 'base', 'comparative', 'fight against', 'flexibility', 'fungus', 'gene discovery', 'genetic element', 'genome database', 'genome sequencing', 'genome-wide', 'graduate student', 'improved', 'interest', 'meetings', 'mortality', 'mutant', 'new technology', 'pathogen', 'protein function', 'public health relevance', 'repository', 'research study', 'resistant strain', 'text searching', 'tool', 'transcription factor', 'web site']",NIDCR,STANFORD UNIVERSITY,R01,2011,772961,-0.002441246005118734
"Genomic Database for Candida Albicans    DESCRIPTION (provided by applicant): The goal of the Candida Genome Database (CGD) is to implement and develop a database containing comprehensive annotated information about the genome of the human fungal pathogen, Candida albicans. C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. It is thus vital that there is a comprehensive and up to date resource for researchers investigating the biology and pathogenesis of C. albicans, as such a resource accelerates their research. In this proposal we propose to improve the C. albicans' genome sequence and its annotation, the incorporation of the genome sequences of other closely related fungi, and curation of the research literature for those organisms. We also propose to incorporate tools for the visualization and searching of microarray data, and to add new protein pages to the database with comprehensive details about predicted proteins encoded by the C. albicans genome. Finally, we propose to continue our core mission of curating the C. albicans literature, extracting gene names, descriptions, phenotypes and Gene Ontology terms from the current literature on an ongoing basis. Together, successful completion of these aims will support and accelerate research into C. albicans, and thus have a positive impact on human health.      PUBLIC HEALTH RELEVANCE: C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. While many antifungal compounds do exist, these drugs are often of limited use because of their toxicity and side effects. In addition, there has been an emergence of antifungal resistance in the clinical setting. For example, significant resistance to the azole class of antifungal drugs has developed, especially in HIV-positive adults, where resistant strains are present in 21-32% of symptomatic patients. Thus, there is a need for alternative antifungal agents that are more specifically directed at the fungal cell and less toxic to human cells, and thus it is vital that C. albicans research continue as rapidly as possible. Successful completion of this project, to provide a curated and comprehensive Candida albicans database, will accelerate Candida research, and in doing so will aid in the fight against C. labicans infections, and thus significantly positively impact human health.           Project Narrative C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. While many antifungal compounds do exist, these drugs are often of limited use because of their toxicity and side effects. In addition, there has been an emergence of antifungal resistance in the clinical setting. For example, significant resistance to the azole class of antifungal drugs has developed, especially in HIV-positive adults, where resistant strains are present in 21-32% of symptomatic patients. Thus, there is a need for alternative antifungal agents that are more specifically directed at the fungal cell and less toxic to human cells, and thus it is vital that C. albicans research continue as rapidly as possible. Successful completion of this project, to provide a curated and comprehensive Candida albicans database, will accelerate Candida research, and in doing so will aid in the fight against C. labicans infections, and thus significantly positively impact human health.",Genomic Database for Candida Albicans,7800372,R01DE015873,"['Adopted', 'Adult', 'Adverse effects', 'Antifungal Agents', 'Azole resistance', 'Binding Sites', 'Biochemical Pathway', 'Biological', 'Biology', 'Blood Circulation', 'Candida', 'Candida albicans', 'Candidiasis', 'Cataloging', 'Catalogs', 'Cells', 'Characteristics', 'Chromatin Structure', 'Chromosomes', 'Clinical', 'Collaborations', 'Communities', 'Community Surveys', 'Comparative Genomic Analysis', 'Computer software', 'Consensus', 'Consultations', 'Data', 'Data Set', 'Databases', 'Development', 'Diploidy', 'Elements', 'Ensure', 'Face', 'Faculty', 'Fungal Drug Resistance', 'Future', 'Genes', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'HIV Seropositivity', 'Haplotypes', 'Health', 'Health Services Accessibility', 'Human', 'Human Genome', 'Hydrophobicity', 'Imagery', 'Infection', 'Institutes', 'Internet', 'Isoelectric Point', 'Laboratories', 'Literature', 'Maintenance', 'Maps', 'Messenger RNA', 'Mission', 'Molecular Weight', 'Names', 'Ontology', 'Organism', 'Orthologous Gene', 'Other Genetics', 'Pathogenesis', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Property', 'Proteins', 'Quality Control', 'Reading', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Respondent', 'Scientist', 'Sequence Analysis', 'Services', 'Site', 'Source', 'Structure', 'Surveys', 'System', 'Techniques', 'Tertiary Protein Structure', 'Testing', 'Toxic effect', 'Unit of Measure', 'base', 'comparative', 'fight against', 'flexibility', 'fungus', 'gene discovery', 'genetic element', 'genome database', 'genome sequencing', 'genome-wide', 'graduate student', 'improved', 'interest', 'meetings', 'mortality', 'mutant', 'new technology', 'pathogen', 'protein function', 'public health relevance', 'repository', 'research study', 'resistant strain', 'text searching', 'tool', 'transcription factor', 'web site']",NIDCR,STANFORD UNIVERSITY,R01,2010,780768,-0.002441246005118734
"Genomic Database for Candida Albicans    DESCRIPTION (provided by applicant): The goal of the Candida Genome Database (CGD) is to implement and develop a database containing comprehensive annotated information about the genome of the human fungal pathogen, Candida albicans. C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. It is thus vital that there is a comprehensive and up to date resource for researchers investigating the biology and pathogenesis of C. albicans, as such a resource accelerates their research. In this proposal we propose to improve the C. albicans' genome sequence and its annotation, the incorporation of the genome sequences of other closely related fungi, and curation of the research literature for those organisms. We also propose to incorporate tools for the visualization and searching of microarray data, and to add new protein pages to the database with comprehensive details about predicted proteins encoded by the C. albicans genome. Finally, we propose to continue our core mission of curating the C. albicans literature, extracting gene names, descriptions, phenotypes and Gene Ontology terms from the current literature on an ongoing basis. Together, successful completion of these aims will support and accelerate research into C. albicans, and thus have a positive impact on human health.      PUBLIC HEALTH RELEVANCE: C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. While many antifungal compounds do exist, these drugs are often of limited use because of their toxicity and side effects. In addition, there has been an emergence of antifungal resistance in the clinical setting. For example, significant resistance to the azole class of antifungal drugs has developed, especially in HIV-positive adults, where resistant strains are present in 21-32% of symptomatic patients. Thus, there is a need for alternative antifungal agents that are more specifically directed at the fungal cell and less toxic to human cells, and thus it is vital that C. albicans research continue as rapidly as possible. Successful completion of this project, to provide a curated and comprehensive Candida albicans database, will accelerate Candida research, and in doing so will aid in the fight against C. labicans infections, and thus significantly positively impact human health.           Project Narrative C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. While many antifungal compounds do exist, these drugs are often of limited use because of their toxicity and side effects. In addition, there has been an emergence of antifungal resistance in the clinical setting. For example, significant resistance to the azole class of antifungal drugs has developed, especially in HIV-positive adults, where resistant strains are present in 21-32% of symptomatic patients. Thus, there is a need for alternative antifungal agents that are more specifically directed at the fungal cell and less toxic to human cells, and thus it is vital that C. albicans research continue as rapidly as possible. Successful completion of this project, to provide a curated and comprehensive Candida albicans database, will accelerate Candida research, and in doing so will aid in the fight against C. labicans infections, and thus significantly positively impact human health.",Genomic Database for Candida Albicans,7649020,R01DE015873,"['Adopted', 'Adult', 'Adverse effects', 'Antifungal Agents', 'Azole resistance', 'Binding Sites', 'Biochemical Pathway', 'Biological', 'Biology', 'Blood Circulation', 'Candida', 'Candida albicans', 'Candidiasis', 'Cataloging', 'Catalogs', 'Cells', 'Characteristics', 'Chromatin Structure', 'Chromosomes', 'Clinical', 'Collaborations', 'Communities', 'Community Surveys', 'Comparative Genomic Analysis', 'Computer software', 'Consensus', 'Consultations', 'Data', 'Data Set', 'Databases', 'Development', 'Diploidy', 'Elements', 'Ensure', 'Face', 'Faculty', 'Fungal Drug Resistance', 'Future', 'Genes', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'HIV Seropositivity', 'Haplotypes', 'Health', 'Health Services Accessibility', 'Human', 'Human Genome', 'Hydrophobicity', 'Imagery', 'Infection', 'Institutes', 'Internet', 'Isoelectric Point', 'Laboratories', 'Literature', 'Maintenance', 'Maps', 'Messenger RNA', 'Mission', 'Molecular Weight', 'Names', 'Ontology', 'Organism', 'Orthologous Gene', 'Other Genetics', 'Pathogenesis', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Property', 'Proteins', 'Quality Control', 'Reading', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Respondent', 'Scientist', 'Sequence Analysis', 'Services', 'Site', 'Source', 'Structure', 'Surveys', 'System', 'Techniques', 'Tertiary Protein Structure', 'Testing', 'Toxic effect', 'Unit of Measure', 'base', 'comparative', 'fight against', 'flexibility', 'fungus', 'gene discovery', 'genetic element', 'genome database', 'genome sequencing', 'genome-wide', 'graduate student', 'improved', 'interest', 'meetings', 'mortality', 'mutant', 'new technology', 'pathogen', 'protein function', 'public health relevance', 'repository', 'research study', 'resistant strain', 'text searching', 'tool', 'transcription factor', 'web site']",NIDCR,STANFORD UNIVERSITY,R01,2009,813322,-0.002441246005118734
"Genomic Database for Candida Albicans    DESCRIPTION (provided by applicant): The goal of the Candida Genome Database (CGD) is to implement and develop a database containing comprehensive annotated information about the genome of the human fungal pathogen, Candida albicans. C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. It is thus vital that there is a comprehensive and up to date resource for researchers investigating the biology and pathogenesis of C. albicans, as such a resource accelerates their research. In this proposal we propose to improve the C. albicans' genome sequence and its annotation, the incorporation of the genome sequences of other closely related fungi, and curation of the research literature for those organisms. We also propose to incorporate tools for the visualization and searching of microarray data, and to add new protein pages to the database with comprehensive details about predicted proteins encoded by the C. albicans genome. Finally, we propose to continue our core mission of curating the C. albicans literature, extracting gene names, descriptions, phenotypes and Gene Ontology terms from the current literature on an ongoing basis. Together, successful completion of these aims will support and accelerate research into C. albicans, and thus have a positive impact on human health.       PUBLIC HEALTH RELEVANCE: C. albicans has become the third or fourth most common nosocomial bloodstream isolate; mortality rates are high (35% or greater) and treatment is costly. While many antifungal compounds do exist, these drugs are often of limited use because of their toxicity and side effects. In addition, there has been an emergence of antifungal resistance in the clinical setting. For example, significant resistance to the azole class of antifungal drugs has developed, especially in HIV-positive adults, where resistant strains are present in 21-32% of symptomatic patients. Thus, there is a need for alternative antifungal agents that are more specifically directed at the fungal cell and less toxic to human cells, and thus it is vital that C. albicans research continue as rapidly as possible. Successful completion of this project, to provide a curated and comprehensive Candida albicans database, will accelerate Candida research, and in doing so will aid in the fight against C. labicans infections, and thus significantly positively impact human health.         ",Genomic Database for Candida Albicans,8437277,R01DE015873,"['Adopted', 'Adult', 'Adverse effects', 'Antifungal Agents', 'Azole resistance', 'Binding Sites', 'Biochemical Pathway', 'Biological', 'Biology', 'Blood Circulation', 'Candida', 'Candida albicans', 'Candidiasis', 'Cataloging', 'Catalogs', 'Cells', 'Characteristics', 'Chromatin Structure', 'Chromosomes', 'Clinical', 'Collaborations', 'Communities', 'Community Surveys', 'Comparative Genomic Analysis', 'Computer software', 'Consensus', 'Consultations', 'Data', 'Data Set', 'Databases', 'Development', 'Diploidy', 'Elements', 'Ensure', 'Faculty', 'Fungal Drug Resistance', 'Future', 'Genes', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'HIV Seropositivity', 'Haplotypes', 'Health', 'Health Services Accessibility', 'Human', 'Human Genome', 'Hydrophobicity', 'Imagery', 'Infection', 'Institutes', 'Internet', 'Isoelectric Point', 'Laboratories', 'Literature', 'Maintenance', 'Maps', 'Messenger RNA', 'Mission', 'Molecular Weight', 'Names', 'Ontology', 'Organism', 'Orthologous Gene', 'Other Genetics', 'Pathogenesis', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Postdoctoral Fellow', 'Property', 'Proteins', 'Quality Control', 'Reading', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Respondent', 'Scientist', 'Sequence Analysis', 'Services', 'Site', 'Source', 'Structure', 'Surveys', 'System', 'Techniques', 'Tertiary Protein Structure', 'Testing', 'Toxic effect', 'Unit of Measure', 'base', 'comparative', 'fight against', 'flexibility', 'fungus', 'gene discovery', 'genetic element', 'genome database', 'genome sequencing', 'genome-wide', 'graduate student', 'improved', 'interest', 'meetings', 'mortality', 'mutant', 'new technology', 'pathogen', 'protein function', 'public health relevance', 'repository', 'research study', 'resistant strain', 'text searching', 'tool', 'transcription factor', 'web site']",NIDCR,STANFORD UNIVERSITY,R01,2013,742044,0.04159854539850175
"Information Integration of Heterogeneous Data Sources    DESCRIPTION (provided by applicant): The wealth of biological and biomedical data constantly being generated promises dramatic advancement in the life sciences. To realize this promise, this pool of rapidly expanding information needs to be efficiently integrated, that is, combined in such a way that it can be queried to extract relevant data that can be subsequently analyzed to answer meaningful research questions. The main objective of this proposal is to develop the GeneTegra System, an information integration solution that provides a common interaction environment to query data and knowledge from multiple sources. Two main obstacles have to be overcome in order to attain an effective integration of knowledge from different data sources: syntactic heterogeneity, where data sources have different representation and access mechanisms; and semantic variability, where similar lexical terms may refer to multiple concepts or dissimilar terms refer to the same concept. The GeneTegra System addresses these obstacles through the use of Semantic Web technologies: ontologies constructed using the Web Ontology Language (OWL) as a common data and knowledge representation for data sources of diverse formats, automated mechanisms for the generation and maintenance of these ontology representations, and a robust system architecture based on reusable, service-oriented mediators. The core of the proposed system consists of general algorithms, procedures, and mechanisms developed during Phase I of this project, that enable the automatic generation of ontologies, the automated identification of semantic correspondences between ontology models, and the creation and execution of queries over these ontology- modeled, distributed, heterogeneous sources. In Phase II, the GeneTegra System will be developed, implemented, and tested as a human-centered solution building on the core components developed during Phase I, incorporating a highly usable interface for query creation and execution, a mechanism for registration, sharing, and re-use of information using Web Services standards, a mechanism for determining quality of data and query reliability, and a security and privacy subsystem that allows the construction of collaborative communities while ensuring that users are properly authenticated and authorized to access information through the system. The GeneTegra System will be designed and evaluated to specifically address the integration of sources relevant to investigations of genotype-phenotype associations and to the identification of genes responsible for human diseases and conditions. PUBLIC HEALTH RELEVANCE The GeneTegra System is an information integration solution that provides a common interaction environment to query data and knowledge from multiple heterogeneous sources. It uses ontologies as the base formulism for semantic and syntactic modeling, and contains automated mechanisms for the generation of these ontologies, and for the reuse and sharing of integration configurations. It is specifically designed to address the integrated querying of sources relevant to investigations of genotype-phenotype associations and to the identification of genes responsible for human diseases and conditions.          n/a",Information Integration of Heterogeneous Data Sources,7798074,R44RR018667,"['Access to Information', 'Address', 'Algorithms', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Characteristics', 'Code', 'Communities', 'Computer software', 'Data', 'Data Quality', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Environment', 'Evaluation', 'Extensible Markup Language', 'Generations', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Heterogeneity', 'Human', 'Human Genetics', 'Information Systems', 'Institution', 'Internet', 'Investigation', 'Knowledge', 'Language', 'Maintenance', 'Maps', 'Mediator of activation protein', 'Mental Health', 'Mental disorders', 'Methods', 'Modeling', 'Ontology', 'Performance', 'Phase', 'Phenotype', 'Predisposition', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Methodology', 'Research Personnel', 'Security', 'Semantics', 'Services', 'Solutions', 'Source', 'Staff Development', 'Strigiformes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'abstracting', 'base', 'computer based Semantic Analysis', 'computerized', 'design', 'human centered computing', 'human disease', 'information gathering', 'information organization', 'lexical', 'programs', 'public health relevance', 'relational database', 'syntax', 'system architecture', 'usability']",NCRR,"INFOTECH SOFT, INC.",R44,2010,507489,0.1380661787046334
"Information Integration of Heterogeneous Data Sources    DESCRIPTION (provided by applicant): The wealth of biological and biomedical data constantly being generated promises dramatic advancement in the life sciences. To realize this promise, this pool of rapidly expanding information needs to be efficiently integrated, that is, combined in such a way that it can be queried to extract relevant data that can be subsequently analyzed to answer meaningful research questions. The main objective of this proposal is to develop the GeneTegra System, an information integration solution that provides a common interaction environment to query data and knowledge from multiple sources. Two main obstacles have to be overcome in order to attain an effective integration of knowledge from different data sources: syntactic heterogeneity, where data sources have different representation and access mechanisms; and semantic variability, where similar lexical terms may refer to multiple concepts or dissimilar terms refer to the same concept. The GeneTegra System addresses these obstacles through the use of Semantic Web technologies: ontologies constructed using the Web Ontology Language (OWL) as a common data and knowledge representation for data sources of diverse formats, automated mechanisms for the generation and maintenance of these ontology representations, and a robust system architecture based on reusable, service-oriented mediators. The core of the proposed system consists of general algorithms, procedures, and mechanisms developed during Phase I of this project, that enable the automatic generation of ontologies, the automated identification of semantic correspondences between ontology models, and the creation and execution of queries over these ontology- modeled, distributed, heterogeneous sources. In Phase II, the GeneTegra System will be developed, implemented, and tested as a human-centered solution building on the core components developed during Phase I, incorporating a highly usable interface for query creation and execution, a mechanism for registration, sharing, and re-use of information using Web Services standards, a mechanism for determining quality of data and query reliability, and a security and privacy subsystem that allows the construction of collaborative communities while ensuring that users are properly authenticated and authorized to access information through the system. The GeneTegra System will be designed and evaluated to specifically address the integration of sources relevant to investigations of genotype-phenotype associations and to the identification of genes responsible for human diseases and conditions. PUBLIC HEALTH RELEVANCE The GeneTegra System is an information integration solution that provides a common interaction environment to query data and knowledge from multiple heterogeneous sources. It uses ontologies as the base formulism for semantic and syntactic modeling, and contains automated mechanisms for the generation of these ontologies, and for the reuse and sharing of integration configurations. It is specifically designed to address the integrated querying of sources relevant to investigations of genotype-phenotype associations and to the identification of genes responsible for human diseases and conditions.          n/a",Information Integration of Heterogeneous Data Sources,7614360,R44RR018667,"['Access to Information', 'Address', 'Algorithms', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Characteristics', 'Code', 'Communities', 'Computer software', 'Data', 'Data Quality', 'Data Sources', 'Databases', 'Development', 'Disease', 'Ensure', 'Environment', 'Evaluation', 'Extensible Markup Language', 'Generations', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Heterogeneity', 'Human', 'Human Genetics', 'Information Systems', 'Institution', 'Internet', 'Investigation', 'Knowledge', 'Language', 'Maintenance', 'Maps', 'Mediator of activation protein', 'Mental Health', 'Mental disorders', 'Methods', 'Modeling', 'Ontology', 'Performance', 'Phase', 'Phenotype', 'Predisposition', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Methodology', 'Research Personnel', 'Security', 'Semantics', 'Services', 'Solutions', 'Source', 'Staff Development', 'Strigiformes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'abstracting', 'base', 'computer based Semantic Analysis', 'computerized', 'design', 'human centered computing', 'human disease', 'information gathering', 'information organization', 'lexical', 'programs', 'public health relevance', 'syntax', 'system architecture', 'usability']",NCRR,"INFOTECH SOFT, INC.",R44,2009,505764,0.1380661787046334
"Information Integration of Heterogeneous Data Sources    DESCRIPTION (provided by applicant): The wealth of biological and biomedical data constantly being generated promises dramatic advancement in the life sciences. To realize this promise, this pool of rapidly expanding information needs to be efficiently integrated, that is, combined in such a way that it can be queried to extract relevant data that can be subsequently analyzed to answer meaningful research questions. The main objective of this proposal is to develop the GeneTegra System, an information integration solution that provides a common interaction environment to query data and knowledge from multiple sources. Two main obstacles have to be overcome in order to attain an effective integration of knowledge from different data sources: syntactic heterogeneity, where data sources have different representation and access mechanisms; and semantic variability, where similar lexical terms may refer to multiple concepts or dissimilar terms refer to the same concept. The GeneTegra System addresses these obstacles through the use of Semantic Web technologies: ontologies constructed using the Web Ontology Language (OWL) as a common data and knowledge representation for data sources of diverse formats, automated mechanisms for the generation and maintenance of these ontology representations, and a robust system architecture based on reusable, service-oriented mediators. The core of the proposed system consists of general algorithms, procedures, and mechanisms developed during Phase I of this project, that enable the automatic generation of ontologies, the automated identification of semantic correspondences between ontology models, and the creation and execution of queries over these ontology- modeled, distributed, heterogeneous sources. In Phase II, the GeneTegra System will be developed, implemented, and tested as a human-centered solution building on the core components developed during Phase I, incorporating a highly usable interface for query creation and execution, a mechanism for registration, sharing, and re-use of information using Web Services standards, a mechanism for determining quality of data and query reliability, and a security and privacy subsystem that allows the construction of collaborative communities while ensuring that users are properly authenticated and authorized to access information through the system. The GeneTegra System will be designed and evaluated to specifically address the integration of sources relevant to investigations of genotype-phenotype associations and to the identification of genes responsible for human diseases and conditions. PUBLIC HEALTH RELEVANCE The GeneTegra System is an information integration solution that provides a common interaction environment to query data and knowledge from multiple heterogeneous sources. It uses ontologies as the base formulism for semantic and syntactic modeling, and contains automated mechanisms for the generation of these ontologies, and for the reuse and sharing of integration configurations. It is specifically designed to address the integrated querying of sources relevant to investigations of genotype-phenotype associations and to the identification of genes responsible for human diseases and conditions.          n/a",Information Integration of Heterogeneous Data Sources,7481818,R44RR018667,"['Access to Information', 'Address', 'Algorithms', 'Architecture', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Characteristics', 'Code', 'Communities', 'Computer software', 'Condition', 'Data', 'Data Quality', 'Data Sources', 'Databases', 'Development', 'Disease', 'Ensure', 'Environment', 'Evaluation', 'Extensible Markup Language', 'Facility Construction Funding Category', 'Generations', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Heterogeneity', 'Human', 'Human Genetics', 'Information Systems', 'Institution', 'Internet', 'Investigation', 'Knowledge', 'Language', 'Maintenance', 'Maps', 'Mediator of activation protein', 'Mental Health', 'Mental disorders', 'Methods', 'Modeling', 'Ontology', 'Performance', 'Phase', 'Phenotype', 'Predisposition', 'Privacy', 'Procedures', 'Process', 'Public Health', 'Purpose', 'Representations, Knowledge (Computer)', 'Research', 'Research Methodology', 'Research Personnel', 'Security', 'Semantics', 'Services', 'Solutions', 'Source', 'Staff Development', 'Standards of Weights and Measures', 'Strigiformes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'abstracting', 'base', 'computer based Semantic Analysis', 'computerized', 'concept', 'design', 'human centered computing', 'human disease', 'information gathering', 'information organization', 'lexical', 'programs', 'syntax', 'usability']",NCRR,"INFOTECH SOFT, INC.",R44,2008,473392,0.1380661787046334
"Integration and analysis tools for protein interaction networks    DESCRIPTION (provided by applicant): The growing size and diversity of biological databases has necessitated the design of new scalable tools that can search across multiple databases and integrate information from multiple data sources. We propose to develop software for integrating and understanding protein-protein interactions, a fundamental problem in biology. A set of tools will be developed for constructing large-scale probabilistic networks of protein interactions using data sources such as microarrays, bioimages, GO annotations, genomic data, literature, and experimental data. The techniques will be based on Bayesian networks (BN) and Support Vector Machines (SVM), and will be made scalable to large datasets. The second goal is to develop tools for analyzing interaction networks for pathway discovery, motif finding, and function identification. These tools will be based on current research in the areas of graph algorithms, bioinformatics, machine learning, and databases. We will target two model organisms: S. cerevisiae (yeast) and C. elegans (worm). The quality of the constructed networks will be evaluated with known protein interactions for these species. Scalability tests will be performed with the worm interactome that is about ten times larger than the yeast interactome. The developed tools will be compatible with current standards and integrated into a database backend. The resulting software will enable assimilation of heterogeneous biological data with the ultimate goal of increased understanding of fundamental processes in molecular biology. The goal of this Phase I project is to prove the feasibility of constructing and analyzing probabilistic protein interaction networks in a scalable manner using new algorithms. The integration of diverse data sources such as microarrays, genomics, literature, and high-throughput experiments into pathways will facilitate the study the biological processes behind human diseases. The understanding of protein interactions within a pathway and interactions between pathways will lead to the selection of appropriate targets for therapeutic intervention, and eventually to cheaper and faster drug discovery.             n/a",Integration and analysis tools for protein interaction networks,7155257,R43RR022659,"['information systems', 'model', 'protein protein interaction', 'proteins']",NCRR,"ACELOT, INC.",R43,2006,199950,0.05817556236818964
"Using CRISPR technology to study the function of paralogous genes Gene annotations in model organisms such as Drosophila are important contributors to our understanding of the functions of human genes, including human disease-associated genes. Paralogs, which share a common ancestor, present a challenging case for identification of gene function in model organisms as in some cases, loss of function of one paralog is masked by compensatory function of the other(s), such that only when both are disrupted will informative phenotypes be observed. In other cases, redundancy is partial, such that knockout of one paralog has a subset of the phenotypes observed when more than one member of the group is disrupted simultaneously. Recent advances in CRISPR technology by our group and others makes it now possible to systematically knockout paralog pairs in Drosophila, including in a stage- and tissue-specific manner. We will use our existing infrastructure for bioinformatics-based identification of orthologs and paralogs, efficient large-scale production of fly stocks, and fly stock and data sharing to develop a resource useful for double-knockdown of paralogs. Our initial characterization of the genes with regards to signal transduction and neurodegeneration, as well as in-depth analyses by the community, will uncover function for paralogous genes, helping to close the ‘phenotype gap’ (lack of associated loss-of-function phenotypes) that currently exists for nearly half of all genes in this important genetic model system. The result will be a fly stock resource for further study by Drosophila experts, a bioinformatics pipeline and methods that can be applied to other model systems, and a data resource that will inform annotation of fly and human genes Genetic analysis is a powerful tool for uncovering conserved gene functions but paralogs can have full or partial overlap in function, preventing discovery in single-gene studies. The issue will be solved using state-of- the-art CRISPR technology to generate a resource that will allow gene function to be uncovered through simultaneous disruption of paralogs.",Using CRISPR technology to study the function of paralogous genes,9966053,R24OD026435,"['Address', 'Animal Model', 'Area', 'Biological Assay', 'Biological Models', 'Biology', 'CRISPR/Cas technology', 'Clone Cells', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Complex', 'Controlled Study', 'Databases', 'Development', 'Disease', 'Drosophila genome', 'Drosophila genus', 'Fluorescence', 'Foundations', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Models', 'Genome', 'Human', 'Human Genome', 'Infrastructure', 'Knock-out', 'Knowledge', 'Maps', 'Masks', 'Methods', 'Modeling', 'Mosaicism', 'Nerve Degeneration', 'Orthologous Gene', 'Phenotype', 'Production', 'Proteins', 'Reiterated Genes', 'Research', 'Resources', 'Role', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'System', 'Technology', 'Testing', 'Tissues', 'Transgenic Organisms', 'base', 'bioinformatics infrastructure', 'bioinformatics pipeline', 'cell type', 'data mining', 'data resource', 'data sharing', 'fly', 'gene function', 'genetic analysis', 'genetic approach', 'human disease', 'insight', 'knock-down', 'knockout gene', 'large scale production', 'loss of function', 'member', 'mutant', 'neuronal cell body', 'paralogous gene', 'pleiotropism', 'prevent', 'text searching', 'tool', 'vector']",OD,HARVARD MEDICAL SCHOOL,R24,2020,629871,0.08161832233293151
"Using CRISPR technology to study the function of paralogous genes PROJECT SUMMARY / ABSTRACT Gene annotations in model organisms such as Drosophila are important contributors to our understanding of the functions of human genes, including human disease-associated genes. Paralogs, which share a common ancestor, present a challenging case for identification of gene function in model organisms as in some cases, loss of function of one paralog is masked by compensatory function of the other(s), such that only when both are disrupted will informative phenotypes be observed. In other cases, redundancy is partial, such that knockout of one paralog has a subset of the phenotypes observed when more than one member of the group is disrupted simultaneously. Recent advances in CRISPR technology by our group and others makes it now possible to systematically knockout paralog pairs in Drosophila, including in a stage- and tissue-specific manner. We will use our existing infrastructure for bioinformatics-based identification of orthologs and paralogs, efficient large-scale production of fly stocks, and fly stock and data sharing to develop a resource useful for double-knockdown of paralogs. Our initial characterization of the genes with regards to signal transduction and neurodegeneration, as well as in-depth analyses by the community, will uncover function for paralogous genes, helping to close the ‘phenotype gap’ (lack of associated loss-of-function phenotypes) that currently exists for nearly half of all genes in this important genetic model system. The result will be a fly stock resource for further study by Drosophila experts, a bioinformatics pipeline and methods that can be applied to other model systems, and a data resource that will inform annotation of fly and human genes PROJECT NARRATIVE Genetic analysis is a powerful tool for uncovering conserved gene functions but paralogs can have full or partial overlap in function, preventing discovery in single-gene studies. The issue will be solved using state-of- the-art CRISPR technology to generate a resource that will allow gene function to be uncovered through simultaneous disruption of paralogs.",Using CRISPR technology to study the function of paralogous genes,9962582,R24OD026435,"['Address', 'Animal Model', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Models', 'Biology', 'CRISPR/Cas technology', 'Clone Cells', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Complex', 'Controlled Study', 'Databases', 'Development', 'Disease', 'Drosophila genome', 'Drosophila genus', 'Fluorescence', 'Foundations', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Models', 'Genome', 'Human', 'Human Genome', 'Infrastructure', 'Knock-out', 'Knowledge', 'Maps', 'Masks', 'Methods', 'Modeling', 'Mosaicism', 'Nerve Degeneration', 'Orthologous Gene', 'Phenotype', 'Production', 'Proteins', 'Reiterated Genes', 'Research', 'Resources', 'Role', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'System', 'Technology', 'Testing', 'Tissues', 'Transgenic Organisms', 'base', 'cell type', 'data mining', 'data resource', 'data sharing', 'fly', 'gene function', 'genetic analysis', 'genetic approach', 'human disease', 'insight', 'knock-down', 'knockout gene', 'large scale production', 'loss of function', 'member', 'mutant', 'neuronal cell body', 'paralogous gene', 'pleiotropism', 'prevent', 'text searching', 'tool', 'vector']",OD,HARVARD MEDICAL SCHOOL,R24,2019,9757,0.08161832233293151
"Using CRISPR technology to study the function of paralogous genes Gene annotations in model organisms such as Drosophila are important contributors to our understanding of the functions of human genes, including human disease-associated genes. Paralogs, which share a common ancestor, present a challenging case for identification of gene function in model organisms as in some cases, loss of function of one paralog is masked by compensatory function of the other(s), such that only when both are disrupted will informative phenotypes be observed. In other cases, redundancy is partial, such that knockout of one paralog has a subset of the phenotypes observed when more than one member of the group is disrupted simultaneously. Recent advances in CRISPR technology by our group and others makes it now possible to systematically knockout paralog pairs in Drosophila, including in a stage- and tissue-specific manner. We will use our existing infrastructure for bioinformatics-based identification of orthologs and paralogs, efficient large-scale production of fly stocks, and fly stock and data sharing to develop a resource useful for double-knockdown of paralogs. Our initial characterization of the genes with regards to signal transduction and neurodegeneration, as well as in-depth analyses by the community, will uncover function for paralogous genes, helping to close the ‘phenotype gap’ (lack of associated loss-of-function phenotypes) that currently exists for nearly half of all genes in this important genetic model system. The result will be a fly stock resource for further study by Drosophila experts, a bioinformatics pipeline and methods that can be applied to other model systems, and a data resource that will inform annotation of fly and human genes Genetic analysis is a powerful tool for uncovering conserved gene functions but paralogs can have full or partial overlap in function, preventing discovery in single-gene studies. The issue will be solved using state-of- the-art CRISPR technology to generate a resource that will allow gene function to be uncovered through simultaneous disruption of paralogs.",Using CRISPR technology to study the function of paralogous genes,9735479,R24OD026435,"['Address', 'Animal Model', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Models', 'Biology', 'CRISPR/Cas technology', 'Clone Cells', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Complex', 'Controlled Study', 'Databases', 'Development', 'Disease', 'Drosophila genome', 'Drosophila genus', 'Fluorescence', 'Foundations', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Models', 'Genome', 'Human', 'Human Genome', 'Infrastructure', 'Knock-out', 'Knowledge', 'Maps', 'Masks', 'Methods', 'Modeling', 'Mosaicism', 'Nerve Degeneration', 'Orthologous Gene', 'Phenotype', 'Production', 'Proteins', 'Reiterated Genes', 'Research', 'Resources', 'Role', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'System', 'Technology', 'Testing', 'Tissues', 'Transgenic Organisms', 'base', 'cell type', 'data mining', 'data resource', 'data sharing', 'fly', 'gene function', 'genetic analysis', 'genetic approach', 'human disease', 'insight', 'knock-down', 'knockout gene', 'large scale production', 'loss of function', 'member', 'mutant', 'neuronal cell body', 'paralogous gene', 'pleiotropism', 'prevent', 'text searching', 'tool', 'vector']",OD,HARVARD MEDICAL SCHOOL,R24,2019,629871,0.08161832233293151
"Using CRISPR technology to study the function of paralogous genes Gene annotations in model organisms such as Drosophila are important contributors to our understanding of the functions of human genes, including human disease-associated genes. Paralogs, which share a common ancestor, present a challenging case for identification of gene function in model organisms as in some cases, loss of function of one paralog is masked by compensatory function of the other(s), such that only when both are disrupted will informative phenotypes be observed. In other cases, redundancy is partial, such that knockout of one paralog has a subset of the phenotypes observed when more than one member of the group is disrupted simultaneously. Recent advances in CRISPR technology by our group and others makes it now possible to systematically knockout paralog pairs in Drosophila, including in a stage- and tissue-specific manner. We will use our existing infrastructure for bioinformatics-based identification of orthologs and paralogs, efficient large-scale production of fly stocks, and fly stock and data sharing to develop a resource useful for double-knockdown of paralogs. Our initial characterization of the genes with regards to signal transduction and neurodegeneration, as well as in-depth analyses by the community, will uncover function for paralogous genes, helping to close the ‘phenotype gap’ (lack of associated loss-of-function phenotypes) that currently exists for nearly half of all genes in this important genetic model system. The result will be a fly stock resource for further study by Drosophila experts, a bioinformatics pipeline and methods that can be applied to other model systems, and a data resource that will inform annotation of fly and human genes Genetic analysis is a powerful tool for uncovering conserved gene functions but paralogs can have full or partial overlap in function, preventing discovery in single-gene studies. The issue will be solved using state-of- the-art CRISPR technology to generate a resource that will allow gene function to be uncovered through simultaneous disruption of paralogs.",Using CRISPR technology to study the function of paralogous genes,9568126,R24OD026435,"['Address', 'Animal Model', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Models', 'Biology', 'CRISPR/Cas technology', 'Clone Cells', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collection', 'Communities', 'Complex', 'Controlled Study', 'Databases', 'Development', 'Disease', 'Drosophila genome', 'Drosophila genus', 'Fluorescence', 'Foundations', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic Models', 'Genome', 'Human', 'Human Genome', 'Knock-out', 'Knowledge', 'Maps', 'Masks', 'Methods', 'Modeling', 'Mosaicism', 'Nerve Degeneration', 'Orthologous Gene', 'Phenotype', 'Production', 'Proteins', 'Reiterated Genes', 'Research', 'Research Infrastructure', 'Resources', 'Role', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'System', 'Technology', 'Testing', 'Tissues', 'Transgenic Organisms', 'base', 'cell type', 'data mining', 'data resource', 'data sharing', 'fly', 'gene function', 'genetic analysis', 'genetic approach', 'human disease', 'insight', 'knock-down', 'knockout gene', 'large scale production', 'loss of function', 'member', 'mutant', 'neuronal cell body', 'paralogous gene', 'pleiotropism', 'prevent', 'text searching', 'tool', 'vector']",OD,HARVARD MEDICAL SCHOOL,R24,2018,647246,0.08161832233293151
"Capturing Health Information from Research Ontologies (CHIRON) Project Summary/Abstract The volume of published evidence in biomedicine is growing at a rapid pace. Doctors and researchers must keep pace with the rapid generation of new evidence to provide up-to-date care. The review and aggregation of knowledge is a time-intensive process that must be repeated to assimilate the latest evidence. Furthermore, published evaluations of evidence (e.g., literature reviews, meta-analyses, systematic reviews) are not in a format that is able to leverage modern data analytics. Charles River Analytics proposes to design and demonstrate a toolkit for Capturing Health Information from Research Ontologies (CHIRON). CHIRON uses human-in-the-loop approaches to build models of research designs and attributes based on an ontological framework. It leverages a Systemic Functional Grammar (SFG) toolkit for rapidly capturing model data and intuitive user interfaces ensuring accuracy. The format of the research models facilitates their sharing and reuse. This sharable format enables doctors and researchers to rapidly aggregate models and assimilate new representations of evidence into an existing body of research using semantic queries. Furthermore, they facilitate qualitative analyses to obtain summaries and insights about the aggregated literature. The proposed workflow, encompassing capture, analysis, and visualization of research models will decrease the amount of time required to find and aggregate relevant research. Project Narrative  Doctors and researchers require novel tools for finding and evaluating medical evidence to keep pace with the rapidly growing volume of research and provide up-to date care. A principal barrier to the timely review of medical literature is the time it takes to locate relevant studies, examine the research design, and then aggregate the data across a body of literature. To increase the efficiency of the review process, the research and medical community would benefit from tools that allow reviewers to capture models of published research that can be aggregated and combined across a research domain, perform qualitative analytics on the aggregated research, and visualize the results to obtain insights that guide the evaluation of evidence.",Capturing Health Information from Research Ontologies (CHIRON),9885889,R43DA050154,"['Algorithms', 'Attention', 'Automation', 'Biomedical Research', 'Books', 'Caring', 'Communities', 'Computer software', 'Data', 'Data Aggregation', 'Data Analytics', 'Engineering', 'Ensure', 'Evaluation', 'Evidence based practice', 'Feedback', 'Generations', 'Government', 'Graph', 'Guidelines', 'Health', 'Human', 'Imagery', 'Insurance Carriers', 'Intuition', 'Knowledge', 'Literature', 'Medical', 'Medical Research', 'Meta-Analysis', 'Methods', 'Modeling', 'Modernization', 'Ontology', 'Outcome', 'Periodicity', 'Persons', 'Pharmacologic Substance', 'Phase', 'Population', 'Process', 'Publications', 'Publishing', 'Readability', 'Reader', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Review Literature', 'Rivers', 'Sample Size', 'Semantics', 'Social Sciences', 'Structure', 'System', 'Time', 'Update', 'Visualization software', 'automated analysis', 'base', 'cognitive system', 'community based practice', 'data modeling', 'design', 'empowered', 'evidence base', 'heuristics', 'human-in-the-loop', 'informatics\xa0tool', 'information organization', 'insight', 'knowledge base', 'novel', 'social science research', 'sociolinguistics', 'sound', 'systematic review', 'theories', 'tool', 'user-friendly']",NIDA,"CHARLES RIVER ANALYTICS, INC.",R43,2019,148719,0.06932940071922755
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6923756,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,395905,0.04282583351691964
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6777028,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,341671,0.04282583351691964
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6936159,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,52940,0.04282583351691964
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6685421,R01GM061372,"['Internet', ' artificial intelligence', ' automated data processing', ' biological signal transduction', ' biomedical automation', ' computer system design /evaluation', ' functional /structural genomics', ' high throughput technology', ' intermolecular interaction', ' method development', ' molecular biology information system', ' statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,323936,0.04282583351691964
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,7109270,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2006,301738,0.037110519976533606
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,6942558,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2005,300000,0.037110519976533606
"Development of dictyBase, an online informatics resource    DESCRIPTION (provided by applicant): One of the major challenges facing biomedical researchers is how to effectively utilize the growing tsunami of genome-wide data that is becoming available as DNA sequencing costs plunge and volumes of available data expand. One solution to this challenge is the development of robust databases that organize, integrate, curate and validate this data, while providing tools to search, visualize and explore this extremely valuable data. dictyBase is the model organism database that uses the genome sequence of Dictyostelium discoiedum to organize biological knowledge resulting from studies using Dictyostelium and related species. Investigators using Dictyostelium in bench research comprise a vibrant community of over 1500 researchers. Dictyostelium's position on the tree of life provides a unique evolutionary perspective that provides great value for evolutionary and computational biologists wishing to employ comparative genomics approaches. Dictyostelium has contributed to improving biological understanding of a variety of fundamental processing including cell migration, phagocytosis, cell-cell and intracellular signaling, cellular differentiation, self-nonself recognition and multicellular morphogenesis. Dictyostelium research has important medical relevance, having contributed, for example, to our understanding of mitochondrial-related diseases, host-pathogen interactions, especially for intracellular pathogens, and the mode of action of mood stabilizing drugs and defining the pathways targeted by bisphosphonates. dictyBase has become the trusted resource for investigators seeking Dictyostelium genome information, annotations, and functional data, having been accessed over 6 million times by over 280,000 independent IP addresses. This application seeks continued funding for dictyBase to allow completion of the annotation of all gene models and integration of important new data types. dictyBase seeks to provide innovative tools, strategies and high quality annotations that enable bench researchers to effectively benefit from rapidly increasing collection of available data. Specific aims for the next funding period are to (1) Annotate the D. discoideum genome and curate experimental results from the literature, (2) Integrate and display novel large data sets including genome sequences for strains and related species, RNAseq based expression data, proteomics data and protein-protein interactions; (3) Maximize the utility of dictyBase to the biomedical research community. Successful completion of these aims will provide a critical resource for biomedical research and will maximize the investment of the NIH in research using Dictyostelium.      PUBLIC HEALTH RELEVANCE: dictyBase enables efficient biomedical research by organizing, integrating, and validating genome-wide data such as genome sequences, proteomics data, RNAseq transciptomics data and functional data captured by automated and manual literature curation and making this data available in a readily searchable format. The work proposed in this application is focused on increasing the variety and extent of datasets that dictyBase integrates, improving interfaces and searchability of the data and making that data widely available to the biomedical research community.           Project Narrative Relevance dictyBase enables efficient biomedical research by organizing, integrating, and validating genome-wide data such as genome sequences, proteomics data, RNAseq transciptomics data and functional data captured by automated and manual literature curation and making this data available in a readily searchable format. The work proposed in this application is focused on increasing the variety and extent of datasets that dictyBase integrates, improving interfaces and searchability of the data and making that data widely available to the biomedical research community.","Development of dictyBase, an online informatics resource",8260577,R01GM064426,"['Address', 'Biochemical', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cells', 'Collection', 'Communities', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Funding', 'Genbank', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Investments', 'Knowledge', 'Life', 'Link', 'Literature', 'Manuals', 'Medical', 'Mission', 'Mitochondria', 'Modeling', 'Modification', 'Moods', 'Morphogenesis', 'Mutagenesis', 'Names', 'Occupations', 'Ontology', 'Pathway interactions', 'Phagocytosis', 'Pharmaceutical Preparations', 'Phenotype', 'Positioning Attribute', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteomics', 'Publications', 'Publishing', 'Reporting', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Signal Transduction', 'Solutions', 'Time', 'Training Support', 'Trees', 'Trust', 'Tsunami', 'United States National Institutes of Health', 'Update', 'Work', 'base', 'bisphosphonate', 'cell motility', 'comparative genomics', 'cost', 'genome sequencing', 'genome-wide', 'improved', 'innovation', 'knowledge of results', 'model organisms databases', 'network models', 'novel', 'pathogen', 'protein protein interaction', 'public health relevance', 'research study', 'scaffold', 'software development', 'text searching', 'tool']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2012,492286,0.07011221001183475
"Development of dictyBase, an online informatics resource    DESCRIPTION (provided by applicant): One of the major challenges facing biomedical researchers is how to effectively utilize the growing tsunami of genome-wide data that is becoming available as DNA sequencing costs plunge and volumes of available data expand. One solution to this challenge is the development of robust databases that organize, integrate, curate and validate this data, while providing tools to search, visualize and explore this extremely valuable data. dictyBase is the model organism database that uses the genome sequence of Dictyostelium discoiedum to organize biological knowledge resulting from studies using Dictyostelium and related species. Investigators using Dictyostelium in bench research comprise a vibrant community of over 1500 researchers. Dictyostelium's position on the tree of life provides a unique evolutionary perspective that provides great value for evolutionary and computational biologists wishing to employ comparative genomics approaches. Dictyostelium has contributed to improving biological understanding of a variety of fundamental processing including cell migration, phagocytosis, cell-cell and intracellular signaling, cellular differentiation, self-nonself recognition and multicellular morphogenesis. Dictyostelium research has important medical relevance, having contributed, for example, to our understanding of mitochondrial-related diseases, host-pathogen interactions, especially for intracellular pathogens, and the mode of action of mood stabilizing drugs and defining the pathways targeted by bisphosphonates. dictyBase has become the trusted resource for investigators seeking Dictyostelium genome information, annotations, and functional data, having been accessed over 6 million times by over 280,000 independent IP addresses. This application seeks continued funding for dictyBase to allow completion of the annotation of all gene models and integration of important new data types. dictyBase seeks to provide innovative tools, strategies and high quality annotations that enable bench researchers to effectively benefit from rapidly increasing collection of available data. Specific aims for the next funding period are to (1) Annotate the D. discoideum genome and curate experimental results from the literature, (2) Integrate and display novel large data sets including genome sequences for strains and related species, RNAseq based expression data, proteomics data and protein-protein interactions; (3) Maximize the utility of dictyBase to the biomedical research community. Successful completion of these aims will provide a critical resource for biomedical research and will maximize the investment of the NIH in research using Dictyostelium.      PUBLIC HEALTH RELEVANCE: dictyBase enables efficient biomedical research by organizing, integrating, and validating genome-wide data such as genome sequences, proteomics data, RNAseq transciptomics data and functional data captured by automated and manual literature curation and making this data available in a readily searchable format. The work proposed in this application is focused on increasing the variety and extent of datasets that dictyBase integrates, improving interfaces and searchability of the data and making that data widely available to the biomedical research community.           dictyBase enables efficient biomedical research by organizing, integrating, and validating genome-wide data such as genome sequences, proteomics data, RNAseq transciptomics data and functional data captured by automated and manual literature curation and making this data available in a readily searchable format. The work proposed in this application is focused on increasing the variety and extent of datasets that dictyBase integrates, improving interfaces and searchability of the data and making that data widely available to the biomedical research community.         ","Development of dictyBase, an online informatics resource",8113573,R01GM064426,"['Address', 'Biochemical', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cells', 'Collection', 'Communities', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Funding', 'Genbank', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Investments', 'Knowledge', 'Life', 'Link', 'Literature', 'Manuals', 'Medical', 'Mission', 'Mitochondria', 'Modeling', 'Modification', 'Moods', 'Morphogenesis', 'Mutagenesis', 'Names', 'Occupations', 'Ontology', 'Pathway interactions', 'Phagocytosis', 'Pharmaceutical Preparations', 'Phenotype', 'Positioning Attribute', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteomics', 'Publications', 'Publishing', 'Reporting', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Signal Transduction', 'Solutions', 'Time', 'Training Support', 'Trees', 'Trust', 'Tsunami', 'United States National Institutes of Health', 'Update', 'Work', 'base', 'bisphosphonate', 'cell motility', 'comparative genomics', 'cost', 'genome sequencing', 'genome-wide', 'improved', 'innovation', 'knowledge of results', 'model organisms databases', 'network models', 'novel', 'pathogen', 'protein protein interaction', 'research study', 'scaffold', 'software development', 'text searching', 'tool']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2011,506196,0.07011221001183475
"Development of dictyBase, an online informatics resource ﻿    DESCRIPTION (provided by applicant):  dictyBase is the manually curated model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. Dictyostelium is widely used to study of cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome includes genes with significant homology to vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics such as human disease, comparative genomics, and self/non-self recognition. dictyBase enables researchers to view and download up-to-date genomic, functional and technical information. Teachers appreciate the wealth of available teaching materials and technological protocols. Many users consider dictyBase their most important community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is never far away. dictyBase users benefit from its tight integration with the Dicty Stock Center. Advances in the biomedical field such as next generation sequencing and proteomic studies demand modern approaches to house and represent data. This application seeks support to continue operating and expanding this important community resource. The goals for this proposal are (1) Annotate Dictyosteliid genomes and curate experimental results from the literature for D. discoideum by (a) expert curation of published data including nomenclature, functional annotations and mutant strains and their phenotypes, as well the new data types such as expression, disease genes, and signaling pathways. All this will be accomplished using a novel, integrated curation tool; (b) community curation will be a supervised form of literature curation by authorized users, as well as gene model curation for the other Dictyosteliids, while (c) automated annotations will use D. discoideum functional and gene model annotations and automatically transfer these to orthologs in the other Dictyosteliids using the MAKER tool; (2) Improve dictyBase utility and usability by (a) implementing new tools for improving data access such as a new, versatile genome browser (JBrowse), DictyAccess, a dashboard tool representing all data comprehensively and visually, a modern guided search tool also known as `faceted search', and dictyMine, an advanced search tool; and (b) modernizing the web interface by using HTML5 and CSS3 technologies and by reorganizing the contents and increasing the dynamic nature of our web presence; we will also continue to (c) support dictyBase users via personalized responses and with online tutorials and videos for our new tools; (3) Integrate, analyze and display novel data sets such as (a) protein- protein interactions, (b) post-translational modifications (c) signaling pathways, and (d) genomic variations. Implementing these improvements will deepen the annotation of D. discoideum and spread first-rate annotations to other Dictyosteliids while modernizing the infrastructure and representing the data we hold in an intuitive and state of the art web interface.         PUBLIC HEALTH RELEVANCE: dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.            ","Development of dictyBase, an online informatics resource",8886205,R01GM064426,"['Address', 'Automated Annotation', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Collection', 'Communities', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Eukaryota', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Housing', 'Internet', 'Investments', 'Knowledge', 'Literature', 'Manuals', 'Mission', 'Modeling', 'Mutagenesis', 'Nature', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphorylation', 'Post-Translational Protein Processing', 'Process', 'Proteomics', 'Protocols documentation', 'Publications', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Resources', 'Science', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Signaling Pathway Gene', 'System', 'Teaching Materials', 'Technology', 'Tertiary Protein Structure', 'Trust', 'United States National Institutes of Health', 'Update', 'Variant', 'Yeasts', 'base', 'cell motility', 'comparative genomics', 'functional genomics', 'genome sequencing', 'genomic variation', 'human disease', 'improved', 'innovation', 'interest', 'microbial', 'model organisms databases', 'mutant', 'next generation sequencing', 'novel', 'online tutorial', 'pathogen', 'protein expression', 'protein protein interaction', 'public health relevance', 'research study', 'response', 'teacher', 'text searching', 'tool', 'transcriptome sequencing', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2015,501125,0.11173388271925606
"Development of dictyBase, an online informatics resource    DESCRIPTION (provided by applicant): One of the major challenges facing biomedical researchers is how to effectively utilize the growing tsunami of genome-wide data that is becoming available as DNA sequencing costs plunge and volumes of available data expand. One solution to this challenge is the development of robust databases that organize, integrate, curate and validate this data, while providing tools to search, visualize and explore this extremely valuable data. dictyBase is the model organism database that uses the genome sequence of Dictyostelium discoiedum to organize biological knowledge resulting from studies using Dictyostelium and related species. Investigators using Dictyostelium in bench research comprise a vibrant community of over 1500 researchers. Dictyostelium's position on the tree of life provides a unique evolutionary perspective that provides great value for evolutionary and computational biologists wishing to employ comparative genomics approaches. Dictyostelium has contributed to improving biological understanding of a variety of fundamental processing including cell migration, phagocytosis, cell-cell and intracellular signaling, cellular differentiation, self-nonself recognition and multicellular morphogenesis. Dictyostelium research has important medical relevance, having contributed, for example, to our understanding of mitochondrial-related diseases, host-pathogen interactions, especially for intracellular pathogens, and the mode of action of mood stabilizing drugs and defining the pathways targeted by bisphosphonates. dictyBase has become the trusted resource for investigators seeking Dictyostelium genome information, annotations, and functional data, having been accessed over 6 million times by over 280,000 independent IP addresses. This application seeks continued funding for dictyBase to allow completion of the annotation of all gene models and integration of important new data types. dictyBase seeks to provide innovative tools, strategies and high quality annotations that enable bench researchers to effectively benefit from rapidly increasing collection of available data. Specific aims for the next funding period are to (1) Annotate the D. discoideum genome and curate experimental results from the literature, (2) Integrate and display novel large data sets including genome sequences for strains and related species, RNAseq based expression data, proteomics data and protein-protein interactions; (3) Maximize the utility of dictyBase to the biomedical research community. Successful completion of these aims will provide a critical resource for biomedical research and will maximize the investment of the NIH in research using Dictyostelium.       PUBLIC HEALTH RELEVANCE: dictyBase enables efficient biomedical research by organizing, integrating, and validating genome-wide data such as genome sequences, proteomics data, RNAseq transciptomics data and functional data captured by automated and manual literature curation and making this data available in a readily searchable format. The work proposed in this application is focused on increasing the variety and extent of datasets that dictyBase integrates, improving interfaces and searchability of the data and making that data widely available to the biomedical research community.         ","Development of dictyBase, an online informatics resource",8645636,R01GM064426,"['Address', 'Biochemical', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cells', 'Collection', 'Communities', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Funding', 'Genbank', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Genomic DNA', 'Genomics', 'Investments', 'Knowledge', 'Life', 'Link', 'Literature', 'Manuals', 'Medical', 'Mission', 'Mitochondria', 'Modeling', 'Modification', 'Moods', 'Morphogenesis', 'Mutagenesis', 'Names', 'Occupations', 'Ontology', 'Pathway interactions', 'Phagocytosis', 'Pharmaceutical Preparations', 'Phenotype', 'Positioning Attribute', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteomics', 'Publications', 'Publishing', 'Reporting', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Signal Transduction', 'Solutions', 'Time', 'Training Support', 'Trees', 'Trust', 'Tsunami', 'United States National Institutes of Health', 'Update', 'Work', 'base', 'bisphosphonate', 'cell motility', 'comparative genomics', 'cost', 'genome annotation', 'genome sequencing', 'genome-wide', 'improved', 'innovation', 'knowledge of results', 'model organisms databases', 'network models', 'novel', 'pathogen', 'protein protein interaction', 'public health relevance', 'research study', 'scaffold', 'software development', 'text searching', 'tool']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2014,481765,0.07587151034110444
"Development of dictyBase, an online informatics resource    DESCRIPTION (provided by applicant): One of the major challenges facing biomedical researchers is how to effectively utilize the growing tsunami of genome-wide data that is becoming available as DNA sequencing costs plunge and volumes of available data expand. One solution to this challenge is the development of robust databases that organize, integrate, curate and validate this data, while providing tools to search, visualize and explore this extremely valuable data. dictyBase is the model organism database that uses the genome sequence of Dictyostelium discoiedum to organize biological knowledge resulting from studies using Dictyostelium and related species. Investigators using Dictyostelium in bench research comprise a vibrant community of over 1500 researchers. Dictyostelium's position on the tree of life provides a unique evolutionary perspective that provides great value for evolutionary and computational biologists wishing to employ comparative genomics approaches. Dictyostelium has contributed to improving biological understanding of a variety of fundamental processing including cell migration, phagocytosis, cell-cell and intracellular signaling, cellular differentiation, self-nonself recognition and multicellular morphogenesis. Dictyostelium research has important medical relevance, having contributed, for example, to our understanding of mitochondrial-related diseases, host-pathogen interactions, especially for intracellular pathogens, and the mode of action of mood stabilizing drugs and defining the pathways targeted by bisphosphonates. dictyBase has become the trusted resource for investigators seeking Dictyostelium genome information, annotations, and functional data, having been accessed over 6 million times by over 280,000 independent IP addresses. This application seeks continued funding for dictyBase to allow completion of the annotation of all gene models and integration of important new data types. dictyBase seeks to provide innovative tools, strategies and high quality annotations that enable bench researchers to effectively benefit from rapidly increasing collection of available data. Specific aims for the next funding period are to (1) Annotate the D. discoideum genome and curate experimental results from the literature, (2) Integrate and display novel large data sets including genome sequences for strains and related species, RNAseq based expression data, proteomics data and protein-protein interactions; (3) Maximize the utility of dictyBase to the biomedical research community. Successful completion of these aims will provide a critical resource for biomedical research and will maximize the investment of the NIH in research using Dictyostelium.       PUBLIC HEALTH RELEVANCE: dictyBase enables efficient biomedical research by organizing, integrating, and validating genome-wide data such as genome sequences, proteomics data, RNAseq transciptomics data and functional data captured by automated and manual literature curation and making this data available in a readily searchable format. The work proposed in this application is focused on increasing the variety and extent of datasets that dictyBase integrates, improving interfaces and searchability of the data and making that data widely available to the biomedical research community.         ","Development of dictyBase, an online informatics resource",8462629,R01GM064426,"['Address', 'Biochemical', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cells', 'Collection', 'Communities', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Funding', 'Genbank', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Investments', 'Knowledge', 'Life', 'Link', 'Literature', 'Manuals', 'Medical', 'Mission', 'Mitochondria', 'Modeling', 'Modification', 'Moods', 'Morphogenesis', 'Mutagenesis', 'Names', 'Occupations', 'Ontology', 'Pathway interactions', 'Phagocytosis', 'Pharmaceutical Preparations', 'Phenotype', 'Positioning Attribute', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteomics', 'Publications', 'Publishing', 'Reporting', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Signal Transduction', 'Solutions', 'Time', 'Training Support', 'Trees', 'Trust', 'Tsunami', 'United States National Institutes of Health', 'Update', 'Work', 'base', 'bisphosphonate', 'cell motility', 'comparative genomics', 'cost', 'genome annotation', 'genome sequencing', 'genome-wide', 'improved', 'innovation', 'knowledge of results', 'model organisms databases', 'network models', 'novel', 'pathogen', 'protein protein interaction', 'public health relevance', 'research study', 'scaffold', 'software development', 'text searching', 'tool']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2013,471757,0.07587151034110444
"Development of dictyBase, an online informatics resource ﻿    DESCRIPTION (provided by applicant):  dictyBase is the manually curated model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. Dictyostelium is widely used to study of cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome includes genes with significant homology to vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics such as human disease, comparative genomics, and self/non-self recognition. dictyBase enables researchers to view and download up-to-date genomic, functional and technical information. Teachers appreciate the wealth of available teaching materials and technological protocols. Many users consider dictyBase their most important community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is never far away. dictyBase users benefit from its tight integration with the Dicty Stock Center. Advances in the biomedical field such as next generation sequencing and proteomic studies demand modern approaches to house and represent data. This application seeks support to continue operating and expanding this important community resource. The goals for this proposal are (1) Annotate Dictyosteliid genomes and curate experimental results from the literature for D. discoideum by (a) expert curation of published data including nomenclature, functional annotations and mutant strains and their phenotypes, as well the new data types such as expression, disease genes, and signaling pathways. All this will be accomplished using a novel, integrated curation tool; (b) community curation will be a supervised form of literature curation by authorized users, as well as gene model curation for the other Dictyosteliids, while (c) automated annotations will use D. discoideum functional and gene model annotations and automatically transfer these to orthologs in the other Dictyosteliids using the MAKER tool; (2) Improve dictyBase utility and usability by (a) implementing new tools for improving data access such as a new, versatile genome browser (JBrowse), DictyAccess, a dashboard tool representing all data comprehensively and visually, a modern guided search tool also known as `faceted search', and dictyMine, an advanced search tool; and (b) modernizing the web interface by using HTML5 and CSS3 technologies and by reorganizing the contents and increasing the dynamic nature of our web presence; we will also continue to (c) support dictyBase users via personalized responses and with online tutorials and videos for our new tools; (3) Integrate, analyze and display novel data sets such as (a) protein- protein interactions, (b) post-translational modifications (c) signaling pathways, and (d) genomic variations. Implementing these improvements will deepen the annotation of D. discoideum and spread first-rate annotations to other Dictyosteliids while modernizing the infrastructure and representing the data we hold in an intuitive and state of the art web interface. PUBLIC HEALTH RELEVANCE: dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9476993,R01GM064426,"['Address', 'Automated Annotation', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Collection', 'Communities', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Eukaryota', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Internet', 'Intuition', 'Investments', 'Knowledge', 'Literature', 'Manuals', 'Mission', 'Modeling', 'Modernization', 'Mutagenesis', 'Nature', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphorylation', 'Post-Translational Protein Processing', 'Process', 'Proteomics', 'Protocols documentation', 'Publications', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Resources', 'Science', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Source', 'Supervision', 'System', 'Teaching Materials', 'Technology', 'Tertiary Protein Structure', 'Trust', 'United States National Institutes of Health', 'Update', 'Variant', 'Yeasts', 'base', 'cell motility', 'comparative genomics', 'dashboard', 'data access', 'experimental study', 'functional genomics', 'genome browser', 'genomic variation', 'human disease', 'improved', 'innovation', 'insertion/deletion mutation', 'interest', 'microbial', 'model organisms databases', 'mutant', 'next generation sequencing', 'novel', 'online tutorial', 'pathogen', 'protein expression', 'protein protein interaction', 'public health relevance', 'response', 'teacher', 'text searching', 'tool', 'transcriptome sequencing', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2018,480653,0.11173388271925606
"Development of dictyBase, an online informatics resource ﻿    DESCRIPTION (provided by applicant):  dictyBase is the manually curated model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. Dictyostelium is widely used to study of cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome includes genes with significant homology to vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics such as human disease, comparative genomics, and self/non-self recognition. dictyBase enables researchers to view and download up-to-date genomic, functional and technical information. Teachers appreciate the wealth of available teaching materials and technological protocols. Many users consider dictyBase their most important community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is never far away. dictyBase users benefit from its tight integration with the Dicty Stock Center. Advances in the biomedical field such as next generation sequencing and proteomic studies demand modern approaches to house and represent data. This application seeks support to continue operating and expanding this important community resource. The goals for this proposal are (1) Annotate Dictyosteliid genomes and curate experimental results from the literature for D. discoideum by (a) expert curation of published data including nomenclature, functional annotations and mutant strains and their phenotypes, as well the new data types such as expression, disease genes, and signaling pathways. All this will be accomplished using a novel, integrated curation tool; (b) community curation will be a supervised form of literature curation by authorized users, as well as gene model curation for the other Dictyosteliids, while (c) automated annotations will use D. discoideum functional and gene model annotations and automatically transfer these to orthologs in the other Dictyosteliids using the MAKER tool; (2) Improve dictyBase utility and usability by (a) implementing new tools for improving data access such as a new, versatile genome browser (JBrowse), DictyAccess, a dashboard tool representing all data comprehensively and visually, a modern guided search tool also known as `faceted search', and dictyMine, an advanced search tool; and (b) modernizing the web interface by using HTML5 and CSS3 technologies and by reorganizing the contents and increasing the dynamic nature of our web presence; we will also continue to (c) support dictyBase users via personalized responses and with online tutorials and videos for our new tools; (3) Integrate, analyze and display novel data sets such as (a) protein- protein interactions, (b) post-translational modifications (c) signaling pathways, and (d) genomic variations. Implementing these improvements will deepen the annotation of D. discoideum and spread first-rate annotations to other Dictyosteliids while modernizing the infrastructure and representing the data we hold in an intuitive and state of the art web interface. PUBLIC HEALTH RELEVANCE: dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9263970,R01GM064426,"['Address', 'Automated Annotation', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Collection', 'Communities', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Eukaryota', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Internet', 'Intuition', 'Investments', 'Knowledge', 'Literature', 'Manuals', 'Mission', 'Modeling', 'Modernization', 'Mutagenesis', 'Nature', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphorylation', 'Post-Translational Protein Processing', 'Process', 'Proteomics', 'Protocols documentation', 'Publications', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Resources', 'Science', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Supervision', 'System', 'Teaching Materials', 'Technology', 'Tertiary Protein Structure', 'Trust', 'United States National Institutes of Health', 'Update', 'Variant', 'Yeasts', 'base', 'cell motility', 'comparative genomics', 'dashboard', 'data access', 'experimental study', 'functional genomics', 'genome browser', 'genomic variation', 'human disease', 'improved', 'innovation', 'insertion/deletion mutation', 'interest', 'microbial', 'model organisms databases', 'mutant', 'next generation sequencing', 'novel', 'online tutorial', 'pathogen', 'protein expression', 'protein protein interaction', 'public health relevance', 'response', 'teacher', 'text searching', 'tool', 'transcriptome sequencing', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2017,480653,0.11173388271925606
"Development of dictyBase, an online informatics resource ﻿    DESCRIPTION (provided by applicant):  dictyBase is the manually curated model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. Dictyostelium is widely used to study of cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome includes genes with significant homology to vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics such as human disease, comparative genomics, and self/non-self recognition. dictyBase enables researchers to view and download up-to-date genomic, functional and technical information. Teachers appreciate the wealth of available teaching materials and technological protocols. Many users consider dictyBase their most important community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is never far away. dictyBase users benefit from its tight integration with the Dicty Stock Center. Advances in the biomedical field such as next generation sequencing and proteomic studies demand modern approaches to house and represent data. This application seeks support to continue operating and expanding this important community resource. The goals for this proposal are (1) Annotate Dictyosteliid genomes and curate experimental results from the literature for D. discoideum by (a) expert curation of published data including nomenclature, functional annotations and mutant strains and their phenotypes, as well the new data types such as expression, disease genes, and signaling pathways. All this will be accomplished using a novel, integrated curation tool; (b) community curation will be a supervised form of literature curation by authorized users, as well as gene model curation for the other Dictyosteliids, while (c) automated annotations will use D. discoideum functional and gene model annotations and automatically transfer these to orthologs in the other Dictyosteliids using the MAKER tool; (2) Improve dictyBase utility and usability by (a) implementing new tools for improving data access such as a new, versatile genome browser (JBrowse), DictyAccess, a dashboard tool representing all data comprehensively and visually, a modern guided search tool also known as `faceted search', and dictyMine, an advanced search tool; and (b) modernizing the web interface by using HTML5 and CSS3 technologies and by reorganizing the contents and increasing the dynamic nature of our web presence; we will also continue to (c) support dictyBase users via personalized responses and with online tutorials and videos for our new tools; (3) Integrate, analyze and display novel data sets such as (a) protein- protein interactions, (b) post-translational modifications (c) signaling pathways, and (d) genomic variations. Implementing these improvements will deepen the annotation of D. discoideum and spread first-rate annotations to other Dictyosteliids while modernizing the infrastructure and representing the data we hold in an intuitive and state of the art web interface. PUBLIC HEALTH RELEVANCE: dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9043101,R01GM064426,"['Address', 'Automated Annotation', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Collection', 'Communities', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Eukaryota', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Housing', 'Internet', 'Investments', 'Knowledge', 'Literature', 'Manuals', 'Mission', 'Modeling', 'Mutagenesis', 'Nature', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphorylation', 'Post-Translational Protein Processing', 'Process', 'Proteomics', 'Protocols documentation', 'Publications', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Resources', 'Science', 'Signal Pathway', 'Signal Transduction', 'Signal Transduction Pathway', 'Signaling Protein', 'System', 'TYRP1 gene', 'Teaching Materials', 'Technology', 'Tertiary Protein Structure', 'Trust', 'United States National Institutes of Health', 'Update', 'Variant', 'Yeasts', 'base', 'cell motility', 'comparative genomics', 'dashboard', 'data access', 'functional genomics', 'genome browser', 'genome sequencing', 'genomic variation', 'human disease', 'improved', 'innovation', 'interest', 'microbial', 'model organisms databases', 'mutant', 'next generation sequencing', 'novel', 'online tutorial', 'pathogen', 'protein expression', 'protein protein interaction', 'research study', 'response', 'teacher', 'text searching', 'tool', 'transcriptome sequencing', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2016,480653,0.11173388271925606
"Development of dictyBase, an online informatics resource PROJECT SUMMARY dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. A community resource, widely supported by the research community, dictyBase contains gold standard expert literature curation of genes, functional annotations using the Gene Ontology and a wide range of genomic resources. Dictyostelium is widely used to study cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome contains significant orthologs of vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics including human disease, multicellular differentiation and comparative genomics. dictyBase enables researchers to search, view and download up-to-date genomic, functional and technical information. It is also widely used by teachers/instructors due to the wealth of available teaching materials and research protocols. Dictyostelium investigators depend on dictyBase as their primary community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is available. We are in the final stages of deploying our completely new technology stack. By the end of this year dictyBase will be run entirely as a cloud-based application. This propoal seeks support to continue operating and expanding this important community resource. Our goals for this proposal are: (Aim 1) To continue (a) expert curation by dictyBase curators and enable (b) Community curation leveraging our strong relationship with the community. We will use additional sequence data to (c) update the AX4 reference genome sequence and improve the efficiency of curation by using (d) Deep learning-based linking of papers to genes prioritizing them for further analysis and curation. (Aim 2) We will improve dictyBase utility and usability by implementing (a) Bulk annotation methods for importing large-scale data sets using both (i) a web interface and (ii) a script/command line method. (b) We will add 10 additional Dictyostelid genomes using automated methods to annotate them. We will improve usability by implementing a (c) concurrent blast search with a new user interface and integrate this with the JBrowse display. (Aim 3) To expand the data and increase the richness of annotations available in dictyBase we will implement mechanisms to capture, store and display: (a) additional context to GO annotations (i) using existing GO extensions and (ii) annotating and displaying biological pathways using GO CAM models; (b) integrate and display genome wide insertion mutant information for over 20 thousand insertional mutants; and (c) develop a graphical display of spatial expression data using Dictyostelium anatomy ontology terms (i) by adding a track in JBrowse for genes annotated with spatial / anatomy expression terms, and (ii) creating a graphical display of these annotations via our Circos-based dashboard tool. As other data sets become available we will add them to dictyBase and develop methods to display the data and make it searchable. PROJECT NARRATIVE dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9738586,R01GM064426,"['Anatomy', 'Animals', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Code', 'Collaborations', 'Communities', 'DNA sequencing', 'Data', 'Data Display', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Engineering', 'Eukaryota', 'FAIR principles', 'Funding', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Gold', 'Information Resources', 'Investments', 'Knowledge', 'Link', 'Literature', 'Manuals', 'Methods', 'Modeling', 'Names', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phenotype', 'Plants', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Running', 'Signal Transduction', 'Site', 'Students', 'Supervision', 'System', 'Teaching Materials', 'United States National Institutes of Health', 'Update', 'Work', 'Yeasts', 'analytical tool', 'base', 'cell motility', 'cloud based', 'comparative genomics', 'contig', 'dashboard', 'data warehouse', 'deep learning', 'experimental study', 'genome annotation', 'genome-wide', 'human disease', 'improved', 'instructor', 'interest', 'microbial', 'model organisms databases', 'mutant', 'new technology', 'novel', 'pathogen', 'reference genome', 'response', 'teacher', 'tool', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,506287,0.12601406627934625
"The Transporter Classification Database (TCDB) ABSTRACT  Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and, when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, individual transport system and constituent proteins. It also includes a superfamily hyperlink.  In this competitive renewal of GM0077402, we propose to continue to expand, update, and semi-automate TCDB. Our specific aims are to (1) upgrade TCDB by characterizing and categorizing protein domains and their topologies, motifs, repeat units, functional interactions, alternative splicing and post-translational modifications, (2) expand TCDB by implementing novel pipelines for data entry that will increase the coverage of transport diversity in TCDB while describing more effectively the complexity of multicomponent transport systems, (3) enter into TCDB transporter modulators such as activators, inhibitors, drugs and xenobiotics as well as internal and external conditions that influence transporter activities, while generating an ontology to describe the effects of chemical modulators that will complement our substrate ontology, (4) incorporate into TCDB synthetic pores/channels (TC subclass 1.D), and carriers (TC subclass 2.B), (5) introduce into TCDB connections between transport and metabolism and (6) expand our plans for long-term TCDB sustainability. PROJECT NARRATIVE  TCDB is the only IUBMB-approved database providing the worldwide scientific community with systematic information about proteins that catalyze transmembrane transport. Transport proteins play critical roles in health-related issues such as personalized medicine, cancer, drug development, bacterial pathogenesis and antimicrobial resistance. Funding of this proposal will allow us to provide research results, as well as high-quality data and software for the identification of transporter proteins useful to scientists whose investigations focus on health issues.",The Transporter Classification Database (TCDB),9895808,R01GM077402,"['Activator Appliances', 'Adopted', 'Affect', 'Alternative Splicing', 'Antimicrobial Resistance', 'Antineoplastic Agents', 'Biochemical', 'Biochemistry', 'Biological Phenomena', 'Biology', 'Biotechnology', 'Carrier Proteins', 'Cells', 'Chemicals', 'Classification', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Data', 'Database Management Systems', 'Databases', 'Development', 'Ecosystem', 'Ensure', 'Eukaryota', 'Family', 'Funding', 'Genetic Diseases', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Homeostasis', 'Human Genetics', 'Individual', 'Information Resources', 'International', 'Investigation', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Ligands', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Membrane Proteins', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Multi-Drug Resistance', 'Ontology', 'Organelles', 'Pathogenesis', 'Pathologic', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Play', 'Post-Translational Protein Processing', 'Production', 'Protein Isoforms', 'Proteins', 'Pump', 'RNA Splicing', 'Research', 'Research Personnel', 'Role', 'Scientist', 'Stimulus', 'Structure', 'System', 'Systems Biology', 'Tertiary Protein Structure', 'Time', 'Tissues', 'Training', 'Transmembrane Transport', 'Update', 'Variant', 'Vertebral column', 'Work', 'Xenobiotics', 'base', 'data pipeline', 'drug development', 'improved', 'inhibitor/antagonist', 'insight', 'member', 'metagenome', 'novel', 'pathogen', 'personalized medicine', 'protein transport', 'screening', 'text searching', 'transmission process', 'tumor', 'whole genome', 'willingness']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2020,315000,0.02675560823715845
"The Transporter Classification Database (TCDB) ABSTRACT  Transporters catalyze entry and exit of molecules into and out of cells and organelles. They achieve cellular homeostasis, are responsible for multidrug resistance in pathogens and tumors, and, when defective, cause dozens of important human genetic diseases. Our laboratory maintains, updates and improves the Transporter Classification Database, TCDB, which houses the Transporter Classification (TC) system, adopted officially by the International Union of Biochemistry and Molecular Biology (IUBMB). TCDB is the internationally acclaimed, carefully annotated, universal standard for classifying and providing information about transporters and transport-related proteins in all major domains of life. It presents sequence, biochemical, physiological, pathological, structural and evolutionary data about these proteins and the transport systems they comprise. It uses a successful system of classification based on transporter class, subclass, family, subfamily, individual transport system and constituent proteins. It also includes a superfamily hyperlink.  In this competitive renewal of GM0077402, we propose to continue to expand, update, and semi-automate TCDB. Our specific aims are to (1) upgrade TCDB by characterizing and categorizing protein domains and their topologies, motifs, repeat units, functional interactions, alternative splicing and post-translational modifications, (2) expand TCDB by implementing novel pipelines for data entry that will increase the coverage of transport diversity in TCDB while describing more effectively the complexity of multicomponent transport systems, (3) enter into TCDB transporter modulators such as activators, inhibitors, drugs and xenobiotics as well as internal and external conditions that influence transporter activities, while generating an ontology to describe the effects of chemical modulators that will complement our substrate ontology, (4) incorporate into TCDB synthetic pores/channels (TC subclass 1.D), and carriers (TC subclass 2.B), (5) introduce into TCDB connections between transport and metabolism and (6) expand our plans for long-term TCDB sustainability. PROJECT NARRATIVE  TCDB is the only IUBMB-approved database providing the worldwide scientific community with systematic information about proteins that catalyze transmembrane transport. Transport proteins play critical roles in health-related issues such as personalized medicine, cancer, drug development, bacterial pathogenesis and antimicrobial resistance. Funding of this proposal will allow us to provide research results, as well as high-quality data and software for the identification of transporter proteins useful to scientists whose investigations focus on health issues.",The Transporter Classification Database (TCDB),9738997,R01GM077402,"['Activator Appliances', 'Adopted', 'Affect', 'Alternative Splicing', 'Antimicrobial Resistance', 'Antineoplastic Agents', 'Biochemical', 'Biochemistry', 'Biological Phenomena', 'Biology', 'Biotechnology', 'Carrier Proteins', 'Cells', 'Chemicals', 'Classification', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Data', 'Data Base Management', 'Data Quality', 'Databases', 'Development', 'Ecosystem', 'Ensure', 'Eukaryota', 'Family', 'Funding', 'Genetic Diseases', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Homeostasis', 'Human Genetics', 'Individual', 'Information Resources', 'International', 'Investigation', 'Island', 'Knowledge', 'Laboratories', 'Life', 'Ligands', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Membrane Proteins', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Multi-Drug Resistance', 'Ontology', 'Organelles', 'Pathogenesis', 'Pathologic', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Play', 'Post-Translational Protein Processing', 'Production', 'Protein Isoforms', 'Proteins', 'Pump', 'RNA Splicing', 'Research', 'Research Personnel', 'Role', 'Scientist', 'Stimulus', 'Structure', 'System', 'Systems Biology', 'Tertiary Protein Structure', 'Time', 'Tissues', 'Training', 'Transmembrane Transport', 'Update', 'Variant', 'Vertebral column', 'Work', 'Xenobiotics', 'base', 'data pipeline', 'drug development', 'improved', 'inhibitor/antagonist', 'insight', 'member', 'metagenome', 'novel', 'pathogen', 'personalized medicine', 'protein transport', 'screening', 'text searching', 'transmission process', 'tumor', 'whole genome', 'willingness']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2019,314750,0.02675560823715845
"Immune System Biological Networks:Case Study Improved Data Integration & Analysis    DESCRIPTION (provided by applicant): Progress in biomedical research and its translation into clinical practice require the integration of data across multiple scales (molecules, cells, organisms), organism types, and fields of research. The need for data integration is especially acute in infectious disease research where organisms interact on all scales, and these interactions result in the emergence of processes and structures specific to these interactions. True data integration, the ability to jointly interpret and analyze data of heterogeneous types, depends on the ability to link data to information about the biological entities to which the data refer. In the face of rapidly growing volumes of data and information, it is imperative that this link from data to information be computable. Automated processing of the links between data and information requires that they be expressed using a common, formalized system for knowledge representation. Efforts at knowledge representation in biology have focused on either ontology development or pathway representation. While the value of both is unquestionable, neither fully supports the data and information integration needs of infectious disease research. We propose an ontology-based approach to pathway representation that extends ontologies beyond single taxonomies and pathway representations to all levels of granularity, thereby allowing the representation of complex biological systems. Our approach builds upon existing ontologies and pathway representations but is grounded in formal ontological and logical principles. Our overall goal is to test empirically the degree to which the ontology-based representation can improve data interpretation and analysis for translational medicine. We will take as our case study Staphylococcus aureus infection, utilizing the invaluable data resources of the Duke Staphylococcus aureus Bacteremia Group. We will achieve our goal through the following three specific aims: 1. Create an ontology-based representation of host-pathogen interactions, focusing on Staphylococcus aureus bacteremia. 2. Empirically test the ability of the ontology-based representation created in Aim 1 to improve data analysis and interpretation by using the representation to predict disease genes associated with Staphylococcus aureus bacteremia. 3. Empirically test the impact of the ontology-based representation created in Aim 1 on understanding of Staphylococcus aureus pathogenesis, on identification of novel therapeutic targets, and on improvement to patient management by testing experimentally the disease gene predictions made under Aim 2. The anticipated outcomes are: an ontology-based method for the representation of complex biological systems and an ontology of host-pathogen interactions, both subjected to tests designed to demonstrate their utility to clinical and translational research; an improved understanding of the immune response to bacterial pathogens; and the identification of genes associated with Staphylococcus aureus bacteremia that can be used to develop novel diagnostics and therapeutics.The resources developed under this proposal will directly improve data integration, retrieval and analysis, will support cross-disciplinary collaborations within infectious disease research, and will provide a foundation from which to develop similar resources for other areas in biomedicine, thus significantly impacting biomedical research and translational medicine.          n/a",Immune System Biological Networks:Case Study Improved Data Integration & Analysis,8147764,R01AI077706,"['Acute', 'Area', 'Bacteremia', 'Biological', 'Biology', 'Biomedical Research', 'Blood Circulation', 'Candidate Disease Gene', 'Case Study', 'Cells', 'Clinical', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Development', 'Disease', 'Foundations', 'Genes', 'Genotype', 'Goals', 'Immune response', 'Immune system', 'Infection', 'Infectious Diseases Research', 'Information Resources', 'Knowledge', 'Link', 'Methods', 'Molecular', 'Ontology', 'Organism', 'Outcome', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Process', 'Research', 'Resources', 'Retrieval', 'Sample Size', 'Source', 'Staphylococcus aureus', 'Structure', 'System', 'Taxonomy', 'Testing', 'Translational Research', 'Translations', 'base', 'clinical practice', 'complex biological systems', 'data integration', 'design', 'improved', 'information organization', 'new therapeutic target', 'novel diagnostics', 'novel therapeutics', 'pathogen', 'success', 'syntax', 'translational medicine']",NIAID,UT SOUTHWESTERN MEDICAL CENTER,R01,2011,336063,0.07020311443170385
"Immune System Biological Networks:Case Study Improved Data Integration & Analysis    DESCRIPTION (provided by applicant): Progress in biomedical research and its translation into clinical practice require the integration of data across multiple scales (molecules, cells, organisms), organism types, and fields of research. The need for data integration is especially acute in infectious disease research where organisms interact on all scales, and these interactions result in the emergence of processes and structures specific to these interactions. True data integration, the ability to jointly interpret and analyze data of heterogeneous types, depends on the ability to link data to information about the biological entities to which the data refer. In the face of rapidly growing volumes of data and information, it is imperative that this link from data to information be computable. Automated processing of the links between data and information requires that they be expressed using a common, formalized system for knowledge representation. Efforts at knowledge representation in biology have focused on either ontology development or pathway representation. While the value of both is unquestionable, neither fully supports the data and information integration needs of infectious disease research. We propose an ontology-based approach to pathway representation that extends ontologies beyond single taxonomies and pathway representations to all levels of granularity, thereby allowing the representation of complex biological systems. Our approach builds upon existing ontologies and pathway representations but is grounded in formal ontological and logical principles. Our overall goal is to test empirically the degree to which the ontology-based representation can improve data interpretation and analysis for translational medicine. We will take as our case study Staphylococcus aureus infection, utilizing the invaluable data resources of the Duke Staphylococcus aureus Bacteremia Group. We will achieve our goal through the following three specific aims: 1. Create an ontology-based representation of host-pathogen interactions, focusing on Staphylococcus aureus bacteremia. 2. Empirically test the ability of the ontology-based representation created in Aim 1 to improve data analysis and interpretation by using the representation to predict disease genes associated with Staphylococcus aureus bacteremia. 3. Empirically test the impact of the ontology-based representation created in Aim 1 on understanding of Staphylococcus aureus pathogenesis, on identification of novel therapeutic targets, and on improvement to patient management by testing experimentally the disease gene predictions made under Aim 2. The anticipated outcomes are: an ontology-based method for the representation of complex biological systems and an ontology of host-pathogen interactions, both subjected to tests designed to demonstrate their utility to clinical and translational research; an improved understanding of the immune response to bacterial pathogens; and the identification of genes associated with Staphylococcus aureus bacteremia that can be used to develop novel diagnostics and therapeutics.The resources developed under this proposal will directly improve data integration, retrieval and analysis, will support cross-disciplinary collaborations within infectious disease research, and will provide a foundation from which to develop similar resources for other areas in biomedicine, thus significantly impacting biomedical research and translational medicine.          n/a",Immune System Biological Networks:Case Study Improved Data Integration & Analysis,7924523,R01AI077706,"['Acute', 'Area', 'Bacteremia', 'Biological', 'Biology', 'Biomedical Research', 'Blood Circulation', 'Candidate Disease Gene', 'Case Study', 'Cells', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Development', 'Disease', 'Face', 'Foundations', 'Genes', 'Genotype', 'Goals', 'Immune response', 'Immune system', 'Infection', 'Infectious Diseases Research', 'Information Resources', 'Knowledge', 'Link', 'Methods', 'Molecular', 'Ontology', 'Organism', 'Outcome', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Process', 'Research', 'Resources', 'Retrieval', 'Sample Size', 'Source', 'Staphylococcus aureus', 'Structure', 'System', 'Taxonomy', 'Testing', 'Therapeutic', 'Translational Research', 'Translations', 'base', 'clinical practice', 'complex biological systems', 'data integration', 'design', 'improved', 'information organization', 'new therapeutic target', 'novel diagnostics', 'pathogen', 'success', 'syntax', 'translational medicine']",NIAID,UT SOUTHWESTERN MEDICAL CENTER,R01,2010,383332,0.07020311443170385
"Immune System Biological Networks:Case Study Improved Data Integration & Analysis    DESCRIPTION (provided by applicant): Progress in biomedical research and its translation into clinical practice require the integration of data across multiple scales (molecules, cells, organisms), organism types, and fields of research. The need for data integration is especially acute in infectious disease research where organisms interact on all scales, and these interactions result in the emergence of processes and structures specific to these interactions. True data integration, the ability to jointly interpret and analyze data of heterogeneous types, depends on the ability to link data to information about the biological entities to which the data refer. In the face of rapidly growing volumes of data and information, it is imperative that this link from data to information be computable. Automated processing of the links between data and information requires that they be expressed using a common, formalized system for knowledge representation. Efforts at knowledge representation in biology have focused on either ontology development or pathway representation. While the value of both is unquestionable, neither fully supports the data and information integration needs of infectious disease research. We propose an ontology-based approach to pathway representation that extends ontologies beyond single taxonomies and pathway representations to all levels of granularity, thereby allowing the representation of complex biological systems. Our approach builds upon existing ontologies and pathway representations but is grounded in formal ontological and logical principles. Our overall goal is to test empirically the degree to which the ontology-based representation can improve data interpretation and analysis for translational medicine. We will take as our case study Staphylococcus aureus infection, utilizing the invaluable data resources of the Duke Staphylococcus aureus Bacteremia Group. We will achieve our goal through the following three specific aims: 1. Create an ontology-based representation of host-pathogen interactions, focusing on Staphylococcus aureus bacteremia. 2. Empirically test the ability of the ontology-based representation created in Aim 1 to improve data analysis and interpretation by using the representation to predict disease genes associated with Staphylococcus aureus bacteremia. 3. Empirically test the impact of the ontology-based representation created in Aim 1 on understanding of Staphylococcus aureus pathogenesis, on identification of novel therapeutic targets, and on improvement to patient management by testing experimentally the disease gene predictions made under Aim 2. The anticipated outcomes are: an ontology-based method for the representation of complex biological systems and an ontology of host-pathogen interactions, both subjected to tests designed to demonstrate their utility to clinical and translational research; an improved understanding of the immune response to bacterial pathogens; and the identification of genes associated with Staphylococcus aureus bacteremia that can be used to develop novel diagnostics and therapeutics.The resources developed under this proposal will directly improve data integration, retrieval and analysis, will support cross-disciplinary collaborations within infectious disease research, and will provide a foundation from which to develop similar resources for other areas in biomedicine, thus significantly impacting biomedical research and translational medicine.          n/a",Immune System Biological Networks:Case Study Improved Data Integration & Analysis,7927981,R01AI077706,"['Acute', 'Area', 'Bacteremia', 'Biological', 'Biology', 'Biomedical Research', 'Blood Circulation', 'Candidate Disease Gene', 'Case Study', 'Cells', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Development', 'Disease', 'Face', 'Foundations', 'Genes', 'Genotype', 'Goals', 'Immune response', 'Immune system', 'Infection', 'Infectious Diseases Research', 'Information Resources', 'Knowledge', 'Link', 'Methods', 'Molecular', 'Ontology', 'Organism', 'Outcome', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Process', 'Research', 'Resources', 'Retrieval', 'Sample Size', 'Source', 'Staphylococcus aureus', 'Structure', 'System', 'Taxonomy', 'Testing', 'Therapeutic', 'Translational Research', 'Translations', 'base', 'clinical practice', 'complex biological systems', 'data integration', 'design', 'improved', 'information organization', 'new therapeutic target', 'novel diagnostics', 'pathogen', 'success', 'syntax', 'translational medicine']",NIAID,DUKE UNIVERSITY,R01,2009,74071,0.07020311443170385
"Immune System Biological Networks:Case Study Improved Data Integration & Analysis    DESCRIPTION (provided by applicant): Progress in biomedical research and its translation into clinical practice require the integration of data across multiple scales (molecules, cells, organisms), organism types, and fields of research. The need for data integration is especially acute in infectious disease research where organisms interact on all scales, and these interactions result in the emergence of processes and structures specific to these interactions. True data integration, the ability to jointly interpret and analyze data of heterogeneous types, depends on the ability to link data to information about the biological entities to which the data refer. In the face of rapidly growing volumes of data and information, it is imperative that this link from data to information be computable. Automated processing of the links between data and information requires that they be expressed using a common, formalized system for knowledge representation. Efforts at knowledge representation in biology have focused on either ontology development or pathway representation. While the value of both is unquestionable, neither fully supports the data and information integration needs of infectious disease research. We propose an ontology-based approach to pathway representation that extends ontologies beyond single taxonomies and pathway representations to all levels of granularity, thereby allowing the representation of complex biological systems. Our approach builds upon existing ontologies and pathway representations but is grounded in formal ontological and logical principles. Our overall goal is to test empirically the degree to which the ontology-based representation can improve data interpretation and analysis for translational medicine. We will take as our case study Staphylococcus aureus infection, utilizing the invaluable data resources of the Duke Staphylococcus aureus Bacteremia Group. We will achieve our goal through the following three specific aims: 1. Create an ontology-based representation of host-pathogen interactions, focusing on Staphylococcus aureus bacteremia. 2. Empirically test the ability of the ontology-based representation created in Aim 1 to improve data analysis and interpretation by using the representation to predict disease genes associated with Staphylococcus aureus bacteremia. 3. Empirically test the impact of the ontology-based representation created in Aim 1 on understanding of Staphylococcus aureus pathogenesis, on identification of novel therapeutic targets, and on improvement to patient management by testing experimentally the disease gene predictions made under Aim 2. The anticipated outcomes are: an ontology-based method for the representation of complex biological systems and an ontology of host-pathogen interactions, both subjected to tests designed to demonstrate their utility to clinical and translational research; an improved understanding of the immune response to bacterial pathogens; and the identification of genes associated with Staphylococcus aureus bacteremia that can be used to develop novel diagnostics and therapeutics.The resources developed under this proposal will directly improve data integration, retrieval and analysis, will support cross-disciplinary collaborations within infectious disease research, and will provide a foundation from which to develop similar resources for other areas in biomedicine, thus significantly impacting biomedical research and translational medicine.          n/a",Immune System Biological Networks:Case Study Improved Data Integration & Analysis,7690285,R01AI077706,"['Acute', 'Area', 'Bacteremia', 'Biological', 'Biology', 'Biomedical Research', 'Blood Circulation', 'Candidate Disease Gene', 'Case Study', 'Cells', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Development', 'Disease', 'Face', 'Foundations', 'Genes', 'Genotype', 'Goals', 'Immune response', 'Immune system', 'Infection', 'Infectious Diseases Research', 'Information Resources', 'Knowledge', 'Link', 'Methods', 'Molecular', 'Ontology', 'Organism', 'Outcome', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Process', 'Research', 'Resources', 'Retrieval', 'Sample Size', 'Source', 'Staphylococcus aureus', 'Structure', 'System', 'Taxonomy', 'Testing', 'Therapeutic', 'Translational Research', 'Translations', 'base', 'clinical practice', 'complex biological systems', 'data integration', 'design', 'improved', 'information organization', 'new therapeutic target', 'novel diagnostics', 'pathogen', 'success', 'syntax', 'translational medicine']",NIAID,DUKE UNIVERSITY,R01,2009,391200,0.07020311443170385
"Immune System Biological Networks:Case Study Improved Data Integration & Analysis    DESCRIPTION (provided by applicant): Progress in biomedical research and its translation into clinical practice require the integration of data across multiple scales (molecules, cells, organisms), organism types, and fields of research. The need for data integration is especially acute in infectious disease research where organisms interact on all scales, and these interactions result in the emergence of processes and structures specific to these interactions. True data integration, the ability to jointly interpret and analyze data of heterogeneous types, depends on the ability to link data to information about the biological entities to which the data refer. In the face of rapidly growing volumes of data and information, it is imperative that this link from data to information be computable. Automated processing of the links between data and information requires that they be expressed using a common, formalized system for knowledge representation. Efforts at knowledge representation in biology have focused on either ontology development or pathway representation. While the value of both is unquestionable, neither fully supports the data and information integration needs of infectious disease research. We propose an ontology-based approach to pathway representation that extends ontologies beyond single taxonomies and pathway representations to all levels of granularity, thereby allowing the representation of complex biological systems. Our approach builds upon existing ontologies and pathway representations but is grounded in formal ontological and logical principles. Our overall goal is to test empirically the degree to which the ontology-based representation can improve data interpretation and analysis for translational medicine. We will take as our case study Staphylococcus aureus infection, utilizing the invaluable data resources of the Duke Staphylococcus aureus Bacteremia Group. We will achieve our goal through the following three specific aims: 1. Create an ontology-based representation of host-pathogen interactions, focusing on Staphylococcus aureus bacteremia. 2. Empirically test the ability of the ontology-based representation created in Aim 1 to improve data analysis and interpretation by using the representation to predict disease genes associated with Staphylococcus aureus bacteremia. 3. Empirically test the impact of the ontology-based representation created in Aim 1 on understanding of Staphylococcus aureus pathogenesis, on identification of novel therapeutic targets, and on improvement to patient management by testing experimentally the disease gene predictions made under Aim 2. The anticipated outcomes are: an ontology-based method for the representation of complex biological systems and an ontology of host-pathogen interactions, both subjected to tests designed to demonstrate their utility to clinical and translational research; an improved understanding of the immune response to bacterial pathogens; and the identification of genes associated with Staphylococcus aureus bacteremia that can be used to develop novel diagnostics and therapeutics.The resources developed under this proposal will directly improve data integration, retrieval and analysis, will support cross-disciplinary collaborations within infectious disease research, and will provide a foundation from which to develop similar resources for other areas in biomedicine, thus significantly impacting biomedical research and translational medicine.          n/a",Immune System Biological Networks:Case Study Improved Data Integration & Analysis,7437571,R01AI077706,"['Acute', 'Area', 'Bacteremia', 'Biological', 'Biology', 'Biomedical Research', 'Blood Circulation', 'Candidate Disease Gene', 'Case Study', 'Cells', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Development', 'Disease', 'Face', 'Foundations', 'Genes', 'Genotype', 'Goals', 'Immune response', 'Immune system', 'Infection', 'Infectious Diseases Research', 'Information Resources', 'Knowledge', 'Link', 'Methods', 'Molecular', 'Ontology', 'Organism', 'Other Resources', 'Outcome', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Process', 'Purpose', 'Representations, Knowledge (Computer)', 'Research', 'Resources', 'Retrieval', 'Sample Size', 'Source', 'Staphylococcus aureus', 'Structure', 'System', 'Taxonomy', 'Testing', 'Therapeutic', 'Translational Research', 'Translations', 'base', 'data integration', 'design', 'improved', 'information organization', 'novel diagnostics', 'novel therapeutics', 'pathogen', 'success', 'syntax', 'therapeutic target', 'translational medicine']",NIAID,DUKE UNIVERSITY,R01,2008,405175,0.07020311443170385
"Comparative genomics of protein structure and function    DESCRIPTION (provided by applicant): This work aims to develop methods to identify functional sites in protein structures and to characterize protein function on a genomic scale. The approach is predicated on the Evolutionary Trace method (ET) to locate functional sites in structures. Preliminary studies enabled us to automate the basic steps towards a complete, automated functional annotation pipeline, namely, functional site analysis with ET; extraction from ET analysis of SD-templates that describe composition and conformation of key residues involved in binding or catalytic function; the search in other structures for geometric matches to these 3D-templates; and the analysis of which of those matched are most biologically relevant. We now seek to increase the sensitivity and specificity of the annotation pipeline by optimizing the definition of 3-D templates, by adding new template features to better judge whether molecular mimicry underlies functional similarity; and by developing novel strategies that use multiple templates to identify function. The result will reveal which regions of proteins are most biologically relevant, and hence logical targets for protein engineering and drug design, and it will extend to three dimensions a functional annotation strategy traditionally based on one- dimensional pattern matching in protein sequences. In so doing, this work addresses a fundamental NIH roadmap problem in ""post-genomic biology"": linking massive and exponentially growing amounts of raw sequence and structure data to the molecular basis of biological function.           n/a",Comparative genomics of protein structure and function,7786185,R01GM079656,"['3-Dimensional', 'Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acids', 'Benchmarking', 'Binding', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Catalysis', 'Collaborations', 'Data', 'Databases', 'Dimensions', 'Drug Design', 'Electrostatics', 'Enzymes', 'Failure', 'Genomics', 'Goals', 'Gold', 'Ligand Binding', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Methods', 'Molecular', 'Molecular Conformation', 'Molecular Mimicry', 'Mutation', 'Outcome', 'PAWR protein', 'Pattern', 'Peptide Sequence Determination', 'Performance', 'Pharmaceutical Preparations', 'Protein Binding', 'Protein Engineering', 'Protein Region', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resources', 'Sampling', 'Sensitivity and Specificity', 'Site', 'Solutions', 'Solvents', 'Source', 'Source Code', 'Specificity', 'Structure', 'Surface', 'Testing', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'comparative genomics', 'computer studies', 'conformer', 'database structure', 'design', 'falls', 'flexibility', 'heuristics', 'inhibitor/antagonist', 'interest', 'mimetics', 'novel strategies', 'programs', 'protein function', 'protein structure', 'protein structure function', 'structural genomics', 'success', 'tool']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2010,278531,0.06774532947769774
"Comparative genomics of protein structure and function    DESCRIPTION (provided by applicant): This work aims to develop methods to identify functional sites in protein structures and to characterize protein function on a genomic scale. The approach is predicated on the Evolutionary Trace method (ET) to locate functional sites in structures. Preliminary studies enabled us to automate the basic steps towards a complete, automated functional annotation pipeline, namely, functional site analysis with ET; extraction from ET analysis of SD-templates that describe composition and conformation of key residues involved in binding or catalytic function; the search in other structures for geometric matches to these 3D-templates; and the analysis of which of those matched are most biologically relevant. We now seek to increase the sensitivity and specificity of the annotation pipeline by optimizing the definition of 3-D templates, by adding new template features to better judge whether molecular mimicry underlies functional similarity; and by developing novel strategies that use multiple templates to identify function. The result will reveal which regions of proteins are most biologically relevant, and hence logical targets for protein engineering and drug design, and it will extend to three dimensions a functional annotation strategy traditionally based on one- dimensional pattern matching in protein sequences. In so doing, this work addresses a fundamental NIH roadmap problem in ""post-genomic biology"": linking massive and exponentially growing amounts of raw sequence and structure data to the molecular basis of biological function.           n/a",Comparative genomics of protein structure and function,7586248,R01GM079656,"['3-Dimensional', 'Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acids', 'Benchmarking', 'Binding', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Catalysis', 'Collaborations', 'Data', 'Databases', 'Dimensions', 'Drug Design', 'Electrostatics', 'Enzymes', 'Failure', 'Genomics', 'Goals', 'Gold', 'Ligand Binding', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Methods', 'Molecular', 'Molecular Conformation', 'Molecular Mimicry', 'Mutation', 'Outcome', 'PAWR protein', 'Pattern', 'Peptide Sequence Determination', 'Performance', 'Pharmaceutical Preparations', 'Protein Binding', 'Protein Engineering', 'Protein Region', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resources', 'Sampling', 'Sensitivity and Specificity', 'Site', 'Solutions', 'Solvents', 'Source', 'Source Code', 'Specificity', 'Structure', 'Surface', 'Testing', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'comparative', 'computer studies', 'conformer', 'database structure', 'design', 'falls', 'flexibility', 'heuristics', 'inhibitor/antagonist', 'interest', 'mimetics', 'novel strategies', 'programs', 'protein function', 'protein structure', 'protein structure function', 'structural genomics', 'success', 'tool']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2009,281344,0.06774532947769774
"Comparative genomics of protein structure and function    DESCRIPTION (provided by applicant): This work aims to develop methods to identify functional sites in protein structures and to characterize protein function on a genomic scale. The approach is predicated on the Evolutionary Trace method (ET) to locate functional sites in structures. Preliminary studies enabled us to automate the basic steps towards a complete, automated functional annotation pipeline, namely, functional site analysis with ET; extraction from ET analysis of SD-templates that describe composition and conformation of key residues involved in binding or catalytic function; the search in other structures for geometric matches to these 3D-templates; and the analysis of which of those matched are most biologically relevant. We now seek to increase the sensitivity and specificity of the annotation pipeline by optimizing the definition of 3-D templates, by adding new template features to better judge whether molecular mimicry underlies functional similarity; and by developing novel strategies that use multiple templates to identify function. The result will reveal which regions of proteins are most biologically relevant, and hence logical targets for protein engineering and drug design, and it will extend to three dimensions a functional annotation strategy traditionally based on one- dimensional pattern matching in protein sequences. In so doing, this work addresses a fundamental NIH roadmap problem in ""post-genomic biology"": linking massive and exponentially growing amounts of raw sequence and structure data to the molecular basis of biological function.           n/a",Comparative genomics of protein structure and function,7391818,R01GM079656,"['3-Dimensional', 'Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acids', 'Benchmarking', 'Binding', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Catalysis', 'Collaborations', 'Data', 'Databases', 'Dimensions', 'Drug Design', 'Electrostatics', 'Enzymes', 'Failure', 'Genomics', 'Goals', 'Gold', 'Ligand Binding', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Methods', 'Molecular', 'Molecular Conformation', 'Molecular Mimicry', 'Mutation', 'Numbers', 'Outcome', 'PAWR protein', 'Pattern', 'Peptide Sequence Determination', 'Performance', 'Pharmaceutical Preparations', 'Pliability', 'Protein Binding', 'Protein Engineering', 'Protein Region', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resources', 'Sampling', 'Sensitivity and Specificity', 'Site', 'Solutions', 'Solvents', 'Source', 'Source Code', 'Specificity', 'Standards of Weights and Measures', 'Structure', 'Surface', 'Testing', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'comparative', 'computer studies', 'conformer', 'design', 'falls', 'heuristics', 'inhibitor/antagonist', 'interest', 'mimetics', 'novel strategies', 'programs', 'protein function', 'protein structure', 'protein structure function', 'structural genomics', 'success', 'tool']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2008,281344,0.06774532947769774
"Comparative genomics of protein structure and function    DESCRIPTION (provided by applicant): This work aims to develop methods to identify functional sites in protein structures and to characterize protein function on a genomic scale. The approach is predicated on the Evolutionary Trace method (ET) to locate functional sites in structures. Preliminary studies enabled us to automate the basic steps towards a complete, automated functional annotation pipeline, namely, functional site analysis with ET; extraction from ET analysis of SD-templates that describe composition and conformation of key residues involved in binding or catalytic function; the search in other structures for geometric matches to these 3D-templates; and the analysis of which of those matched are most biologically relevant. We now seek to increase the sensitivity and specificity of the annotation pipeline by optimizing the definition of 3-D templates, by adding new template features to better judge whether molecular mimicry underlies functional similarity; and by developing novel strategies that use multiple templates to identify function. The result will reveal which regions of proteins are most biologically relevant, and hence logical targets for protein engineering and drug design, and it will extend to three dimensions a functional annotation strategy traditionally based on one- dimensional pattern matching in protein sequences. In so doing, this work addresses a fundamental NIH roadmap problem in ""post-genomic biology"": linking massive and exponentially growing amounts of raw sequence and structure data to the molecular basis of biological function.           n/a",Comparative genomics of protein structure and function,7192957,R01GM079656,"['3-Dimensional', 'Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acids', 'Benchmarking', 'Binding', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Catalysis', 'Collaborations', 'Data', 'Databases', 'Dimensions', 'Drug Design', 'Electrostatics', 'Enzymes', 'Failure', 'Genomics', 'Goals', 'Gold', 'Ligand Binding', 'Link', 'Machine Learning', 'Manuals', 'Maps', 'Measures', 'Methods', 'Molecular', 'Molecular Conformation', 'Molecular Mimicry', 'Mutation', 'Numbers', 'Outcome', 'PAWR protein', 'Pattern', 'Peptide Sequence Determination', 'Performance', 'Pharmaceutical Preparations', 'Pliability', 'Protein Binding', 'Protein Engineering', 'Protein Region', 'Protein Structure Initiative', 'Proteins', 'Research Personnel', 'Resources', 'Sampling', 'Sensitivity and Specificity', 'Site', 'Solutions', 'Solvents', 'Source', 'Source Code', 'Specificity', 'Standards of Weights and Measures', 'Structure', 'Surface', 'Testing', 'United States National Institutes of Health', 'Variant', 'Work', 'base', 'comparative', 'computer studies', 'conformer', 'design', 'falls', 'heuristics', 'inhibitor/antagonist', 'interest', 'mimetics', 'novel strategies', 'programs', 'protein function', 'protein structure', 'protein structure function', 'structural genomics', 'success', 'tool']",NIGMS,BAYLOR COLLEGE OF MEDICINE,R01,2007,290844,0.06774532947769774
"Comprehensive annotation of subcellular localization of entire organisms    DESCRIPTION (provided by applicant): The difference between the number of proteins with known sequence and those with well- studied function (sequence-function gap) is growing daily. One well-defined coarse-grained aspect of function is the native subcellular localization of a protein that has a central role in the Gene Ontology (GO) hierarchy. Many detailed and high-throughput experiments annotate localization. Where experiments do not reach, homology-based and de novo prediction methods succeed. Here, we propose the development of a comprehensive system that combines experimental resources with data mining techniques and novel prediction methods with the objective to annotate localization for entirely sequenced eukaryotes at an unprecedented detail and accuracy. Firstly, we propose to gather all available data and all relevant methods to build a comprehensive localization atlas for human and Arabidopsis. Secondly, we plan to develop novel methods tailored specifically to capture proteins for which we are left with no reliable annotations after completing the first step. We assume that these methods will focus on the prediction of the particular type of membrane into which an integral membrane protein is inserted, and of the native localization for minor eukaryotic compartments (ER, Golgi, lysosome). Thirdly, we propose the implementation of specific improvements over today's motif-based methods for secreted and nuclear proteins, as well as the extension of de novo predictions for the major compartments. An important objective will be to maintain high levels of performance for splice variants and for sequence fragments. Overall, the project will require the analysis of existing biological databases, the development of novel methods, and the combination of existing ones; it will generate novel information available through internet servers, standalone programs and databases.      RELEVANCE: The annotations generated by our system will aid the design of detailed and high-throughput experimental studies. In particular, localization may increase in its relevance as one essential feature used to infer networks of interactions. The ultimate goal of our project is the generation of an atlas that maps all proteins in a cell. Eventually, this atlas will constitute a 4D map; it will localize proteins in their 3D cellular environments and resolve the coarse-grained dynamics of the system, e.g. ""expression on ribosomes, bind importin, transport into nucleus, bind DNA, bind exportin, export out of nucleus; next cell cycle"". The components proposed here constitute one crucial building block toward such a 4D map of a cell.          n/a",Comprehensive annotation of subcellular localization of entire organisms,7924860,R01GM079767,"['Accounting', 'Affect', 'Algorithms', 'Animal Model', 'Arabidopsis', 'Arts', 'Atlases', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological Transport', 'Biological databases', 'Biology', 'Categories', 'Cell Cycle', 'Cell Nucleus', 'Cells', 'Cereals', 'Chloroplasts', 'Collection', 'Communities', 'Computational Biology', 'Computer Simulation', 'DNA Binding', 'Data', 'Data Set', 'Databases', 'Development', 'Environment', 'Eukaryota', 'Evolution', 'Fingerprint', 'Follow-Up Studies', 'Generations', 'Genes', 'Genome', 'Goals', 'Golgi Apparatus', 'Homo sapiens', 'Human', 'Integral Membrane Protein', 'Internet', 'Laboratory Organism', 'Left', 'Locales', 'Lysosomes', 'Machine Learning', 'Mammals', 'Maps', 'Membrane', 'Methods', 'Mining', 'Minor', 'Mitochondria', 'Mouse-ear Cress', 'Nuclear', 'Nuclear Localization Signal', 'Nuclear Matrix-Associated Proteins', 'Nuclear Protein', 'Nuclear Proteins', 'Ontology', 'Organism', 'Outcome', 'Peptide Signal Sequences', 'Performance', 'Plants', 'Plasma', 'Play', 'Property', 'Proteins', 'Proteome', 'Protocols documentation', 'PubMed', 'Quality Control', 'RNA Splicing', 'Research', 'Resources', 'Ribosomes', 'Role', 'Sequence Alignment', 'Sequence Analysis', 'Signal Transduction', 'Software Tools', 'Sorting - Cell Movement', 'Structure', 'SwissProt', 'System', 'Techniques', 'Training', 'Translating', 'Variant', 'base', 'data mining', 'design', 'improved', 'insight', 'novel', 'numb protein', 'programs', 'protein function', 'research study', 'software systems', 'structural genomics', 'success', 'tool']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,312066,0.0724922793704811
"Comprehensive annotation of subcellular localization of entire organisms    DESCRIPTION (provided by applicant): The difference between the number of proteins with known sequence and those with well- studied function (sequence-function gap) is growing daily. One well-defined coarse-grained aspect of function is the native subcellular localization of a protein that has a central role in the Gene Ontology (GO) hierarchy. Many detailed and high-throughput experiments annotate localization. Where experiments do not reach, homology-based and de novo prediction methods succeed. Here, we propose the development of a comprehensive system that combines experimental resources with data mining techniques and novel prediction methods with the objective to annotate localization for entirely sequenced eukaryotes at an unprecedented detail and accuracy. Firstly, we propose to gather all available data and all relevant methods to build a comprehensive localization atlas for human and Arabidopsis. Secondly, we plan to develop novel methods tailored specifically to capture proteins for which we are left with no reliable annotations after completing the first step. We assume that these methods will focus on the prediction of the particular type of membrane into which an integral membrane protein is inserted, and of the native localization for minor eukaryotic compartments (ER, Golgi, lysosome). Thirdly, we propose the implementation of specific improvements over today's motif-based methods for secreted and nuclear proteins, as well as the extension of de novo predictions for the major compartments. An important objective will be to maintain high levels of performance for splice variants and for sequence fragments. Overall, the project will require the analysis of existing biological databases, the development of novel methods, and the combination of existing ones; it will generate novel information available through internet servers, standalone programs and databases.      RELEVANCE: The annotations generated by our system will aid the design of detailed and high-throughput experimental studies. In particular, localization may increase in its relevance as one essential feature used to infer networks of interactions. The ultimate goal of our project is the generation of an atlas that maps all proteins in a cell. Eventually, this atlas will constitute a 4D map; it will localize proteins in their 3D cellular environments and resolve the coarse-grained dynamics of the system, e.g. ""expression on ribosomes, bind importin, transport into nucleus, bind DNA, bind exportin, export out of nucleus; next cell cycle"". The components proposed here constitute one crucial building block toward such a 4D map of a cell.          n/a",Comprehensive annotation of subcellular localization of entire organisms,7681626,R01GM079767,"['Accounting', 'Affect', 'Algorithms', 'Animal Model', 'Arabidopsis', 'Arts', 'Atlases', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological Transport', 'Biological databases', 'Biology', 'Categories', 'Cell Cycle', 'Cell Nucleus', 'Cells', 'Cereals', 'Chloroplasts', 'Collection', 'Communities', 'Computational Biology', 'Computer Simulation', 'DNA Binding', 'Data', 'Data Set', 'Databases', 'Development', 'Environment', 'Eukaryota', 'Evolution', 'Fingerprint', 'Follow-Up Studies', 'Generations', 'Genes', 'Genome', 'Goals', 'Golgi Apparatus', 'Homo sapiens', 'Human', 'Integral Membrane Protein', 'Internet', 'Laboratory Organism', 'Left', 'Locales', 'Lysosomes', 'Machine Learning', 'Mammals', 'Maps', 'Membrane', 'Methods', 'Mining', 'Minor', 'Mitochondria', 'Mouse-ear Cress', 'Nuclear', 'Nuclear Localization Signal', 'Nuclear Matrix-Associated Proteins', 'Nuclear Protein', 'Nuclear Proteins', 'Ontology', 'Organism', 'Outcome', 'Peptide Signal Sequences', 'Performance', 'Plants', 'Plasma', 'Play', 'Property', 'Proteins', 'Proteome', 'Protocols documentation', 'PubMed', 'Quality Control', 'RNA Splicing', 'Research', 'Resources', 'Ribosomes', 'Role', 'Sequence Alignment', 'Sequence Analysis', 'Signal Transduction', 'Software Tools', 'Sorting - Cell Movement', 'Structure', 'SwissProt', 'System', 'Techniques', 'Training', 'Translating', 'Variant', 'base', 'data mining', 'design', 'improved', 'insight', 'novel', 'numb protein', 'programs', 'protein function', 'research study', 'software systems', 'structural genomics', 'success', 'tool']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,306088,0.0724922793704811
"Comprehensive annotation of subcellular localization of entire organisms    DESCRIPTION (provided by applicant): The difference between the number of proteins with known sequence and those with well- studied function (sequence-function gap) is growing daily. One well-defined coarse-grained aspect of function is the native subcellular localization of a protein that has a central role in the Gene Ontology (GO) hierarchy. Many detailed and high-throughput experiments annotate localization. Where experiments do not reach, homology-based and de novo prediction methods succeed. Here, we propose the development of a comprehensive system that combines experimental resources with data mining techniques and novel prediction methods with the objective to annotate localization for entirely sequenced eukaryotes at an unprecedented detail and accuracy. Firstly, we propose to gather all available data and all relevant methods to build a comprehensive localization atlas for human and Arabidopsis. Secondly, we plan to develop novel methods tailored specifically to capture proteins for which we are left with no reliable annotations after completing the first step. We assume that these methods will focus on the prediction of the particular type of membrane into which an integral membrane protein is inserted, and of the native localization for minor eukaryotic compartments (ER, Golgi, lysosome). Thirdly, we propose the implementation of specific improvements over today's motif-based methods for secreted and nuclear proteins, as well as the extension of de novo predictions for the major compartments. An important objective will be to maintain high levels of performance for splice variants and for sequence fragments. Overall, the project will require the analysis of existing biological databases, the development of novel methods, and the combination of existing ones; it will generate novel information available through internet servers, standalone programs and databases.      RELEVANCE: The annotations generated by our system will aid the design of detailed and high-throughput experimental studies. In particular, localization may increase in its relevance as one essential feature used to infer networks of interactions. The ultimate goal of our project is the generation of an atlas that maps all proteins in a cell. Eventually, this atlas will constitute a 4D map; it will localize proteins in their 3D cellular environments and resolve the coarse-grained dynamics of the system, e.g. ""expression on ribosomes, bind importin, transport into nucleus, bind DNA, bind exportin, export out of nucleus; next cell cycle"". The components proposed here constitute one crucial building block toward such a 4D map of a cell.          n/a",Comprehensive annotation of subcellular localization of entire organisms,7500843,R01GM079767,"['Accounting', 'Affect', 'Algorithms', 'Animal Model', 'Arabidopsis', 'Arts', 'Atlases', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological Transport', 'Biological databases', 'Biology', 'Categories', 'Cell Cycle', 'Cell Nucleus', 'Cells', 'Cereals', 'Chloroplasts', 'Class', 'Collection', 'Communities', 'Computational Biology', 'Computer Simulation', 'DNA Binding', 'Daily', 'Data', 'Data Set', 'Databases', 'Development', 'Environment', 'Eukaryota', 'Eukaryotic Cell', 'Evolution', 'Fingerprint', 'Follow-Up Studies', 'Generations', 'Genes', 'Genome', 'Goals', 'Golgi Apparatus', 'Homo sapiens', 'Human', 'Integral Membrane Protein', 'Internet', 'Laboratory Organism', 'Left', 'Locales', 'Localized', 'Lysosomes', 'Machine Learning', 'Mammals', 'Maps', 'Membrane', 'Methods', 'Mining', 'Minor', 'Mitochondria', 'Mouse-ear Cress', 'Nuclear', 'Nuclear Localization Signal', 'Nuclear Matrix-Associated Proteins', 'Nuclear Protein', 'Nuclear Proteins', 'Ontology', 'Organism', 'Outcome', 'Peptide Signal Sequences', 'Performance', 'Plants', 'Plasma', 'Play', 'Property', 'Proteins', 'Proteome', 'Protocols documentation', 'PubMed', 'Quality Control', 'RNA Splicing', 'Research', 'Resources', 'Ribosomes', 'Role', 'Sequence Alignment', 'Sequence Analysis', 'Signal Transduction', 'Software Tools', 'Sorting - Cell Movement', 'Structure', 'SwissProt', 'System', 'Techniques', 'Today', 'Training', 'Translating', 'Variant', 'base', 'data mining', 'design', 'improved', 'insight', 'novel', 'numb protein', 'programs', 'protein function', 'research study', 'software systems', 'structural genomics', 'success', 'tool']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2008,318063,0.0724922793704811
"Comprehensive annotation of subcellular localization of entire organisms    DESCRIPTION (provided by applicant): The difference between the number of proteins with known sequence and those with well- studied function (sequence-function gap) is growing daily. One well-defined coarse-grained aspect of function is the native subcellular localization of a protein that has a central role in the Gene Ontology (GO) hierarchy. Many detailed and high-throughput experiments annotate localization. Where experiments do not reach, homology-based and de novo prediction methods succeed. Here, we propose the development of a comprehensive system that combines experimental resources with data mining techniques and novel prediction methods with the objective to annotate localization for entirely sequenced eukaryotes at an unprecedented detail and accuracy. Firstly, we propose to gather all available data and all relevant methods to build a comprehensive localization atlas for human and Arabidopsis. Secondly, we plan to develop novel methods tailored specifically to capture proteins for which we are left with no reliable annotations after completing the first step. We assume that these methods will focus on the prediction of the particular type of membrane into which an integral membrane protein is inserted, and of the native localization for minor eukaryotic compartments (ER, Golgi, lysosome). Thirdly, we propose the implementation of specific improvements over today's motif-based methods for secreted and nuclear proteins, as well as the extension of de novo predictions for the major compartments. An important objective will be to maintain high levels of performance for splice variants and for sequence fragments. Overall, the project will require the analysis of existing biological databases, the development of novel methods, and the combination of existing ones; it will generate novel information available through internet servers, standalone programs and databases.      RELEVANCE: The annotations generated by our system will aid the design of detailed and high-throughput experimental studies. In particular, localization may increase in its relevance as one essential feature used to infer networks of interactions. The ultimate goal of our project is the generation of an atlas that maps all proteins in a cell. Eventually, this atlas will constitute a 4D map; it will localize proteins in their 3D cellular environments and resolve the coarse-grained dynamics of the system, e.g. ""expression on ribosomes, bind importin, transport into nucleus, bind DNA, bind exportin, export out of nucleus; next cell cycle"". The components proposed here constitute one crucial building block toward such a 4D map of a cell.          n/a",Comprehensive annotation of subcellular localization of entire organisms,7342317,R01GM079767,"['Accounting', 'Affect', 'Algorithms', 'Animal Model', 'Arabidopsis', 'Arts', 'Atlases', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological Transport', 'Biological databases', 'Biology', 'Categories', 'Cell Cycle', 'Cell Nucleus', 'Cells', 'Cereals', 'Chloroplasts', 'Class', 'Collection', 'Communities', 'Computational Biology', 'Computer Simulation', 'DNA Binding', 'Daily', 'Data', 'Data Set', 'Databases', 'Development', 'Environment', 'Eukaryota', 'Eukaryotic Cell', 'Evolution', 'Fingerprint', 'Follow-Up Studies', 'Generations', 'Genes', 'Genome', 'Goals', 'Golgi Apparatus', 'Homo sapiens', 'Human', 'Integral Membrane Protein', 'Internet', 'Laboratory Organism', 'Left', 'Locales', 'Localized', 'Lysosomes', 'Machine Learning', 'Mammals', 'Maps', 'Membrane', 'Methods', 'Mining', 'Minor', 'Mitochondria', 'Mouse-ear Cress', 'Nuclear', 'Nuclear Localization Signal', 'Nuclear Matrix-Associated Proteins', 'Nuclear Protein', 'Nuclear Proteins', 'Ontology', 'Organism', 'Outcome', 'Peptide Signal Sequences', 'Performance', 'Plants', 'Plasma', 'Play', 'Property', 'Proteins', 'Proteome', 'Protocols documentation', 'PubMed', 'Quality Control', 'RNA Splicing', 'Research', 'Resources', 'Ribosomes', 'Role', 'Sequence Alignment', 'Sequence Analysis', 'Signal Transduction', 'Software Tools', 'Sorting - Cell Movement', 'Structure', 'SwissProt', 'System', 'Techniques', 'Today', 'Training', 'Translating', 'Variant', 'base', 'data mining', 'design', 'improved', 'insight', 'novel', 'numb protein', 'programs', 'protein function', 'research study', 'software systems', 'structural genomics', 'success', 'tool']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2007,294337,0.0724922793704811
"Regenbase: A Searchable Database to Organize Regeneration Knowledge via Ontologie     DESCRIPTION (provided by applicant): This application seeks funds to develop RegenBase - a novel information system to seamlessly integrate diverse data that are produced by neuroscientists and cell biologists studying nervous system injury, disease and cell motility with other resources, such as the Neuroscience Information Framework and the BioAssay Ontology. Over the past decade the NIH has funded the development of informatics tools and ontologies to allow the integration and interrogation of the massive and diverse data sets that have been produced by the human genome project. In the area of neuroscience the most advancement have been made in the creation and annotation of large anatomical data sets that reveal patterns of gene expression and connectivity. Genesat and the BrainMaps are excellent examples and are easily searched using the Neuroscience Information Framework (NIF) portal. But it is still surprisingly difficult to search for information related to repairing the injured nrvous system. To overcome this road block it is critical to build the essential tools that allow semantic web approaches to link diverse data repositories with ontologies that allow them to be interpreted and analyzed. The success of this initiative critically relies on an effective informatcs solution to integrate the various (current and future) data types generated by neuroscientists working on nervous system injury, as well as large-scale screening efforts (such as the Molecular Libraries Probe Center Network, MLPCN) into coherent data sets and to make them accessible, interpretable, and actionable for scientists of different backgrounds and with different objectives. We propose to develop a novel knowledge-based, extensible information system of interconnected components that leverages semantic-web technologies and domain level ontologies. This system is tentatively called RegenBase (Regeneration dataBase). Tremendous progress has been made during the last decade developing semantic web technologies with the goals of formalizing knowledge, linking information across different domains, and integrating large heterogeneous data sets from diverse sources. To develop RegenBase on a fast-track with limited resources, we will leverage technologies and tools from the National Center for Biological Onotology and the recently launched BioAssay Ontology. The long-term goal of the RegenBase system is seamless ""on-the fly"" data integration and analysis via a semantic ""Linked Data"" approach that is scalable with respect to information volume and complexity. RegenBase will incorporate biomedical domain-level ontologies, including our recently developed BioAssay Ontology (BAO), to semantically associate related data types and to provide a knowledge context of the underlying experiments and screening outcomes. The overarching goal of this proposed RegenBase system is to allow bench scientists to link data and results from studies on nervous system injury and disease to data and knowledge from other domains with an emphasis on molecular targets and the small molecules that perturb their function to speed the development of novel therapeutics.          Public and private organizations are generating diverse data sets as they attempt to develop therapies for nervous system injury and disease. One reason therapy development is slow lies in the difficulty of collecting, analyzing and displaying information from the thousands of different experiments done on nervous system injury and interpreting it based on knowledge from other areas, such as genomics, cell biology, cancer, immunology and drug discovery. We propose to develop a novel information system that will help neuroscientists working on nerve regeneration to access and use information generated by scientists around the world.                ",Regenbase: A Searchable Database to Organize Regeneration Knowledge via Ontologie,8839677,R01NS080145,"['Afferent Neurons', 'Area', 'Axon', 'Back', 'Biological', 'Biological Assay', 'Brain', 'Brain Diseases', 'Brain Injuries', 'Budgets', 'Cancer Immunology Science', 'Cell Death', 'Cells', 'Cellular biology', 'Chemical Structure', 'Cicatrix', 'Complementary DNA', 'Computer software', 'Corticospinal Tracts', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dendrites', 'Development', 'Disease', 'Drug Compounding', 'Evaluation', 'Family', 'Funding', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genomics', 'Goals', 'Growth', 'Healthcare Systems', 'Human Genome Project', 'Individual', 'Informatics', 'Information Systems', 'Injury', 'Investigation', 'Knowledge', 'Link', 'Literature', 'Maps', 'Methodology', 'Molecular Bank', 'Molecular Target', 'Natural regeneration', 'Nerve', 'Nerve Regeneration', 'Nervous System Trauma', 'Nervous system structure', 'Neurons', 'Neurosciences', 'Online Systems', 'Ontology', 'Outcome', 'Paper', 'Pattern', 'Peripheral Nerves', 'PubChem', 'Resources', 'Scientist', 'Semantics', 'Societies', 'Solutions', 'Source', 'Speed', 'Spinal cord damage', 'Spinal cord injury', 'Stroke', 'Structure of rubrospinal tract', 'System', 'Technology', 'Terminology', 'Text', 'Translational Research', 'Traumatic Brain Injury', 'United States National Institutes of Health', 'Work', 'axon growth', 'axon regeneration', 'base', 'biomedical ontology', 'cell motility', 'computer based Semantic Analysis', 'data integration', 'data mining', 'drug discovery', 'graphical user interface', 'information display', 'information framework', 'injured', 'knockout gene', 'knowledge base', 'novel', 'novel therapeutics', 'overexpression', 'painful neuropathy', 'repaired', 'research study', 'screening', 'small molecule', 'success', 'text searching', 'therapy development', 'tool', 'vector']",NINDS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2015,570144,0.11807120525838259
"Regenbase: A Searchable Database to Organize Regeneration Knowledge via Ontologie     DESCRIPTION (provided by applicant): This application seeks funds to develop RegenBase - a novel information system to seamlessly integrate diverse data that are produced by neuroscientists and cell biologists studying nervous system injury, disease and cell motility with other resources, such as the Neuroscience Information Framework and the BioAssay Ontology. Over the past decade the NIH has funded the development of informatics tools and ontologies to allow the integration and interrogation of the massive and diverse data sets that have been produced by the human genome project. In the area of neuroscience the most advancement have been made in the creation and annotation of large anatomical data sets that reveal patterns of gene expression and connectivity. Genesat and the BrainMaps are excellent examples and are easily searched using the Neuroscience Information Framework (NIF) portal. But it is still surprisingly difficult to search for information related to repairing the injured nrvous system. To overcome this road block it is critical to build the essential tools that allow semantic web approaches to link diverse data repositories with ontologies that allow them to be interpreted and analyzed. The success of this initiative critically relies on an effective informatcs solution to integrate the various (current and future) data types generated by neuroscientists working on nervous system injury, as well as large-scale screening efforts (such as the Molecular Libraries Probe Center Network, MLPCN) into coherent data sets and to make them accessible, interpretable, and actionable for scientists of different backgrounds and with different objectives. We propose to develop a novel knowledge-based, extensible information system of interconnected components that leverages semantic-web technologies and domain level ontologies. This system is tentatively called RegenBase (Regeneration dataBase). Tremendous progress has been made during the last decade developing semantic web technologies with the goals of formalizing knowledge, linking information across different domains, and integrating large heterogeneous data sets from diverse sources. To develop RegenBase on a fast-track with limited resources, we will leverage technologies and tools from the National Center for Biological Onotology and the recently launched BioAssay Ontology. The long-term goal of the RegenBase system is seamless ""on-the fly"" data integration and analysis via a semantic ""Linked Data"" approach that is scalable with respect to information volume and complexity. RegenBase will incorporate biomedical domain-level ontologies, including our recently developed BioAssay Ontology (BAO), to semantically associate related data types and to provide a knowledge context of the underlying experiments and screening outcomes. The overarching goal of this proposed RegenBase system is to allow bench scientists to link data and results from studies on nervous system injury and disease to data and knowledge from other domains with an emphasis on molecular targets and the small molecules that perturb their function to speed the development of novel therapeutics.          Public and private organizations are generating diverse data sets as they attempt to develop therapies for nervous system injury and disease. One reason therapy development is slow lies in the difficulty of collecting, analyzing and displaying information from the thousands of different experiments done on nervous system injury and interpreting it based on knowledge from other areas, such as genomics, cell biology, cancer, immunology and drug discovery. We propose to develop a novel information system that will help neuroscientists working on nerve regeneration to access and use information generated by scientists around the world.                ",Regenbase: A Searchable Database to Organize Regeneration Knowledge via Ontologie,8653627,R01NS080145,"['Afferent Neurons', 'Area', 'Axon', 'Back', 'Biological', 'Biological Assay', 'Brain', 'Brain Diseases', 'Brain Injuries', 'Budgets', 'Cancer Immunology Science', 'Cell Death', 'Cells', 'Cellular biology', 'Chemical Structure', 'Cicatrix', 'Complementary DNA', 'Computer software', 'Corticospinal Tracts', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dendrites', 'Development', 'Disease', 'Drug Compounding', 'Evaluation', 'Family', 'Funding', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genomics', 'Goals', 'Growth', 'Healthcare Systems', 'Human Genome Project', 'Individual', 'Informatics', 'Information Systems', 'Injury', 'Investigation', 'Knowledge', 'Link', 'Literature', 'Maps', 'Methodology', 'Molecular Bank', 'Molecular Target', 'Natural regeneration', 'Nerve', 'Nerve Regeneration', 'Nervous System Trauma', 'Nervous system structure', 'Neurons', 'Neurosciences', 'Online Systems', 'Ontology', 'Outcome', 'Paper', 'Pattern', 'Peripheral Nerves', 'PubChem', 'Resources', 'Scientist', 'Semantics', 'Societies', 'Solutions', 'Source', 'Speed', 'Spinal cord damage', 'Spinal cord injury', 'Stroke', 'Structure of rubrospinal tract', 'System', 'Technology', 'Terminology', 'Text', 'Translational Research', 'Traumatic Brain Injury', 'United States National Institutes of Health', 'Work', 'axon growth', 'axon regeneration', 'base', 'biomedical ontology', 'cell motility', 'computer based Semantic Analysis', 'data integration', 'data mining', 'drug discovery', 'graphical user interface', 'information display', 'information framework', 'injured', 'knockout gene', 'knowledge base', 'novel', 'novel therapeutics', 'overexpression', 'painful neuropathy', 'repaired', 'research study', 'screening', 'small molecule', 'success', 'text searching', 'therapy development', 'tool', 'vector']",NINDS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2014,564445,0.11807120525838259
"Regenbase: A Searchable Database to Organize Regeneration Knowledge via Ontologie     DESCRIPTION (provided by applicant): This application seeks funds to develop RegenBase - a novel information system to seamlessly integrate diverse data that are produced by neuroscientists and cell biologists studying nervous system injury, disease and cell motility with other resources, such as the Neuroscience Information Framework and the BioAssay Ontology. Over the past decade the NIH has funded the development of informatics tools and ontologies to allow the integration and interrogation of the massive and diverse data sets that have been produced by the human genome project. In the area of neuroscience the most advancement have been made in the creation and annotation of large anatomical data sets that reveal patterns of gene expression and connectivity. Genesat and the BrainMaps are excellent examples and are easily searched using the Neuroscience Information Framework (NIF) portal. But it is still surprisingly difficult to search for information related to repairing the injured nrvous system. To overcome this road block it is critical to build the essential tools that allow semantic web approaches to link diverse data repositories with ontologies that allow them to be interpreted and analyzed. The success of this initiative critically relies on an effective informatcs solution to integrate the various (current and future) data types generated by neuroscientists working on nervous system injury, as well as large-scale screening efforts (such as the Molecular Libraries Probe Center Network, MLPCN) into coherent data sets and to make them accessible, interpretable, and actionable for scientists of different backgrounds and with different objectives. We propose to develop a novel knowledge-based, extensible information system of interconnected components that leverages semantic-web technologies and domain level ontologies. This system is tentatively called RegenBase (Regeneration dataBase). Tremendous progress has been made during the last decade developing semantic web technologies with the goals of formalizing knowledge, linking information across different domains, and integrating large heterogeneous data sets from diverse sources. To develop RegenBase on a fast-track with limited resources, we will leverage technologies and tools from the National Center for Biological Onotology and the recently launched BioAssay Ontology. The long-term goal of the RegenBase system is seamless ""on-the fly"" data integration and analysis via a semantic ""Linked Data"" approach that is scalable with respect to information volume and complexity. RegenBase will incorporate biomedical domain-level ontologies, including our recently developed BioAssay Ontology (BAO), to semantically associate related data types and to provide a knowledge context of the underlying experiments and screening outcomes. The overarching goal of this proposed RegenBase system is to allow bench scientists to link data and results from studies on nervous system injury and disease to data and knowledge from other domains with an emphasis on molecular targets and the small molecules that perturb their function to speed the development of novel therapeutics.          Public and private organizations are generating diverse data sets as they attempt to develop therapies for nervous system injury and disease. One reason therapy development is slow lies in the difficulty of collecting, analyzing and displaying information from the thousands of different experiments done on nervous system injury and interpreting it based on knowledge from other areas, such as genomics, cell biology, cancer, immunology and drug discovery. We propose to develop a novel information system that will help neuroscientists working on nerve regeneration to access and use information generated by scientists around the world.                ",Regenbase: A Searchable Database to Organize Regeneration Knowledge via Ontologie,8465934,R01NS080145,"['Afferent Neurons', 'Area', 'Axon', 'Back', 'Biological', 'Biological Assay', 'Brain', 'Brain Diseases', 'Brain Injuries', 'Budgets', 'Cancer Immunology Science', 'Cell Death', 'Cells', 'Cellular biology', 'Chemical Structure', 'Cicatrix', 'Complementary DNA', 'Computer software', 'Corticospinal Tracts', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dendrites', 'Development', 'Disease', 'Drug Compounding', 'Evaluation', 'Family', 'Funding', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genomics', 'Goals', 'Growth', 'Healthcare Systems', 'Human Genome Project', 'Individual', 'Informatics', 'Information Systems', 'Injury', 'Investigation', 'Knowledge', 'Link', 'Literature', 'Maps', 'Methodology', 'Molecular Bank', 'Molecular Target', 'Natural regeneration', 'Nerve', 'Nerve Regeneration', 'Nervous System Trauma', 'Nervous system structure', 'Neurons', 'Neurosciences', 'Online Systems', 'Ontology', 'Outcome', 'Paper', 'Pattern', 'Peripheral Nerves', 'PubChem', 'Resources', 'Scientist', 'Semantics', 'Societies', 'Solutions', 'Source', 'Speed', 'Spinal cord damage', 'Spinal cord injury', 'Stroke', 'Structure of rubrospinal tract', 'System', 'Technology', 'Terminology', 'Text', 'Translational Research', 'Traumatic Brain Injury', 'United States National Institutes of Health', 'Work', 'axon growth', 'axon regeneration', 'base', 'biomedical ontology', 'cell motility', 'computer based Semantic Analysis', 'data integration', 'data mining', 'drug discovery', 'graphical user interface', 'information display', 'information framework', 'injured', 'knockout gene', 'knowledge base', 'novel', 'novel therapeutics', 'overexpression', 'painful neuropathy', 'repaired', 'research study', 'screening', 'small molecule', 'success', 'text searching', 'therapy development', 'tool', 'vector']",NINDS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2013,550188,0.11807120525838259
"Regenbase: A Searchable Database to Organize Regeneration Knowledge via Ontologie     DESCRIPTION (provided by applicant): This application seeks funds to develop RegenBase - a novel information system to seamlessly integrate diverse data that are produced by neuroscientists and cell biologists studying nervous system injury, disease and cell motility with other resources, such as the Neuroscience Information Framework and the BioAssay Ontology. Over the past decade the NIH has funded the development of informatics tools and ontologies to allow the integration and interrogation of the massive and diverse data sets that have been produced by the human genome project. In the area of neuroscience the most advancement have been made in the creation and annotation of large anatomical data sets that reveal patterns of gene expression and connectivity. Genesat and the BrainMaps are excellent examples and are easily searched using the Neuroscience Information Framework (NIF) portal. But it is still surprisingly difficult to search for information related to repairing the injured nrvous system. To overcome this road block it is critical to build the essential tools that allow semantic web approaches to link diverse data repositories with ontologies that allow them to be interpreted and analyzed. The success of this initiative critically relies on an effective informatcs solution to integrate the various (current and future) data types generated by neuroscientists working on nervous system injury, as well as large-scale screening efforts (such as the Molecular Libraries Probe Center Network, MLPCN) into coherent data sets and to make them accessible, interpretable, and actionable for scientists of different backgrounds and with different objectives. We propose to develop a novel knowledge-based, extensible information system of interconnected components that leverages semantic-web technologies and domain level ontologies. This system is tentatively called RegenBase (Regeneration dataBase). Tremendous progress has been made during the last decade developing semantic web technologies with the goals of formalizing knowledge, linking information across different domains, and integrating large heterogeneous data sets from diverse sources. To develop RegenBase on a fast-track with limited resources, we will leverage technologies and tools from the National Center for Biological Onotology and the recently launched BioAssay Ontology. The long-term goal of the RegenBase system is seamless ""on-the fly"" data integration and analysis via a semantic ""Linked Data"" approach that is scalable with respect to information volume and complexity. RegenBase will incorporate biomedical domain-level ontologies, including our recently developed BioAssay Ontology (BAO), to semantically associate related data types and to provide a knowledge context of the underlying experiments and screening outcomes. The overarching goal of this proposed RegenBase system is to allow bench scientists to link data and results from studies on nervous system injury and disease to data and knowledge from other domains with an emphasis on molecular targets and the small molecules that perturb their function to speed the development of novel therapeutics.        PUBLIC HEALTH RELEVANCE: Public and private organizations are generating diverse data sets as they attempt to develop therapies for nervous system injury and disease. One reason therapy development is slow lies in the difficulty of collecting, analyzing and displaying information from the thousands of different experiments done on nervous system injury and interpreting it based on knowledge from other areas, such as genomics, cell biology, cancer, immunology and drug discovery. We propose to develop a novel information system that will help neuroscientists working on nerve regeneration to access and use information generated by scientists around the world.                  Public and private organizations are generating diverse data sets as they attempt to develop therapies for nervous system injury and disease. One reason therapy development is slow lies in the difficulty of collecting, analyzing and displaying information from the thousands of different experiments done on nervous system injury and interpreting it based on knowledge from other areas, such as genomics, cell biology, cancer, immunology and drug discovery. We propose to develop a novel information system that will help neuroscientists working on nerve regeneration to access and use information generated by scientists around the world.                ",Regenbase: A Searchable Database to Organize Regeneration Knowledge via Ontologie,8365739,R01NS080145,"['Afferent Neurons', 'Area', 'Axon', 'Back', 'Biological', 'Biological Assay', 'Brain', 'Brain Diseases', 'Brain Injuries', 'Budgets', 'Cancer Immunology Science', 'Cell Death', 'Cells', 'Cellular biology', 'Chemical Structure', 'Cicatrix', 'Complementary DNA', 'Computer software', 'Corticospinal Tracts', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dendrites', 'Development', 'Disease', 'Drug Compounding', 'Evaluation', 'Family', 'Funding', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genomics', 'Goals', 'Growth', 'Healthcare Systems', 'Human Genome Project', 'Individual', 'Informatics', 'Information Systems', 'Injury', 'Investigation', 'Knowledge', 'Link', 'Literature', 'Maps', 'Methodology', 'Molecular Bank', 'Molecular Target', 'Natural regeneration', 'Nerve', 'Nerve Regeneration', 'Nervous System Trauma', 'Nervous system structure', 'Neurons', 'Neurosciences', 'Online Systems', 'Ontology', 'Outcome', 'Paper', 'Pattern', 'Peripheral Nerves', 'PubChem', 'Resources', 'Scientist', 'Screening procedure', 'Semantics', 'Societies', 'Solutions', 'Source', 'Speed', 'Spinal cord damage', 'Spinal cord injury', 'Stroke', 'Structure of rubrospinal tract', 'System', 'Technology', 'Terminology', 'Text', 'Translational Research', 'Traumatic Brain Injury', 'United States National Institutes of Health', 'Work', 'axon growth', 'axon regeneration', 'base', 'biomedical ontology', 'cell motility', 'computer based Semantic Analysis', 'data integration', 'data mining', 'drug discovery', 'graphical user interface', 'information display', 'information framework', 'injured', 'knockout gene', 'knowledge base', 'novel', 'novel therapeutics', 'overexpression', 'painful neuropathy', 'repaired', 'research study', 'small molecule', 'success', 'text searching', 'therapy development', 'tool', 'vector']",NINDS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2012,603798,0.09697823689389493
"PRO: A Protein Ontology in OBO Foundry for Scalable Integration of Biomedical Knowledge ﻿    DESCRIPTION (provided by applicant): PROJECT SUMMARY Biomedical ontologies are critical to the accurate representation and integration of genome-scale data in biomedical and clinical research. The Protein Ontology (PRO)-the reference ontology for protein entities in the OBO (Open Biological and Biomedical Ontologies) Foundry-represents protein families, multiple protein forms (proteoforms) arising from single genes, and protein complexes. This competitive renewal grant application will further establish PRO as a scalable, flexible, collaborative research infrastructure for protein-centric semantic integration of biomedical data of increasing volume and complexity. Specific aims are to: (i) enable scalable and dynamic representation of protein types; (ii) provide comprehensive coverage of human proteoforms in their biological context; (iii) develop collaborative use cases and support an expanding community of users to advance protein-disease understanding; and (iv) broaden dissemination to support semantic computing, dynamic term mapping, and interoperability and reusability. We will increase PRO coverage by semi-automated import of proteoforms and complexes from curated databases and via established text mining approaches. Expert curation will focus on human variant forms, post-translational modification (PTM) forms and complexes critical to disease processes, along with homologous mouse forms. We will develop a new PRO sub-ontology of protein sites-amino acid positions of significance-to enable automatic and dynamic definition of combinatoric proteoforms. We will establish PRO OWL and RDF versions and provide a SPARQL query endpoint to support semantic computing. We will organize annual workshops and annotation jamborees to develop use cases and promote PRO co-development with community collaborators to address specific disciplinary needs. PRO has several unique features. For knowledge representation, PRO defines precise protein entities to support accurate annotation at the appropriate level of granularity and provides the ontological framework to connect PTM and variant proteoforms and complexes necessary to model human health and disease. For semantic data integration, PRO provides the ontological structure to connect-via specified relations-the vast amounts of proteomics data and biomedical knowledge to support hypothesis generation and testing. PRO therefore addresses the gaps in the bioinformatics infrastructure for protein representations in a way that makes knowledge about proteins more accessible to computational reasoning, fully complementing existing knowledge sources. The proposed research will allow the PRO Consortium to deepen and broaden PRO for scalable semantic integration of biomedical data, facilitating protein-disease knowledge discovery and clinical applications by an expanding community of biomedical, clinical and computational users. PUBLIC HEALTH RELEVANCE: PROJECT NARRATIVE The Protein Ontology (PRO) will allow researchers to capture and accurately represent scientific knowledge of proteins thus providing a research infrastructure for modeling biological systems and for protein-centric integration of existing and emerging experimental and clinical data. As a component of the global informatics resource network, the PRO resource will be instrumental in computational analysis and knowledge discovery of genome-scale data and will aid in improving our understanding of human disease, and in the identification of potential diagnostic and therapeutic targets.",PRO: A Protein Ontology in OBO Foundry for Scalable Integration of Biomedical Knowledge,9546737,R01GM080646,"['Address', 'Adopted', 'Amino Acid Sequence', 'Amino Acids', 'Applications Grants', 'Automobile Driving', 'Bioinformatics', 'Biological', 'Biological Models', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Research', 'Combinatorics', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Educational workshop', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Generations', 'Genetic Polymorphism', 'Health', 'Histones', 'Human', 'Imagery', 'Knowledge', 'Knowledge Discovery', 'Literature', 'Mus', 'Natural Language Processing', 'Ontology', 'Organism', 'Phenotype', 'Phosphotransferases', 'Play', 'Positioning Attribute', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Process', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Services', 'Signal Transduction', 'Site', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Testing', 'Training', 'Variant', 'base', 'biological systems', 'biomedical ontology', 'clinical application', 'computational reasoning', 'data integration', 'disease classification', 'flexibility', 'genome-wide', 'human disease', 'human model', 'improved', 'information organization', 'interoperability', 'knowledge base', 'protein complex', 'public health relevance', 'symposium', 'text searching', 'therapeutic target', 'usability', 'web portal', 'web services']",NIGMS,UNIVERSITY OF DELAWARE,R01,2018,688354,0.1417831473984112
"PRO: A Protein Ontology in OBO Foundry for Scalable Integration of Biomedical Knowledge ﻿    DESCRIPTION (provided by applicant): PROJECT SUMMARY Biomedical ontologies are critical to the accurate representation and integration of genome-scale data in biomedical and clinical research. The Protein Ontology (PRO)-the reference ontology for protein entities in the OBO (Open Biological and Biomedical Ontologies) Foundry-represents protein families, multiple protein forms (proteoforms) arising from single genes, and protein complexes. This competitive renewal grant application will further establish PRO as a scalable, flexible, collaborative research infrastructure for protein-centric semantic integration of biomedical data of increasing volume and complexity. Specific aims are to: (i) enable scalable and dynamic representation of protein types; (ii) provide comprehensive coverage of human proteoforms in their biological context; (iii) develop collaborative use cases and support an expanding community of users to advance protein-disease understanding; and (iv) broaden dissemination to support semantic computing, dynamic term mapping, and interoperability and reusability. We will increase PRO coverage by semi-automated import of proteoforms and complexes from curated databases and via established text mining approaches. Expert curation will focus on human variant forms, post-translational modification (PTM) forms and complexes critical to disease processes, along with homologous mouse forms. We will develop a new PRO sub-ontology of protein sites-amino acid positions of significance-to enable automatic and dynamic definition of combinatoric proteoforms. We will establish PRO OWL and RDF versions and provide a SPARQL query endpoint to support semantic computing. We will organize annual workshops and annotation jamborees to develop use cases and promote PRO co-development with community collaborators to address specific disciplinary needs. PRO has several unique features. For knowledge representation, PRO defines precise protein entities to support accurate annotation at the appropriate level of granularity and provides the ontological framework to connect PTM and variant proteoforms and complexes necessary to model human health and disease. For semantic data integration, PRO provides the ontological structure to connect-via specified relations-the vast amounts of proteomics data and biomedical knowledge to support hypothesis generation and testing. PRO therefore addresses the gaps in the bioinformatics infrastructure for protein representations in a way that makes knowledge about proteins more accessible to computational reasoning, fully complementing existing knowledge sources. The proposed research will allow the PRO Consortium to deepen and broaden PRO for scalable semantic integration of biomedical data, facilitating protein-disease knowledge discovery and clinical applications by an expanding community of biomedical, clinical and computational users. PUBLIC HEALTH RELEVANCE: PROJECT NARRATIVE The Protein Ontology (PRO) will allow researchers to capture and accurately represent scientific knowledge of proteins thus providing a research infrastructure for modeling biological systems and for protein-centric integration of existing and emerging experimental and clinical data. As a component of the global informatics resource network, the PRO resource will be instrumental in computational analysis and knowledge discovery of genome-scale data and will aid in improving our understanding of human disease, and in the identification of potential diagnostic and therapeutic targets.",PRO: A Protein Ontology in OBO Foundry for Scalable Integration of Biomedical Knowledge,9331689,R01GM080646,"['Address', 'Adopted', 'Amino Acid Sequence', 'Amino Acids', 'Applications Grants', 'Automobile Driving', 'Bioinformatics', 'Biological', 'Biological Models', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Research', 'Combinatorics', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Educational workshop', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Generations', 'Genetic Polymorphism', 'Health', 'Histones', 'Human', 'Imagery', 'Knowledge', 'Knowledge Discovery', 'Literature', 'Modeling', 'Mus', 'Natural Language Processing', 'Ontology', 'Organism', 'Phenotype', 'Phosphotransferases', 'Play', 'Positioning Attribute', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Process', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Services', 'Signal Transduction', 'Site', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Testing', 'Training', 'Variant', 'base', 'biological systems', 'biomedical ontology', 'clinical application', 'computational reasoning', 'data integration', 'disease classification', 'flexibility', 'genome-wide', 'human disease', 'improved', 'information organization', 'interoperability', 'knowledge base', 'protein complex', 'public health relevance', 'symposium', 'text searching', 'therapeutic target', 'usability', 'web portal', 'web services']",NIGMS,UNIVERSITY OF DELAWARE,R01,2017,708354,0.1417831473984112
"PRO: A Protein Ontology in OBO Foundry for Scalable Integration of Biomedical Knowledge ﻿    DESCRIPTION (provided by applicant): PROJECT SUMMARY Biomedical ontologies are critical to the accurate representation and integration of genome-scale data in biomedical and clinical research. The Protein Ontology (PRO)-the reference ontology for protein entities in the OBO (Open Biological and Biomedical Ontologies) Foundry-represents protein families, multiple protein forms (proteoforms) arising from single genes, and protein complexes. This competitive renewal grant application will further establish PRO as a scalable, flexible, collaborative research infrastructure for protein-centric semantic integration of biomedical data of increasing volume and complexity. Specific aims are to: (i) enable scalable and dynamic representation of protein types; (ii) provide comprehensive coverage of human proteoforms in their biological context; (iii) develop collaborative use cases and support an expanding community of users to advance protein-disease understanding; and (iv) broaden dissemination to support semantic computing, dynamic term mapping, and interoperability and reusability. We will increase PRO coverage by semi-automated import of proteoforms and complexes from curated databases and via established text mining approaches. Expert curation will focus on human variant forms, post-translational modification (PTM) forms and complexes critical to disease processes, along with homologous mouse forms. We will develop a new PRO sub-ontology of protein sites-amino acid positions of significance-to enable automatic and dynamic definition of combinatoric proteoforms. We will establish PRO OWL and RDF versions and provide a SPARQL query endpoint to support semantic computing. We will organize annual workshops and annotation jamborees to develop use cases and promote PRO co-development with community collaborators to address specific disciplinary needs. PRO has several unique features. For knowledge representation, PRO defines precise protein entities to support accurate annotation at the appropriate level of granularity and provides the ontological framework to connect PTM and variant proteoforms and complexes necessary to model human health and disease. For semantic data integration, PRO provides the ontological structure to connect-via specified relations-the vast amounts of proteomics data and biomedical knowledge to support hypothesis generation and testing. PRO therefore addresses the gaps in the bioinformatics infrastructure for protein representations in a way that makes knowledge about proteins more accessible to computational reasoning, fully complementing existing knowledge sources. The proposed research will allow the PRO Consortium to deepen and broaden PRO for scalable semantic integration of biomedical data, facilitating protein-disease knowledge discovery and clinical applications by an expanding community of biomedical, clinical and computational users. PUBLIC HEALTH RELEVANCE: PROJECT NARRATIVE The Protein Ontology (PRO) will allow researchers to capture and accurately represent scientific knowledge of proteins thus providing a research infrastructure for modeling biological systems and for protein-centric integration of existing and emerging experimental and clinical data. As a component of the global informatics resource network, the PRO resource will be instrumental in computational analysis and knowledge discovery of genome-scale data and will aid in improving our understanding of human disease, and in the identification of potential diagnostic and therapeutic targets.",PRO: A Protein Ontology in OBO Foundry for Scalable Integration of Biomedical Knowledge,9303592,R01GM080646,"['Address', 'Adopted', 'Amino Acid Sequence', 'Amino Acids', 'Applications Grants', 'Automobile Driving', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Research', 'Combinatorics', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Educational workshop', 'Fostering', 'Gene Proteins', 'Generations', 'Genetic Polymorphism', 'Health', 'Histones', 'Human', 'Imagery', 'Knowledge', 'Knowledge Discovery', 'Literature', 'Maps', 'Modeling', 'Mus', 'Natural Language Processing', 'Ontology', 'Organism', 'Peptide Sequence Determination', 'Phosphotransferases', 'Play', 'Positioning Attribute', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Process', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Services', 'Signal Transduction', 'Site', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Taxon', 'Testing', 'Training', 'Variant', 'base', 'biological systems', 'biomedical ontology', 'clinical application', 'computational reasoning', 'data integration', 'disease classification', 'disease phenotype', 'flexibility', 'genome-wide', 'human disease', 'improved', 'information organization', 'interoperability', 'knowledge base', 'protein complex', 'public health relevance', 'symposium', 'text searching', 'therapeutic target', 'usability', 'web portal', 'web services']",NIGMS,UNIVERSITY OF DELAWARE,R01,2016,20000,0.1417831473984112
"PRO: A Protein Ontology in OBO Foundry for Scalable Integration of Biomedical Knowledge ﻿    DESCRIPTION (provided by applicant): PROJECT SUMMARY Biomedical ontologies are critical to the accurate representation and integration of genome-scale data in biomedical and clinical research. The Protein Ontology (PRO)-the reference ontology for protein entities in the OBO (Open Biological and Biomedical Ontologies) Foundry-represents protein families, multiple protein forms (proteoforms) arising from single genes, and protein complexes. This competitive renewal grant application will further establish PRO as a scalable, flexible, collaborative research infrastructure for protein-centric semantic integration of biomedical data of increasing volume and complexity. Specific aims are to: (i) enable scalable and dynamic representation of protein types; (ii) provide comprehensive coverage of human proteoforms in their biological context; (iii) develop collaborative use cases and support an expanding community of users to advance protein-disease understanding; and (iv) broaden dissemination to support semantic computing, dynamic term mapping, and interoperability and reusability. We will increase PRO coverage by semi-automated import of proteoforms and complexes from curated databases and via established text mining approaches. Expert curation will focus on human variant forms, post-translational modification (PTM) forms and complexes critical to disease processes, along with homologous mouse forms. We will develop a new PRO sub-ontology of protein sites-amino acid positions of significance-to enable automatic and dynamic definition of combinatoric proteoforms. We will establish PRO OWL and RDF versions and provide a SPARQL query endpoint to support semantic computing. We will organize annual workshops and annotation jamborees to develop use cases and promote PRO co-development with community collaborators to address specific disciplinary needs. PRO has several unique features. For knowledge representation, PRO defines precise protein entities to support accurate annotation at the appropriate level of granularity and provides the ontological framework to connect PTM and variant proteoforms and complexes necessary to model human health and disease. For semantic data integration, PRO provides the ontological structure to connect-via specified relations-the vast amounts of proteomics data and biomedical knowledge to support hypothesis generation and testing. PRO therefore addresses the gaps in the bioinformatics infrastructure for protein representations in a way that makes knowledge about proteins more accessible to computational reasoning, fully complementing existing knowledge sources. The proposed research will allow the PRO Consortium to deepen and broaden PRO for scalable semantic integration of biomedical data, facilitating protein-disease knowledge discovery and clinical applications by an expanding community of biomedical, clinical and computational users. PUBLIC HEALTH RELEVANCE: PROJECT NARRATIVE The Protein Ontology (PRO) will allow researchers to capture and accurately represent scientific knowledge of proteins thus providing a research infrastructure for modeling biological systems and for protein-centric integration of existing and emerging experimental and clinical data. As a component of the global informatics resource network, the PRO resource will be instrumental in computational analysis and knowledge discovery of genome-scale data and will aid in improving our understanding of human disease, and in the identification of potential diagnostic and therapeutic targets.",PRO: A Protein Ontology in OBO Foundry for Scalable Integration of Biomedical Knowledge,9120920,R01GM080646,"['Address', 'Adopted', 'Amino Acid Sequence', 'Amino Acids', 'Applications Grants', 'Automobile Driving', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Research', 'Combinatorics', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Educational workshop', 'Fostering', 'Gene Proteins', 'Generations', 'Genetic Polymorphism', 'Health', 'Histones', 'Human', 'Imagery', 'Knowledge', 'Knowledge Discovery', 'Literature', 'Maps', 'Modeling', 'Mus', 'Natural Language Processing', 'Ontology', 'Organism', 'Peptide Sequence Determination', 'Phosphotransferases', 'Play', 'Positioning Attribute', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Process', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Services', 'Signal Transduction', 'Site', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Taxon', 'Testing', 'Training', 'Variant', 'base', 'biological systems', 'biomedical ontology', 'clinical application', 'computational reasoning', 'data integration', 'disease classification', 'disease phenotype', 'flexibility', 'genome-wide', 'human disease', 'improved', 'information organization', 'interoperability', 'knowledge base', 'protein complex', 'public health relevance', 'symposium', 'text searching', 'therapeutic target', 'usability', 'web portal', 'web services']",NIGMS,UNIVERSITY OF DELAWARE,R01,2016,688354,0.1417831473984112
"PRO: A Protein Ontology in OBO Foundry for Scalable Integration of Biomedical Knowledge ﻿    DESCRIPTION (provided by applicant): PROJECT SUMMARY Biomedical ontologies are critical to the accurate representation and integration of genome-scale data in biomedical and clinical research. The Protein Ontology (PRO)-the reference ontology for protein entities in the OBO (Open Biological and Biomedical Ontologies) Foundry-represents protein families, multiple protein forms (proteoforms) arising from single genes, and protein complexes. This competitive renewal grant application will further establish PRO as a scalable, flexible, collaborative research infrastructure for protein-centric semantic integration of biomedical data of increasing volume and complexity. Specific aims are to: (i) enable scalable and dynamic representation of protein types; (ii) provide comprehensive coverage of human proteoforms in their biological context; (iii) develop collaborative use cases and support an expanding community of users to advance protein-disease understanding; and (iv) broaden dissemination to support semantic computing, dynamic term mapping, and interoperability and reusability. We will increase PRO coverage by semi-automated import of proteoforms and complexes from curated databases and via established text mining approaches. Expert curation will focus on human variant forms, post-translational modification (PTM) forms and complexes critical to disease processes, along with homologous mouse forms. We will develop a new PRO sub-ontology of protein sites-amino acid positions of significance-to enable automatic and dynamic definition of combinatoric proteoforms. We will establish PRO OWL and RDF versions and provide a SPARQL query endpoint to support semantic computing. We will organize annual workshops and annotation jamborees to develop use cases and promote PRO co-development with community collaborators to address specific disciplinary needs. PRO has several unique features. For knowledge representation, PRO defines precise protein entities to support accurate annotation at the appropriate level of granularity and provides the ontological framework to connect PTM and variant proteoforms and complexes necessary to model human health and disease. For semantic data integration, PRO provides the ontological structure to connect-via specified relations-the vast amounts of proteomics data and biomedical knowledge to support hypothesis generation and testing. PRO therefore addresses the gaps in the bioinformatics infrastructure for protein representations in a way that makes knowledge about proteins more accessible to computational reasoning, fully complementing existing knowledge sources. The proposed research will allow the PRO Consortium to deepen and broaden PRO for scalable semantic integration of biomedical data, facilitating protein-disease knowledge discovery and clinical applications by an expanding community of biomedical, clinical and computational users.         PUBLIC HEALTH RELEVANCE: PROJECT NARRATIVE The Protein Ontology (PRO) will allow researchers to capture and accurately represent scientific knowledge of proteins thus providing a research infrastructure for modeling biological systems and for protein-centric integration of existing and emerging experimental and clinical data. As a component of the global informatics resource network, the PRO resource will be instrumental in computational analysis and knowledge discovery of genome-scale data and will aid in improving our understanding of human disease, and in the identification of potential diagnostic and therapeutic targets.            ",PRO: A Protein Ontology in OBO Foundry for Scalable Integration of Biomedical Knowledge,8964875,R01GM080646,"['Address', 'Adopted', 'Amino Acid Sequence', 'Amino Acids', 'Applications Grants', 'Automobile Driving', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Research', 'Combinatorics', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Educational workshop', 'Fostering', 'Gene Proteins', 'Generations', 'Genetic Polymorphism', 'Health', 'Histones', 'Human', 'Imagery', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Literature', 'Maps', 'Modeling', 'Mus', 'Natural Language Processing', 'Ontology', 'Organism', 'Peptide Sequence Determination', 'Phosphotransferases', 'Play', 'Positioning Attribute', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Process', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Informatics', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Services', 'Signal Transduction', 'Site', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Taxon', 'Testing', 'Training', 'Variant', 'base', 'biological systems', 'biomedical ontology', 'clinical application', 'data integration', 'disease classification', 'disease phenotype', 'flexibility', 'genome-wide', 'human disease', 'improved', 'information organization', 'interoperability', 'knowledge base', 'protein complex', 'public health relevance', 'symposium', 'text searching', 'therapeutic target', 'usability', 'web services']",NIGMS,UNIVERSITY OF DELAWARE,R01,2015,745136,0.1417831473984112
"PRO: A Protein Ontology in OBO Foundry for Integration of Biomedical Knowledge    DESCRIPTION (provided by applicant): Biomedical ontologies are critical tools for the accurate representation and integration of genome-scale data in biomedical and translational research. The OBO (Open Biological and Biomedical Ontologies) Foundry is a community effort to develop a systematic and coordinated framework for evidence-based ontology development on the basis of an evolving set of best practice principles. The Protein Ontology (PRO) is the reference ontology for proteins within the OBO Foundry, and is, with the Gene Ontology, one of the first six ontologies recommended by the Foundry as preferred targets for community convergence. To provide a basic ontological framework to capture protein knowledge in a systems biology context, PRO encompasses three sub-ontologies to represent (1) proteins from homologous genes based on evolutionary relatedness (ProEvo); (2) protein forms produced from a given gene, including splice isoforms, mutation variants, and co- or post-translationally modified forms (ProForm); and (3) protein-containing complexes (ProComp).  This competitive renewal grant application aims to further develop PRO in order to facilitate its semantic and computational use by the biomedical research community and thereby broaden its scientific impact for discovery and reasoning in the health sciences. The specific aims are: (i) to enhance the PRO ontological framework; (ii) to broaden the coverage of protein objects; (iii) to enhance the PRO curation platform, website and visual representation; (iv) to develop driving clinical projects; and (v) to expand the scientific impact, adoption and dissemination of PRO. The ontological framework will capture new types of protein objects and relations and connect to semantic resources and reasoning tools. PRO will broaden coverage through mappings and definitions of relations to connect protein objects in existing knowledge bases, and via semi-automated import of protein forms and complexes from curated databases. A graphical network representation will seamlessly connect protein forms and complexes across tax in biological context for disease modeling. Use cases and two specific Driving Clinical Projects-one for reasoning and hypothesis generation for Alzheimer's disease, and one for flow cytometry data representation and immune system modeling-will demonstrate knowledge integration in the OBO Foundry framework as an enabling research infrastructure for reasoning and modeling in the health sciences. We will host annual PRO Scientific Dissemination Meetings addressing the protein-related needs of the bio- and clinical informatics research communities. PRO will be disseminated via multiple websites and ontological services, as well as through reciprocal links with major knowledge resources. ID management for protein objects will include mapping of PRO terms to common database identifiers with well-defined relations and expedited creation of requested PRO terms and UIDs.  The PRO research has several unique features and its significance is multi-fold. For knowledge representation, PRO defines precise protein objects to support accurate annotation at the appropriate granularity and provides the ontological framework to connect all protein types necessary to model biology, in particular linking specific protein forms to particular complexes in biological context. For semantic data integration, PRO provides the ontological structure to connect-via specified relations- the vast amounts of protein knowledge contained in databases to support new hypothesis generation and testing. PRO therefore addresses the current gaps in the bioinformatics infrastructure for protein representations in a way that makes knowledge about proteins more accessible to computational reasoning, fully leveraging and complementing existing knowledge sources. The proposed research will allow the PRO Consortium to bring together the resources and expertise from several collaborating institutions to deepen and broaden PRO as a mature research infrastructure for biomedical knowledge discovery and translational science.        The PRO ontology will allow researchers to capture and accurately represent scientific knowledge of proteins, providing a research infrastructure for modeling biological systems, improving the understanding of human disease, and aiding in the identification of potential diagnostic and therapeutic targets.         ",PRO: A Protein Ontology in OBO Foundry for Integration of Biomedical Knowledge,8700422,R01GM080646,"['Address', 'Adoption', 'Alzheimer&apos', 's Disease', 'Applications Grants', 'Area', 'Automobile Driving', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Biology', 'Biomedical Research', 'Buffaloes', 'Cells', 'Clinical', 'Clinical Informatics', 'Collaborations', 'Communities', 'Community Outreach', 'Complement', 'Complex', 'Data', 'Databases', 'Delaware', 'Development', 'Diagnostic', 'Disease model', 'Educational workshop', 'Ensure', 'Flow Cytometry', 'Generations', 'Genes', 'Genome', 'Goals', 'Grant', 'Health Sciences', 'Homologous Gene', 'Human', 'Immune system', 'Informatics', 'Information Resources', 'Institution', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Link', 'Maps', 'Modeling', 'Molecular', 'Mus', 'Mutation', 'New York', 'Object Relations', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Protein Import', 'Protein Isoforms', 'Proteins', 'RNA Splicing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'Systems Biology', 'Taxes', 'Taxon', 'Testing', 'Training', 'Translational Research', 'Universities', 'Variant', 'Visual', 'Work', 'base', 'biological systems', 'biomedical ontology', 'data integration', 'data mining', 'drug discovery', 'evidence base', 'human disease', 'improved', 'information organization', 'knowledge base', 'meetings', 'mouse genome', 'protein complex', 'text searching', 'therapeutic target', 'tool', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,R01,2014,874165,0.14614914608152493
"PRO: A Protein Ontology in OBO Foundry for Integration of Biomedical Knowledge    DESCRIPTION (provided by applicant): Biomedical ontologies are critical tools for the accurate representation and integration of genome-scale data in biomedical and translational research. The OBO (Open Biological and Biomedical Ontologies) Foundry is a community effort to develop a systematic and coordinated framework for evidence-based ontology development on the basis of an evolving set of best practice principles. The Protein Ontology (PRO) is the reference ontology for proteins within the OBO Foundry, and is, with the Gene Ontology, one of the first six ontologies recommended by the Foundry as preferred targets for community convergence. To provide a basic ontological framework to capture protein knowledge in a systems biology context, PRO encompasses three sub-ontologies to represent (1) proteins from homologous genes based on evolutionary relatedness (ProEvo); (2) protein forms produced from a given gene, including splice isoforms, mutation variants, and co- or post-translationally modified forms (ProForm); and (3) protein-containing complexes (ProComp).  This competitive renewal grant application aims to further develop PRO in order to facilitate its semantic and computational use by the biomedical research community and thereby broaden its scientific impact for discovery and reasoning in the health sciences. The specific aims are: (i) to enhance the PRO ontological framework; (ii) to broaden the coverage of protein objects; (iii) to enhance the PRO curation platform, website and visual representation; (iv) to develop driving clinical projects; and (v) to expand the scientific impact, adoption and dissemination of PRO. The ontological framework will capture new types of protein objects and relations and connect to semantic resources and reasoning tools. PRO will broaden coverage through mappings and definitions of relations to connect protein objects in existing knowledge bases, and via semi-automated import of protein forms and complexes from curated databases. A graphical network representation will seamlessly connect protein forms and complexes across tax in biological context for disease modeling. Use cases and two specific Driving Clinical Projects-one for reasoning and hypothesis generation for Alzheimer's disease, and one for flow cytometry data representation and immune system modeling-will demonstrate knowledge integration in the OBO Foundry framework as an enabling research infrastructure for reasoning and modeling in the health sciences. We will host annual PRO Scientific Dissemination Meetings addressing the protein-related needs of the bio- and clinical informatics research communities. PRO will be disseminated via multiple websites and ontological services, as well as through reciprocal links with major knowledge resources. ID management for protein objects will include mapping of PRO terms to common database identifiers with well-defined relations and expedited creation of requested PRO terms and UIDs.  The PRO research has several unique features and its significance is multi-fold. For knowledge representation, PRO defines precise protein objects to support accurate annotation at the appropriate granularity and provides the ontological framework to connect all protein types necessary to model biology, in particular linking specific protein forms to particular complexes in biological context. For semantic data integration, PRO provides the ontological structure to connect-via specified relations- the vast amounts of protein knowledge contained in databases to support new hypothesis generation and testing. PRO therefore addresses the current gaps in the bioinformatics infrastructure for protein representations in a way that makes knowledge about proteins more accessible to computational reasoning, fully leveraging and complementing existing knowledge sources. The proposed research will allow the PRO Consortium to bring together the resources and expertise from several collaborating institutions to deepen and broaden PRO as a mature research infrastructure for biomedical knowledge discovery and translational science.        The PRO ontology will allow researchers to capture and accurately represent scientific knowledge of proteins, providing a research infrastructure for modeling biological systems, improving the understanding of human disease, and aiding in the identification of potential diagnostic and therapeutic targets.         ",PRO: A Protein Ontology in OBO Foundry for Integration of Biomedical Knowledge,8516051,R01GM080646,"['Address', 'Adoption', 'Alzheimer&apos', 's Disease', 'Applications Grants', 'Area', 'Automobile Driving', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Biology', 'Biomedical Research', 'Buffaloes', 'Cells', 'Clinical', 'Clinical Informatics', 'Collaborations', 'Communities', 'Community Outreach', 'Complement', 'Complex', 'Data', 'Databases', 'Delaware', 'Development', 'Diagnostic', 'Disease model', 'Educational workshop', 'Ensure', 'Flow Cytometry', 'Generations', 'Genes', 'Genome', 'Goals', 'Grant', 'Health Sciences', 'Homologous Gene', 'Human', 'Immune system', 'Informatics', 'Information Resources', 'Institution', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Link', 'Maps', 'Modeling', 'Molecular', 'Mus', 'Mutation', 'New York', 'Object Relations', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Protein Import', 'Protein Isoforms', 'Proteins', 'RNA Splicing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'Systems Biology', 'Taxes', 'Taxon', 'Testing', 'Training', 'Translational Research', 'Universities', 'Variant', 'Visual', 'Work', 'base', 'biological systems', 'biomedical ontology', 'data integration', 'data mining', 'drug discovery', 'evidence base', 'human disease', 'improved', 'information organization', 'knowledge base', 'meetings', 'mouse genome', 'protein complex', 'text searching', 'therapeutic target', 'tool', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,R01,2013,843962,0.14614914608152493
"PRO: A Protein Ontology in OBO Foundry for Integration of Biomedical Knowledge    DESCRIPTION (provided by applicant): Biomedical ontologies are critical tools for the accurate representation and integration of genome-scale data in biomedical and translational research. The OBO (Open Biological and Biomedical Ontologies) Foundry is a community effort to develop a systematic and coordinated framework for evidence-based ontology development on the basis of an evolving set of best practice principles. The Protein Ontology (PRO) is the reference ontology for proteins within the OBO Foundry, and is, with the Gene Ontology, one of the first six ontologies recommended by the Foundry as preferred targets for community convergence. To provide a basic ontological framework to capture protein knowledge in a systems biology context, PRO encompasses three sub-ontologies to represent (1) proteins from homologous genes based on evolutionary relatedness (ProEvo); (2) protein forms produced from a given gene, including splice isoforms, mutation variants, and co- or post-translationally modified forms (ProForm); and (3) protein-containing complexes (ProComp).  This competitive renewal grant application aims to further develop PRO in order to facilitate its semantic and computational use by the biomedical research community and thereby broaden its scientific impact for discovery and reasoning in the health sciences. The specific aims are: (i) to enhance the PRO ontological framework; (ii) to broaden the coverage of protein objects; (iii) to enhance the PRO curation platform, website and visual representation; (iv) to develop driving clinical projects; and (v) to expand the scientific impact, adoption and dissemination of PRO. The ontological framework will capture new types of protein objects and relations and connect to semantic resources and reasoning tools. PRO will broaden coverage through mappings and definitions of relations to connect protein objects in existing knowledge bases, and via semi-automated import of protein forms and complexes from curated databases. A graphical network representation will seamlessly connect protein forms and complexes across tax in biological context for disease modeling. Use cases and two specific Driving Clinical Projects-one for reasoning and hypothesis generation for Alzheimer's disease, and one for flow cytometry data representation and immune system modeling-will demonstrate knowledge integration in the OBO Foundry framework as an enabling research infrastructure for reasoning and modeling in the health sciences. We will host annual PRO Scientific Dissemination Meetings addressing the protein-related needs of the bio- and clinical informatics research communities. PRO will be disseminated via multiple websites and ontological services, as well as through reciprocal links with major knowledge resources. ID management for protein objects will include mapping of PRO terms to common database identifiers with well-defined relations and expedited creation of requested PRO terms and UIDs.  The PRO research has several unique features and its significance is multi-fold. For knowledge representation, PRO defines precise protein objects to support accurate annotation at the appropriate granularity and provides the ontological framework to connect all protein types necessary to model biology, in particular linking specific protein forms to particular complexes in biological context. For semantic data integration, PRO provides the ontological structure to connect-via specified relations- the vast amounts of protein knowledge contained in databases to support new hypothesis generation and testing. PRO therefore addresses the current gaps in the bioinformatics infrastructure for protein representations in a way that makes knowledge about proteins more accessible to computational reasoning, fully leveraging and complementing existing knowledge sources. The proposed research will allow the PRO Consortium to bring together the resources and expertise from several collaborating institutions to deepen and broaden PRO as a mature research infrastructure for biomedical knowledge discovery and translational science.        The PRO ontology will allow researchers to capture and accurately represent scientific knowledge of proteins, providing a research infrastructure for modeling biological systems, improving the understanding of human disease, and aiding in the identification of potential diagnostic and therapeutic targets.         ",PRO: A Protein Ontology in OBO Foundry for Integration of Biomedical Knowledge,8336867,R01GM080646,"['Address', 'Adoption', 'Alzheimer&apos', 's Disease', 'Applications Grants', 'Area', 'Automobile Driving', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Biology', 'Biomedical Research', 'Buffaloes', 'Cells', 'Clinical', 'Clinical Informatics', 'Collaborations', 'Communities', 'Community Outreach', 'Complement', 'Complex', 'Data', 'Databases', 'Delaware', 'Development', 'Diagnostic', 'Disease model', 'Educational workshop', 'Ensure', 'Flow Cytometry', 'Generations', 'Genes', 'Genome', 'Goals', 'Grant', 'Health Sciences', 'Homologous Gene', 'Human', 'Immune system', 'Informatics', 'Information Resources', 'Institution', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Link', 'Maps', 'Modeling', 'Molecular', 'Mus', 'Mutation', 'New York', 'Object Relations', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Protein Import', 'Protein Isoforms', 'Proteins', 'RNA Splicing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'Systems Biology', 'Taxes', 'Taxon', 'Testing', 'Training', 'Translational Research', 'Universities', 'Variant', 'Visual', 'Work', 'base', 'biological systems', 'biomedical ontology', 'data integration', 'data mining', 'drug discovery', 'evidence base', 'human disease', 'improved', 'information organization', 'knowledge base', 'meetings', 'mouse genome', 'protein complex', 'text searching', 'therapeutic target', 'tool', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,R01,2012,785773,0.14614914608152493
"PRO: A Protein Ontology in OBO Foundry for Integration of Biomedical Knowledge    DESCRIPTION (provided by applicant): Biomedical ontologies are critical tools for the accurate representation and integration of genome-scale data in biomedical and translational research. The OBO (Open Biological and Biomedical Ontologies) Foundry is a community effort to develop a systematic and coordinated framework for evidence-based ontology development on the basis of an evolving set of best practice principles. The Protein Ontology (PRO) is the reference ontology for proteins within the OBO Foundry, and is, with the Gene Ontology, one of the first six ontologies recommended by the Foundry as preferred targets for community convergence. To provide a basic ontological framework to capture protein knowledge in a systems biology context, PRO encompasses three sub-ontologies to represent (1) proteins from homologous genes based on evolutionary relatedness (ProEvo); (2) protein forms produced from a given gene, including splice isoforms, mutation variants, and co- or post-translationally modified forms (ProForm); and (3) protein-containing complexes (ProComp).  This competitive renewal grant application aims to further develop PRO in order to facilitate its semantic and computational use by the biomedical research community and thereby broaden its scientific impact for discovery and reasoning in the health sciences. The specific aims are: (i) to enhance the PRO ontological framework; (ii) to broaden the coverage of protein objects; (iii) to enhance the PRO curation platform, website and visual representation; (iv) to develop driving clinical projects; and (v) to expand the scientific impact, adoption and dissemination of PRO. The ontological framework will capture new types of protein objects and relations and connect to semantic resources and reasoning tools. PRO will broaden coverage through mappings and definitions of relations to connect protein objects in existing knowledge bases, and via semi-automated import of protein forms and complexes from curated databases. A graphical network representation will seamlessly connect protein forms and complexes across tax in biological context for disease modeling. Use cases and two specific Driving Clinical Projects-one for reasoning and hypothesis generation for Alzheimer's disease, and one for flow cytometry data representation and immune system modeling-will demonstrate knowledge integration in the OBO Foundry framework as an enabling research infrastructure for reasoning and modeling in the health sciences. We will host annual PRO Scientific Dissemination Meetings addressing the protein-related needs of the bio- and clinical informatics research communities. PRO will be disseminated via multiple websites and ontological services, as well as through reciprocal links with major knowledge resources. ID management for protein objects will include mapping of PRO terms to common database identifiers with well-defined relations and expedited creation of requested PRO terms and UIDs.  The PRO research has several unique features and its significance is multi-fold. For knowledge representation, PRO defines precise protein objects to support accurate annotation at the appropriate granularity and provides the ontological framework to connect all protein types necessary to model biology, in particular linking specific protein forms to particular complexes in biological context. For semantic data integration, PRO provides the ontological structure to connect-via specified relations- the vast amounts of protein knowledge contained in databases to support new hypothesis generation and testing. PRO therefore addresses the current gaps in the bioinformatics infrastructure for protein representations in a way that makes knowledge about proteins more accessible to computational reasoning, fully leveraging and complementing existing knowledge sources. The proposed research will allow the PRO Consortium to bring together the resources and expertise from several collaborating institutions to deepen and broaden PRO as a mature research infrastructure for biomedical knowledge discovery and translational science.        The PRO ontology will allow researchers to capture and accurately represent scientific knowledge of proteins, providing a research infrastructure for modeling biological systems, improving the understanding of human disease, and aiding in the identification of potential diagnostic and therapeutic targets.         ",PRO: A Protein Ontology in OBO Foundry for Integration of Biomedical Knowledge,8521853,R01GM080646,"['Address', 'Adoption', 'Alzheimer&apos', 's Disease', 'Applications Grants', 'Area', 'Automobile Driving', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Biology', 'Biomedical Research', 'Buffaloes', 'Cells', 'Clinical', 'Clinical Informatics', 'Collaborations', 'Communities', 'Community Outreach', 'Complement', 'Complex', 'Data', 'Databases', 'Delaware', 'Development', 'Diagnostic', 'Disease model', 'Educational workshop', 'Ensure', 'Flow Cytometry', 'Generations', 'Genes', 'Genome', 'Goals', 'Grant', 'Health Sciences', 'Homologous Gene', 'Human', 'Immune system', 'Informatics', 'Information Resources', 'Institution', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Link', 'Maps', 'Modeling', 'Molecular', 'Mus', 'Mutation', 'New York', 'Object Relations', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Protein Import', 'Protein Isoforms', 'Proteins', 'RNA Splicing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'Systems Biology', 'Taxes', 'Taxon', 'Testing', 'Training', 'Translational Research', 'Universities', 'Variant', 'Visual', 'Work', 'base', 'biological systems', 'biomedical ontology', 'data integration', 'data mining', 'drug discovery', 'evidence base', 'human disease', 'improved', 'information organization', 'knowledge base', 'meetings', 'mouse genome', 'protein complex', 'text searching', 'therapeutic target', 'tool', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,R01,2012,82863,0.14614914608152493
"PRO: A Protein Ontology in OBO Foundry for Integration of Biomedical Knowledge    DESCRIPTION (provided by applicant): Biomedical ontologies are critical tools for the accurate representation and integration of genome-scale data in biomedical and translational research. The OBO (Open Biological and Biomedical Ontologies) Foundry is a community effort to develop a systematic and coordinated framework for evidence-based ontology development on the basis of an evolving set of best practice principles. The Protein Ontology (PRO) is the reference ontology for proteins within the OBO Foundry, and is, with the Gene Ontology, one of the first six ontologies recommended by the Foundry as preferred targets for community convergence. To provide a basic ontological framework to capture protein knowledge in a systems biology context, PRO encompasses three sub-ontologies to represent (1) proteins from homologous genes based on evolutionary relatedness (ProEvo); (2) protein forms produced from a given gene, including splice isoforms, mutation variants, and co- or post-translationally modified forms (ProForm); and (3) protein-containing complexes (ProComp).  This competitive renewal grant application aims to further develop PRO in order to facilitate its semantic and computational use by the biomedical research community and thereby broaden its scientific impact for discovery and reasoning in the health sciences. The specific aims are: (i) to enhance the PRO ontological framework; (ii) to broaden the coverage of protein objects; (iii) to enhance the PRO curation platform, website and visual representation; (iv) to develop driving clinical projects; and (v) to expand the scientific impact, adoption and dissemination of PRO. The ontological framework will capture new types of protein objects and relations and connect to semantic resources and reasoning tools. PRO will broaden coverage through mappings and definitions of relations to connect protein objects in existing knowledge bases, and via semi-automated import of protein forms and complexes from curated databases. A graphical network representation will seamlessly connect protein forms and complexes across tax in biological context for disease modeling. Use cases and two specific Driving Clinical Projects-one for reasoning and hypothesis generation for Alzheimer's disease, and one for flow cytometry data representation and immune system modeling-will demonstrate knowledge integration in the OBO Foundry framework as an enabling research infrastructure for reasoning and modeling in the health sciences. We will host annual PRO Scientific Dissemination Meetings addressing the protein-related needs of the bio- and clinical informatics research communities. PRO will be disseminated via multiple websites and ontological services, as well as through reciprocal links with major knowledge resources. ID management for protein objects will include mapping of PRO terms to common database identifiers with well-defined relations and expedited creation of requested PRO terms and UIDs.  The PRO research has several unique features and its significance is multi-fold. For knowledge representation, PRO defines precise protein objects to support accurate annotation at the appropriate granularity and provides the ontological framework to connect all protein types necessary to model biology, in particular linking specific protein forms to particular complexes in biological context. For semantic data integration, PRO provides the ontological structure to connect-via specified relations- the vast amounts of protein knowledge contained in databases to support new hypothesis generation and testing. PRO therefore addresses the current gaps in the bioinformatics infrastructure for protein representations in a way that makes knowledge about proteins more accessible to computational reasoning, fully leveraging and complementing existing knowledge sources. The proposed research will allow the PRO Consortium to bring together the resources and expertise from several collaborating institutions to deepen and broaden PRO as a mature research infrastructure for biomedical knowledge discovery and translational science.      PUBLIC HEALTH RELEVANCE: The PRO ontology will allow researchers to capture and accurately represent scientific knowledge of proteins, providing a research infrastructure for modeling biological systems, improving the understanding of human disease, and aiding in the identification of potential diagnostic and therapeutic targets.           The PRO ontology will allow researchers to capture and accurately represent scientific knowledge of proteins, providing a research infrastructure for modeling biological systems, improving the understanding of human disease, and aiding in the identification of potential diagnostic and therapeutic targets.         ",PRO: A Protein Ontology in OBO Foundry for Integration of Biomedical Knowledge,8187900,R01GM080646,"['Address', 'Adoption', 'Alzheimer&apos', 's Disease', 'Applications Grants', 'Area', 'Automobile Driving', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Biology', 'Biomedical Research', 'Buffaloes', 'Cells', 'Clinical', 'Clinical Informatics', 'Collaborations', 'Communities', 'Community Outreach', 'Complement', 'Complex', 'Data', 'Databases', 'Delaware', 'Development', 'Diagnostic', 'Disease model', 'Educational workshop', 'Ensure', 'Flow Cytometry', 'Generations', 'Genes', 'Genome', 'Goals', 'Grant', 'Health Sciences', 'Homologous Gene', 'Human', 'Immune system', 'Informatics', 'Information Resources', 'Institution', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Link', 'Maps', 'Modeling', 'Molecular', 'Mus', 'Mutation', 'New York', 'Object Relations', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Protein Import', 'Protein Isoforms', 'Proteins', 'RNA Splicing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'Systems Biology', 'Taxes', 'Taxon', 'Testing', 'Training', 'Translational Research', 'Universities', 'Variant', 'Visual', 'Work', 'base', 'biological systems', 'biomedical ontology', 'data integration', 'data mining', 'drug discovery', 'evidence base', 'human disease', 'improved', 'information organization', 'knowledge base', 'meetings', 'mouse genome', 'protein complex', 'text searching', 'therapeutic target', 'tool', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,R01,2011,846522,0.1486441400157675
"Ontology-based Information Network to Support Vaccine Research  Project Summary (Abstract):  Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.  Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,8311060,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'abstracting', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2012,264994,0.16607423127818066
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,8120230,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2011,264994,0.16607423127818066
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,7935464,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2010,267671,0.16607423127818066
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,7735790,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2009,270375,0.16607423127818066
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7928868,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'biomedical ontology', 'computer science', 'computerized tools', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'natural language', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2010,605009,0.19798824431736242
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7684604,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Body of uterus', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'biomedical ontology', 'computer science', 'computerized tools', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'natural language', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2009,639134,0.19798824431736242
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7502636,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Body of uterus', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Numbers', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'computer science', 'computerized tools', 'concept', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2008,640921,0.19798824431736242
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7364235,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Body of uterus', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Numbers', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'computer science', 'computerized tools', 'concept', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2007,631600,0.19798824431736242
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7426246,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Depth', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Facility Construction Funding Category', 'Funding', 'GDF15 gene', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Numbers', 'Ontology', 'PLAB Protein', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2008,266213,0.05881735808427412
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,8055527,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Traumatic Stress Disorders', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,248610,0.05881735808427412
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7799875,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,250650,0.05881735808427412
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7591237,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,272818,0.05881735808427412
"Modeling the Effect of Drugs in Intergomics by Linking Drug Ontology and Pathways    DESCRIPTION (provided by applicant): High throughput experiments are now frequently part of clinical trials and many institutions establish prospective biospecimen collections from routine patient populations in order to run proteomics and metabolomics experiments. Because these samples are collected from routine clinical populations with co-morbidities and on drug therapies, it is necessary to account for the drugs in the data analysis in a systematic manner. Increasingly multiple high-throughput technologies, such as proteomics and metabolomics, are being used together which requires comprehensive pathway networks that make changes in the metabolome traceable to the proteome and in turn to the gene expression profile. In this study we propose to link drug therapies found in the clinical data with biologic pathways data used for the integrated analysis of high-throughput experimental results. Specifically we will integrate formal knowledge of biological pathways with drug knowledge found in the emergent national standard for the comprehensive knowledge representation for medicinal products, including the Veteran Administrations National Drug File Reference Terminology (NDF-RT), the National Drug Codes and the RxNorm clinical drug vocabulary. The hub of these terminologies is the Structured Product Labeling (SPL) standard for drug knowledge representation. SPL is part of the HL7 version 3 standards and based on the HL7 Reference Information Model (RIM). All U.S. pharmaceutical manufacturers today submit SPL data to the Food and Drug Agency (FDA). Pharmacodynamic knowledge is represented in SPL as a using the NDF-RT mechanism of action (MoA) classes. In this project we will expand these MoA classes with their ontological definitions, and thus link to the biologic pathways described in various pathway network resources including KEGG, Reactome, and the NCI/Nature Protein Interaction Database (PID), all of which use different formats and models. Our methodology for integration consists of (1) transforming the original pathway resource into a common data schema, and (2) purpose-driven reconciliation of overlapping content, by (3) connecting the MoA classes. Because no single pathway data schema exists and because SPL is already the hub of the national federated drug terminology, we propose to integrate the pathway data into the drug knowledge base itself. The resulting integrated data will be evaluated against the frequency of drugs encountered in clinical care and research. This work will yield revisions of existing ontologies and, only where necessary, new ontologies to describe drug- pathway interactions. The project will demonstrate how the HL7/ISO Reference Information Model can represent biological entities and processes taking a realist perspective, and thus set an important precedence for cross-domain data integration between basic sciences and clinical medicine that is essential for the translational research agenda. PUBLIC HEALTH RELEVANCE: The project will combine the national drug catalog with models of the biochemical regulation and metabolism of body functions. This will allow researchers to understand better the effect which drugs have on the results of laboratory tests which measure a large number of proteins and chemicals in the body at the same time.          n/a",Modeling the Effect of Drugs in Intergomics by Linking Drug Ontology and Pathways,7558891,R01GM086302,"['Accounting', 'Affect', 'Autacoids', 'Back', 'Basic Science', 'Biochemical Pathway', 'Biological', 'Caring', 'Chemicals', 'Class', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Trials', 'Code', 'Collection', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Data Sources', 'Databases', 'Drug Catalogs', 'Drug Delivery Systems', 'Drug Industry', 'Drug Interactions', 'Drug Labeling', 'Evaluation', 'Food', 'Frequencies', 'Gene Expression Profile', 'Health Services Research', 'Human', 'Hypersensitivity', 'Industry', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Manuals', 'Manufacturer Name', 'Maps', 'MeSH Thesaurus', 'Measures', 'Metabolism', 'Methodology', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Numbers', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacodynamics', 'Pharmacologic Substance', 'Pharmacotherapy', 'Population', 'Population Research', 'Process', 'Product Labeling', 'Proteins', 'Proteome', 'Proteomics', 'Public Health', 'Purpose', 'Regulation', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resources', 'Running', 'Safety', 'Sampling', 'Source', 'Standards of Weights and Measures', 'Structure', 'System', 'Terminology', 'Testing', 'Time', 'Today', 'Translational Research', 'United States Department of Veterans Affairs', 'Vocabulary', 'Work', 'base', 'biochemical model', 'data integration', 'data modeling', 'drug standard', 'feeding', 'high throughput analysis', 'high throughput technology', 'information organization', 'interest', 'knowledge base', 'metabolomics', 'numb protein', 'prospective', 'research study']",NIGMS,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2008,244127,0.06276277036395402
"Modeling the Effect of Drugs in Intergomics by Linking Drug Ontology and Pathways    DESCRIPTION (provided by applicant): High throughput experiments are now frequently part of clinical trials and many institutions establish prospective biospecimen collections from routine patient populations in order to run proteomics and metabolomics experiments. Because these samples are collected from routine clinical populations with co-morbidities and on drug therapies, it is necessary to account for the drugs in the data analysis in a systematic manner. Increasingly multiple high-throughput technologies, such as proteomics and metabolomics, are being used together which requires comprehensive pathway networks that make changes in the metabolome traceable to the proteome and in turn to the gene expression profile. In this study we propose to link drug therapies found in the clinical data with biologic pathways data used for the integrated analysis of high-throughput experimental results. Specifically we will integrate formal knowledge of biological pathways with drug knowledge found in the emergent national standard for the comprehensive knowledge representation for medicinal products, including the Veteran Administrations National Drug File Reference Terminology (NDF-RT), the National Drug Codes and the RxNorm clinical drug vocabulary. The hub of these terminologies is the Structured Product Labeling (SPL) standard for drug knowledge representation. SPL is part of the HL7 version 3 standards and based on the HL7 Reference Information Model (RIM). All U.S. pharmaceutical manufacturers today submit SPL data to the Food and Drug Agency (FDA). Pharmacodynamic knowledge is represented in SPL as a using the NDF-RT mechanism of action (MoA) classes. In this project we will expand these MoA classes with their ontological definitions, and thus link to the biologic pathways described in various pathway network resources including KEGG, Reactome, and the NCI/Nature Protein Interaction Database (PID), all of which use different formats and models. Our methodology for integration consists of (1) transforming the original pathway resource into a common data schema, and (2) purpose-driven reconciliation of overlapping content, by (3) connecting the MoA classes. Because no single pathway data schema exists and because SPL is already the hub of the national federated drug terminology, we propose to integrate the pathway data into the drug knowledge base itself. The resulting integrated data will be evaluated against the frequency of drugs encountered in clinical care and research. This work will yield revisions of existing ontologies and, only where necessary, new ontologies to describe drug- pathway interactions. The project will demonstrate how the HL7/ISO Reference Information Model can represent biological entities and processes taking a realist perspective, and thus set an important precedence for cross-domain data integration between basic sciences and clinical medicine that is essential for the translational research agenda. PUBLIC HEALTH RELEVANCE: The project will combine the national drug catalog with models of the biochemical regulation and metabolism of body functions. This will allow researchers to understand better the effect which drugs have on the results of laboratory tests which measure a large number of proteins and chemicals in the body at the same time.          n/a",Modeling the Effect of Drugs in Intergomics by Linking Drug Ontology and Pathways,7692176,R01GM086302,"['Accounting', 'Affect', 'Autacoids', 'Back', 'Basic Science', 'Biochemical Pathway', 'Biological', 'Chemicals', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Trials', 'Code', 'Collection', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Drug Catalogs', 'Drug Delivery Systems', 'Drug Industry', 'Drug Interactions', 'Drug Labeling', 'Evaluation', 'Food', 'Frequencies', 'Gene Expression Profile', 'Health Services Research', 'Human', 'Hypersensitivity', 'Industry', 'Information Systems', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Manuals', 'Manufacturer Name', 'Maps', 'MeSH Thesaurus', 'Measures', 'Metabolism', 'Methodology', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Ontology', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacodynamics', 'Pharmacologic Substance', 'Pharmacotherapy', 'Population', 'Population Research', 'Process', 'Product Labeling', 'Proteins', 'Proteome', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resources', 'Running', 'Safety', 'Sampling', 'Source', 'Structure', 'System', 'Terminology', 'Testing', 'Time', 'Translational Research', 'United States Department of Veterans Affairs', 'Vocabulary', 'Work', 'base', 'biochemical model', 'clinical care', 'data integration', 'data modeling', 'drug standard', 'feeding', 'high throughput analysis', 'high throughput technology', 'information organization', 'interest', 'knowledge base', 'metabolomics', 'numb protein', 'patient population', 'prospective', 'public health relevance', 'research study']",NIGMS,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2009,234119,0.06276277036395402
"Modeling the Effect of Drugs in Intergomics by Linking Drug Ontology and Pathways    DESCRIPTION (provided by applicant): High throughput experiments are now frequently part of clinical trials and many institutions establish prospective biospecimen collections from routine patient populations in order to run proteomics and metabolomics experiments. Because these samples are collected from routine clinical populations with co-morbidities and on drug therapies, it is necessary to account for the drugs in the data analysis in a systematic manner. Increasingly multiple high-throughput technologies, such as proteomics and metabolomics, are being used together which requires comprehensive pathway networks that make changes in the metabolome traceable to the proteome and in turn to the gene expression profile. In this study we propose to link drug therapies found in the clinical data with biologic pathways data used for the integrated analysis of high-throughput experimental results. Specifically we will integrate formal knowledge of biological pathways with drug knowledge found in the emergent national standard for the comprehensive knowledge representation for medicinal products, including the Veteran Administrations National Drug File Reference Terminology (NDF-RT), the National Drug Codes and the RxNorm clinical drug vocabulary. The hub of these terminologies is the Structured Product Labeling (SPL) standard for drug knowledge representation. SPL is part of the HL7 version 3 standards and based on the HL7 Reference Information Model (RIM). All U.S. pharmaceutical manufacturers today submit SPL data to the Food and Drug Agency (FDA). Pharmacodynamic knowledge is represented in SPL as a using the NDF-RT mechanism of action (MoA) classes. In this project we will expand these MoA classes with their ontological definitions, and thus link to the biologic pathways described in various pathway network resources including KEGG, Reactome, and the NCI/Nature Protein Interaction Database (PID), all of which use different formats and models. Our methodology for integration consists of (1) transforming the original pathway resource into a common data schema, and (2) purpose-driven reconciliation of overlapping content, by (3) connecting the MoA classes. Because no single pathway data schema exists and because SPL is already the hub of the national federated drug terminology, we propose to integrate the pathway data into the drug knowledge base itself. The resulting integrated data will be evaluated against the frequency of drugs encountered in clinical care and research. This work will yield revisions of existing ontologies and, only where necessary, new ontologies to describe drug- pathway interactions. The project will demonstrate how the HL7/ISO Reference Information Model can represent biological entities and processes taking a realist perspective, and thus set an important precedence for cross-domain data integration between basic sciences and clinical medicine that is essential for the translational research agenda. PUBLIC HEALTH RELEVANCE: The project will combine the national drug catalog with models of the biochemical regulation and metabolism of body functions. This will allow researchers to understand better the effect which drugs have on the results of laboratory tests which measure a large number of proteins and chemicals in the body at the same time.          n/a",Modeling the Effect of Drugs in Intergomics by Linking Drug Ontology and Pathways,7924784,R01GM086302,"['Accounting', 'Affect', 'Autacoids', 'Back', 'Basic Science', 'Biochemical Pathway', 'Biological', 'Chemicals', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Trials', 'Code', 'Collection', 'Comorbidity', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Drug Catalogs', 'Drug Delivery Systems', 'Drug Industry', 'Drug Interactions', 'Drug Labeling', 'Evaluation', 'Food', 'Frequencies', 'Gene Expression Profile', 'Health Services Research', 'Human', 'Hypersensitivity', 'Industry', 'Information Systems', 'Institution', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Link', 'Manuals', 'Manufacturer Name', 'Maps', 'MeSH Thesaurus', 'Measures', 'Metabolism', 'Methodology', 'Modeling', 'Nature', 'Ontology', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacodynamics', 'Pharmacologic Substance', 'Pharmacotherapy', 'Population', 'Population Research', 'Process', 'Product Labeling', 'Proteins', 'Proteome', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resources', 'Running', 'Safety', 'Sampling', 'Source', 'Structure', 'System', 'Terminology', 'Testing', 'Time', 'Translational Research', 'United States Department of Veterans Affairs', 'Vocabulary', 'Work', 'base', 'biochemical model', 'clinical care', 'data integration', 'data modeling', 'drug standard', 'feeding', 'high throughput analysis', 'high throughput technology', 'information organization', 'interest', 'knowledge base', 'metabolomics', 'numb protein', 'patient population', 'prospective', 'public health relevance', 'research study']",NIGMS,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R01,2010,232566,0.06276277036395402
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,7565504,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computer Systems Development', 'Computers', 'Computing Methodologies', 'Conflict (Psychology)', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Life', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Outsourcing', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Source', 'Staging', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'public health relevance', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2009,529858,0.17682053522949953
"Collaborative Development of Biomedical Ontologies and Terminologies DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient. PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.",Collaborative Development of Biomedical Ontologies and Terminologies,8997510,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Craniofacial Abnormalities', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Health', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial development', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'new technology', 'novel', 'open source', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2016,510376,0.18264730672220647
"Collaborative Development of Biomedical Ontologies and Terminologies DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient. PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.",Collaborative Development of Biomedical Ontologies and Terminologies,8803385,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Craniofacial Abnormalities', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Health', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial development', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'new technology', 'novel', 'open source', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2015,523965,0.18264730672220647
"Collaborative Development of Biomedical Ontologies and Terminologies     DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient.         PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.            ",Collaborative Development of Biomedical Ontologies and Terminologies,8628132,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'malformation', 'new technology', 'novel', 'open source', 'public health relevance', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2014,525880,0.18264730672220647
"Collaborative Development of Biomedical Ontologies and Terminologies     DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient.         PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.            ",Collaborative Development of Biomedical Ontologies and Terminologies,8504843,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'Future', 'Generations', 'Genes', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'malformation', 'new technology', 'novel', 'open source', 'public health relevance', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2013,527736,0.18264730672220647
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,8242742,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Health', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Systems Development', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2012,392767,0.17682053522949953
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,8039246,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Health', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Systems Development', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2011,526649,0.17682053522949953
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,7774343,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computer Systems Development', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Life', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Outsourcing', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'public health relevance', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2010,525262,0.17682053522949953
"Gene Wiki: expanding the ecosystem of community-intelligence resources DESCRIPTION (provided by applicant): This proposal describes the continued maintenance and development of the Gene Wiki, the goal of which is to create a continuously-updated, community-reviewed, and collaboratively-written review article for every human gene. The Gene Wiki was created directly within Wikipedia as an informal collection of 10,646 gene-specific articles. In the first funding period, the infrastructure to keep Gene Wiki ""infoboxes"" in sync wit the source databases in the genomics community was developed. Next, methods to assess and quantify the trustworthiness of each word of a Gene Wiki article were developed and implemented in a system called WikiTrust. And finally, simple text-mining applied to Gene Wiki was able to identify thousands of novel gene annotations. During the next project period, the Gene Wiki is poised to make further strides. First, the scope of the Gene Wiki will be expanded to also include review articles on diseases and drugs. Thousands of articles will either be created or maintained through this initiative with a particular emphasis on rare diseases. Second, a dedicated outreach component will ensure that the community of editors is poised to grow. This outreach effort will engage both faculty members who are experts on specific genes of interest, as well as classroom instructors at all levels who want to design curriculum based on the Gene Wiki for a class project. Third, a ""Centralized Model Organism Database"" will be constructed in the Wikidata environment, which will serve as a clearing house of microbial gene and genome annotation data. And fourth an entirely new crowdsourcing application will be created that taps into crowds of patient-aligned individuals and their desire to advance research. These individuals will be a novel crowd that will be applied to the challenge of systematically annotating the biomedical literature. In summary, the Gene Wiki is a useful tool for biomedical research. Successful completion of this proposal will result in more efficient knowledge management and dissemination through crowdsourcing. PUBLIC HEALTH RELEVANCE: The Gene Wiki is a highly used resource for understanding gene function. The goal of this project is to create a collaboratively-written, community-reviewed, and continuously-updated review article for every human gene, drug, and disease.",Gene Wiki: expanding the ecosystem of community-intelligence resources,9265870,R01GM089820,"['Address', 'Biological', 'Biomedical Research', 'Collection', 'Communities', 'Crowding', 'Data', 'Data Display', 'Databases', 'Development', 'Disease', 'Ecosystem', 'Educational Curriculum', 'Ensure', 'Environment', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Harvest', 'Human', 'Incentives', 'Individual', 'Information Resources Management', 'Intelligence', 'Internet', 'Journals', 'Knowledge', 'Learning', 'Life', 'Link', 'Literature', 'Maintenance', 'Methods', 'Microbiology', 'Mining', 'Modeling', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Population', 'Progress Reports', 'Property', 'Publications', 'Rare Diseases', 'Reader', 'Research', 'Research Infrastructure', 'Resources', 'Solid', 'Source', 'Students', 'System', 'Text', 'Time', 'Trees', 'Update', 'Wit', 'Work', 'base', 'crowdsourcing', 'database structure', 'design', 'file format', 'gene function', 'genetic resource', 'genome annotation', 'genome browser', 'genome sequencing', 'genomic data', 'improved', 'instructor', 'interest', 'member', 'microbial', 'model organisms databases', 'novel', 'outreach', 'public health relevance', 'text searching', 'tool', 'web interface', 'whole genome', 'wiki']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2017,491537,0.06520874482526312
"Gene Wiki: expanding the ecosystem of community-intelligence resources DESCRIPTION (provided by applicant): This proposal describes the continued maintenance and development of the Gene Wiki, the goal of which is to create a continuously-updated, community-reviewed, and collaboratively-written review article for every human gene. The Gene Wiki was created directly within Wikipedia as an informal collection of 10,646 gene-specific articles. In the first funding period, the infrastructure to keep Gene Wiki ""infoboxes"" in sync wit the source databases in the genomics community was developed. Next, methods to assess and quantify the trustworthiness of each word of a Gene Wiki article were developed and implemented in a system called WikiTrust. And finally, simple text-mining applied to Gene Wiki was able to identify thousands of novel gene annotations. During the next project period, the Gene Wiki is poised to make further strides. First, the scope of the Gene Wiki will be expanded to also include review articles on diseases and drugs. Thousands of articles will either be created or maintained through this initiative with a particular emphasis on rare diseases. Second, a dedicated outreach component will ensure that the community of editors is poised to grow. This outreach effort will engage both faculty members who are experts on specific genes of interest, as well as classroom instructors at all levels who want to design curriculum based on the Gene Wiki for a class project. Third, a ""Centralized Model Organism Database"" will be constructed in the Wikidata environment, which will serve as a clearing house of microbial gene and genome annotation data. And fourth an entirely new crowdsourcing application will be created that taps into crowds of patient-aligned individuals and their desire to advance research. These individuals will be a novel crowd that will be applied to the challenge of systematically annotating the biomedical literature. In summary, the Gene Wiki is a useful tool for biomedical research. Successful completion of this proposal will result in more efficient knowledge management and dissemination through crowdsourcing. PUBLIC HEALTH RELEVANCE: The Gene Wiki is a highly used resource for understanding gene function. The goal of this project is to create a collaboratively-written, community-reviewed, and continuously-updated review article for every human gene, drug, and disease.",Gene Wiki: expanding the ecosystem of community-intelligence resources,9063441,R01GM089820,"['Address', 'Biological', 'Biomedical Research', 'Collection', 'Communities', 'Crowding', 'Data', 'Data Display', 'Databases', 'Development', 'Disease', 'Ecosystem', 'Educational Curriculum', 'Ensure', 'Environment', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'Genes', 'Genomics', 'Goals', 'Growth', 'Harvest', 'Health', 'Housing', 'Human', 'Incentives', 'Individual', 'Information Resources Management', 'Intelligence', 'Internet', 'Journals', 'Knowledge', 'Learning', 'Life', 'Link', 'Literature', 'Maintenance', 'Methods', 'Microbiology', 'Mining', 'Modeling', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Population', 'Progress Reports', 'Property', 'Publications', 'Rare Diseases', 'Reader', 'Research', 'Research Infrastructure', 'Resources', 'Solid', 'Source', 'Students', 'System', 'Text', 'Time', 'Trees', 'Update', 'Wit', 'Work', 'Writing', 'base', 'crowdsourcing', 'database structure', 'design', 'file format', 'gene function', 'genetic resource', 'genome annotation', 'genome browser', 'genome sequencing', 'genomic data', 'improved', 'instructor', 'interest', 'member', 'microbial', 'model organisms databases', 'novel', 'outreach', 'text searching', 'tool', 'web interface', 'whole genome', 'wiki']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2016,491537,0.06520874482526312
"Gene Wiki: expanding the ecosystem of community-intelligence resources DESCRIPTION (provided by applicant): This proposal describes the continued maintenance and development of the Gene Wiki, the goal of which is to create a continuously-updated, community-reviewed, and collaboratively-written review article for every human gene. The Gene Wiki was created directly within Wikipedia as an informal collection of 10,646 gene-specific articles. In the first funding period, the infrastructure to keep Gene Wiki ""infoboxes"" in sync wit the source databases in the genomics community was developed. Next, methods to assess and quantify the trustworthiness of each word of a Gene Wiki article were developed and implemented in a system called WikiTrust. And finally, simple text-mining applied to Gene Wiki was able to identify thousands of novel gene annotations. During the next project period, the Gene Wiki is poised to make further strides. First, the scope of the Gene Wiki will be expanded to also include review articles on diseases and drugs. Thousands of articles will either be created or maintained through this initiative with a particular emphasis on rare diseases. Second, a dedicated outreach component will ensure that the community of editors is poised to grow. This outreach effort will engage both faculty members who are experts on specific genes of interest, as well as classroom instructors at all levels who want to design curriculum based on the Gene Wiki for a class project. Third, a ""Centralized Model Organism Database"" will be constructed in the Wikidata environment, which will serve as a clearing house of microbial gene and genome annotation data. And fourth an entirely new crowdsourcing application will be created that taps into crowds of patient-aligned individuals and their desire to advance research. These individuals will be a novel crowd that will be applied to the challenge of systematically annotating the biomedical literature. In summary, the Gene Wiki is a useful tool for biomedical research. Successful completion of this proposal will result in more efficient knowledge management and dissemination through crowdsourcing. PUBLIC HEALTH RELEVANCE: The Gene Wiki is a highly used resource for understanding gene function. The goal of this project is to create a collaboratively-written, community-reviewed, and continuously-updated review article for every human gene, drug, and disease.",Gene Wiki: expanding the ecosystem of community-intelligence resources,8891444,R01GM089820,"['Address', 'Biological', 'Biomedical Research', 'Collection', 'Communities', 'Crowding', 'Data', 'Data Display', 'Databases', 'Development', 'Disease', 'Ecosystem', 'Educational Curriculum', 'Ensure', 'Environment', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Harvest', 'Health', 'Housing', 'Human', 'Incentives', 'Individual', 'Information Resources Management', 'Intelligence', 'Internet', 'Journals', 'Knowledge', 'Learning', 'Life', 'Link', 'Literature', 'Maintenance', 'Methods', 'Microbiology', 'Mining', 'Modeling', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Population', 'Progress Reports', 'Property', 'Publications', 'Rare Diseases', 'Reader', 'Research', 'Research Infrastructure', 'Resources', 'Solid', 'Source', 'Students', 'System', 'Text', 'Time', 'Trees', 'Update', 'Wit', 'Work', 'Writing', 'base', 'database structure', 'design', 'file format', 'gene function', 'genetic resource', 'genome annotation', 'genome sequencing', 'improved', 'instructor', 'interest', 'member', 'microbial', 'model organisms databases', 'novel', 'outreach', 'text searching', 'tool', 'web interface', 'wiki']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2015,485310,0.06520874482526312
"Gene Wiki: expanding the ecosystem of community-intelligence resources DESCRIPTION (provided by applicant): This proposal describes the continued maintenance and development of the Gene Wiki, the goal of which is to create a continuously-updated, community-reviewed, and collaboratively-written review article for every human gene. The Gene Wiki was created directly within Wikipedia as an informal collection of 10,646 gene-specific articles. In the first funding period, the infrastructure to keep Gene Wiki ""infoboxes"" in sync wit the source databases in the genomics community was developed. Next, methods to assess and quantify the trustworthiness of each word of a Gene Wiki article were developed and implemented in a system called WikiTrust. And finally, simple text-mining applied to Gene Wiki was able to identify thousands of novel gene annotations. During the next project period, the Gene Wiki is poised to make further strides. First, the scope of the Gene Wiki will be expanded to also include review articles on diseases and drugs. Thousands of articles will either be created or maintained through this initiative with a particular emphasis on rare diseases. Second, a dedicated outreach component will ensure that the community of editors is poised to grow. This outreach effort will engage both faculty members who are experts on specific genes of interest, as well as classroom instructors at all levels who want to design curriculum based on the Gene Wiki for a class project. Third, a ""Centralized Model Organism Database"" will be constructed in the Wikidata environment, which will serve as a clearing house of microbial gene and genome annotation data. And fourth an entirely new crowdsourcing application will be created that taps into crowds of patient-aligned individuals and their desire to advance research. These individuals will be a novel crowd that will be applied to the challenge of systematically annotating the biomedical literature. In summary, the Gene Wiki is a useful tool for biomedical research. Successful completion of this proposal will result in more efficient knowledge management and dissemination through crowdsourcing. PUBLIC HEALTH RELEVANCE: The Gene Wiki is a highly used resource for understanding gene function. The goal of this project is to create a collaboratively-written, community-reviewed, and continuously-updated review article for every human gene, drug, and disease.",Gene Wiki: expanding the ecosystem of community-intelligence resources,8696695,R01GM089820,"['Address', 'Biological', 'Biomedical Research', 'Collection', 'Communities', 'Crowding', 'Data', 'Data Display', 'Databases', 'Development', 'Disease', 'Ecosystem', 'Educational Curriculum', 'Ensure', 'Environment', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Harvest', 'Health', 'Housing', 'Human', 'Incentives', 'Individual', 'Information Resources Management', 'Intelligence', 'Internet', 'Journals', 'Knowledge', 'Learning', 'Life', 'Link', 'Literature', 'Maintenance', 'Methods', 'Microbiology', 'Mining', 'Modeling', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Population', 'Progress Reports', 'Property', 'Publications', 'Rare Diseases', 'Reader', 'Research', 'Research Infrastructure', 'Resources', 'Solid', 'Source', 'Students', 'System', 'Text', 'Time', 'Trees', 'Update', 'Wit', 'Work', 'Writing', 'base', 'database structure', 'design', 'file format', 'gene function', 'genetic resource', 'genome annotation', 'genome sequencing', 'improved', 'instructor', 'interest', 'member', 'microbial', 'microbial alkaline proteinase inhibitor', 'model organisms databases', 'novel', 'outreach', 'text searching', 'tool', 'web interface', 'wiki']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2014,413289,0.06520874482526312
"A multidisciplinary approach to elucidating gene function in a model Gram-positiv    DESCRIPTION (provided by applicant): Because of the diversity of niches they inhabit, their versatility, and the threat their members pose to human health, Gram-positive bacteria are of paramount importance to multiple facets of microbiology. Nonetheless, there has been no systematic effort to elucidate gene functions and relationships on a genome-wide scale in any Gram-positive bacterium. Bacillus subtilis is the premier model organism for studies of Gram-positive bacteria because it offers powerful classical and molecular genetics, high-resolution cell biology, as well as a large and vibrant community of researchers. Evidence from other model organisms demonstrates that genome-wide approaches accelerate functional discovery. The opportunity for GO funding has catalyzed a broad-based community of researchers to pool their expertise to create a focused, coordinated, multidisciplinary effort to develop the genomic resources necessary to implement a comprehensive gene function effort. This initiative will serve as the blueprint for similar efforts in other model and pathogenic bacteria in the future. We will: 1. Construct genome-scale tools for global phenotypic analysis. These will include two bar-coded null mutant libraries, an ordered over-expression library, 300 promoter fusions to yfp and a C-terminal epitope- tagged library for all essential genes. 2. Develop and provide proof-of-principle experiments for key phenotyping methods. These will include high-throughput genetic interaction analysis and chemical genetic profiling; global cytological phenotyping; and transcriptional profiling. 3. Integrate and analyze the data across species. Each global phenotyping dataset will be analyzed in collaboration with bioinformaticists. This analysis will generate probabilistic functional interaction maps and transcriptional networks that will provide key information about gene functions and relationships. These will be used for comparative analysis among Gram-positive organisms and between Gram-positives and Gram- negatives. 4. Develop a B. subtilis database (subtilisHUB). We will build a web-based data resource to support ongoing functional annotation. This site will document the construction of all the genomic tools, house protocols and datasets, and provide infrastructure to enable Gene Ontology assignments, literature mining, and community discussion. 5. Engage the community of microbiologists in the genomic resources and approaches. A broad community of research specialists will launch genome-scale projects taking advantage of the tools, methodologies, and resources.      PUBLIC HEALTH RELEVANCE: This proposal is aimed at elucidating the function of all the genes in Bacillus subtilis, the major model organism for an important class of bacteria, Gram-positive bacteria. The project will generate genome-scale tools; pioneer the development of global phenotypic analysis for bacteria; and establish a web resource that will ensure continuity beyond the funding term. The concerted, community-based effort we propose will have far-reaching impact on research in bacteria in general, and on many Gram-positive pathogens.              Project Narrative This proposal is aimed at elucidating the function of all the genes in Bacillus subtilis, the major model organism for an important class of bacteria, Gram-positive bacteria. The project will generate genome-scale tools; pioneer the development of global phenotypic analysis for bacteria; and establish a web resource that will ensure continuity beyond the funding term. The concerted, community-based effort we propose will have far-reaching impact on research in bacteria in general, and on many Gram-positive pathogens.",A multidisciplinary approach to elucidating gene function in a model Gram-positiv,7942971,RC2GM092616,"['Animal Model', 'Antibiotic Resistance', 'Antibiotics', 'Bacillus subtilis', 'Bacteria', 'Bar Codes', 'Bioinformatics', 'C-terminal', 'Cell Size', 'Cell division', 'Cellular biology', 'Chemicals', 'Chromosomes', 'Collaborations', 'Collection', 'Communities', 'Competence', 'DNA biosynthesis', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Ensure', 'Epitopes', 'Escherichia coli', 'Essential Genes', 'Experimental Designs', 'Expression Library', 'Fluorescence Microscopy', 'Funding', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Gram-Positive Bacteria', 'Growth', 'Health', 'Housing', 'Human', 'Institutes', 'Internet', 'Lead', 'Letters', 'Libraries', 'Literature', 'Maps', 'Measures', 'Metabolism', 'Methodology', 'Methods', 'Microbial Biofilms', 'Microbial Genetics', 'Microbiology', 'Modeling', 'Molecular', 'Molecular Genetics', 'Molecular Profiling', 'Noise', 'Online Systems', 'Ontology', 'Organism', 'Participant', 'Pathogenicity', 'Pathway interactions', 'Phenotype', 'Protocols documentation', 'Publishing', 'RNA Degradation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resources', 'Site', 'Specialist', 'Texas', 'abstracting', 'base', 'cell motility', 'chemical genetics', 'comparative', 'detection of nutrient', 'drug sensitivity', 'experience', 'functional genomics', 'gene function', 'genetic profiling', 'genome database', 'genome-wide', 'interdisciplinary approach', 'interest', 'killings', 'knockout gene', 'member', 'multidisciplinary', 'mutant', 'overexpression', 'pathogen', 'pathogenic bacteria', 'promoter', 'research study', 'software development', 'text searching', 'tool', 'trait', 'transcription factor', 'web site', 'wiki']",NIGMS,HARVARD MEDICAL SCHOOL,RC2,2010,1034875,0.034671797059650115
"A multidisciplinary approach to elucidating gene function in a model Gram-positiv    DESCRIPTION (provided by applicant): Because of the diversity of niches they inhabit, their versatility, and the threat their members pose to human health, Gram-positive bacteria are of paramount importance to multiple facets of microbiology. Nonetheless, there has been no systematic effort to elucidate gene functions and relationships on a genome-wide scale in any Gram-positive bacterium. Bacillus subtilis is the premier model organism for studies of Gram-positive bacteria because it offers powerful classical and molecular genetics, high-resolution cell biology, as well as a large and vibrant community of researchers. Evidence from other model organisms demonstrates that genome-wide approaches accelerate functional discovery. The opportunity for GO funding has catalyzed a broad-based community of researchers to pool their expertise to create a focused, coordinated, multidisciplinary effort to develop the genomic resources necessary to implement a comprehensive gene function effort. This initiative will serve as the blueprint for similar efforts in other model and pathogenic bacteria in the future. We will: 1. Construct genome-scale tools for global phenotypic analysis. These will include two bar-coded null mutant libraries, an ordered over-expression library, 300 promoter fusions to yfp and a C-terminal epitope- tagged library for all essential genes. 2. Develop and provide proof-of-principle experiments for key phenotyping methods. These will include high-throughput genetic interaction analysis and chemical genetic profiling; global cytological phenotyping; and transcriptional profiling. 3. Integrate and analyze the data across species. Each global phenotyping dataset will be analyzed in collaboration with bioinformaticists. This analysis will generate probabilistic functional interaction maps and transcriptional networks that will provide key information about gene functions and relationships. These will be used for comparative analysis among Gram-positive organisms and between Gram-positives and Gram- negatives. 4. Develop a B. subtilis database (subtilisHUB). We will build a web-based data resource to support ongoing functional annotation. This site will document the construction of all the genomic tools, house protocols and datasets, and provide infrastructure to enable Gene Ontology assignments, literature mining, and community discussion. 5. Engage the community of microbiologists in the genomic resources and approaches. A broad community of research specialists will launch genome-scale projects taking advantage of the tools, methodologies, and resources.      PUBLIC HEALTH RELEVANCE: This proposal is aimed at elucidating the function of all the genes in Bacillus subtilis, the major model organism for an important class of bacteria, Gram-positive bacteria. The project will generate genome-scale tools; pioneer the development of global phenotypic analysis for bacteria; and establish a web resource that will ensure continuity beyond the funding term. The concerted, community-based effort we propose will have far-reaching impact on research in bacteria in general, and on many Gram-positive pathogens.              Project Narrative This proposal is aimed at elucidating the function of all the genes in Bacillus subtilis, the major model organism for an important class of bacteria, Gram-positive bacteria. The project will generate genome-scale tools; pioneer the development of global phenotypic analysis for bacteria; and establish a web resource that will ensure continuity beyond the funding term. The concerted, community-based effort we propose will have far-reaching impact on research in bacteria in general, and on many Gram-positive pathogens.",A multidisciplinary approach to elucidating gene function in a model Gram-positiv,7854281,RC2GM092616,"['Animal Model', 'Antibiotic Resistance', 'Antibiotics', 'Bacillus subtilis', 'Bacteria', 'Bar Codes', 'Bioinformatics', 'C-terminal', 'Cell Size', 'Cell division', 'Cellular biology', 'Chemicals', 'Chromosomes', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Competence', 'DNA biosynthesis', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Ensure', 'Epitopes', 'Escherichia coli', 'Essential Genes', 'Experimental Designs', 'Expression Library', 'Fluorescence Microscopy', 'Funding', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Gram-Positive Bacteria', 'Growth', 'Health', 'Housing', 'Human', 'Institutes', 'Internet', 'Lead', 'Letters', 'Libraries', 'Literature', 'Maps', 'Measures', 'Metabolism', 'Methodology', 'Methods', 'Microbial Biofilms', 'Microbial Genetics', 'Microbiology', 'Modeling', 'Molecular', 'Molecular Genetics', 'Molecular Profiling', 'Noise', 'Online Systems', 'Ontology', 'Organism', 'Participant', 'Pathogenicity', 'Pathway interactions', 'Phenotype', 'Protocols documentation', 'Publishing', 'RNA Degradation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resources', 'Site', 'Specialist', 'Texas', 'base', 'cell motility', 'chemical genetics', 'comparative', 'detection of nutrient', 'drug sensitivity', 'experience', 'functional genomics', 'gene function', 'genetic profiling', 'genome database', 'genome-wide', 'interdisciplinary approach', 'interest', 'killings', 'knockout gene', 'member', 'multidisciplinary', 'mutant', 'overexpression', 'pathogen', 'pathogenic bacteria', 'promoter', 'public health relevance', 'research study', 'software development', 'text searching', 'tool', 'trait', 'transcription factor', 'web site', 'wiki']",NIGMS,HARVARD MEDICAL SCHOOL,RC2,2009,965125,0.034671797059650115
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7558424,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Area', 'Belief', 'Bioinformatics', 'Biological Assay', 'Budgets', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Research', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Electrocardiogram', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Grant', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Length', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Peer Review', 'Phenotype', 'Physiological', 'Preparation', 'Protocols documentation', 'Public Health', 'Published Comment', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'base', 'bench to bedside', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'graphical user interface', 'interest', 'meetings', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly', 'working group']",NHLBI,WASHINGTON UNIVERSITY,R01,2009,488000,0.07142936374818215
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7851333,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Belief', 'Bioinformatics', 'Biological Assay', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Phenotype', 'Physiological', 'Protocols documentation', 'Public Health', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly']",NHLBI,WASHINGTON UNIVERSITY,R01,2010,474912,0.07142936374818215
"BrainMap Tracker: Automated Annotation of Brain Mapping Experiments DESCRIPTION (provided by applicant): Cognitive neuroimaging methods are well regarded as powerful research tools for studying the neural correlates of both health and disease, and have led to the generation of enormous amounts of data. As a result, quantitative meta-analysis methods have been developed and adopted by the community as a means to organize and synthesize large-scale data sets. Meta-analyses allow diverse results regarding cognitive and cortical dysfunction in disease and treatment studies to be assessed, and the most reliable patterns of results to be determined. However, the most labor-intensive step of these procedures is identification of the appropriate literature. Currently, researchers manually execute multiple PubMed searches utilizing different keywords from alternate terminologies to capture the entirety of the studies they seek. The Cognitive Paradigm Ontology (CogPO) was created in 2009 to address the non-standard vocabulary that exists for describing behavioral tasks or paradigms in brain mapping experiments. Here, we propose to leverage the National Center for Biomedical Ontologies' bioinformatics tools to integrate CogPO and the NCBO Annotator, to develop and test a new computational resource, BrainMap Tracker, that will enable automatic identification of candidate studies for neuroimaging meta-analyses. This tool will allow rapid filtering of PubMed abstracts to identify what paradigms have been utilized to study brain activations for a given disease, or vice versa. The proposed system will be a novel web-based resource for cognitive and clinical neuroscientists to provide coherent groupings of studies suitable for meta-analysis. BrainMap Tracker will alleviate the problem of incomplete neuroimaging meta-analyses, provide additional informatics support through semantic annotations for large scale text-mining and data visualization efforts, and offer initial recommendations for new annotation terms for better categorization of new studies. The integration of results across the thousands of neuroimaging papers being published every year in neuropsychiatric diseases is needed to quickly and reliably identify the brain circuitry underlying cognitive and emotional dysfunction across different diagnostic categories. The BrainMap Tracker uses NCBO tools and semantic information to allow researchers to search PubMed and identify cross-disease groupings of studies using similar experimental methods or similar studies within a disease for meta-analysis.",BrainMap Tracker: Automated Annotation of Brain Mapping Experiments,8523982,R56MH097870,"['Address', 'Adopted', 'Algorithms', 'Archives', 'Autistic Disorder', 'Automated Annotation', 'Behavioral', 'Bioinformatics', 'Bipolar Disorder', 'Brain', 'Brain Mapping', 'Categories', 'Characteristics', 'Classification', 'Clinical', 'Cognitive', 'Collaborations', 'Communities', 'Data', 'Data Set', 'Databases', 'Diagnostic', 'Disease', 'Emotional', 'Experimental Designs', 'Foxes', 'Functional disorder', 'Generations', 'Goals', 'Gold', 'Grouping', 'Health', 'Image', 'Imagery', 'Informatics', 'Learning', 'Link', 'Literature', 'Location', 'Manuals', 'Mental Depression', 'Mental disorders', 'Meta-Analysis', 'Methods', 'Online Systems', 'Ontology', 'Paper', 'Pattern', 'Performance', 'Procedures', 'Process', 'PubMed', 'Publishing', 'Recommendation', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Retrieval', 'Schizophrenia', 'Semantics', 'Solutions', 'Structure', 'System', 'Taxonomy', 'Terminology', 'Testing', 'Text', 'Validation', 'Vocabulary', 'abstracting', 'base', 'biomedical ontology', 'candidate identification', 'computing resources', 'design', 'driving force', 'improved', 'indexing', 'interest', 'neuroimaging', 'neuropsychiatry', 'novel', 'relating to nervous system', 'repository', 'research study', 'text searching', 'tool', 'trend', 'web interface']",NIMH,THE MIND RESEARCH NETWORK,R56,2013,84935,0.09249747292630418
"BrainMap Tracker: Automated Annotation of Brain Mapping Experiments DESCRIPTION (provided by applicant): Cognitive neuroimaging methods are well regarded as powerful research tools for studying the neural correlates of both health and disease, and have led to the generation of enormous amounts of data. As a result, quantitative meta-analysis methods have been developed and adopted by the community as a means to organize and synthesize large-scale data sets. Meta-analyses allow diverse results regarding cognitive and cortical dysfunction in disease and treatment studies to be assessed, and the most reliable patterns of results to be determined. However, the most labor-intensive step of these procedures is identification of the appropriate literature. Currently, researchers manually execute multiple PubMed searches utilizing different keywords from alternate terminologies to capture the entirety of the studies they seek. The Cognitive Paradigm Ontology (CogPO) was created in 2009 to address the non-standard vocabulary that exists for describing behavioral tasks or paradigms in brain mapping experiments. Here, we propose to leverage the National Center for Biomedical Ontologies' bioinformatics tools to integrate CogPO and the NCBO Annotator, to develop and test a new computational resource, BrainMap Tracker, that will enable automatic identification of candidate studies for neuroimaging meta-analyses. This tool will allow rapid filtering of PubMed abstracts to identify what paradigms have been utilized to study brain activations for a given disease, or vice versa. The proposed system will be a novel web-based resource for cognitive and clinical neuroscientists to provide coherent groupings of studies suitable for meta-analysis. BrainMap Tracker will alleviate the problem of incomplete neuroimaging meta-analyses, provide additional informatics support through semantic annotations for large scale text-mining and data visualization efforts, and offer initial recommendations for new annotation terms for better categorization of new studies. The integration of results across the thousands of neuroimaging papers being published every year in neuropsychiatric diseases is needed to quickly and reliably identify the brain circuitry underlying cognitive and emotional dysfunction across different diagnostic categories. The BrainMap Tracker uses NCBO tools and semantic information to allow researchers to search PubMed and identify cross-disease groupings of studies using similar experimental methods or similar studies within a disease for meta-analysis.",BrainMap Tracker: Automated Annotation of Brain Mapping Experiments,8517433,R56MH097870,"['Address', 'Adopted', 'Algorithms', 'Archives', 'Autistic Disorder', 'Automated Annotation', 'Behavioral', 'Bioinformatics', 'Bipolar Disorder', 'Brain', 'Brain Mapping', 'Categories', 'Characteristics', 'Classification', 'Clinical', 'Cognitive', 'Collaborations', 'Communities', 'Data', 'Data Set', 'Databases', 'Diagnostic', 'Disease', 'Emotional', 'Experimental Designs', 'Foxes', 'Functional disorder', 'Generations', 'Goals', 'Gold', 'Grouping', 'Health', 'Image', 'Imagery', 'Informatics', 'Learning', 'Link', 'Literature', 'Location', 'Manuals', 'Mental Depression', 'Mental disorders', 'Meta-Analysis', 'Methods', 'Online Systems', 'Ontology', 'Paper', 'Pattern', 'Performance', 'Procedures', 'Process', 'PubMed', 'Publishing', 'Recommendation', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Retrieval', 'Schizophrenia', 'Semantics', 'Solutions', 'Structure', 'System', 'Taxonomy', 'Terminology', 'Testing', 'Text', 'Validation', 'Vocabulary', 'abstracting', 'base', 'biomedical ontology', 'candidate identification', 'computing resources', 'design', 'driving force', 'improved', 'indexing', 'interest', 'neuroimaging', 'neuropsychiatry', 'novel', 'relating to nervous system', 'repository', 'research study', 'text searching', 'tool', 'trend', 'web interface']",NIMH,THE MIND RESEARCH NETWORK,R56,2012,627209,0.09249747292630418
"Protege: An Ontology-Development Platform for Biomedical Scientists DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Protégé to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Protégé generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Protégé user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries. PUBLIC HEALTH RELEVANCE: Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: An Ontology-Development Platform for Biomedical Scientists,8987580,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Health', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Terminology', 'Thinking', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'light weight', 'next generation', 'novel', 'online community', 'open source', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2016,526540,0.19161798925942086
"Protege: An Ontology-Development Platform for Biomedical Scientists DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Protégé to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Protégé generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Protégé user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries. PUBLIC HEALTH RELEVANCE: Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: An Ontology-Development Platform for Biomedical Scientists,8788417,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Health', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2015,526540,0.19161798925942086
"Protege: An Ontology-Development Platform for Biomedical Scientists     DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Prot¿g¿ system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Prot¿g¿ users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Prot¿g¿ to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Prot¿g¿ generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Prot¿g¿ user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries.          PUBLIC HEALTH RELEVANCE: Prot�g� is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Prot�g� supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Prot�g� resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.            ",Protege: An Ontology-Development Platform for Biomedical Scientists,8597446,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Mails', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'Support System', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'public health relevance', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2014,533554,0.19161798925942086
"Protege: An Ontology-Development Platform for Biomedical Scientists     DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Prot¿g¿ system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Prot¿g¿ users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Prot¿g¿ to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Prot¿g¿ generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Prot¿g¿ user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries.          PUBLIC HEALTH RELEVANCE: Prot�g� is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Prot�g� supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Prot�g� resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.            ",Protege: An Ontology-Development Platform for Biomedical Scientists,8438361,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Mails', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'Support System', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'public health relevance', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2013,533554,0.19161798925942086
"Human-Specific Gain and Loss of Function DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university. PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.",Human-Specific Gain and Loss of Function,8796200,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomic Segment', 'Genomic approach', 'Genomics', 'Goals', 'Health', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2015,54194,0.05070064939082328
"Human-Specific Gain and Loss of Function     DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university.         PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.            ",Human-Specific Gain and Loss of Function,8635216,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomics', 'Goals', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'public health relevance', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2014,51530,0.05070064939082328
"Human-Specific Gain and Loss of Function     DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university.         PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.            ",Human-Specific Gain and Loss of Function,8457179,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomics', 'Goals', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'public health relevance', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2013,47114,0.05070064939082328
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.        Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8733018,UH3HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH3,2013,500000,0.08484300497086404
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.        Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8303361,UH2HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH2,2012,516448,0.08484300497086404
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.      PUBLIC HEALTH RELEVANCE: Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.           Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8145134,UH2HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH2,2011,540294,0.06142631439934247
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9407024,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2018,395628,0.060271640038125525
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9193091,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2017,396248,0.060271640038125525
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8985684,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2016,405708,0.060271640038125525
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS     DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans.         PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.            ",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8817212,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Books', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2015,406247,0.060271640038125525
"A Scalable Platform for Exploring and Analyzing Whole Brain Tissue Cleared Images Abstract  The ability of accurate localize and characterize cells in light sheet fluorescence microscopy (LSFM) image is indispensable for shedding new light on the understanding of three dimensional structures of the whole brain. In our previous work, we have successfully developed a 2D nuclear segmentation method for the nuclear cleared microscopy images using deep learning techniques. Although the convolutional neural networks show promise in segmenting cells in LSFM images, our previous work is confined in 2D segmentation scenario and suffers from the limited number of annotated data. In this project, we aim to develop a high throughput 3D cell segmentation engine, with the focus on improving the segmentation accuracy and generality. First, we will develop a cloud based semi-automatic annotation platform using the strength of virtual reality (VR) and crowd sourcing. The user-friendly annotation environment and stereoscopic view in VR can significantly improve the efficiency of manual annotation. We design a semi-automatic annotation workflow to largely reduce human intervention, and thus improve both the accuracy and the replicability of annotation across different users. Enlightened by the spirit of citizen science, we will extend the annotation software into a crowd sourcing platform which allows us to obtain a massive number of manual annotations in short time. Second, we will develop a fully 3D cell segmentation engine using 3D convolutional neural networks trained with the 3D annotated samples. Since it is often difficult to acquire isotropic LSFM images, we will further develop a super resolution method to impute a high resolution image to facilitate the 3D cell segmentation. Third, we will develop a transfer learning framework to make our 3D cell segmentation engine general enough to the application of novel LSFM data which might have significant gap of image appearance due to different imaging setup or clearing/staining protocol. This general framework will allow us to rapidly develop a specific cell segmentation solution for new LSFM data with very few or even no manual annotations, by transferring the existing 3D segmentation engine that has been trained with a sufficient number of annotated samples. Fourth, we will apply our computational tools to several pilot neuroscience studies: (1) Investigating how topoisomerase I (one of the autism linked transcriptional regulators) regulates brain structure, and (2) Investigating genetic influence on cell types in the developing human brain by quantifying the number of progenitor cells in fetal cortical tissue. Successful carrying out our project will have wide-reaching impact in neuroscience community in visualizing and analyzing complete cellular resolution maps of individual cell types within healthy and disease brain. The improved cell segmentation engine in 3D allows scientists from all over the world to share and process each other’s data accurately and efficiently, thus increasing reproducibility and power. Project Narrative This proposal aims to develop a next generation cell segmentation engine for the whole brain tissue cleared images. Our proposed work is built upon our previous 2D nuclear segmentation project using deep learning techniques. However, we found that our current computational tool is limited in 2D segmentation scenario and insufficient of annotated training samples. To address these limitations, we will first develop a cloud-based semi-automatic annotation tool with the capacity of virtual reality. Our annotation tool is designed to be cross- platform, which allows us to partner with “SciStarter” (the largest citizen science projects in the world) and acquire large amount of cell annotations from the science enthusiastic volunteers. Meanwhile, we will develop next generation 3D cell segmentation engine using an end-to-end fully connected convolution neural network. To facilitate 3D cell segmentation, we will also develop a super resolution method to impute an isotropic high- resolution image from a low-resolution microscopy image. After the development of 3D cell segmentation engine, we will continue to improve its generality by developing a transfer learning framework which enables us to rapidly deploy our 3D cell segmentation engine to the novel microscopy images without the time-consuming manual annotation step. Finally, we will apply our segmentation tool to visualize and quantify brain structure differences within genetically characterized mouse and human brain tissue at UNC neuroscience center. In the end of this project, we will release the software (both binary program and source code) and the 3D cell annotations, in order to facilitate the similar neuroscience studies in other institutes. Considering the importance of high throughput computational tools in quantifying three dimensional brain structure, this cutting- edge technique will be very useful in neuroscience research community.",A Scalable Platform for Exploring and Analyzing Whole Brain Tissue Cleared Images,9923760,R01NS110791,"['3-Dimensional', 'Address', 'Affect', 'Anecdotes', 'Appearance', 'Area', 'Biological', 'Brain', 'Brain Diseases', 'Brain region', 'Cell Nucleus', 'Cells', 'Communities', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Consumption', 'Data', 'Development', 'Environment', 'Evaluation', 'Fluorescence Microscopy', 'Genetic', 'Genetic Transcription', 'Genotype', 'Gold', 'Human', 'Image', 'Individual', 'Institutes', 'Intervention', 'Knock-out', 'Label', 'Lead', 'Learning', 'Light', 'Link', 'Manuals', 'Maps', 'Methods', 'Microscopy', 'Modeling', 'Mus', 'Neurosciences', 'Neurosciences Research', 'Noise', 'Nuclear', 'Performance', 'Process', 'Protocols documentation', 'Psychological Transfer', 'Reproducibility', 'Resolution', 'Sampling', 'Science', 'Scientist', 'Shapes', 'Slice', 'Source Code', 'Stains', 'Structure', 'Techniques', 'Technology', 'Time', 'Tissues', 'Training', 'Type I DNA Topoisomerases', 'Visual', 'Visualization', 'Work', 'annotation  system', 'autism spectrum disorder', 'base', 'bioimaging', 'brain tissue', 'cell type', 'citizen science', 'cloud based', 'computerized tools', 'contrast imaging', 'convolutional neural network', 'crowdsourcing', 'deep learning', 'design', 'fetal', 'flexibility', 'high resolution imaging', 'improved', 'microscopic imaging', 'next generation', 'novel', 'programs', 'stem cells', 'stereoscopic', 'success', 'three dimensional structure', 'tissue processing', 'tool', 'two-dimensional', 'user-friendly', 'virtual reality', 'volunteer']",NINDS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2020,335344,0.09107404817356031
"A Scalable Platform for Exploring and Analyzing Whole Brain Tissue Cleared Images Abstract  The ability of accurate localize and characterize cells in light sheet fluorescence microscopy (LSFM) image is indispensable for shedding new light on the understanding of three dimensional structures of the whole brain. In our previous work, we have successfully developed a 2D nuclear segmentation method for the nuclear cleared microscopy images using deep learning techniques. Although the convolutional neural networks show promise in segmenting cells in LSFM images, our previous work is confined in 2D segmentation scenario and suffers from the limited number of annotated data. In this project, we aim to develop a high throughput 3D cell segmentation engine, with the focus on improving the segmentation accuracy and generality. First, we will develop a cloud based semi-automatic annotation platform using the strength of virtual reality (VR) and crowd sourcing. The user-friendly annotation environment and stereoscopic view in VR can significantly improve the efficiency of manual annotation. We design a semi-automatic annotation workflow to largely reduce human intervention, and thus improve both the accuracy and the replicability of annotation across different users. Enlightened by the spirit of citizen science, we will extend the annotation software into a crowd sourcing platform which allows us to obtain a massive number of manual annotations in short time. Second, we will develop a fully 3D cell segmentation engine using 3D convolutional neural networks trained with the 3D annotated samples. Since it is often difficult to acquire isotropic LSFM images, we will further develop a super resolution method to impute a high resolution image to facilitate the 3D cell segmentation. Third, we will develop a transfer learning framework to make our 3D cell segmentation engine general enough to the application of novel LSFM data which might have significant gap of image appearance due to different imaging setup or clearing/staining protocol. This general framework will allow us to rapidly develop a specific cell segmentation solution for new LSFM data with very few or even no manual annotations, by transferring the existing 3D segmentation engine that has been trained with a sufficient number of annotated samples. Fourth, we will apply our computational tools to several pilot neuroscience studies: (1) Investigating how topoisomerase I (one of the autism linked transcriptional regulators) regulates brain structure, and (2) Investigating genetic influence on cell types in the developing human brain by quantifying the number of progenitor cells in fetal cortical tissue. Successful carrying out our project will have wide-reaching impact in neuroscience community in visualizing and analyzing complete cellular resolution maps of individual cell types within healthy and disease brain. The improved cell segmentation engine in 3D allows scientists from all over the world to share and process each other’s data accurately and efficiently, thus increasing reproducibility and power. Project Narrative This proposal aims to develop a next generation cell segmentation engine for the whole brain tissue cleared images. Our proposed work is built upon our previous 2D nuclear segmentation project using deep learning techniques. However, we found that our current computational tool is limited in 2D segmentation scenario and insufficient of annotated training samples. To address these limitations, we will first develop a cloud-based semi-automatic annotation tool with the capacity of virtual reality. Our annotation tool is designed to be cross- platform, which allows us to partner with “SciStarter” (the largest citizen science projects in the world) and acquire large amount of cell annotations from the science enthusiastic volunteers. Meanwhile, we will develop next generation 3D cell segmentation engine using an end-to-end fully connected convolution neural network. To facilitate 3D cell segmentation, we will also develop a super resolution method to impute an isotropic high- resolution image from a low-resolution microscopy image. After the development of 3D cell segmentation engine, we will continue to improve its generality by developing a transfer learning framework which enables us to rapidly deploy our 3D cell segmentation engine to the novel microscopy images without the time-consuming manual annotation step. Finally, we will apply our segmentation tool to visualize and quantify brain structure differences within genetically characterized mouse and human brain tissue at UNC neuroscience center. In the end of this project, we will release the software (both binary program and source code) and the 3D cell annotations, in order to facilitate the similar neuroscience studies in other institutes. Considering the importance of high throughput computational tools in quantifying three dimensional brain structure, this cutting- edge technique will be very useful in neuroscience research community.",A Scalable Platform for Exploring and Analyzing Whole Brain Tissue Cleared Images,9714223,R01NS110791,"['3-Dimensional', 'Address', 'Affect', 'Anecdotes', 'Appearance', 'Area', 'Biological', 'Brain', 'Brain Diseases', 'Brain region', 'Cell Nucleus', 'Cells', 'Communities', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Consumption', 'Data', 'Development', 'Dimensions', 'Environment', 'Evaluation', 'Fluorescence Microscopy', 'Genetic', 'Genetic Transcription', 'Genotype', 'Gold', 'Human', 'Image', 'Imagery', 'Individual', 'Institutes', 'Intervention', 'Knock-out', 'Label', 'Lead', 'Learning', 'Light', 'Link', 'Manuals', 'Maps', 'Methods', 'Microscopy', 'Modeling', 'Mus', 'Neurosciences', 'Neurosciences Research', 'Noise', 'Nuclear', 'Performance', 'Process', 'Protocols documentation', 'Psychological Transfer', 'Reproducibility', 'Resolution', 'Sampling', 'Science', 'Scientist', 'Shapes', 'Slice', 'Source Code', 'Stains', 'Stem cells', 'Structure', 'Techniques', 'Technology', 'Time', 'Tissues', 'Training', 'Type I DNA Topoisomerases', 'Visual', 'Work', 'annotation  system', 'autism spectrum disorder', 'base', 'bioimaging', 'brain tissue', 'cell type', 'citizen science', 'cloud based', 'computerized tools', 'contrast imaging', 'convolutional neural network', 'crowdsourcing', 'deep learning', 'design', 'fetal', 'flexibility', 'high resolution imaging', 'improved', 'microscopic imaging', 'next generation', 'novel', 'programs', 'stereoscopic', 'success', 'three dimensional structure', 'tissue processing', 'tool', 'two-dimensional', 'user-friendly', 'virtual reality', 'volunteer']",NINDS,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,335573,0.09107404817356031
"Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms Project Summary The nematode worm Caenorhabditis elegans has proven valuable as a model for many high-impact medical conditions. The strength of C. elegans derives from the extensive homologies between human and nematode genes (60-80%) and the many powerful tools available to manipulate genes in C. elegans, including expressing human genes. Researchers utilizing medical models based on C. elegans have converged on two main quantifiable measures of health and disease: locomotion and feeding; the latter is the focus of this proposal. C. elegans feeds on bacteria ingested through the pharynx, a rhythmic muscular pump in the worm’s throat. Alterations in pharyngeal activity are a sensitive indicator of dysfunction in muscles and neurons, as well as the animal’s overall health and metabolic state. C. elegans neurobiologists have long recognized the utility of the elec- tropharyngeogram (EPG), a non-invasive, whole-body electrical recording analogous to an electrocardiogram (ECG), which provides a quantitative readout of feeding. However, technical barriers associated with whole- animal electrophysiology have limited its adoption to fewer than fifteen laboratories world-wide. NemaMetrix Inc. surmounted these barriers by developing a turn-key, microfluidic system for EPG acquisition and analysis called the the ScreenChip platform. The proposed research and commercialization activities significantly expand the capabilities of the ScreenChip platform in two key respects. First, they enlarge the phenotyping capabilities of the platform by incorporating high-speed video of whole animal and pharyngeal movements. Second they develop a cloud database compatible with Gene Ontology, Open Biomedical Ontologies and Worm Ontology standards, allowing data-mining of combined electrophysiological, imaging and other data modalities. The machine-readable database will be compatible with artificial intelligence and machine learning algorithms. It will be accessible to all researchers to enable discovery of relationships between genotypes, phenotypes and treatments using large-scale analysis of multidimensional phenotypic profiles. The research and commercialization efforts culminate in an unprecedented integration of genetic, cellular, and organismal levels of analysis, with minimal training and effort required by users. Going forward, we envision the PheNom platform as a gold standard for medical research using C. elegans. n/a",Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms,9671422,R44GM119906,"['Adopted', 'Adoption', 'Aging', 'Amplifiers', 'Animal Model', 'Animals', 'Artificial Intelligence', 'Bacteria', 'Biomedical Research', 'Caenorhabditis elegans', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Electrocardiogram', 'Electrodes', 'Electrophysiology (science)', 'Equipment', 'Face', 'Familiarity', 'Feeds', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Models', 'Genotype', 'Gold', 'Health', 'Health Status', 'Human', 'Image', 'Ingestion', 'Kinetics', 'Laboratories', 'Locomotion', 'Market Research', 'Measures', 'Medical', 'Medical Research', 'Metabolic', 'Metabolic dysfunction', 'Metadata', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Molecular', 'Movement', 'Muscle', 'Nematoda', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Neurons', 'Ontology', 'Optics', 'Periodicity', 'Pharyngeal structure', 'Phase', 'Phenotype', 'Physiological', 'Pre-Clinical Model', 'Pump', 'Readability', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Signal Transduction', 'Speed', 'Statistical Data Interpretation', 'Stream', 'Structure', 'Surveys', 'System', 'Training', 'United States National Institutes of Health', 'Video Microscopy', 'addiction', 'automated image analysis', 'base', 'biomedical ontology', 'commercialization', 'data mining', 'design', 'feeding', 'human disease', 'human model', 'machine learning algorithm', 'member', 'phenotypic data', 'prevent', 'software development', 'success', 'tool', 'trait', 'web app']",NIGMS,"NEMAMETRIX, INC.",R44,2019,475637,0.0651521392406013
"Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms Project Summary The nematode worm Caenorhabditis elegans has proven valuable as a model for many high-impact medical conditions. The strength of C. elegans derives from the extensive homologies between human and nematode genes (60-80%) and the many powerful tools available to manipulate genes in C. elegans, including expressing human genes. Researchers utilizing medical models based on C. elegans have converged on two main quantifiable measures of health and disease: locomotion and feeding; the latter is the focus of this proposal. C. elegans feeds on bacteria ingested through the pharynx, a rhythmic muscular pump in the worm’s throat. Alterations in pharyngeal activity are a sensitive indicator of dysfunction in muscles and neurons, as well as the animal’s overall health and metabolic state. C. elegans neurobiologists have long recognized the utility of the elec- tropharyngeogram (EPG), a non-invasive, whole-body electrical recording analogous to an electrocardiogram (ECG), which provides a quantitative readout of feeding. However, technical barriers associated with whole- animal electrophysiology have limited its adoption to fewer than fifteen laboratories world-wide. NemaMetrix Inc. surmounted these barriers by developing a turn-key, microfluidic system for EPG acquisition and analysis called the the ScreenChip platform. The proposed research and commercialization activities significantly expand the capabilities of the ScreenChip platform in two key respects. First, they enlarge the phenotyping capabilities of the platform by incorporating high-speed video of whole animal and pharyngeal movements. Second they develop a cloud database compatible with Gene Ontology, Open Biomedical Ontologies and Worm Ontology standards, allowing data-mining of combined electrophysiological, imaging and other data modalities. The machine-readable database will be compatible with artificial intelligence and machine learning algorithms. It will be accessible to all researchers to enable discovery of relationships between genotypes, phenotypes and treatments using large-scale analysis of multidimensional phenotypic profiles. The research and commercialization efforts culminate in an unprecedented integration of genetic, cellular, and organismal levels of analysis, with minimal training and effort required by users. Going forward, we envision the PheNom platform as a gold standard for medical research using C. elegans. n/a",Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms,9467327,R44GM119906,"['Adopted', 'Adoption', 'Aging', 'Algorithms', 'Amplifiers', 'Animal Model', 'Animals', 'Artificial Intelligence', 'Bacteria', 'Biomedical Research', 'Caenorhabditis elegans', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Electrocardiogram', 'Electrodes', 'Electrophysiology (science)', 'Equipment', 'Face', 'Familiarity', 'Feeds', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Models', 'Genotype', 'Gold', 'Health', 'Health Status', 'Human', 'Image', 'Image Analysis', 'Kinetics', 'Laboratories', 'Locomotion', 'Machine Learning', 'Market Research', 'Measures', 'Medical', 'Medical Research', 'Metabolic', 'Metabolic dysfunction', 'Metadata', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Molecular', 'Movement', 'Muscle', 'Nematoda', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Neurons', 'Ontology', 'Optics', 'Periodicity', 'Pharyngeal structure', 'Phase', 'Phenotype', 'Physiological', 'Pre-Clinical Model', 'Pump', 'Readability', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Signal Transduction', 'Speed', 'Statistical Data Interpretation', 'Stream', 'Structure', 'Surveys', 'System', 'Training', 'United States National Institutes of Health', 'Video Microscopy', 'addiction', 'base', 'biomedical ontology', 'commercialization', 'data mining', 'design', 'feeding', 'human disease', 'human model', 'member', 'phenotypic data', 'prevent', 'software development', 'success', 'tool', 'trait', 'web app']",NIGMS,"NEMAMETRIX, INC.",R44,2018,510448,0.0651521392406013
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9532909,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'Exposure to', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Post-Translational Regulation', 'Process', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Site', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'bioinformatics resource', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'individualized medicine', 'information organization', 'innovation', 'interoperability', 'kinase inhibitor', 'knowledge base', 'knowledge integration', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2018,365327,0.07636142279573158
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9326315,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'MicroRNAs', 'Mining', 'Modification', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Post-Translational Regulation', 'Process', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Site', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'information organization', 'innovation', 'interoperability', 'kinase inhibitor', 'knowledge base', 'knowledge integration', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2017,370434,0.07636142279573158
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9195864,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphorylation Site', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Process', 'Protein Databases', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Regulation', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Staging', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'abstracting', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'information organization', 'innovation', 'kinase inhibitor', 'knowledge base', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2016,374400,0.07636142279573158
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9404042,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2018,564487,0.19077358967504338
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9217457,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'clinical development', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2017,578512,0.19077358967504338
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9848600,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'Infrastructure', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data quality', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'large datasets', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2020,559088,0.19077358967504338
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9607599,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'Infrastructure', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2019,559237,0.19077358967504338
"Integrative approach to studying LncRNA functions ABSTRACT  Long non-coding RNAs (lncRNAs) play regulatory roles in biological cell process and disease development. It has been emerging as a key regulator of diverse cellular processes. Great efforts have been made towards investigation of lncRNA functions with both experimental determination and theoretical modeling, leading to a rudimentary understanding of this class of RNAs. However, all of these cannot keep pace with the fast growth of diverse genetic data and urgent request of individual lncRNA function annotation, which is inhibited by the tremendous amount of lncRNAs and expensive experimental cost. This propose aim to address this issue by providing efficient and user-friendly tools for key lncRNA discovery and lncRNA function annotation. To do so, we will develop a unique bioinformatics and Systems Biology integrated approach, ISSNLncFA system, which enables the integration of all sorts of omics data and a comprehensive understanding of lncRNA functions.  We propose three specific aims for the ultimate lncRNA function annotation: (1) To develop a novel Co- Modules-based LncRNA Function Annotation (CoMoLncFA) model to detect key lncRNAs and to annotate lncRNA functions at post transcription level as lncRNA-PCG co-modules, lncRNA-pathways association network and lncRNA’s triplets (lncRNA-miRNA/TF-PCG) by considering the expression profiles of lncRNA, protein coding genes and miRNAs and transcript factors, and integrating the curated protein-protein interactions and biological pathways. (2) To develop a novel Structure-based LncRNA-protein Function Annotation (STRULncFA) model to characterize lncRNAs identified from Aim 1 by using their primary sequences and secondary structures for detecting lncRNA-protein functional relations; and to further reveal the regulatory roles and mechanism of these lncRNAs by determining the binding sites in both lncRNA and protein. (3) To experimentally validate the identified abnormal lncRNAs and their cellular products, to validate the identified lncRNA-protein interacting pairs and the predicted binding sites, and to develop software tools and an environment for functional annotation of lncRNAs, use these tools to evaluate the overall proposed approach, and apply them to identify lncRNA functions that may be involved in cell states, species, diseases and cancers and build lncRNA function databases.  We believe that we will build the models, tools and databases, and make them available to the public in a timely fashion. Our achievements will lead to a complete understanding of lncRNA functions and regulatory roles in cell and disease states. Moreover, our models and tools will be feasibly transformed to other function annotation tasks and disease studies with appropriate changes, and thus will move forward the general function annotation community and disease-related drug or therapy development. Project Narrative The ISSNLncFA software package will allow the investigators to characterize the LncRNA functions by integrating RNA-Seq, micorRNA-Seq, and together with protein and RNA structure data with systems biology approaches. As a prototype to test our system, we aim to use this system to study myelodysplastic syndromes (MDSs). Although MDS is used as the prototype of disease for this proposal, the system developed will be applicable to multiple diseases with complex phenotypes.",Integrative approach to studying LncRNA functions,9983714,R01GM123037,"['3-Dimensional', 'Achievement', 'Address', 'Architecture', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Process', 'Breast', 'CD34 gene', 'Cell Proliferation', 'Cell physiology', 'Cells', 'Chromatin', 'Chronic Disease', 'Code', 'Colorectal', 'Communities', 'Complex', 'Computer software', 'Consumption', 'Data', 'Databases', 'Development', 'Disease', 'Dysmyelopoietic Syndromes', 'Embryonic Development', 'Environment', 'Enzymes', 'Evaluation', 'Expression Profiling', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Imprinting', 'Growth', 'Heart Diseases', 'Hematopoietic stem cells', 'Individual', 'Investigation', 'Liver', 'Malignant - descriptor', 'Malignant Neoplasms', 'Methods', 'MicroRNAs', 'Modeling', 'Modification', 'Neoplasm Metastasis', 'Network-based', 'Pathway interactions', 'Phenotype', 'Play', 'Process', 'Proteins', 'RNA', 'Regulator Genes', 'Research Personnel', 'Role', 'Signal Transduction', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Theoretical model', 'Time', 'Transcript', 'Triplet Multiple Birth', 'Untranslated RNA', 'Validation', 'Work', 'X Chromosome', 'base', 'chromatin modification', 'chromatin remodeling', 'cost', 'deep learning', 'drug development', 'drug discovery', 'gene function', 'genomic data', 'high throughput technology', 'learning network', 'next generation', 'novel', 'outcome forecast', 'protein function', 'protein protein interaction', 'prototype', 'scaffold', 'software development', 'structured data', 'support vector machine', 'therapy development', 'tool', 'tool development', 'transcription factor', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'user-friendly']",NIGMS,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2020,302225,0.06864002309199055
"Integrative approach to studying LncRNA functions ABSTRACT  Long non-coding RNAs (lncRNAs) play regulatory roles in biological cell process and disease development. It has been emerging as a key regulator of diverse cellular processes. Great efforts have been made towards investigation of lncRNA functions with both experimental determination and theoretical modeling, leading to a rudimentary understanding of this class of RNAs. However, all of these cannot keep pace with the fast growth of diverse genetic data and urgent request of individual lncRNA function annotation, which is inhibited by the tremendous amount of lncRNAs and expensive experimental cost. This propose aim to address this issue by providing efficient and user-friendly tools for key lncRNA discovery and lncRNA function annotation. To do so, we will develop a unique bioinformatics and Systems Biology integrated approach, ISSNLncFA system, which enables the integration of all sorts of omics data and a comprehensive understanding of lncRNA functions.  We propose three specific aims for the ultimate lncRNA function annotation: (1) To develop a novel Co- Modules-based LncRNA Function Annotation (CoMoLncFA) model to detect key lncRNAs and to annotate lncRNA functions at post transcription level as lncRNA-PCG co-modules, lncRNA-pathways association network and lncRNA’s triplets (lncRNA-miRNA/TF-PCG) by considering the expression profiles of lncRNA, protein coding genes and miRNAs and transcript factors, and integrating the curated protein-protein interactions and biological pathways. (2) To develop a novel Structure-based LncRNA-protein Function Annotation (STRULncFA) model to characterize lncRNAs identified from Aim 1 by using their primary sequences and secondary structures for detecting lncRNA-protein functional relations; and to further reveal the regulatory roles and mechanism of these lncRNAs by determining the binding sites in both lncRNA and protein. (3) To experimentally validate the identified abnormal lncRNAs and their cellular products, to validate the identified lncRNA-protein interacting pairs and the predicted binding sites, and to develop software tools and an environment for functional annotation of lncRNAs, use these tools to evaluate the overall proposed approach, and apply them to identify lncRNA functions that may be involved in cell states, species, diseases and cancers and build lncRNA function databases.  We believe that we will build the models, tools and databases, and make them available to the public in a timely fashion. Our achievements will lead to a complete understanding of lncRNA functions and regulatory roles in cell and disease states. Moreover, our models and tools will be feasibly transformed to other function annotation tasks and disease studies with appropriate changes, and thus will move forward the general function annotation community and disease-related drug or therapy development. Project Narrative The ISSNLncFA software package will allow the investigators to characterize the LncRNA functions by integrating RNA-Seq, micorRNA-Seq, and together with protein and RNA structure data with systems biology approaches. As a prototype to test our system, we aim to use this system to study myelodysplastic syndromes (MDSs). Although MDS is used as the prototype of disease for this proposal, the system developed will be applicable to multiple diseases with complex phenotypes.",Integrative approach to studying LncRNA functions,9751927,R01GM123037,"['Achievement', 'Address', 'Architecture', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Process', 'Breast', 'CD34 gene', 'Cell Proliferation', 'Cell physiology', 'Cells', 'Chromatin', 'Chromosomes, Human, Pair 22', 'Chronic Disease', 'Code', 'Colorectal', 'Communities', 'Complex', 'Computer software', 'Consumption', 'Data', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dysmyelopoietic Syndromes', 'Embryonic Development', 'Environment', 'Enzymes', 'Evaluation', 'Expression Profiling', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Imprinting', 'Growth', 'Heart Diseases', 'Hematopoietic stem cells', 'Individual', 'Investigation', 'Liver', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Methods', 'MicroRNAs', 'Modeling', 'Modification', 'Neoplasm Metastasis', 'Network-based', 'Pathway interactions', 'Phenotype', 'Play', 'Process', 'Proteins', 'RNA', 'Regulator Genes', 'Research Personnel', 'Role', 'Signal Transduction', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Theoretical model', 'Time', 'Transcript', 'Triplet Multiple Birth', 'Untranslated RNA', 'Validation', 'Work', 'X Chromosome', 'base', 'chromatin modification', 'chromatin remodeling', 'cost', 'deep learning', 'drug development', 'drug discovery', 'gene function', 'genomic data', 'high throughput technology', 'learning network', 'next generation', 'novel', 'outcome forecast', 'protein function', 'protein protein interaction', 'prototype', 'scaffold', 'software development', 'therapy development', 'tool', 'tool development', 'transcription factor', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'user-friendly']",NIGMS,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2019,302225,0.06864002309199055
"Integrative approach to studying LncRNA functions ABSTRACT  Long non-coding RNAs (lncRNAs) play regulatory roles in biological cell process and disease development. It has been emerging as a key regulator of diverse cellular processes. Great efforts have been made towards investigation of lncRNA functions with both experimental determination and theoretical modeling, leading to a rudimentary understanding of this class of RNAs. However, all of these cannot keep pace with the fast growth of diverse genetic data and urgent request of individual lncRNA function annotation, which is inhibited by the tremendous amount of lncRNAs and expensive experimental cost. This propose aim to address this issue by providing efficient and user-friendly tools for key lncRNA discovery and lncRNA function annotation. To do so, we will develop a unique bioinformatics and Systems Biology integrated approach, ISSNLncFA system, which enables the integration of all sorts of omics data and a comprehensive understanding of lncRNA functions.  We propose three specific aims for the ultimate lncRNA function annotation: (1) To develop a novel Co- Modules-based LncRNA Function Annotation (CoMoLncFA) model to detect key lncRNAs and to annotate lncRNA functions at post transcription level as lncRNA-PCG co-modules, lncRNA-pathways association network and lncRNA’s triplets (lncRNA-miRNA/TF-PCG) by considering the expression profiles of lncRNA, protein coding genes and miRNAs and transcript factors, and integrating the curated protein-protein interactions and biological pathways. (2) To develop a novel Structure-based LncRNA-protein Function Annotation (STRULncFA) model to characterize lncRNAs identified from Aim 1 by using their primary sequences and secondary structures for detecting lncRNA-protein functional relations; and to further reveal the regulatory roles and mechanism of these lncRNAs by determining the binding sites in both lncRNA and protein. (3) To experimentally validate the identified abnormal lncRNAs and their cellular products, to validate the identified lncRNA-protein interacting pairs and the predicted binding sites, and to develop software tools and an environment for functional annotation of lncRNAs, use these tools to evaluate the overall proposed approach, and apply them to identify lncRNA functions that may be involved in cell states, species, diseases and cancers and build lncRNA function databases.  We believe that we will build the models, tools and databases, and make them available to the public in a timely fashion. Our achievements will lead to a complete understanding of lncRNA functions and regulatory roles in cell and disease states. Moreover, our models and tools will be feasibly transformed to other function annotation tasks and disease studies with appropriate changes, and thus will move forward the general function annotation community and disease-related drug or therapy development. Project Narrative The ISSNLncFA software package will allow the investigators to characterize the LncRNA functions by integrating RNA-Seq, micorRNA-Seq, and together with protein and RNA structure data with systems biology approaches. As a prototype to test our system, we aim to use this system to study myelodysplastic syndromes (MDSs). Although MDS is used as the prototype of disease for this proposal, the system developed will be applicable to multiple diseases with complex phenotypes.",Integrative approach to studying LncRNA functions,9565604,R01GM123037,"['Achievement', 'Address', 'Architecture', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Process', 'Breast', 'CD34 gene', 'Cell Proliferation', 'Cell physiology', 'Cells', 'Chromatin', 'Chromosomes, Human, Pair 22', 'Chronic Disease', 'Code', 'Colorectal', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dysmyelopoietic Syndromes', 'Embryonic Development', 'Environment', 'Enzymes', 'Evaluation', 'Expression Profiling', 'Gene Expression Regulation', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Imprinting', 'Growth', 'Heart Diseases', 'Hematopoietic stem cells', 'Individual', 'Investigation', 'Liver', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Methods', 'MicroRNAs', 'Modeling', 'Modification', 'Neoplasm Metastasis', 'Network-based', 'Pathway interactions', 'Phenotype', 'Play', 'Process', 'Proteins', 'RNA', 'Regulator Genes', 'Research Personnel', 'Role', 'Signal Transduction', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Theoretical model', 'Time', 'Transcript', 'Triplet Multiple Birth', 'Untranslated RNA', 'Validation', 'Work', 'X Chromosome', 'base', 'chromatin modification', 'chromatin remodeling', 'cost', 'deep learning', 'drug development', 'drug discovery', 'gene function', 'genomic data', 'high throughput technology', 'learning network', 'next generation', 'novel', 'outcome forecast', 'protein function', 'protein protein interaction', 'prototype', 'scaffold', 'software development', 'therapy development', 'tool', 'tool development', 'transcription factor', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'user-friendly']",NIGMS,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2018,302225,0.06864002309199055
"Integrative approach to studying LncRNA functions ABSTRACT  Long non-coding RNAs (lncRNAs) play regulatory roles in biological cell process and disease development. It has been emerging as a key regulator of diverse cellular processes. Great efforts have been made towards investigation of lncRNA functions with both experimental determination and theoretical modeling, leading to a rudimentary understanding of this class of RNAs. However, all of these cannot keep pace with the fast growth of diverse genetic data and urgent request of individual lncRNA function annotation, which is inhibited by the tremendous amount of lncRNAs and expensive experimental cost. This propose aim to address this issue by providing efficient and user-friendly tools for key lncRNA discovery and lncRNA function annotation. To do so, we will develop a unique bioinformatics and Systems Biology integrated approach, ISSNLncFA system, which enables the integration of all sorts of omics data and a comprehensive understanding of lncRNA functions.  We propose three specific aims for the ultimate lncRNA function annotation: (1) To develop a novel Co- Modules-based LncRNA Function Annotation (CoMoLncFA) model to detect key lncRNAs and to annotate lncRNA functions at post transcription level as lncRNA-PCG co-modules, lncRNA-pathways association network and lncRNA’s triplets (lncRNA-miRNA/TF-PCG) by considering the expression profiles of lncRNA, protein coding genes and miRNAs and transcript factors, and integrating the curated protein-protein interactions and biological pathways. (2) To develop a novel Structure-based LncRNA-protein Function Annotation (STRULncFA) model to characterize lncRNAs identified from Aim 1 by using their primary sequences and secondary structures for detecting lncRNA-protein functional relations; and to further reveal the regulatory roles and mechanism of these lncRNAs by determining the binding sites in both lncRNA and protein. (3) To experimentally validate the identified abnormal lncRNAs and their cellular products, to validate the identified lncRNA-protein interacting pairs and the predicted binding sites, and to develop software tools and an environment for functional annotation of lncRNAs, use these tools to evaluate the overall proposed approach, and apply them to identify lncRNA functions that may be involved in cell states, species, diseases and cancers and build lncRNA function databases.  We believe that we will build the models, tools and databases, and make them available to the public in a timely fashion. Our achievements will lead to a complete understanding of lncRNA functions and regulatory roles in cell and disease states. Moreover, our models and tools will be feasibly transformed to other function annotation tasks and disease studies with appropriate changes, and thus will move forward the general function annotation community and disease-related drug or therapy development. Project Narrative The ISSNLncFA software package will allow the investigators to characterize the LncRNA functions by integrating RNA-Seq, micorRNA-Seq, and together with protein and RNA structure data with systems biology approaches. As a prototype to test our system, we aim to use this system to study myelodysplastic syndromes (MDSs). Although MDS is used as the prototype of disease for this proposal, the system developed will be applicable to multiple diseases with complex phenotypes.",Integrative approach to studying LncRNA functions,9288292,R01GM123037,"['Achievement', 'Address', 'Architecture', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Process', 'Breast', 'CD34 gene', 'Cell Proliferation', 'Cell physiology', 'Cells', 'Chromatin', 'Chromosomes, Human, Pair 22', 'Chronic Disease', 'Code', 'Colorectal', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dysmyelopoietic Syndromes', 'Embryonic Development', 'Environment', 'Enzymes', 'Evaluation', 'Gene Expression Regulation', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Imprinting', 'Growth', 'Heart Diseases', 'Hematopoietic stem cells', 'Individual', 'Investigation', 'Liver', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Methods', 'MicroRNAs', 'Modeling', 'Modification', 'Molecular Profiling', 'Neoplasm Metastasis', 'Network-based', 'Pathway interactions', 'Phenotype', 'Play', 'Process', 'Proteins', 'RNA', 'Regulator Genes', 'Research Personnel', 'Role', 'Signal Transduction', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Theoretical model', 'Time', 'Transcript', 'Triplet Multiple Birth', 'Untranslated RNA', 'Validation', 'Work', 'X Chromosome', 'base', 'chromatin modification', 'chromatin remodeling', 'cost', 'drug development', 'drug discovery', 'gene function', 'genomic data', 'high throughput technology', 'learning network', 'next generation', 'novel', 'outcome forecast', 'protein function', 'protein protein interaction', 'prototype', 'scaffold', 'software development', 'therapy development', 'tool', 'tool development', 'transcription factor', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'user-friendly']",NIGMS,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R01,2017,302225,0.06864002309199055
"GlyGen Supplement: Develop automatic literature mining tool for extracting context specific glycan-protein data that will enhance the extent and quality of data in GlyGen ABSTRACT With significance in biotechnology, biomedicine, and basic research, glycobiology’s applications are widespread. Technological advancements in the field of glycobiology have expanded in parallel with the influx of an array of data within the glycosciences community. The broad range of experimental approaches, disparate nature of available datasets, and the seemingly piecemeal strategies required to construct comprehensive interpretations create inherent barriers for glycoscience researchers to utilize all available information. The mission of GlyGen has been to target and mitigate such challenges by developing procedures and a platform which integrates or builds upon glycoconjugate structure-function data from different resources. GlyGen, a NIH-funded international effort, captures and integrates over 90% of available glycoconjugate data, harmonizing and managing diverse outputs such as glycans, proteins, and genes integrated with genomics, pathway, and disease information. Since its inception, the GlyGen team has built a user-friendly platform complete with analytical tools and comprehensive, exportable data sets to ease the burden for researchers. Within the Swiss Institute of Bioinformatics (SIB), the Proteome Informatics Group (PIG) has worked extensively to develop the glycoinformatics resource GlyConnect, which focuses on the molecular characterization of protein glycosylation through an integrated, expertly- curated platform, specializing in structure analysis and producing novel data sets, such as site-specific glycan data. Despite each resource’s efforts to mitigate challenges, difficulties in amassing the amalgam of data required to fully examine microheterogeneity within glycobiology still persist. By utilizing their distinct strengths, the proposed collaborative research between GlyGen and GlyConnect will focus on further integrating site-specific protein-glycan data to generate more comprehensive data sets, where increasing the data availability in GlyGen is expected to accelerate basic and translational research. Currently, the major resources for site-specific protein-glycan data are UniCarbKB and UniProtKB, though the amount of available data from these databases, or other similar resources, is not substantial. To address this limitation, GlyConnect and GlyGen will develop an advanced, scalable, and site-specific protein-glycan annotation pipeline. This pipeline will be constructed using existing data in GlyConnect, in addition to roughly 100 publications identified and prioritized through current literature mining efforts in GlyGen. Moreover, front and back-end software developments will be implemented on the GlyGen platform, allowing glycoscience researchers to submit site-specific glycan data through a validated submission system. The proposed research will create a standardized methodology for more efficient data submission efforts, expand on the available site-specific protein-glycan data for the glycobiology community, and facilitate data sharing amongst glycoscience researchers. 1 NARRATIVE The proposed research will focus on improving coverage of site-specific annotations in GlyGen and result in the development of an efficient methodology for extracting and integrating site-specific glycoconjugate data into glycoinformatics resources. Along with documenting methodology (automated and manual rules), the extracted annotations will be easily accessed by the entire community via the GlyGen portal. The annotations will be linked back to the corresponding resources to increase and maintain the data flow across valuable bioinformatics databases. 2",GlyGen Supplement: Develop automatic literature mining tool for extracting context specific glycan-protein data that will enhance the extent and quality of data in GlyGen,10154002,U01GM125267,"['Address', 'Amalgam', 'Back', 'Basic Science', 'Bioinformatics', 'Biotechnology', 'Categories', 'Collaborations', 'Communities', 'Computer software', 'Consensus', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Feedback', 'Funding', 'Genes', 'Genomics', 'Glycobiology', 'Glycoconjugates', 'Glycoproteins', 'Growth', 'Image', 'Informatics', 'Institutes', 'International', 'Knowledge', 'Link', 'Manuals', 'Methodology', 'Mission', 'Molecular', 'Nature', 'Output', 'Pathway interactions', 'Polysaccharides', 'Procedures', 'Process', 'Protein Glycosylation', 'Proteins', 'Proteome', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Site', 'Source', 'Standardization', 'Structure', 'Structure-Activity Relationship', 'System', 'Techniques', 'Translational Research', 'United States National Institutes of Health', 'Update', 'Validation', 'Work', 'analytical tool', 'base', 'bioinformatics resource', 'data harmonization', 'data sharing', 'data submission', 'glycosylation', 'improved', 'novel', 'software development', 'text searching', 'tool', 'user-friendly']",NIGMS,UNIVERSITY OF GEORGIA,U01,2020,138658,0.08112926307056707
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9270498,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Biological', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Logistics', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'System', 'Taxonomy', 'Technology', 'Testing', 'Time', 'Work', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'experimental study', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'multidisciplinary', 'novel', 'open source', 'oral behavior', 'oral microbiome', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2017,311153,0.0784788473013111
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9158909,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Process', 'Reading', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'Technology', 'Testing', 'Time', 'Work', 'abstracting', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'multidisciplinary', 'novel', 'open source', 'oral behavior', 'oral microbiome', 'research study', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2016,311803,0.0784788473013111
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9474101,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Biological', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Logistics', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Work', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'experimental study', 'human microbiota', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiome research', 'multidisciplinary', 'novel', 'open source', 'operational taxonomic units', 'oral behavior', 'oral microbial community', 'oral microbiome', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2018,350321,0.0784788473013111
"The ODIE Toolkit - Software for Information Extraction and Biomedical Ontology De    DESCRIPTION (provided by applicant): We propose a program of research with two interlocking, foundational goals: (1) to develop and evaluate software for information extraction from clinical text corpora using existing Open Biomedical Ontologies (OBO) and (2) to develop and evaluate software for enrichment of existing biomedical ontologies from clinical text corpora. As a result of our work we will deliver the Ontology Development and Information Extraction Toolkit (ODIE) - a set of software components integrated with GATE, Prot¿g¿ and LexGrid, that will assist researchers and ontology developers in performing these tasks. As a testbed for our work, we will focus mainly on the National Cancer Institute Thesaurus - an existing OBO ontology, but will develop many of our components to be generalizable to other OBO ontologies. We have chosen the domain of hematopathology as a test case because of the rich and varied source of clinical documents, and the potential for our software to advance translational biomedical research in this area. However the majority of the components that we develop will be domain-neutral and will generalize to other areas within and outside of Oncology. The work we propose is significant for three contributions. First, we will develop novel methods or modify existing methods for accomplishing information extraction and ontology enrichment and we will evaluate the performance of these alternatives. Second, we will develop and disseminate generic software resources for performing these tasks, which leverage the National Center for Biomedical Ontology supported tools. Third, we will contribute to the development of existing OBO ontologies. The results of this work will use OBO ontologies in fundamental ways to advance biomedicine. This grant propose to develop a set of computer tools to assist researchers in (1) extracting meaning and codifying medical documents, and (2) building formal representations of knowledge from those documents. This work would benefit the general public by increasing the speed and efficiency of determining what information is in a particular medical document and allowing automated processing of large numbers of documents. Additionally, the project would contribute to the software for developing other applications by helping researchers build more comprehensive ontologies. The results of this work may benefit both medical research and patient care.          n/a",The ODIE Toolkit - Software for Information Extraction and Biomedical Ontology De,7896739,R01CA127979,"['Address', 'Architecture', 'Area', 'Biomedical Research', 'Clinical', 'Computer software', 'Computers', 'Detection', 'Development', 'General Population', 'Generic Drugs', 'Goals', 'Grant', 'Hematologic Neoplasms', 'Hematopathology', 'Human', 'Individual', 'Lesion', 'Medical', 'Medical Research', 'Metadata', 'Methods', 'Names', 'National Cancer Institute', 'Ontology', 'Patient Care', 'Performance', 'Positioning Attribute', 'Process', 'Reference Standards', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Source', 'Speech', 'Speed', 'Stream', 'Structure', 'Suggestion', 'Testing', 'Text', 'Thesauri', 'Translational Research', 'Visual', 'Work', 'base', 'biomedical ontology', 'information organization', 'novel', 'oncology', 'phrases', 'programs', 'software development', 'statistics', 'text searching', 'tool']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2010,427872,0.18663043629900247
"The ODIE Toolkit - Software for Information Extraction and Biomedical Ontology De    DESCRIPTION (provided by applicant): We propose a program of research with two interlocking, foundational goals: (1) to develop and evaluate software for information extraction from clinical text corpora using existing Open Biomedical Ontologies (OBO) and (2) to develop and evaluate software for enrichment of existing biomedical ontologies from clinical text corpora. As a result of our work we will deliver the Ontology Development and Information Extraction Toolkit (ODIE) - a set of software components integrated with GATE, Prot¿g¿ and LexGrid, that will assist researchers and ontology developers in performing these tasks. As a testbed for our work, we will focus mainly on the National Cancer Institute Thesaurus - an existing OBO ontology, but will develop many of our components to be generalizable to other OBO ontologies. We have chosen the domain of hematopathology as a test case because of the rich and varied source of clinical documents, and the potential for our software to advance translational biomedical research in this area. However the majority of the components that we develop will be domain-neutral and will generalize to other areas within and outside of Oncology. The work we propose is significant for three contributions. First, we will develop novel methods or modify existing methods for accomplishing information extraction and ontology enrichment and we will evaluate the performance of these alternatives. Second, we will develop and disseminate generic software resources for performing these tasks, which leverage the National Center for Biomedical Ontology supported tools. Third, we will contribute to the development of existing OBO ontologies. The results of this work will use OBO ontologies in fundamental ways to advance biomedicine. This grant propose to develop a set of computer tools to assist researchers in (1) extracting meaning and codifying medical documents, and (2) building formal representations of knowledge from those documents. This work would benefit the general public by increasing the speed and efficiency of determining what information is in a particular medical document and allowing automated processing of large numbers of documents. Additionally, the project would contribute to the software for developing other applications by helping researchers build more comprehensive ontologies. The results of this work may benefit both medical research and patient care.          n/a",The ODIE Toolkit - Software for Information Extraction and Biomedical Ontology De,7669420,R01CA127979,"['Address', 'Architecture', 'Area', 'Biomedical Research', 'Body of uterus', 'Clinical', 'Computer software', 'Computers', 'Detection', 'Development', 'General Population', 'Generic Drugs', 'Goals', 'Grant', 'Hematologic Neoplasms', 'Hematopathology', 'Human', 'Individual', 'Lesion', 'Medical', 'Medical Research', 'Metadata', 'Methods', 'Names', 'National Cancer Institute', 'Ontology', 'Patient Care', 'Performance', 'Positioning Attribute', 'Process', 'Reference Standards', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Source', 'Speech', 'Speed', 'Stream', 'Structure', 'Suggestion', 'Testing', 'Text', 'Thesauri', 'Translational Research', 'Visual', 'Work', 'base', 'biomedical ontology', 'information organization', 'novel', 'oncology', 'phrases', 'programs', 'software development', 'statistics', 'text searching', 'tool']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,564761,0.18663043629900247
"The ODIE Toolkit - Software for Information Extraction and Biomedical Ontology De    DESCRIPTION (provided by applicant): We propose a program of research with two interlocking, foundational goals: (1) to develop and evaluate software for information extraction from clinical text corpora using existing Open Biomedical Ontologies (OBO) and (2) to develop and evaluate software for enrichment of existing biomedical ontologies from clinical text corpora. As a result of our work we will deliver the Ontology Development and Information Extraction Toolkit (ODIE) - a set of software components integrated with GATE, Prot¿g¿ and LexGrid, that will assist researchers and ontology developers in performing these tasks. As a testbed for our work, we will focus mainly on the National Cancer Institute Thesaurus - an existing OBO ontology, but will develop many of our components to be generalizable to other OBO ontologies. We have chosen the domain of hematopathology as a test case because of the rich and varied source of clinical documents, and the potential for our software to advance translational biomedical research in this area. However the majority of the components that we develop will be domain-neutral and will generalize to other areas within and outside of Oncology. The work we propose is significant for three contributions. First, we will develop novel methods or modify existing methods for accomplishing information extraction and ontology enrichment and we will evaluate the performance of these alternatives. Second, we will develop and disseminate generic software resources for performing these tasks, which leverage the National Center for Biomedical Ontology supported tools. Third, we will contribute to the development of existing OBO ontologies. The results of this work will use OBO ontologies in fundamental ways to advance biomedicine. This grant propose to develop a set of computer tools to assist researchers in (1) extracting meaning and codifying medical documents, and (2) building formal representations of knowledge from those documents. This work would benefit the general public by increasing the speed and efficiency of determining what information is in a particular medical document and allowing automated processing of large numbers of documents. Additionally, the project would contribute to the software for developing other applications by helping researchers build more comprehensive ontologies. The results of this work may benefit both medical research and patient care.          n/a",The ODIE Toolkit - Software for Information Extraction and Biomedical Ontology De,7500835,R01CA127979,"['Address', 'Architecture', 'Area', 'Biomedical Research', 'Body of uterus', 'Clinical', 'Compatible', 'Computer software', 'Computers', 'Detection', 'Development', 'General Population', 'Generic Drugs', 'Goals', 'Grant', 'Hematologic Neoplasms', 'Hematopathology', 'Human', 'Individual', 'Lesion', 'Medical', 'Medical Research', 'Metadata', 'Methods', 'Names', 'National Cancer Institute', 'Numbers', 'Ontology', 'Patient Care', 'Performance', 'Positioning Attribute', 'Process', 'Reference Standards', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Source', 'Speech', 'Speed', 'Stream', 'Structure', 'Suggestion', 'Testing', 'Text', 'Thesauri', 'Translational Research', 'Visual', 'Work', 'base', 'concept', 'information organization', 'novel', 'oncology', 'programs', 'software development', 'statistics', 'text searching', 'tool']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2008,551134,0.18663043629900247
"The ODIE Toolkit - Software for Information Extraction and Biomedical Ontology De    DESCRIPTION (provided by applicant): We propose a program of research with two interlocking, foundational goals: (1) to develop and evaluate software for information extraction from clinical text corpora using existing Open Biomedical Ontologies (OBO) and (2) to develop and evaluate software for enrichment of existing biomedical ontologies from clinical text corpora. As a result of our work we will deliver the Ontology Development and Information Extraction Toolkit (ODIE) - a set of software components integrated with GATE, Prot¿g¿ and LexGrid, that will assist researchers and ontology developers in performing these tasks. As a testbed for our work, we will focus mainly on the National Cancer Institute Thesaurus - an existing OBO ontology, but will develop many of our components to be generalizable to other OBO ontologies. We have chosen the domain of hematopathology as a test case because of the rich and varied source of clinical documents, and the potential for our software to advance translational biomedical research in this area. However the majority of the components that we develop will be domain-neutral and will generalize to other areas within and outside of Oncology. The work we propose is significant for three contributions. First, we will develop novel methods or modify existing methods for accomplishing information extraction and ontology enrichment and we will evaluate the performance of these alternatives. Second, we will develop and disseminate generic software resources for performing these tasks, which leverage the National Center for Biomedical Ontology supported tools. Third, we will contribute to the development of existing OBO ontologies. The results of this work will use OBO ontologies in fundamental ways to advance biomedicine. This grant propose to develop a set of computer tools to assist researchers in (1) extracting meaning and codifying medical documents, and (2) building formal representations of knowledge from those documents. This work would benefit the general public by increasing the speed and efficiency of determining what information is in a particular medical document and allowing automated processing of large numbers of documents. Additionally, the project would contribute to the software for developing other applications by helping researchers build more comprehensive ontologies. The results of this work may benefit both medical research and patient care.          n/a",The ODIE Toolkit - Software for Information Extraction and Biomedical Ontology De,7360352,R01CA127979,"['Address', 'Architecture', 'Area', 'Biomedical Research', 'Body of uterus', 'Clinical', 'Compatible', 'Computer software', 'Computers', 'Detection', 'Development', 'General Population', 'Generic Drugs', 'Goals', 'Grant', 'Hematologic Neoplasms', 'Hematopathology', 'Human', 'Individual', 'Medical', 'Medical Research', 'Metadata', 'Methods', 'Names', 'National Cancer Institute', 'Numbers', 'Ontology', 'Patient Care', 'Performance', 'Positioning Attribute', 'Process', 'Reference Standards', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Source', 'Speech', 'Speed', 'Stream', 'Structure', 'Suggestion', 'Testing', 'Text', 'Thesauri', 'Translational Research', 'Visual', 'Work', 'base', 'concept', 'information organization', 'novel', 'oncology', 'programs', 'software development', 'statistics', 'text searching', 'tool']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2007,570418,0.18663043629900247
"Addressing Open Challenges of Computational Genome Annotation We propose to capitalize on success of ongoing collaboration between the bioinformatics teams at the University of Greifswald (Germany) and at the Georgia Institute of Technology (USA) and address open challenges in computational genome annotation. In the course of this development, we plan to implement new algorithmic ideas and satisfy the needs of unbiased integration of different types of OMICS data.  We plan to address one of the long-standing problems at interface of bioinformatics and machine learning – automatic generative and discriminative parameterization of gene finding algorithms. Current methods of combining OMICS evidence frequently result in under predicting or over predicting tools. Having good understanding of the difficulties and the properties of different types of OMICS evidence we propose an optimized approach to the full unsupervised, generative and discriminative training.  We will introduce novel means to optimize integration of multiple OMICS evidence into gene prediction. These ideas will develop further the protein family-based gene finding implemented in AUGUSTUS-PPX. We propose to create representations of protein families for gene finding that for the first time include cross-species gene structure information.  We will develop a new approach that will unify two advanced research areas - transcript reconstruction from RNA-Seq and statistical gene finding that integrates RNA-Seq and homology information. We will describe a new, comprehensive model and EM-like algorithmic technique (the “wholistic” approach) to identify the sets of transcripts and their expression levels that best fit the available OMICS evidence.  We will also develop an automatic gene-finding algorithm for a full content of metagenomes including eukaryotic and viral metagenomic sequences. This task is conventionally considered too challenging. We propose a solution exploiting and advancing algorithmic ideas and approaches that we mastered in the course of creating gene finders for prokaryotic metagenomes as well as eukaryotic genomes.  All new tools will be available to the community under open source licenses. The goal of this project is to advance the science of genome interpretation by developing much needed computational methods and tools for high precision annotation of eukaryotic genomes and metagenomes. This advance will make an impact in research on model and non-model organisms including important human pathogens, parasites and viruses. New high throughput technologies generate volumes of sequence data on complex genomes as well as metagenomes. Still these big data volumes have to be transformed into scientific knowledge. Our new bioinformatics tools, matching the latest sequencing technology in speed and performance, will make a significant impact in genomic research aiming at ultimate understanding of human health and disease.",Addressing Open Challenges of Computational Genome Annotation,9501001,R01GM128145,"['Address', 'Algorithms', 'Alternative Splicing', 'Area', 'Bacteriophages', 'Benchmarking', 'Big Data', 'Bioinformatics', 'Chronic', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complement', 'Complex', 'Computing Methodologies', 'Data', 'Deterioration', 'Development', 'Development Plans', 'Disease', 'Gene Family', 'Gene Structure', 'Genes', 'Genome', 'Genomics', 'Germany', 'Goals', 'Health', 'Human', 'Insecta', 'Institutes', 'Introns', 'Knowledge', 'Length', 'Licensing', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Nested Genes', 'Noise', 'Overlapping Genes', 'Parasites', 'Performance', 'Population', 'Positioning Attribute', 'Property', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'RNA Splicing', 'Research', 'Running', 'Speed', 'Spliced Genes', 'Statistical Models', 'Supervision', 'Techniques', 'Technology', 'Time', 'Training', 'Transcript', 'Universities', 'Viral', 'Virus', 'annotation  system', 'base', 'computerized tools', 'cost', 'course development', 'design', 'evidence base', 'expectation', 'gene complementation', 'genome annotation', 'genome sciences', 'high throughput technology', 'improved', 'instrument', 'member', 'metagenome', 'multiple omics', 'nanopore', 'new technology', 'novel', 'novel strategies', 'open source', 'operation', 'pathogen', 'predictive tools', 'protein profiling', 'reconstruction', 'success', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NIGMS,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2018,359810,0.05590578312261554
"Addressing Open Challenges of Computational Genome Annotation We propose to capitalize on success of ongoing collaboration between the bioinformatics teams at the University of Greifswald (Germany) and at the Georgia Institute of Technology (USA) and address open challenges in computational genome annotation. In the course of this development, we plan to implement new algorithmic ideas and satisfy the needs of unbiased integration of different types of OMICS data.  We plan to address one of the long-standing problems at interface of bioinformatics and machine learning – automatic generative and discriminative parameterization of gene finding algorithms. Current methods of combining OMICS evidence frequently result in under predicting or over predicting tools. Having good understanding of the difficulties and the properties of different types of OMICS evidence we propose an optimized approach to the full unsupervised, generative and discriminative training.  We will introduce novel means to optimize integration of multiple OMICS evidence into gene prediction. These ideas will develop further the protein family-based gene finding implemented in AUGUSTUS-PPX. We propose to create representations of protein families for gene finding that for the first time include cross-species gene structure information.  We will develop a new approach that will unify two advanced research areas - transcript reconstruction from RNA-Seq and statistical gene finding that integrates RNA-Seq and homology information. We will describe a new, comprehensive model and EM-like algorithmic technique (the “wholistic” approach) to identify the sets of transcripts and their expression levels that best fit the available OMICS evidence.  We will also develop an automatic gene-finding algorithm for a full content of metagenomes including eukaryotic and viral metagenomic sequences. This task is conventionally considered too challenging. We propose a solution exploiting and advancing algorithmic ideas and approaches that we mastered in the course of creating gene finders for prokaryotic metagenomes as well as eukaryotic genomes.  All new tools will be available to the community under open source licenses. The goal of this project is to advance the science of genome interpretation by developing much needed computational methods and tools for high precision annotation of eukaryotic genomes and metagenomes. This advance will make an impact in research on model and non-model organisms including important human pathogens, parasites and viruses. New high throughput technologies generate volumes of sequence data on complex genomes as well as metagenomes. Still these big data volumes have to be transformed into scientific knowledge. Our new bioinformatics tools, matching the latest sequencing technology in speed and performance, will make a significant impact in genomic research aiming at ultimate understanding of human health and disease.",Addressing Open Challenges of Computational Genome Annotation,9975182,R01GM128145,"['Address', 'Algorithms', 'Alternative Splicing', 'Area', 'Bacteriophages', 'Benchmarking', 'Big Data', 'Bioinformatics', 'Chronic', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complement', 'Complex', 'Computing Methodologies', 'Data', 'Deterioration', 'Development', 'Development Plans', 'Disease', 'Gene Family', 'Gene Structure', 'Genes', 'Genome', 'Genomics', 'Germany', 'Goals', 'Health', 'Human', 'Insecta', 'Institutes', 'Introns', 'Knowledge', 'Length', 'Licensing', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Nested Genes', 'Noise', 'Overlapping Genes', 'Parasites', 'Performance', 'Population', 'Positioning Attribute', 'Property', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'RNA Splicing', 'Research', 'Running', 'Speed', 'Spliced Genes', 'Statistical Models', 'Supervision', 'Techniques', 'Technology', 'Time', 'Training', 'Transcript', 'Universities', 'Viral', 'Virus', 'annotation  system', 'base', 'bioinformatics tool', 'computerized tools', 'cost', 'course development', 'design', 'evidence base', 'expectation', 'gene complementation', 'genome annotation', 'genome sciences', 'high throughput technology', 'human pathogen', 'improved', 'instrument', 'member', 'metagenome', 'multiple omics', 'nanopore', 'new technology', 'novel', 'novel strategies', 'open source', 'operation', 'predictive tools', 'protein profiling', 'reconstruction', 'success', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NIGMS,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2020,342390,0.05590578312261554
"Addressing Open Challenges of Computational Genome Annotation We propose to capitalize on success of ongoing collaboration between the bioinformatics teams at the University of Greifswald (Germany) and at the Georgia Institute of Technology (USA) and address open challenges in computational genome annotation. In the course of this development, we plan to implement new algorithmic ideas and satisfy the needs of unbiased integration of different types of OMICS data.  We plan to address one of the long-standing problems at interface of bioinformatics and machine learning – automatic generative and discriminative parameterization of gene finding algorithms. Current methods of combining OMICS evidence frequently result in under predicting or over predicting tools. Having good understanding of the difficulties and the properties of different types of OMICS evidence we propose an optimized approach to the full unsupervised, generative and discriminative training.  We will introduce novel means to optimize integration of multiple OMICS evidence into gene prediction. These ideas will develop further the protein family-based gene finding implemented in AUGUSTUS-PPX. We propose to create representations of protein families for gene finding that for the first time include cross-species gene structure information.  We will develop a new approach that will unify two advanced research areas - transcript reconstruction from RNA-Seq and statistical gene finding that integrates RNA-Seq and homology information. We will describe a new, comprehensive model and EM-like algorithmic technique (the “wholistic” approach) to identify the sets of transcripts and their expression levels that best fit the available OMICS evidence.  We will also develop an automatic gene-finding algorithm for a full content of metagenomes including eukaryotic and viral metagenomic sequences. This task is conventionally considered too challenging. We propose a solution exploiting and advancing algorithmic ideas and approaches that we mastered in the course of creating gene finders for prokaryotic metagenomes as well as eukaryotic genomes.  All new tools will be available to the community under open source licenses. The goal of this project is to advance the science of genome interpretation by developing much needed computational methods and tools for high precision annotation of eukaryotic genomes and metagenomes. This advance will make an impact in research on model and non-model organisms including important human pathogens, parasites and viruses. New high throughput technologies generate volumes of sequence data on complex genomes as well as metagenomes. Still these big data volumes have to be transformed into scientific knowledge. Our new bioinformatics tools, matching the latest sequencing technology in speed and performance, will make a significant impact in genomic research aiming at ultimate understanding of human health and disease.",Addressing Open Challenges of Computational Genome Annotation,9761554,R01GM128145,"['Address', 'Algorithms', 'Alternative Splicing', 'Area', 'Bacteriophages', 'Benchmarking', 'Big Data', 'Bioinformatics', 'Chronic', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complement', 'Complex', 'Computing Methodologies', 'Data', 'Deterioration', 'Development', 'Development Plans', 'Disease', 'Gene Family', 'Gene Structure', 'Genes', 'Genome', 'Genomics', 'Germany', 'Goals', 'Health', 'Human', 'Insecta', 'Institutes', 'Introns', 'Knowledge', 'Length', 'Licensing', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Nested Genes', 'Noise', 'Overlapping Genes', 'Parasites', 'Performance', 'Population', 'Positioning Attribute', 'Property', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'RNA Splicing', 'Research', 'Running', 'Speed', 'Spliced Genes', 'Statistical Models', 'Structural Genes', 'Supervision', 'Techniques', 'Technology', 'Time', 'Training', 'Transcript', 'Universities', 'Viral', 'Virus', 'annotation  system', 'base', 'bioinformatics tool', 'computerized tools', 'cost', 'course development', 'design', 'evidence base', 'expectation', 'gene complementation', 'genome annotation', 'genome sciences', 'high throughput technology', 'human pathogen', 'improved', 'instrument', 'member', 'metagenome', 'multiple omics', 'nanopore', 'new technology', 'novel', 'novel strategies', 'open source', 'operation', 'predictive tools', 'protein profiling', 'reconstruction', 'success', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NIGMS,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2019,344107,0.05590578312261554
"Machine learning approaches for improved accuracy and speed in sequence annotation Summary/Abstract Alignment of biological sequences is a key step in understanding their evolution, function, and patterns of activity. Here, we describe Machine Learning approaches to improve both accuracy and speed of highly- sensitive sequence alignment. To improve accuracy, we develop methods to reduce erroneous annotation caused by (1) the existence of low complexity and repetitive sequence and (2) the overextension of alignments of true homologs into unrelated sequence. We describe approaches based on both hidden Markov models and Artificial Neural Networks to dramatically reduce these sorts of sequence annotation error. We also address the issue of annotation speed, with development of a custom Deep Learning architecture designed to very quickly filter away large portions of candidate sequence comparisons prior to the relatively-slow sequence-alignment step. The results of these efforts will be incorporated into forks of the open source sequence alignment tools HMMER, MMSeqs, and (where appropriate) BLAST; we will also work with community developers of annotation pipelines, such as RepeatMasker and IMG/M, to incorporate these approaches. The development and incorporation into these widely used bioinformatics tools will lead to widespread impact on sequence annotation efforts. Narrative Modern molecular biology depends on effective methods for creating sequence alignments quickly and accurately. This proposal describes a plan to develop novel Machine Learning approaches that will dramatically increase the speed of highly-sensitive sequence alignment, and will also address two significant sources of erroneous sequence annotation, (i) the presence of repetitive sequence in biological sequences, and (ii) the tendency for sequence alignment algorithms to extend alignments beyond the boundaries of true homology. The proposed methods represent a mix of applications of hidden Markov models and Artificial Neural Networks, and build on prior success in applying such methods to the problem of sensitive sequence annotation.",Machine learning approaches for improved accuracy and speed in sequence annotation,10020995,R01GM132600,"['Address', 'Algorithms', 'Architecture', 'Bioinformatics', 'Biological', 'Classification', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Consumption', 'Custom', 'DNA Transposable Elements', 'Data Set', 'Deletion Mutation', 'Descriptor', 'Development', 'Error Sources', 'Evolution', 'Foundations', 'Genome', 'Genomics', 'Hour', 'Human', 'Human Genome', 'Industry Standard', 'Insertion Mutation', 'Institutes', 'Intervention', 'Joints', 'Label', 'Letters', 'Licensing', 'Machine Learning', 'Manuals', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Molecular Biology', 'Network-based', 'Nucleotides', 'Pattern', 'Pilot Projects', 'Proteins', 'Repetitive Sequence', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Speed', 'Statistical Models', 'Takifugu', 'Work', 'annotation  system', 'artificial neural network', 'base', 'bioinformatics tool', 'computing resources', 'convolutional neural network', 'deep learning', 'density', 'design', 'genomic data', 'improved', 'markov model', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'software development', 'statistics', 'success', 'tool']",NIGMS,UNIVERSITY OF MONTANA,R01,2020,287504,0.1177617097957312
"Machine learning approaches for improved accuracy and speed in sequence annotation Summary/Abstract Alignment of biological sequences is a key step in understanding their evolution, function, and patterns of activity. Here, we describe Machine Learning approaches to improve both accuracy and speed of highly- sensitive sequence alignment. To improve accuracy, we develop methods to reduce erroneous annotation caused by (1) the existence of low complexity and repetitive sequence and (2) the overextension of alignments of true homologs into unrelated sequence. We describe approaches based on both hidden Markov models and Artificial Neural Networks to dramatically reduce these sorts of sequence annotation error. We also address the issue of annotation speed, with development of a custom Deep Learning architecture designed to very quickly filter away large portions of candidate sequence comparisons prior to the relatively-slow sequence-alignment step. The results of these efforts will be incorporated into forks of the open source sequence alignment tools HMMER, MMSeqs, and (where appropriate) BLAST; we will also work with community developers of annotation pipelines, such as RepeatMasker and IMG/M, to incorporate these approaches. The development and incorporation into these widely used bioinformatics tools will lead to widespread impact on sequence annotation efforts. Narrative Modern molecular biology depends on effective methods for creating sequence alignments quickly and accurately. This proposal describes a plan to develop novel Machine Learning approaches that will dramatically increase the speed of highly-sensitive sequence alignment, and will also address two significant sources of erroneous sequence annotation, (i) the presence of repetitive sequence in biological sequences, and (ii) the tendency for sequence alignment algorithms to extend alignments beyond the boundaries of true homology. The proposed methods represent a mix of applications of hidden Markov models and Artificial Neural Networks, and build on prior success in applying such methods to the problem of sensitive sequence annotation.",Machine learning approaches for improved accuracy and speed in sequence annotation,9887588,R01GM132600,"['Address', 'Algorithms', 'Architecture', 'Bioinformatics', 'Biological', 'Classification', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Consumption', 'Custom', 'DNA Transposable Elements', 'Data Set', 'Deletion Mutation', 'Descriptor', 'Development', 'Error Sources', 'Evolution', 'Foundations', 'Genome', 'Genomics', 'Hour', 'Human', 'Human Genome', 'Industry Standard', 'Insertion Mutation', 'Institutes', 'Intervention', 'Joints', 'Label', 'Letters', 'Licensing', 'Machine Learning', 'Manuals', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Molecular Biology', 'Network-based', 'Nucleotides', 'Pattern', 'Pilot Projects', 'Proteins', 'Repetitive Sequence', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Speed', 'Statistical Models', 'Takifugu', 'Work', 'annotation  system', 'artificial neural network', 'base', 'bioinformatics tool', 'computing resources', 'convolutional neural network', 'deep learning', 'density', 'design', 'genomic data', 'improved', 'markov model', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'software development', 'statistics', 'success', 'tool']",NIGMS,UNIVERSITY OF MONTANA,R01,2019,286435,0.1177617097957312
"Functional annotation of new genes aided by deep learning New genes (NGs) are generated by multiple mechanisms and their end-piece sequences are identified as the chimeric transcript sequence from multiple human sources including healthy and disease tissues. Therefore, NGs have been recognized as important biomarkers and therapeutic targets for precision medicine. Many efforts have been made to study individual NG function and to identify relevant drug targets. However, the current in-depth research and achievements are mainly concentrated on several driver NGs, and classical cancer drugs have been directly used to target the NG domains, such as the kinase domain of BCR-ABL1 fusion protein in leukemia. Some of the fusion proteins with retaining DNA-binding domains such as transcription factors can directly bind their target genes, such as the EWSR1-FLI fusion actively recruiting BAF complex. Recently, the downstream effectors of driver FGs have emerged as therapeutic targets. For example, targeting the downstream CCND2 inhibited RUNX1/ETO-driven leukemic expansion in vitro and in vivo and inhibition of STAT5, the downstream factor of NUP214-ABL1 led to the induction of leukemia cell death. However, the functions of most identified FGs have not been systematically investigated. This is mainly due to the limitations of traditional tools and the high cost of experimental procedures. Therefore, there is an urgent need to develop new tools for analyzing NG breakpoint-specific features systemically in the human genome and predict their originating and regulatory mechanisms, such as upstream and downstream effectors. In-depth annotation based on NG structure is important for understanding the cellular mechanisms of NGs. Effective use of systematic bioinformatics tools for functional annotation can provide a deeper insight into the role of NGs in the development and progression of diseases such as cancers to find direct and indirect therapeutic targets. In this study, we will develop five bioinformatics tools for the functional annotation and feature analysis of NGs, a predictive pipeline for automatic analysis of downstream effects of NGs, and a predictive method for tracing the origin of NGs. This project will be a substantial contribution to public health by systematicallydeep annotating thefunction of new genes (NGs) in cancer and neurodegenerative diseases such as Alzheimer's disease. These systematic annotation results will be performed through ChimerAnno (a tool for functional annotation of human chimeric genes), FGviewer (a tool for visualizing multi-tiered functional features of fusion genes), NGeffector with DeepChIP (a tool for predicting NG downstream effectors with enhanced transcription factor binding site prediction with deep learning), DeepCLIP (a tool for providing evidence of origin of NGs for trans-splicing mechanism) and hBPAI (a tool for feature extraction of human new genes' breakpoint (BP)). The application of these approaches on all kinds of human BPs of new genes will be integrated into NewGeneDB, New Gene annotation Database. This study will advance our knowledge in NG regulatory mechanisms in diseases and open up the possibility of novel treatments of cancer tools and platforms for analyzing NGs. and other diseases in the future by providing powerful",Functional annotation of new genes aided by deep learning,10029297,R35GM138184,"['ABL1 gene', 'Achievement', 'Alzheimer&apos', 's Disease', 'Antineoplastic Agents', 'Binding', 'Binding Sites', 'Biological Markers', 'CCND2 gene', 'Cell Death', 'Chimeric Proteins', 'Complex', 'DNA Binding Domain', 'Databases', 'Development', 'Disease', 'Disease Progression', 'Drug Targeting', 'EWSR1 gene', 'Future', 'Gene Structure', 'Genes', 'Human', 'Human Genome', 'In Vitro', 'Individual', 'Knowledge', 'Leukemic Cell', 'Malignant Neoplasms', 'Methods', 'NUP214 gene', 'Neurodegenerative Disorders', 'Phosphotransferases', 'Procedures', 'Public Health', 'RUNX1 gene', 'Recruitment Activity', 'Regulator Genes', 'Research', 'Role', 'Source', 'Stat5 protein', 'Tissues', 'Trans-Splicing', 'Transcript', 'base', 'bioinformatics tool', 'cancer therapy', 'chimeric gene', 'cost', 'deep learning', 'feature extraction', 'fusion gene', 'gene function', 'in vivo', 'insight', 'leukemia', 'novel', 'precision medicine', 'predictive tools', 'therapeutic target', 'tool', 'transcription factor']",NIGMS,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R35,2020,334778,0.08040478145162602
"A family-based framework of quality assurance for biomedical ontologies DESCRIPTION (provided by applicant):  We will develop a family-based Quality Assurance (QA) framework for biomedical ontologies. Ontology QA is critical for increasing the use of ontologies in interdisciplinary research and in electronic health records (EHRs). We will develop computational techniques for identifying concepts with high probability of errors to improve efficiency and effectiveness of ontology QA.  Biomedical ontologies are large, complex knowledge representation systems that enable the integration of knowledge from different fields. The largest, best-known ontology repository is the Bioportal of the National Center for Biomedical Ontologies, containing more than 300 ontologies and tools for editing, browsing, and visualizing these ontologies.  However, many errors have been discovered in BioPortal's ontologies. QA in BioPortal has been mostly focused on use-cases and ad hoc techniques. Our computational techniques will automatically identify sets of concepts with a high likelihood of errors to empower ontology QA.  In past research, we have designed many QA techniques for single ontologies and have shown that sets of complex and uncommonly classified concepts have significantly higher percentages of errors. The theoretical bases for our QA are Abstraction Networks (AbNs), which summarize ontologies in a compact way. Using AbNs, we identified many error-prone concepts.  In this project, we will perform QA for whole families of ontologies. We have already identified seven preliminary families, based on structural properties. If a classification of concepts yields higher than usual error rates in several ontologies of a family F then we hypothesize that this will be true for such classifications for most ontologies of F. We will build a prototype software tool (BLUOWL) for determining AbNs for each family, to support QA of its ontologies.  Our primary test beds will be seven cancer-related ontologies, e.g., the National Cancer Institute thesaurus (NCIt), with different properties and purposes. Some non-cancer ontologies will also be included. We have published preliminary QA results for four such ontologies.  In evaluation studies, we will formulate and test hypotheses, statistically expressing the error expectations for various kinds of concepts. Ontologies' curators were recruited to review the suspicious concepts we will identify as part of their regular QA efforts (outside of our budget).  In summary, we will: Identify families of BioPortal ontologies based on ontology structure and design a unified methodology for deriving their abstraction networks; Build a software tool (BLUOWL) for QA of each family; Investigate concept classifications more likely to be erroneous in each family; Perform evaluation of our QA methodologies and usability studies for BLUOWL. PUBLIC HEALTH RELEVANCE:  Biomedical ontologies are critical for interdisciplinary research and electronic health records (EHRs). The largest, best-known ontology repository is the Bioportal of the National Center for Biomedical Ontologies, containing more than 300 ontologies. However, many errors have been discovered in BioPortal's ontologies. Quality Assurance (QA) in BioPortal has been mostly focused on use-cases and ad hoc techniques. We will develop a systematic, family-based framework for QA of biomedical ontologies. The theoretical basis for our QA methods is constituted by Abstraction Networks, which summarize ontologies in a compact way. The Abstraction Networks will support the detection of sets of concepts with a high likelihood of errors, which will improve the yield of the QA activities. A prototype software tool (BLUOWL) implementing our QA theory will be built.",A family-based framework of quality assurance for biomedical ontologies,9228344,R01CA190779,"['Beds', 'Biomedical Research', 'Budgets', 'Classification', 'Clinical Research', 'Complex', 'Computational Technique', 'Derivation procedure', 'Detection', 'Effectiveness', 'Electronic Health Record', 'Ensure', 'Evaluation', 'Evaluation Studies', 'Family', 'Foundations', 'Funding', 'Interdisciplinary Study', 'Knowledge', 'Light', 'Malignant Neoplasms', 'Methodology', 'Methods', 'National Cancer Institute', 'Ontology', 'Probability', 'Process', 'Property', 'Publishing', 'Recruitment Activity', 'Research', 'Research Design', 'Research Project Grants', 'Semantics', 'Software Tools', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'United States', 'War', 'Work', 'anticancer research', 'base', 'biomedical ontology', 'design', 'expectation', 'health care delivery', 'improved', 'information organization', 'innovation', 'knowledge integration', 'prototype', 'public health relevance', 'quality assurance', 'repository', 'theories', 'tool', 'usability']",NCI,NEW JERSEY INSTITUTE OF TECHNOLOGY,R01,2017,555758,0.15369247409749298
"A family-based framework of quality assurance for biomedical ontologies DESCRIPTION (provided by applicant):  We will develop a family-based Quality Assurance (QA) framework for biomedical ontologies. Ontology QA is critical for increasing the use of ontologies in interdisciplinary research and in electronic health records (EHRs). We will develop computational techniques for identifying concepts with high probability of errors to improve efficiency and effectiveness of ontology QA.  Biomedical ontologies are large, complex knowledge representation systems that enable the integration of knowledge from different fields. The largest, best-known ontology repository is the Bioportal of the National Center for Biomedical Ontologies, containing more than 300 ontologies and tools for editing, browsing, and visualizing these ontologies.  However, many errors have been discovered in BioPortal's ontologies. QA in BioPortal has been mostly focused on use-cases and ad hoc techniques. Our computational techniques will automatically identify sets of concepts with a high likelihood of errors to empower ontology QA.  In past research, we have designed many QA techniques for single ontologies and have shown that sets of complex and uncommonly classified concepts have significantly higher percentages of errors. The theoretical bases for our QA are Abstraction Networks (AbNs), which summarize ontologies in a compact way. Using AbNs, we identified many error-prone concepts.  In this project, we will perform QA for whole families of ontologies. We have already identified seven preliminary families, based on structural properties. If a classification of concepts yields higher than usual error rates in several ontologies of a family F then we hypothesize that this will be true for such classifications for most ontologies of F. We will build a prototype software tool (BLUOWL) for determining AbNs for each family, to support QA of its ontologies.  Our primary test beds will be seven cancer-related ontologies, e.g., the National Cancer Institute thesaurus (NCIt), with different properties and purposes. Some non-cancer ontologies will also be included. We have published preliminary QA results for four such ontologies.  In evaluation studies, we will formulate and test hypotheses, statistically expressing the error expectations for various kinds of concepts. Ontologies' curators were recruited to review the suspicious concepts we will identify as part of their regular QA efforts (outside of our budget).  In summary, we will: Identify families of BioPortal ontologies based on ontology structure and design a unified methodology for deriving their abstraction networks; Build a software tool (BLUOWL) for QA of each family; Investigate concept classifications more likely to be erroneous in each family; Perform evaluation of our QA methodologies and usability studies for BLUOWL. PUBLIC HEALTH RELEVANCE:  Biomedical ontologies are critical for interdisciplinary research and electronic health records (EHRs). The largest, best-known ontology repository is the Bioportal of the National Center for Biomedical Ontologies, containing more than 300 ontologies. However, many errors have been discovered in BioPortal's ontologies. Quality Assurance (QA) in BioPortal has been mostly focused on use-cases and ad hoc techniques. We will develop a systematic, family-based framework for QA of biomedical ontologies. The theoretical basis for our QA methods is constituted by Abstraction Networks, which summarize ontologies in a compact way. The Abstraction Networks will support the detection of sets of concepts with a high likelihood of errors, which will improve the yield of the QA activities. A prototype software tool (BLUOWL) implementing our QA theory will be built.",A family-based framework of quality assurance for biomedical ontologies,9027817,R01CA190779,"['Beds', 'Biomedical Research', 'Budgets', 'Classification', 'Clinical Research', 'Complex', 'Computational Technique', 'Derivation procedure', 'Detection', 'Effectiveness', 'Electronic Health Record', 'Ensure', 'Evaluation', 'Evaluation Studies', 'Family', 'Funding', 'Interdisciplinary Study', 'Knowledge', 'Light', 'Malignant Neoplasms', 'Methodology', 'Methods', 'National Cancer Institute', 'Ontology', 'Probability', 'Process', 'Property', 'Publishing', 'Recruitment Activity', 'Research', 'Research Design', 'Research Project Grants', 'Semantics', 'Software Tools', 'Structure', 'System', 'Techniques', 'Testing', 'Thesauri', 'United States', 'War', 'Work', 'anticancer research', 'base', 'biomedical ontology', 'design', 'empowered', 'expectation', 'health care delivery', 'improved', 'information organization', 'innovation', 'prototype', 'public health relevance', 'quality assurance', 'repository', 'theories', 'tool', 'usability']",NCI,NEW JERSEY INSTITUTE OF TECHNOLOGY,R01,2016,600426,0.15369247409749298
"A family-based framework of quality assurance for biomedical ontologies     DESCRIPTION (provided by applicant):  We will develop a family-based Quality Assurance (QA) framework for biomedical ontologies. Ontology QA is critical for increasing the use of ontologies in interdisciplinary research and in electronic health records (EHRs). We will develop computational techniques for identifying concepts with high probability of errors to improve efficiency and effectiveness of ontology QA.  Biomedical ontologies are large, complex knowledge representation systems that enable the integration of knowledge from different fields. The largest, best-known ontology repository is the Bioportal of the National Center for Biomedical Ontologies, containing more than 300 ontologies and tools for editing, browsing, and visualizing these ontologies.  However, many errors have been discovered in BioPortal's ontologies. QA in BioPortal has been mostly focused on use-cases and ad hoc techniques. Our computational techniques will automatically identify sets of concepts with a high likelihood of errors to empower ontology QA.  In past research, we have designed many QA techniques for single ontologies and have shown that sets of complex and uncommonly classified concepts have significantly higher percentages of errors. The theoretical bases for our QA are Abstraction Networks (AbNs), which summarize ontologies in a compact way. Using AbNs, we identified many error-prone concepts.  In this project, we will perform QA for whole families of ontologies. We have already identified seven preliminary families, based on structural properties. If a classification of concepts yields higher than usual error rates in several ontologies of a family F then we hypothesize that this will be true for such classifications for most ontologies of F. We will build a prototype software tool (BLUOWL) for determining AbNs for each family, to support QA of its ontologies.  Our primary test beds will be seven cancer-related ontologies, e.g., the National Cancer Institute thesaurus (NCIt), with different properties and purposes. Some non-cancer ontologies will also be included. We have published preliminary QA results for four such ontologies.  In evaluation studies, we will formulate and test hypotheses, statistically expressing the error expectations for various kinds of concepts. Ontologies' curators were recruited to review the suspicious concepts we will identify as part of their regular QA efforts (outside of our budget).  In summary, we will: Identify families of BioPortal ontologies based on ontology structure and design a unified methodology for deriving their abstraction networks; Build a software tool (BLUOWL) for QA of each family; Investigate concept classifications more likely to be erroneous in each family; Perform evaluation of our QA methodologies and usability studies for BLUOWL.         PUBLIC HEALTH RELEVANCE:  Biomedical ontologies are critical for interdisciplinary research and electronic health records (EHRs). The largest, best-known ontology repository is the Bioportal of the National Center for Biomedical Ontologies, containing more than 300 ontologies. However, many errors have been discovered in BioPortal's ontologies. Quality Assurance (QA) in BioPortal has been mostly focused on use-cases and ad hoc techniques. We will develop a systematic, family-based framework for QA of biomedical ontologies. The theoretical basis for our QA methods is constituted by Abstraction Networks, which summarize ontologies in a compact way. The Abstraction Networks will support the detection of sets of concepts with a high likelihood of errors, which will improve the yield of the QA activities. A prototype software tool (BLUOWL) implementing our QA theory will be built.            ",A family-based framework of quality assurance for biomedical ontologies,8802486,R01CA190779,"['Beds', 'Biomedical Research', 'Budgets', 'Classification', 'Clinical Research', 'Complex', 'Computational Technique', 'Derivation procedure', 'Detection', 'Effectiveness', 'Electronic Health Record', 'Ensure', 'Evaluation', 'Evaluation Studies', 'Family', 'Funding', 'Interdisciplinary Study', 'Knowledge', 'Light', 'Malignant Neoplasms', 'Methodology', 'Methods', 'National Cancer Institute', 'Ontology', 'Probability', 'Process', 'Property', 'Publishing', 'Recruitment Activity', 'Research', 'Research Design', 'Research Project Grants', 'Semantics', 'Software Tools', 'Structure', 'System', 'Techniques', 'Testing', 'Thesauri', 'United States', 'War', 'Work', 'anticancer research', 'base', 'biomedical ontology', 'design', 'empowered', 'expectation', 'health care delivery', 'improved', 'information organization', 'innovation', 'prototype', 'public health relevance', 'quality assurance', 'repository', 'theories', 'tool', 'usability']",NCI,NEW JERSEY INSTITUTE OF TECHNOLOGY,R01,2015,598037,0.15369247409749298
"Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) promises to transform the way that physicians treat their patients, resulting in better quality and more consistent care informed directly by the totality of relevant evidence. However, clinicians do not have the time to keep up to date with the vast medical literature. Systematic reviews, which provide rigorous, comprehensive and transparent assessments of the evidence pertaining to specific clinical questions, promise to mitigate this problem by concisely summarizing all pertinent evidence. But producing such reviews has become increasingly burdensome (and hence expensive) due in part to the exponential expansion of the biomedical literature base, hampering our ability to provide evidence-based care.  If we are to scale EBM to meet the demands imposed by the rapidly growing volume of published evidence, then we must modernize EBM tools and methods. More specifically, if we are to continue generating up-to-date evidence syntheses, then we must optimize the systematic review process. Toward this end, we propose developing new methods that combine crowdsourcing and machine learning to facilitate efficient annotation of the full-texts of articles describing clinical trials. These annotations will comprise mark-up of sections of text that discuss clinically relevant fields of importance in EBM, such as discussion of patient characteristics, interventions studied and potential sources of bias. Such annotations would make literature search and data extraction much easier for systematic reviewers, thus reducing their workload and freeing more time for them to conduct thoughtful evidence synthesis.  This will be the first in-depth exploration of crowdsourcing for EBM. We will collect annotations from workers with varying levels of expertise and cost, ranging from medical students to workers recruited via Amazon Mechanical Turk. We will develop and evaluate novel methods of aggregating annotations from such heterogeneous sources. And we will use the acquired manual annotations to train machine learning models that automate this markup process. Models capable of automatically identifying clinically salient text snippets in full-text articles describing clinical trials would be broadly useful for biomedical literature retrieval tasks and would have impact beyond our immediate application of EBM. PUBLIC HEALTH RELEVANCE: We propose to develop crowdsourcing and machine learning methods to annotate clinically important sentences in full-text articles describing clinical trials Ultimately, we aim to automate such annotation, thereby enabling more efficient practice of evidence- based medicine (EBM).",Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities,9275458,UH2CA203711,"['Artificial Intelligence', 'Automated Annotation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Trials', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Development', 'Effectiveness', 'Eligibility Determination', 'Environment', 'Evidence Based Medicine', 'Evidence based practice', 'Health', 'Human', 'Hybrids', 'Individual', 'Information Retrieval', 'Intervention', 'Intervention Studies', 'Literature', 'Machine Learning', 'Manuals', 'Mechanics', 'Medical', 'Medical Students', 'Medicine', 'Methods', 'Modeling', 'Modernization', 'Outcome', 'Paper', 'Participant', 'Patients', 'Physicians', 'Population', 'Population Characteristics', 'Preparation', 'Process', 'Publishing', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Retrieval', 'Rewards', 'Source', 'System', 'Text', 'Time', 'Training', 'Work', 'Workload', 'base', 'clinically relevant', 'collaborative environment', 'cost', 'crowdsourcing', 'demographics', 'detector', 'dosage', 'evidence base', 'learning strategy', 'novel', 'phrases', 'public health relevance', 'scale up', 'skills', 'systematic review', 'tool']",NCI,NORTHEASTERN UNIVERSITY,UH2,2017,210198,0.11233760061618969
"Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) promises to transform the way that physicians treat their patients, resulting in better quality and more consistent care informed directly by the totality of relevant evidence. However, clinicians do not have the time to keep up to date with the vast medical literature. Systematic reviews, which provide rigorous, comprehensive and transparent assessments of the evidence pertaining to specific clinical questions, promise to mitigate this problem by concisely summarizing all pertinent evidence. But producing such reviews has become increasingly burdensome (and hence expensive) due in part to the exponential expansion of the biomedical literature base, hampering our ability to provide evidence-based care.  If we are to scale EBM to meet the demands imposed by the rapidly growing volume of published evidence, then we must modernize EBM tools and methods. More specifically, if we are to continue generating up-to-date evidence syntheses, then we must optimize the systematic review process. Toward this end, we propose developing new methods that combine crowdsourcing and machine learning to facilitate efficient annotation of the full-texts of articles describing clinical trials. These annotations will comprise mark-up of sections of text that discuss clinically relevant fields of importance in EBM, such as discussion of patient characteristics, interventions studied and potential sources of bias. Such annotations would make literature search and data extraction much easier for systematic reviewers, thus reducing their workload and freeing more time for them to conduct thoughtful evidence synthesis.  This will be the first in-depth exploration of crowdsourcing for EBM. We will collect annotations from workers with varying levels of expertise and cost, ranging from medical students to workers recruited via Amazon Mechanical Turk. We will develop and evaluate novel methods of aggregating annotations from such heterogeneous sources. And we will use the acquired manual annotations to train machine learning models that automate this markup process. Models capable of automatically identifying clinically salient text snippets in full-text articles describing clinical trials would be broadly useful for biomedical literature retrieval tasks and would have impact beyond our immediate application of EBM.         PUBLIC HEALTH RELEVANCE: We propose to develop crowdsourcing and machine learning methods to annotate clinically important sentences in full-text articles describing clinical trials Ultimately, we aim to automate such annotation, thereby enabling more efficient practice of evidence- based medicine (EBM).         ",Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities,9076888,UH2CA203711,"['Artificial Intelligence', 'Automated Annotation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Trials', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Development', 'Effectiveness', 'Eligibility Determination', 'Environment', 'Evidence Based Medicine', 'Health', 'Human', 'Hybrids', 'Individual', 'Information Retrieval', 'Intervention', 'Intervention Studies', 'Literature', 'Machine Learning', 'Manuals', 'Mechanics', 'Medical', 'Medical Students', 'Methods', 'Modeling', 'Outcome', 'Paper', 'Participant', 'Patients', 'Physicians', 'Population', 'Population Characteristics', 'Preparation', 'Process', 'Publishing', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Retrieval', 'Rewards', 'Source', 'System', 'Text', 'Time', 'Training', 'Work', 'Workload', 'abstracting', 'base', 'clinically relevant', 'collaborative environment', 'cost', 'crowdsourcing', 'demographics', 'detector', 'dosage', 'evidence base', 'learning strategy', 'meetings', 'novel', 'phrases', 'public health relevance', 'scale up', 'skills', 'systematic review', 'text searching', 'tool']",NCI,NORTHEASTERN UNIVERSITY,UH2,2016,240650,0.11233760061618969
"HistoTools:  A suite of digital pathology tools for quality control, annotation and dataset identification ABSTRACT: Roughly 40% of the US population will be diagnosed with some form of cancer in their lifetime. In a majority of these cases, a definitive cancer diagnosis is only possible via histopathologic confirmation using a tissue slide. Increasingly, these slides are being digitally scanned as high-resolution images for usage in both clinical and research digital pathology (DP) workflows. Our group has been pioneering the use of deep learning (DL), a form of machine learning, for segmentation, detection, and classification of various cancers using digital pathology images. DL learns features and their associated weighting from large datasets to maximally discriminate between user labeled data (e.g., cancer vs non-cancer, nuclei vs non-nuclei); a paradigm known as “learn from data”. Unfortunately, this paradigm makes DL especially sensitive to low quality slides, noise induced by small errors in the manual user labeling process, and general dataset heterogeneity. As many groups do not intentionally account for these problems, they learn that successful employment of DL technologies relies heavily on explicitly addressing challenges associated with (a) carefully curating high quality slides without preparation or scanning artifacts, (b) obtaining a large precise collection of annotations delineating objects of interest, and (c) selecting diverse datasets to ensure robust classifier performance when clinically deploying the model. To address these challenges we propose HistoTools, a suite of three modules or “Apps”: (1) HistoQC examines slides for artifacts and computes metrics associated with slide presentation characteristics (e.g., stain intensity, compression levels) helping to quantify ranges of acceptable characteristics for downstream algorithmic evaluation. (2) HistoAnno drastically improves the efficiency of annotation efforts using a combined active learning and deep learning approach to ensure experts focus only on regions which are important for classifier improvement. (3) HistoFinder aids in selecting suitable training and test cohorts to guarantee that various tissue level characteristics are well balanced, leading to increased reproducibility. Our team already has working prototypes of HistoQC (100% concordance with a pathologist, evaluated on n>1200 slides) and HistoAnno (30% efficiency improvement during annotation tasks). In this U01, we seek to further develop and evaluate HistoTools in the context of enhancing two companion diagnostic (CDx) assays being developed in our group. First, we will use HistoTools to quality control and annotate nuclei, tubules, and mitosis for improving our CDx classifier for predicting recurrence in breast cancers using a cohort of n>900 patients from completed trial ECOG 2197. Secondly, HistoTools will be employed for quality control and identification of tumor infiltrating lymphocytes and cancer nuclei towards improving our CDx classifier for predicting response to immunotherapy in lung cancer using the n>700 patients from completed clinical trials Checkmate 017 and 057. These tools will build on our existing open source tool repository to aid in real-time feedback and dissemination throughout the ITCR and cancer research community. RELEVANCE: This project will result in development of HistoTools, a new digital pathology toolkit for common pre-experiment machine learning tasks in the oncology domain such as (a) timely identification of poor quality slides and slide regions, (b) quantitative metrics driving optimized cohort selection, and (c) generation of highly precise and relevant annotations. Each component is designed to directly combat an existing bottleneck in the evolving usage of digital pathology. HistoTools will significantly enhance the functionality of existing toolboxes and pipelines, facilitating increasingly sophisticated machine learning applications in oncology.","HistoTools:  A suite of digital pathology tools for quality control, annotation and dataset identification",9897498,U01CA239055,"['Active Learning', 'Address', 'Adoption', 'Algorithms', 'American', 'American Society of Clinical Oncology', 'Automobile Driving', 'Cancer Patient', 'Cell Nucleus', 'Characteristics', 'Classification', 'Clinical', 'Clinical Pathology', 'Clinical Research', 'Clinical Trials', 'Collection', 'Communities', 'Computer Assisted', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Eastern Cooperative Oncology Group', 'Employment', 'Ensure', 'Environment', 'Estrogen receptor positive', 'Evaluation', 'Feedback', 'Generations', 'Histologic', 'Histology', 'Image', 'Immunotherapy', 'International', 'Label', 'Learning', 'Lymphocyte', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Masks', 'Mitosis', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Nature', 'Nivolumab', 'Noise', 'Non-Small-Cell Lung Carcinoma', 'Nuclear', 'Oncology', 'Optics', 'Outcome', 'Paper', 'Pathologist', 'Patients', 'Performance', 'Population', 'Preparation', 'Process', 'Quality Control', 'Recurrence', 'Reproducibility', 'Research', 'Role', 'Scanning', 'Slide', 'Societies', 'Stains', 'Technology', 'Testing', 'The Cancer Imaging Archive', 'Time', 'Tissues', 'Training', 'Tumor-Infiltrating Lymphocytes', 'Validation', 'Visualization', 'Weight', 'Work', 'anticancer research', 'base', 'cancer diagnosis', 'cancer recurrence', 'cohort', 'combat', 'companion diagnostics', 'deep learning', 'design', 'diagnostic assay', 'digital', 'digital pathology', 'experimental study', 'heterogenous data', 'high resolution imaging', 'imaging informatics', 'improved', 'indexing', 'industry partner', 'innovation', 'interactive tool', 'interest', 'large datasets', 'learning network', 'malignant breast neoplasm', 'open source', 'outcome forecast', 'outcome prediction', 'pathology imaging', 'photonics', 'predicting response', 'prognostic', 'prototype', 'quantitative imaging', 'repository', 'response', 'tool', 'tumor heterogeneity']",NCI,CASE WESTERN RESERVE UNIVERSITY,U01,2020,381138,0.010544840696208205
"HistoTools:  A suite of digital pathology tools for quality control, annotation and dataset identification ABSTRACT: Roughly 40% of the US population will be diagnosed with some form of cancer in their lifetime. In a majority of these cases, a definitive cancer diagnosis is only possible via histopathologic confirmation using a tissue slide. Increasingly, these slides are being digitally scanned as high-resolution images for usage in both clinical and research digital pathology (DP) workflows. Our group has been pioneering the use of deep learning (DL), a form of machine learning, for segmentation, detection, and classification of various cancers using digital pathology images. DL learns features and their associated weighting from large datasets to maximally discriminate between user labeled data (e.g., cancer vs non-cancer, nuclei vs non-nuclei); a paradigm known as “learn from data”. Unfortunately, this paradigm makes DL especially sensitive to low quality slides, noise induced by small errors in the manual user labeling process, and general dataset heterogeneity. As many groups do not intentionally account for these problems, they learn that successful employment of DL technologies relies heavily on explicitly addressing challenges associated with (a) carefully curating high quality slides without preparation or scanning artifacts, (b) obtaining a large precise collection of annotations delineating objects of interest, and (c) selecting diverse datasets to ensure robust classifier performance when clinically deploying the model. To address these challenges we propose HistoTools, a suite of three modules or “Apps”: (1) HistoQC examines slides for artifacts and computes metrics associated with slide presentation characteristics (e.g., stain intensity, compression levels) helping to quantify ranges of acceptable characteristics for downstream algorithmic evaluation. (2) HistoAnno drastically improves the efficiency of annotation efforts using a combined active learning and deep learning approach to ensure experts focus only on regions which are important for classifier improvement. (3) HistoFinder aids in selecting suitable training and test cohorts to guarantee that various tissue level characteristics are well balanced, leading to increased reproducibility. Our team already has working prototypes of HistoQC (100% concordance with a pathologist, evaluated on n>1200 slides) and HistoAnno (30% efficiency improvement during annotation tasks). In this U01, we seek to further develop and evaluate HistoTools in the context of enhancing two companion diagnostic (CDx) assays being developed in our group. First, we will use HistoTools to quality control and annotate nuclei, tubules, and mitosis for improving our CDx classifier for predicting recurrence in breast cancers using a cohort of n>900 patients from completed trial ECOG 2197. Secondly, HistoTools will be employed for quality control and identification of tumor infiltrating lymphocytes and cancer nuclei towards improving our CDx classifier for predicting response to immunotherapy in lung cancer using the n>700 patients from completed clinical trials Checkmate 017 and 057. These tools will build on our existing open source tool repository to aid in real-time feedback and dissemination throughout the ITCR and cancer research community. RELEVANCE: This project will result in development of HistoTools, a new digital pathology toolkit for common pre-experiment machine learning tasks in the oncology domain such as (a) timely identification of poor quality slides and slide regions, (b) quantitative metrics driving optimized cohort selection, and (c) generation of highly precise and relevant annotations. Each component is designed to directly combat an existing bottleneck in the evolving usage of digital pathology. HistoTools will significantly enhance the functionality of existing toolboxes and pipelines, facilitating increasingly sophisticated machine learning applications in oncology.","HistoTools:  A suite of digital pathology tools for quality control, annotation and dataset identification",9734599,U01CA239055,"['Active Learning', 'Address', 'Adoption', 'Algorithms', 'American', 'American Society of Clinical Oncology', 'Automobile Driving', 'Cancer Patient', 'Cell Nucleus', 'Characteristics', 'Classification', 'Clinical', 'Clinical Pathology', 'Clinical Research', 'Clinical Trials', 'Collection', 'Communities', 'Computer Assisted', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Eastern Cooperative Oncology Group', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Feedback', 'Generations', 'Heterogeneity', 'Histologic', 'Histology', 'Image', 'Imagery', 'Immunotherapy', 'International', 'Label', 'Learning', 'Lymphocyte', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Masks', 'Mitosis', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Nature', 'Nivolumab', 'Noise', 'Non-Small-Cell Lung Carcinoma', 'Nuclear', 'Optics', 'Outcome', 'Paper', 'Pathologist', 'Patients', 'Performance', 'Population', 'Preparation', 'Process', 'Quality Control', 'Recurrence', 'Reproducibility', 'Research', 'Role', 'Scanning', 'Slide', 'Societies', 'Stains', 'Technology', 'Testing', 'Time', 'Tissues', 'Training', 'Tumor-Infiltrating Lymphocytes', 'Validation', 'Weight', 'Work', 'anticancer research', 'base', 'cancer diagnosis', 'cancer recurrence', 'cohort', 'combat', 'companion diagnostics', 'deep learning', 'design', 'diagnostic assay', 'digital', 'digital pathology', 'experimental study', 'high resolution imaging', 'imaging informatics', 'improved', 'indexing', 'industry partner', 'innovation', 'interactive tool', 'interest', 'learning network', 'malignant breast neoplasm', 'oncology', 'open source', 'outcome forecast', 'outcome prediction', 'pathology imaging', 'photonics', 'predicting response', 'prognostic', 'prototype', 'quantitative imaging', 'repository', 'response', 'tool', 'tumor heterogeneity']",NCI,CASE WESTERN RESERVE UNIVERSITY,U01,2019,383459,0.010544840696208205
"Natural Language Processing and Machine Learning for Cancer Surveillance The purpose of this call order is to provide support in the area of quality control and improvement of cancer data, specifically for Clinical Document Annotation and Processing Pipeline (CDAP), LabKey Software, and the development of annotation schema. n/a",Natural Language Processing and Machine Learning for Cancer Surveillance,10281318,6116004B91020F00002,"['Area', 'Automated Annotation', 'Clinical', 'Data', 'Machine Learning', 'Malignant Neoplasms', 'Natural Language Processing', 'Quality Control', 'software development']",NCI,"WESTAT, INC.",N02,2020,149865,0.0667519206442325

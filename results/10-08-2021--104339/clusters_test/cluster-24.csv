text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"High Performance Text Mining for Translator We propose to build a knowledge provider that will seek out, integrate and provide AI-ready, BioLink-compatible models via high-performance text-mining of the biomedical literature. Problems with Translator’s current mining of the biomedical literature that we intend to solve include: (1) weaknesses in framework extensibility and benchmarking that make integrating and validating new text-mining approaches difficult; (2) problematic licensing of software, terminologies and other resources that do not adequately support FAIR (and TLC) best practices; (3) processing only PubMed titles and abstracts, not full text publications; (4) Translator’s use of older NLP technology with relatively poor performance; (5) lack of a mechanism for community feedback regarding errors and other problems; (6) lack of continuous updates to add knowledge from new publications; (7) output knowledge representation that is simplistic and vague, failing to reflect the richness of what is expressed in scientific documents. Plan for implementation: Our team has a long history of productive NLP research, successful open source software projects, effective benchmarking and broad community engagement. We will build on the results of NLM-funded work in information extraction, our gold-standard Colorado Richly Annotated Full Text (CRAFT) corpus, a recent BioNLP Open Shared Task (BioNLP-OST) that we organized, and recent advances in state-of-the-art NLP. For Segment 1, we will: (1) Demonstrate BioStacks, an extensible, cloud-based text-mining framework that produces knowledge graphs grounded in the Open Biomedical Ontologies (OBOs). This BioStacks demo will include a state-of-the-art OBO concept recognizer for multiple ontologies, a state-of-the-art semantic relationship prediction tool, and a state-of-the-art structural analysis tool. All generated assertions will have provenance metadata linking the assertion to a particular text span in a document specified by PMCID. (2) Demonstrate CRAFTST, a cloud-based text-mining evaluation system that evaluates the performance of text-mining systems against the CRAFT gold standard. (3) Demonstrate an adaptive machine learning process illustrating how to efficiently create tools to extract BioLink association types. For Segment 2, we propose to extend the text-mining and evaluation frameworks to align with BioLink and the Translator community, improve text-mining quality and expand the collection of source documents mined. Specifically, we propose to target 10 long term milestones: (1) Align CRAFT to BioLink. (2) Develop new tools for extracting associations from text. (3) Develop and manage a community engagement process on text-mining for Translator. (4) Extend benchmarking. (5) Improve recall. (6) Improve precision. (7) Improve computational efficiency. (8) Expand BioStacks to include all available full text biomedical journal articles. (9) Expand document collections to include Patents & Regulatory filings. (10) Develop a scientist-based movement to improve document access for text-mining from non-open publishers. The types of questions the resulting knowledge graph can be used to address are extremely broad, as it is generated by mining a large part of the biomedical literature. Questions that can be answered include those about specific assertions (e.g. is this drug an agonist-activator of this protein?), general relations (are these two proteins often mentioned together?), and documents (which publications mention this gene, mutation and drug?). Integration: We are long-time contributors to the open-science community and have longstanding collaborations with existing awardees; we were participants in the NIH Data Commons Pilot. We propose to align the output of text-mining tools to the BioLink model via OBO terms. We propose to implement our frameworks in NIH Cloud Computing environments. We propose to adopt the CD2H Contributor Attribution Model to foreground community contributions. We plan to coordinate with the NLM’s nascent benchmarking activities and the SmartAPI effort to build Translator standard interfaces. Challenges and gaps: High-performance mining of rich, contextualized knowledge from the literature remains a difficult task, and is unlikely to be solved in the next five years. Many important publications remain inaccessible to text-mining due to restrictive licensing. n/a",High Performance Text Mining for Translator,10334356,OT2TR003422,"['Address', 'Adopted', 'Agonist', 'Benchmarking', 'Cloud Computing', 'Collaborations', 'Collection', 'Colorado', 'Communities', 'Computer software', 'Data Commons', 'Environment', 'Evaluation', 'Feedback', 'Funding', 'Gene Mutation', 'Gold', 'Information Retrieval', 'Knowledge', 'Legal patent', 'Licensing', 'Link', 'Literature', 'Machine Learning', 'Metadata', 'Mining', 'Modeling', 'Movement', 'Ontology', 'Output', 'Participant', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Provider', 'PubMed', 'Publications', 'Recording of previous events', 'Research', 'Resources', 'Scientist', 'Semantics', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Terminology', 'Text', 'Time', 'United States National Institutes of Health', 'Update', 'Work', 'base', 'biomedical ontology', 'cloud based', 'improved', 'information organization', 'journal article', 'knowledge graph', 'knowledge of results', 'open data', 'open source', 'text searching', 'tool']",NCATS,UNIVERSITY OF COLORADO DENVER,OT2,2021,471239
"The Metadata Powerwash - Integrated tools to make biomedical data FAIR Project Summary  The metadata that describe scientific data are fundamental resources to enable (1) the discovery and reuse of the data and (2) the reproducibility of the experiments that generated the data in the first place. Metadata are essential for scientists to understand the associated data and to reuse them, as well as for information technology to index the data, to make the data available, and to provide filters for scientists to search for the corresponding datasets. Currently, the scientific metadata hosted in public repositories suffer from multiple quality issues that limit scientists’ ability to find and reuse the experimental datasets to which they refer. It can take many weeks of a scientist’s time to identify a collection of datasets that fulfill specific criteria when the data are so poorly described—and the majority of the process is necessarily manual.  We propose to develop an end-to-end solution to standardize biomedical metadata with the help of ontologies—data structures that define the terms in an application domain and the relationships among them. There are hundreds of ontologies that provide standard terms for use in biomedicine, and they are essential resources to make biomedical metadata interoperable and reusable. Our approach also will build on the technology created by the Center for Expanded Data Annotation and Retrieval (CEDAR), which offers a library of building blocks and common data elements for defining computer-based metadata templates based on community standards.  Our plan involves three specific aims. First, we will develop a method and tool to standardize the multiple, ad hoc metadata field names that may appear in metadata to represent the same type of information by replacing those field names with the field names used in standard metadata templates or, if no appropriate template match is available, with terms from a relevant ontology. Second, we will develop methods and tools to standardize different types of metadata field values, for example, categorical values such as drugs or diseases, and numerical values such as age, or sample collection date. Third, we will evaluate the speed, precision, and recall of our metadata transformation pipeline—built out of the methods and tools to standardize field names and values—on a large corpus of metadata that we will manually curate based on existing public metadata. We will also carry out experiments to test the effect of the standardized metadata when biomedical scientists perform dataset search in the context of their work. Project Narrative Data that offer precise descriptions of data—metadata—are critical scientific resources that facilitate the discovery, reuse, and reproducibility of the data to which they refer. Our goal is to create methods and tools that improve the quality of scientific metadata hosted in public repositories, and thus enhance the discoverability and re-use of public biomedical datasets. Making data more accessible through scientifically rigorous metadata will accelerate the ability to make transformative data-driven biomedical discoveries using public data archives.",The Metadata Powerwash - Integrated tools to make biomedical data FAIR,10093841,R01LM013498,"['Age', 'Biological Specimen Banks', 'Categories', 'Collection', 'Common Data Element', 'Communities', 'Computers', 'Data', 'Data Science', 'Data Set', 'Disease', 'FAIR principles', 'Funding Agency', 'Goals', 'Gold', 'Information Technology', 'Knowledge', 'Libraries', 'Link', 'Manuals', 'Metadata', 'Methods', 'Names', 'Natural Language Processing', 'Numerical value', 'Ontology', 'Pharmaceutical Preparations', 'Problem Solving', 'Process', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Sampling', 'Science', 'Scientist', 'Specific qualifier value', 'Speed', 'Standardization', 'Structure', 'Technology', 'Testing', 'Time', 'Variant', 'Work', 'base', 'biomedical scientist', 'data archive', 'data repository', 'data reuse', 'experimental study', 'improved', 'indexing', 'information organization', 'interoperability', 'metadata standards', 'public repository', 'repository', 'sample collection', 'search engine', 'secondary analysis', 'tool']",NLM,STANFORD UNIVERSITY,R01,2021,334847
